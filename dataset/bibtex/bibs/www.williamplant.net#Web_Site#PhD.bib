
%% Created for William Plant at 2009-04-29 23:23:12 +0100 


%% Saved with string encoding Unicode (UTF-8) 


@inproceedings{Muller:2002la,
	Author = {H. M\"{u}ller and S. Marchand-Maillet and T. Pun},
	Booktitle = {International Conference on Image and Video Retrieval},
	Date-Added = {2009-05-10 14:11:08 +0100},
	Date-Modified = {2009-05-10 14:12:30 +0100},
	Pages = {38-49},
	Title = {The Truth about Corel - Evaluation in Image Retrieval},
	Year = {2002}}

@misc{Flickr:2009,
	author = {Flickr},
	note = {http://www.flickr.com/},
	Year = {2009}}

@misc{Google:2009fp,
	author = {Google Picasa},
	note = {http://picasa.google.com/},
	Year = {2009}}

@misc{Apple:2009eu,
	author = {Apple iPhoto},
	note = {http://www.apple.com/ilife/iphoto/},
	Year = {2009}}

@article{Sarkar:1994rm,
	Annote = {@article{198384,
 author = {Sarkar,, Manojit and Brown,, Marc H.},
 title = {Graphical fisheye views},
 journal = {Commun. ACM},
 volume = {37},
 number = {12},
 year = {1994},
 issn = {0001-0782},
 pages = {73--83},
 doi = {http://doi.acm.org/10.1145/198366.198384},
 publisher = {ACM},
 address = {New York, NY, USA},
 }},
	Author = {M. Sarkar and M. Brown},
	Booktitle = {Proceedings of CHI'92 Conference on Human Factors in Computing Systems},
	Date-Added = {2009-04-29 22:42:10 +0100},
	Date-Modified = {2009-04-29 22:46:55 +0100},
	Journal = {Communications of the ACM},
	Number = {12},
	Pages = {73-83},
	Title = {Graphical fisheye views},
	Volume = {37},
	Year = {1994}}

@inproceedings{Koikkalainen:1990sf,
	Annote = {, ``,'' 
in , Washington, 
DC, 1990, pp. 279--285. },
	Author = {P. Koikkalainen and E. Oja},
	Booktitle = {Proceedings of International Joint Conference on Neural Networks},
	Date-Added = {2009-04-29 22:38:47 +0100},
	Date-Modified = {2009-04-29 22:40:43 +0100},
	Pages = {279--285},
	Title = {Self-organizing hierarchical feature maps},
	Volume = {2},
	Year = {1990}}

@book{Jain:1988ta,
	Annote = {. , Prentice Hall, 1988},
	Author = {A. K. Jain and R. C. Dubes},
	Date-Added = {2009-04-28 18:51:31 +0100},
	Date-Modified = {2009-04-28 18:52:07 +0100},
	Publisher = {Prentice Hall},
	Title = {Algorithms for Clustering Data},
	Year = {1988}}

@article{Gupta:1997zh,
	Annote = {@article{253798,
 author = {Gupta,, Amarnath and Jain,, Ramesh},
 title = {Visual information retrieval},
 journal = {Commun. ACM},
 volume = {40},
 number = {5},
 year = {1997},
 issn = {0001-0782},
 pages = {70--79},
 doi = {http://doi.acm.org/10.1145/253769.253798},
 publisher = {ACM},
 address = {New York, NY, USA},
 }},
	Author = {A. Gupta and R. Jain},
	Date-Added = {2009-04-28 18:47:25 +0100},
	Date-Modified = {2009-04-28 18:49:00 +0100},
	Journal = {Communications of the ACM},
	Number = {5},
	Pages = {70-79},
	Title = {Visual information retrieval},
	Volume = {40},
	Year = {1997}}

@book{Atkinson:1989wf,
	Annote = {Kendall E. Atkinson. An Introduction to Numerical Analysis. John Wiley & Sons - 1989
},
	Author = {K. E. Atkinson},
	Date-Added = {2009-04-28 18:40:41 +0100},
	Date-Modified = {2009-04-28 18:41:46 +0100},
	Publisher = {John Wiley and Sons },
	Title = {An Introduction to Numerical Analysis},
	Year = {1989}}

@article{Laaksonen:2002qm,
	Author = {J. Laaksonen and M. Koskela and E. Oja},
	Date-Added = {2009-04-28 17:54:32 +0100},
	Date-Modified = {2009-04-28 17:57:53 +0100},
	Journal = {IEEE Transactions on Neural Networks: Special Issue on Multimedia Processing },
	Number = {4},
	Pages = {841-853},
	Title = {Pic{SOM} -- {S}elf-{O}rganizing {I}mage {R}etrieval with {MPEG}-7 {C}ontent {D}escriptors},
	Volume = {13},
	Year = {2002}}

@inproceedings{Worring:2007il,
	Annote = {@inproceedings{1290125,
 author = {Worring,, Marcel and de Rooij,, Ork and van Rijn,, Ton},
 title = {Browsing visual collections using graphs},
 booktitle = {MIR '07: Proceedings of the international workshop on Workshop on multimedia information retrieval},
 year = {2007},
 isbn = {978-1-59593-778-0},
 pages = {307--312},
 location = {Augsburg, Bavaria, Germany},
 doi = {http://doi.acm.org/10.1145/1290082.1290125},
 publisher = {ACM},
 address = {New York, NY, USA},
 }},
	Author = {M. Worring and O. de Rooij and T. van Rijn},
	Booktitle = {International Workshop on Multimedia Information Retrieval},
	Date-Added = {2009-04-28 16:31:44 +0100},
	Date-Modified = {2009-04-28 16:33:20 +0100},
	Pages = {307-312},
	Title = {Browsing visual collections using graphs},
	Year = {2007}}

@inproceedings{Cruz-Neira:1993tw,
	Author = {C. Cruz-Neira and D. Sandin and T. DeFanti},
	Booktitle = {Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques},
	Date-Added = {2009-04-28 16:19:55 +0100},
	Date-Modified = {2009-04-28 16:21:16 +0100},
	Pages = {135-142},
	Title = {Surround-Screen Projection-based Virtual Reality: The Design and Implementation of the CAVE},
	Year = {1993}}

@article{Linde:1980ec,
	Author = {Y. Linde and A. Buzo and R. Gray},
	Date-Added = {2009-04-28 16:09:19 +0100},
	Date-Modified = {2009-04-28 16:10:41 +0100},
	Journal = {IEEE Transactions on Communications},
	Pages = {84-94},
	Title = {An Algorithm for Vector Quantizer Design},
	Volume = {28},
	Year = {1980}}

@inproceedings{Schaefer:2006fe,
	Annote = {AUTHOR = "Schaefer, G. and Ruszala, S.",
        TITLE = "Image Database Navigation on a Hierarchical MDS Grid",
        BOOKTITLE = DAGM06,
        YEAR = "2006",
        PAGES = "304-313",},
	Author = {Schaefer, G. and Ruszala, S.},
	Booktitle = {28th Pattern Recognition Symposium},
	Date-Added = {2009-04-28 16:03:58 +0100},
	Date-Modified = {2009-04-28 16:04:39 +0100},
	Pages = {304-313},
	Title = {Image Database Navigation on a Hierarchical {MDS} Grid},
	Year = {2006}}

@book{Jambu:1991xr,
	Annote = {@book{532191,
 author = {Jambu,, Michel},
 title = {Exploratory and Multivariate Data Analysis},
 year = {1991},
 isbn = {0123800900},
 publisher = {Academic Press, Inc.},
 address = {Orlando, FL, USA},
 }},
	Author = {M. Jambu},
	Date-Added = {2009-04-28 15:57:57 +0100},
	Date-Modified = {2009-04-28 15:58:55 +0100},
	Publisher = {Academic Press, Inc.},
	Title = {Exploratory and Multivariate Data Analysis},
	Year = {1991}}

@article{Tenenbaum:2000fu,
	Annote = { J.B. Tenenbaum, V.D. Silva, J.C. Langford, A global geometric framework for nonlinear dimensionality 
reduction, Science 290 (5500) 2000, 2319--2322},
	Author = {J. Tenenbaum and  V. Silva and  J. Langford},
	Date-Added = {2009-04-28 15:51:41 +0100},
	Date-Modified = {2009-04-28 15:54:09 +0100},
	Journal = {Science},
	Number = {5500},
	Pages = {2319--2322},
	Title = {A global geometric framework for nonlinear dimensionality reduction},
	Volume = {290},
	Year = {2000}}

@article{Roweis:2000dz,
	Annote = {@article{citeulike:108703,
	author = { },
	citeulike-article-id = {108703},
	comment = {曲がった空間を伸ばす},
	doi = {10.1126/science.290.5500.2323},
	journal = {Science},
	keywords = {locally-linear-embedding, machine-learning},
	month = {December},
	number = {5500},
	pages = {2323--2326},
	posted-at = {2006-01-13 02:31:43},
	priority = {2},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	url = {http://dx.doi.org/10.1126/science.290.5500.2323},
	volume = {290},
	year = {2000}
}},
	Author = {S. Roweis and L. Saul},
	Date-Added = {2009-04-28 15:49:48 +0100},
	Date-Modified = {2009-04-28 15:51:13 +0100},
	Journal = {Science},
	Number = {5500},
	Pages = {2323-2326},
	Title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	Volume = {290},
	Year = {2000}}

@inproceedings{Hinton:2002fv,
	Author = {G. Hinton and S. Roweis},
	Booktitle = {Advances in Neural Information Processing Systems 15},
	Date-Added = {2009-04-28 15:41:00 +0100},
	Date-Modified = {2009-04-28 23:03:13 +0100},
	Pages = {833-840},
	Title = {Stochastic Neighbor Embedding},
	Year = {2002}}

@book{Kohonen:1997it,
	Author = {T. Kohonen},
	Date-Added = {2009-04-28 15:35:50 +0100},
	Date-Modified = {2009-04-28 15:38:10 +0100},
	Publisher = {Springer-Verlag},
	Title = {Self-organizing Maps},
	Year = {1997}}

@article{Sammon:1969ai,
	Annote = {Sammon, Jr., J. W. (1969) A nonlinear mapping for data structure analysis. IEEE Transactions on Computers, 18:401-409.},
	Author = {J. W. Sammon },
	Date-Added = {2009-04-22 11:47:06 +0100},
	Date-Modified = {2009-04-22 11:48:24 +0100},
	Journal = {IEEE Transactions on Computers},
	Volume = {18},
	Number = {5},
	Pages = {401-409},
	Title = {A nonlinear mapping for data structure analysis},
	Year = {1969}}

@article{Rui1998,
	Annote = { Relevance feedback: a power tool for interactive 
content-based image retrieval, , 
1998, pp. . 
},
	Author = {Y. Rui and T.S. Huang and M. Ortega and M. Mehrotra},
	Date-Added = {2009-04-14 16:16:30 +0100},
	Date-Modified = {2009-04-14 16:17:42 +0100},
	Journal = {IEEE Transaction on Circuits and systems for Video Technology 8},
	Pages = {644--655},
	Title = {Relevance feedback: a power tool for interactive content-based image retrieval},
	Year = {1998}}

@article{Porta:2009zr,
	Author = {M. Porta},
	Date-Added = {2009-03-11 09:53:56 +0000},
	Date-Modified = {2009-03-12 13:46:34 +0000},
	Journal = {International Journal of Image and Graphics},
	Number = {1},
	Pages = {27-49},
	Title = {New Visualization Modes for Effective Image Presentation},
	Volume = {9},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvUG9ydGEyMDA5enIucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9Qb3J0YTIwMDl6ci5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD0X7xd02/QAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxd02/QAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlBvcnRhMjAwOXpyLnBkZgAADgAgAA8AUABvAHIAdABhADIAMAAwADkAegByAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1BvcnRhMjAwOXpyLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@inproceedings{Rodden:2000ys,
	Annote = {K. Rodden, W. Basalaj, D. Sinclair, and K. Wood, A comparison of measures for visualising image similarity, Proceedings of The Challenge of Image Retrieval (CIR 2000)},
	Author = {K. Rodden and W. Basalaj and D. Sinclair and K. Wood},
	Booktitle = {The Challenge of Image Retrieval},
	Date-Added = {2009-03-11 09:51:09 +0000},
	Date-Modified = {2009-03-11 09:51:58 +0000},
	Title = {A comparison of measures for visualising image similarity},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvUm9kZGVuMjAwMHlzLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQUm9kZGVuMjAwMHlzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9GYMXdOdoAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXdOdoAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSb2RkZW4yMDAweXMucGRmAA4AIgAQAFIAbwBkAGQAZQBuADIAMAAwADAAeQBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JvZGRlbjIwMDB5cy5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Qiu:2007gb,
	Author = {Qiu,G. and Morris, J. and Fan, X.},
	Date-Added = {2009-03-02 10:55:59 +0000},
	Date-Modified = {2009-03-12 13:46:41 +0000},
	Journal = {Pattern Recognition},
	Number = {6},
	Pages = {1711-1721},
	Title = {Visual guided navigation for image retrieval},
	Volume = {40},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvUWl1MjAwN2diLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNUWl1MjAwN2diLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7u5cXRa4oAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXRa4oAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpRaXUyMDA3Z2IucGRmAAAOABwADQBRAGkAdQAyADAAMAA3AGcAYgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9RaXUyMDA3Z2IucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Chen:1997tw,
	Author = {J. Chen and C. Bouman and J.P. Allebach},
	Booktitle = {IEEE International Conference on Image Processing},
	Date-Added = {2009-03-02 09:52:50 +0000},
	Date-Modified = {2009-03-02 09:54:21 +0000},
	Pages = {827-830},
	Title = {Fast Image Database Search using Tree-Structured VQ},
	Volume = {2},
	Year = {1997},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvQ2hlbjE5OTd0dy5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkNoZW4xOTk3dHcucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAO7jfF0VzRAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADF0VzRAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Q2hlbjE5OTd0dy5wZGYADgAeAA4AQwBoAGUAbgAxADkAOQA3AHQAdwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9DaGVuMTk5N3R3LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@article{Assfalg:2000ao,
	Author = {J. Assfalg and A. Del-Bimbo and P. Pala},
	Date-Added = {2009-03-02 09:40:49 +0000},
	Date-Modified = {2009-03-12 13:49:07 +0000},
	Journal = {Journal of Visual Languages and Computing},
	Number = {2},
	Pages = {105-124},
	Title = {Virtual Reality for Image Retrieval},
	Volume = {11},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvQXNzZmFsZzIwMDBhby5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUFzc2ZhbGcyMDAwYW8ucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAO7grF0VnUAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADF0VnUAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6QXNzZmFsZzIwMDBhby5wZGYAAA4AJAARAEEAcwBzAGYAYQBsAGcAMgAwADAAMABhAG8ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQXNzZmFsZzIwMDBhby5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@article{Cohen:1997la,
	Author = {Cohen, H.A.},
	Date-Added = {2009-03-02 09:34:34 +0000},
	Date-Modified = {2009-03-12 13:48:21 +0000},
	Journal = {Journal of Visual Communication and Image Representation},
	Number = {2},
	Pages = {226-234},
	Title = {Retrieval and Browsing of Images Using Image Thumbnails},
	Volume = {8},
	Year = {1997},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvQ29oZW4xOTk3bGEucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9Db2hlbjE5OTdsYS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADu3MxdFYjAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxdFYjAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkNvaGVuMTk5N2xhLnBkZgAADgAgAA8AQwBvAGgAZQBuADEAOQA5ADcAbABhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0NvaGVuMTk5N2xhLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@inproceedings{Meiers:2002fy,
	Author = {Meiers, T. and Sikora, T. and Keller, I.},
	Booktitle = {Proceedings of the International Conference on Image Processing},
	Date-Added = {2009-03-02 09:28:48 +0000},
	Date-Modified = {2009-03-12 13:46:11 +0000},
	Pages = {593-596},
	Title = {Hierarchical image database browsing environment with embedded relevance feedback},
	Volume = {2},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvTWVpZXJzMjAwMmZ5LnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQTWVpZXJzMjAwMmZ5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7toMXRVzgAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXRVzgAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpNZWllcnMyMDAyZnkucGRmAA4AIgAQAE0AZQBpAGUAcgBzADIAMAAwADIAZgB5AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL01laWVyczIwMDJmeS5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Schweitzer:1999kl,
	Author = {H. Schweitzer},
	Date-Added = {2009-03-02 09:24:24 +0000},
	Date-Modified = {2009-03-12 13:47:16 +0000},
	Journal = {Image and Vision Computing},
	Pages = {501-511},
	Title = {Organizing image databases as visual-content search trees},
	Volume = {17},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBtQYXBlcnMvU2Nod2VpdHplcjE5OTlrbC5wZGbSGw8cHVdOUy5kYXRhTxEB0AAAAAAB0AACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTFFNjaHdlaXR6ZXIxOTk5a2wucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAO7WrF0VY5AAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADF0VY5AAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAUk1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6U2Nod2VpdHplcjE5OTlrbC5wZGYADgAqABQAUwBjAGgAdwBlAGkAdAB6AGUAcgAxADkAOQA5AGsAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARVVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9TY2h3ZWl0emVyMTk5OWtsLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAyADNANUCqQKrArACuQLEAsgC1gLdAuYC6wLuAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAvs=}}

@inproceedings{Keller:2001oq,
	Author = {Keller, I. and Meiers, T. and Ellerbrock, T. and Sikora, T.},
	Booktitle = {IEEE Workshop on Content-Based Access of Image and Video Libraries},
	Date-Added = {2009-03-02 09:18:11 +0000},
	Date-Modified = {2009-03-12 13:47:57 +0000},
	Pages = {102-108},
	Title = {Image browsing with {PCA}-assisted user-interaction},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvS2VsbGVyMjAwMW9xLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQS2VsbGVyMjAwMW9xLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7tMsXRVLkAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXRVLkAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpLZWxsZXIyMDAxb3EucGRmAA4AIgAQAEsAZQBsAGwAZQByADIAMAAwADEAbwBxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0tlbGxlcjIwMDFvcS5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Tian:2000hl,
	Author = {Tian, G.Y. and Taylor, D.},
	Booktitle = {IEEE International Conference on Information Visualization},
	Date-Added = {2009-03-02 09:13:27 +0000},
	Date-Modified = {2009-03-12 13:47:08 +0000},
	Pages = {221-225},
	Title = {Colour image retrieval using virtual reality},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvVGlhbjIwMDBobC5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDlRpYW4yMDAwaGwucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAO7PvF0VOSAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADF0VOSAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6VGlhbjIwMDBobC5wZGYADgAeAA4AVABpAGEAbgAyADAAMAAwAGgAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9UaWFuMjAwMGhsLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Liere:1999fp,
	Author = {R. van Liere and W. de Leeuw},
	Booktitle = {Workshop on New Paradigms in Information Visualization and Manipulation, held in conjunction with the eighth ACM International Conference on Information and Knowledge Management},
	Date-Added = {2009-03-02 09:07:38 +0000},
	Date-Modified = {2009-03-12 13:47:42 +0000},
	Pages = {83-86},
	Publisher = {ACM},
	Title = {Exploration of large image collections using virtual reality devices},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvTGllcmUxOTk5ZnAucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9MaWVyZTE5OTlmcC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADuzLxdFSMQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxdFSMQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkxpZXJlMTk5OWZwLnBkZgAADgAgAA8ATABpAGUAcgBlADEAOQA5ADkAZgBwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0xpZXJlMTk5OWZwLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Bartolini:2006eu,
	Author = {I. Bartolini and P. Ciaccia and M. Patella },
	Date-Added = {2009-03-02 08:59:54 +0000},
	Date-Modified = {2009-03-02 09:02:17 +0000},
	Journal = {Multimedia Tools and Applications},
	Number = {3},
	Pages = {269-286},
	Title = {Adaptively browsing image databases with PIBE},
	Volume = {31},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvQmFydG9saW5pMjAwNmV1LnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTQmFydG9saW5pMjAwNmV1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7sg8XRT0sAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXRT0sAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpCYXJ0b2xpbmkyMDA2ZXUucGRmAAAOACgAEwBCAGEAcgB0AG8AbABpAG4AaQAyADAAMAA2AGUAdQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9CYXJ0b2xpbmkyMDA2ZXUucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@article{Eidenberger:2004hz,
	Annote = {This paper is the inspiration for my SVG hexagonal system, using (self-organising maps)SOM to assign video frames to clusters, indexing shots hierarchically based on either content or time. The content based tree is created using bottom-up clustering, whereas the temporal tree is created using a divisive strategy.

The paper describes a SOM as "a two-layer fully connected neural network that uses feed-forward learning. SOM's are mainly used for high dimensional data".

Here, the input layer is a 1D feature vector, created from an image using MPEG-7 descriptors. The output is a 2D map of clusters. The clusters are either rectangular or hexagonal, and are described by a codebook vector (weight vector pointing to centre of cluster). Input vectors are mapped to clusters via minimal euclidean distances (closest cluster to feature vector). The map is adapted iteratively my fractionally moving the location of the clusters, based on some learning rate. 

The authors state that an added feature of SOMs over ordinary clustering is the ability to apply neighbourhood kernels, which are 2D functions defining the fraction at which the BMU (best match uni) and neighbouring clusters should be adapted to. This results in "natural clusters that intuitively fit with humans' similarity perception".

The paper describes the keyframe cutting methods used.

For the 2 trees, the leaves in the context are individual shots, wheras in time, the leaves are frames of a video. Layers in the map are arranged by content similarity (from MPEG-7 descriptors). On the top level, 1 map exists. For every subsequent level, a map exists for every cluster.

All clusters that contain a number of elements exceeding a threshold are recursively divided into further clusters. For the time based tree, every cluster has an image, whereas by content, gaps may occur. The frame most similar to the codebook vector is selected as the representative image.

For switching between the trees, a cluster in each of the trees is said to be corresponding if they share the same representative media. To reduce this to 1 candidate, cell nearest to the current layer is selected.

As the time-indexed tree has all frames indexed, therefore any cell selected in the content based tree will have a correseponding cell in the time based. This is not the same the other way round. To counteract this, the leaf node of the corresponding cell is selected and a message displayed to the user in order to avoid confusion.

Seven of the MPEG-7 descriptors were used, and the remainder of the paper describes the interface with figures. In their prototype, 48 cells are displayed to the user, along with previews of previous/next layers and preview of the image/frame. 

No user study is submitted, however the authors say from initial testing, the content based tree appears to be the primary browsing tool. One disadvantage labelled by the author is that the system loses the temporal organisation of the video. For a pure image implemenation, this is not an issue.},
	Author = {H. Eidenberger },
	Date-Added = {2009-02-25 19:19:21 +0000},
	Date-Modified = {2009-04-14 16:08:02 +0100},
	Journal = {International Journal of Fuzzy Systems},
	Keywords = {SOM},
	Number = {3},
	Read = {Yes},
	Title = {A Video Browsing Application Based on Visual {MPEG-7} Descriptors and Self-Organising Maps},
	Volume = {6},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBxQYXBlcnMvRWlkZW5iZXJnZXIyMDA0aHoucGRm0hsPHB1XTlMuZGF0YU8RAdQAAAAAAdQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxVFaWRlbmJlcmdlcjIwMDRoei5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADruvxch6MgAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxch6MgAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFNNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkVpZGVuYmVyZ2VyMjAwNGh6LnBkZgAADgAsABUARQBpAGQAZQBuAGIAZQByAGcAZQByADIAMAAwADQAaAB6AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBGVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0VpZGVuYmVyZ2VyMjAwNGh6LnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDJAM4A1gKuArACtQK+AskCzQLbAuIC6wLwAvMAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADAA==}}

@inproceedings{Clough:2005hc,
	Author = {Clough, P. and Joho, H. and Sanderson, M.},
	Booktitle = {Proceeding of ACM Multimedia Workshop for the Special Interest Group on Information Retrieval},
	Date-Added = {2009-02-19 23:45:13 +0000},
	Date-Modified = {2009-02-19 23:49:37 +0000},
	Title = {Automatically organising images using concept hierarchies},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvQ2xvdWdoMjAwNWhjLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQQ2xvdWdoMjAwNWhjLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6fwcXDl6kAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXDl6kAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpDbG91Z2gyMDA1aGMucGRmAA4AIgAQAEMAbABvAHUAZwBoADIAMAAwADUAaABjAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0Nsb3VnaDIwMDVoYy5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{MacCuish:1996yg,
	Author = {MacCuish, J. and McPherson, A. and Barros, J. and Kelly, P.},
	Booktitle = {Proceedings of SPIE Conference on Visual Data Exploration and Analysis III},
	Date-Added = {2009-02-19 23:39:47 +0000},
	Date-Modified = {2009-03-12 13:47:34 +0000},
	Editor = {2656},
	Pages = {104-115},
	Title = {Interactive layout mechanisms for image database retrieval},
	Year = {1996},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvTWFjQ3Vpc2gxOTk2eWcuUERG0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJNYWNDdWlzaDE5OTZ5Zy5QREYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADqAvxcOaNgAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxcOaNgAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOk1hY0N1aXNoMTk5NnlnLlBERgAOACYAEgBNAGEAYwBDAHUAaQBzAGgAMQA5ADkANgB5AGcALgBQAEQARgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTWFjQ3Vpc2gxOTk2eWcuUERGAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@article{Vendrig:2001cy,
	Author = {Vendrig,, Jeroen and Worring,, Marcl and Smeulders,, Arnold W. M.},
	Date-Added = {2009-02-19 23:30:27 +0000},
	Date-Modified = {2009-03-12 13:46:58 +0000},
	Journal = {Multimedia Tools Appl.},
	Number = {1},
	Pages = {83-103},
	Title = {Filter Image Browsing: Interactive Image Retrieval by Using Database Overviews},
	Volume = {15},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvVmVuZHJpZzIwMDFjeS5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEVZlbmRyaWcyMDAxY3kucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOoFDFw5umAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFw5umAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6VmVuZHJpZzIwMDFjeS5wZGYAAA4AJAARAFYAZQBuAGQAcgBpAGcAMgAwADAAMQBjAHkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvVmVuZHJpZzIwMDFjeS5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Hilliges:2007bl,
	Annote = {This paper details PhotoHelix, an interactive tabletop system. Using a specially adapted piece of hardware (a cross between an IKEA egg timer and an optical mouse), when placed on an interactive touch screen surface, a graphical helix is displayed ordering images by time. The paper does touch upon the previous work of the PDH (described in Moghaddam:2004rz) but states that their system requires no image annotation.

The authors state 4 principal activities for any future system:
1) Filing - sorting images to albums/folders
2) Selecting - deciding which images to keep/dispose
3) Sharing - e-mailing, printing etc.
4) Browsing

The paper also explains photo talk - the process of looking at physical images with friends and family whilst explaining what is happening in the photo. Future programs should adhere to the following design rules:

1) Overview at all times
2) Details on demand - quick individual image viewing
3) Support for temporary structures - placing images into temp. groups in order to explain a story
4) Flexible spatial arrangements

In the spiral, new groups (piles) are shown as larger. A lens on the spiral causes any image or pile displayed under the lens to be magnified and shown in more detail. An "umbilical cord" is used to show groups away from the spiral, but show where in the spiral the group occurs. Groups can be removed by "cutting" the umbilical cord. Individual images can be moved freely on the surface, as well as rotated and resized using a widget.

Image selection, as described in Hilliges previous work, is not implemented due to its performance not being stable enough.

A user study is conducted, with 4 tasks:
1) File images permanently according to some event
2) Browse for an event, choose a particular photo and display it
3) Sharing - give an update about a recent vacation
4) Choose image candidates from the 4 seasons to best represent it.

Users were very positive about the system, however some complaints were made such as images in the calander being too small, and that groups become visually cluttered over 30 images. The system can only cope with a few hundred images over the course of 2 years.

},
	Author = {Hilliges, O. and Baur, D. and Butz, A.},
	Booktitle = {Proceedings of the 2nd IEEE Tabletop Workshop on Horizontal Interactive Human-Computer Systems},
	Date-Added = {2009-02-19 16:50:02 +0000},
	Date-Modified = {2009-03-05 13:22:47 +0000},
	Pages = {87-94},
	Read = {Yes},
	Title = {Photohelix: Browsing, Sorting and Sharing Digital Photo Collections},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvSGlsbGlnZXMyMDA3YmwucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJIaWxsaWdlczIwMDdibC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADptwxcM9CAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxcM9CAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkhpbGxpZ2VzMjAwN2JsLnBkZgAOACYAEgBIAGkAbABsAGkAZwBlAHMAMgAwADAANwBiAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvSGlsbGlnZXMyMDA3YmwucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Hilliges:2007vh,
	Annote = {This paper explores browsing personal image collections, along with the process of classifying images as either "good" or "bad". The paper explores which image features work best for this classification.

In the overview, images are clustered based upon low-level features and each cluster represented by selected thumbnails (selection process not detailed). These thumbnails give an approximate ratio between good and bad images. The cluster size depicts the number of images in the cluster.

Once fully zoomed, images can be selected individually. In a zoomed in view of the cluster, good images are located at the centroid whilst poorer images are located further away, and grouped based on their difficiency (i.e. blurry, over exposed etc.). Images can be directly manipulated, or in the case of poor images, deleted. The authors specify that with digital photography users tend to take multilple images of the same scene at different perspectives.

Colour and texture are used. For the colour, YUV colour co-ordinates are used. The U and V channels are split into 6 sections, resulting in a 36D histogram. For texture the first 4 roughness moments are taken, aswell as the Haralick textural feature number 11. To cluster the images, X-Means (a variation of k-means) is used.

Using Support Vector Machines (SVM), the linear seperation of classes in the feature space is found by calculating a maximum hyperplane between the training examples. In this work, a "one-vs-one" approach is taken, where a single SVM is computed between every pair of classes, and the boolean AND operator used.

During testing, the YUV was found to cluster images best. For exposure detection (under/normal/over) roughness was superior, whilst the haralick feature 11 can distinguish between sharp and blurry images.

A user study was conducted, with most users praising the system. However one common comment from users is that chronological ordering would be easier in certain situations.},
	Author = {Hilliges, O. and Kunath, P. and Pryakhin, A. and Butz, A. and Kriegel, H.P.},
	Booktitle = {Proceedings of the International Conference on Human-Computer Interaction},
	Date-Added = {2009-02-19 16:46:40 +0000},
	Date-Modified = {2009-03-05 13:00:05 +0000},
	Pages = {882-891},
	Publisher = {Springer},
	Read = {Yes},
	Title = {Browsing and Sorting Digital Pictures using Automatic Image Classification and Quality Analysis},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvSGlsbGlnZXMyMDA3dmgucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJIaWxsaWdlczIwMDd2aC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADpuKxcM9RAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxcM9RAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkhpbGxpZ2VzMjAwN3ZoLnBkZgAOACYAEgBIAGkAbABsAGkAZwBlAHMAMgAwADAANwB2AGgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvSGlsbGlnZXMyMDA3dmgucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Hilliges:2008ak,
	Annote = {This is a short paper, in which the authors explain the need to support users with increasingly flexible interfaces. One of the core reasons for this being that during a user study of PhotoHelix in Hillges:2007bl, users stumbling upon images in the collection often caused a massive change in user ambition. The paper outlines some key design principles, and stress the importance of casual browsing as appossed to direct searching. },
	Author = {Hilliges, O.},
	Booktitle = {Workshop Collocated Social Practices Surrounding Photos in conjunction with the ACM CHI '08},
	Date-Added = {2009-02-19 16:42:16 +0000},
	Date-Modified = {2009-03-05 12:37:52 +0000},
	Read = {Yes},
	Title = {Finding the Unknown - Serendipitous Discovery in Co-Located Consumption of Digital Photo Collections},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvSGlsbGlnZXMyMDA4YWsucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJIaWxsaWdlczIwMDhhay5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADptTxcM8NQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxcM8NQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkhpbGxpZ2VzMjAwOGFrLnBkZgAOACYAEgBIAGkAbABsAGkAZwBlAHMAMgAwADAAOABhAGsALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvSGlsbGlnZXMyMDA4YWsucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Walter:2003hi,
	Author = {J. Walter and J. Ontrup and D. Wessling and H. Ritter},
	Booktitle = {IEEE International Conference on Data Mining},
	Date-Added = {2009-02-19 16:31:07 +0000},
	Date-Modified = {2009-02-19 23:43:41 +0000},
	Pages = {355-362},
	Title = {Interactive visualization and navigation in large data collections using the hyperbolic space},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvV2FsdGVyMjAwM2hpLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQV2FsdGVyMjAwM2hpLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6ansXDONcAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXDONcAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpXYWx0ZXIyMDAzaGkucGRmAA4AIgAQAFcAYQBsAHQAZQByADIAMAAwADMAaABpAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1dhbHRlcjIwMDNoaS5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Bederson:2004xu,
	Author = {Bederson, B. and Shneiderman, B. and Wattenberg, M.},
	Date-Added = {2009-02-19 16:22:44 +0000},
	Date-Modified = {2009-03-12 13:48:46 +0000},
	Journal = {ACM Transactions on Graphics},
	Number = {4},
	Pages = {833-854},
	Title = {Ordered and quantum treemaps: Making effective use of 2D space to display hierarchies},
	Volume = {21},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvQmVkZXJzb24yMDA0eHUucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJCZWRlcnNvbjIwMDR4dS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADpoExcM3VAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxcM3VAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkJlZGVyc29uMjAwNHh1LnBkZgAOACYAEgBCAGUAZABlAHIAcwBvAG4AMgAwADAANAB4AHUALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQmVkZXJzb24yMDA0eHUucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Abdel-Mottaleb:1998ez,
	Annote = {This paper compares agglomerative (hierarchical) and divisive (k-means) clustering for CBIR. The authors also apply 3 different similarity measures: L1, L2 and histogram intersection. The paper explains that clustered approaches to CBIR means only a fraction of the images in the database nedd be compared, as opposed to an exhaustive linear search through all images in the database.

For the feature vector, images are divided into 16 rectangular regions, with a normailzed histogram for each region. 

As L1 and L2 are distance measures and HI is a similarity measure, the distances were converted to similarities by taking their negative values.

The paper outlines the two clustering algorithms.

For the experiments, 2000 images were used, with 133 clusters (15 images per cluster).

The paper defines their own perfomrance metric. The retrieval performance is over 90% accuracy in both clustering methods whilst comparing only the top 13 clusters (less than 300 comparisons).

In their tests, heirarchical clustering performed marginally better than k-means. Histogram intersection is the superior similarity measure, followed closely by L1 then L2 (significantly). The authors also note that HI and L1 also produced more uniform clusters.},
	Author = {M. Abdel-Mottaleb and S. Krischnamachari and N.J. Mankovich},
	Booktitle = {IEEE Computer Society Workshop on Empirical Evaluation of Computer Vision Algorithms },
	Date-Added = {2009-02-19 16:00:06 +0000},
	Date-Modified = {2009-03-12 12:28:50 +0000},
	Keywords = {Clustering},
	Read = {Yes},
	Title = {Performance Evaluation of Clustering Algorithms for Scalable Image Retrieval},
	Year = {1998},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEB9QYXBlcnMvQWJkZWwtTW90dGFsZWIxOTk4ZXoucGRm0hsPHB1XTlMuZGF0YU8RAeAAAAAAAeAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxhBYmRlbC1Nb3R0YWxlYjE5OThlei5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADpjkxcMyXAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxcMyXAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFZNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkFiZGVsLU1vdHRhbGViMTk5OGV6LnBkZgAOADIAGABBAGIAZABlAGwALQBNAG8AdAB0AGEAbABlAGIAMQA5ADkAOABlAHoALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAElVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQWJkZWwtTW90dGFsZWIxOTk4ZXoucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDMANEA2QK9Ar8CxALNAtgC3ALqAvEC+gL/AwIAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADDw==}}

@article{Jin:2001ix,
	Author = {Jin, J.S. and Kurniawati, R. and Xu, G.Y. and Bai, X},
	Date-Added = {2009-02-19 12:03:31 +0000},
	Date-Modified = {2009-03-12 13:48:05 +0000},
	Journal = {Journal of Visual Communication and Image Representation},
	Keywords = {Browsing},
	Number = {2},
	Pages = {123-135},
	Title = {Using Browsing to Improve Content-Based Image Retrieval},
	Volume = {12},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvSmluMjAwMWl4LnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNSmluMjAwMWl4LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6WxsXC/GgAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMXC/GgAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpKaW4yMDAxaXgucGRmAAAOABwADQBKAGkAbgAyADAAMAAxAGkAeAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9KaW4yMDAxaXgucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Howarth:2004gs,
	Annote = {This paper was read as it gives a summary of the meaning of the principle
texture feature extraction techniques available in CBIR. They propose the
use of a Tamura image, encoding textures in joint histograms of low
dimensional texture characteristics such as 3D colour histograms.

The 3 main approaches covered are statistical (Co-occurrence matrices),
psychological (Tamura's features) and signal processing (Gabor wavelets).
The TRECVID2003 image database was used for testing. The L1 distance is
used bewteen features.

The paper conludes the top 3 texture features performs better than
previously used colour features, although performance is improved when
using a comnination of colour and texture.},
	Author = {P. Howarth and S. R{\"u}ger},
	Booktitle = {In Proceedings of the International Conference on Image and Video Retrieval},
	Date-Added = {2009-02-17 23:47:54 +0000},
	Date-Modified = {2009-04-14 20:00:07 +0100},
	Keywords = {Texture Features},
	Pages = {326-334},
	Publisher = {Springer},
	Read = {Yes},
	Title = {Evaluation of texture features for content-based image retrieval},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvSG93YXJ0aDIwMDRncy5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUhvd2FydGgyMDA0Z3MucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOhyLFwPyaAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFwPyaAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6SG93YXJ0aDIwMDRncy5wZGYAAA4AJAARAEgAbwB3AGEAcgB0AGgAMgAwADAANABnAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvSG93YXJ0aDIwMDRncy5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Fazl-Ersi:2008vo,
	Author = {Fazl-Ersi, E. and MacKenzie, I. S. and Tsotsos, J. K.},
	Booktitle = {Proceedings of the 8th ACM/IEEE-CS Joint Conference on Digital Libraries},
	Date-Added = {2009-02-14 22:38:26 +0000},
	Date-Modified = {2009-02-14 22:43:25 +0000},
	Pages = {351-354},
	Title = {slab: smart labeling of family photos through an interactive interface},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvRmF6bC1FcnNpMjAwOHZvLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTRmF6bC1FcnNpMjAwOHZvLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4amsW8+DsAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW8+DsAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpGYXpsLUVyc2kyMDA4dm8ucGRmAAAOACgAEwBGAGEAegBsAC0ARQByAHMAaQAyADAAMAA4AHYAbwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9GYXpsLUVyc2kyMDA4dm8ucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@inproceedings{Schneidewind:2004xb,
	Author = {A. Schneidewind and P. Neumann and I. Schmitt },
	Booktitle = {Proceedings of the Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)},
	Date-Added = {2009-02-14 22:13:02 +0000},
	Date-Modified = {2009-02-14 22:15:24 +0000},
	Title = {An Approach to Visualize Image Retrieval Results},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEB1QYXBlcnMvU2NobmVpZGV3aW5kMjAwNHhiLnBkZtIbDxwdV05TLmRhdGFPEQHYAAAAAAHYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MWU2NobmVpZGV3aW5kMjAwNHhiLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4Zs8W88dgAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW88dgAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBUTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpTY2huZWlkZXdpbmQyMDA0eGIucGRmAA4ALgAWAFMAYwBoAG4AZQBpAGQAZQB3AGkAbgBkADIAMAAwADQAeABiAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBHVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1NjaG5laWRld2luZDIwMDR4Yi5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMoAzwDXArMCtQK6AsMCzgLSAuAC5wLwAvUC+AAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAMF}}

@article{Janecek:2005zj,
	Author = {P. Janecek and P. Pu },
	Date-Added = {2009-02-14 21:58:36 +0000},
	Date-Modified = {2009-02-14 22:00:36 +0000},
	Journal = {International Journal on Digital Libraries},
	Number = {1},
	Pages = {42-56},
	Title = {An evaluation of semantic fisheye views for opportunistic search in an annotated image collection},
	Volume = {5},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvSmFuZWNlazIwMDV6ai5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUphbmVjZWsyMDA1emoucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOGTzFvO7lAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFvO7lAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6SmFuZWNlazIwMDV6ai5wZGYAAA4AJAARAEoAYQBuAGUAYwBlAGsAMgAwADAANQB6AGoALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvSmFuZWNlazIwMDV6ai5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@article{Urban:2006gt,
	Annote = {The novelty of this system is that it learns from groupings made by the user. The system explores an alternative to RF, whereby generally systems ask the user to either select (ir)relevant images, a use a slider to specify some degree of relevance. This system is similar to that of ImageGrouper, however EGO uses the saved groupings to obtain some contextual meaning of the group and adapt the long term running of the system.

The authors claim that the system and user work interactively, both grouping images. This overall interaction allows the system to cater for both the long term and short term needs of the user.

The interface has 3 panels: query, search and workspace. The query panel can be used to enter a QBE image, with resulting images being shown in the results panel. The user can drag images from the search pane to the workspace, where images can be manually grouped. The system then recommends similar images that maybe added to the group.

An image can belong to multiple groups. Users can zoom and pan around the workspace, with an overview included to aid usr navigation. There is no indication that images are arranged in any way on the workspace. When images are added to a group (whether recommended by the system or selected by the user), the system updates its learning parameters.

The paper remarks "because the user ultimately decides on group memberships, the group reflects the current semantics in context of usage of the image collection. 

The euclidean distance is used to measure similarity between features. The feature vector is made up of average RGB, colour moments, co-occurence, autcorrelation, edge frequency and invariant moments.

For the recommendation system, each grouped image is treated as a positive training example. The RF is used to learn the feature weights.

For querying, a mulitipoint query representation is implemented for each group. Each group is clustered. A query image is submitted to a group, and the clusters from each group rank their images based on the query. The most similar images are returned to the user. The authors state that this algorithm is very efficient and database friendly.

For testing, the Corel database is used, and the concepts are used as the ground truth. User simulated testing is applied to see how well the system groups the images. The authors state that each image category can be split into homogeneous and heterogeneous catagories. An example of homo is roses, where all images are similar. An example of hetero is animals, as they can be visually very different. They state that for homo, single point querying is adequate, whereas multi-point seems to suit hetero catagories more.

24 users compared EGO to a simple yes/no RF system. For the yes/no, the top 100 images were on the workspace, with EGO supplying the top 10 images on the workspace. Two main user tasks were supplied, one being a target search, the other an open ended design task.

The users rated the yes/no system as significantly easier to learn and use, but EGO more stimulating, flexible and better for browsing the database. The number of images found in the target search was more in the yes/no system, as it was a time limited task and image selection was faster than dragging. 


},
	Author = {J. Urban and J.M. Jose},
	Date-Added = {2009-02-14 21:49:28 +0000},
	Date-Modified = {2009-04-14 16:13:52 +0100},
	Journal = {International Journal of Intelligent Systems},
	Keywords = {Image Grouping},
	Number = {7},
	Pages = {725-745},
	Read = {Yes},
	Title = {{EGO}: A personalized multimedia management and retrieval tool},
	Volume = {21},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvVXJiYW4yMDA2Z3QucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9VcmJhbjIwMDZndC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADhjgxbzskwAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbzskwAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlVyYmFuMjAwNmd0LnBkZgAADgAgAA8AVQByAGIAYQBuADIAMAAwADYAZwB0AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1VyYmFuMjAwNmd0LnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Yang:2007hv,
	Annote = {This paper discusses Value and Relation (VaR) and this paper is focussed more genrally on high dimensional data, but is applied to image in Yang2006. 

From a dataset of 10,417 images, 89 features are extracted. For each feature, or dimension, a glyph is created. This is a square with each datapoint (value for that feature from each image) is plotted on a spiral. A datapoint is represented by a single pixel. Later in the paper the authors experiment with scatter graph plots of data poitns. For all glyphs (in this case 89), a table of pairwise distances is created. This is used for MDS, and fufills the main objective of the VaR which is to reveal similarity between dimensions in a high dimensional space.

The authors experiment with semi-transprent glyph representations in order to combat overlapping by MDS. 

Another feature experimented with in this paper is a rainfall animation. Based on the similarity distance between glyphs, an glyph selected by a user is placed at the bottom of the screen, with all other images located at the top. When the use commences the animation, more similar glyphs appear to fall fatser to the bottom of the display (like rain) than less similar glyphs. The user can alter the speed of the animation.

The remainder of the paper discusses angles and representation of the glyph labels, aswell as user studies.},
	Author = {Yang, J. and Hubball, D. and Ward, M.O. and Rundensteiner, E.A. and Ribarsky, W. },
	Date-Added = {2009-02-14 21:37:54 +0000},
	Date-Modified = {2009-02-20 18:01:10 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Number = {3},
	Pages = {494-507},
	Read = {Yes},
	Title = {Value and Relation Display: Interactive Visual Exploration of Large Data Sets with Hundreds of Dimensions},
	Volume = {13},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvWWFuZzIwMDdodi5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDllhbmcyMDA3aHYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOGCjFvOn3AAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFvOn3AAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6WWFuZzIwMDdodi5wZGYADgAeAA4AWQBhAG4AZwAyADAAMAA3AGgAdgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9ZYW5nMjAwN2h2LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@article{Bruns:2009zs,
	Author = {E. Bruns and O. Bimber},
	Date-Added = {2009-02-14 21:06:29 +0000},
	Date-Modified = {2009-02-14 21:08:00 +0000},
	Journal = {Personal and Ubiquitous Computing},
	Keywords = {Mobile CBIR},
	Number = {2},
	Pages = {165-178},
	Title = {Adaptive training of video sets for image recognition on mobile phones },
	Volume = {13},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvQnJ1bnMyMDA5enMucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9CcnVuczIwMDl6cy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADhdxxbzifQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbzifQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkJydW5zMjAwOXpzLnBkZgAADgAgAA8AQgByAHUAbgBzADIAMAAwADkAegBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0JydW5zMjAwOXpzLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Cox:2000eh,
	Author = {I. Cox and M. Miller and T. Minka and T. Papathomas and P. Yianilos},
	Cited-By = {Smeulders:2000zl, Barnard2001},
	Cites = {Cox:1996qf, Pentland:1996oz, Huang:1997zl, Pass:1996fv, },
	Date-Added = {2009-02-14 20:40:56 +0000},
	Date-Modified = {2009-02-16 19:50:48 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {User Study, CBIR System},
	Pages = {20-37},
	Title = {The bayesian image retrieval system, PicHunter: Theory, implementation, and psychophysical experiments},
	Volume = {9},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvQ294MjAwMGVoLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNQ294MjAwMGVoLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4VzsW83LEAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW83LEAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpDb3gyMDAwZWgucGRmAAAOABwADQBDAG8AeAAyADAAMAAwAGUAaAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9Db3gyMDAwZWgucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Gomi:2008ev,
	Annote = {This work presents CAT (Clustered Album Thumbnail).  After conducting a study into the features of a system users may require, they devised 4 policies:

1) Users are used to using a heirarchical file system structure, therefore should try maintain the use of a hierarchcy.

2) Group by metadata first rather than image similarity (keywords, time stamp).

3) Many images displayed in well aligned manner (no overlap)

4) Representative images for clusters.

The system uses annotated images, and allows user to zoom in/out on clusters. The authors comment that the system is "quite analogous to PhotoMesa". This is because it divides the visible region into rectangular subregions, and prevents image overlap.

As with PhotoMesa, lower-level images are clickable and evenly sized thumbnails. The authors prepared a questionaire with 2 questions:

1) How do you look for an image?
2) How would you catagorize your images?

For the first question, a variety of answers were given (12 students). The majority (83%) said they browsed the OS GUI file system. For the second question, there was a 50/50 divide between timestamp and keywords. Similarity was suggested as a secondary or third sorting mechanism. Rodden et al. is not cited in this section.

CAT initially clusters images by keyword, then creates inter-clusters based on colour and texture features. The paper does not define similarity between keywords, i.e. if an image is labelled "plant" and another "flower", would these be grouped together?

For the content features, an image is divided into regions in a grid, with the average colour of each sub-section and wavelets uset for texture. All image pairs have cosine value calculated, and agglomerative clustering is performed merging images that are close to one another. CAT selects a representative image for each cluster, closest to the centroid of the cluster.

When a user selects a keyword, CAT constructs a subset tree structure consisting of all images with the chosen keywords.

A three phase packing algorithm works as follows:

Phase 1: Cluster thumbnails are arranged as a grid and encloses them in a border. 
Phase 2: Lower level clusters are bordered together to for the higher level cluster.
Phase 3: CAT packs all clusters into a singular cluster.

The ratio size of the rectangles is maintained to fit a regular grid of images, and PCA and MDS are used as a template for where images should be placed in the grid in their respective rectangles. Users can zoom into clusters.

For testing usability, 10 people were asked to test 5 variations of the system, based on hierarchy/no heirarchy and no/representative images. The studies found that representative images were important, and that keyword based clustering is very effective.


},
	Author = {A. Gomi and R. Miyazaki and T. Itoh and J. Li},
	Booktitle = {12th International Conference on Information Visualization},
	Date-Added = {2009-02-14 19:19:07 +0000},
	Date-Modified = {2009-02-20 15:22:52 +0000},
	Keywords = {Hierarchical Browsing},
	Pages = {82-87},
	Read = {Yes},
	Title = {{CAT}: A Hierarchical Image Browser Using a Rectangle Packing Technique},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvR29taTIwMDhldi5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkdvbWkyMDA4ZXYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOE1bFvMlxAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFvMlxAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6R29taTIwMDhldi5wZGYADgAeAA4ARwBvAG0AaQAyADAAMAA4AGUAdgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9Hb21pMjAwOGV2LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@article{Janjusevic:2008hf,
	Abstract = {In this paper we address two relevant tasks in image visualisation research: layout methods for presenting content and relations within image databases; and optimal solutions for efficient use of the entire display space. We introduce a novel approach to enable users searching on large image archives to distinguish heterogeneous sets of images. Thus, helping them to navigate or browse image databases according to relevant query directions. Two methods for mapping similarity relations between images and cognitive partitioning of the display space are presented.},
	Author = {Janjusevic, T.  and Izquierdo, E. },
	Citeulike-Article-Id = {3473032},
	Date-Added = {2009-02-14 19:13:32 +0000},
	Date-Modified = {2009-02-14 19:13:32 +0000},
	Doi = {http://dx.doi.org/10.1109/IV.2008.55},
	Journal = {12th International Conference on Information Visualisation },
	Keywords = {Visualization},
	Pages = {88-93},
	Posted-At = {2008-11-02 05:43:51},
	Priority = {2},
	Title = {Layout Methods for Intuitive Partitioning of Visualization Space},
	Url = {http://dx.doi.org/10.1109/IV.2008.55},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBtQYXBlcnMvSmFuanVzZXZpYzIwMDhoZi5wZGbSGw8cHVdOUy5kYXRhTxEB0AAAAAAB0AACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTFEphbmp1c2V2aWMyMDA4aGYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPCiHF1UJnAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADF1UJnAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAUk1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6SmFuanVzZXZpYzIwMDhoZi5wZGYADgAqABQASgBhAG4AagB1AHMAZQB2AGkAYwAyADAAMAA4AGgAZgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARVVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9KYW5qdXNldmljMjAwOGhmLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAyADNANUCqQKrArACuQLEAsgC1gLdAuYC6wLuAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAvs=}}

@inproceedings{Janecek:2003wp,
	Author = {P. Janecek and P. Pu },
	Booktitle = {On The Move to Meaningful Internet Systems 2003: OTM 2003 Workshops},
	Date-Added = {2009-02-14 18:56:51 +0000},
	Date-Modified = {2009-02-14 21:55:35 +0000},
	Keywords = {Semantic System, Visualisation},
	Publisher = {Springer},
	Title = {Searching with semantics: An interactive visualization technique for exploring an annotated image collection},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvSmFuZWNlazIwMDN3cC5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUphbmVjZWsyMDAzd3AucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOEnzFvMOnAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFvMOnAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6SmFuZWNlazIwMDN3cC5wZGYAAA4AJAARAEoAYQBuAGUAYwBlAGsAMgAwADAAMwB3AHAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvSmFuZWNlazIwMDN3cC5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Naud:2007gu,
	Abstract = {Queries in content-based image retrieval are usually ill-defined since the variability of query attributes is not explicitly specified and depends on user tolerance. In this paper, we aim at determining the query variability from user interaction and integrating it into the search process by adapting the calculated similarity to a perceived one. We examine in-depth the query understanding in the case of object retrieval. Our method is tested on the system THIS [14], results are promising.},
	Author = {Naud, E. and Idrissi, K. and Tellez, B. },
	Booktitle = {International Workshop on Content-Based Multimedia Indexing (CBMI) },
	Citeulike-Article-Id = {2669442},
	Date-Added = {2009-02-14 18:51:54 +0000},
	Date-Modified = {2009-02-14 18:51:54 +0000},
	Doi = {http://dx.doi.org/10.1109/CBMI.2007.385429},
	Journal = {Content-Based Multimedia Indexing, 2007. CBMI '07. International Workshop on},
	Keywords = {CBIR, Query},
	Pages = {323--327},
	Posted-At = {2008-04-14 15:54:20},
	Priority = {2},
	Title = {Query Understanding in Content-Based Image Retrieval Context},
	Url = {http://dx.doi.org/10.1109/CBMI.2007.385429},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CBMI.2007.385429}}

@inproceedings{Porta:2006kx,
	Author = {Porta, M. },
	Booktitle = {Proceedings of the 8th International Working Conference on Advanced Visual Interfaces (AVI 2006)},
	Citeulike-Article-Id = {1133576},
	Comment = {They have developed various methods for visualizing and exploring large collection of images (cube, snow, snake, volcano and funnel).  New methods ------------ In this article they propose new methods for exploring: elastic image browsing, shot display, spot display, cylinder display, rotor display, tornado display and tornado of planes display. The prototype was implemented in Macromedia Flash.  Experimentation ----------------- They did informal experiments that consisted of testers (people) searching for images in a collection of 400 images belonging to one of six themes, each theme with 30 images. The idea is to find as much pictures of a certain theme as possible with a time limit of 3 minutes. In the experimentation, they adjust some parameters for each method. They compare their methods with conventional grid method.   Measures ---------- Search efficiency: ratio between percentage of correct images selected and the browsing duration. They show two graphics: search efficiency per method and search time per method.},
	Date-Added = {2009-02-14 18:45:57 +0000},
	Date-Modified = {2009-02-14 20:52:27 +0000},
	Doi = {http://dx.doi.org/10.1145/1133265.1133354},
	Isbn = {1595933530},
	Keywords = {Visualisation, Browsing},
	Pages = {440--444},
	Posted-At = {2008-11-14 01:02:41},
	Priority = {2},
	Publisher = {ACM},
	Title = {Browsing large collections of images through unconventional visualization techniques},
	Url = {http://dx.doi.org/10.1145/1133265.1133354},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/1133265.1133354}}

@inproceedings{Malave:2004kv,
	Author = {Malav\'e, L. and V\'elez, B.  },
	Booktitle = {Communications, Internet, and Information Technology},
	Citeulike-Article-Id = {3280572},
	Date-Added = {2009-02-14 18:41:43 +0000},
	Date-Modified = {2009-02-14 18:41:43 +0000},
	Keywords = {Visualisation},
	Pages = {280--285},
	Posted-At = {2008-09-17 15:57:40},
	Priority = {2},
	Publisher = {IASTED/ACTA Press},
	Title = {Terrascope image clustering: Applying clustering techniques to image agglomeration in image retrieval systems},
	Url = {http://dblp.uni-trier.de/rec/bibtex/conf/ciit/MalaveV04},
	Year = {2004},
	Bdsk-Url-1 = {http://dblp.uni-trier.de/rec/bibtex/conf/ciit/MalaveV04}}

@inproceedings{Mukhopadhyay:2004qw,
	Abstract = {In this paper, we present a computer-assisted image browsing system based on Pathfinder Networks. Similarity of images to one another is determined through a proposed method of automatic shape feature discovery. Local features are generated by clustering small (on the order of 10 by 10 pixels) binary image blocks culled from the edge analysis of images in the database and using the cluster means as the local feature detectors. The clustering method for the binary image blocks is based on the Hausdorff metric of distance between sets of points. Relationships between local features then determine the similarity between images. Pathfinder Networks are then used to visually represent similarity between images. The results are presented on a database containing three categories of images.},
	Author = {Mukhopadhyay, R. and Ma, A. and Sethi, I. },
	Booktitle = {Proceedings of the IEEE Sixth International Symposium on Multimedia Software Engineering},
	Citeulike-Article-Id = {3432017},
	Date-Added = {2009-02-14 18:35:48 +0000},
	Date-Modified = {2009-02-14 18:36:48 +0000},
	Isbn = {0-7695-2217-3},
	Keywords = {Pathfinder Network, Visualisation},
	Pages = {522--528},
	Posted-At = {2008-10-21 02:43:46},
	Priority = {2},
	Title = {Pathfinder Networks for Content Based Image Retrieval Based on Automated Shape Feature Discovery},
	Url = {http://portal.acm.org/citation.cfm?id=1038911},
	Year = {2004},
	Bdsk-Url-1 = {http://portal.acm.org/citation.cfm?id=1038911}}

@phdthesis{Nguyen:2006si,
	Annote = {G.P. Nguyen, Interactive Image Search using Similarity-Based Visualization, PhD thesis, 134 pages, December 2006},
	Author = {G. P. Nguyen},
	Date-Added = {2009-02-14 15:06:54 +0000},
	Date-Modified = {2009-02-14 18:02:21 +0000},
	Keywords = {Visualisation},
	School = {University of Amsterdam},
	Title = {Interactive Image Search using Similarity-Based Visualization},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvTmd1eWVuMjAwNnNpLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQTmd1eWVuMjAwNnNpLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4KUMW8jiwAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW8jiwAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpOZ3V5ZW4yMDA2c2kucGRmAA4AIgAQAE4AZwB1AHkAZQBuADIAMAAwADYAcwBpAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL05ndXllbjIwMDZzaS5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Samadani:2007mi,
	Annote = {@inproceedings{DBLP:conf/icip/SamadaniLT07,
  author    = {Ramin Samadani and
               Suk Hwan Lim and
               Daniel Tretter},
  title     = {Representative Image Thumbnails for Good Browsing},
  booktitle = {ICIP (2)},
  year      = {2007},
  pages     = {193-196},
  ee        = {http://dx.doi.org/10.1109/ICIP.2007.4379125},
  crossref  = {DBLP:conf/icip/2007},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/icip/2007,
  title     = {Proceedings of the International Conference on Image Processing,
               ICIP 2007, September 16-19, 2007, San Antonio, Texas, USA},
  booktitle = {ICIP},
  publisher = {IEEE},
  year      = {2007},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
},
	Author = {R. Samadani and S. Lim and D. Tretter},
	Booktitle = {Proceedings of the International Conference on Image Processing (ICIP)},
	Date-Added = {2009-02-14 14:53:00 +0000},
	Date-Modified = {2009-02-14 19:31:17 +0000},
	Keywords = {Browsing},
	Pages = {16-19},
	Title = {Representative Image Thumbnails for Good Browsing},
	Year = {2007}}

@inproceedings{Bederson:2001fv,
	Annote = {This paper discusses Quantum Treemaps and Bubblemaps, as implemented in the PhotoMesa system. The motivation for this work was undertaken by the author, as he wanted to share his digital photo collection with his young daughter.

Quantum treemaps are designed to display images or other objects of indivisable size, whereas Bubblemaps fills the space with indivisable items but generate non-rectangula groups.

Images with a shared attribute (i.e. directory, time taken, keyword) are grouped together. PhotoMesa allows users to explore multiple directories of images in a zoomable browser. Clustering is performed solely on metadata, and requires no annotation from users. When a user moves the mouse cursor over a directory, the directory name is shown in full. Clicking the directory enlarges the display of that directory. Right-clicking zooms out of the directory. If the mouse cursor remains static over a thumbnail, a magnified version of the thumbnail is displayed. A search pane is included that allows the user to enter a keyword that will be comared with filename metadata.

If the user selects to search all images by year, all images from that year are grouped together. Searching by time and filename metadata avoids any user annotation. However a potential flaw in this system is that the author assumes users give images meaningful filenames, which has been show not to be the case (Rodden).

When an image is loaded into the database, multiple sized thumbnails of the same image are stored in a filesystem and dynamically loaded based on the size of the rectangular sections.

Treemaps are recursive, so a rectangle inside a treemap can contain a treemap. Rectangles with an aspect ratio close to 1 are visually more attractive. Ratio = max (height/width, width/height).

The paper discusses the previous Treemap implemenations, such as Sneidermanns "Slice and Dice" method, where first the space is divided into skinny vertical regions, and if applied recursively, then split horizontally. 

For the Quantum treemap algorithm, the input is a list of numbers specifying the size of the rectangles, and the display space. The output is the layout of the rectangles. The Quantum Treemap genreates rectangles with integer multiples of a given element size. All the grids of elements align perfectly. When images are assigned to their groups, an evening algorithm is aligned to re-arrange the images in the boxes. The authors note that an issue arising from this is there can be alot of wasted space on the screen, particulary when the number of images in a group is small.

The authors show that this technique can result in irregular layouts. In the paper, the layout created means the user must scan in 3 directions to see 4 boxes. They suggest 2 alternative arrangements of quad, produces 2x2 rectangles. The second, snake, lays the 4 rectangles sequentially. Since none of they arrangements is perfect each time, PhotoMesa calculates each arrangement (when 5 or less groups) and chooses the best.
PhotoMesa also uses 3 different pivot selection strategies (box spacing), calculating them all and selecting the best.  

The authors note that the QT works best when there are more elements per a group, as the algorithm has more flexibility in its layout.

An experiment between ordered treemaps OT and QT showed that QT generate the better aspect ratios, whereas OT have less wasted space. However the QT displays equally sized images on a global grid.

In an attempt to completely remove unused space, the paper introduces the idea of using bubblemaps. In a bubblemap, images are still on a local grid, put the areas can be of arbituary shapes - not just rectangular. They assume the input is pre-clustered and attempts to keep clustered items together. This algoithm works in O(n) time.

The paper comments that bubblemaps are easy to implement and use space efficeintly, but the irregularity of the shapes makes them "visually difficult to parse".

The paper submits no user study. },
	Author = {B. Bederson},
	Booktitle = {14th Annual ACM Symposium on User Interface Software and Technology},
	Cited-By = {Graham:2002zr},
	Cites = {Combs:1999ys, Kang:2000ly, Platt:2000kx, Rodden:2001cr, },
	Date-Added = {2009-02-14 14:43:24 +0000},
	Date-Modified = {2009-02-22 15:00:21 +0000},
	Keywords = {Browsing},
	Pages = {71-80},
	Read = {Yes},
	Title = {Quantum treemaps and bubblemaps for a zoomable image browser},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvQmVkZXJzb24yMDAxZnYucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJCZWRlcnNvbjIwMDFmdi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgkQxbyIcAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbyIcAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkJlZGVyc29uMjAwMWZ2LnBkZgAOACYAEgBCAGUAZABlAHIAcwBvAG4AMgAwADAAMQBmAHYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQmVkZXJzb24yMDAxZnYucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Khella:2004zl,
	Annote = {This work explores implementing PhotoMesa on a mobile device, with a limited screen resolution and processing power. The system is implemented with a Pocket PC SDK. The paper discusses Treemaps and PhotoMesa in general, much like in Bederson:2001fv.

For the interface, all images are displayed on the screen using Quantum Strip Treemaps. The user can zoom into groups and individual images. Individual images can be seen at full resolution, with the user panning around through scroll bars. Users can zoom out by tapping white space, and can zoom in by dragging a rectangle over a part of the screen.

A problem encountered was the smooth animation required for zooming into a group or image (due to reduced computational power). A solution was to precompute zoom locations in a tree structure.

The authors compare the new system with a previous ACDSee interface, whic shows the file directory at the top of the screen, and the images in a grid format in the lower protion of the screen.

The study saw 15 CS students use the ACDSee interface, along with Pocket PhotoMesa with the animated zoom turned both on and off. Each user was given 5 images to find. These images were of varying difficulty, with some being 'visually ambiguous', not distinct from all the other images in the database. PhotoMesa with animation proved to be the quickest interface, aswell as the easiest to use and most enjoyable to use. User questioning suggested that animation slowed the user down, but helped them navigate.

A problem found using the ACDSee was that images occasionally were not grouped in a directory as the user had anticipated e.g. does an image of kangaroo fall in the folder animals of Australia?! The semantic gap is not mentioned in the text.



 },
	Author = {Khella, A. and Bederson, B},
	Booktitle = {Proceedings of Mobile and Ubiquitous Multimedia},
	Date-Added = {2009-02-14 14:37:50 +0000},
	Date-Modified = {2009-04-14 16:09:44 +0100},
	Keywords = {Zooming, Browsing, Mobile CBIR},
	Pages = {19-24},
	Publisher = {ACM},
	Read = {Yes},
	Title = {Pocket Photo{M}esa: A Zooming Image Browser for {PDA}'s},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvS2hlbGxhMjAwNHpsLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQS2hlbGxhMjAwNHpsLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA14u8WzTGsAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWzTGsAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpLaGVsbGEyMDA0emwucGRmAA4AIgAQAEsAaABlAGwAbABhADIAMAAwADQAegBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0toZWxsYTIwMDR6bC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Kang:2000rz,
	Annote = {Kang, H. and Shneiderman, B. Visualization methods for personal photo collections: Browsing and searching in the PhotoFinder, Proceedings of the IEEE Conference on Multimedia and Expo (July 2000).},
	Author = {Kang, H. and Shneiderman, B.},
	Booktitle = {Proceedings of the IEEE Conference on Multimedia and Expo},
	Date-Added = {2009-02-14 14:34:46 +0000},
	Date-Modified = {2009-02-14 16:45:12 +0000},
	Keywords = {Browsing, CBIR System},
	Title = {Visualization methods for personal photo collections: Browsing and searching in the PhotoFinder},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvS2FuZzIwMDByei5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkthbmcyMDAwcnoucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANeR3Fs1CEAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFs1CEAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6S2FuZzIwMDByei5wZGYADgAeAA4ASwBhAG4AZwAyADAAMAAwAHIAegAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9LYW5nMjAwMHJ6LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Shneiderman:2002vn,
	Author = {Shneiderman, B. and Kang, H. and Kules, B. and Plaisant, C. and Rose, A. and Rucheir, R.},
	Booktitle = {Interactions},
	Date-Added = {2009-02-14 14:30:05 +0000},
	Date-Modified = {2009-02-14 18:11:23 +0000},
	Keywords = {PhotoFinder, CBIR System},
	Number = {3},
	Pages = {17-23},
	Title = {A photo history of SIGCHI: evolution of design from personal to public},
	Volume = {9},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBxQYXBlcnMvU2huZWlkZXJtYW4yMDAydm4ucGRm0hsPHB1XTlMuZGF0YU8RAdQAAAAAAdQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxVTaG5laWRlcm1hbjIwMDJ2bi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADXkKxbNQKAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbNQKAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFNNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlNobmVpZGVybWFuMjAwMnZuLnBkZgAADgAsABUAUwBoAG4AZQBpAGQAZQByAG0AYQBuADIAMAAwADIAdgBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBGVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1NobmVpZGVybWFuMjAwMnZuLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDJAM4A1gKuArACtQK+AskCzQLbAuIC6wLwAvMAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADAA==}}

@inproceedings{Kustanowitz:2005kx,
	Author = {Kustanowitz, J. and Shneiderman, B.},
	Booktitle = {Proceedings of the 5th ACM/IEEE-CS joint conference on Digital libraries},
	Date-Added = {2009-02-14 14:18:40 +0000},
	Date-Modified = {2009-02-14 18:00:42 +0000},
	Keywords = {Hierarchical Browsing},
	Pages = {188-196},
	Title = {Meaningful presentations of photo libraries: rationale and applications of bi-level radial quantum layouts},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBxQYXBlcnMvS3VzdGFub3dpdHoyMDA1a3gucGRm0hsPHB1XTlMuZGF0YU8RAdQAAAAAAdQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxVLdXN0YW5vd2l0ejIwMDVreC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADXjexbNOtgAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbNOtgAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFNNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkt1c3Rhbm93aXR6MjAwNWt4LnBkZgAADgAsABUASwB1AHMAdABhAG4AbwB3AGkAdAB6ADIAMAAwADUAawB4AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBGVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0t1c3Rhbm93aXR6MjAwNWt4LnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDJAM4A1gKuArACtQK+AskCzQLbAuIC6wLwAvMAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADAA==}}

@article{Shneiderman:2006uq,
	Annote = {@article{1121985,
 author = {Shneiderman,, Ben and Bederson,, Benjamin B. and Drucker,, Steven M.},
 title = {Find that photo!: interface strategies to annotate, browse, and share},
 journal = {Commun. ACM},
 volume = {49},
 number = {4},
 year = {2006},
 issn = {0001-0782},
 pages = {69--71},
 doi = {http://doi.acm.org/10.1145/1121949.1121985},
 publisher = {ACM},
 address = {New York, NY, USA},
 }
},
	Author = {Shneiderman, B. and Bederson, B. and Drucker, M.},
	Date-Added = {2009-02-14 14:13:36 +0000},
	Date-Modified = {2009-02-14 18:03:29 +0000},
	Journal = {Communications of the ACM},
	Keywords = {PhotoFinder, CBIR System},
	Number = {4},
	Pages = {69-71},
	Title = {Find that photo!: interface strategies to annotate, browse, and share},
	Volume = {49},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBxQYXBlcnMvU2huZWlkZXJtYW4yMDA2dXEucGRm0hsPHB1XTlMuZGF0YU8RAdQAAAAAAdQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxVTaG5laWRlcm1hbjIwMDZ1cS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADXjPxbNNVQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbNNVQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFNNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlNobmVpZGVybWFuMjAwNnVxLnBkZgAADgAsABUAUwBoAG4AZQBpAGQAZQByAG0AYQBuADIAMAAwADYAdQBxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBGVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1NobmVpZGVybWFuMjAwNnVxLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDJAM4A1gKuArACtQK+AskCzQLbAuIC6wLwAvMAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADAA==}}

@article{Li:2008tk,
	Annote = {ALIPR: Jia Li and James Z. Wang, ``Real-time Computerized Annotation of Pictures,'' IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 6, pp. 985-1002, 2008. Patent Pending},
	Author = {J. Li and J.Z. Wang},
	Date-Added = {2009-02-13 11:36:00 +0000},
	Date-Modified = {2009-02-13 11:37:44 +0000},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {ALIPR, CBIR System, Semantic System},
	Number = {6},
	Pages = {985-1002},
	Title = {Real-time Computerized Annotation of Pictures},
	Volume = {30},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvTGkyMDA4dGsucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxMaTIwMDh0ay5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADf/Kxbr3tQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbr3tQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkxpMjAwOHRrLnBkZgAOABoADABMAGkAMgAwADAAOAB0AGsALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTGkyMDA4dGsucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@article{Huang:1999wf,
	Annote = { , Volume 35, Number 3, December 1999 , pp. 245-268(24)},
	Author = {J. Huang and S. Ravi Kumar and M. Mitra and W. Zhu},
	Cites = {Cox:1996qf, Huang:1997zl, Pass:1996fv, Pentland:1996oz, Ballard:1991yq},
	Date-Added = {2009-02-13 11:29:25 +0000},
	Date-Modified = {2009-02-15 11:41:12 +0000},
	Journal = {International Journal of Computer Vision},
	Keywords = {Correlogram, Histogram, CCV},
	Number = {3},
	Pages = {245-268},
	Title = {Spatial Color Indexing and Applications},
	Volume = {35},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvSHVhbmcxOTk5d2YucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9IdWFuZzE5OTl3Zi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgDTxbr7TAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbr7TAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkh1YW5nMTk5OXdmLnBkZgAADgAgAA8ASAB1AGEAbgBnADEAOQA5ADkAdwBmAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0h1YW5nMTk5OXdmLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Rubner:2000jl,
	Annote = {Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover's distance as a metric for image retrieval. International Journal of Computer Vision, 40(2):99-121, November 2000},
	Author = {Y. Rubner and C. Tomasi and L. J. Guibas},
	Cites = {Orengo:1995rz, Ballard:1991yq},
	Date-Added = {2009-02-13 10:02:09 +0000},
	Date-Modified = {2009-02-15 13:59:56 +0000},
	Journal = {International Journal of Computer Vision},
	Keywords = {Distances, EMD, MDS},
	Number = {2},
	Pages = {99-121},
	Title = {The earth mover's distance as a metric for image retrieval},
	Volume = {40},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvcnVibmVySWpjdjAwLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQcnVibmVySWpjdjAwLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3/PcW69V4AAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW69V4AAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpydWJuZXJJamN2MDAucGRmAA4AIgAQAHIAdQBiAG4AZQByAEkAagBjAHYAMAAwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL3J1Ym5lcklqY3YwMC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@electronic{colourcube:2009jt,
	Annote = {Discretised Colour Cube},
	Author = {Unknown},
	Date-Added = {2009-02-12 20:13:48 +0000},
	Date-Modified = {2009-02-14 19:31:46 +0000},
	Keywords = {Figure},
	Lastchecked = {12/02/2009},
	Read = {Yes},
	Title = {Colour Cube},
	Url = {http://www.blinkworks.com/build/bpix/clrcube.jpg},
	Year = {2009},
	Bdsk-Url-1 = {http://www.blinkworks.com/build/bpix/clrcube.jpg}}

@article{Chen:2005er,
	Abstract = {In a typical content-based image retrieval (CBIR) system, target images (images in the database) are sorted by feature similarities with respect to the query. Similarities among target images are usually ignored. This paper introduces a new technique, cluster-based retrieval of images by unsupervised learning (CLUE), for improving user interaction with image retrieval systems by fully exploiting the similarity information. CLUE retrieves image clusters by applying a graph-theoretic clustering algorithm to a collection of images in the vicinity of the query. Clustering in CLUE is dynamic. In particular, clusters formed depend on which images are retrieved in response to the query. CLUE can be combined with any real-valued symmetric similarity measure (metric or nonmetric). Thus, it may be embedded in many current CBIR systems, including relevance feedback systems. The performance of an experimental image retrieval system using CLUE is evaluated on a database of around 60,000 images from COREL. Empirical results demonstrate improved performance compared with a CBIR system using the same image similarity measure. In addition, results on images returned by Google's Image Search reveal the potential of applying CLUE to real-world image data and integrating CLUE as a part of the interface for keyword-based image retrieval systems.},
	Annote = {@article{citeulike:1116214,
	abstract = {In a typical content-based image retrieval (CBIR) system, target images (images in the database) are sorted by feature similarities with respect to the query. Similarities among target images are usually ignored. This paper introduces a new technique, cluster-based retrieval of images by unsupervised learning (CLUE), for improving user interaction with image retrieval systems by fully exploiting the similarity information. CLUE retrieves image clusters by applying a graph-theoretic clustering algorithm to a collection of images in the vicinity of the query. Clustering in CLUE is dynamic. In particular, clusters formed depend on which images are retrieved in response to the query. CLUE can be combined with any real-valued symmetric similarity measure (metric or nonmetric). Thus, it may be embedded in many current CBIR systems, including relevance feedback systems. The performance of an experimental image retrieval system using CLUE is evaluated on a database of around 60,000 images from COREL. Empirical results demonstrate improved performance compared with a CBIR system using the same image similarity measure. In addition, results on images returned by Google's Image Search reveal the potential of applying CLUE to real-world image data and integrating CLUE as a part of the interface for keyword-based image retrieval systems.},
	author = {Chen, Yixin   and Wang, J. Z.  and Krovetz, R. },
	citeulike-article-id = {1116214},
	doi = {http://dx.doi.org/10.1109/TIP.2005.849770},
	journal = {Image Processing, IEEE Transactions on},
	keywords = {visualization},
	number = {8},
	pages = {1187--1201},
	posted-at = {2008-09-17 16:52:06},
	priority = {2},
	title = {CLUE: cluster-based retrieval of images by unsupervised learning},
	url = {http://dx.doi.org/10.1109/TIP.2005.849770},
	volume = {14},
	year = {2005}
}

	},
	Author = {Chen, Y. and Wang, J. Z.  and Krovetz, R. },
	Date-Added = {2009-02-11 14:46:12 +0000},
	Date-Modified = {2009-02-14 17:34:27 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {Clustering, CBIR System},
	Number = {8},
	Pages = {1187-1201},
	Title = {CLUE: cluster-based retrieval of images by unsupervised learning},
	Volume = {14},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvQ2hlbjIwMDVlci5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkNoZW4yMDA1ZXIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN7I/FuJT6AAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFuJT6AAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Q2hlbjIwMDVlci5wZGYADgAeAA4AQwBoAGUAbgAyADAAMAA1AGUAcgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9DaGVuMjAwNWVyLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Reddy:2007ya,
	Abstract = {Imagers are an increasingly significant source of sensory observations about human activity and the urban environment. ImageScape is a software tool for processing, clustering, and browsing large sets of images. Implemented as a set of web services with an Adobe Flash-based user interface, it supports clustering by both image features and context tags, as well as re-tagging of images in the user interface. Though expected to be useful in many applications, ImageScape was designed as an analysis component of DietSense, a software system under development at UCLA to support (1) the use of mobile devices for automatic multimedia documentation of dietary choices with just-in-time annotation, (2) efficient post facto review of captured media by participants and researchers, and (3) easy authoring and dissemination of the automatic data collection protocols. A pilot study, in which participants ran software that enabled their phones to autonomously capture images of their plates during mealtime, was conducted using an early prototype of the DietSense system, and the resulting image set used in the creation of ImageScape. ImageScape will support two kinds of users within the DietSense application: The participants in dietary studies will have the ability to easily audit their images, while the recipients of the images, health care professionals managing studies and performing analysis, will be able to rapidly browse and annotate large sets of images.},
	Annote = {@inproceedings{citeulike:3280660,
	abstract = {Imagers are an increasingly significant source of sensory observations about human activity and the urban environment. ImageScape is a software tool for processing, clustering, and browsing large sets of images. Implemented as a set of web services with an Adobe Flash-based user interface, it supports clustering by both image features and context tags, as well as re-tagging of images in the user interface. Though expected to be useful in many applications, ImageScape was designed as an analysis component of DietSense, a software system under development at UCLA to support (1) the use of mobile devices for automatic multimedia documentation of dietary choices with just-in-time annotation, (2) efficient post facto review of captured media by participants and researchers, and (3) easy authoring and dissemination of the automatic data collection protocols. A pilot study, in which participants ran software that enabled their phones to autonomously capture images of their plates during mealtime, was conducted using an early prototype of the DietSense system, and the resulting image set used in the creation of ImageScape. ImageScape will support two kinds of users within the DietSense application: The participants in dietary studies will have the ability to easily audit their images, while the recipients of the images, health care professionals managing studies and performing analysis, will be able to rapidly browse and annotate large sets of images.},
	address = {New York, NY, USA},
	author = {Reddy, Sasank   and Parker, Andrew   and Hyman, Josh   and Burke, Jeff   and Estrin, Deborah   and Hansen, Mark  },
	booktitle = {EmNets '07: Proceedings of the 4th workshop on Embedded networked sensors},
	citeulike-article-id = {3280660},
	doi = {http://dx.doi.org/10.1145/1278972.1278975},
	isbn = {978-1-59593-694-3},
	keywords = {visualization},
	location = {Cork, Ireland},
	pages = {13--17},
	posted-at = {2008-09-17 16:15:17},
	priority = {2},
	publisher = {ACM},
	title = {Image browsing, processing, and clustering for participatory sensing: lessons from a DietSense prototype},
	url = {http://dx.doi.org/10.1145/1278972.1278975},
	year = {2007}
}

	},
	Author = {Reddy, S. and Parker, A. and Hyman, J. and Burke, J. and Estrin, D. and Hansen, M.},
	Booktitle = {Proceedings of the 4th Workshop on Embedded Networked Sensors},
	Date-Added = {2009-02-11 14:42:05 +0000},
	Date-Modified = {2009-02-14 18:09:52 +0000},
	Keywords = {Clustering, Browsing},
	Pages = {13-17},
	Publisher = {ACM},
	Title = {Image browsing, processing, and clustering for participatory sensing: lessons from a DietSense prototype},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvUmVkZHkyMDA3eWEucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9SZWRkeTIwMDd5YS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADd+IxbdY9QAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbdY9QAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlJlZGR5MjAwN3lhLnBkZgAADgAgAA8AUgBlAGQAZAB5ADIAMAAwADcAeQBhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JlZGR5MjAwN3lhLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Brucher:2008wk,
	Abstract = {Manifold learning may be seen as a procedure aiming at capturing the degrees of freedom and structure characterizing a set of high-dimensional data, such as images or patterns. The usual goals are data understanding, visualization, classification, and the computation of means. In a linear framework, this problem is typically addressed by principal component analysis (PCA). We propose here a nonlinear extension to PCA. Firstly, the reduced variables are determined in the metric multidimensional scaling framework. Secondly, regression of the original variables with respect to the reduced variables is achieved considering a piecewise linear model. Both steps parameterize the (noisy) manifold holding the original data. Finally, we address the projection of data onto the manifold. The problem is cast in a Bayesian framework. Application of the proposed approach to standard data sets such as the COIL-20 database is presented.},
	Author = {Brucher, M. and Heinrich, Ch. and Armspach, J. P.},
	Date-Added = {2009-02-11 14:38:04 +0000},
	Date-Modified = {2009-02-14 17:48:23 +0000},
	Journal = {Journal on Advances in Signal Processing},
	Keywords = {MDS},
	Title = {A Metric Multidimensional Scaling-Based Nonlinear Manifold Learning Approach for Unsupervised Data Reduction},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvQnJ1Y2hlcjIwMDh3ay5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUJydWNoZXIyMDA4d2sucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN3w/Ft1cqAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFt1cqAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6QnJ1Y2hlcjIwMDh3ay5wZGYAAA4AJAARAEIAcgB1AGMAaABlAHIAMgAwADAAOAB3AGsALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQnJ1Y2hlcjIwMDh3ay5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Chen:2000gn,
	Author = {Chen, Chaomei   and Gagaudakis, George   and Rosin, Paul  L. },
	Booktitle = {The Proceedings of the: IEEE International Conference on Information Visualization},
	Cited-By = {Nakazato:2001rp},
	Cites = {Pentland:1996oz, Chen:2000ly, Ballard:1991yq},
	Date-Added = {2009-02-11 14:33:46 +0000},
	Date-Modified = {2009-03-12 13:48:39 +0000},
	Keywords = {Visualisation, Pathfinder Network},
	Pages = {13-18},
	Publisher = {IEEE},
	Title = {Content-Based Image Visualization},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvQ2hlbjIwMDBnbi5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkNoZW4yMDAwZ24ucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN31XFt1hpAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFt1hpAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Q2hlbjIwMDBnbi5wZGYADgAeAA4AQwBoAGUAbgAyADAAMAAwAGcAbgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9DaGVuMjAwMGduLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Deng:2004ja,
	Author = {Deng, D. and Zhang, J. and Purvis, M.},
	Booktitle = {Australasian Workshop on Data Mining and Web Intelligence (DMWI2004)},
	Date-Added = {2009-02-11 14:29:58 +0000},
	Date-Modified = {2009-03-12 13:46:02 +0000},
	Keywords = {SOM, Visualisation},
	Pages = {97-102},
	Title = {Visualisation and Comparison of Image Collections based on Self-organised Maps},
	Volume = {32},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvRGVuZzIwMDRqYS5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkRlbmcyMDA0amEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN373Ft1mRAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFt1mRAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6RGVuZzIwMDRqYS5wZGYADgAeAA4ARABlAG4AZwAyADAAMAA0AGoAYQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9EZW5nMjAwNGphLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Dontcheva:2005qh,
	Abstract = {We present an interactive visualization approach for browsinga collection of images and their taxonomy. Using massspring simulation we generate a layout that clusters images
with similar metadata and provide the user with a number of interaction mechanisms for browsing the metadata clusters. It is our hope that this type of interactive visualization will accelerate image search and further aid users in better understanding the contents of a collection of images.},
	Annote = {This short paper uses images from Flickr and arranges them using mass-spring simulation. Images are initially placed randomly in a grid with N spaces (N being the number of images in the database). Image similarity is based on the 'tag' words, added by users when they upload images to Flickr. A spring is made between 2 images if they share an associated keyword. The length of the spring is is formulated by L(i , j) = k * N(t). Where L(i , j) is the link, N(t) is the number of images with an associated tag, and k is a constant used to control the density of the layout. To generate the layout the simulation is evolved using the Runge-Kutta method with a time step of 0.075. The image tag is placed at the centre of a cluster. Other UI actions include dimming of images not part of the selected cluster, or the ability to select and prune the display. The paper concludes that this technique will only be suitable for a few hundred images (due to the time required to stablise the arrangement). The authors do note that performance could be improved if initial clustering was performed first (much like PCA with MDS). },
	Author = {Dontcheva, M. and Agrawala, M. and Cohen, M.},
	Booktitle = {18th Annual ACM Symposium on User Interface Software and Technology},
	Date-Added = {2009-02-11 14:25:34 +0000},
	Date-Modified = {2009-02-20 15:02:49 +0000},
	Keywords = {Visualisation},
	Read = {Yes},
	Title = {Metadata Visualization for Image Browsing},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvRG9udGNoZXZhMjAwNXFoLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTRG9udGNoZXZhMjAwNXFoLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3ihMW3k18AAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW3k18AAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpEb250Y2hldmEyMDA1cWgucGRmAAAOACgAEwBEAG8AbgB0AGMAaABlAHYAYQAyADAAMAA1AHEAaAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9Eb250Y2hldmEyMDA1cWgucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@techreport{Torralba:2007yt,
	Annote = {@techreport{tinyTR,
	author = {Torralba, A.  and Fergus, R.  and Freeman, W. T. },
	citeulike-article-id = {3280644},
	institution = {Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology},
	keywords = {visualization},
	number = {MIT-CSAIL-TR-2007-024},
	posted-at = {2008-09-17 16:12:06},
	priority = {2},
	title = {Tiny Images},
	url = {http://dspace.mit.edu/handle/1721.1/37291},
	year = {2007}
}},
	Author = {Torralba, A.  and Fergus, R.  and Freeman, W. T. },
	Date-Added = {2009-02-11 14:20:52 +0000},
	Date-Modified = {2009-02-14 17:54:57 +0000},
	Institution = {Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology},
	Keywords = {Semantic System},
	Title = {Tiny Images},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvVG9ycmFsYmEyMDA3eXQucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJUb3JyYWxiYTIwMDd5dC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADd+rxbdZYwAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbdZYwAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlRvcnJhbGJhMjAwN3l0LnBkZgAOACYAEgBUAG8AcgByAGEAbABiAGEAMgAwADAANwB5AHQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvVG9ycmFsYmEyMDA3eXQucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Chen:2008vy,
	Abstract = {Current photo browsers for personal photo collections mostly use the
folder structure or camera-recorded time stamps as the only ordering principle.
Some also allow manually provided meta-data (tags) for organizing and retrieving
photos, but currently, only professional tools allow a pre-grouping of photos
by image similarity. We believe that similarity is indeed a useful criterion both
for image retrieval and casual browsing, and that the tight integration of content
analysis techniques in media UIs in general can lead to more powerful UIs. In
this paper we present a prototype, in which we have tightly integrated image
analysis techniques and user feedback into a zoomable user interface for browsing
and sorting digital photos. We have discussed our system with domain experts
and received encouragement as well as valuable ideas for future research.},
	Annote = {This paper discusses PhotoSim, a system aimed at "a general audience, who produce a high number of photos, but also are very reluctant to explicitly manually tag them". The system creates a hierarchical structure based on clustering on time data aswell as low level content features.

The user interface is made up of 4 panels; Graph and Tree views, an overview and a cotrol panel.The graph view, located on the left of the screen, displays images clustered by content. Dents in the groupings represent outliers.

The low level features used are a colour histogram in the YUV space, aswell as colour moments. This results in 48 colour features. Texture and roughness are also extracted resulting in 161 feature dimensions. 813 photos were used in the database.

K-means clustering is performed on these colour features. The example shows that the algorithm successfully groups portraits aswell as buildings at day and night.

For relevance feedback, the user can drag and drop images between clusters. The user can also create create new manual groups (which are saved) and automatic clusters, by selecting a relevant image and dragging it out of its original cluster, a new cluster is created, with all other similar images from that time period added to the newly formed cluster.

Although no user study has been performed, disscusions with a former professional photographer suggested the system would be extremely useful if the groups formed could be translated back to a file system structure.},
	Author = {Y. Chen and A. Butz},
	Booktitle = {International Symposium on Smart Graphics},
	Date-Added = {2009-02-11 14:15:32 +0000},
	Date-Modified = {2009-04-14 16:07:24 +0100},
	Keywords = {CBIR System, Clustering, Relevance Feedback},
	Publisher = {Springer},
	Read = {Yes},
	Title = {PhotoSim: Tightly Integrating Image Analysis into a Photo Browsing {UI}},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvQ2hlbjIwMDh2eS5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkNoZW4yMDA4dnkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN32rFt1ipAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAADFt1ipAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Q2hlbjIwMDh2eS5wZGYADgAeAA4AQwBoAGUAbgAyADAAMAA4AHYAeQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9DaGVuMjAwOHZ5LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Rodden:1999ec,
	Annote = {This paper investigates the arrangement of images by similarity. The first experiment tests whether arranging images according to their visual similarity faster than a random assortment of the images. For the similarity metric, the image is first dynamically divided into regions of homogenous colour. The shape, colour and texture are recorded for each region, aswell as a binary operator if the region was large or small (less than 0.1%).

An image is also divided into 9 equal segments, 4 histograms are taken for large smooth regions, large textured regions, small regular and small irregular regions. The largest region in each of the nine section is recorded. The Chi-squared distance between image pairs is used, whereby the Mahalanobis distance is used between dominant regions in corresponding image segments. MDS is used as to layout images by similarity.

The first hypothesis was that the similairty based layout would indeed be faster than the random assortment. 16 subjects were used, with 48 sets of 80 images. The test involved the user being shown an image for 10 seconds, and asked to locate the image within 20 seconds. Only landscape images were selected to prevent users filtering specificly orientated images.

The results showed that users were significantly faster using the MDS display. One comment from users was that it was easier to find images of bright/contrasting colour than specific scenes i.e. buildings. By measuring the distinctiveness of an image (it's average similarity to all other images in the database) the authors proved that the distinctiveness of the image impacts upon the search time. Images located closer to the centre of the screen were also shown to improve search time.

During a post-experiment questionaire, all subjects commented that overlap in MDS had impacted their search. The authors decided to assign an MDS structure to a grid, assigning each image to a discretised area on the screen. Using a replacing approach, overlapping images (images in same grid cell) are moved. 3 approaches were tested:

Basic: a spiral search is undertaken, the first vacant cell is the new location.
Swap: an image originally located to a cell is moved to the next nearest cell as another image with a smaller euclidean distance is to be placed in the cell.
Bump: multiple images are moved outwards to put closer image in a nearer cell. Tests showed descending bump appeared to be the best strategy overall as it minimizes the maximum error (distance image is removed from the originally assigned cell).},
	Author = {K. Rodden and W. Basalaj and D. Sinclair and K. Wood},
	Booktitle = {IEEE Symposium on Information Visualisation},
	Cited-By = {Rodden:2001cr},
	Cites = {Jacobs:1995bh, Chen1998, Rubner:1997dd},
	Date-Added = {2009-02-11 11:14:50 +0000},
	Date-Modified = {2009-02-20 16:46:53 +0000},
	Keywords = {User Study},
	Pages = {36-43},
	Read = {Yes},
	Title = {Evaluating a Visualisation of Image Similarity as a Tool for Image Browsing},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvUm9kZGVuMTk5OWVjLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQUm9kZGVuMTk5OWVjLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3nR8W4Y4QAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW4Y4QAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSb2RkZW4xOTk5ZWMucGRmAA4AIgAQAFIAbwBkAGQAZQBuADEAOQA5ADkAZQBjAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JvZGRlbjE5OTllYy5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Nakazato:2003rw,
	Annote = {ImageGrouper, like EGO, allows users to group images as they see fit. This
system allows users to ``Query-by-group''. The paper explains how most RF
systems provide the user only with a ``Incremental Search'' whereby the user
selects an image, results are presented, and an image from this subset is
selected as the new query image. The authors note this can cause the user
to become trapped within a section of the system.
The paper remarks on novel RF techniques, such as Santini's El Nino system
allowing the user to manually drag images closer/further away to redefine
similarity. The authors of this paper state the issue whereby a user will
not know the exact effect any movement may incur on the system.

The user can first search the database, by content or keyword (from manual
annotations) and drag images from the results pane to the workspace, in
order to start grouping images. The user can group a selection of the
results together and label them as either positive, negative or neutral.
The system then returns more images based on the positive groupings. The
paper notes that most user actions are very similar to common OS commands.
A ``trial and error'' search pattern (as opposed to incremental search) can
be performed by dragging images in/out the group and querying.

The system allows for grouping within a group, i.e. in a group of cars, a
user may create a sub group of red cars. The user may then specify the red
cars group as positive and the other cars negative. When compared with a
traditional RF system (both with identical underlying RF algorithm and
feature extraction), the recall measure on the ImageGrouper was higher for
all image categories than the traditional system.

The paper moves on to discuss the group annotation method available to
ImageGrouper users. A user can annotate a selected group. Dragging an
image into an annotated group automatically annotates the newly added
image. Group annotations can be applied to sub-groups. Images can be
annotated with multiple keywords.

To test the system, ImageGrouper is compared with a selection RF system
(QBIC) and a slider system (MARS). 10 users were shown an image and asked
to select all the `semantically' relevant images. The selection based
system had the best average image selection time, followed by IG and the
slider system. The authors state that this is no surprise, as IG requires
drag and drop time.

The system is implemented as a client/server system, and the low level
features used are colour, edge and texture. For the RF algorithm, a query
vector is formed from the groupings by the user (for each separate
feature). Images with a common feature have the common feature weighted
more highly.


},
	Author = {M. Nakazato and L. Manola and T. Huang},
	Date-Added = {2009-02-11 10:20:16 +0000},
	Date-Modified = {2009-04-14 16:10:55 +0100},
	Journal = {Journal of Visual Language and Computing},
	Keywords = {Image Grouping, CBIR System},
	Number = {4},
	Pages = {363-386},
	Read = {Yes},
	Title = {Image{G}rouper: A Group-Oriented User Interface for Content-based Image Retrieval and Digital Image Arrangement},
	Volume = {14},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvTmFrYXphdG8yMDAzcncucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJOYWthemF0bzIwMDNydy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADeWRAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOk5ha2F6YXRvMjAwM3J3LnBkZgAOACYAEgBOAGEAawBhAHoAYQB0AG8AMgAwADAAMwByAHcALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTmFrYXphdG8yMDAzcncucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Nakazato:2001rp,
	Annote = {This paper describes 3D Mars, a system aimed at visualising an image database in 3D. Images are projected onto 4 walls in front of the user (left, right, front and floor), wearing shutter glasses and using a wand to interact with the images.

The system starts by displaying a set of random images from the database. As the user moves, the images rotate to face the user. The user can fly through the space, with a virtual compass provided on the floor to prevent them from becoming lost. Users can select an image as a positive or negative example for RF.

The axes X,Y and Z represent the colour, texture and structure descriptions of an image respectively. The most relevant images (according to the users choice) are located closer to the origin. The system also has a sphere mode, making it easier for the user to visualize clusters in the space.

However the paper also describes a dynamic axes approach where FastMap is applied to the feature vectors, and the axes are transformed so the query vector is placed at the origin.

For the features, colour is the first 2 moments from each colour channel in the HSV colour space, texture is 10 standard deviation values from a wavelet filter bank, and a water-fill edge detector is used for edge structure.

The paper details the RF algorithm used, which assigns a higher weight to features which have the smallest distances (in the positive RF examples).

The authors state a flaw in the system being that the user must QBE from a random selection. This could be improved by browsing.},
	Author = {M. Nakazato and T. Huang},
	Booktitle = {Proceedings of the 2001 IEEE International Conference on Multimedia and Expo},
	Cites = {Orengo:1995rz, Faloutsos:1995ad, Kruskal:1978os, Santini:2000fv, Rubner:1997dd, Chen:2000gn, Pecenovic:2000cr},
	Date-Added = {2009-02-11 10:07:32 +0000},
	Date-Modified = {2009-04-14 16:10:37 +0100},
	Keywords = {Visualisation},
	Read = {Yes},
	Title = {{3D MARS}: Immersive Virtual Reality for Content-Based Image Retrieval},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvTmFrYXphdG8yMDAxcnAucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJOYWthemF0bzIwMDFycC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADeS2xbhS8AAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbhS8AAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOk5ha2F6YXRvMjAwMXJwLnBkZgAOACYAEgBOAGEAawBhAHoAYQB0AG8AMgAwADAAMQByAHAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTmFrYXphdG8yMDAxcnAucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@article{Ma:1999gd,
	Abstract = {We present here an implementation of NeTra, a prototype image retrieval system that uses color, texture, shape and spatial location information in segmented image regions to search and retrieve similar regions from the database. A distinguishing aspect of this system is its incorporation of a robust automated image segmentation algorithm that allows object- or region-based search. Image segmentation significantly improves the quality of image retrieval when images contain multiple complex...},
	Author = {Ma, W. Y.  and Manjunath, B. S.},
	Cited-By = {Krischnamachari1999,},
	Cites = {Rubner:1997dd, Smith:1996dq, Ballard:1991yq, Orengo:1995rz},
	Date-Added = {2009-02-11 09:58:06 +0000},
	Date-Modified = {2009-04-14 16:10:18 +0100},
	Journal = {Multimedia Systems},
	Keywords = {CBIR System},
	Number = {3},
	Pages = {184-198},
	Title = {Ne{T}ra: A Toolbox for Navigating Large Image Databases},
	Volume = {7},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvTWExOTk5Z2QucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxNYTE5OTlnZC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADeSbAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOk1hMTk5OWdkLnBkZgAOABoADABNAGEAMQA5ADkAOQBnAGQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTWExOTk5Z2QucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@inproceedings{Faloutsos:1995ad,
	Annote = {FastMap is an alternative dimension reduction technique in order to reduce high dimensional data and visualise it on a 2D/3D space. The paper discusses MDS and PCA as traditional techniques, but states the complexity of MDS hampers its performance for use in real-time querying. It also states that PCA (or the Karhunen-Transform KL) can be slow for large databases when objects have many attributes.

The paper describes the mapping of high dimensional spaces to a 1D space (line). This technique chooses two 'pivot objects' which is any arbituary point and its furthest possible neighbour. All points are mapped to the line that connects the 2 points.

The paper then shows how this technique can be applied to 2D/3D spaces. Using 2 pivot points a with the 1D technique, this time a hyperplane located perpendicular (at a right angle) to the line is used to map all the points to.

The algorithm is listed in the paper, and takes the distance matrix between all objects and the objects. The algorithmis shown to operate in O(n). The paper show a small example of how the algorithm works, and demonstrates how well clusters of similar objects in the high dimensional space is replicated in a 2D/3D scatter plot.

The paper ends by testing FastMap against MDS, showing more than comparable results in much shorter times.},
	Author = {C. Faloutsos and K. Lin},
	Booktitle = {Proceedings of SIGMOD' 95},
	Cited-By = {Smeulders:2000zl, Nakazato:2001rp},
	Cites = {Kruskal:1978os},
	Date-Added = {2009-02-11 09:19:51 +0000},
	Date-Modified = {2009-04-14 16:08:19 +0100},
	Keywords = {MDS},
	Pages = {163-174},
	Read = {Yes},
	Title = {Fast{M}ap: A fast algorithm for indexing, datamining and visualization of traditional and multimedia datasets},
	Year = {1995},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvRmFsb3V0c29zMTk5NWFkLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTRmFsb3V0c29zMTk5NWFkLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3jssW4SKAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW4SKAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpGYWxvdXRzb3MxOTk1YWQucGRmAAAOACgAEwBGAGEAbABvAHUAdABzAG8AcwAxADkAOQA1AGEAZAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9GYWxvdXRzb3MxOTk1YWQucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@article{Pentland:1996oz,
	Annote = {

Authors: Pentland A.1, 2; Picard R.W.1, 3; Sclaroff S.1, 4

Source: International Journal of Computer Vision, Volume 18, Number 3, 1996 , pp. 233-254(22)

Publisher: Springer
},
	Author = {A. Pentland and W.R. Picard and S. Sclaroff},
	Cited-By = {Smeulders:2000zl ,Chen:2000ly, Chen:2000gn, Cox:2000eh, Pass:1996fv, Rubner:1997dd, Huang:1997zl, Huang:1999wf, Carson:1999rz, },
	Cites = {Ballard:1991yq},
	Date-Added = {2009-02-10 15:40:40 +0000},
	Date-Modified = {2009-02-15 16:38:18 +0000},
	Journal = {International Journal of Computer Vision},
	Keywords = {CBIR System},
	Number = {3},
	Pages = {233-254},
	Title = {Photobook: Content-Based Manipulation of Image Databases},
	Volume = {18},
	Year = {1996},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvUGVudGxhbmQxOTk2b3oucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJQZW50bGFuZDE5OTZvei5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADdnCxbZ88QAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbZ88QAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlBlbnRsYW5kMTk5Nm96LnBkZgAOACYAEgBQAGUAbgB0AGwAYQBuAGQAMQA5ADkANgBvAHoALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvUGVudGxhbmQxOTk2b3oucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Schaefer:2006zp,
	Annote = {This paper is based on the system devised in Schaefer:2005vl but looks to rid the sphere of occluded and overlapped images caused by the authors previous implementation (as overlapping is deemed undesirable in a user study conducted in Rodden:1999ec. Instead, images are not overlapped and are fitted to a 24 x 30 or 27 x 35 grid (dependent on screen size). 

In larger image datasets, the paper remarks that this will cause muliplt images to be assigned to one grid square. To reduce this, and to fill most squares on the initial grid, if an empty grid is found in a neighborhood, images with co-ordinates on the boundary of the empty grid square will be moved to the vacant cell. This routine is repeated 3/4 times. Not all squares will be non-empty as moving images too far from their original co-ordinates will disrupt the display.

Using the grid from the initial display, this forms natural clusters that are used for heirarchical browsing. A representative image is selected from the cluster (the centroid image relative to all other images in the cluster). Each level of the tree is mapped to the grid to be displayed on the sphere. The paper claims that 3 levels of the hierarchy can be used to represent 23 million images (if 40% of the cells are occupied. The structure can be computed off-line.

To spread images between cells at lower levels of the tree, and to prevent further levels of the tree being produced (as more than 1 image in a cell present) the "place", "bump" and "double-bump". First a "spiral scan" around the cells in the grid is performed. Should  an empty cell in the first ring be detected, the image is placed in this cell. Should an empty cell be detected in the second ring, the closest image in the first cell is transferred to the second ring empty cell using "bumb". This process is repeated between the second and third rings using "double bump". This technique is taken from Rodden:1999ec.},
	Author = {G. Schaefer and S. Ruszala},
	Booktitle = {Advances in Visual Computing},
	Date-Added = {2009-02-10 15:30:27 +0000},
	Date-Modified = {2009-02-14 18:04:03 +0000},
	Keywords = {Visualisation},
	Pages = {814-823},
	Read = {Yes},
	Title = {Image database navigation on a hierarchical hue sphere},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvU2NoYWVmZXIyMDA2enAucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJTY2hhZWZlcjIwMDZ6cC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADdxVxbdL2gAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbdL2gAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlNjaGFlZmVyMjAwNnpwLnBkZgAOACYAEgBTAGMAaABhAGUAZgBlAHIAMgAwADAANgB6AHAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvU2NoYWVmZXIyMDA2enAucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Schaefer:2005vl,
	Annote = {This is the first of two conference papers by Schaefer and Ruszala, based on the visualisation of large image datasets projected on to a 3D globe structure (the second paper is Schaefer:2006zp). In this work, the application of MDS is discredited as it is computationally expensive and therefore "almost impossible" to  achieve real time navigation. Also the addition of images to the dataset could prove troublesome. The paper also mentions 3D MARS (and the 2D version ImageGrouper) but rues the quality of the global browsing interface (were images are randomly placed).

To position images on their globe, the RGB components of an image were converted to ths HSV colour space. The V (Value) represented the latitude (in a global sense) so that one pole is black, the other white. A V of 0.5 corresponds to a position on the equator. Hue (Colour) represents the latitude., so as the globe is rotated, the genral colour of the images will change.

The paper claims that as the co-ordinates are taken directly from the image, mapping is much faster than a MDS. The user has access to tools whereby they may zoom in/out of areas of interest and may rotate the cube. },
	Author = {G. Schaefer and S. Ruszala},
	Booktitle = {Advances in Visual Computing},
	Date-Added = {2009-02-10 15:22:47 +0000},
	Date-Modified = {2009-02-14 18:02:02 +0000},
	Keywords = {Visualisation, Browsing},
	Pages = {279-286},
	Read = {Yes},
	Title = {Image database navigation: a globe-al approach},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvU2NoYWVmZXIyMDA1dmwucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJTY2hhZWZlcjIwMDV2bC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADdxexbdL4gAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbdL4gAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlNjaGFlZmVyMjAwNXZsLnBkZgAOACYAEgBTAGMAaABhAGUAZgBlAHIAMgAwADAANQB2AGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvU2NoYWVmZXIyMDA1dmwucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Heesch:2004rt,
	Annote = {This work describes 2 techniques for CBIR, and 1 for Content-Based browsing. The browsing method is that used in Heesch:2004rt, however this paper describes a a similar interface. This is covered in the section  "Using NNk for browsing". 

The example query in the database is searching for a monument, mostly white in colour. From the clustered overview of the database, the user selects a football as being most similar. The football image is placed in the centre of the screen, whilst it's NNk neighbours are placed around this central image. The distance a NNk neighbour is displayed away from the central image is related to the weight space proportion that NNk takes up (higher proportion = closer).

This example query does not seem ideal to me, as a general user searching for a monument would not be selecting a football as a natural relation.

The other two implementations in this paper use a relevance feedback technique and a NNk search. For the RF, images are presented in a spiral away from the center image. The user can select and move images closer or further away from the centre. This position adjustment will alter the weighting of the distance metric used within the system.

For the NNk search, a spiral visualisation is also used. According to the proportion of the weight space an image takes up, it is placed on the spiral. More highly weighted images occur closer to the central image on the spiral. The authors state that a primary advantage of this arrangement is that similar images are located close to the query image and thus in the users attention area. Less similar images will appear on the periphary of the users attention area, as they are deemed less important.

When a user selects an image as similar, the attributed weight set is retrieved and used as the basis to query remaining images in the database.

Although this is stated as the method of visualisation, one aspect of the work that is not clear is that Figure 3, supposedly displaying the screen output of such a search, clearly has some technique that bases the size of the thumbnail on the weight space of the NNk neighbour.

},
	Author = {D. Heesch and S. R\"{u}ger},
	Booktitle = {International Conference on Image and Video Retrieval},
	Date-Added = {2009-02-09 08:54:20 +0000},
	Date-Modified = {2009-02-14 17:29:45 +0000},
	Keywords = {NNk Network, Visualisation, Relevance Feedback},
	Pages = {491-499},
	Read = {Yes},
	Title = {Three interfaces for content-based access to image collections},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvSGVlc2NoMjAwNHJ0LnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQSGVlc2NoMjAwNHJ0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3D58W0384AAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW0384AAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpIZWVzY2gyMDA0cnQucGRmAA4AIgAQAEgAZQBlAHMAYwBoADIAMAAwADQAcgB0AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0hlZXNjaDIwMDRydC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Heesch:2004yq,
	Annote = {This paper outlines the concept of NNk networks and their potential use fo CBIR. This paper is a foundation of the work detailed in Heesch:2004rt. The basic is principle behind this concept is that a directed graph is formed between every image and it's nearest neighbours, based on 7 features. These are:

1) HSV Colour Histogram
2) Colour Structure Descriptor (for detailing spatial formation of colour)
3) Thumbnail Feature (image is scaled down and gray values calculated. This i used for near identical images, such as sequential image frames in a video)
4) 3 Texture features
5) 'Bag of words' (stemmed words taken from text attributted to image(s))

In NNk, NN stands for Nearest Neighbour, and k represents the k different feature types (in this case 7). An arc (link) between images is formed if there exists at least 1 possible combination of features for which the image is the top ranked of the other. 

A weight space is used which is a predefined number of individual weights of each feature. The number of the weight sets for which an image is top ranked, forms the similarity measure between images.

E.g. Say three weight sets are defined, and we have some query image Q. If image A is ranked top in the first image set, but B is ranked top in the 2nd and 3rd weight sets, B takes a higher proportion of the weight space and therefore is deemed more similar than A.

The distance between individual features (i.e. Colour histograms) is used to measure top rating images under certain features. The paper remarks that the L1 metric is used.

Each image in the networks stores it's nearest neighbours, along with the proportion of feature combinations for which the image is ranked top. This value is used as the similarity measure. 

In the interface defined in this paper, images with a higher proportion are displayed closer to the query (focus) image, which is centralised on the display. A threshold value, T, is used to determine the number of nearest neighbour to be displayed. This is the top X similar images, and can be user defined or dependent on current window size.

For the original representaion of the database, clustering is performed on all images. The most representative image of the cluster is displayed at the rough location of the cluster as displayed in 2D (rough as some displacement occurs due to reduction of overlap).

The display had 2 panels for which the user could store favoured images or images that could be used to formulate a query.

The system was tested using the TRECVID 2003 dataset and test, whereby a system must return the top 1000 shots to some query image, with a set amount of user interaction.

The authors state "the resulting network helps expose the semantic richness of images".},
	Author = {D. Heesch and S. R\"{u}ger},
	Booktitle = {European Conference on Information Retrieval},
	Date-Added = {2009-02-09 08:52:08 +0000},
	Date-Modified = {2009-04-14 16:08:44 +0100},
	Keywords = {NNk Network, Visualisation},
	Pages = {253-266},
	Read = {Yes},
	Title = {{NN}k networks for content-based image retrieval},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvSGVlc2NoMjAwNHlxLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQSGVlc2NoMjAwNHlxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA3D2sW035kAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW035kAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpIZWVzY2gyMDA0eXEucGRmAA4AIgAQAEgAZQBlAHMAYwBoADIAMAAwADQAeQBxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0hlZXNjaDIwMDR5cS5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Kuchinsky:1999ai,
	Author = {Kuchinsky, A. and Pering, C. and Creech, M. and Freeze, D. and Serra, B. and Gwizdka, J.},
	Booktitle = {SIGCHI Conference on Human Factors in Computing Systems},
	Date-Added = {2009-02-05 18:01:04 +0000},
	Date-Modified = {2009-02-14 18:10:43 +0000},
	Keywords = {Browsing, CBIR System},
	Pages = {496-503},
	Publisher = {ACM},
	Title = {FotoFile: a consumer multimedia organization and retrieval system},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvS3VjaGluc2t5MTk5OWFpLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTS3VjaGluc2t5MTk5OWFpLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA1YWMWw2LQAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWw2LQAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpLdWNoaW5za3kxOTk5YWkucGRmAAAOACgAEwBLAHUAYwBoAGkAbgBzAGsAeQAxADkAOQA5AGEAaQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9LdWNoaW5za3kxOTk5YWkucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@inproceedings{Smith:1996dq,
	Author = {Smith, J. R. and Chang, S.},
	Booktitle = {Proceedings of the fourth ACM International Conference on Multimedia},
	Cited-By = {Ma:1999gd, Platt:2000kx},
	Cites = {Ballard:1991yq, Jacobs:1995bh, Pass:1996fv},
	Date-Added = {2009-02-05 17:58:31 +0000},
	Date-Modified = {2009-02-15 14:05:32 +0000},
	Keywords = {CBIR System},
	Pages = {87-98},
	Title = {VisualSEEk: a fully automated content-based image query system},
	Year = {1996},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvU21pdGgxOTk2ZHEucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9TbWl0aDE5OTZkcS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADVh7xbDZPwAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbDZPwAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlNtaXRoMTk5NmRxLnBkZgAADgAgAA8AUwBtAGkAdABoADEAOQA5ADYAZABxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1NtaXRoMTk5NmRxLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@inproceedings{Jacobs:1995bh,
	Annote = {This paper describes a new metric for comparing images. The authors describe their use of wavelet decompositions in order to produce a signature on poor quality images (such has hand sketched or scanned images) comparable with images in the database.

Like QBIC, the system creators claim the system only to be an information filter, therefore selecting 20 images in the database similar to that of the query image, and let the user browse the returned images to select the required target. The paper claims there new method can perform a comparison between a query image (128 * 128 pixels) and 20,000 images in a database in less than 1/2 second. The same query using the L1 distance took over 14 minutes.

The novelty of the work is said to be the multiresolution approach. The system uses the YIQ colour space, with Haar wavelets co-efficients computed for each colour channel. Rather than keep each co-efficient for every pixel/channel, the top M largest magnitude coefficients in each channel are used. Each significant co-efficient is quantized to either +1 and -1 (for large positive or negative co-efficients).

A score between each image in the database and the query image is created from the metric, which is also weighted according to the type of images present in the database.

The signature is composed of the average image colour, and the indices of the M largest wavelet co-efficients. Each image in the database is assigned to 1 of 6 search arrays: for each combination of sign (+ | -) and each colour channel. Therefore when given a query image, only 1/6th of the database is compared with the query image.

Given the speed at which the database can be searched, whilst a user pauses for a brief moment whilst sketching an image, possible target images are updated accordingly. As well as being faster than the L1/2 metrics, the quality of the returned results is at least as good, if not improved.},
	Author = {Jacobs,, C. E. and Finkelstein,, A. and Salesin,, D. H.},
	Booktitle = {Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques},
	Cited-By = {Smith:1996dq, Rodden:1999ec, Carson:1999rz, Platt:2000kx},
	Cites = {Faloutsos:1994lq},
	Date-Added = {2009-02-05 17:53:15 +0000},
	Date-Modified = {2009-02-17 17:13:51 +0000},
	Keywords = {Distances},
	Pages = {277-286},
	Publisher = {ACM},
	Read = {Yes},
	Title = {Fast multiresolution image querying},
	Year = {1995},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvSmFjb2JzMTk5NWJoLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQSmFjb2JzMTk5NWJoLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA1YOcWw18gAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWw18gAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpKYWNvYnMxOTk1YmgucGRmAA4AIgAQAEoAYQBjAG8AYgBzADEAOQA5ADUAYgBoAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0phY29iczE5OTViaC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Cox:1996qf,
	Author = {Cox, I. J. and Miller, M. L. and Omohundro, S. M. and Yianilos, P. N.},
	Booktitle = {Proceedings of the International Conference on Pattern Recognition},
	Cited-By = {Huang:1997zl, Huang:1999wf, Cox:2000eh, Platt:2000kx, },
	Date-Added = {2009-02-05 17:48:30 +0000},
	Date-Modified = {2009-02-15 14:05:20 +0000},
	Keywords = {CBIR System, Relevance Feedback},
	Pages = {361},
	Publisher = {IEEE},
	Title = {PicHunter: Bayesian Relevance Feedback for Image Retrieval},
	Volume = {3},
	Year = {1996},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvQ294MTk5NnFmLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNQ294MTk5NnFmLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA1YG8Ww1scAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWw1scAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpDb3gxOTk2cWYucGRmAAAOABwADQBDAG8AeAAxADkAOQA2AHEAZgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9Db3gxOTk2cWYucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Loui:1999ve,
	Author = {Loui, A. and Wood, M.},
	Booktitle = {Proceedings of the seventh ACM International Conference on Multimedia (Part 2)},
	Date-Added = {2009-02-05 17:39:58 +0000},
	Date-Modified = {2009-02-14 18:10:52 +0000},
	Pages = {159 - 162 },
	Title = {A software system for automatic albuming of consumer pictures},
	Year = {1999}}

@inproceedings{Graham:2002zr,
	Author = {A. Graham and H. Garcia-Molina and A. Paepcke and T. Winograd},
	Booktitle = {2nd ACM/IEEE-CS Joint Conference on Digital libraries},
	Cites = {Platt2002, Rodden:2001cr, Platt:2000kx, Bederson:2001fv, Kang:2000ly},
	Date-Added = {2009-02-05 17:28:44 +0000},
	Date-Modified = {2009-02-15 17:03:19 +0000},
	Keywords = {Time Browsing},
	Pages = {326-335},
	Publisher = {ACM},
	Title = {Time as essence for photo browsing through personal digital libraries},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvR3JhaGFtMjAwMnpyLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQR3JhaGFtMjAwMnpyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA1Xi8Ww0nQAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWw0nQAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpHcmFoYW0yMDAyenIucGRmAA4AIgAQAEcAcgBhAGgAYQBtADIAMAAwADIAegByAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0dyYWhhbTIwMDJ6ci5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Combs:1999ys,
	Annote = {This paper studies the functionality of zooming within an image browsing system. The paper offers a dictionary definition of browsing, as "to examine in a casual way". The authors state the motivation for the research as the belief that 3D andzooming browsers make better use of screen space than scroll bars.

The tests compare the authors system - ZIB (zoomable image browser) with other commercial systems, available at www.trivista.com. ZIB has 3 panels, for query, query history and browsing. The user enters a boolean text entry (images are presumed to be annotated), with the query history being shown in history pane, and appropriate images shown in the browsing window. The system has a thesaurus of all words attributed to images in the database. Images are not arranged in any order within the browsing window.

The 2 3D systems used are a photo-carousel, where images are on a rotating shop rack (always facing the user) and 3D SimpleLandscape, where the images are presented on billboards.

For the experiments between systems, the authors proposed the hypothesis that there would be no significant difference between time taken to locate a target image, preferred image browser and incorrect selections made.

30 user participated in the study. Aswell as a search for locating a target image, recall was also tested. The user was asked if 4 images appeared in the collection they had just examined.

ZIB was the fastest browser, but not significantly than ZoomBrowser, a 2D collection of images arranged in a file structure. This was the same for user preference. Recall was 15% higher than ZoomBrowser.

Half of the particiapnts did not use the zoom facility provided by ZIB, and as images are not arranged by similarity, make it very similar to ZoomBrowser but with a search facility.

The number of incorrect selections increased as the number of images increased. Users commented that ZoomBrowser may have been the most effective, had a clustering by content been applied.
 },
	Author = {T. Combs and B. Bederson },
	Booktitle = {4th ACM Conference on Digital Libraries},
	Cited-By = {Bederson:2001fv, Rodden:2001cr},
	Cites = {Plaisant:1995vn},
	Date-Added = {2009-02-05 17:25:31 +0000},
	Date-Modified = {2009-02-20 17:15:04 +0000},
	Keywords = {Zooming},
	Pages = {130-137},
	Read = {Yes},
	Title = {Does Zooming Improve Image Browsing?},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvQ29tYnMxOTk5eXMucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9Db21iczE5OTl5cy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADVdxxbDRdQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbDRdQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkNvbWJzMTk5OXlzLnBkZgAADgAgAA8AQwBvAG0AYgBzADEAOQA5ADkAeQBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0NvbWJzMTk5OXlzLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Plaisant:1995vn,
	Annote = {This paper attempts to set guidelines in order to construct a good image
browser. However in this sense, it describes the exploration of 1 large
image rather than multiple images. These guidelines may still apply if
considering the exploration of a large MDS created plane of image
similarity. The paper reviews previous explorations of large images (i.e.
maps) with papers proving the usefulness of overview windows, roaming and
zooming techniques, as opposed to mere scroll bars.

In section 1.3, the work lists some good definitions including Detail,
Global views, zooming factor etc. Section 3 describes various types of
browser including zoom and replace, whereby the global view is replaced
with a zoomed image when a user drags a rectangular selection tool over a
required area of an image. Another example is a tiled multi-view browser,
with the example given as a global, intermediate and detailed view of
Paris. Bifocal browser allows the user to place a magnifying region over
some area of interest in an image.

The paper moves to explore the different tasks required by users,
including browsing, diagnostic (medical applications) navigation (maps) or
monitoring. The paper states the importance of smooth zooming within these
types of browsers, to avoid confusing the user. Automatic window placement
systems are recommended to save the user time on rearranging them.

A global view in this work is seen as a contents page in a book. The
``Sticky Hand'' metaphor is appropriate when real-time navigation of an
image is available.},
	Author = {C. Plaisant and D. Carr and B. Shneiderman},
	Cited-By = {Combs:1999ys},
	Date-Added = {2009-02-05 17:22:07 +0000},
	Date-Modified = {2009-03-02 08:48:34 +0000},
	Journal = {IEEE Software},
	Keywords = {Visualisation, Visualisation Requirements, Zooming},
	Pages = {21-32},
	Read = {Yes},
	Title = {Image browsers: taxonomy, guidelines, and informal specifications},
	Volume = {12},
	Year = {1995},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvUGxhaXNhbnQxOTk1dm4ucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJQbGFpc2FudDE5OTV2bi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADVdIxbDQcgAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxbDQcgAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlBsYWlzYW50MTk5NXZuLnBkZgAOACYAEgBQAGwAYQBpAHMAYQBuAHQAMQA5ADkANQB2AG4ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvUGxhaXNhbnQxOTk1dm4ucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Platt:2000kx,
	Annote = {This work is the earlier version of the PhotoTOC system. The authors describe the system as a "browsing user interface explicity designed for consumer digital photography". This paper describes the clustering algorithm in more detail. Described as Best-First clustering, the paper first describes how the assignment of images to albums can be seen as a Hidden-Markov Model, with the album membership being the hidden state variable.

The work describes Best-First clustering as "like agglomerative clustering, but is probablistic and works on ordered data sets".

Each album has a generative model (based on the average colour histogram for that album). The colour is modelled in the CIE u' v' colour space. Best-First greedily merges clusters closest to each other, in order to reduce data loss. This merging continues until a preset number of clusters is reached.

The paper comments that their algorithm can assign 405 images to 60 clusters in 0.34 seconds.

The most interesting part of this work is the user study. The authors use a metric known in this paper as F1. First the human user clusters images images as they see natural. Then each image is treated as a query. Using the number of false positives, true positives and false negatives a % match is calculated. This is averaged over all images in the database to measure the quality of the clustering.

The results showed that on a database when the timestamp data was either lost or corrupted, colour based clustering performed best with a F1 score of around 65%. When both time and colour data are available, the user study showed the F1 score to be around 96%.},
	Author = {J.C. Platt},
	Booktitle = {IEEE Workshop on Content-Based Access of Image and Video Libraries},
	Cited-By = {Bederson:2001fv, Graham:2002zr},
	Cites = {Chen1998, Cox:1996qf, Jacobs:1995bh, Smith:1996dq},
	Date-Added = {2009-02-05 16:32:44 +0000},
	Date-Modified = {2009-04-14 16:13:39 +0100},
	Keywords = {Visualisation, Clustering, User Study, Time Browsing},
	Pages = {96-100},
	Publisher = {IEEE},
	Read = {Yes},
	Title = {Auto{A}lbum: Clustering digital photographs using probalistic model mergining},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvUGxhdHQyMDAwa3gucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9QbGF0dDIwMDBreC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADUXlxa8pJQAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAxa8pJQAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlBsYXR0MjAwMGt4LnBkZgAADgAgAA8AUABsAGEAdAB0ADIAMAAwADAAawB4AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1BsYXR0MjAwMGt4LnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@phdthesis{Rodden:2001fy,
	Author = {K. Rodden},
	Cited-By = {Rodden:2001cr, },
	Date-Added = {2009-02-03 13:51:41 +0000},
	Date-Modified = {2009-02-15 16:58:16 +0000},
	Keywords = {User Study},
	School = {University of Cambridge Computer Laboratory},
	Title = {Evaluating Similarity-Based Visualisations as Interfaces for Image Browsing},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvUm9kZGVuMjAwMWZ5LnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQUm9kZGVuMjAwMWZ5LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA0/J8Wt+7QAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWt+7QAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSb2RkZW4yMDAxZnkucGRmAA4AIgAQAFIAbwBkAGQAZQBuADIAMAAwADEAZgB5AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JvZGRlbjIwMDFmeS5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Rodden:2003th,
	Annote = {This is the second paper by this author concerning a user study into how people use image management software. In this study, 13 participants were each given a digital camera and a system named "Shoebox" which was said to offer the same capabilities as commercially availabe systems of the time. However two additional features were included in shoebox:

1) the ability to annotate images through voice recognition
2) content based image analysis, including image segmentation and QBVE

The two key areas of interest in this study were the differences between the organisation of digital and normal printed photograph collections, and the use of advanced multimedia processing.

The shoebox system recorded the features used by the user. The users were interviewed before and after the trial, lasting 6 months, to gauge their expectations, reviews and current practises.

In the organisation of printed photo's, the study suggests most participants order their images based on specific events, with 1 album used per an event. Within the album, the photos are stored chronologically.

It was found that for the digital collections, some users continued in this fashion by storing specific events in a seperate titled album. Other users decided to just pool all the photos into 1 album, relying on the chronological ordering of the time stamps from each image as an appropriate browsing mechanism. From the interviews and log files, it appeared no more effort had been expended in ordering digital photos as with conventional photos.

The study finds that many of the participants, despite keeping some notes on the back of printed photographs, very little annotation was performed on the digital library. Furthmore, only 3 changed the title of a single photograph, stating that a folder title was sufficient.

One flaw in the study as pointed out by the author is that the chronological ordering was considered sufficient by the users, most likely because all the images taken were recent and fresh in the users mind.

The user reviews on the voice annotation system were mixed, stating deficiencies with the usefulness and also its ability to handle names.

The author states 3 occasions when people tend to search through older images:
1) a set of images from a particular event e.g. holiday
2) an individual photo
3) a set of images from various events sharing a distinct quality i.e. a person

For a particular photo, the user study found that remembering the rough date of the event, and browsing the collection from there is sufficient. One of the reasons given for users finding it much simpler to find an image in the digital collection rather than the non-digital version was that many small thumbnails of each image can be displayed on a screen at any time, rather than searching through individual images.

The study found that users did not take to annotating images, thus removing the ability to search by keyword. Another issue derived from annotation is that the shoebox system could not group similar words i.e. the name David Smith with Dave.

QBVE did not go down well with the participants of the study either, and was the lowest rated feature after the post study interviews. One reason given by the authors for this is the enhanced expectation and ability of a CBIR system. An interesting comment made by 1 participant was that s/he did not know what the system was doing with the query image and thus when the system produced unsatisfactory results, the user had no idea how s/he could improve the query image.

The paper concludes that the 2 most important features for a user can be provided very easily - the ability to show many images at once, and to order the images chronologically.},
	Author = {Rodden, K. and Wood, K.},
	Booktitle = {SIGCHI Conference on Human factors in Computing Systems},
	Date-Added = {2009-02-03 13:44:24 +0000},
	Date-Modified = {2009-02-14 18:11:33 +0000},
	Keywords = {Visualisation, User Study},
	Pages = {409-416},
	Read = {Yes},
	Title = {How do people manage their digital photographs?},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvUm9kZGVuMjAwM3RoLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQUm9kZGVuMjAwM3RoLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA0+9MWt+kIAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMWt+kIAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSb2RkZW4yMDAzdGgucGRmAA4AIgAQAFIAbwBkAGQAZQBuADIAMAAwADMAdABoAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JvZGRlbjIwMDN0aC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@phdthesis{Heesch:2005sp,
	Author = {D. Heesch},
	Date-Added = {2009-02-03 13:37:18 +0000},
	Date-Modified = {2009-04-14 16:08:51 +0100},
	Keywords = {NNk Network, Browsing},
	School = {Imperial College London},
	Title = {The {NN}k technique for image searching and browsing},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvSGVlc2NoMjAwNXNwLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQSGVlc2NoMjAwNXNwLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03cwAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpIZWVzY2gyMDA1c3AucGRmAA4AIgAQAEgAZQBlAHMAYwBoADIAMAAwADUAcwBwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0hlZXNjaDIwMDVzcC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Heesch:2006dp,
	Author = {D. Heesch and A. Yavlinsky and S. Ruger},
	Booktitle = {Proceedings of the ACM International Conference on Multimedia (SIGMM)},
	Date-Added = {2009-02-03 13:28:29 +0000},
	Date-Modified = {2009-04-14 16:08:37 +0100},
	Keywords = {NNk Network, Visualisation},
	Pages = {220-224},
	Title = {{NN}k networks and automated annotation for browsing large image collections from the World Wide Web},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvSGVlc2NoMjAwNmRwLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQSGVlc2NoMjAwNmRwLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03kwAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpIZWVzY2gyMDA2ZHAucGRmAA4AIgAQAEgAZQBlAHMAYwBoADIAMAAwADYAZABwAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0hlZXNjaDIwMDZkcC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Roussopoulos:1995cr,
	Author = {N. Roussopoulos and S. Kelley and F. Vincent},
	Booktitle = {Proceedings of the ACM International Conference Management of Data},
	Cited-By = {Kurniawati1997, Chen2000},
	Date-Added = {2009-02-03 13:19:57 +0000},
	Date-Modified = {2009-02-15 14:12:18 +0000},
	Keywords = {Visualisation Context},
	Organization = {ACM},
	Title = {Nearest neighbor queries},
	Year = {1995},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEB1QYXBlcnMvUm91c3NvcG91bG9zMTk5NWNyLnBkZtIbDxwdV05TLmRhdGFPEQHYAAAAAAHYAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MWUm91c3NvcG91bG9zMTk5NWNyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03kAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBUTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSb3Vzc29wb3Vsb3MxOTk1Y3IucGRmAA4ALgAWAFIAbwB1AHMAcwBvAHAAbwB1AGwAbwBzADEAOQA5ADUAYwByAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBHVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JvdXNzb3BvdWxvczE5OTVjci5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMoAzwDXArMCtQK6AsMCzgLSAuAC5wLwAvUC+AAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAMF}}

@conference{Milanese:1996dq,
	Annote = {This paper describes the use of correspondance analysis (CA) in order to reduce the dimensionality of the data. The paper referes to PCA and Karhunen-Loeve decomposition. Using a data table G, a mathmatical function can be applied in order to create an observation matrix P, which can be be used with the eigenvectors of a covariance matrix in order to project the data table into the 2D space (G').

This formulation allows both images and features (rows and columns of G') to be projected onto a common space. This can be useful when distinguishing which feature is close to a particular cluster of images.

The paper then describes a binary hierarchical indexing of images in the database. This method is only applied to 54 gray scale images. Images are clustered based on their euclidean distances in the projected space. A binary tree is constructed, with {1,0} assigned to left and right child nodes.},
	Author = {R. Milanese and D. Squire and T. Pun },
	Booktitle = {IEEE International Conference on Image Processing },
	Cited-By = {Chen2000},
	Date-Added = {2009-02-03 13:13:22 +0000},
	Date-Modified = {2009-03-12 13:11:06 +0000},
	Keywords = {Hierarchical Browsing},
	Pages = {859-862},
	Read = {Yes},
	Title = {Correspondence analysis and hierarchical indexing for content-based image retrieval},
	Year = {1996},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvTWlsYW5lc2UxOTk2ZHEucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJNaWxhbmVzZTE5OTZkcS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTePAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOk1pbGFuZXNlMTk5NmRxLnBkZgAOACYAEgBNAGkAbABhAG4AZQBzAGUAMQA5ADkANgBkAHEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTWlsYW5lc2UxOTk2ZHEucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Urban:2003vn,
	Author = {J. Urban and J. Jose and C. van Rijsbergen},
	Booktitle = {Proceedings of the International Workshop Content-Based Multimedia Indexing},
	Date-Added = {2009-02-02 21:30:19 +0000},
	Date-Modified = {2009-02-14 17:05:42 +0000},
	Keywords = {Ostensive Browsing, Relevance Feedback},
	Pages = {119-126},
	Title = {An adaptive approach towards content-based image retrieval},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvVXJiYW4yMDAzdm4ucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9VcmJhbjIwMDN2bi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTeWAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlVyYmFuMjAwM3ZuLnBkZgAADgAgAA8AVQByAGIAYQBuADIAMAAwADMAdgBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1VyYmFuMjAwM3ZuLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@inproceedings{Zhang1995,
	Author = {H. Zhang and D. Zhong},
	Booktitle = {Proceedings of the SPIE/IS\&T Conference Storage and Retrieval for Image and Video Databases III},
	Cited-By = {Chen2000, },
	Cites = {Ballard:1991yq, Faloutsos:1994lq },
	Date-Added = {2009-02-02 21:01:30 +0000},
	Date-Modified = {2009-02-16 19:31:07 +0000},
	Keywords = {Hierarchical Browsing, SOM},
	Pages = {36-46},
	Title = {A scheme for visual feature based image indexing},
	Volume = {2420},
	Year = {1995},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvWmhhbmcxOTk1LnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNWmhhbmcxOTk1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03sQAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpaaGFuZzE5OTUucGRmAAAOABwADQBaAGgAYQBuAGcAMQA5ADkANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9aaGFuZzE5OTUucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Yang2006,
	Author = {J. Yang and  J. Fan and D. Hubball and Y. Gao and H. Luo and W. Ribarsky and M. Ward},
	Booktitle = {IEEE Symposium on Visual Analytics Science and Technology},
	Date-Added = {2009-02-02 20:55:54 +0000},
	Date-Modified = {2009-02-14 18:06:15 +0000},
	Keywords = {Browsing, Semantic System},
	Pages = {191-198},
	Title = {Semantic image browser: bridging information visualization with automated intelligent image analysis},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvWWFuZzIwMDYucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxZYW5nMjAwNi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTeuAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOllhbmcyMDA2LnBkZgAOABoADABZAGEAbgBnADIAMAAwADYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvWWFuZzIwMDYucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@inproceedings{Yang2004,
	Author = {J. Yang and A. Patro and S. Huang and N. Mehta and M. Ward and E. Rundensteiner},
	Booktitle = {Proceedings of the IEEE Symposium on Information Visualization},
	Date-Added = {2009-02-02 20:54:45 +0000},
	Date-Modified = {2009-02-14 17:31:05 +0000},
	Keywords = {Visualisation},
	Pages = {73-80},
	Title = {Value and relation display for interactive exploration of high dimensional datasets},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvWWFuZzIwMDQucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxZYW5nMjAwNC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTezAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOllhbmcyMDA0LnBkZgAOABoADABZAGEAbgBnADIAMAAwADQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvWWFuZzIwMDQucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@inproceedings{Robertson1991,
	Annote = {As with (Green and Rekimoto, 1993), this work is seen was seen as an
extension of the field of "Scientific Visualisation" (the abitlity to
represent numerical data in a high dimensional space).

The authors developed a system, which they call "The Information
Visualizer". They state that "the inrteractive animation reduces cognitive
load by exploiting the human perceptual system". They represent
heirachical information, rather than arbituary graphs. Rather than use a
3D input such as the glove used by (Green and Rekimoto, 1993), they opt to
use the 2D input of a mouse, as the authors believe information
visualisation is aimed at end users who may not be willing to wear such
radical equipment (i.e. office workers).

The Cone Tree are heirachies laid out uniformly in 3D, with each node
represented as a 3 x 5 style index card. Each cone is shaded transparently
in order for the cone to be eaily percieved without blocking the view to
cones that maybe behind the cone. When a node is selected by a mouse
button click by the user, the tree is rotated so that the selected cone is
located at the front of the users view, aswell as being highlighted. Text
in the cards are only shown for the selected path, in order to simplify
the view for the user. The rotations performed by the system asre animated
as to not confuse the user.

The diameters and height of the cones are sized dynamically in order to
fit the screen. A fish eye view is provided to the user. Shadows are drawn
upon the floor surface and cast on other cones in order to provide
additional structural information on the heirachy. The system has
"gardening" operations asscoiated with it. The user can "grow" (expand)
parts of the tree or "prune" (remove) other parts. Another tool added by
the system developers is the ability tosearch for text located on the 3 x
5 cards. This search is undertaken in another process, in order to reduce
degradation of the systems interactive performance. A node is highlighted
with a red bar which demonstrates the similairty measure between the query
and the text present at that particular node. For their tests, a Unix
directory was used, with 600 directories and 10,000 files.

During experiments, the authors claim "It is easy to demonstrate that
animation shifts cognitive load to the human perceptual system". Also an
apparent limit on the system is stated at 1000 nodes, 10 layers and a
branching factor of 30.},
	Author = {Robertson, G. and Mackinlay, J. and Card, S.},
	Booktitle = {SIGCHI Conference on Human factors in Computing Systems},
	Cited-By = {Green1993, Faloutsos:1995ad},
	Date-Added = {2009-02-02 20:45:31 +0000},
	Date-Modified = {2009-02-15 10:56:58 +0000},
	Keywords = {Visualisation Context},
	Pages = {189-194},
	Read = {Yes},
	Title = {Cone Trees: animated 3D visualizations of hierarchical information},
	Year = {1991},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvUm9iZXJ0c29uMTk5MS5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEVJvYmVydHNvbjE5OTEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN6wAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Um9iZXJ0c29uMTk5MS5wZGYAAA4AJAARAFIAbwBiAGUAcgB0AHMAbwBuADEAOQA5ADEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvUm9iZXJ0c29uMTk5MS5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@techreport{Platt2002,
	Annote = {This paper conducts a user study on their system, PhotoTOC, against the authors previous system (AutoAlbum) and other standard practises. The work comments that this study was into user's personal photographs, which is different to generic image databases. These differ in the fact users are more reluctant to annotate their images, and also the user will have a idea of when the image was taken.

This system uses a detail+overview approach. The user interface has both a detail pane and an overview pane. In the overview, clusters of the database are represented by the most representative image for that cluster. The detail pane shows all images in the database. Should the user select an image (cluster) in the overview pane, the system automatically scrolls to this part of the database in the detail pane.

The clustering is performed using a combination of time and colour histograms. The timestamp, automatically attatched to the image file when an image is taken, is used to cluster images into events. The system automatically derives whether a gap in time between images is significant enough to warrant a new event.

Should  a clustered event contain more than 23 images (not stated why 23), images are re-clustered based on colour. Roughly, the number of colour clusters is 1/12 the number of images in the cluster.

For the most representative image of the cluster, the Kullback-Leibler measure between every image histogram in the cluster and the average histogram for all images is used. The authors note that the representative image normally selected by this algorithm is of sufficient quality.

For the study, 8 participants were asked to perform the task of finding a target image within the database. Each participant was tested on 10 different query images, using 4 different interfaces. These were standard folders (to compare users current methods), lightbox where all thumbnails are preseneted and scrollable (to compare quality of using clusters), the authors previous system AutoAlbum (which, in the detail view of a cluster, only presented the images present in that cluster).

The average time for queries showed that the lightbox implementation was indeed the fastest, followed by PhotoTOC. During questioning, users stated that they felt more at ease with their own folders system, but admitted PhotoTOC made the search easier.},
	Author = {J. Platt and M. Czerwinski and B. Field},
	Cited-By = {Graham:2002zr, },
	Date-Added = {2009-02-02 20:43:20 +0000},
	Date-Modified = {2009-02-16 14:55:42 +0000},
	Institution = {Microsoft Research},
	Keywords = {Visualisation, Clustering, User Study, Time Browsing},
	Read = {Yes},
	Title = {Photo{TOC}: automatic clustering for browsing personal photographs},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvUGxhdHQyMDAyLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNUGxhdHQyMDAyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03tQAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpQbGF0dDIwMDIucGRmAAAOABwADQBQAGwAYQB0AHQAMgAwADAAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9QbGF0dDIwMDIucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Lim2005,
	Annote = {This work investigates the use of MPEG-7 texture descriptors in order to
browse an image database. This standard defines 2 texture descriptors:
Texture Retrieval Descriptor (TRD) and Texture Browsing Descriptor (TBD).
The paper describes how to extract these features. TBD conforms to human
perception in terms regularity, coarseness and directionality. It is made
up of 5 dimensions.

Several configurations for browsing are demonstrated. First the TBD is
used for MDS (distances between descriptors measured using the L1 norm).
Another idea is that the 5 dimensions making up the vector can be reduced,
as the 2 scale values (v4 and v5) can be added up, directions (v2 and v3)
may not be required valid. Therefore V1 (structure) and v4+v5 are used as
the 2 mapping dimensions. The overall 4 layouts were:
-	MDS-TBD: browsing descriptor used in MDS
-	MDS-L1: retrieval descriptor used in MDS with L1 distance
-	MDS-EMD: as above but with EMD
-	2D-TBD: 2 co-ordinate TBD used

Spatial precision and recall graphs were used, with known ground truth for
the image database (common in texture retrieval). The MDS-L1 and MDS-EMD
are concluded as more suitable for browsing, as shown by the PR graphs and
visual inspection.},
	Author = {S. Lim and L. Chen and G. Lu and R. Smith},
	Booktitle = {Proceedings of the International Conference on Multimedia Modelling},
	Date-Added = {2009-02-02 20:37:17 +0000},
	Date-Modified = {2009-03-02 08:50:52 +0000},
	Keywords = {MDS},
	Pages = {328-333},
	Publisher = {IEEE},
	Read = {Yes},
	Title = {Browsing texture image databases},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBJQYXBlcnMvTGltMjAwNS5wZGbSGw8cHVdOUy5kYXRhTxEBrAAAAAABrAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTC0xpbTIwMDUucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN7IAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIASU1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6TGltMjAwNS5wZGYAAA4AGAALAEwAaQBtADIAMAAwADUALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADxVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTGltMjAwNS5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAvwDEAMwCfAJ+AoMCjAKXApsCqQKwArkCvgLBAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAs4=}}

@article{Kurniawati1997,
	Author = {R. Kurniawati and J. Jin and J. Shepherd},
	Cites = {Faloutsos:1994lq, Roussopoulos:1995cr},
	Date-Added = {2009-02-02 20:21:13 +0000},
	Date-Modified = {2009-02-15 11:25:47 +0000},
	Journal = {The Australian Computer Journal},
	Keywords = {Visualisation Survey},
	Number = {4},
	Pages = {122-130},
	Title = {Techniques for supporting efficient content-based retrieval in multimedia databases},
	Volume = {29},
	Year = {1997},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvS3Vybmlhd2F0aTE5OTcucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJLdXJuaWF3YXRpMTk5Ny5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTe2AAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkt1cm5pYXdhdGkxOTk3LnBkZgAOACYAEgBLAHUAcgBuAGkAYQB3AGEAdABpADEAOQA5ADcALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvS3Vybmlhd2F0aTE5OTcucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@article{Chen2000,
	Author = {J. Chen and C. Bouman and J. C. Dalton },
	Cites = {Rubner:1997dd, Milanese:1996dq, Zhang1995, Roussopoulos:1995cr, Chen1998, },
	Date-Added = {2009-02-02 20:14:21 +0000},
	Date-Modified = {2009-03-02 09:58:14 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {Hierarchical Browsing},
	Number = {3},
	Pages = {442 - 455},
	Title = {Hierarchical Browsing and Search of Large Image Databases },
	Volume = {9},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvQ2hlbjIwMDAucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxDaGVuMjAwMC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTevAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkNoZW4yMDAwLnBkZgAOABoADABDAGgAZQBuADIAMAAwADAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQ2hlbjIwMDAucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@inproceedings{Chen1998,
	Author = {J. Chen and C. Bouman and J. Dalton},
	Booktitle = {Proceedings of the SPIE Conference on Human Vision and Electronic Imaging III},
	Cited-By = {Rodden:1999ec, Platt:2000kx, Chen2000, Pecenovic:2000cr},
	Cites = {Rubner:1997dd, },
	Date-Added = {2009-02-02 20:06:14 +0000},
	Date-Modified = {2009-02-15 14:16:47 +0000},
	Keywords = {Similarity Pyrimid},
	Pages = {563-575},
	Title = {Similarity pyramids for browsing and organization of large image databases },
	Volume = {3299},
	Year = {1998},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvQ2hlbjE5OTgucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxDaGVuMTk5OC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTetAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkNoZW4xOTk4LnBkZgAOABoADABDAGgAZQBuADEAOQA5ADgALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQ2hlbjE5OTgucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@inproceedings{Barnard2001,
	Author = {K. Barnard and D. Forsyth},
	Booktitle = {Proceedings of IEEE International Conference on Computer Vision},
	Cites = {Cox:2000eh, },
	Date-Added = {2009-02-02 20:01:49 +0000},
	Date-Modified = {2009-02-15 16:53:44 +0000},
	Keywords = {Semantic System, Clustering},
	Pages = {408-415},
	Publisher = {IEEE},
	Title = {Learning the semantics of words and pictures},
	Volume = {2},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvQmFybmFyZDIwMDEucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9CYXJuYXJkMjAwMS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTe0AAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkJhcm5hcmQyMDAxLnBkZgAADgAgAA8AQgBhAHIAbgBhAHIAZAAyADAAMAAxAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0Jhcm5hcmQyMDAxLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@inproceedings{Krischnamachari1999,
	Annote = {This paper details the use of Colour histograms of images in order to create clusters of similar images which a user may browse through a hierarchical structure. Images are divided into regular sub-regions, for each of which a colour histogram is computed. 

Bottom-up (agglomerative) clustering is performed merging clusters with the shortest distance (most similar). The structure is a binary tree, so each parent node has 2 child nodes, l for the left, k for the right. Similarity between clusters is defined in terms of similarity between the images within the clusters. This means cluster centroids do not have to be re-calcluated after every merging of clusters.

Representative images are selected, dependent on a user specified number of representative images, from low level clusters. In the example given in the paper (3 representative images), from 3 lower clusters, the cluster with a single image is selected, and from 2 groups of images, the image most similar to all other images in that cluster are selected.

For testing, a database of 3856 images was used, with 25 representative images for each cluster. As the user traverses down the tree, images become more similar to the users interest. The average number of images in a database was 22, with the smallest being 5 and the largest being 80. Using a cluster histogram (the average of all images within the cluster) QBVE maybe performed on the database. 

Testing showed that in the top 10 images returned to the user, 90% retrieval accuracy can be achieved by performing 540 similarity comparisons as appose to an exhaustive search of all 3856 images.},
	Author = {S. Krischnamachari and M. Abdel-Mottaleb},
	Booktitle = {IEEE Symposium Computers and Communications},
	Cites = {Ballard:1991yq, Ma:1999gd},
	Date-Added = {2009-02-02 19:55:40 +0000},
	Date-Modified = {2009-02-20 15:48:34 +0000},
	Keywords = {Hierarchical Browsing},
	Pages = {301-307},
	Read = {Yes},
	Title = {Image browsing using hierarchical clustering},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEB5QYXBlcnMvS3Jpc2NobmFtYWNoYXJpMTk5OS5wZGbSGw8cHVdOUy5kYXRhTxEB3AAAAAAB3AACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTF0tyaXNjaG5hbWFjaGFyaTE5OTkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN7AAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAVU1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6S3Jpc2NobmFtYWNoYXJpMTk5OS5wZGYAAA4AMAAXAEsAcgBpAHMAYwBoAG4AYQBtAGEAYwBoAGEAcgBpADEAOQA5ADkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEhVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvS3Jpc2NobmFtYWNoYXJpMTk5OS5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAywDQANgCuAK6Ar8CyALTAtcC5QLsAvUC+gL9AAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAwo=}}

@conference{Green1993,
	Annote = {This paper adopts a 3D visualisation approach in order to display a OS
file structure. The paper states that the work of visualisating
non-numerical data (with number data being labelled in the field
"Scientific Visualisation") is up and coming. The authors uses transparent
cubes located inside 1 another in order to represent the hierachical
structure of a file system. The common problem often encountered in these
techniques "is that the screen space is too limited to display a large
tree structure.

The paper briefly discusses Robertson et. al and the use of cone trees,
and that the limitation of such an implementation is that it does not work
well for balanced heirachies or such structures with deep nesting.

In there transparent cube structure, top level data is represented on the
exterior of the cube, with a label representing the filename. The cubes
are semi-transparent so that the user can see what is inside the current
level. The work claims "Virtually no effort is required for users to
understand the meaning of the visualisation, because we are quite familiar
with the concept and the usage of a box - as a container - in our daily
lives".

An algorithm is used to pack the cubes. If the cubes were to be packed to
tightly, the user would struggle to see what was inside of the box. The
algorithm has 2 parts:
1) Recursively determine the size of each cube.
2) Calculate cubes position

In their implementation, a VR glove is used in order to rotate the box.
During user testing, they limited rotation to just the vertical axis in
order to reduce the navigation complexity.

A "focus" command was available to the user, which the authors state as
"equivilant to the Unix "cd" command". In order to present navigation
transitions more naturally user and to prevent confusion, the animation
slow-in-slow-out principle was employed.

The authors state that the experiments were conducted on a file directory
of 1500 files within 50 folders.},
	Author = {M. Green and J. Rekimoto},
	Booktitle = {Proceedings of the Third Annual Workshop on Information Technologies and Systems},
	Cited-By = {Robertson1991},
	Cites = {Robertson1991},
	Date-Added = {2009-02-02 19:48:22 +0000},
	Date-Modified = {2009-02-16 19:45:56 +0000},
	Keywords = {Visualisation Context},
	Pages = {125-132},
	Read = {Yes},
	Title = {The Information Cube: Using Transparency in 3D Visualisation},
	Year = {1993},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvR3JlZW4xOTkzLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNR3JlZW4xOTkzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03qwAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpHcmVlbjE5OTMucGRmAAAOABwADQBHAHIAZQBlAG4AMQA5ADkAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9HcmVlbjE5OTMucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Chen:2000ly,
	Author = {C. Chen and G. Gagaudakis and P. Rosin},
	Booktitle = {International Conference on Intelligent Information Processing},
	Cited-By = {Chen:2000gn},
	Cites = {Pentland:1996oz, Ballard:1991yq,},
	Date-Added = {2009-01-28 12:28:06 +0000},
	Date-Modified = {2009-02-16 19:19:24 +0000},
	Keywords = {Browsing, Pathfinder Network},
	Pages = {206-213},
	Title = {Similarity-based image browsing},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvQ2hlbjIwMDBseS5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkNoZW4yMDAwbHkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN1kAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Q2hlbjIwMDBseS5wZGYADgAeAA4AQwBoAGUAbgAyADAAMAAwAGwAeQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9DaGVuMjAwMGx5LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@conference{Rodden:2001cr,
	Annote = {In this work, Rodden performs a pair of user studies that compare different image system interfaces. Using MDS, images are arranged according to their similarity in some high dimensional feature space. In this work, this arrangement is fitted to a 2D grid in order to remove overlapping of images (as an earlier study of the auhors' had found a general dislike for such an arrangement).

Based on Borland and Ingwersens proposed "simulated work task situation" (applied by Jose et al. for image retrieval) the authors of this work devised a task where users were asked to select 3 images from 100 which best represented a travel article on some holiday destination. 100 images were chosen as it states that Markkula and Sormunen studied journalists who were on average willing to look through 100 images before re-formulating their queries.

In the system, 100 thumbnail images were shown, with the area under the mouse pointer magnified by 3x. If an image was selected, it was copied to an area in the bottom left of the screen. For the study, all user actions were logged and timed.

The first study was conducted at during the infodesign 99 conference. For this experiment, two interfaces were tested. The first was images based on similarity, the second was a grouping based on image caption. The users were tested on 4 places (1 used to instruct the user). The user could switch between the 2 arrangements. The user was instructed to find the 3 most suitable images, and hit "Done" when they felt they had completed the task. 66% of the participants preferred the caption based arrangement, with 63% opting to use that interface more often than the similarity based arrangement. 40/54 searches the caption based image was selected first.

It is noted by the authors that 11 of the 18 participants had prior experience to carrying out image selection, and these tended to prefer the visual arrangement. A critism made was that the captioning was insufficient, i.e. in the Kenya search, animals were not grouped as animals. 

For their 2nd experiment, the similarity based arrangement was compared with a random arrangement of images (named library and the participants told were arranged by chronological order). 10 graphic design students were asked to find the appropriate images for 6 places. Half of these were performed with each of the interfaces. After this, to reduce repetiveness, the testers issued set 1 of Raven's Advanced Progressive Matrices (APM) which is a culture free test of spatial reasoning evaluating the ability to think about abstract catagories. In the second part of the experiment was 3 more places, but the participants could select and switch between interfaces.

An interseting observation made by this experiment was that the participants took longer during the visual search than the random search, but were more satisfied with their image selection using the visual implementation.

The reason given for it being slower is that during a random arrangement, different images may stand out more from its neighbours, whilst when arranged by similarity, the area of the display is amde of very similar images thus making it difficult to select more salient images.

For the caption based arrangements in the first experiment, one user comment was that it is difficult to percieve how caption regions are arranged.},
	Author = {K. Rodden and W. Basalaj and D. Sinclair and K. Wood},
	Booktitle = {SIGCHI Conference on Human factors in Computing Systems},
	Cited-By = {Bederson:2001fv, Graham:2002zr},
	Cites = {Combs:1999ys, Rodden:2001fy, Rodden:1999ec},
	Date-Added = {2009-01-28 12:14:34 +0000},
	Date-Modified = {2009-02-15 17:04:19 +0000},
	Keywords = {Visualisation, User Study, Clustering},
	Pages = {190-197},
	Read = {Yes},
	Title = {Does Organisation by Similarity Assist Image Browsing?},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvUm9kZGVuMjAwMWNyLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQUm9kZGVuMjAwMWNyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03YQAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSb2RkZW4yMDAxY3IucGRmAA4AIgAQAFIAbwBkAGQAZQBuADIAMAAwADEAYwByAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1JvZGRlbjIwMDFjci5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Borth:2008ys,
	Annote = {This paper describes Navidgator, a balanced heirarchical browsing system designed to explore video stills. A demo of the wok is available at http://demo.iupr.org:8180/navidgator-tv 

The feature vector comprises of 8x8x8 colour histogram and Tamura texture features.

The distance matrix is created representing similarities between all pairs of keyframes. The distance measure used is L2. Once a new cluster is formed (from merged clusters) the distance table is updated. The paper discusses various linkage methods for the table, and concludes that their preferred method should group images by content, aswell as produce a balanced tree (dendogram). They explain and derive their own method, namely "balanced linkage" .

The GUI has a Zoom function. Zooming in reveals clusters of images similar to that selected whilst zooming out chooses less similar images. Due to the clustering technique, keyframes of subclusters are ordered in stripes according to their similarity.},
	Author = {Borth, D. and Schulze, C. and Ulges, A. and Breuel, T.},
	Date-Added = {2009-01-28 12:10:34 +0000},
	Date-Modified = {2009-03-12 12:54:44 +0000},
	Journal = {Proceedings of the 31st Annual German Conference on Advances in Artificial Intelligence},
	Keywords = {Hierarchical Browsing},
	Pages = {22-29},
	Read = {Yes},
	Title = {Navidgator - Similarity Based Browsing for Image and Video Databases},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvQm9ydGgyMDA4eXMucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9Cb3J0aDIwMDh5cy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTdWAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkJvcnRoMjAwOHlzLnBkZgAADgAgAA8AQgBvAHIAdABoADIAMAAwADgAeQBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0JvcnRoMjAwOHlzLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Nguyen:2008vn,
	Annote = {This work looks at the problems facing visualisation techniques and several proposals on how best to tackle these. The 3 main requirements are listed as:

1) Overview Requirement
The paper describes this problem as being the inability to present the entire image collection to the user, due to the lack of space on the screen. They mention past implementations of this, including the selection of a random subset of images to present to the user to browse and where images are represented as points, and are only shown to the user once a point is selected. The paper describes these methods as inadequate.

2) Structure Preservation Requirement
Here, the authors are describing the need for the relations of similarity between images in the high dimensional feature space to be retained in the lower 2D screen dimensions as best as possible. They list such methods as:

PCA(Principle Component Analysis), MDS(Multi Dimensional Scaling), 
or
(more recent methods) ISOMAP(ISOmetric MAPping)[19], SNE(Stochastic Neighbor Embedding)[7], LLE(Local Linear Embedding)[13].

3) The Visability requirement
In some of the mapping techniques, when images are applied to the points derived from mapping techniques as listed above, images may overlap and therefore be hidden from the user, which effectively makes the image lost from the database.

The authors note that these 3 main requirements are not all indendent of one another, for example the preservation of the inter image similarities in a reduced dimensional space will be affected by the movement of images in order to reduce overlapping (as part of the visability requirement).

The paper describes the mapping methods in more detail before settling on ISOSNE. This is a compination of ISOMAP and SNE. The paper briefly describes SNE as a technique which calculates the probilibilty of 2 neighbours being next to each other in the high and low dimensional spaces. For ISOMAP, 3 steps are taken:

1) A neighbourhood graph is made up of every point using k nearest neighbours.
2) The shortest path between every pair of points is made, and then the sum of all distances is calculated linking one point to the other.
3) MDS is applied

However in ISOSNE, the 3rd step is replaced by SNE.

For the overview requirement, k-means clustering is performed in order to select the most representitive images from the dataset.

For the visibility requirement, the authors proposed a technique whereby a circle is placed in the centre of the image (all images same size), and an overlap equation was formulated. should no part of 2 image circles overlap, a perfect score of 0 was assigned.

The remainder of the paper details the 2 main relationships between the 3 requirements:
1) The selection of representative images
The higher the selection, the better the representation. However this will cause reduced visibility of all the images.
2) The preservation of overall structure against the visibility of the images. Moving the images to make them more visible will effect the similarity structure  representation.

The paper proposes 2 balancing(cost) functions in order to best retain these relationships. The first, between representative images and visibilty will be performed offline, whilst the second will be performed online.

The work was shown to perform reasonably well on a database of 10,000 images. 

},
	Author = {G.P. Nguyen and M.Worring},
	Date-Added = {2009-01-28 12:00:33 +0000},
	Date-Modified = {2009-02-14 17:39:24 +0000},
	Journal = {Journal of Visual Languages and Computing},
	Keywords = {Visualisation Requirements, Visualisation},
	Pages = {203-224},
	Read = {Yes},
	Title = {Interactive access to large image collections using similarity based visualization},
	Volume = {19},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvTmd1eWVuMjAwOHZuLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQTmd1eWVuMjAwOHZuLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4KNcW8jTwAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAMW8jTwAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpOZ3V5ZW4yMDA4dm4ucGRmAA4AIgAQAE4AZwB1AHkAZQBuADIAMAAwADgAdgBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL05ndXllbjIwMDh2bi5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Muller:2001zl,
	Annote = {This paper explores the use of performance measures in the field of CBIR and the problems encountered by many researchers in the field. The paper states the main 3 problems as being:

a) No common image collection

Unlike IR which has a standard data collection for which to test systems (TREC), CBIR has no similar collection of images. One of the main issues is due to copyright issues, inhibiting the ability to readily distribute images that are not your own. For a new standard image set to be successful, the authors state the images should be freely available to download online, and have much diversity over many domains. Previos systems have used collections shuch as Corel (not free) or perhaps made their own collection (relatively small). In an IR system, the database is made up of well over 1,000,000 documents, whereas in CBIR, it is more like 100s or 1,000s.

b) Obtaining relevance judgements

The major difficulty here is that what one user may define as similar, another may not. Some image datasets are distributed with a ground truh file, pre-defining which images are similar. The production of such a file is extremely tedious and again is open to interpretation and confrontation. Other issues maybe that in specialist domains, such as in medcal imaging, one would require expert knowledge to define similarity which may not be readily available.

c) Performance Measures

No 1 standard CBIR system performance measure exists, yet many different measures have been developed. Some of these can be tailoured to improve the apparent quality of a system.

1) Rank of best match:
With a pre-defined best image, where in the returned results does this image reside? The paper states that the top 50 images are easier for visualisation, and that the user will probably only search through a maximum of 500 images.

2) Average rank of all relavent images
Again using a pre-defined specification of images deemed similar to the query images, calculate the average rank of where the images resides in the returned list. The authors state although this is a good indication of system performance, it is vunerable to outliers.

3) Precision and recall
These are the standard measures used in IR. One important note made by the authors is that these measures should be used in tandem, or with a specified number of images. For example, when all images in the databse are returned, recall = 1, and precision will be high when only a few results are returned.

4) Target Testing
Here a user is given a target image to find and the quality of the system is measured by how many images the user is required to sort through before finding the target.

5) Error Rate
Number of non relevant images retrieved/ Total retrieved

6) Retrieval Efficeincy
if No. retrieved > No. rel [No. rel im ret/Total ret]
else				      [No. rel im ret/total rel ret]

The paper states that the most common form of visualising performance data are precision/recall graphs. The authors note however that these should always be presented together and in full, to avoid misleading the reader into an improved system performance.

The work states 5 main proposals to combat these problems:

1) Only use freely available images, or if creating a bespoke database, make this freely available for other researchers.

2) All relevance judgements from these databases should also be made available, and from a variety of different people.

3) Measure presentation:

a) Rank(1) or Rank: rank at which 1st relevant image is retrieved, or normailsed average rank

b) P(20), P(50), P(Nr): precision after 20, 50 and Nr images retrieved

c) Rp(0.5) and R(100): recall at precision 0.5 and 100 images retrieved

d) Precision/Recall graph

4) They propose a new simple average ranking system:

RANK = 1/N*Nr(Sum Ri - Nr(Nr-1)/2)

5) For relevance feedback, all relevant images returned in first 20 images and evaluated over a minimum of 2 feedback steps. 

Furthermore, the authors state that another useful addition would be the execution times of the system, along with the specification of the computer on which the experiments were performed on.
 },
	Author = {H. M\"{u}ller and W. M\"{u}ller and D. M. Squire and S. Marchand-Maillet and T. Pun},
	Date-Added = {2009-01-21 11:01:38 +0000},
	Date-Modified = {2009-02-14 17:08:34 +0000},
	Journal = {Pattern Recognition Letters},
	Keywords = {Performance Measures},
	Number = {5},
	Pages = {593-601},
	Read = {Yes},
	Title = {Performance evaluation in content-based image retrieval: overview and proposals},
	Volume = {22},
	Year = {2001},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvTXVsbGVyMjAwMXpsLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQTXVsbGVyMjAwMXpsLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03ZgAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpNdWxsZXIyMDAxemwucGRmAA4AIgAQAE0AdQBsAGwAZQByADIAMAAwADEAegBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL011bGxlcjIwMDF6bC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@book{Chen:2004dw,
	Address = {London},
	Annote = {This book details mainly visualisation tecniques applied to IR methods. The disspapointing aspect from the point of view of image database visualisation is that most of the methods listed are applied to text/dcument retrieval, with little apparent scope on how these could be applied or extended to the field of research required. However the book does contain some interesting methologies that could be applied to the field of CBIR visualisation.

The first chapter of the book details geographical visualisation, and explains that it is a popular choice as it is easy for the human mind to comprehend such a representation. An idea from this section could be that, especially from a mobile computing angle (but not merely limited to this area) is the use of location metadata, for example GPS co-ordinates, in order to provide a visualisation to the user whereby images are superimposed over a map, dependent on where the photo was taken.

The book discusses a majority of the concepts covered in (Heesch, 2008) including  MDS, trajectory mapping (which is similar to MDS, but expresses explicit links rather than proximity when detailing similarities within the dataset), clustering and pathfinder networking (using a triangular inequality condition, if satisfied then links between nodes remain). An intersesting reference is made to (Morrison et al.,2003) in which it is claimed that an extremely fast MDS algorithm has been developed which can lay 108,000 items in just 360 seconds.

The book details many systems and algorithms available, but again these seem to be more tailoured to the text retrieval domain and appear mainly graph like in structure. Page 83 bears reference to a fast graph drawing algorithm, There were systems that did look particulary interesting however, including cone trees (a 3D representation of a heirachy) and a system named LyberWorld (described on page 90). This system uses a "relevance sphere" which places terms on the surface of the sphere, and the documents relevant to the terms are placed within the sphere. Documents deemed more relevant are placed nearer the surface of the system. },
	Author = {C. Chen},
	Date-Added = {2009-01-19 12:04:13 +0000},
	Date-Modified = {2009-02-14 18:00:19 +0000},
	Edition = {2nd},
	Keywords = {Visualisation},
	Publisher = {Springer-Verlag},
	Read = {Yes},
	Title = {Information Visualization},
	Year = {2004}}

@unpublished{Smith:2002ee,
	Annote = {This is a tutorial (available at www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) and describes the mathematics required in order to calculate the principal components of high dimensional data. The paper describes PCA as a technique with the ability to identify patterns in the data, and express in a way in order highlight similarities and distances in the data.

The tutorial starts by describing standard deviation (how far the data is spread from the mean), variance (another representation of this) and covariance (the relationship between 2 or more dimensions). From this matrix, the eigenvectors and respective eigenvalues are computed. These are vectors which remain unchanged in direction under transformation. The tutorial states that a computer program is best for this search, i.e. Matlab [a,b] = eig(matrix) will place the eigenvectors in a, eigenvalue in b from the given matrix. The eigenvalue is the scale factor of the eigenvector (although an eigenvector does not change in direction, it may change in length).

The main 5 steps as listed in this tutorial are:

1) Obtain the data 
2) Subtract each value in a dimension by the mean of that dimension (resulting in a mean of 0)
3) Calculate the covariance matrix
4) Calculate the eigenvectors and eigenvalues for the matrix, and normalise so that the length is equal to 1
List the eigenvectors in order of decreasing scale. Select the top n vectors dependent on the n dimensions you would like to reduce the data to.
5) Derive the new data set by multiplying eigenvector feature vector by original adjusted data.},
	Author = {L. Smith},
	Date-Added = {2009-01-19 11:05:28 +0000},
	Date-Modified = {2009-02-14 19:36:05 +0000},
	Keywords = {MDS, visualisation},
	Read = {Yes},
	Title = {A tutorial on Principal Components Analysis},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvU21pdGgyMDAyZWUucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9TbWl0aDIwMDJlZS5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTekAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlNtaXRoMjAwMmVlLnBkZgAADgAgAA8AUwBtAGkAdABoADIAMAAwADIAZQBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1NtaXRoMjAwMmVlLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@book{Kruskal:1978os,
	Annote = {This is an overview of MDS, taken from http://www.analytictech.com/borgatti/mds.htm. It takes text and figures directly from Kruskal's original work. The work details how a similarity matrix, made up of distances or indeed similarities, can be used by MDS to display a configuration of the data that best maintains the similarities/distances in a high dimensional space.

A quote from the paper is "The rule of thumb we use is that anything under 0.1 is excellent and anything over 0.15 is unacceptable" which is describing the range of acceptable values that should be generated by Kruskals STRESS formula.

The overview does not mention that there are indeed 2 diferet types of MDS, metric (where the similarity matrix contains metric values) and non-metric MDS. Non-metric requires a different STRESS formula. 

MDS is an iterative process, changing the position of the data to be displayed gradually until it reaches a threshold STRESS value or perhaps exceeds a certain number of predefined iterations.},
	Author = {Kruskal, J.B. and M. Wish},
	Cited-By = {Nakazato:2001rp},
	Date-Added = {2009-01-19 11:03:17 +0000},
	Date-Modified = {2009-02-15 16:41:53 +0000},
	Keywords = {MDS, Visualisation},
	Publisher = {Sage},
	Read = {Yes},
	Title = {Multidimensional Scaling},
	Year = {1978},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvS3J1c2thbDE5Nzhvcy5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUtydXNrYWwxOTc4b3MucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN2IAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6S3J1c2thbDE5Nzhvcy5wZGYAAA4AJAARAEsAcgB1AHMAawBhAGwAMQA5ADcAOABvAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvS3J1c2thbDE5Nzhvcy5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Ruszala:2004kk,
	Annote = {This paper outlines 6 visualisation methods, along with their advantages and disadvantages. The first is PCA, which the paper states is faster and more efficient than MDS as it uses just one iteration on the matrix of similarities between feature vectors to estimate the relative position of the location in the feature space.

This leads to MDS, which uses multiple iterations to reduce the Kruskals STRESS formula (taking into account euclidean distance and screen distance) until either a threshold stress value is reached or a preset number of iterations (which ever is first). However the paper conludes that this method is much more computationally expensive (quadratic) and that these calculations can not be performed for real-time browsing. The addition of images to the database will also require a complete re-calculation of the feature space.

FastMap appears to be the optimal combination of the 2. The FastMap algorithm (detailed in Faloutsos:1995ad) obtains features directly from the image (by supplying the distance fucntion). This algorithm uses linear mapping. Two images are selected as pivot points (1 arbiturary image and it's furthest possible neighbour) and a line mapped between them. The algorithm continues in this fashion till all images have been mapped to lones. This method is much faster than MDS (linear complexity) and produces accuracy not significantly less than MDS. One disadvantage with this method listed is that occasionally approximations can be significantly different fro an acceptable value.

The following section of the paper details a system picSOM, which uses SOMs (Self Organizing Maps) to construct a browsable tree. The apparent novelty of this system is the relevance feedback module which rearranges the similarity of images based on the users interactions with the system. 

MARS 3D (Nakazato:2001rp) is described in the paper as a novel fully immersive system whereby a user interacts with four screens and a wand. One initial downfall of the system is the expensive hardware not available to the average user. Two methods for dataset exploration exist. The first is general browsing, whereby images are placed randomly around the screen. The second, after a query, images are placed using the 3 axis for red, green and blue averages. The paper comments that a better initial arrangement is required in order to expand the image databse to a set say 500,000 images in size.

The final system covered in this review is that of a hierarchical approach as detailed in Krischnamachari1999. Disadvantages listed of the heirarchical approach is that images placed wrongly in the tree will be lost to the user, and that perhaps the 1D scrolling of images within a cluster will not scale well to large image datasets.

The paper concludes that for 2D displays, PCA/MDS/FastMap appera the way forward, with FastMap being the preferred option unless accuracy of the feature space representation is important, whereby MDS is better suited.},
	Author = {Ruszala, S. and Schaefer, G.},
	Booktitle = {Irish Machine Vision and Image Processing Conference},
	Date-Added = {2009-01-19 10:34:53 +0000},
	Date-Modified = {2009-03-09 13:14:38 +0000},
	Keywords = {Visualisation, Browsing Survey, MDS},
	Pages = {186-191},
	Read = {Yes},
	Title = {Visualisation models for image databases: A comparison of six approaches},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvUnVzemFsYTIwMDRray5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEVJ1c3phbGEyMDA0a2sucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN54AAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6UnVzemFsYTIwMDRray5wZGYAAA4AJAARAFIAdQBzAHoAYQBsAGEAMgAwADAANABrAGsALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvUnVzemFsYTIwMDRray5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Cox:1992oq,
	Annote = {This paper is an early example in favour of browsing rather than explicit query formulation. Cox compares the browsing model with the more conventional query model. One of the main advantages stated in this paper is that at no point during the browsing process is a subset of images isolated, i.e. during a query, only a selection of images from the database will be returned for the user to search through. Therefore if the correct image is not selected, the user will be unable to obtain it.

Cox supports the idea of using the prowess of the human cognitive system as a means of filtering a collection of images. In able to implement this specifically, Cox specifies certain guidelines that should be adhered to, based upon HCI principles.

Cox presents a simple Information Retrieval system based on figures made up of shapes (rectangles, circles etc.). With this system, the user is able to select either entire figures or individual shapes in order to save them. An operation 'similar' can then be conducted (with parameters such as size, position of shapes in the figure) in order to retrieve similar figures or shapes from the database. The interface also gives the user a global viewof the database, with a pointer relative to their current location in the database.},
	Author = {K. Cox},
	Booktitle = {Proceedings of The Fifth International Conference on New Information Technology},
	Date-Added = {2009-01-08 14:26:01 +0000},
	Date-Modified = {2009-02-15 16:44:52 +0000},
	Keywords = {Browsing, Visualisation Requirements},
	Pages = {69-80},
	Read = {Yes},
	Title = {Information retrieval by browsing},
	Year = {1992},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvQ294MTk5Mm9xLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNQ294MTk5Mm9xLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03WgAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpDb3gxOTkyb3EucGRmAAAOABwADQBDAG8AeAAxADkAOQAyAG8AcQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9Db3gxOTkyb3EucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Pecenovic:2000cr,
	Annote = {This work starts by discussing the two principle search paradigms of direct search and browsing. In the latter, a user is shown the entire image collection and can ``zoom in'' on desired images or areas of similar images. Images gathered during browsing can be used as query images. Due to the power of human perception, Images can be judged as relevant instantly.

For the direct search paradigm, a user may not have full idea of the search or may become ``trapped'' in subset of undesirable images.

A quote taken directy from the work is:

``Experiments show that the users preferred to view retrieved results in 2-D maps rather than in ranked lists by similarity in a ``reading order''{\ldots}{\ldots}''

Unfortunatley this is not backed up by a reference, but could be used as a premise for an experiment. In this paper, the author descibes the developed CIRCUS (Content-based Image Retrieval and Consultation User System). This system implements both direct and browsing modes. All the feature extraction and the construction of the browsing map is performed off-line. The features extracted from each image were colour histograms and moments, texture features and shape features (via wavelet values). The
L2 distance was used to estimate similarity between images
 
For the mapping, Sammon's Projection. The paper lists the other available methods (PCA, MDS etc.) but opts for Sammon's as: ``The SP is an non-linear projection method and is shown to be more adaptive to complex data sets so it is chosen in our system''
Much like MDS, the technique reduces dimensionality by minimizing an error term (much like MDS). An issue stated with these mapping techniques is the addition of images to DB requires an entire re-run of algorithm. A reference is listed which demonstrates a possible solution in using BPNN. The paper notes that PCA was first applied to give initial co-ordinates as this is faster and more efficient.

Images are clustered hierarchically with each cluster having a representative image.
The clustering performed via k-means using pre-defined cluster sizes. For querying the database CIRCUS uses QBVE, Keyword, Colour and QBSE. An interesting feature of the QBSE is that an image previously in the database can be modified by the user. The example given is a pair of pliers with a red handle, user can colour handles green and submit as a query.

For the navigation, a user can navigate over all the clusters, then enter cluster s/he finds of interest.Using the multiple search paradigms, some images may be filtered out from the overall browsing view. For example, the keyword `horse' will gray out various non-relevant images and leave only animal/horse related images.

Unfortunatley, although testing is mentioned at the end of the work, no full scale usability test has been conducted on the system other than the monitoring of a small collection of novice and occasional users.},
	Author = {Z. Pecenovic and M. Do and M. Vetterli and P. Pu },
	Booktitle = {International Conference on Advances in Visual Information Systems},
	Cited-By = {Nakazato:2001rp},
	Cites = {Chen1998, Orengo:1995rz, Ballard:1991yq},
	Date-Added = {2009-01-08 14:13:52 +0000},
	Date-Modified = {2009-02-15 16:43:44 +0000},
	Keywords = {Visualisation, MDS, Browsing},
	Pages = {279-289},
	Read = {Yes},
	Title = {Integrated browsing and searching of large image collections},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvUGVjZW5vdmljMjAwMGNyLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTUGVjZW5vdmljMjAwMGNyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03nAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpQZWNlbm92aWMyMDAwY3IucGRmAAAOACgAEwBQAGUAYwBlAG4AbwB2AGkAYwAyADAAMAAwAGMAcgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9QZWNlbm92aWMyMDAwY3IucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@article{Heesch:2008rz,
	Annote = {Heesch outlines some of the past uses of High Dimensional data visualisation techniques and how they have been applied to CBIR. His work draws upon 100 references, of which some are of his own work. As with (Cox, 1992) Heesch supports a browsing, rather than query based model. His arguments include (amongst others) users being unable to express query sufficiently (i.e. not 100% sure what it is they are searching for) and exploiting the power of the human mind (fast recognition).  

The techniques Heesch focusses on first are hierachies, in particualr clustered hierachies whereby a cluster is represented by a parent image which is said to be the most representative for that cluster (or clusters). Heesch specifies the potential downfall in such a structure is if the user is unable to reliable predict where a particular image should reside in a tree. Here a user can traverse the tree in completely the wrong direction, this making the required image impossible to reach. He suggests a possible solution to this (Hard Clustering) as Fuzzy clustering, in which it is possible for an image to belong to several clusters. Heesch describes Agglomerative (merging of clusters) and divisive (dividing of clusters) clustering, along with fuzzy clustering (image assigned to path of tree with highest probability) in some detail.

The following part of the paper discusses static network structures, in particular nearest neighbour networks, Pathfinder networks (whereby an algorithm is used to find only the most accurate (most efficient) path between to related nodes) and NN^k networks. It is apparent from the references that Heesch himself has done much work into NN^k networks, whereby the disatnce measure is specified in terms of feature specific weights, rather than a ranking to another image. This allows the user more freedom in choosing an image similar to the 1 currently selected.

Heesch also mentions obstensive browsing, which he describes as a dynamic structure due to the fact these methods incorporate relevance feedback. The discussion chapter of the work contains a table with the referenced works compared along with the size of the image collection used and also the complexity of the system on and offline. },
	Author = {D. Heesch},
	Date-Added = {2009-01-06 14:58:59 +0000},
	Date-Modified = {2009-02-14 17:39:08 +0000},
	Journal = {Multimedia Tools and Applications},
	Keywords = {Visualisation, Browsing Survey},
	Number = {2},
	Pages = {261-284},
	Read = {Yes},
	Title = {A survey of browsing models for content based image retrieval},
	Volume = {40},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvSGVlc2NoMjAwOHJ6LnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQSGVlc2NoMjAwOHJ6LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03XwAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpIZWVzY2gyMDA4cnoucGRmAA4AIgAQAEgAZQBlAHMAYwBoADIAMAAwADgAcgB6AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0hlZXNjaDIwMDhyei5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Liu:2004ml,
	Annote = {This paper is of particular interest in the field of visualisation in CBIR. The work begins by proposing a similarity metric whereby the basis is that objects (known in the paper as attention objects) in an image are assigned some 'attention value' (AV) and a 'region of interest' (ROI). The AV corresponds to the weight (or fraction of image) of the object, whereas the ROI is the spatial region object corresponds to. 

3 different measures were used:

a) An image similarity matrix
b) A region similarity matrix (similarity between attention regions)
c) A combnation of the 2

The remainder of the paper is a 10 person user study based on the visualisation of results returned by Google Image Search. 3 interfaces were tested against the control (the standard inteface):

a) List
b) MDS assigned to a grid like structure (with the paper detailing the algorithm and subsequent complexity of the algorithm)
c) Clustered approach where similar images are grouped together, and the user can select a preview thumbnail which shows 4 images most reresentative of that cluster). 

The users also had access to 3 tools to aid in the navigation. These were:
a) "Fish Eye View" whereby the central (or focussed) image is made cleares as the surrounding ones are distorted
b) Cropped Thumbnails
c) Slider to change image size and distance

The slider was scored as the most useful to the users. 

The users were asked to perform 17 queiries on 3400 JPEG images. Results showed that the Similarity search and Clustered searches both outperformed the standard list approach.
},
	Author = {Liu, H. and Xie, X. and Tang, X. and Li, Z. W.  and Ma, W. Y.},
	Booktitle = {6th ACM SIGMM International Workshop on Multimedia Information Retrieval},
	Date-Added = {2008-11-28 12:17:44 +0000},
	Date-Modified = {2009-02-14 17:25:06 +0000},
	Keywords = {Visualisation, MDS, User Study},
	Pages = {84-90},
	Read = {Yes},
	Title = {Effective browsing of web image search results},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvTGl1MjAwNG1sLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNTGl1MjAwNG1sLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03ZAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpMaXUyMDA0bWwucGRmAAAOABwADQBMAGkAdQAyADAAMAA0AG0AbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9MaXUyMDA0bWwucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Rubner:1997dd,
	Annote = {This is a fundamental piece of work in this research field. Not only does this paper define the Earth Movers Distance as a potential distance measure, it also introduces the use of MDS in displaying. In this work the image is converted to the CIE-LAB colour space. The reason given for this is that the Euclidean distances between colours are defined as a good representation of the manner in which human's percieve colour.

To index the images, K-d tree clustering was performed. Agglomerative clustering was applied to merge clusters that were deemed close together. 

Each cluster can be represented as (P, Wp) where P is the average colour and Wp is the weight, or fraction of the image. The paper notes that typically an image would contain 8 - 12 clusters. The idea is that the points are strongly correlated with human perception.

For the distance measure, EMD is described as "the amount of work required to transform 1 signature into another". Here, work is defined as the portion of weight being moved * euclidean distance between the old and new locations.

Using 2 signatures, the key is to discover a matrix (Cij) of weights which can minimize the amount of work (Cij * euclidean distance). The paper states the ability for EMD to be treated as a metric (if total weights in both signatures are equal) and it's efficient calculation as being major advantages to implementing this distance measure.

The remainder of the paper presents MDS as a manner of visualising the query results.},
	Author = {Rubner, Y.  and Guibas, L. J.  and Tomasi, C.},
	Booktitle = {APRA Image Understanding Workshop},
	Cited-By = {Chen1998, Rodden:1999ec, Ma:1999gd, Chen2000, Nakazato:2001rp, },
	Cites = {Orengo:1995rz, Pentland:1996oz, Ballard:1991yq},
	Date-Modified = {2009-02-15 16:43:05 +0000},
	Keywords = {Distances, EMD, MDS},
	Pages = {661-668},
	Read = {Yes},
	Title = {The earth movers distance, multi-dimensional scaling, and color-based image retrieval.},
	Year = {1997},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvUnVibmVyMTk5N2RkLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQUnVibmVyMTk5N2RkLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03nQAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpSdWJuZXIxOTk3ZGQucGRmAA4AIgAQAFIAdQBiAG4AZQByADEAOQA5ADcAZABkAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1J1Ym5lcjE5OTdkZC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Fockler:2005ek,
	Address = {Christchurch, New Zealand},
	Annote = {This paper introduces a simple ob ject recognition approach using a trained Neural Network. The novelty of this system is that unlike the implementations in (Sarvas et al.,2004), (Yeh et al., 2004) and (Hare & Lewis, 2005), the system requires no communication with a server, thus reducing data call costs and delays caused by heavy networks traffic. Using normalised colour features, the paper claims to be able to recognise an ob ject from a total of 50, from various perspectives, using less than 6Kb of data, being an obvious advantage when memory is at a premium. 

The authors comment that their system improve on previous approaches, including systems where exhibitors key in a code on their phone to retrieve audio and text information, and another whereby IR sensors are used to gauge the location of the user. For each object in the museum, multiple feature vectors are associated with it (views from different perspectives). 

There were two possible match strategies; closest neighbor match strategy and a linear separation strategy. For the closest neighbor search, the image is determined from previously taken ones which come closest to the new image, based on their feature vector. For the linear strategy, a single layer neural network is implemented on the mobile phone. The training of the NN compresses all the feature vectors belonging to that ob ject into a single set of normalised weights. A weight vector is assigned to a single ob ject, and serves as a fingerprint for recognising the same object in different images. The weight vector has the same number of dimensions as the feature vector. 

In the NN, each object has it's own Neuron. The feature vector consisted of 21 dimensions, relating to the colour and intensity, aswell as the structure. For the NN training, each Neuron input channel is assigned a weight component from the corresponding weight vector. These values were initialised between 0 and 1. For recognition of an object, a new image must be taken and the feature vector computed. The object is recognised by finding the perceptron with the maximal output excitation over all perceptrons. If recognition fails, the NN must be trained by amplifying the weights of the perceptron that should have been activated. When an object is recognised, appropriate metadata is returned to the user. 

I like the idea of this system. The success rate is over 90% of the system recognising the object in the image. The system is entirely on the phone and is lightweight. One concern would be the system is only tested up to 50 different objects. With the use of external memory, this could easily be increased but the recognition rate may be hampered. 
},
	Author = {Paul F{\"o}ckler and Thomas Zeidler and Oliver Bimber },
	Booktitle = {Proceedings of the 4th international conference on Mobile and ubiquitous multimedia},
	Date-Added = {2008-11-24 17:35:42 +0000},
	Date-Modified = {2009-01-21 10:02:49 +0000},
	Keywords = {Mobile CBIR},
	Pages = {3-10},
	Read = {Yes},
	Title = {PhoneGuide: Museum Guidance Supported by On Device Object Recognition on Mobile Phones },
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvRm9ja2xlcjIwMDVlay5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUZvY2tsZXIyMDA1ZWsucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN10AAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6Rm9ja2xlcjIwMDVlay5wZGYAAA4AJAARAEYAbwBjAGsAbABlAHIAMgAwADAANQBlAGsALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvRm9ja2xlcjIwMDVlay5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@inproceedings{Weijer:2005uo,
	Annote = {This paper describes a proposed new model for a Colour Constancy algorithm. CC is required by colour histograms to be more successful, according to (Swain & Ballard, 1991). The paper describes colour CC as "....the ability to recognise colours of objects invariant to the colour of the light source". The authors describe such an algorithm as a 2 step process, first to estimate the colour of the light source and secondly to output the image with unaffected lighting.

The paper briefly describes the previous main two methods:

1) Max-RGB 
Estimates the light source from the maximum response of the different colour channels

2) Grey-World
Assumes the average reflectance in the scene to be achromatic (free from colour)

The paper states that although there have been other methods developed, these are the most popular due to their low computational cost. The paper references a recent study demonstrating how these 2 methods can be considered the same algorithm if the L1 or Minkowski norm (L infinity) are used in calculation of the error function.

Here a new method is described, named Grey-Edge. Here, it is assumed that the average edge difference in the scene is achromatic. The paper explains the calculation and that during their experiments, it outperformed the previous 2 implementations. },
	Author = {J. Weijer and T. Gevers},
	Booktitle = {Proceedings of IEEE International Conference on Image Processing (ICIP)},
	Date-Added = {2008-11-24 17:30:36 +0000},
	Date-Modified = {2009-02-14 19:36:32 +0000},
	Keywords = {Colour Constancy},
	Read = {Yes},
	Title = {Color Constancy based on the Grey-Edge Hypothesis},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvV2VpamVyMjAwNXVvLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQV2VpamVyMjAwNXVvLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03pwAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpXZWlqZXIyMDA1dW8ucGRmAA4AIgAQAFcAZQBpAGoAZQByADIAMAAwADUAdQBvAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1dlaWplcjIwMDV1by5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Sarvas:2004dn,
	Annote = {This paper details the creation of the MMM System (Mobile Media Metadata). The paper states that although metadata provides semantically high level information, most users are reluctant to add it as it too time consuming and tedious. Here the authors have devised a system that automatically leverages where an image is taken, when it was taken and who is in the picture. The creators of the system stress the importance of adding such metadata when the image is taken, else the information may be forgotten by the user. This system also uses the Nokia 3650, as used in (Yeh et al., 2004) and (Yeh et al. 2005). An interesting note is that it uses the Nokia image upload API. The data transfer between client and server uses HTTP over the GPRS network. 

Here is a use case scenario. An image of a tower is taken by a user. The location, time, date and user name is stored and uploaded to the server along with the query image. The server processes the image and metadata to find similarities between these and previous images taken. The server suggests relevant metadata and an accuracy estimate, for example: ``Location City: Berkeley (100%), Day: Thursday (100%), Picture: Camomile Tower(62%), Picture Taken: Outside (86%)''. The data which the system is not 100% sure on is presented to the user to validate. Once validated by the user, the system returns all the relevant contextual information. The authors list the 1 button press 
confirmation as a way of counteracting tedious feedback loops. 

The information returned as 100% in the example, is extracted from low level information. For example the location can be calculated using the GPS co-ordinates, or 
the time can be extracted from the phone's system clock. Other possible data that can be extrapolated at a relatively low level include using nearby users (i.e. data extracted from bluetooth connectivity), the username of the user etc. 

The authors also state that as phones become more and more advanced, features such as a calender could provide more information. For example if a meeting is scheduled with somebody at a time and date, and an photo is taken by the users phone at that time and date with a person in it, then chances they are that person! 

On the server side of the implementation, two different algorithms are used. The first is a person guessing algorithm. Here the times of images and the number of occurrences a username has with a particular ob ject recurring in images can be used to guess who is in the image. The second is a location guessing algorithm, and again uses time similarities and common user occurrences to derive a location. 

The MMM system is more like a social interaction application than a true CBIR. There appear to be many flaws with this implementation. First, it is difficult to see how this could possibly reach a commercial market. Personally, I would not like my name and personal images shared with other users of the system. Such a system would be better integrated into a social networking site, such as Facebook, Bebo etc. as these sites already have an established base of users regularly uploading images. An association with such an organization would allow the users to choose privacy settings, and would also be able to harness the ability to see if friends appeared in their images.},
	Author = {R. Sarvas and E. Herrarte and A. Wilhelm and M. Davis},
	Booktitle = {Proceedings of the 2nd International Conference on Mobile Systems, Applications, and Services},
	Date-Added = {2008-11-24 17:25:00 +0000},
	Date-Modified = {2009-02-14 17:26:02 +0000},
	Keywords = {Mobile CBIR},
	Pages = {36-48  },
	Read = {Yes},
	Title = {Metadata Creation System for Mobile Images },
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvU2FydmFzMjAwNGRuLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQU2FydmFzMjAwNGRuLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03oAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpTYXJ2YXMyMDA0ZG4ucGRmAA4AIgAQAFMAYQByAHYAYQBzADIAMAAwADQAZABuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1NhcnZhczIwMDRkbi5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Yeh:2004rc,
	Annote = {This paper describes a system which uses a hybrid of image and keyword searching paradigm. The novelty of this system is that unlike systems such as that presented in (Hare et al., 2005) for the recognition of the location, the system is not required to have been at that location before. As the system uses Google image search, 425 million images can be searched without having to apply image content algorithms to each and every image.

A query image taken with a mobile phone, is compared with images in a bootstrap image database. Each image in the database is linked to a web page. If the query image is similar to that in the database, the keywords from within the web page are extracted and used to form a text based search in a Google query. The images retrieved by the Google query (with access to a quoted 425 million images) are then filtered so they remain as close to the original query image as possible. The authors state that as many images exist of landmarks and prominent locations on the web, these may be used to provide a reasonable location cue.

2 image matching metrics were used for the refinement of the images Google returned. First was the energy spectrum of the image (the squared magnitude of the windowed fourier transform). The authors list the invariance to object arrangement and identity in support of using such a technique. The second technique used was Wavelet Decompositions. Here, steerable filters over 2 scales on the greyscale of the image were used. 6 different angles were used resulting in 12 sub-bands. Using only the local, mean values of magnitude of the local features averaged over large windows resulted in 4*4*12 = 96 dimensions. Nearest neighbor search was conducted for the bootstrap database. 

In order to reduce the dimensionality, only the first 32 principal components were used to form the feature vector of each image. For the text extraction from the web page, tf-idf weighting was used. Images that were similar from the Google search were clustered together. 

Unfortunately it is unclear which of the 2 image matching algorithms performed best, as the graph is in black and white and the key must originally been in colour. The results of the experiments show that similar images appear in the top 16 images of their results. This system is successfully implemented on the Nokia 3650 phone.

There are however some unexplained elements to this paper. Firstly, despite using a bootstrap database, surely not every major location will be covered here, thus making the Google search impossible (i.e. no keywords to query). Therefore only those images will have been searched, not the 425 million. Also it is unclear what results are returned to the user. One would imagine a textual description would be sought i.e. if taking a picture of the Liberty Statue, ``You are in New York, looking at the Liberty Statue". Perhaps the system returns the appropriate web page, but the description is too vague.},
	Author = {Yeh, T. and Tollmar, K. and Darrell, T.},
	Booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	Date-Added = {2008-11-24 17:16:56 +0000},
	Date-Modified = {2009-02-14 17:28:03 +0000},
	Keywords = {Mobile CBIR},
	Pages = {76-81},
	Read = {Yes},
	Title = {Searching the Web with Mobile Images for Location Recognition },
	Volume = {2},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvWWVoMjAwNHJjLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNWWVoMjAwNHJjLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03qAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpZZWgyMDA0cmMucGRmAAAOABwADQBZAGUAaAAyADAAMAA0AHIAYwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9ZZWgyMDA0cmMucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Yeh:2005sj,
	Annote = {This paper uses a mobile phone camera in order to recognise an object and return an appropriate web page (CBIR during user testing). The novelty of this paper appears to be a new search paradigm for a mobile device. Rather than awkward text and number entry, the user can pass an image to the search engine. For this, the user takes 2 images; one with the required object, 1 without. The system devised then uses a simple computer vision technique for foreground/background estimation.

For the testing, they gave users 2 search paradigms. The first was to simply trace round the object using the mobile joystick, the other was to use their 2 shot approach. For the purpose of testing, the database of images used was online shop images. The user study found their 2 shot approach to be quicker, and therefore easier for the users to operate. They state that this is particularly useful as the object to be compared will be the only one present in the shops image. Of course this caused the authors of the paper to cite their system as an online shopping aid. 

A major downfall of this particular method is that immovable objects, (for example buildings, statues, shop window displays) would not be able to use the 2 shot method.},
	Author = {Yeh, T. and Grauman, K. and Tollmar, K. and Darrell, T.},
	Booktitle = {Extended abstracts on Human Factors in Computing Systems},
	Date-Added = {2008-11-24 17:13:39 +0000},
	Date-Modified = {2009-02-14 20:59:53 +0000},
	Keywords = {Mobile CBIR},
	Pages = {2025-2028},
	Publisher = {ACM},
	Read = {Yes},
	Title = {A picture is worth a thousand keywords: image-based object search on a mobile platform},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBRQYXBlcnMvWWVoMjAwNXNqLnBkZtIbDxwdV05TLmRhdGFPEQG0AAAAAAG0AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MNWWVoMjAwNXNqLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03qQAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBLTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpZZWgyMDA1c2oucGRmAAAOABwADQBZAGUAaAAyADAAMAA1AHMAagAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPlVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9ZZWgyMDA1c2oucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMEAxgDOAoYCiAKNApYCoQKlArMCugLDAsgCywAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALY}}

@inproceedings{Hare:2005rz,
	Annote = {This paper implements a CBIR system on a Compaq IPAQ PDA device, attached to a digital camera. The aim is to use this as a guide for viewing art in a gallery. The novelty of this paper relates to the quality of image retrieval when the query image is distorted or particularly noisy (as is usually the case with images taken with mobile cameras). The system devised here uses a client server implementation. The server side of the system uses Lowes SIFT algorithm as it is a feature descriptor that works well in poor conditions, and is designed to be invariant to small shifts in the position of the salient region. However due to the complexity of SIFT (128 dimensions per salient feature), an indexing feature common to text retrieval was used. 

First, the vector features were quantisised into a known set a feature sets. This forms a sort of known vocabulary, which is known as ``Stemming" in text retrieval (words broken down i.e. connected would become connect). To form this vocabulary of images, k-means clustering was applied to a sample set of images, in which the centroids of the clusters became the words. Every image in the DB was the quantisised by these words.

For each word (from now on salient feature) a weighting is applied to the feature. This is known as (tf-idf) ``Term frequency-inverse document frequency". The same system is used by Google, whereby the weight of a web page is the number of links to that page. The word frequency increases the weight of the words occurring often in the document, whereas the inverse down-weights the words occurring often in the database.

The results are ranked by the normalised scalar product (cosine of the angle). The top N results based on geometric consistency are returned. Another algorithm, named RANSAC, re-ranks images based on consistency between those returned. The highest ranked result is returned with the appropriate meta-data of the art photographed.},
	Author = {Hare, J. S. and Lewis, P. H.},
	Booktitle = {Storage and Retrieval Methods and Applications for Multimedia},
	Date-Added = {2008-11-24 17:08:54 +0000},
	Date-Modified = {2009-02-14 19:39:15 +0000},
	Keywords = {Mobile CBIR},
	Read = {Yes},
	Title = {Content-based image retrieval using a mobile device as a novel interface},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvSGFyZTIwMDVyei5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDkhhcmUyMDA1cnoucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN14AAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6SGFyZTIwMDVyei5wZGYADgAeAA4ASABhAHIAZQAyADAAMAA1AHIAegAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9IYXJlMjAwNXJ6LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@article{Faloutsos:1994lq,
	Annote = {QBIC is a famous CBIR system developed by IBM in the early 1990's, with the authors of the work describing the system as "an information filter" rather than a CBIR with a full semantic comprehnsion of the images contianed within the database as it was said to be beyond the current capability of computer vision.

The system offers the user QBVE, aswell as QBS and the ability to specify colours in certain locations or % of overall image, select from a colour palette.

For the image feature vector, colour shape and texture features were combined. For the colour features, a K-element colour histogram was computed for each object and scene. K was either 256 or 64 and these were tested in representing both objects in an image and the overall image. The distance measure between the colour histograms was oe of the most revolutionary aspects of this system. Here the L2, or euclidean distance was used. For the texture and shape features, these were weighted euclidean distance, with the weight dependent on the importance of the vector being measures. 

For the distance between colour histograms, an identity matrix is used along side the L2 metric. The identity matrix is used to specify colour similarity. The advantage of this is that with unlike conventional histograms, (such as with Swain & Ballard) if a histogram predominantly made up with a Red colour, is compared with a colour histogram from a image predominantly made up of an Orange colour, there is a possibility they will not fall in the same bin and the distance between the histograms will be greater than it should be. With QBIC however, the identity matrix will reflect a similarity between the colours and therefore there is a greater chance that the images will be matched.},
	Author = {C. Faloutsos and W. Equitz and M. Flickner and W. Niblack and D. Petkovic and R. Barber},
	Cited-By = {Orengo:1995rz, Jacobs:1995bh, Zhang1995, Kurniawati1997, Sebe:1998rz, },
	Cites = {Ballard:1991yq},
	Date-Added = {2008-11-18 15:19:01 +0000},
	Date-Modified = {2009-02-15 11:37:27 +0000},
	Journal = {Journal of Intelligent Information Systems},
	Keywords = {QBIC, CBIR System},
	Pages = {231-262},
	Read = {Yes},
	Title = {Efficient and effective querying by image content},
	Volume = {3},
	Year = {1994},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvRmFsb3V0c29zMTk5NGxxLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTRmFsb3V0c29zMTk5NGxxLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03XAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpGYWxvdXRzb3MxOTk0bHEucGRmAAAOACgAEwBGAGEAbABvAHUAdABzAG8AcwAxADkAOQA0AGwAcQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9GYWxvdXRzb3MxOTk0bHEucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@article{Santini:2000fv,
	Annote = {This is work undertaken at the University of California, San Diego. The principal concept in this paper is the belief that the semantic gap can be crosses by replacing the query process "with a more active exploration process". The system, named El Nino, is the collective name for a group of tools relating to their CBIR. 

The system uses MDS to display images to the user. A relevance feedback mechanism allows the user to manipulate the position of the images on the screen, based on their own definition of similarity. This involves redefining the internal similarity measure. The system presents between 100 and 300 images to the user. The reorganization based on the new distances proposed by the use will cause some images to be removed from the display.

Aswell as user input for similarity configuration, two other key tools are used. Firstly a visual concept. This is a set of images that the user defines as equivilent. The second is a visual dictionary, whereby a subset of the images are labelled, allowing for QBK to be invoked by the user. Of course due to features in the image, similar images will be returned.

This system attempts to add contextual information to the images, aswell as allowing the user to browse sub sections of the database and modify the distances in order to re-adjust the weighting in the similarity function.},
	Author = {S. Santini and R. Jain},
	Cited-By = {Nakazato:2001rp, Smeulders:2000zl},
	Cites = {Orengo:1995rz},
	Date-Added = {2008-11-18 15:16:00 +0000},
	Date-Modified = {2009-02-15 16:42:45 +0000},
	Journal = {IEEE Multimedia},
	Keywords = {Relevance Feedback, MDS, Visualisation},
	Pages = {26-39},
	Read = {Yes},
	Title = {Integrated Browsing and Querying for Image Databases},
	Volume = {7},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvU2FudGluaTIwMDBmdi5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEVNhbnRpbmkyMDAwZnYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN58AAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6U2FudGluaTIwMDBmdi5wZGYAAA4AJAARAFMAYQBuAHQAaQBuAGkAMgAwADAAMABmAHYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvU2FudGluaTIwMDBmdi5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@article{Zhou:2003zl,
	Author = {X. Zhou and T. Huang},
	Date-Added = {2008-11-18 15:11:00 +0000},
	Date-Modified = {2009-02-14 18:12:42 +0000},
	Journal = {Multimedia Systems},
	Keywords = {Relevance Feedback},
	Number = {6},
	Pages = {536-544},
	Read = {No},
	Title = {Relevance feedback in image retrieval: A comprehensive review },
	Volume = {8},
	Year = {2003},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvWmhvdTIwMDN6bC5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDlpob3UyMDAzemwucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN6oAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6WmhvdTIwMDN6bC5wZGYADgAeAA4AWgBoAG8AdQAyADAAMAAzAHoAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9aaG91MjAwM3psLnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@article{Moghaddam:2004rz,
	Annote = {This paper details the theory behind the personal digital historian (PDH)
project. The aim is to "help people construct, organize, navigate and
share digital collections in an interactive multi-person conversational
setting". They want the system to enable natural face to face
converstaion, easy and fun to use interactively, and to remove the need
for a computer. The system uses a touch screen circular table top display.
It is implemented in DiamondSpinm a circular table java toolkit.

The ambition of the project is "to develop conent organization and
retrieval metaphors that can be easily understandable by users without
distracting from the conversation. The main query types are who, what,
where and when. The control panel includes buttons enabling these queries.
When a user selects "where", the display will switch to a map of the
world.The images which include an annotation of the location will appear
as a small thumbnail based on the location text.


One major issue with this system is that it assumes all images in the
database are annotated.

The user can formulate boolean queries using the 4 control panel W's. The
work presented in this paper falls into the catagory of visualisation.

Despite being on a circular display, the authors describe layout in a
rectangular format, rather than using polar co-ordinates. The authors also
comment that the need to recalculate co-ordinates each time in MDS, make
it an unattractive approach for real-time browsing. Instead a system based
on PCA is implemented. For the feature vector, colour moments, wavelet
based textures and water-filling edge-based structure feature. The HSV
colour space is used for its perceptual uniformity.

The reason given for choosing PCA over MDS is: "the fact that it fails to
model nonlinear mappings (which MDS succeeds at) is in our opinion a minor
compromise given the advantages of real-time, repeatable and
mathematically tractable linear projections. In their implementation,
after a "PCA Splat", images deemed more similar to the query image are
made larger or brighter. A number is also attributted acording to its
rank.

To reduce the effect of overlapping, the authors invoke a "Display
Optimization" technique.
},
	Author = {B. Moghaddam and Q. Tian and N. Lesh and C. Shen and T. Huang},
	Date-Added = {2008-11-18 15:06:05 +0000},
	Date-Modified = {2009-02-25 18:39:29 +0000},
	Journal = {International Journal of Computer Vision},
	Keywords = {Visualisation, MDS, Relevance Feedback},
	Number = {1/2},
	Pages = {109-130},
	Read = {Yes},
	Title = {Visualization and user-modeling for browsing personal photo libraries},
	Volume = {56},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvTW9naGFkZGFtMjAwNHJ6LnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTTW9naGFkZGFtMjAwNHJ6LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03ZQAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpNb2doYWRkYW0yMDA0cnoucGRmAAAOACgAEwBNAG8AZwBoAGEAZABkAGEAbQAyADAAMAA0AHIAegAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9Nb2doYWRkYW0yMDA0cnoucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@article{Carson:2002zl,
	Annote = {These works are based on the Blobworld system. Whilst (Carson, 1999) focuses more on the indexing efficiency of their system, (Carson, 2002) gives a more in depth discussion on the segmentation technique and recognition of the textures. 

Both papers describe the 3 main requirements a user expects from a quality retrieval system. These were the time required for an individual query, the quality of the query (based on precision or recall) and the ease for the user to understand the results and refine a query based on initial results. In Blobworld, a fully automatic algorithm was used to segment contiguous colour regions into blobs. The 10,000 Corel images used are still available, along with another 178,000 images at http://calphotos.berkeley.edu/. The 1999 work contains a good section of related work, with the QBIC, Photobook, Virage, VisualSEEk and Chabot systems all referenced. Blobworld is based on QBIC, where the image is split into regions of colour. 

Each blob is described by a colour distribution and mean texture descriptors. The 2002 paper explains these in some detail. Maybe return to these sections when more read about textures. Each pixel uses an 8D vector containing 3 values for the colour (from L*a*b* colour space). The 3 values for texture relate to the contrast, anisotropy and polarity. The final 2 are the x and y co-ordinates of the pixel. The features on the image are first smoothed to avoid over segmentation. An example given in the 2002 paper is a tiger, whereby over segmentation would cause the stripes to become their own regions. 

The 2002 work discusses in detail the colour, texture and scale features from the image. A post processing step, whereby pixels in between segments (with colour i) are assigned to the region with the highest bin value for colour i. The 8D space of an image is modelled with between 2 and 5 Gaussian models. The number is chosen with the Minimum Description Length (MDL) and is described in the 2002 work. An Expectation-Maximization algorithm is used to fit the Gaussian models to the data. The 2002 work comments the image segmentation took 5-7 minutes on a 300Mhz Pentium I I computer. 

Once the image is segmented, the blobs colour is represented by a colour histogram. The histogram had 500 bins, 20 units in width. 5 of these represented the L space, whilst a and b had 10. 218 fell into the gamut (number of colours that maybe represented) by the RGB colour space. To measure the distance between histograms, the quadratic distance was used. A symmetric matrix of weights between 0 and 1 representing similarity was used between bins i and j based on distance between bin centres: adjacent bins had a weight of 0.5. Each blob stored the mean texture contrast and anisotropy.
 
Blobworld is a QBVE system. The user enters the query image, then blobworld returns the blobs detected in the image. The user can select the blob(s) they are most interested in. These can be ranked, and spatial information can even be added, i.e. blob1 left of blob2. Due to problems discovered in the 1999 work, the facility to add the background as a blob was added. The 2002 paper supplies an example whereby a user may require an image of a bird in the sky, so the sky is particularly important. 

For indexing of the system, the 1999 work describes the use of R trees. These are index suitable structures for data that can be represented as points in N-dimensional space. The GiST framework (reference [9]) was used to experiment with the indices. The 1999 work describes this process in some detail (may need to review).

4 Experiments were implemented to test the precision of blobworld compared with histograms, the indexing performance compared with a full scan on the database and the loss of quality caused by indexing. 50 queries with blobworld and global colour and texture histograms were implemented. The histograms used the same 218 bins and quadratic distance as with the blob colour histograms. For the texture histograms, the 2 texture features were split into 21 bins each. 10 categories of image were used; tigers, cheetahs, zebras, planes, bald eagles, black bears, brown bears, elephants, horses and polar bears. 

5 queries using for each category were conducted with 1 blob, 2 blobs and global histograms. For the tiger, cheetah, zebra and plane categories, the difference between the blob and global histograms techniques showed clearly. The authors found results fell into 3 categories: Distinctive ob jects: when the colour and texture of the sub ject is distinctive, blobworlds query precision was higher than the global histogram approach. 

Distinctive scenes: For the plane images, the entire scene is distinctive. The plane region always has a common colour and texture. Global histograms performed better in this case. It was because of this problem the makers of blobworld added the ability for the user to select the background as a second blob. 
Other: in the 6 other categories, the blobs extracted and the global histogram com- 
puted were similar between images and therefore the performance was affected. The authors comment that blobworld has the ability to expand and solve this problem, whereas global histograms have no room for improvement. The results of the experiments found that the index speed improved as the number of dimensions decrease. Therefore they chose to compare indices on colour vector alone. Recall was measured whilst using nearest neighbour search to retrieve and rank the top 40 images compared with full query. 

It was found that the recall for blobs was better as an approximate for a full scan. To test the precision of the indexed and full queries, a ground truth was used to test the nearest 400 database ob jects to the query image. Blobs were found to be superior. It was found an indexed run took between a 1/3 and ? of the time of a full query. The authors comment that as the size of the databases were likely to increase, this reduction.},
	Author = {C. Carson and S. Belongie and H. Greenspan and J. Malik},
	Date-Added = {2008-10-28 10:50:04 +0000},
	Date-Modified = {2009-02-14 17:18:25 +0000},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {Blobworld, CBIR System, Segmentation, Clustering},
	Pages = {1026-1038},
	Read = {Yes},
	Title = {Blobworld: Image segmentation using expectation-maximization and its application to image querying},
	Volume = {24},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvQ2Fyc29uMjAwMnpsLnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQQ2Fyc29uMjAwMnpsLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03WAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpDYXJzb24yMDAyemwucGRmAA4AIgAQAEMAYQByAHMAbwBuADIAMAAwADIAegBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0NhcnNvbjIwMDJ6bC5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Carson:1999rz,
	Annote = {These works are based on the Blobworld system. Whilst (Carson, 1999) focuses more on the indexing efficiency of their system, (Carson, 2002) gives a more in depth discussion on the segmentation technique and recognition of the textures. 

Both papers describe the 3 main requirements a user expects from a quality retrieval system. These were the time required for an individual query, the quality of the query (based on precision or recall) and the ease for the user to understand the results and refine a query based on initial results. In Blobworld, a fully automatic algorithm was used to segment contiguous colour regions into blobs. The 10,000 Corel images used are still available, along with another 178,000 images at http://calphotos.berkeley.edu/. The 1999 work contains a good section of related work, with the QBIC, Photobook, Virage, VisualSEEk and Chabot systems all referenced. Blobworld is based on QBIC, where the image is split into regions of colour. 

Each blob is described by a colour distribution and mean texture descriptors. The 2002 paper explains these in some detail. Maybe return to these sections when more read about textures. Each pixel uses an 8D vector containing 3 values for the colour (from L*a*b* colour space). The 3 values for texture relate to the contrast, anisotropy and polarity. The final 2 are the x and y co-ordinates of the pixel. The features on the image are first smoothed to avoid over segmentation. An example given in the 2002 paper is a tiger, whereby over segmentation would cause the stripes to become their own regions. 

The 2002 work discusses in detail the colour, texture and scale features from the image. A post processing step, whereby pixels in between segments (with colour i) are assigned to the region with the highest bin value for colour i. The 8D space of an image is modelled with between 2 and 5 Gaussian models. The number is chosen with the Minimum Description Length (MDL) and is described in the 2002 work. An Expectation-Maximization algorithm is used to fit the Gaussian models to the data. The 2002 work comments the image segmentation took 5-7 minutes on a 300Mhz Pentium I I computer. 

Once the image is segmented, the blobs colour is represented by a colour histogram. The histogram had 500 bins, 20 units in width. 5 of these represented the L space, whilst a and b had 10. 218 fell into the gamut (number of colours that maybe represented) by the RGB colour space. To measure the distance between histograms, the quadratic distance was used. A symmetric matrix of weights between 0 and 1 representing similarity was used between bins i and j based on distance between bin centres: adjacent bins had a weight of 0.5. Each blob stored the mean texture contrast and anisotropy.
 
Blobworld is a QBVE system. The user enters the query image, then blobworld returns the blobs detected in the image. The user can select the blob(s) they are most interested in. These can be ranked, and spatial information can even be added, i.e. blob1 left of blob2. Due to problems discovered in the 1999 work, the facility to add the background as a blob was added. The 2002 paper supplies an example whereby a user may require an image of a bird in the sky, so the sky is particularly important. 

For indexing of the system, the 1999 work describes the use of R trees. These are index suitable structures for data that can be represented as points in N-dimensional space. The GiST framework (reference [9]) was used to experiment with the indices. The 1999 work describes this process in some detail (may need to review).

4 Experiments were implemented to test the precision of blobworld compared with histograms, the indexing performance compared with a full scan on the database and the loss of quality caused by indexing. 50 queries with blobworld and global colour and texture histograms were implemented. The histograms used the same 218 bins and quadratic distance as with the blob colour histograms. For the texture histograms, the 2 texture features were split into 21 bins each. 10 categories of image were used; tigers, cheetahs, zebras, planes, bald eagles, black bears, brown bears, elephants, horses and polar bears. 

5 queries using for each category were conducted with 1 blob, 2 blobs and global histograms. For the tiger, cheetah, zebra and plane categories, the difference between the blob and global histograms techniques showed clearly. The authors found results fell into 3 categories: Distinctive ob jects: when the colour and texture of the sub ject is distinctive, blobworlds query precision was higher than the global histogram approach. 

Distinctive scenes: For the plane images, the entire scene is distinctive. The plane region always has a common colour and texture. Global histograms performed better in this case. It was because of this problem the makers of blobworld added the ability for the user to select the background as a second blob. 
Other: in the 6 other categories, the blobs extracted and the global histogram com- 
puted were similar between images and therefore the performance was affected. The authors comment that blobworld has the ability to expand and solve this problem, whereas global histograms have no room for improvement. The results of the experiments found that the index speed improved as the number of dimensions decrease. Therefore they chose to compare indices on colour vector alone. Recall was measured whilst using nearest neighbour search to retrieve and rank the top 40 images compared with full query. 

It was found that the recall for blobs was better as an approximate for a full scan. To test the precision of the indexed and full queries, a ground truth was used to test the nearest 400 database ob jects to the query image. Blobs were found to be superior. It was found an indexed run took between a 1/3 and ? of the time of a full query. The authors comment that as the size of the databases were likely to increase, this reduction.},
	Author = {C. Carson and M. Thomas and S. Belongie and J.Hellerstein and J. Malik},
	Booktitle = {Third International Conference on Visual Information Systems},
	Cites = {Jacobs:1995bh, Pentland:1996oz},
	Date-Added = {2008-10-28 09:37:58 +0000},
	Date-Modified = {2009-02-15 12:08:17 +0000},
	Keywords = {Blobworld, CBIR System, Segmentation},
	Number = {1614},
	Pages = {509-516},
	Publisher = {Springer},
	Read = {Yes},
	Title = {Blobworld: A System for Region-Based Image Indexing and Retrieval},
	Year = {1999},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvQ2Fyc29uMTk5OXJ6LnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQQ2Fyc29uMTk5OXJ6LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03VwAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpDYXJzb24xOTk5cnoucGRmAA4AIgAQAEMAYQByAHMAbwBuADEAOQA5ADkAcgB6AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0NhcnNvbjE5OTlyei5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@inproceedings{Pass:1996fv,
	Annote = {This paper discusses a new technique of indexing images called Colour Coherence Vectors CCV, where pixels are partitioned based on their spatial coherence. It begins by implying the idea that colour histograms alone are not strong enough to represent images in database, stating that images with similar colour histograms may have dramatically different appearances. 

The authors define the idea of define the concept of Histogram Refinement, whereby pixels in a given bin (of a histogram) are split into classes based upon some local property. These may then be compared on a bin-to-bin basis, much like the GCH. In the paper, they describe 2 classes: central/non-central. A pixel is defined as central if it occurs in the pixels representing 75% of the image from the center. 

For comparing refined histograms, pixels in different classes are not compared. This is because a split histogram is generated (between the classes).Pixels in the same class can be compared using any standard metric (i.e. the L1 distance). CCVs are a more sophisticated form of histogram refinement. The colour bins are split by colour coherence. If the pixel is part of sizeable contiguous area of colour it is defined as coherent, else it is incoherent. To discover the coherence of pixels, the image is blurred with an average of neighbourhood values. The colours are discretized into n colours. 4 connected neighbours are tested. If a connecting component, C, between the 4 pixels is greater than a value t, the pixels are said to be coherent. 

For a colour j, the colour vector maybe represented as: 
(Aj, Bj) where a represents the coherent pixels of colour j, and b is the incoherent pixels for colour j. Therefore the CCV of an image can be ((Aj, Bj),..., (An, Bn)). In a colour histogram, the coherent/incoherent region are not separated and thus maybe represented as (Aj + Bj). 

For the experiments, a database of 14,554 images from QBIC, Chabot, Corel and some video stills was used. Each image contained 38,976 pixels. The value t = 300 was used so that a region is coherent if it takes up an area of about 1% of the image. The authors found the average image had 75% coherent pixels, with a standard deviation of 11%. Tested varying colour histograms, with the L1 and L2 metrics and 64 and 512 colour buckets also varied. They found that the L1 metric and 64 buckets, using the RGB colour space, gave best the performance. There was75 pairs of images (with different views of the same scene). 1 was used as the query image, the 2nd would be used as the correct
answer to a search. In 69 of the 75 cases, centering refinement produced better results than normal GCH. CCV produced better results than GCH in 68/75 cases, but the ranking improvement was better than centre refinement. The authors state that the reason the GCH performed better in the 7 remaining tests, was due to brightness effecting the colour. It states the use of a better colour space, such as CIE Labs, would combat this problem. In regards to efficiency for the 2 indexing mechanisms, CCV were found to be more computationally expensive. On a 50mhz SPARCstation using image resolutions of 232 x 168, 67 histograms could be computed per second, compared to just 5 CCVs. 21,940 histograms could be compared per second, compared to 7,746 CCVs. 

The paper submits the idea of Successive Refinement, whereby bins are divided based on additional features. For example, 4 classes could be used; centre/not centre and coherent/not coherent. The L1 metric could be used to compare split histograms. They had also produced preliminary tests on using gradient magnitude or direction as a classification. It states the results produced were promising and the later a significant 
improvement on CCV. 
},
	Author = {G. Pass and R. Zabih},
	Booktitle = {IEEE Workshop on Applications of Computer Vision},
	Cited-By = {Smith:1996dq, Huang:1997zl, Huang:1999wf, Cox:2000eh},
	Cites = {Pentland:1996oz, Ballard:1991yq},
	Date-Added = {2008-10-14 20:15:42 +0100},
	Date-Modified = {2009-02-15 12:15:23 +0000},
	Keywords = {CCV},
	Pages = {96-102},
	Read = {Yes},
	Title = {Histogram Refinement for Content-Based Image Retrieval},
	Year = {1996},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvUGFzczE5OTZmdi5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDlBhc3MxOTk2ZnYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN5sAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6UGFzczE5OTZmdi5wZGYADgAeAA4AUABhAHMAcwAxADkAOQA2AGYAdgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9QYXNzMTk5NmZ2LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Huang:1997zl,
	Annote = {This work proposes the idea of using a colour correlogram. The initial arguments for using such a technique are: 

* Contains spatial correlation of colours 
* Can be used to describe the global distribution of local spatial correlation of colours 
* Easy to compute 
* Size of the feature is fairly small. 

The colour correlogram can be described as an indexed table of colour pairs. Therefore: 
The kth entry for (i, j) specifies the probability of finding a pixel of colour j, at a distance k, from pixel of colour i in the image 

The paper argues that this technique is robust enough to withstand view position changes, a change in background scene, partial occlusion, and camera zooms causing 
change in shape. For simplify the problem, a autocorrelogram is proposed, where only identical colours are used, i.e. The kth entry for (i, i).........

The paper discusses the use of 2 different algorithms. The purpose of these is to reduce the running time, dependent on when d is small or large. d defines the total distance from the pixel. In the experiments, d = 1,3,5,7. 1,3,5,7 are matrices, each individually labelled k. d is all matrices used. 

For the experiments, the L1 distance is used as it is described as "more simple and robust". Another reason for this choice is that "the L1 metric is generally less affected by outliers". The distance measure used was relative. 

The paper describes 2 forms of measurement. The first, r, is the rank of the correct answer. The second, p1, is the precision at recall. A good result was a low value for r, 
and a high p1 value. Efficiency of the retrieval system is considered in section 5.1. The use of a parallelization of the feature base, exploiting the scarcity of the feature vector, and an initial global filtering technique, could all improve the efficiency. 

For the experiments, a heterogeneous database of many types of image was used. There were 14,554 JPEG images all sized at 232 x 168 pixels. The featurebase had a RGB colour space with colour quantization into 64 colours. The images were smoothed by a small amount. For each image the histogram, CCV, CCV/V and autocorrelogram (it is noted this was sufficient for that experiment). D (a 18subset of the afore mentioned d) = 1,3,5,7. As D was small, computation time was low. The featurebase was constructed in parallel. There were 77 queries to the database, each with 1 unique correct answer. The query response time was less than 2 seconds. The experiments show that the autocorrelogram outperforms the colour histogram considerably, and also the CCV and CCV/V indexing methods. It is also noted improvements are made when a histogram is first used as a filtering mechanism. 
},
	Author = {J. Huang and S. Ravi Kumar and M. Mitra and W. Zhu and R. Zabih},
	Booktitle = {Proceedings of the 1997 Conference on Computer Vision and Pattern Recognition},
	Cited-By = {Huang:1999wf, Cox:2000eh, },
	Cites = {Cox:1996qf, Pass:1996fv, Pentland:1996oz, Ballard:1991yq},
	Date-Added = {2008-10-14 18:46:29 +0100},
	Date-Modified = {2009-02-15 12:15:12 +0000},
	Keywords = {Correlogram, Histogram, CCV},
	Pages = {762-768},
	Rating = {5},
	Read = {Yes},
	Title = {Image Indexing Using Color Correlograms},
	Year = {1997},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvSHVhbmcxOTk3emwucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9IdWFuZzE5OTd6bC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTdgAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkh1YW5nMTk5N3psLnBkZgAADgAgAA8ASAB1AGEAbgBnADEAOQA5ADcAegBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0h1YW5nMTk5N3psLnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@inproceedings{Li:2006rz,
	Annote = {Automatic Linguistic Indexing of Pictures - Real Time (ALIPR) is the theme for this paper. It discusses the techniques used on constructing their system, available at www.alipr.com. Their technique was to use a 2-D Multi-resolution Hidden Markov Model (MHMM). This is explained more in Wang and Li, 2002. In their system, images are categorised by their theme. Each theme is called a semantic concept. 

The paper claims AIPR is the FIRST real-time annotation system. It claims words 
maybe associated with an image within 1.4 seconds on a machine with a 3.0Ghz processor. 

Each image is assigned a signature, which comprises of 2 distributions; for colour and texture. For each semantic concept in the database, the signatures of the images within that concept are used to create a model. The Corel database contains 60,000 heterogeneous photos, with 599 semantic concepts. Each concept contains roughly 100 photos. Each photo may contain several words, and the total vocabulary of the system is 332 distinct words. 80 images from each concept were used to model the database. 
The paper discusses their choice for image feature extraction and statistical models. They comment in their previous system, ALIP, the processing time was around 10 minutes, and clearly would not suffice for a real-time system. They comment that the chosen method of creating image signatures does actually work best on scenes (global) rather than individual objects, such as discussed in (Vasconcelos, 2007).

For colour, the RGB colour components of each pixel are converted to LUV colour components. These are clustered using the k-means algorithm, with the number of clusters being determined dynamically based on an average distance from cluster threshold. Wavelet coefficients are used with k-means clustering to develop the texture signature. Both the colour and texture signatures are cast into a discrete distribution. Because a Euclidian measure cannot be used, the Mallows Distance is adopted. The paper also explains the use of mixture modelling, and that it is widely used for classification and clustering. 

To test their systems, the authors list 3 cases in which the performance of their system was studied: 
a) Annotating images not included in the training set but within the Coral database (20 images per a semantic concept) 
b) Annotating images outside the Corel database and checking the correctness of annotation words manually by a dedicated examiner 
c) Annotating images uploaded by arbitrary with the words checked by the users 

The paper states that the author chose not to compare their system directly with other annotation systems, as the ultimate test would be the performance assessed by users on images outside the training database. 

For the training (80 images in each semantic concept), it took 109 seconds to train (with a standard deviation of 145 seconds) on a 2.4Ghz AMD processor. It is not clear if this is in fact for the entire system, a concept or even a picture. For the performance on the Corel images, the true annotation of every image was taken as the words assigned to its category. The ALIPR annotation was labelled 'correct' if it appeared in the given annotation. For the performance on images outside the Corel database, 54,700 images were taken from flickr.com. It is reported that it took an Intel 3.0Ghz computer just 1.4 seconds to analyze an image, decompress the JPEG, abstract signatures and find suitable annotation words.They found that incorrectly labelled images were rare, however the causes included: unusual background, a fuzzy shot (camera movement), occlusion or an unusual white balance (the example of this in the paper is a photo with man looking orange). Problems also occurred when an ob ject had not been learnt by the system. The humanly judged accuracy of the annotation of 5,411 images from flickr.com concluded that from 15 words (ranked in descending order of likelihood) assigned for each image, an average of 4.1 words were correct. The performance based on ALIPR Users found some interesting trends. For example, with time, users became stricter with words they chose as correct. Also, users often attempted to upload pictures to 'challenge' the system, such as gray scale images (something the system was not designed for). At the time of writing, 10,809 images were in the database. Of the 15 words predicted by APLIR, the user considered an average of 2.24 correct. Also, on average, users had added 1.67 words for each picture. 

The paper concludes with the thoughts that future work may include the use of 3D images or shape information to supplement the colour and texture signatures. The paper also notes that more high quality images maybe required to improve training, with each of the images containing more contextual information. 
},
	Author = {Jia Li and James Z. Wang},
	Booktitle = {Proceedings of the ACM Multimedia Conference},
	Date-Added = {2008-10-14 16:12:09 +0100},
	Date-Modified = {2009-02-14 19:35:54 +0000},
	Keywords = {ALIPR, CBIR System, Semantic System},
	Organization = {ACM},
	Pages = {911-920},
	Read = {Yes},
	Title = {Real-time Computerized Annotation of Pictures},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBNQYXBlcnMvTGkyMDA2cnoucGRm0hsPHB1XTlMuZGF0YU8RAbAAAAAAAbAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UwxMaTIwMDZyei5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTdjAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAEpNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkxpMjAwNnJ6LnBkZgAOABoADABMAGkAMgAwADAANgByAHoALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAD1Vc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvTGkyMDA2cnoucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDAAMUAzQKBAoMCiAKRApwCoAKuArUCvgLDAsYAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC0w==}}

@article{Datta:2008rz,
	Annote = {The introduction section begins with the motives for writing the paper and indeed the general motives for research into Image Retrieval. The authors give the definition of a CBIR system as: 

''Content-based image retrieval (CBIR), as we see it today, is any technology that in principle helps to organize digital picture archives by their visual content'' 

It continues with a quote relating to recent public interest into the technology: 

''Of late, there is renewed interest in the media about potential real-world ap- plications of CBIR and image analysis technologies, as evidenced by publications in Scintific American [Mirsky 2006], Discovery News [Staedter 2006] and on CNN [2005].'' 

The authors predict that eventually, groundbreaking success in the field will be achieved, due in part to a paradigm shift in which the goals of researchers looking at CBIR is to make such systems suitable for the average end user. The motive for writing this paper is stated as the extensive growth in research since 2000. The authors demonstrate the extent of this growth with a graph with research publication years. This shows a dramatic growth in papers published between 1995 and 2005. They used sites such as using Google Scholar, ieee, Springer etc. for this research. 

Section 1.1 describes the early years. It states that the initial phase of research and development occurred between 1994 and 2000. The paper lists the 2 major issues that need to be overcome to by any good CBIR. 

* Sensory Gap 
``The sensory gap is the gap between the object in the world and the information in a (computational) description derived from a recording of that scene'' 

* Semantic Gap 
``The semantic gap is the lack of coincidence between the information that one can extract from the visual data and the interpretation that the same data has for a user in a given situation. '' 

The authors list QBIC, Pictoseek and VisualSEEK as the pioneering feature extraction systems. For the user to specify the characteristics of the image for which it is interested, Relevance Feedback is defined whereby a user grades images returned as suitable or not suitable. An early example of this is given as Rui et al. [1998]. Content-based image retrieval systems that gained prominence in this era were, for example, IBM QBIC [Flickner et al. 1995], VIRAGE [Gupta and Jain 1997], and NEC AMORE [Mukherjea et al. 1999] in the commercial domain, and MIT Photobook [Pentland et al. 1994], Columbia VisualSEEK and WebSEEK [Smith and Chang 1997b], UCSB NeTra [Ma and Manjunath 1997], and Stanford WBI IS [Wang et al. 1998] in the academic domain. 

Section 2 describes image retrieval in the real world. It comments that an increasing number of people have some form of digital camera (be it camera or a mobile phone), and the low cost and easy access to web hosting and file sharing has caused a boom in the number of images available to all. 

This section also lists several factors needing considering in regards to the user: 

1) how does the user wish the results to be presented, 
2) where does the user desire to search, and 
3) what is the nature of user input/interaction. 

Section 2.1 looks at the different needs, for various types of users, a CBIR system maybe required to satisfy. The first is referred to as ''Browser''. This is a user with no clear end-goal. The user may conduct a series of unrelated searches across many topics during the course of a search. The authors comment this type of user value the ease of use rather than a time critical search. 

A ''Surfer'' is described as a user searching with some idea of an end-goal. First the user will explore the database with certain searches , but this scope will eventually narrow to an end result. This type of user will require a search mechanism that provides an input towards a final goal. 

A ''Searcher'' is described as a user who is very clear about what they are looking for. The session will typically be short, with coherent searches leading to an end-result. This type of user will value the quality of results and the clarity of the representation. The paper remarks that real-world usage from the user viewpoint has not been extensively studied. One of the few studies categorizes users as experts and novices, and studies their interaction patterns with respect to a video library [Christel and Conescu 2005]. 

Section 2.2 discusses the data scope. As discussed in [13], domains can be described as broad or narrow. Here, these domains are segmented further (quoted from the paper): 

* Personal Collection 
This consists of a largely homogeneous collection, generally small in size, accessible primarily to its owner, and usually stored on a local storage media. 

* Domain-Specific Collection 
This is a homogeneous collection providing access to controlled users with very specific objectives. The collection may be large and hosted on distributed storage, depending upon the domain. Examples of such a collection are biomedical and satellite image databases. 

* Enterprise Collection 
We define this as a heterogeneous collection of pictures accessible to users within an organization's intranet. Pictures may be stored in many different locations. Access may be uniform or nonuniform, depending upon the Intranet design. 

* Archives 
These are usually of historical interest and contain large volumes of structured or semi-structured homogeneous data pertaining to specific topics. Archives may be accessible to most people on the Internet, with some control of usage. Data is usually stored in multiple disks or large disk arrays. 

* Web 
World Wide Web pictures are accessible to practically everyone with an Internet connection. Current WWW image search engines such as Google and Yahoo! images have a key crawler component which regularly updates their local database to reflect on the dynamic nature of the Web. Image collection is semi-structured, nonhomogeneous, and massive in volume, and is usually stored in large disk arrays. 

Section 2.3. describes the different types of queries used by various CBIR systems (taken directly from the text): 

* Keywords 
This is a search in which the user poses a simple query in the form of a word or bigram. This is currently the most popular way to search images, for example, the Google and Yahoo! image search engines. 

* Free-Text 
This is where the user frames a complex phrase, sentence, question, or story about what she desires from the system. 

* Image 
Here, the user wishes to search for an image similar to a query image. Using an example image is perhaps the most representative way of querying a CBIR system in the absence of reliable metadata. 

* Graphics 
This consists of a hand-drawn or computer-generated picture, or graphics could be presented as query. 

* Composite 
These are methods that involve using one or more of the aforesaid modalities for querying a system. This also covers interactive querying such as in relevance feedback systems. There has been other types of input, with hand gestures and speech for querying and relevance feedback being used in Kaster et al. [2003]. [Fang et al. 2005] statistically model the user's interest and [Jaimes et al. 2004; Nagamine et al. 2004] help the user queries by providing cues and hints. 

Another area where CBIR has been trailed is within the MOBILE DEVICES domain. This survey paper lists Vinay et al. [2005, 2004] for testing relevance feedback on a mobile device, Bertini et al. [2005] on a small device. Xie et al. [2005] is also included in this area. 

The ESP game pits 2 players with each other and both must label pictures as they see them, rewarded when both players match with their captions.The use of text associated with an image can be used to aid with text-based querying, and the construction of training databases. 

Section 2.4 focusses on the Visualisation of images (taken diretcly from paper) 

* Relevance-Ordered 
The most popular way to present search results is relevance-ordered, as adopted by Google and Yahoo! for their image search engines. Results are ordered by some numeric measure of relevance to the query. 

* Time-Ordered 
In time-ordered image search, pictures are shown in a chronological ordering rather than by relevance. Google's Picasa system [Picasa 2004] for personal collections provides an option to visualize a chronological timeline using pictures. 

* Clustered 
Clustering of images by their metadata or visual content has been an active research topic for several years (discussed in Section 3). Clustering of search results, besides being an intuitive and desirable form of presentation, has also been used to improve retrieval performance [Chen et al. 2005]. 

* Hierarchical 
If metadata associated with images can be arranged in tree order (e.g., WordNet topical hierarchies [Miller 1995]), it can be a very useful aid in visualization. Hierarchical visualization of search results is desirable for archives, especially for educational purposes. 

* Composite 
Combining consists of mixing two or more of the preceding forms of visualization scheme, and is used especially for personalized systems. Hierarchical clustering and visualization of concept graphs are examples of composite visualizations. The section ends with an interesting quote: 

''We believe that the future of real-world image retrieval lies in exploiting both text- and content-based search technologies''. An example of this is ALIPR. 

Section 3 discusses the feature extraction techniques. Research into these features are more common. Machine learning, clustering and classification have been used to automatically tune the search for features. The other common search paradigm is iterative user feedback. Blobworld is an example of this, whereby an initial selection of images is returned by the user, who can select the particular features they require, before more images are selected. 

Section 3.1 focusses on the different ways in which the visual signature has been extracted from the images. The location and colour features of an image are often used. Texture features have also been explored, and are intended to capture the granularity and repetitive patterns of surfaces within in a picture. As regards to texture there are many different techniques that have been explored. The most common are wavelet and discrete cosine transforms, and can be found in [Do and Vetterli, 2002] and [Li et al., 2000]. Other approaches have been explored including a thesaurus for texture, geared toward aerial image retrieval, was proposed in [Ma and Manjunath, 1998]. In Hadjidemetriou et al. [2004], a multiresolution histogram, together with its associated image matching algorithm, is shown effective in retrieving textured images. 

A shape descriptor for similarity matching, referred to as shape context is used in [Belongie et al. 2002] which is fairly compact yet robust to a number of geometric transformations. Representation of shape using discrete curve evolution to simplify contours is discussed in Latecki and Lakamper [2000]. The section also mentions a 2D-Be-string, used in [Wang 2003]. The authors state: 

''We have witnessed a ma jor shift from global feature representations for images, such as color histograms and global shape descriptors, to local features and descriptors, such as salient points, region-based features, spatial model features, and robust local shape characterizations". 

Sparser topologies have been proposed, such as the star topology [Fergus et al. 2005], a hierarchy with the lowest levels corresponding to local features [Bouchard and Triggs 2005], and a geometry where local features are spatially dependent on their nearest neighbors [Carneiro and Lowe 2006]. 

A discussion on the pros and cons of different types of color interest points used in image retrieval can be found in Gouet and Boujemaa [2002], while a comparative performance evaluation of the various proposed interest point detectors is reported in Mikola jczk and Schmid [2003]. A survey and performance comparison of some recent algorithms on the topic can be found in Guyon and Elisseeff [2003]. 

Section 3.2 discusses image similarity between similarity measures. This section discusses the blobworld system, and the feature which allows the user to select areas of importance. Without such refinement, image similarity measures attempt to take all the regions in an image to consideration. 

TABLE 1 IS AN EXCELLENT COMPARISSON OF DISTANCE MEASURES! 

Section 3.3 describes clustering and classification. It suggests the review paper by Hastie et al. [2001] for the basic principles and a more comprehensive review on clustering and classification techniques. 

TABLE 2 IS AN EXCELLENT TABLE ON LEARNING TECHNIQUES 

The paper states that Domain-specific collections, such as medical image databases, remotely sensed imagery, and art and cultural image databases are examples where categorization can be beneficial. It states that clustering helps in visualization and retrieval efficiency, but the problem is defining the number of clusters and clusters themselves.
 
Section 3.4 discusses relevance feedback, and offers a survey paper on the topic (Zhou and Huang [2003]). Semantic feedback is proposed in Yang et al. [2005b]. The paper comments that many rounds of relevance feedback can confuse and annoy the user. To avoid this, user logs of earlier feedback maybe used, as in Hoi and Lyu [2004b]. The paper lists Jaimes et al. [2004] and Fang and Geman [2005] as presenting new relevance feedback ideas. The authors comment that while progress is evident in the relevance feedback field, the major issue is the lack of relevance feedback technology, either in the image or text retrieval domains, in real-world use. 

Section 3.5 remarks that Multimodal Fusion and Retrieval has received very little attention in CBIR, and this could open avenues for exploring novel user interfaces, querying models, and resulting visualization techniques pertinent to image retrieval, in combination with other media. The authors believe that the need for mutimodal retrieval in relation to images will soon grow in stature. 

Section 4.2 briefly looks at the application of an image given a short piece of text, such as a story. 4.5 discusses images and their use with the web. The paper comments that a particular problem is that a system developer cannot make any assumptions on the users, as the domain is so vast. There are several CBIR search engines, such as 
ALIPR, RIYA, CORTINA etc. Rowe [2002] presents a web crawler associating captions 
with images. Image grouping methods such as unsupervised clustering are explored in 
Wang et al. [2004a], Gao et al. [2005], Cai et al. [2004], and Jing et al. [2006]. 

An interesting use of image retrieval is CAPTCHA, used for internet security, whereby distorted text is used to verify the user is human (i.e. ticketmaster) has nearly been broken. Therefore a proposed use of CBIR could be presenting an image to the user. 

Section 5 focuses on the evaluation strategies. The paper states that any dataset used, should use general pictures and should be large enough for the evaluation to be statistically significant. A ground truth can be used stating which images in a database are similar. Of course similarity is in the eye of the beholder, and is very subjective.

Section 5.1 describes the common evaluation metrics. It describes: 

* Precision, referring to the percentage of retrieved pictures that are relevant to the query. 
* Recall, the percentage of all the relevant pictures in the search database which are retrieved. 
},
	Author = {R. Datta and D. Joshi and J. Li and J. Z. Wang},
	Date-Added = {2008-10-13 11:42:34 +0100},
	Date-Modified = {2009-02-14 19:35:45 +0000},
	Journal = {ACM Computing Surveys},
	Keywords = {CBIR Survey},
	Number = {2},
	Pages = {1-60},
	Read = {Yes},
	Title = {Image retrieval: Ideas, influences, and trends of the new age},
	Volume = {40},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBZQYXBlcnMvRGF0dGEyMDA4cnoucGRm0hsPHB1XTlMuZGF0YU8RAbwAAAAAAbwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03Uw9EYXR0YTIwMDhyei5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTdbAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAE1NYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOkRhdHRhMjAwOHJ6LnBkZgAADgAgAA8ARABhAHQAdABhADIAMAAwADgAcgB6AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBAVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL0RhdHRhMjAwOHJ6LnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDDAMgA0AKQApIClwKgAqsCrwK9AsQCzQLSAtUAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC4g==}}

@article{Vasconcelos:2007rz,
	Annote = {This paper mainly focuses on work undertaken at 'The Statistical Visual Computing Laboratory at the University of California, San Diego' (www.svcl.ucsd.edu). The paper briefly discusses Query By Visual Example (QBVE), whereby a user-entered image is used in order to select similar candidate images from the database. This is done by comparing the signature of the user-entered image, and comparing with all the signatures in the database. 

The paper discuses SVCLs Minimum Probability of Error (MPE) retrieval system. The image is split into bags of local features (such as texture, edginess and colour) and creates a GMM (Gaussian mixture model). The paper states An image signature is a compact probabilistic representation of how the features make up the feature space.The query image is split into bags and the system compares how well the GMM matches others in the database. The quality of the match is assigned a probability, in order to rank the database candidates returned to the user. The system also uses external information when assigning a probability to an image being similar to the query image. Such information includes previous queries, typical search patterns (i.e. football match pictures on a Saturday night), or perhaps surrounding text in a web page. The paper claims that their system is currently amongst the top performing QBVE systems. The system accesses a database of 1500 images, all of varied content and lighting conditions. Semantics come to the fore during the next stage of the paper. The Semantic Gap is discussed, and the example given is how for a user entered image of a train crossing a bridge, a system may return many pictures of just bridges which is not what the user intended. It states that 2 primary goals for a semantic based system is image annotation and search. A semantic based system requires a training database in order for it to learn image concepts. Such a database must have images labelled in natural language so that it may map similarities in words and the images. This mapping can be used to search for an image by keyword, or label new, unlabelled images. The semantic system extends their QBVE system. They note a significant difference between the 2 is that the semantic system requires a set of images to create GMMs for a concept, rather than the single image required for the QBVE system. Images undergo the MPE technique but this time to associate keywords. The paper demonstrates examples of when the system works correctly, and a few where it falls short. 

The paper then introduces QBSE (Query By Semantic Example) where a user entered image is semantically identified and images in the database are returned based on these semantics. The paper claims the system is much less affected by multiple semantic interpretations (as in the train and bridge example) and may also return images of concepts not learned by the system. The example given is a man fishing image. The system may not understand the concept of fishing, but may know water, man, boat etc. Therefore there is a good chance images returned by the system WILL contain fishing scenes. 

The paper goes on to discuss the results of their system(s), showing positive results in both QBSE and QBVE. It concludes that by no means do they present a final solution,but MPE and other methods have seriously evolved the domain of CBIR.},
	Author = {N. Vasconcelos},
	Date-Added = {2008-10-09 12:46:04 +0100},
	Date-Modified = {2009-02-13 11:20:34 +0000},
	Journal = {Computer},
	Keywords = {Semantic System},
	Number = {7},
	Pages = {20 - 26},
	Read = {Yes},
	Title = {From Pixels to Semantic Spaces: Advances in Content-Based Image Retrieval},
	Volume = {40},
	Year = {2007},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBxQYXBlcnMvVmFzY29uY2Vsb3MyMDA3cnoucGRm0hsPHB1XTlMuZGF0YU8RAdQAAAAAAdQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxVWYXNjb25jZWxvczIwMDdyei5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTemAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFNNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlZhc2NvbmNlbG9zMjAwN3J6LnBkZgAADgAsABUAVgBhAHMAYwBvAG4AYwBlAGwAbwBzADIAMAAwADcAcgB6AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBGVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL1Zhc2NvbmNlbG9zMjAwN3J6LnBkZgATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDJAM4A1gKuArACtQK+AskCzQLbAuIC6wLwAvMAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADAA==}}

@inbook{Stehling:2002nx,
	Annote = {This is a chapter from the book Multimedia Mining: A Highway to Intelligent Multimedia Documents and provides a brief overview of all the different aspects of CBIR. The first section (after the introduction) describes the colour spaces used in CBIR. It lists the 3 types: 

* Hardware-orientated: 
This is RGB. It is a device dependent colour space, i.e. different hardware will produce its own colour specification. The differences between the colours do not reflect the differences seen by humans. 

* User-orientated: 
The HSI and HSV colour spaces exploit characteristics used by humans. HUE: The dominant wavelength producing Red, Yellow, Green and Blue (or combination of the 2) SATURATION: The purity of that colour, standard deviation around the dominant wavelength INTENSITY: The amount of brightness (white) in that colour. 

* Uniform colour spaces: 
In the Lab and L*U*V colour spaces numerical differences between colours are consistent with differences perceived by humans. These are in the pairs; Red/Green, Yellow/Blue, Black/White pairs. These are device independent. 

The following section discusses the use of colour as a means of creating an image index. It explains that there are far too many pixels in an image; so keeping a % of each colour in an image is an obvious work around. It also mentions the use of spatial distributions of colours indicating where in an image the colours occur. These indexes can be further compressed to save valuable memory. 

The paper discusses static and dynamic reduction techniques. Static quantization is the dividing of colours in an image into a uniform distribution of colours. The advantage of this is that the number of colours, and the colours themselves, are constant for all images in the database. One main disadvantage is the problem that colours in an image are not necessarily uniformly distributed in the colour space. This method does not work on non-uniform colour-spaces since colours maybe separated and non-similar colours classified together. A suggested alternative to this is the use of average colour for an image. This is simple, but may produce false positives, due to other images having a similar colour make up. 

The section on static reduction methods concludes with the introduction of the concept of partitioning an image in to smaller grids in order to calculate the colours for each grid. 
This would reduce the spatial distribution of the colours. This method is good for compression, storage, access and retrieval of images. The downfall of this theory is there is no single scheme known to be optimal for distinct CBIR applications. 

The following section describes dynamic reduction techniques, and lists boundary detection, region growing, region splitting and merging, density estimation and hierarchical clustering as examples. These are all referenced. These exploit the visual content in the image in order to split it up. The above methods are described in brief. 

The book chapter moves to discuss global, partition-based and local representations of images. For global, the document describes the GCH (global colour histogram). For the partition-based, the authors describe splitting the image into cells, and taking LCH (local colour histograms), for each cell. Regional representations are similar to partition based, however the cell dimensions are dynamically chosen, and maybe all of varying sizes. 

The subsequent section of the paper describes distance functions. It notes that the selection of a good measure is vital in both the speed of computation, and the quality of image retrieval. The better the distance simulates human vision, the more effective the CBIR will be. Representing visual features in a vectorial space allows easy computation of the geometric distance between 2 vectors. The section contains a good description on metrics. 

The chapter continues to section 6, which discusses similarity search. It explains that a sequential search does not work well on a large image database but suggests filters such as average colour to reduce this search.

The paper splits access methods into 2 sections: Spatial and Metric. For spatial, the co-ordinates on the space are used to group and classify points in the space. 6.2 contains a reference for a good survey paper. Metric access methods use the absolute spatial location of ob jects to partition and search the vectorial space. Section 6.4, as in (Smeulders et al., 2000), discusses approximate and exact query searches. Section 7 
concentrates on some of the existing CBIR approaches, split into the global, partition-based and regional approaches:

* Global: 
Most simple and well known is quantizing RGB colour space into 64 colours and storing in a GCH. These are compared using the L1 metric. 
Advantages - efficient 
Disadvantages - No spatial information 

* Partition-Based: 
Most simple system used a fixed grid of cells (i.e. 3x3, 4x4). Colour details stored in LCH. Distance between 2 images is computed as an average of the distances between LCHs of equivalent cells. 
Advantages - Contains some spatial information 
Disadvantages - 2 Images with same ob jects but in different positions will be at 
maximum distance away from each other. 

* Regional-Based: 
Images decomposed based on their visual content. The most common approach is to compare the regions of images individually. QBIC, SIMPLIcity, CBC, BlobWorld, are all examples of such a system. Techniques include clustering and k- means clustering. 

The remainder of the paper is dedicated to the 2 well-known problems associated with CBIR. How to measure effectiveness and the semantic gap. 
},
	Author = {Stehling, R. and Nascimento, M. and Falcao, A.},
	Chapter = {4 - Techniques for Color-Based Image Retrieval},
	Date-Added = {2008-10-08 15:20:09 +0100},
	Date-Modified = {2009-02-14 18:12:22 +0000},
	Editor = {Djeraba, C.},
	Keywords = {CBIR Survey},
	Pages = {61-80},
	Publisher = {Kluwer Academic Publishers},
	Read = {Yes},
	Title = {Multimedia Mining: A Highway to Intelligent Multimedia Documents},
	Year = {2002}}

@inproceedings{Stricker:1994zl,
	Annote = {This paper compares the L1 and L2 metrics (used by Swain & Ballard and in QBIC respectively) and conducts experiments into discovering the maximal number of distinguishable histograms that maybe stored, and the maximal number of retrieved images. 

The paper examines both metrics and makes the following observations: 
1) L1 Metric does not take into account colour similarity between bins, and therefore results in false negatives (images not chosen as colours not exactly the same). 
2) For the L2 Metric, false positives occur, due to histograms with many non zero bins will always be close to any other histogram an thus many unsuitable histograms will be returned.
 
For both metrics, the paper first distinguishes an ideal value for t, representing the threshold in the difference of 2 images. Observation 3 from this paper suggests a reasonable value for t is from the first interval where the distance distribution increases rapidly. To test the capacity and maximum match numbers, the Smithsonian Image Database, of 500 pictures was used with each image cropped to 10,000 pixels. A randomly generated database of images was also tested. 

For the maximum capacity of histograms, L1 outperforms L2 at lower % of t in max value, and vice versa for larger % of t in max value. For the maximum number of matches, results from tests with 100, 1000 and 10,000 histograms tested with a t -threshold of similarity showed that for: 
L1 - all results were returned within 60 - 65% of the max. distance 
L2 - all results were returned within 20 - 24% of the max. distance. 

The paper notes that the relatively small interval for L2 makes it hard to define a good retrieval threshold for this metric. The paper concludes by stating it has demonstrated which metric is suitable for a given application. },
	Author = {M. Stricker},
	Booktitle = {Proceedings of SPIE Storage and Retrieval for Image and Video Databases},
	Cited-By = {Orengo:1995rz},
	Cites = {Ballard:1991yq},
	Date-Added = {2008-10-08 13:41:14 +0100},
	Date-Modified = {2009-02-15 11:08:40 +0000},
	Keywords = {Distances},
	Pages = {15-24},
	Read = {Yes},
	Title = {Bounds for the discrimination power of color indexing techniques},
	Volume = {2185},
	Year = {1994},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBlQYXBlcnMvU3RyaWNrZXIxOTk0emwucGRm0hsPHB1XTlMuZGF0YU8RAcgAAAAAAcgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMWfycZIKwAAAA03UxJTdHJpY2tlcjE5OTR6bC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTelAAAAAAAAAAAAAAAAAAEAAwAACSAAAAAAAAAAAAAAAAAAAAAGUGFwZXJzABAACAAAxZ/JxgAAABEACAAAAAAAAAAAAAEAGAANN1MACZrbAAmIcwAHswYAB7L5AABxnwACAFBNYWNpbnRvc2ggSEQ6VXNlcnM6d2lsbGlhbXBsYW50OkRvY3VtZW50czpVbmkgV29yazpQaEQ6UGFwZXJzOlN0cmlja2VyMTk5NHpsLnBkZgAOACYAEgBTAHQAcgBpAGMAawBlAHIAMQA5ADkANAB6AGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAENVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvU3RyaWNrZXIxOTk0emwucGRmAAATAAEvAAAVAAIAE///AACABtIfICEiWCRjbGFzc2VzWiRjbGFzc25hbWWjIiMkXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN00h8gJieiJyRcTlNEaWN0aW9uYXJ5AAgAEQAbACQAKQAyAEQASQBMAFEAUwBcAGIAaQB0AHwAgwCGAIgAigCNAI8AkQCTAKAAqgDGAMsA0wKfAqECpgKvAroCvgLMAtMC3ALhAuQAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAC8Q==}}

@inproceedings{Sebe:1998rz,
	Annote = {This paper compares three metrics for comparing distances between images in a Euclidean space. These include the L1 and L2 metrics, along with their newly proposed metric Lc. The metric devised in this work is a parameterised metric based on a Cauchy distribution, which is used to model the probability of the difference between two images. The ``probability density of noise'' (the difference between the two images) can be found using the Maximum Likelihood Estimator. The paper gives the mathematical formula as evidence for their choice, also stating that the tail values are much too great to be appropriately represented as a Gaussian distribution (as used in previous research).

For the experiments, each of the metrics was tested on the Leiden 19th Century Portrait Database. The total size of the database at the time of writing was 6,656 images. The authors state that they anticipated this to expand to over 50,000. Images in this database are repeated, but often with varying degrees of degradation. For example, some of the images may have incurred moisture damage or degraded colour intensity.

120 image copy pairs were used as the ground truth, in an attempt to provide a clear definition of image similarity. Using this definition, the real distribution of similarity noise for the intensity, gradient and feature spaces were calculated. The authors state that for each space, the best representative distributions were:

*	Intensity -- Double exponential/Cauchy
*	Gradient - Double exponential/Cauchy
*	Features -- Gaussian

The paper uses figures to represent and justify why each space had been assigned a specific distribution. Query images were used along with the different distance metrics in order to discover which offered the most reliable returned image set, according to the performance measures of retrieval used by Huijsmans et al. (1997).

The paper also investigates ``Stereo Matching''. This is the process of determining corresponding differences between entities in related images. The authors applied their idea of similarity noise to this process, testing their theories on an image set comprised of towers and castles. The sets contained images at multiple views, with known accurate information of the location of the objects in 3D.

The paper concludes that in their experiments, the L1 metric outperformed the L2 metric. However it is reported that their new metric can further improve the quality of the returned image set.

},
	Author = {N. Sebe and M. Lew and D. P. Huijsmans},
	Booktitle = {International Conference on Pattern Recognition},
	Cites = {Ballard:1991yq, Faloutsos:1994lq},
	Date-Added = {2008-10-08 12:44:54 +0100},
	Date-Modified = {2009-02-15 11:37:12 +0000},
	Keywords = {Distances},
	Pages = {265-271},
	Rating = {4},
	Read = {Yes},
	Title = {Which ranking metric is optimal? with applications in image retrieval and stereo matching},
	Year = {1998},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvU2ViZTE5OThyei5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDlNlYmUxOTk4cnoucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN6EAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6U2ViZTE5OThyei5wZGYADgAeAA4AUwBlAGIAZQAxADkAOQA4AHIAegAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9TZWJlMTk5OHJ6LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@inproceedings{Sebe:2000rz,
	Annote = {This is a short paper, which uses the ImageScape system, as defined in an earlier paper. In this paper, it discusses the visual matching techniques which function with the user entering a sketch or iconic representation of an image as the query for the database (in this case the image repository is the Web). 

The search engine (ImageScape) uses Relevance Feedback. The user is presented with positive and negative examples based on the visual concept entered. A variety of colour, texture and shape features are extrapolated and used to find a subset to maximise the discriminatory power. This subset would have the distribution of features measured in the positive and negative examples, the use the correlation of the features and Kullback relative information of the features to produce N features which MINIMIZE correlation and MAXIMISE kullback. This was the original algorithm for the system. 

The paper discusses a new algorithm, which prevents the original algorithm producing inappropriate results. The example given is skin in a colour image, will not be recognised in a gray scale image. In the new algorithm, a Decision Set is trained to discriminate between classified and mis-classified images. This new set is used to calculate when the normal set was inappropriate. If the decision set determines the normal set is inappropriate, it is passed to an Outlier Set to distinguish between misclassified images and non-concept images. 

For the sketched input images from the user, a Sobel operator is used to match the edges of the sketch to candidate images in the database. Recognition is done in 2 phases; Global and Local. For the Global, 7 moment invariants are used to act as a fast filter. Local shape recognition is done with measuring the deformation energy from active contours. The N Candidates selected by the global comparison, are then ranked in order of the deformation energy.},
	Author = {M.S. Lew and N. Sebe},
	Booktitle = {Proceedings of Computer Vision and Pattern Recognition},
	Cited-By = {Smeulders:2000zl, },
	Date-Added = {2008-10-06 15:47:48 +0100},
	Date-Modified = {2009-02-15 16:37:36 +0000},
	Keywords = {QBSE, Query},
	Pages = {788-789},
	Rating = {2},
	Read = {Yes},
	Title = {Visual Websearching Using Iconic Queries},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBVQYXBlcnMvU2ViZTIwMDByei5wZGbSGw8cHVdOUy5kYXRhTxEBuAAAAAABuAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTDlNlYmUyMDAwcnoucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN6IAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIATE1hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6U2ViZTIwMDByei5wZGYADgAeAA4AUwBlAGIAZQAyADAAMAAwAHIAegAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAP1VzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9TZWJlMjAwMHJ6LnBkZgAAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAwgDHAM8CiwKNApICmwKmAqoCuAK/AsgCzQLQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAt0=}}

@article{Smeulders:2000zl,
	Annote = {This is an influential survey paper, referenced by many subsequent papers. It discusses the many areas of research into CBIR and was published almost 10 years after the pioneering paper by (Swain & Ballard, 1991). 

The paper starts by stating that papers on the topic prior to 1991 are rare, but since 1997, the number of published papers has increased dramatically. They herald the break through of the first web browser, Mosaic, causing the number of images the average user has access to increase dramatically. After justifying the choices of papers used (importance to the field, accessibility etc.) the authors move swiftly on to the applications of a CBIR system. First the user trends are analysed, and split into 3 categories: 

* Search by Association 
User looks for interesting things at start, then iterates and refines search based on interesting things found. 

* Search by Target 
The user has some idea what they are after, either exact copy or in mind. 

* Category Search 
An image is required of a particular group, i.e. sky scenes, water scenes etc. 

Further works in this are included in  [Enser, 1995]. 

In [Ornager, 1999] it reads that journalists have 5 typical search patterns: 
* Search for a specific image 
* General browsing to make an interactive choice 
* A picture to go with broad story 
* Illustrate a document 
* Fill-ins on esthetic value of image. 

An example of a system that can perform 3) is part of the ALIPR project (Li & Wang, 2006). The following section talks about the concept of the SENSORY GAP and the image domain. The paper discusses BROAD and NARROW domains. The Internet is an example of a broad domain, where almost any type of scene/ob ject may be in a picture. A narrow domain could perhaps be a database of facial images. It states it easier to design a CBIR system for a narrow domain, as the geometrical structure of the images will always be similar. The SENSORY GAP is also discussed here. 

Section 2.3 states the importance of domain knowledge in order to understand the images (for that given domain). The paper states the opinion that the downfall of early systems may be due to the lack of attention given to semantics. The paper lists examples of developments trying to bridge this gap by adding contextual data (such as text) to the images. Such examples of this are ALIPR (Li & Wang, 2006). The paper also gives examples of web-crawlers, obtaining text from surrounding web pages [Chang et. Al., 1997]. 

''The challenge for image search engines on a broad domain is to tailor the engine to the narrow domain the user has in mind via specification, examples, and interaction''. 

The paper states that the key for successful image processing is to enhance elements of the image relevant to the query, but to reduce the remaining aspects. The paper moves onto colour image processing techniques. Of particular interest is the reference to (Swain & Ballard, 1991) which uses the opponent color axes, rather than invariant descrip- 
tions. This limits the brightness to the 3rd axis. This axis is weighted more heavily than the 2 coloured axis, due to humans being more sensitive to light than colour. This section also describes the other colour spaces, such as the Lab-spaces (uniformity to model human vision) and HSV (has invariant properties). It also talks about colour constancy. 

The following part of the paper moves to image shape processing techniques. Many concepts and papers are discussed here, for example the use of Gabor filters in [Manjunath,1996]. Then texture is considered. A quote from the paper is: 

"In computer vision, texture is defined as all what is left after color and local shape have been considered or it is 
defined by such terms as structure and randomness". 

The paper lists many examples of texture recognition, such as the use of wavelets in [Daubechies, 1992]. The paper states much work has been applied into the extraction of textures in the domains of text and satellites. The paper continues by discussing image partitioning. First it states that image segmentation is extremely difficult for broad domains and general images. It writes that [Picard and Minka, 1995] is an example of images being broken down into tiles and each of these has the features extracted from them. 

The paper then moves to global features, and how they are accumulated. This section discusses the use of colour histograms, geometric histograms, combined histograms, colour correlograms, etc. The remainder of the section discusses data compression of images and its effect on image retrieval.

The subsequent section describes salient features. A definition of salient is something that has a quality that thrusts itself into attention. There are references o many works that have looked into salient features, including [Lindeburg, 2000] which gradually blurs out the image and picks out the parts of the image that still remain clear. The paper then moves to describe signs, listing OCR in text as using signs. Following this, Shape and lay-out of ob jects are discussed. A structural feature descriptions maybe captured in a graph or hierarchy. The paper contains a decent figure, representing the key features in an image (representing the side of a house) as a tree structure. For the layout, many example reference papers are given, including the 2D string representation used in [Chang and Hsu, 1992]. Shape features also have much work considered, including [Mehtre, 1997] where a 500 element trademark set was used to compare shape extraction techniques. 

For their discussion on features, they state that it is difficult to employ strong segmentation in large domains. They also imply they feel that the extraction of salient requires further work and that also in the future, they expect the work of local, partial or fuzzy structural descriptors. 

The next section discusses the similarity distances used to compare features of images. 
L1 and L2 are both discussed. Labeling images into categories due to similarities between images, is detailed and 
has references regarding to, in sections 5.6 and 5.7. 

The paper moves its focus to the user in section 6. The section describes the query space of a database. 6.2 describes the 2 ma jor query types, exact queries (where all images that match a certain criteria are returned) and approximate queries (where images are assigned a score based on them meeting a criteria, and returned based on 
this score). The 3 types of exact query are: 

1) Spatial predicate 
2) Image predicate 
3) Exact query by group predicate. 

The 3 types of approximate query are; 
* Approximate query by spatial example, such as a sketched image.
* Approximate query by image example, where the system is given an image, and it is required to return n similar images in the database 
* Approximate image query by group example, here a group of images is given as the query, and the system attempts to make a semantic bridge between all images in group to return suitable candidate images from the database. 

The focus of the paper shifts to the display of candidate images. Section 6.3 contains many references and ideas as to how database visualization has occurred. Therefore this section require much further reading. The user theme continues with a look into relevance feedback, this section also requires further reading. 

The final section of this paper looks into System Architecture. It discusses the importance of storage and indexing, and how the complexity of signature extraction can hinder a system. It also talks about system evaluation, and how the primary measurement is recall and precision are suitable for testing images with labels or textual descritions.. 
},
	Author = {A. Smeulders and M. Worring and S. Santini and A. Gupta and R. Jain},
	Cites = {Cox:2000eh, Faloutsos:1995ad, Sebe:2000rz, Pentland:1996oz, Orengo:1995rz, Ballard:1991yq, },
	Date-Added = {2008-10-03 16:09:07 +0100},
	Date-Modified = {2009-02-15 16:37:00 +0000},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {CBIR Survey},
	Number = {12},
	Pages = {1349-1380},
	Rating = {4},
	Read = {Yes},
	Title = {Content-Based Image Retrieval at the End of the Early Years},
	Volume = {22},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBpQYXBlcnMvU21ldWxkZXJzMjAwMHpsLnBkZtIbDxwdV05TLmRhdGFPEQHMAAAAAAHMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MTU21ldWxkZXJzMjAwMHpsLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03owAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBRTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpTbWV1bGRlcnMyMDAwemwucGRmAAAOACgAEwBTAG0AZQB1AGwAZABlAHIAcwAyADAAMAAwAHoAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL3dpbGxpYW1wbGFudC9Eb2N1bWVudHMvVW5pIFdvcmsvUGhEL1BhcGVycy9TbWV1bGRlcnMyMDAwemwucGRmABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMcAzADUAqQCpgKrArQCvwLDAtEC2ALhAuYC6QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAL2}}

@inproceedings{Orengo:1995rz,
	Annote = {This paper details 2 new approaches to the colour indexing of images. The paper begins with an analysis of existing metrics, and draws the same conclusions as in (Stricker,1994). The first of the new techniques talked about is the use of a Cumalitive Colour Histogram (CCH), modelled on a discrete form of a Probability Distribution. The 
paper argues that a CCH is more robust than a standard colour histogram.
 
The second new approach in this paper was to again treat the colour distribution as a probability distribution, but only keep the key features of the distribution, i.e. the bins with the highest values. In a probability distribution, these are known as the Moments. The authors used the 1st, 2nd and 3rd moments from each colour channel (from HSV colour space). The first was the average colour, the second and third are the variance and skewness of each channel. The standard deviation and 3rd root of the skewness of each colour channel were also stored in the index. 

Along with these values, weightings were used to edit the function for certain situations. The examples given are that if it is known all images are taken in the same light conditions, the weights are changed to penalise shifts in the average colour. If the pictures are from varying light conditions, such as outdoor images, the weights can be reduced to cater for this. Because the HSV colour space is used, the weights were set so the Hue channel has more significance than the Saturation channel. For the experiments, a database of 3000, 8-bit, JPEG images were used with a resolution of 185 x 123. For the 9 moments, 3 different weighting matrices were used. The experiment used 2 different query images: a gas tank (with 3 obvious matches in the database) and an owl perched on a tree trunk (6 matches). In both cases the 9 moments method clearly out performed the histogram methods. 

For the cumulative and normal histogram methods, 2 variations of index bins for each colour channel were tested. The paper remarks that a higher colour quantization for any L-metrics does not always yield better results and therefore tuning of histogram indexing is difficult to optimise. 

The paper concludes that their new method, using moments, improves results, reduces storage overhead (as index is 9) and improves retrieval time.},
	Author = {Markus Stricker and Markus Orengo},
	Booktitle = {Storage and Retrieval for Image and Video Databases},
	Cited-By = {Nakazato:2001rp, Smeulders:2000zl, Santini:2000fv ,Rubner:1997dd, Ma:1999gd, Rubner:2000jl, Pecenovic:2000cr, },
	Cites = {Stricker:1994zl, Ballard:1991yq},
	Date-Added = {2008-10-03 15:14:06 +0100},
	Date-Modified = {2009-02-15 16:41:26 +0000},
	Keywords = {CCH, Moments, Distances},
	Number = {381--392},
	Publisher = {SPIE},
	Read = {Yes},
	Title = {Similarity of Color Images},
	Volume = {2420},
	Year = {1995},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBdQYXBlcnMvT3JlbmdvMTk5NXJ6LnBkZtIbDxwdV05TLmRhdGFPEQHAAAAAAAHAAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADFn8nGSCsAAAANN1MQT3JlbmdvMTk5NXJ6LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA03aAAAAAAAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMWfycYAAAARAAgAAAAAAAAAAAABABgADTdTAAma2wAJiHMAB7MGAAey+QAAcZ8AAgBOTWFjaW50b3NoIEhEOlVzZXJzOndpbGxpYW1wbGFudDpEb2N1bWVudHM6VW5pIFdvcms6UGhEOlBhcGVyczpPcmVuZ28xOTk1cnoucGRmAA4AIgAQAE8AcgBlAG4AZwBvADEAOQA5ADUAcgB6AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBBVXNlcnMvd2lsbGlhbXBsYW50L0RvY3VtZW50cy9VbmkgV29yay9QaEQvUGFwZXJzL09yZW5nbzE5OTVyei5wZGYAABMAAS8AABUAAgAT//8AAIAG0h8gISJYJGNsYXNzZXNaJGNsYXNzbmFtZaMiIyRdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3TSHyAmJ6InJFxOU0RpY3Rpb25hcnkACAARABsAJAApADIARABJAEwAUQBTAFwAYgBpAHQAfACDAIYAiACKAI0AjwCRAJMAoACqAMQAyQDRApUClwKcAqUCsAK0AsICyQLSAtcC2gAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAALn}}

@article{Ballard:1991yq,
	Annote = {This is one of the most influential works in CBIR as we know it today. Swain & Ballard proposed and implemented their idea of using colour histograms in order to index colour images. The colours present in an image (represented in the RGB colour space) are discretionised into bins. 

A query histogram can be computed from a given image and compared with the histograms from each image in the database, using Histogram Intersection (the number of pixels present in both histograms). The paper proposes the L1 distance, otherwise known as the city-block or manhattan distance for calculating the similarity between histograms.

The main arguments in support for colour histograms were that they are simple to compute and are relatively stable when dealing with changes in view, translation and rotation in the viewing axis, and also partial occlusion. The paper claims a 3D object can be fully represented with just a few histograms.

Swain & Ballard use an opponent colour axis as stated below:

rg = r - g
by = 2 * b - r - g
wb = r + g + b 

This was done to limit lighting differences to 1 axis (wb). The wb axis was divided into 8 bins, whilst the rg and by axis were divided into 16 bins each.

Another fundamental idea introduced in this paper is image back projection, whereby the colour histogram of an object can be used to locate that object within an image.

Incremental Intersection is also proposed, whereby to save computation and comlexity, only top top x bins are compared.},
	Author = {M. Swain and D. Ballard  },
	Cited-By = {Smeulders:2000zl, Pecenovic:2000cr ,Chen:2000ly ,Chen:2000gn, Rubner:2000jl, Krischnamachari1999, Ma:1999gd, Huang:1999wf, Sebe:1998rz, Huang:1997zl, Rubner:1997dd, Pass:1996fv, Smith:1996dq, Faloutsos:1994lq, Stricker:1994zl, Zhang1995, Orengo:1995rz, Pentland:1996oz},
	Date-Added = {2008-10-02 12:43:49 +0100},
	Date-Modified = {2009-02-15 16:38:18 +0000},
	Journal = {International Journal of Computer Vision},
	Keywords = {Colour Histogram},
	Number = {1},
	Pages = {11-32},
	Rating = {4},
	Read = {Yes},
	Title = {Color Indexing},
	Volume = {7},
	Year = {1991},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGBwpZJGFyY2hpdmVyWCR2ZXJzaW9uVCR0b3BYJG9iamVjdHNfEA9OU0tleWVkQXJjaGl2ZXISAAGGoNEICVRyb290gAGoCwwXGBkaHiVVJG51bGzTDQ4PEBMWWk5TLm9iamVjdHNXTlMua2V5c1YkY2xhc3OiERKABIAFohQVgAKAA4AHXHJlbGF0aXZlUGF0aFlhbGlhc0RhdGFfEBhQYXBlcnMvQmFsbGFyZDE5OTF5cS5wZGbSGw8cHVdOUy5kYXRhTxEBxAAAAAABxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAxZ/JxkgrAAAADTdTEUJhbGxhcmQxOTkxeXEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANN1UAAAAAAAAAAAAAAAAAAQADAAAJIAAAAAAAAAAAAAAAAAAAAAZQYXBlcnMAEAAIAADFn8nGAAAAEQAIAAAAAAAAAAAAAQAYAA03UwAJmtsACYhzAAezBgAHsvkAAHGfAAIAT01hY2ludG9zaCBIRDpVc2Vyczp3aWxsaWFtcGxhbnQ6RG9jdW1lbnRzOlVuaSBXb3JrOlBoRDpQYXBlcnM6QmFsbGFyZDE5OTF5cS5wZGYAAA4AJAARAEIAYQBsAGwAYQByAGQAMQA5ADkAMQB5AHEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEJVc2Vycy93aWxsaWFtcGxhbnQvRG9jdW1lbnRzL1VuaSBXb3JrL1BoRC9QYXBlcnMvQmFsbGFyZDE5OTF5cS5wZGYAEwABLwAAFQACABP//wAAgAbSHyAhIlgkY2xhc3Nlc1okY2xhc3NuYW1loyIjJF1OU011dGFibGVEYXRhVk5TRGF0YVhOU09iamVjdNIfICYnoickXE5TRGljdGlvbmFyeQAIABEAGwAkACkAMgBEAEkATABRAFMAXABiAGkAdAB8AIMAhgCIAIoAjQCPAJEAkwCgAKoAxQDKANICmgKcAqECqgK1ArkCxwLOAtcC3ALfAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAuw=}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple Computer//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>To Be Read</string>
		<key>keys</key>
		<string>Schweitzer:1999kl,Porta:2006kx,Jin:2001ix,Bederson:2004xu,Urban:2003vn,MacCuish:1996yg,Rodden:2000ys,Walter:2003hi,Clough:2005hc,Assfalg:2000ao,Chen:1997tw,Cohen:1997la,Yang2006,Chen:2000gn,Qiu:2007gb,Deng:2004ja,Kustanowitz:2005kx,Tian:2000hl,Graham:2002zr,Vendrig:2001cy,Schneidewind:2004xb,Barnard2001,Heesch:2006dp,Liere:1999fp,Chen:2005er,Porta:2009zr,Zhang1995,Chen2000,Chen:2000ly,Janecek:2005zj,Chen1998,Bartolini:2006eu,Keller:2001oq,Meiers:2002fy</string>
	</dict>
</array>
</plist>
}}

@comment{BibDesk Smart Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple Computer//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>4</integer>
				<key>key</key>
				<string>Read</string>
				<key>value</key>
				<string>Yes</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Read Papers</string>
	</dict>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>4</integer>
				<key>key</key>
				<string>Read</string>
				<key>value</key>
				<string>No</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>Unread Papers</string>
	</dict>
</array>
</plist>
}}
