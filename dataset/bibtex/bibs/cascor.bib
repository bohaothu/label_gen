
@InProceedings{	  mahadevan:representation,
  title		= {Representation Policy Iteration.},
  author	= {Sridhar Mahadevan},
  booktitle	= {UAI},
  pages		= {372-379},
  publisher	= {AUAI Press},
  url		= {http://dblp.uni-trier.de/db/conf/uai/uai2005.html#Mahadevan05}
		  ,
  year		= {2005},
  biburl	= {http://www.bibsonomy.org/bibtex/2112a37e59756b5a485b082b13f14db94/dblp}
		  ,
  description	= {dblp},
  ee		= {http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=1240&proceeding_id=21}
		  ,
  isbn		= {0-9749039-1-4},
  date		= {2007-07-26},
  keywords	= {dblp }
}

@InProceedings{	  mahadevan:samuel,
  author	= {Sridhar Mahadevan},
  title		= {Samuel Meets Amarel: Automating Value Function
		  Approximation Using Global State Space Analysis},
  booktitle	= {AAAI},
  year		= {2005},
  pages		= {1000-1005},
  editor	= {Manuela M. Veloso and Subbarao Kambhampati},
  publisher	= {AAAI Press / The MIT Press},
  isbn		= {1-57735-236-X}
}

@InProceedings{	  johns.mahadevan:constructing,
  author	= {Jeff Johns and Sridhar Mahadevan},
  title		= {Constructing basis functions from directed graphs for
		  value function approximation},
  booktitle	= {ICML '07: Proceedings of the 24th international conference
		  on Machine learning},
  year		= {2007},
  isbn		= {978-1-59593-793-3},
  pages		= {385--392},
  location	= {Corvalis, Oregon},
  doi		= {http://doi.acm.org/10.1145/1273496.1273545},
  publisher	= {ACM},
  address	= {New York, NY, USA}
}

@Article{	  mahadevan.maggioni:proto-value,
  author	= {Sridhar Mahadevan and Mauro Maggioni},
  title		= {Proto-value Functions: A Laplacian Framework for Learning
		  Representation and Control in Markov Decision Processes},
  journal	= {Journal of Machine Learning Research},
  volume	= {8},
  year		= {2007},
  issn		= {1533-7928},
  pages		= {2169--2231},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA}
}

@Article{	  menache.mannor.ea:basis,
  author	= "Ishai Menache and Shie Mannor and Nahum Shimkin",
  title		= "Basis Function Adaptation in Temporal Difference
		  Reinforcement Learning",
  journal	= "Annals of Operations Research",
  volume	= "134",
  year		= "February 2005",
  abstract	= "Reinforcement Learning (RL) is an approach for solving
		  complex multi-stage decision problems that fall under the
		  general framework of Markov Decision Problems (MDPs), with
		  possibly unknown parameters. Function approximation is
		  essential for problems with a large state space, as it
		  facilitates compact representation and enables
		  generalization. Linear approximation architectures (where
		  the adjustable parameters are the weights of pre-fixed
		  basis functions) have recently gained prominence due to
		  efficient algorithms and convergence guarantees.
		  Nonetheless, an appropriate choice of basis function is
		  important for the success of the algorithm. In the present
		  paper we examine methods for adapting the basis function
		  during the learning process in the context of evaluating
		  the value function under a fixed control policy. Using the
		  Bellman approximation error as an optimization criterion,
		  we optimize the weights of the basis function while
		  simultaneously adapting the (non-linear) basis function
		  parameters. We present two algorithms for this problem. The
		  first uses a gradient-based approach and the second applies
		  the Cross Entropy method. The performance of the proposed
		  algorithms is evaluated and compared in simulations.",
  pages		= "215-238(24)",
  url		= "http://www.ingentaconnect.com/content/klu/anor/2005/00000134/00000001/00005732"
		  ,
  doi		= "doi:10.1007/s10479-005-5732-z"
}

@InProceedings{	  keller.mannor.ea:automatic,
  author	= {Philipp W. Keller and Shie Mannor and Doina Precup},
  title		= {Automatic basis function construction for approximate
		  dynamic programming and reinforcement learning},
  booktitle	= {ICML '06: Proceedings of the 23rd international conference
		  on Machine learning},
  year		= {2006},
  isbn		= {1-59593-383-2},
  pages		= {449--456},
  location	= {Pittsburgh, Pennsylvania},
  doi		= {http://doi.acm.org/10.1145/1143844.1143901},
  publisher	= {ACM},
  address	= {New York, NY, USA}
}

@PhDThesis{	  coulom:reinforcement,
  author	= "R\'emi Coulom",
  title		= "Reinforcement Learning Using Neural Networks with
		  Applications to Motor Control",
  school	= "Institut National Polytechnique de Grenoble",
  year		= 2002
}

@Book{		  sutton.barto:reinforcement,
  title		= {Reinforcement Learning: An Introduction},
  author	= {Richard S. Sutton and Andrew G. Barto},
  publisher	= {MIT Press},
  address	= {Cambridge, MA},
  year		= 1998,
  note		= {A Bradford Book},
  keywords	= {RL},
  readdate	= {2002/06/04}
}

@Article{	  lagoudakis.parr:least-squares,
  author	= {Michail G. Lagoudakis and Ronald Parr},
  title		= {Least-squares policy iteration},
  journal	= {Journal of Machine Learning Research},
  volume	= {4},
  year		= {2003},
  issn		= {1533-7928},
  pages		= {1107--1149},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA}
}

@InProceedings{	  fahlman.lebiere:cascade-correlation,
  author	= "S. E. Fahlman and C. Lebiere",
  title		= "The Cascade-Correlation Learning Architecture",
  booktitle	= "Advances in Neural Information Processing Systems",
  volume	= "2",
  publisher	= "Morgan Kaufmann, San Mateo",
  address	= "Denver 1989",
  editor	= "D. S. Touretzky",
  pages		= "524--532",
  year		= "1990",
  url		= "citeseer.ist.psu.edu/fahlman90cascadecorrelation.html"
}

@InProceedings{	  rivest.precup:combining,
  author	= {Fran\c{c}ois Rivest and Doina Precup},
  title		= {Combining TD-learning with Cascade-correlation Networks},
  booktitle	= {ICML '03: Proceedings of the 20th international conference
		  on Machine learning},
  year		= {2003},
  pages		= {632-639},
  editor	= {Tom Fawcett and Nina Mishra},
  publisher	= {AAAI Press},
  isbn		= {1-57735-189-4},
  bibsource	= {DBLP, http://dblp.uni-trier.de}
}

@InProceedings{	  parr.painter-wakefield.ea:analyzing,
  author	= {Ronald Parr and Christopher Painter-Wakefield and Lihong
		  Li and Michael Littman},
  title		= {Analyzing feature generation for value-function
		  approximation},
  booktitle	= {ICML '07: Proceedings of the 24th international conference
		  on Machine learning},
  year		= {2007},
  isbn		= {978-1-59593-793-3},
  pages		= {737--744},
  location	= {Corvalis, Oregon},
  doi		= {http://doi.acm.org/10.1145/1273496.1273589},
  publisher	= {ACM},
  address	= {New York, NY, USA}
}

@InProceedings{	  lagoudakis.parr:model-free,
  author	= {Michail G. Lagoudakis and Ronald Parr},
  title		= {Model-Free Least-Squares Policy Iteration},
  booktitle	= {NIPS},
  year		= {2001},
  pages		= {1547-1554},
  ee		= {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/CN13.ps.gz}
		  ,
  editor	= {Thomas G. Dietterich and Suzanna Becker and Zoubin
		  Ghahramani},
  publisher	= {MIT Press},
  bibsource	= {DBLP, http://dblp.uni-trier.de}
}

@InProceedings{	  riedmiller:neural,
  author	= {Martin Riedmiller},
  title		= {Neural Fitted Q Iteration - First Experiences with a Data
		  Efficient Neural Reinforcement Learning Method},
  booktitle	= {ECML},
  year		= {2005},
  pages		= {317-328},
  ee		= {http://dx.doi.org/10.1007/11564096_32},
  editor	= {Jo{\~a}o Gama and Rui Camacho and Pavel Brazdil and
		  Al\'{\i}pio Jorge and Lu\'{\i}s Torgo},
  publisher	= {Springer},
  series	= {Lecture Notes in Computer Science},
  volume	= {3720},
  isbn		= {3-540-29243-8},
  bibsource	= {DBLP, http://dblp.uni-trier.de}
}

@InProceedings{	  li.liao.ea:incremental,
  author	= {Hui Li and Xuejun Liao and Lawrence Carin},
  title		= {Incremental Least Squares Policy Iteration for POMDPs},
  booktitle	= {AAAI},
  year		= {2006},
  publisher	= {AAAI Press},
  bibsource	= {DBLP, http://dblp.uni-trier.de}
}

@InProceedings{	  mahadevan:representation*1,
  author	= "Sridhar Mahadevan ",
  title		= "Representation Policy Iteration",
  booktitle	= "Proceedings of the 21th Annual Conference on Uncertainty
		  in Artificial Intelligence (UAI-05)",
  publisher	= "AUAI Press",
  address	= "Arlington, Virginia",
  year		= "2005",
  pages		= "372-37"
}

@Book{		  bertsekas.tsitsiklis:neuro-dynamic,
  author	= {D. P. Bertsekas and J. N. Tsitsiklis},
  title		= {Neuro-Dynamic Programming},
  publisher	= {Athena Scientific},
  address	= {Belmont, MA},
  year		= {1996}
}

@InProceedings{	  munos:error,
  author	= {R{\'e}mi Munos},
  title		= {Error Bounds for Approximate Policy Iteration},
  booktitle	= {ICML '03: Proceedings of the 20th international conference
		  on Machine learning},
  year		= {2003},
  pages		= {560-567},
  publisher	= {AAAI Press},
  editor	= {Tom Fawcett and Nina Mishra},
  isbn		= {1-57735-189-4},
  bibsource	= {DBLP, http://dblp.uni-trier.de}
}

@Article{	  bradtke.barto:linear,
  author	= {Steven J. Bradtke and Andrew G. Barto},
  title		= {Linear Least-Squares Algorithms for Temporal Difference
		  Learning},
  journal	= {Machine Learning},
  volume	= {22},
  number	= {1-3},
  year		= {1996},
  pages		= {33-57},
  bibsource	= {DBLP, http://dblp.uni-trier.de}
}

@TechReport{	  bertsekas.ioffe:temporal,
  author	= {D. Bertsekas and S. Ioffe},
  title		= {Temporal differences-based policy iteration and
		  applications in neuro-dynamic programming},
  institution	= {MIT},
  year		= 1996,
  number	= {LIDS-P-2349}
}

@InProceedings{	  riedmiller.braun:direct,
  author	= { Martin Riedmiller and Heinrich Braun },
  citeulike-article-id={370368},
  journal	= {Neural Networks, 1993., IEEE International Conference on},
  pages		= {586--591 vol.1},
  priority	= {2},
  title		= {A direct adaptive method for faster backpropagation
		  learning: the RPROP algorithm},
  url		= {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=298623}
		  ,
  year		= {1993}
}
