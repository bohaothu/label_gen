@STRING{colt93	= "Proceedings of the Sixth Annual ACM Conference on
		  Computational Learning Theory" }
@STRING{jacm	= "Journal of the Association for Computing Machinery" }
@STRING{jma	= "Journal of Multivariate Analysis" }
@STRING{lncs	= "Lecture Notes of Computer Science, Springer" }
@STRING{lncs	= "Lecture Notes in Computer Science" }
@STRING{nips	= "Advances in Neural Information Processing Systems" }
@STRING{pecs	= "Colloquia Mathematica Societatis Janos Bolai, 57.\ Limit
		  Theorem in Probability and Statistics, Pecs (Hungary)" }
@STRING{spl	= "Statistics \& Probability Letters" }

@Article{AlonMaass:86,
  author	= {N. Alon and W. Maass},
  journal	= {Proceedings of the 27th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {410--417},
  title		= {Meanders, Ramsey's theorem and lower bounds for branching
		  programs},
  year		= {1986}
}

@Article{AlonMaass:88,
  author	= {N. Alon and W. Maass},
  journal	= {J. Comput. System Sci.},
  note		= {Invited paper for a special issue of J. Comput. System
		  Sci.},
  pages		= {118--129},
  title		= {Meanders and their applications in lower bound arguments},
  volume	= {37},
  year		= {1988}
}

@InProceedings{AuerETAL:01,
  author	= {P. Auer and H. Burgsteiner and W. Maass},
  title		= {Reducing Communication for Distributed Learning in Neural
		  Networks},
  booktitle	= {<a
		  href="http://www.springer.de/comp/lncs/index.html">Proc. of
		  the International Conference on Artificial Neural Networks
		  -- ICANN 2002</a>},
  series	= {Lecture Notes in Computer Science},
  editor	= {Jos\'{e} R. Dorronsoro},
  pages		= {123--128},
  volume	= {2415},
  year		= {2002},
  publisher	= {Springer},
  keywords	= {learning algorithm, perceptrons, parallel, aVLSI },
  abstract	= {A learning algorithm is presented for circuits consisting
		  of a single layer of perceptrons. We refer to such circuits
		  as parallel perceptrons. In spite of their simplicity,
		  these circuits are universal approximators for arbitrary
		  boolean and continuous functions. In contrast to backprop
		  for multi-layer perceptrons, our new learning algorithm -
		  the parallel delta rule (p-delta rule) - only has to tune a
		  single layer of weights, and it does not require the
		  computation and communication of analog values with high
		  precision. This distinguishes our new learning rule also
		  from other learning rules for such circuits such as
		  MADALINE with far higher communication. Our algorithm also
		  provides an interesting new hypothesis for the organization
		  of learning in biological neural systems. A theoretical
		  analysis shows that the p-delta rule does in fact implement
		  gradient descent - with regard to a suitable error measure
		  - although it does not require to compute derivatives.
		  Furthermore it is shown through experiments on common
		  real-world benchmark datasets that its performance is
		  competitive with that of other learning approaches from
		  neural networks and machine learning.}
}

@Article{AuerETAL:01a,
  author	= {P. Auer and H. Burgsteiner and W. Maass},
  title		= {A learning rule for very simple universal approximators
		  consisting of a single layer of perceptrons},
  journal	= {Neural Networks},
  year		= {2008},
  volume	= {21},
  number	= {5},
  pages		= {786--795},
  abstract	= {A learning algorithm is presented for circuits consisting
		  of a single layer of perceptrons (= threshold gates, or
		  equivalently gates with a Heaviside activation function).
		  We refer to such circuits as parallel perceptrons. In spite
		  of their simplicity, these circuits can compute any boolean
		  function if one views the majority of the binary perceptron
		  outputs as the binary outputs of the parallel perceptron,
		  and they are universal approximators for arbitrary
		  continuous functions with values in [0,1] if one views the
		  fraction of perceptrons that output 1 as the analog output
		  of the parallel perceptron. For a long time one has thought
		  that there exists no competitive learning algorithms for
		  these extremely simple circuits consisting of gates with
		  binary outputs, which also became known as committee
		  machines. It is commonly believed that one has to replace
		  the hard threshold gates by sigmoidal gates and that one
		  has to tune the weights on at least two successive layers
		  in order to get satisfactory learning results. We show that
		  this is not true by exhibiting a simple learning algorithm
		  for parallel perceptrons - the parallel delta rule (p-delta
		  rule), whose performance is comparable to that of backprop
		  for multilayer networks consisting of sigmoidal gates. In
		  contrast to backprop, the p-delta rule does not require the
		  computation and communication of analog values with high
		  precision, although it does in fact implement gradient
		  descent - with regard to a suitable error measure.
		  Therefore it provides an interesting new hypothesis for the
		  organization of learning in biological neural systems.}
}

@InProceedings{AuerETAL:93,
  author	= {P. Auer and P. M. Long and W. Maass and G. J. Woeginger},
  booktitle	= {Proceedings of the 5th Annual ACM Conference on
		  Computational Learning Theory},
  pages		= {392--401},
  title		= {On the complexity of function learning},
  year		= {1993}
}

@Article{AuerETAL:93j,
  author	= {P. Auer and P. M. Long and W. Maass and G. J. Woeginger},
  title		= {On the complexity of function learning},
  journal	= {Machine Learning},
  note		= {Invited paper in a special issue of Machine Learning},
  year		= {1995},
  volume	= {18},
  pages		= {187--230}
}

@InProceedings{AuerETAL:95,
  author	= {P. Auer and R. C. Holte and W. Maass},
  booktitle	= {Proc. of the 12th International Machine Learning
		  Conference, Tahoe City (USA)},
  publisher	= {Morgan Kaufmann (San Francisco)},
  pages		= {21--29},
  title		= {Theory and applications of agnostic {PAC}-learning with
		  small decision trees},
  year		= {1995}
}

@InProceedings{AuerETAL:96,
  author	= {P. Auer and S. Kwek and W. Maass and M. K. Warmuth},
  booktitle	= {Proc. of the 9th Conference on Computational Learning
		  Theory 1996},
  pages		= {333--343},
  publisher	= {ACM-Press (New York)},
  title		= {Learning of depth two neural nets with constant fan-in at
		  the hidden nodes},
  year		= {1996}
}

@Article{AuerMaass:98,
  author	= {P. Auer and W. Maass},
  title		= {Introduction to the Special Issue on Computational
		  Learning Theory},
  journal	= {Algorithmica},
  year		= {1998},
  volume	= {22},
  number	= {1/2},
  pages		= {1--2}
}

@Unpublished{BachlerMaass:06,
  author	= {M. Bachler and W. Maass},
  title		= {A {B}ayesian {H}ebb Rule for Incremental Learning of
		  Optimal Inference in {B}ayesian Networks},
  note		= {submitted for publication},
  year		= {2006}
}

@InCollection{BartlettMaass:03,
  author	= {Peter L. Bartlett and W. Maass},
  title		= {Vapnik-{C}hervonenkis Dimension of Neural Nets},
  booktitle	= {The Handbook of Brain Theory and Neural Networks},
  publisher	= {MIT Press (Cambridge)},
  year		= {2003},
  editor	= {M. A. Arbib},
  edition	= {2nd},
  pages		= {1188--1192}
}

@Article{BillETAL:10,
  author	= {J. Bill and K. Schuch and D. Br\"uderle and J. Schemmel
		  and W. Maass and K. Meier},
  title		= {Compensating inhomogeneities of neuromorphic {VLSI}
		  devices via short-term synaptic plasticity},
  journal	= {Frontiers in Computational Neuroscience},
  year		= {2010},
  volume	= {4},
  number	= {},
  pages		= {1--14},
  abstract	= {Recent developments in neuromorphic hardware engineering
		  make mixed-signal VLSI neural network models promising
		  candidates for neuroscientific research tools and massively
		  parallel computing devices, especially for tasks which
		  exhaust the computing power of software simulations. Still,
		  like all analog hardware systems, neuromorphic models
		  suffer from a constricted configurability and
		  production-related fluctuations of device characteristics.
		  Since also future systems, involving ever-smaller
		  structures, will inevitably exhibit such inhomogeneities on
		  the unit level, self-regulation properties become a crucial
		  requirement for their successful operation. By applying a
		  cortically inspired self-adjusting network architecture, we
		  show that the activity of generic spiking neural networks
		  emulated on a neuromorphic hardware system can be kept
		  within a biologically realistic firing regime and gain a
		  remarkable robustness against transistorlevel variations.
		  As a first approach of this kind in engineering practice,
		  the short-term synaptic depression and facilitation
		  mechanisms implemented within an analog VLSI model of I\&F
		  neurons are functionally utilized for the purpose of
		  network level stabilization. We present experimental data
		  acquired both from the hardware model and from comparative
		  software simulations which prove the applicability of the
		  employed paradigm to neuromorphic VLSI devices.},
  note		= {article 129}
}

@Article{BuesingETAL:11,
  author	= {L. B\"using and J. Bill and B. Nessler and W. Maass},
  title		= {Neural Dynamics as Sampling: A Model for Stochastic
		  Computation in Recurrent Networks of Spiking Neurons},
  journal	= {PLoS Computational Biology},
  year		= {2011},
  pages		= {},
  volume	= {},
  abstract	= {The organization of computations in networks of spiking
		  neurons in the brain is still largely unknown, in
		  particular in view of the inherently stochastic features of
		  their firing activity and the experimentally observed
		  trial-to-trial variability of neural systems in the brain.
		  In principle there exists a powerful computational
		  framework for stochastic computations, probabilistic
		  inference by sampling, which can explain a large number of
		  macroscopic experimental data in neuroscience and cognitive
		  science. But it has turned out to be surprisingly difficult
		  to create a link between these abstract models for
		  stochastic computations and more detailed models of the
		  dynamics of networks of spiking neurons. Here we create
		  such a link, and show that under some conditions the
		  stochastic firing activity of networks of spiking neurons
		  can be interpreted as probabilistic inference via Markov
		  chain Monte Carlo (MCMC) sampling. Since common methods for
		  MCMC sampling in distributed systems, such as Gibbs
		  sampling, are inconsistent with the dynamics of spiking
		  neurons, we introduce a different approach based on
		  non-reversible Markov chains, that is able to reflect
		  inherent temporal processes of spiking neuronal activity
		  through a suitable choice of random variables. We propose a
		  neural network model and show by a rigorous theoretical
		  analysis that its neural activity implements MCMC sampling
		  of a given distribution, both for the case of discrete and
		  continuous time. This provides a step towards closing the
		  gap between abstract functional models of cortical
		  computations and more detailed models of networks of
		  spiking neurons.},
  note		= {in press}
}

@InProceedings{BuesingMaass:07,
  author	= {L. Buesing and W. Maass},
  title		= {Simplified Rules and Theoretical Analysis for Information
		  Bottleneck Optimization and {PCA} with Spiking Neurons},
  booktitle	= {Proc. of NIPS 2007, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2008},
  volume	= {20},
  pages		= {},
  abstract	= {We show that under suitable assumptions (primarily
		  linearization) a simple and perspicuous online learning
		  rule for Information Bottleneck optimization with spiking
		  neurons can be derived. This rule performs on common
		  benchmark tasks as well as a rather complex rule that has
		  previously been proposed [2]. Furthermore, the transparency
		  of this new learning rule makes a theoretical analysis of
		  its convergence properties feasible. If this learning rule
		  is applied to an assemble of neurons, it provides a
		  theoretically founded method for performing principal
		  component analysis ({PCA}) with spiking neurons. In
		  addition it makes it possible to preferentially extract
		  those principal components from incoming signals X that are
		  related to some additional target signal $Y_T$ . This
		  target signal $Y_T$ (also called relevance variable) could
		  represent in a biological interpretation proprioception
		  feedback, input from other sensory modalities, or top-down
		  signals.}
}

@Article{BuesingMaass:09,
  author	= {Buesing, L. and Maass, W.},
  title		= {A Spiking Neuron as Information Bottleneck},
  journal	= {submitted for publication},
  year		= {2009}
}

@Article{BuesingMaass:10,
  author	= {L. Buesing and W. Maass},
  title		= {A Spiking Neuron as Information Bottleneck},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  number	= {},
  pages		= {1961--1992},
  abstract	= {Neurons receive thousands of presynaptic input spike
		  trains while emitting a single output spike train. This
		  drastic dimensionality reduction suggests to consider a
		  neuron as a bottleneck for information transmission.
		  Extending recent results, we propose a simple learning rule
		  for the weights of spiking neurons derived from the
		  Information Bottleneck (IB) framework that minimizes the
		  loss of relevant information transmitted in the output
		  spike train. In the IB framework relevance of information
		  is defined with respect to contextual information, the
		  latter entering the proposed learning rule as a "third"
		  factor besides pre- and postsynaptic activities. This
		  renders the theoretically motivated learning rule a
		  plausible model for experimentally observed synaptic
		  plasticity phenomena involving three factors. Furthermore,
		  we show that the proposed IB learning rule allows spiking
		  neurons to learn a "predictive code",i.e. to extract those
		  parts of their input that are predictive for future
		  input.}
}

@InProceedings{BultmanMaass:91,
  author	= {W. J. Bultman and W. Maass},
  booktitle	= {Proceedings of the 4th Annual ACM Workshop on
		  Computational Learning Theory,},
  pages		= {337--353},
  title		= {Fast identification of geometric objects with membership
		  queries},
  year		= {1991}
}

@Article{BultmanMaass:91j,
  author	= {W. J. Bultman and W. Maass},
  title		= {Fast identification of geometric objects with membership
		  queries},
  journal	= {Information and Computation},
  year		= 1995,
  volume	= 118,
  pages		= {48--64}
}

@Article{BuonomanoMaass:08,
  author	= {D. Buonomano and W. Maass},
  title		= {State-dependent Computations: Spatiotemporal Processing in
		  Cortical Networks.},
  journal	= {Nature Reviews in Neuroscience},
  year		= {2009},
  volume	= {10},
  number	= {2},
  pages		= {113--125},
  note		= {},
  abstract	= {A conspicuous ability of the brain is to seamlessly
		  assimilate and process spatial and temporal features of
		  sensory stimuli. This ability is indispensable for the
		  recognition of natural stimuli. Yet, a general
		  computational framework for processing spatiotemporal
		  stimuli remains elusive. Recent theoretical and
		  experimental work suggests that spatiotemporal processing
		  emerges from the interaction between incoming stimuli and
		  the internal dynamic state of neural networks which
		  includes not only ongoing spiking activity, but also
		  'hidden' neuronal states such as short-term synaptic
		  plasticity.}
}

@InProceedings{ChenMaass:92,
  author	= {Z. Chen and W. Maass},
  booktitle	= {Proceedings of the 5th Annual ACM Workshop on
		  Computational Learning Theory},
  pages		= {16--28},
  title		= {On-line learning of rectangles},
  year		= {1992}
}

@InProceedings{ChenMaass:92a,
  author	= {Z. Chen and W. Maass},
  booktitle	= {Proceedings of the 3rd Int. Workshop on Analogical and
		  Inductive Inference},
  pages		= {26--34},
  publisher	= {Springer},
  series	= {Lecture Notes in Artificial Intelligence},
  title		= {A solution of the credit assignment problem in the case of
		  learning rectangles},
  volume	= {642},
  year		= {1992}
}

@Article{ChenMaass:94,
  author	= {Z. Chen and W. Maass},
  journal	= {Machine Learning},
  note		= {Invited paper for a special issue of Machine Learning},
  pages		= {201--223},
  title		= {On-line learning of rectangles and unions of rectangles},
  volume	= {17},
  year		= {1994}
}

@Article{DietzfelbingerETAL:91a,
  author	= {M. Dietzfelbinger and W. Maass and G. Schnitger},
  journal	= {Theoretical Computer Science},
  pages		= {113--129},
  title		= {The complexity of matrix transposition on one-tape
		  off-line {T}uring machines},
  volume	= {82},
  year		= {1991}
}

@InProceedings{DietzfelbingerMaass:85,
  author	= {M. Dietzfelbinger and W. Maass},
  booktitle	= {Proceedings of the 1984 Recursion Theory Week Oberwolfach,
		  Germany},
  pages		= {89--120},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Mathematics},
  title		= {Strong reducibilities in alpha- and beta-recursion
		  theory},
  volume	= {1141},
  year		= {1985}
}

@InProceedings{DietzfelbingerMaass:86,
  author	= {M. Dietzfelbinger and W. Maass},
  booktitle	= {Proceedings of the Structure in Complexity Theory
		  Conference, Berkeley 1986},
  pages		= {163--183},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {Two lower bound arguments with ``inaccessible'' numbers},
  volume	= {223},
  year		= {1986}
}

@Article{DietzfelbingerMaass:88,
  author	= {M. Dietzfelbinger and W. Maass},
  journal	= {J. Comput. System Sci.},
  note		= {Invited paper for a special issue of J. Comput. System
		  Sci.},
  pages		= {313--335},
  title		= {Lower bound arguments with ``inaccesible'' numbers},
  volume	= {36},
  year		= {1988}
}

@InProceedings{DietzfelbingerMaass:88a,
  author	= {M. Dietzfelbinger and W. Maass},
  booktitle	= {Proceedings of the 15th International Colloquium on
		  Automata, Languages and Programming},
  pages		= {188--200},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {The complexity of matrix transposition on one-tape
		  off-line {T}uring machines with output tape},
  volume	= {317},
  year		= {1988}
}

@Article{DietzfelbingerMaass:93,
  author	= {M. Dietzfelbinger and W. Maass},
  journal	= {Theoretical Computer Science},
  pages		= {271--290},
  title		= {The complexity of matrix transposition on one-tape
		  off-line {T}uring machines with output tape},
  volume	= {108},
  year		= {1993}
}

@Article{DobkinETAL:96,
  author	= {D. P. Dobkin and D. Gunopulos and W. Maass},
  journal	= {Journal of Computer and System Sciences},
  month		= {June},
  number	= {3},
  pages		= {453--470},
  title		= {Computing the maximum bichromatic discrepancy, with
		  applications to computer graphics and machine learning},
  volume	= {52},
  year		= {1996}
}

@InProceedings{FregnacETAL:05,
  author	= {Y. Fregnac and M. Blatow and J.-P. Changeux and J. de
		  Felipe and A. Lansner and W. Maass and D. A. McCormick and
		  C. M. Michel and H. Monyer and E. Szathmary and R. Yuste},
  title		= {U{P}s and {DOWN}s in Cortical Computation},
  booktitle	= {The Interface between Neurons and Global Brain Function},
  editor	= {S. Grillner and A. M. Graybiel},
  year		= {2006},
  pages		= {393--433},
  chapter	= {19},
  publisher	= {MIT Press},
  series	= {Dahlem Workshop Report 93}
}

@Article{HaeuslerETAL:03,
  author	= {S. Haeusler and H. Markram and W. Maass},
  title		= {Perspectives of the High Dimensional Dynamics of Neural
		  Microcircuits from the Point of View of Low-Dimensional
		  Readouts},
  journal	= {Complexity (Special Issue on Complex Adaptive Systems)},
  year		= {2003},
  volume	= {8},
  number	= {4},
  pages		= {39--50}
}

@Article{HaeuslerETAL:07,
  author	= {S. Haeusler and W. Singer and W. Maass and D. Nikolic},
  title		= {Superposition of information in large ensembles of neurons
		  in primary visual cortex},
  journal	= {37th Annual Conference of the Society for Neuroscience,
		  Program 176.2, Poster II23},
  year		= {2007},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {We applied methods from machine learning in order to
		  analyze the temporal evolution of stimulus-related
		  information in the spiking activity of large ensembles of
		  around 100 neurons in primary visual cortex of anesthetized
		  cats. We present ed sequences of up to 3 different visual
		  stimuli (letters) that lasted 100 ms and followed at
		  intervals of 100 ms. We f ound that most of the information
		  about visual stimuli extractable by advanced methods from
		  machine learning (e.g., Sup port Vector Machines) could
		  also be extracted by simple linear classifiers
		  (perceptrons). Hence, in principle this info rmation can be
		  extracted by a biological neuron. A surprising result was
		  that new stimuli did not erase information abo ut previous
		  stimuli. In fact, information about the nature of the
		  preceding stimulus remained as high as the informatio n
		  about the current stimulus. Separately trained linear
		  readouts could retrieve information about both the current
		  and the preceding stimulus from responses to the current
		  stimulus. This information was encoded both in the
		  discharge rates (response amplitudes) of the ensemble of
		  neurons and in the precise timing of individual spikes, and
		  persisted for seve ral 100 ms beyond the offset of stimuli.
		  This superposition of information about sequentially
		  presented stimuli constrains computational models for
		  visual proce ssing. It poses a conundrum for models that
		  assume separate classification processes for each frame of
		  visual input and supports models for cortical computation
		  ([Buonomano, Merzenich, 1995], [Maass, Natschlaeger,
		  Markram, 2002]) which arg ue that a frame-by frame
		  processing is neither feasible within highly recurrent
		  networks nor useful for classifying and predicting rapidly
		  changing stimulus sequences. Specific predictions of these
		  alternative computational models are tha i) information
		  from different frames of visual input is superimposed in
		  recurrent circuits and ii) nonlinear combinations of
		  different information components are immediately provided
		  in the spike output. Our results indicate that the network
		  from which we recorded provided nonlinear combinations of
		  information from sequen tial frames. Such nonlinear
		  preprocessing increases the discrimination capability of
		  any linear readout neurons receivi ng distributed input
		  from the kind of cells we recorded from. These readout
		  neurons could be implemented within V1 and/ or at
		  subsequent processing levels.}
}

@Article{HaeuslerETAL:08,
  author	= {S. Haeusler and K. Schuch and W. Maass},
  title		= {Motif distribution, dynamical properties, and
		  computational performance of two data-based cortical
		  microcircuit templates},
  journal	= {J. of Physiology (Paris)},
  year		= {2009},
  volume	= {103},
  number	= {1-2},
  pages		= {73--87},
  abstract	= {The neocortex is a continuous sheet composed of rather
		  stereotypical local microcircuits that consist of neurons
		  on several laminae with characteristic synaptic
		  connectivity patterns. An understanding of the structure
		  and computational function of these cortical microcircuits
		  may hold the key for understanding the enormous
		  computational power of the neocortex. Two templates for the
		  structure of laminar cortical microcircuits have recently
		  been published by Thomson et al. and Binzegger et al., both
		  resulting from long-lasting experimental studies (but based
		  on different methods). We analyze and compare in this
		  article the structure of these two microcircuit templates.
		  In particular, we examine the distribution of network
		  motifs, i.e. of subcircuits consisting of a small number of
		  neurons. The distribution of these building blocks has
		  recently emerged as a method for characterizing
		  similarities and differences among complex networks. We
		  show that the two microcircuit templates have quite
		  different distributions of network motifs, although they
		  both have a characteristic small-world property. In order
		  to understand the dynamical and computational properties of
		  these two microcircuit templates, we have generated
		  computer models of them, consisting of Hodgkin-Huxley point
		  neurons with conductance based synapses that have a
		  biologically realistic short-term plasticity. The
		  performance of these two cortical microcircuit models was
		  studied for seven generic computational tasks that require
		  accumulation and merging of information contained in two
		  afferent spike inputs. Although the two models exhibit a
		  different performance for some of these tasks, their
		  average computational performance is very similar. When we
		  changed the connectivity structure of these two
		  microcircuit models in order to see which aspects of it are
		  essential for computational performance, we found that the
		  distribution of degrees of nodes is a common key factor for
		  their computational performance. We also show that their
		  computational performance is correlated with specific
		  statistical properties of the circuit dynamics that is
		  induced by a particular distribution of degrees of nodes.}
}

@Article{HaeuslerETAL:08a,
  author	= {S. Haeusler and K. Schuch and W. Maass},
  title		= {Motif distribution and computational performance of two
		  data-based cortical microcircuit templates},
  journal	= {38th Annual Conference of the Society for Neuroscience,
		  Program 220.9},
  year		= {2008},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {The neocortex is a continuous sheet composed of rather
		  stereotypical local microcircuits that consist of neurons
		  on several laminae with characteristic synaptic
		  connectivity patterns. An understanding of the structure
		  and computational function of these cortical microcircuits
		  may hold the key for understanding the enormous
		  computational power of the neocortex. Two templates for the
		  structure of laminar cortical microcircuits have recently
		  been published by Thomson et al. (2002) and Binzegger et
		  al. (2004), both resulting from long-lasting experimental
		  studies (but based on different methods). We analyze and
		  compare in this study the structure and computational
		  properties of these two microcircuit templates. In
		  particular, we examine the distribution of network motifs,
		  i.e. of sub-circuits consisting of a small number of
		  neurons. The distribution of these building blocks of
		  complex networks has recently emerged as a method for
		  characterizing similarities and differences among complex
		  networks. We show that the two microcircuit templates have
		  quite different distributions of network motifs, although
		  they both share characteristic global structural
		  properties, like degree distributions (distribution of the
		  number of synapses per neuron) and small-world properties.
		  In order to understand the computational properties of the
		  two microcircuit templates, we have generated computer
		  models of them, consisting of Hodgkin-Huxley point neurons
		  with conductance based synapses that have a biologically
		  realistic short-term plasticity. The information processing
		  capabilities of the two cortical microcircuit models were
		  studied for 7 generic computational tasks that require
		  accumulation and merging of information contained in two
		  afferent spike inputs. Although the two models exhibit a
		  different performance for some of these tasks, their
		  average computational performance is very similar. When we
		  changed the connectivity structure of these two
		  microcircuit models in order to see which aspects of it are
		  essential for computational performance, we found that the
		  distribution of degrees of nodes is a key factor for their
		  computational performance. References Thomson et al.
		  (2002), Cerebral Cortex, 12(9):936 Binzegger et al. (2004),
		  J. Neurosci., 24(39):8441}
}

@Article{HaeuslerMaass:04,
  author	= {S. Haeusler and W. Maass},
  title		= {A statistical analysis of information processing
		  properties of lamina-specific cortical microcircuit
		  models},
  journal	= {Cerebral Cortex},
  year		= 2007,
  volume	= {17},
  number	= {1},
  pages		= {149-162}
}

@Article{HaeuslerMaass:06,
  author	= {S. Haeusler and W. Maass},
  title		= {Computational impact of laminar structure and small world
		  properties of cortical microcircuit models},
  journal	= {submitted for publication},
  year		= 2006
}

@InProceedings{HajnalETAL:87a,
  author	= {A. Hajnal and W. Maass and P. Pudlak and M. Szegedy and G.
		  Turan},
  booktitle	= {Proceedings of the 28th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {99--110},
  title		= {Threshold circuits of bounded depth},
  year		= {1987}
}

@InProceedings{HajnalETAL:88,
  author	= {A. Hajnal and W. Maass and G. Turan},
  booktitle	= {Proceedings of the 20th Annual ACM Symposium on Theory of
		  Computing},
  pages		= {186--191},
  title		= {On the communication complexity of graph properties},
  year		= {1988}
}

@Article{HajnalETAL:93,
  author	= {A. Hajnal and W. Maass and P. Pudlak and M. Szegedy and G.
		  Turan},
  journal	= {J. Comput. System Sci.},
  pages		= {129--154},
  title		= {Threshold circuits of bounded depth},
  abstract	= {We examine a powerful model of parallel computation:
		  polynomial size threshold circuits of bounded depth (the
		  gates compute threshold functions with polynomial weights).
		  Lower bounds are given to separate polynomial size
		  threshold circuits of depth 2 from polynomial size
		  threshold circuits of depth 3 and from probabilistic
		  polynomial size circuits of depth 2. With regard to the
		  unreliability of bounded depth circuits, it is shown that
		  the class of functions computed reliably with bounded depth
		  circuits of unreliable A, v , 1 gates is narrow. On the
		  other hand, functions computable by bounded depth,
		  polynomial-size threshold circuits can also be computed by
		  such circuits of unreliable threshold gates. Furthermore we
		  examine to what extent imprecise threshold gates (which
		  behave unpredictably near the threshold value) can compute
		  nontrivial functions in bounded depth and a bound is given
		  for the permissible amount of imprecision. We also discuss
		  threshold quantifiers and prove an undefinability result
		  for graph connectivity.},
  volume	= {46},
  year		= {1993}
}

@InProceedings{HauserETAL:07,
  author	= {H. Hauser and G. Neumann and A. J. Ijspeert and W. Maass},
  booktitle	= {Proceedings of the {IEEE}-{RAS} 7th {I}nternational
		  {C}onference on {H}umanoid {R}obots ({H}umanoids 2007)},
  title		= {Biologically Inspired Kinematic Synergies Provide a New
		  Paradigm for Balance Control of Humanoid Robots},
  publisher	= {},
  year		= {2007},
  pages		= {},
  abstract	= {Nature has developed methods for controlling the movements
		  of organisms with many degrees of freedom which differ
		  strongly from existing approaches for balance control in
		  humanoid robots: Biological organisms employ kinematic
		  synergies that simultaneously engage many joints, and which
		  are apparently designed in such a way that their
		  superposition is approximately linear. We show in this
		  article that this control strategy can in principle also be
		  applied to balance control of humanoid robots. In contrast
		  to existing approaches, this control strategy reduces the
		  need to carry out complex computations in real time
		  (replacing the iterated solution of quadratic optimization
		  problems by a simple linear controller), and it does not
		  require knowledge of a dynamic model of the robot.
		  Therefore it can handle unforeseen changes in the dynamics
		  of the robot that may for example arise from wind or other
		  external forces. We demonstrate the feasibility of this
		  novel approach to humanoid balance control through
		  simulations of the humanoid robot HOAP-2 for tasks that
		  require balance control on a randomly moving surfboard.},
  note		= {Best {P}aper {A}ward.
		  http://planning.cs.cmu.edu/humanoids07/p/37.pdf}
}

@Article{HauserETAL:11,
  author	= {H. Hauser and G. Neumann and A. J. Ijspeert and W. Maass},
  title		= {Biologically Inspired Kinematic Synergies Enable Linear
		  Balance Control of a Humanoid Robot},
  journal	= {Biological Cybernetics},
  year		= {2011},
  pages		= {},
  volume	= {},
  note		= {Published online: 27 April 2011},
  abstract	= {Despite many efforts, balance control of humanoid robots
		  in the presence of unforeseen external or internal forces
		  has remained an unsolved problem. The difficulty of this
		  problem is a consequence of the high dimensionality of the
		  action space of a humanoid robot, due to its large number
		  of degrees of freedom (joints), and of nonlinearities in
		  its kinematic chains. Biped biological organisms face
		  similar difficulties, but have nevertheless solved this
		  problem. Experimental data reveal that many biological
		  organisms reduce the high dimensionality of their action
		  space by generating movements through linear superposition
		  of a rather small number of stereotypical combinations of
		  simultaneous movements of many joints, to which we refer as
		  kinematic synergies in this paper. We show that by
		  constructing two suitable nonlinear kinematic synergies for
		  the lower part of the body of a humanoid robot, balance
		  control can in fact be reduced to a linear control problem,
		  at least in the case of relatively slow movements. We
		  demonstrate for a variety of tasks that the humanoid robot
		  HOAP-2 acquires through this approach the capability to
		  balance dynamically against unforeseen disturbances that
		  may arise from external forces or from manipulating unknown
		  loads.}
}

@InProceedings{HochbaumMaass:84,
  author	= {D. Hochbaum and W. Maass},
  booktitle	= {Proceedings of Symp. on Theoretical Aspects of Computer
		  Science (Paris 1984)},
  pages		= {55--62},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {Approximation schemes for covering and packing problems in
		  robotics and {VLSI} (extended abstract)},
  volume	= {166},
  year		= {1984}
}

@Article{HochbaumMaass:85,
  author	= {D. Hochbaum and W. Maass},
  journal	= {J. Assoc. Comp. Mach.},
  pages		= {130--136},
  title		= {Approximation algorithms for covering and packing problems
		  in image processing and {VLSI}},
  volume	= {32},
  year		= {1985}
}

@Article{HochbaumMaass:87,
  author	= {D. Hochbaum and W. Maass},
  journal	= {J. Algorithms},
  pages		= {305--323},
  title		= {Fast approximation algorithms for a nonconvex problem},
  volume	= {8},
  year		= {1987}
}

@Article{HomerMaass:83,
  author	= {S. Homer and W. Maass},
  journal	= {Theoretical Computer Science},
  pages		= {279--289},
  title		= {Oracle dependent properties of the lattice of {NP}-sets},
  volume	= {24},
  year		= {1983}
}

@Article{JoshiETAL:09,
  author	= {P. Joshi and G. Rainer and W. Maass},
  title		= {Computational role of theta oscillations in
		  delayed-decision tasks},
  journal	= {in preparation},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@InProceedings{JoshiMaass:03,
  author	= {P. Joshi and W. Maass},
  title		= {Movement Generation and Control with Generic Neural
		  Microcircuits},
  booktitle	= {Biologically Inspired Approaches to Advanced Information
		  Technology. First International Workshop, Bio{ADIT} 2004,
		  Lausanne, Switzerland, January 2004, Revised Selected
		  Papers},
  pages		= {258--273},
  year		= {2004},
  editor	= {A. J. Ijspeert and M. Murata and N. Wakamiya},
  volume	= {3141},
  series	= {Lecture Notes in Computer Science},
  publisher	= {Springer Verlag},
  abstract	= {Simple linear readouts from generic neural microcircuit
		  models consisting of spiking neurons and dynamic synapses
		  can be trained to generate and control basic movements, for
		  example, reaching with an arm to various target points.
		  After suitable training of these readouts on a small number
		  of target points; reaching movements to other target points
		  can also be generated. Sensory or proprioceptive feedback
		  turns out to be essential for such movement control, even
		  if it is noisy and substantially delayed. Such feedback
		  turns out to optimally improve the performance of the
		  neural microcircuit model if it arrives with a biologically
		  realistic delay of 100 to 200 ms. Furthermore, additional
		  feedbacks of ``prediction of sensory variables'' are shown
		  to improve the performance significantly. The proposed
		  model also provides a new approach for movement control in
		  robotics. Existing control methods in robotics that take
		  the particular dynamics of the sensors and actuators into
		  account (``embodiment of robot control'') are taken one
		  step further by this approach, which provides methods for
		  also using the ``embodiment of computation'', i.e. the
		  inherent dynamics and spatial structure of neural circuits,
		  for the design of robot movement controllers.}
}

@Article{JoshiMaass:04,
  author	= {P. Joshi and W. Maass},
  journal	= {Neural Computation},
  title		= {Movement Generation with Circuits of Spiking Neurons},
  year		= {2005},
  volume	= 17,
  number	= 8,
  pages		= {1715--1738},
  abstract	= {How can complex movements that take hundreds of
		  milliseconds be generated by stereotypical neural
		  microcircuits consisting of spiking neurons with a much
		  faster dynamics? We show that linear readouts from generic
		  neural microcircuit models can be trained to generate basic
		  arm movements. Such movement generation is independent of
		  the arm-model used and the type of feedbacks that the
		  circuit receives. We demonstrate this by considering two
		  different models of a two-jointed arm, a standard model
		  from robotics and a standard model from biology, that each
		  generate different kinds of feedback. Feedbacks that arrive
		  with biologically realistic delays of 50--280 ms turn out
		  to give rise to the best performance. If a feedback with
		  such desirable delay is not available, the neural
		  microcircuit model also achieves good performance if it
		  uses internally generated estimates of such feedback.
		  Existing methods for movement generation in robotics that
		  take the particular dynamics of sensors and actuators into
		  account (``embodiment of motor systems'') are taken one
		  step further with this approach, which provides methods for
		  also using the ``embodiment of motion generation
		  circuitry'', i.e., the inherent dynamics and spatial
		  structure of neural circuits, for the generation of
		  movements.}
}

@Article{KaskeMaass:04,
  author	= {A. Kaske and W. Maass},
  title		= {A model for the interaction of oscillations and pattern
		  generation with real-time computing in generic neural
		  microcircuit models},
  journal	= {Neural Networks},
  year		= {2006},
  volume	= {19},
  number	= {5},
  pages		= {600--609},
  abstract	= {It is shown that real-time computations on spike patterns
		  and temporal integration of information in neural
		  microcircuit models are compatible with potentially
		  descruptive additional inputs such as oscillations. A minor
		  change in the connection statistics of such circuits
		  (making synaptic connections to more distal target neurons
		  more likely for excitatory than for inhibitory neurons)
		  endows such generic neural microcircuit model with the
		  ability to generate periodic patterns autonomously. We show
		  that such pattern generation can also be multiplexed with
		  pattern classification and temporal integration of
		  information in the same neural circuit. These results can
		  be interpreted as showing that periodic activity provides a
		  second channel for communication in neural systems which
		  can be used to synchronize or coordinate spatially
		  separated processes, without encumbering local real-time
		  computations on spike trains in diverse neural circuits.}
}

@Article{KlampflETAL:07,
  author	= {S. Klampfl and R. Legenstein and W. Maass},
  title		= {Spiking neurons can learn to solve information bottleneck
		  problems and extract independent components},
  journal	= {Neural Computation},
  year		= {2009},
  volume	= {21},
  number	= {4},
  pages		= {911--959},
  abstract	= {Independent Component Analysis (or blind source
		  separation) is assumed to be an essential component of
		  sensory processing in the brain and could provide a less
		  redundant representation about the external world. Another
		  powerful processing strategy is the optimization of
		  internal representations according to the information
		  bottleneck method. This method would allow to extract
		  preferentially those components from high-dimensional
		  sensory input streams that are related to other information
		  sources, such as internal predictions or proprioceptive
		  feedback. However there exists a lack of models that could
		  explain how spiking neurons could learn to execute either
		  of these two processing strategies. We show in this article
		  how stochastically spiking neurons with refractoriness
		  could in principle learn in an unsupervised manner to carry
		  out both information bottleneck optimization and the
		  extraction of independent components. We derive suitable
		  learning rules, which extend the well known BCM-rule, from
		  abstract information optimization principles. These rules
		  will simultaneously keep the firing rate of the neuron
		  within a biologically realistic range.}
}

@InProceedings{KlampflETAL:07b,
  author	= {S. Klampfl and R. Legenstein and W. Maass},
  title		= {Information Bottleneck Optimization and Independent
		  Component Extraction with Spiking Neurons},
  booktitle	= {Proc. of NIPS 2006, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2007},
  volume	= {19},
  pages		= {713--720},
  abstract	= {The extraction of statistically independent components
		  from high-dimensional multi-sensory input streams is
		  assumed to b e an essential component of sensory processing
		  in the brain. Such independent component analysis (or blind
		  source separat ion) could provide a less redundant
		  representation of inform ation about the external world.
		  Another powerful processing strategy is to extract
		  preferentially those components from high-dimensional input
		  streams that are related to other inf ormation sources,
		  such as internal predictions or propriocep tive feedback.
		  This strategy allows the optimization of inte rnal
		  representation according to the information bottleneck
		  method. However, concrete learning rules that implement
		  thes e general unsupervised learning principles for spiking
		  neuro ns are still missing. We show how both information
		  bottlenec k optimization and the extraction of independent
		  components can in principle be implemented with
		  stochastically spiking neurons with refractoriness. The new
		  learning rule that achi eves this is derived from abstract
		  information optimization principles.}
}

@Article{KlampflETAL:09,
  author	= {S. Klampfl and S.V. David and P. Yin and S.A. Shamma and
		  W. Maass},
  title		= {Integration of stimulus history in information conveyed by
		  neurons in primary auditory cortex in response to tone
		  sequences},
  journal	= {39th Annual Conference of the Society for Neuroscience,
		  Program 163.8, Poster T6 },
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {A critical component of auditory processing is integrating
		  information about sound features that change over time.
		  Previous studies have shown that the context of a sound --
		  the immediate history of auditory stimulation -- can have a
		  substantial effect on responses of auditory neurons to a
		  current sound. In order to characterize these effects, we
		  measured the information contained in the neural activity
		  in primary auditory cortex (A1) about both current and
		  preceding sounds. Neural recordings were made from single
		  A1 neurons (n=122) isolated from 23 multi-channel
		  recordings in 4 passively listening ferrets. The stimulus
		  was a sequence of tones (150 ms duration). The frequency
		  step between two consecutive tones was always half an
		  octave up or down. For each neuron, we measured at
		  particular points in time the mutual information (MI)
		  between its response during a sliding window (20ms) and the
		  identity of the current and preceding tone. Since direct
		  estimates of MI from spike trains typically suffer from a
		  systematic error (bias) due to the limited number of
		  available response trials for a given stimulus, we used a
		  recently proposed shuffling-based estimator with additional
		  quadratic extrapolation bias correction (Panzeri et al.,
		  2007). This method produces reliable information estimates
		  for this particular setup. We found that most responses
		  (102 out of 122 neurons) contained significant information
		  about the stimulus throughout the duration of the tone. Of
		  this information, on average, 60\% was about the current
		  tone, while 40\% was about the previous tone. We also
		  trained linear classifiers (Support Vector Machines with
		  linear kernel) on the low-pass filtered response spike
		  trains of multiple simultaneously recorded neurons (4-10)
		  to discriminate between the two possible predecessors for a
		  given tone. The performance the linear classifier can be
		  viewed as a lower bound on the information contained in the
		  responses about the previous tone. Performance of up to
		  80\% was achieved. These results quantify the amount of
		  information contained in the responses of A1 neurons about
		  both currently and previously played tones and demonstrate
		  that neurons in A1 integrate information about previous
		  input into their current responses. References: Panzeri et
		  al. (2007), J Neurophysiol, 98(3):1064}
}

@Article{KlampflMaass:09,
  author	= {S. Klampfl and W. Maass},
  title		= {A neuron can learn anytime classification of trajectories
		  of network states without supervision},
  journal	= {submitted for publication},
  year		= {Feb. 2009},
  volume	= {},
  number	= {},
  pages		= {},
  note		= {}
}

@Article{KlampflMaass:09a,
  author	= {S. Klampfl and W. Maass},
  title		= {A theoretical basis for emergent pattern discrimination in
		  neural systems through slow feature extraction},
  journal	= {submitted},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@InProceedings{KlampflMaass:09b,
  author	= {S. Klampfl and W. Maass},
  title		= {Replacing supervised classification learning by {S}low
		  {F}eature {A}nalysis in spiking neural networks},
  booktitle	= {Proc. of NIPS 2009: Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2010},
  volume	= {22},
  pages		= {988--996},
  abstract	= {Many models for computations in recurrent networks of
		  neurons assume that the network state moves from some
		  initial state to some fixed point attractor or limit cycle
		  that represents the output of the computation. However
		  experimental data show that in response to a sensory
		  stimulus the network state moves from its initial state
		  through a trajectory of network states and eventually
		  returns to the initial state, without reaching an attractor
		  or limit cycle in between. This type of network response,
		  where salient information about external stimuli is encoded
		  in characteristic trajectories of continuously varying
		  network states, raises the question how a neural system
		  could compute with such code, and arrive for example at a
		  temporally stable classification of the external stimulus.
		  We show that a known unsupervised learning algorithm, Slow
		  Feature Analysis (SFA), could be an important ingredient
		  for extracting stable information from these network
		  trajectories. In fact, if sensory stimuli are more often
		  followed by another stimulus from the same class than by a
		  stimulus from another class, SFA approaches the
		  classification capability of Fisher’s Linear Discriminant
		  (FLD), a powerful algorithm for supervised learning. We
		  apply this principle to simulated cortical microcircuits,
		  and show that it enables readout neurons to learn
		  discrimination of spoken digits and detection of repeating
		  firing patterns within a stream of spike trains with the
		  same firing statistics, without requiring any supervision
		  for learning.}
}

@Article{KlampflMaass:10,
  author	= {S. Klampfl and W. Maass},
  title		= {A theoretical basis for emergent pattern discrimination in
		  neural systems through slow feature extraction},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  number	= {12},
  pages		= {2979--3035},
  abstract	= {Neurons in the brain are able to detect and discriminate
		  salient spatio-temporal patterns in the firing activity of
		  presynaptic neurons. It is open how they can learn to
		  achieve this, especially without the help of a supervisor.
		  We show that a well-known unsupervised learning algorithm
		  for linear neurons, Slow Feature Analysis (SFA), is able to
		  acquire the discrimination capability of one of the best
		  algorithms for supervised linear discrimination learning,
		  the Fisher Linear Discriminant (FLD), given suitable input
		  statistics. We demonstrate the power of this principle by
		  showing that it enables readout neurons from simulated
		  cortical microcircuits to learn without any supervision to
		  discriminate between spoken digits, and to detect repeated
		  firing patterns that are embedded into a stream of noise
		  spike trains with the same firing statistics. Both these
		  computer simulations and our theoretical analysis show that
		  slow feature extraction enables neurons to extract and
		  collect information that is spread out over a trajectory of
		  firing states that lasts several hundred ms. In addition,
		  it enables neurons to learn without supervision to keep
		  track of time (relative to a stimulus onset, or the
		  initiation of a motor response). Hence these results
		  elucidate how the brain could compute with trajectories of
		  firing states, rather than only with fixed point
		  attractors. It also provides a theoretical basis for
		  understanding recent experimental results on the emergence
		  of view- and position-invariant classification of visual
		  objects in inferior temporal cortex.},
  note		= {Epub 2010 Sep 21}
}

@Article{LegensteinETAL:03,
  author	= {R. Legenstein and H. Markram and W. Maass},
  title		= {Input Prediction and Autonomous Movement Analysis in
		  Recurrent Circuits of Spiking Neurons},
  year		= {2003},
  journal	= {Reviews in the Neurosciences (Special Issue on
		  Neuroinformatics of Neural and Artificial Computation)},
  volume	= {14},
  number	= {1--2},
  pages		= {5--19},
  abstract	= {Temporal integration of information and prediction of
		  future sensory inputs are assumed to be important
		  computational tasks of generic cortical microcircuits.
		  However it has remained open how cortical microcircuits
		  could possibly achieve this, especially since they consist
		  in contrast to most neural network models of neurons and
		  synapses with heterogeneous dynamic responses. However it
		  turns out that the diversity of computational units
		  increases the capability of microcircuit models for
		  temporal integration. Furthermore the prediction of future
		  input may be rather easy for such circuits since it
		  suffices to train the readouts from such microcircuits. In
		  this article we show that very simple readouts from a
		  generic recurrently connected circuit of integrate-and-fire
		  neurons with diverse dynamic synapses can be trained in an
		  unsupervised manner to predict movements of different
		  objects, that move within an unlimited number of
		  combinations of speed, angle, and offset over a simulated
		  sensor field. The autonomously trained microcircuit model
		  is also able to compute the direction of motion, which is a
		  computationally difficult problem ("aperture problem")
		  since it requires disambiguation of local sensor readings
		  through the context of other sensor readings at the current
		  and preceding moments. Furthermore the same circuit can be
		  trained simultaneously in a supervised manner to also
		  report the shape and velocity of the moving object. Finally
		  it is shown that the trained neural circuit supports
		  novelty detection and the generation of "imagined
		  movements". Altogether the results of this article suggest
		  that it is not necessary to construct specific and
		  biologically unrealistic neural circuit models for specific
		  sensory processing tasks, since "found" generic cortical
		  microcircuit models in combination with very simple
		  perceptron-like readouts can easily be trained to solve
		  such computational tasks.}
}

@Article{LegensteinETAL:04,
  author	= {R. Legenstein and C. Naeger and W. Maass},
  title		= {What can a Neuron Learn with Spike-Timing-Dependent
		  Plasticity?},
  journal	= {Neural Computation},
  year		= {2005},
  volume	= {17},
  number	= {11},
  pages		= {2337--2382},
  abstract	= {Spiking neurons are very flexible computational modules,
		  which can implement with different values of their
		  adjustable synaptic parameters an enormous variety of
		  different transformations F from input spike trains to
		  output spike trains. We examine in this letter the question
		  to what extent a spiking neuron with biologically realistic
		  models for dynamic synapses can be taught via
		  spike-timing-dependent plasticity (STDP) to implement a
		  given transformation F. We consider a supervised learning
		  paradigm where during training, the output of the neuron is
		  clamped to the target signal (teacher forcing). The
		  well-known perceptron convergence theorem asserts the
		  convergence of a simple supervised learning algorithm for
		  drastically simplified neuron models (McCulloch-Pitts
		  neurons). We show that in contrast to the perceptron
		  convergence theorem, no theoretical guarantee can be given
		  for the convergence of STDP with teacher forcing that holds
		  for arbitrary input spike patterns. On the other hand, we
		  prove that average case versions of the perceptron
		  convergence theorem hold for STDP in the case of
		  uncorrelated and correlated Poisson input spike trains and
		  simple models for spiking neurons. For a wide class of
		  cross-correlation functions of the input spike trains, the
		  resulting necessary and sufficient condition can be
		  formulated in terms of linear separability, analogously as
		  the well-known condition of learnability by perceptrons.
		  However, the linear separability criterion has to be
		  applied here to the columns of the correlation matrix of
		  the Poisson input. We demonstrate through extensive
		  computer simulations that the theoretically predicted
		  convergence of STDP with teacher forcing also holds for
		  more realistic models for neurons, dynamic synapses, and
		  more general input distributions. In addition, we show
		  through computer simulations that these positive learning
		  results hold not only for the common interpretation of
		  STDP, where STDP changes the weights of synapses, but also
		  for a more realistic interpretation suggested by
		  experimental data where STDP modulates the initial release
		  probability of dynamic synapses.}
}

@InProceedings{LegensteinETAL:04a,
  author	= {R. Legenstein and W. Maass},
  title		= {A criterion for the convergence of learning with spike
		  timing dependent plasticity},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {Y. Weiss and B. Schoelkopf and J. Platt},
  volume	= {18},
  pages		= {763--770},
  year		= 2006,
  publisher	= {MIT Press},
  abstract	= {We investigate under what conditions a neuron can learn by
		  experimentally supported rules for spike timing dependent
		  plasticity (STDP) to predict the arrival times of strong
		  ``teacher inputs'' to the same neuron. It turns out that in
		  contrast to the famous Perceptron Convergence Theorem,
		  which predicts convergence of the perceptron learning rule
		  for a strongly simplified neuron model whenever a stable
		  solution exists, no equally strong convergence guarantee
		  can be given for spiking neurons with STDP. But we derive a
		  criterion on the statistical dependency structure of input
		  spike trains which characterizes exactly when learning with
		  STDP will converge on average for a simple model of a
		  spiking neuron. This criterion is reminiscent of the linear
		  separability criterion of the Perceptron Convergence
		  Theorem, but it applies here to the rows of a correlation
		  matrix related to the spike inputs. In addition we show
		  through computer simulations for more realistic neuron
		  models that the resulting analytically predicted positive
		  learning results not only hold for the common
		  interpretation of STDP where STDP changes the weights of
		  synapses, but also for a more realistic interpretation
		  suggested by experimental data where STDP modulates the
		  initial release probability of dynamic synapses.}
}

@InProceedings{LegensteinETAL:08,
  author	= {R. Legenstein and D. Pecevski and W. Maass},
  title		= {Theoretical Analysis of Learning with Reward-Modulated
		  Spike-Timing-Dependent Plasticity},
  booktitle	= {Proc. of NIPS 2007, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2008},
  volume	= {20},
  pages		= {881--888},
  abstract	= {Reward-modulated spike-timing-dependent plasticity
		  ({STDP}) has recently emerged as a candidate for a learning
		  rule that could explain how local learning rules at single
		  synapses support behaviorally relevant adaptive changes in
		  complex networks of spiking neurons. However the potential
		  and limitations of this learning rule could so far only be
		  tested through computer simulations. This article provides
		  tools for an analytic treatment of reward-modulated {STDP},
		  which allow us to predict under which conditions
		  reward-modulated {STDP} will be able to achieve a desired
		  learning effect. In particular, we can produce in this way
		  a theoretical explanation and a computer model for a
		  fundamental experimental finding on biofeedback in monkeys
		  (reported in [1])}
}

@Article{LegensteinETAL:08a,
  author	= {R. Legenstein and D. Pecevski and W. Maass},
  title		= {A Learning Theory for Reward-Modulated
		  Spike-Timing-Dependent Plasticity with Application to
		  Biofeedback},
  journal	= {PLoS Computational Biology},
  year		= {2008},
  volume	= {4},
  number	= {10},
  pages		= {1--27},
  abstract	= {Reward-modulated spike-timing-dependent plasticity
		  ({STDP}) has recently emerged as a candidate for a learning
		  rule that could explain how behaviorally relevant adaptive
		  changes in complex networks of spiking neurons could be
		  achieved in a self-organizing manner through local synaptic
		  plasticity. However the capabilities and limitations of
		  this learning rule could so far only be tested through
		  computer simulations. This article provides tools for an
		  analytic treatment of reward-modulated {STDP}, which allows
		  us to predict under which conditions reward-modulated
		  {STDP} will achieve a desired learning effect. These
		  analytical results imply that neurons can learn through
		  reward-modulated {STDP} to classify not only spatial, but
		  also temporal firing patterns of presynaptic neurons. They
		  also can learn to respond to specific presynaptic firing
		  patterns with particular spike patterns. Finally, the
		  resulting learning theory predicts that even difficult
		  credit-assignment problems, where it is very hard to tell
		  which synaptic weights should be modified in order to
		  increase the global reward for the system, can be solved in
		  a self-organizing manner through reward-modulated {STDP}.
		  This yields an explanation for a fundamental experimental
		  result on biofeedback in monkeys by Fetz and Baker. In this
		  experiment monkeys were rewarded for increasing the firing
		  rate of a particular neuron in the cortex, and were able to
		  solve this extremely difficult credit assignment problem.
		  Our model for this experiment relies on a combination of
		  reward-modulated {STDP} with variable spontaneous firing
		  activity. Hence it also provides a possible functional
		  explanation for trial-to-trial variability, which is
		  characteristic for cortical networks of neurons, but has no
		  analogue in currently existing artificial computing
		  systems. In addition our model demonstrates that
		  reward-modulated {STDP} can be applied to all synapses in a
		  large recurrent neural network without endangering the
		  stability of the network dynamics.}
}

@Article{LegensteinETAL:08b,
  author	= {R. Legenstein and D. Pecevski and W. Maass},
  title		= {Supplementary Information to: "{A} Learning Theory for
		  Reward-Modulated Spike-Timing-Dependent Plasticity with
		  Application to Biofeedback"},
  journal	= {PLoS Computational Biology},
  year		= {2008},
  volume	= {4},
  number	= {10},
  pages		= {}
}

@Article{LegensteinETAL:08c,
  author	= {R. Legenstein and S. A. Chase and A. B. Schwartz and W.
		  Maass},
  title		= {A model for learning effects in motor cortex that may
		  facilitate the brain control of neuroprosthetic devices},
  journal	= {38th Annual Conference of the Society for Neuroscience,
		  Program 517.6},
  year		= {2008},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {Recent experimental results have shown that the direction
		  preference of neurons in monkey motor cortex changes in
		  order to compensate for purposeful misreading of preferred
		  directions for brain control of a robot arm. We show that a
		  simple neural network model in combination with a new rule
		  for reward-modulated Hebbian plasticity can explain this
		  effect. This rule requires substantial trial-to-trial
		  variability of the neuronal output for exploration. In
		  contrast to previously proposed rules for reward-modulated
		  Hebbian plasticity, the new rule does not require that the
		  plasticity mechanism `knows' the noise explicitly. It is
		  able to optimize the performance of the model system within
		  biologically realistic periods of time and under high noise
		  levels. When the neuronal noise is fitted to experimental
		  data, the model produces learning effects similar to those
		  found in monkey experiments. We quantified these effects
		  and found a surprisingly good match to those observed in
		  experiments. This study shows that reward-modulated
		  learning can explain detailed experimental results about
		  neuronal tuning changes in a motor control task and
		  suggests that reward-modulated learning is an essential
		  plasticity mechanism in the cortex for the acquisition of
		  goal-directed behavior. Self-tuning effects of the type
		  considered in this model are obviously important for
		  successful use of neuroprosthetic devices.}
}

@InProceedings{LegensteinETAL:09a,
  author	= {R. Legenstein and S. A. Chase and A. B. Schwartz and W.
		  Maass},
  title		= {Functional network reorganization in motor cortex can be
		  explained by reward-modulated {H}ebbian learning},
  booktitle	= {Proc. of NIPS 2009: Advances in Neural Information
		  Processing Systems},
  editor	= {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  publisher	= {MIT Press},
  year		= {2010},
  volume	= {22},
  pages		= {1105--1113},
  abstract	= {The control of neuroprosthetic devices from the activity
		  of motor cortex neurons benefits from learning effects
		  where the function of these neurons is adapted to the
		  control task. It was recently shown that tuning properties
		  of neurons in monkey motor cortex are adapted selectively
		  in order to compensate for an erroneous interpretation of
		  their activity. In particular, it was shown that the tuning
		  curves of those neurons whose preferred directions had been
		  misinterpreted changed more than those of other neurons. In
		  this article, we show that the experimentally observed
		  self-tuning properties of the system can be explained on
		  the basis of a simple learning rule. This learning rule
		  utilizes neuronal noise for exploration and performs
		  Hebbian weight updates that are modulated by a global
		  reward signal. In contrast to most previously proposed
		  reward-modulated Hebbian learning rules, this rule does not
		  require extraneous knowledge about what is noise and what
		  is signal. The learning rule is able to optimize the
		  performance of the model system within biologically
		  realistic periods of time and under high noise levels. When
		  the neuronal noise is fitted to experimental data, the
		  model produces learning effects similar to those found in
		  monkey experiments.}
}

@Article{LegensteinETAL:09b,
  author	= {R. Legenstein and S. M. Chase and A. B. Schwartz and W.
		  Maass},
  title		= {A reward-modulated {H}ebbian learning rule can explain
		  experimentally observed network reorganization in a brain
		  control task},
  journal	= {The Journal of Neuroscience},
  year		= {2010},
  volume	= {30},
  number	= {25},
  pages		= {8400--8410},
  abstract	= {It has recently been shown in a brain-computer interface
		  experiment that motor cortical neurons change their tuning
		  properties selectively to compensate for errors induced by
		  displaced decoding parameters. In particular, it was shown
		  that the three-dimensional tuning curves of neurons whose
		  decoding parameters were reassigned changed more than those
		  of neurons whose decoding parameters had not been
		  reassigned. In this article, we propose a simple learning
		  rule that can reproduce this effect. Our learning rule uses
		  Hebbian weight updates driven by a global reward signal and
		  neuronal noise. In contrast to most previously proposed
		  learning rules, this approach does not require extrinsic
		  information to separate noise from signal. The learning
		  rule is able to optimize the performance of a model system
		  within biologically realistic periods of time under high
		  noise levels. Furthermore, when the model parameters are
		  matched to data recorded during the brain-computer
		  interface learning experiments described above, the model
		  produces learning effects strikingly similar to those found
		  in the experiments.}
}

@TechReport{LegensteinMaass:04,
  author	= {R. Legenstein and W. Maass},
  title		= {Additional material to the paper: What can a Neuron Learn
		  with Spike-Timing-Dependent Plasticity?},
  institution	= {Institute for Theoretical Computer Science, Graz
		  University of Technology},
  htmlnote	= {(<a href="./psfiles/154suplement.pdf">PDF</a>)},
  year		= {2004}
}

@InCollection{LegensteinMaass:05,
  author	= {R. Legenstein and W. Maass},
  title		= {What makes a dynamical system computationally powerful?},
  booktitle	= {New Directions in Statistical Signal Processing: From
		  Systems to Brains},
  publisher	= {MIT Press},
  editor	= {S. Haykin and J. C. Principe and T.J. Sejnowski and J.G.
		  McWhirter},
  pages		= {127--154},
  year		= {2007},
  abstract	= {We review methods for estimating the computational
		  capability of a complex dynamical system. The main examples
		  that we discuss are models for cortical neural
		  microcircuits with varying degrees of biological accuracy,
		  in the context of online computations on complex input
		  streams. We address in particular the question to what
		  extent earlier results ab out the relationship between the
		  edge of chaos and the compu tational power of dynamical
		  systems in discrete time for off -line computing also apply
		  to this case.}
}

@Article{LegensteinMaass:05a,
  author	= {R. Legenstein and W. Maass},
  title		= {Edge of Chaos and Prediction of Computational Performance
		  for Neural Circuit Models},
  journal	= {Neural Networks},
  year		= {2007},
  volume	= {20},
  number	= {3},
  pages		= {323--334},
  note		= {},
  abstract	= {We analyze in this article the significance of the edge of
		  chaos for real-time computations in neural microcircuit
		  models consisting of spiking neurons and dynamic synapses.
		  We find that the edge of chaos predicts quite well those
		  values of circuit parameters that yield maximal
		  computational performance. But obviously it makes no
		  prediction of their computational performance for other
		  parameter values. Therefore, we propose a new method for
		  predicting the computational performance of neural
		  microcircuit models. The new measure estimates directly the
		  kernel property and the generalization capability of a
		  neural microcircuit.We validate the proposed measure by
		  comparing its prediction with direct evaluations of the
		  computational performance of various neural microcircuit
		  models. The proposed method also allows us to quantify
		  differences in the computational performance and
		  generalization capability of neural circuits in different
		  dynamic regimes (UP- and DOWN-states) that have been
		  demonstrated through intracellular recordings in vivo.}
}

@Article{LegensteinMaass:07,
  author	= {R. Legenstein and W. Maass},
  title		= {On the classification capability of sign-constrained
		  perceptrons},
  journal	= {Neural Computation},
  volume	= {20},
  number	= {1},
  pages		= {288--309},
  year		= {2008},
  abstract	= {The perceptron (also referred to as McCulloch-Pitts
		  neuron, or linear threshold gate) is commonly used as a
		  simplified model for the discrimination and learning
		  capability of a biological neuron. Criteria that tell us
		  when a perceptron can implement (or learn to implement) all
		  possible dichotomies over a given set of input patterns are
		  well-known, but only for the idealized case where one
		  assumes that the sign of a synaptic weight can be switched
		  during learning. We present in this article an analysis of
		  the classification capability of the biologically more
		  realistic model of a sign-constrained perceptron, where the
		  signs of synaptic weights remain fixed during learning
		  (which is the case for most types of biological synapses).
		  In particular, the VC-dimension of sign-constrained
		  perceptrons is determined, and a necessary and sufficient
		  criterion is provided that tells us when all $2^m$
		  dichotomies over a given set of m patterns can be learned
		  by sign-constrained perceptron. We also show that
		  uniformity of {L1} norms of input patterns is a sufficient
		  condition for full representation power in the case where
		  all weights are required to be nonnegative. Finally, we
		  also exhibit cases where the sign-constraint of a
		  perceptron drastically reduces its classification
		  capability. Our theoretical analysis is complemented by
		  computer simulations, which demonstrate in particular that
		  sparse input patterns improve the classification capability
		  of sign-constrained perceptrons.}
}

@Article{LegensteinMaass:09,
  author	= {R. Legenstein and W. Maass},
  title		= {An integrated learning rule for branch strength
		  potentiation and {STDP}},
  journal	= {39th Annual Conference of the Society for Neuroscience,
		  Program 895.20, Poster HH36},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {Recent experimental data (Losonczy, Makara, and Magee,
		  Nature 2008) show that not only the strength of synaptic
		  efficacy is plastic, but also the coupling between
		  dendritic branches and the soma (via dendritic spikes).
		  More precisely, the strength of this coupling can be
		  increased both through a coincidence of dendritic branch
		  activations with action potential generation, and through a
		  coincidence of branch activation with ACh. This effect has
		  been called Branch Strength Potentiation (BSP). We show
		  through theoretical analysis and computer simulations that
		  the learning capability of single neurons is substantially
		  increased if STDP is combined with BSP. More precisely, we
		  show that a simple learning rule, based on a
		  error-minimization principle, contains both BSP and STDP as
		  special cases. The learning rule includes a homeostatic
		  mechanism which acts locally at the site of the dendritic
		  branch. The depression that was observed for
		  post-before-pre pairings in standard STDP experiments is
		  also observed in simulations of this learning rule. It can
		  be explained by the combined effect of this local
		  homeostatic mechanism and the backpropagating action
		  potential. This powerful new learning rule endows single
		  neurons with learning capabilities which were previously
		  unattainable. For example, a single neuron acquires through
		  this new learning rule the capability to solve a "binding
		  problem". I.e., a single neuron can learn to respond to
		  fire upon activation of presynaptic pools A and B, and also
		  upon activation of presynaptic pools C and D, but NOT in
		  response to concurrent activation of presynaptic pools A
		  and C, or B and D. We also consider a variation of this
		  learning rule where changes at synapses and branches are
		  not only based on local activity, but also on a global
		  reward signal that is indicated to the neuron by the
		  concentration of a neuromodulatory signal such as ACh. We
		  show that this biologically plausible learning rule for
		  reward-based learning is much more efficient than
		  previously proposed rules based on simple neuron models
		  without nonlinear branches.}
}

@Article{LegensteinMaass:11,
  author	= {R. Legenstein and W. Maass},
  title		= {Branch-specific plasticity enables self-organization of
		  nonlinear computation in single neurons},
  journal	= {The Journal of Neuroscience},
  year		= {2011},
  volume	= {31},
  number	= {30},
  pages		= {10787--10802},
  abstract	= {It has been conjectured that nonlinear processing in
		  dendritic branches endows individual neurons with the
		  capability to perform complex computational operations that
		  are needed in order to solve for example the binding
		  problem. However, it is not clear how single neurons could
		  acquire such functionality in a self-organized manner,
		  since most theoretical studies of synaptic plasticity and
		  learning concentrate on neuron models without nonlinear
		  dendritic properties. In the meantime, a complex picture of
		  information processing with dendritic spikes and a variety
		  of plasticity mechanisms in single neurons has emerged from
		  experiments. In particular, new experimental data on
		  dendritic branch strength potentiation in rat hippocampus
		  have not yet been incorporated into such models. In this
		  article, we investigate how experimentally observed
		  plasticity mechanisms, such as depolarization-dependent
		  STDP and branch-strength potentiation could be integrated
		  to self-organize nonlinear neural computations with
		  dendritic spikes. We provide a mathematical proof that in a
		  simplified setup these plasticity mechanisms induce a
		  competition between dendritic branches, a novel concept in
		  the analysis of single neuron adaptivity. We show via
		  computer simulations that such dendritic competition
		  enables a single neuron to become member of several
		  neuronal ensembles, and to acquire nonlinear computational
		  capabilities, such as for example the capability to bind
		  multiple input features. Hence our results suggest that
		  nonlinear neural computation may self-organize in single
		  neurons through the interaction of local synaptic and
		  dendritic plasticity mechanisms.},
  note		= {}
}

@Article{LiebeETAL:09,
  author	= {S. Liebe and G. Hoerzer and N.K. Logothetis and W. Maass
		  and G. Rainer},
  title		= {Long range coupling between {V4} and {PF} in theta band
		  during visual short-term memory},
  journal	= {39th Annual Conference of the Society for Neuroscience,
		  Program 652.20, Poster Y31},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {Both extrastriate area V4 and the lateral prefrontal
		  cortex (PF) are thought to be part of a neural network
		  contributing to sensory and mnemonic processing of visual
		  information. However, it is not well understood how V4 and
		  PF might interact during visual memory. Here, we addressed
		  this question by recording Local Field Potentials (LFP)
		  simultaneously in both brain regions while two rhesus
		  monkeys performed a delayed matching-to sample task. In the
		  task, a sample stimulus (250ms) was presented followed by a
		  probe stimulus (600ms) after a delay period (1500ms). A
		  lever press was required if the sample stimulus matched the
		  probe. We assessed coupling between LFP sites within and
		  between the different brain regions by both measuring
		  pair-wise phase-synchrony (phase locking value, PLV) using
		  a wavelet based method and employing a coupling measure
		  that relies on the concept of Granger causality
		  (partial-directed coherence; PDC) using multivariate
		  autoregressive (MVAR) modeling. In both monkeys we
		  consistently found increases in theta-band phase synchrony
		  (3.5-7 Hz) between V4 and PF LFP site pairs during the
		  delay period of the task. Specifically, a significant
		  proportion of pairs (26.1\%, 62/231 for monkey 1 and 25\%,
		  40/160 for monkey 2, p<0.001) showed increased coherence
		  during the delay phase compared to the pre-stimulus
		  baseline period. In contrast, only a small proportion of
		  sites showed significant coupling in gamma (42-97 Hz,
		  5.9\%/13\% for monkeys 1/2, respectively) or beta (16-36Hz,
		  6.9\%/16\%) frequencies. In addition, we obtained
		  comparable results using PDC, which also assesses the
		  directionality of information flow between the brain areas.
		  Our preliminary results indicate that the interaction
		  between V4 and PF during short-term memory might be
		  primarily mediated through neuronal coherence in the theta
		  band. Furthermore, our analyses using MVAR modeling suggest
		  that this interaction can be characterized by a
		  bidirectional information flow between these areas. These
		  findings support the idea that long-range interactions play
		  an important role in short-term maintenance of short-term
		  memory.}
}

@InCollection{Maass:00,
  author	= {W. Maass},
  title		= {Spike trains -- Im {R}hythmus neuronaler {Z}ellen},
  booktitle	= {Katalog der steirischen Landesausstellung gr2000az},
  pages		= {36--42},
  publisher	= {Springer Verlag},
  year		= {2000},
  editor	= {H. Konrad, R. Kriesche}
}

@InCollection{Maass:00a,
  author	= {W. Maass},
  title		= {Lernende {M}aschinen},
  booktitle	= {Katalog der steirischen Landesausstellung gr2000az},
  pages		= {50--56},
  publisher	= {Springer Verlag},
  year		= 2000,
  editor	= {H. Konrad, R. Kriesche}
}

@InCollection{Maass:00b,
  author	= {W. Maass},
  title		= {Neural computation: a research topic for theoretical
		  computer science? {S}ome thoughts and pointers},
  booktitle	= {Current Trends in Theoretical Computer Science, Entering
		  the 21th Century},
  publisher	= {World Scientific Publishing},
  year		= {2001},
  pages		= {680--690},
  editor	= {Rozenberg G. and Salomaa A. and Paun G.}
}

@InProceedings{Maass:00c,
  author	= {W. Maass},
  title		= {Neural computation: a research topic for theoretical
		  computer science? {S}ome thoughts and pointers},
  booktitle	= {Bulletin of the European Association for Theoretical
		  Computer Science (EATCS)},
  pages		= {149--158},
  year		= {2000},
  volume	= {72}
}

@InProceedings{Maass:01a,
  author	= {W. Maass},
  title		= {wetware ({E}nglish version)},
  booktitle	= {{TAKEOVER}: {W}ho is {D}oing the {A}rt of {T}omorrow
		  ({A}rs {E}lectronica 2001)},
  pages		= {148--152},
  year		= {2001},
  publisher	= {Springer}
}

@InProceedings{Maass:01b,
  author	= {W. Maass},
  title		= {wetware (deutsche {V}ersion)},
  booktitle	= {{TAKEOVER}: {W}ho is {D}oing the {A}rt of {T}omorrow
		  ({A}rs {E}lectronica 2001)},
  year		= {2001},
  pages		= {153--157},
  publisher	= {Springer}
}

@Article{Maass:02,
  author	= {W. Maass},
  title		= {Computing with Spikes},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  year		= {2002},
  pages		= {32--36},
  volume	= {8},
  number	= {1}
}

@InProceedings{Maass:02a,
  author	= {W. Maass},
  title		= {On the Computational Power of Neural Microcircuit Models:
		  Pointers to the Literature},
  booktitle	= {<a
		  href="http://www.springer.de/comp/lncs/index.html">Proc. of
		  the International Conference on Artificial Neural Networks
		  -- ICANN 2002</a>},
  pages		= {254--256},
  year		= {2002},
  editor	= {Jos\'{e} R. Dorronsoro},
  volume	= {2415},
  series	= {Lecture Notes in Computer Science},
  publisher	= {Springer}
}

@InCollection{Maass:03,
  author	= {W. Maass},
  title		= {Computation with Spiking Neurons},
  booktitle	= {<a href="http://mitpress.mit.edu/0262011972">The Handbook
		  of Brain Theory and Neural Networks</a>},
  edition	= {2nd},
  publisher	= {MIT Press (Cambridge)},
  editor	= {M. A. Arbib},
  year		= {2003},
  pages		= {1080--1083}
}

@Article{Maass:06,
  author	= {W. Maass},
  title		= {Book Review of "{I}mitation of life: how biology is
		  inspiring computing" by {N}ancy {F}orbes},
  journal	= {Pattern Analysis and Applications},
  year		= {2006},
  volume	= {8},
  number	= {4},
  pages		= {390--391},
  note		= {Springer (London)}
}

@InProceedings{Maass:07,
  author	= {W. Maass},
  booktitle	= {Proceedings of the Conference CiE'07: {COMPUTABILITY IN
		  EUROPE} 2007, Siena (Italy)},
  title		= {Liquid Computing},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  volume	= {},
  year		= {2007},
  pages		= {507--516},
  abstract	= {This review addresses structural differences between that
		  type of computation on which computability theory and
		  computational complexity theory have focused so far, and
		  those computations that are usually carried out in
		  biological organisms (either in the brain, or in the form
		  of gene regulation within a single cell). These differences
		  concern the role of time, the way in which the input is
		  presented, the way in which an algorithm is implemented,
		  and in the end also the definition of what a computation
		  is. This article describes liquid computing as a new
		  framework for analyzing those types of computations that
		  are usually carried out in biological organisms.}
}

@InCollection{Maass:09,
  author	= {W. Maass},
  title		= {Liquid State Machines: Motivation, Theory, and
		  Applications},
  booktitle	= {Computability in Context: Computation and Logic in the
		  Real World},
  editor	= {B. Cooper and A. Sorbi},
  year		= {2010},
  publisher	= {Imperial College Press},
  pages		= {275--296},
  keywords	= {},
  note		= {},
  abstract	= {The Liquid State Machine (LSM) has emerged as a
		  computational model that is more adequate than the Turing
		  machine for describing computations in biological networks
		  of neurons. Characteristic features of this new model are
		  (i) that it is a model for adaptive computational systems,
		  (ii) that it provides a method for employing randomly
		  connected circuits, or eve "found" physical objects for
		  meaningful computations, (iii) that it provides a
		  theoretical context where heterogeneous, rather than
		  stereotypical, local gates or processors increase the
		  computational power of a circuit, (iv) that it provides a
		  method for multiplexing different computations (on a common
		  input) within the same circuit. This chapter reviews the
		  motivation for this model, its theoretical background, and
		  current work on implementations of this model in innovative
		  artificial computing devices.}
}

@InProceedings{Maass:75,
  author	= {W. Maass},
  booktitle	= {Proof Theory Symposium Kiel 1974},
  editor	= {J. Diller and G. H. Mueller},
  journal	= {Lecture Notes in Mathematics, Springer (Berlin)},
  pages		= {257--263},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Mathematics},
  title		= {Church Rosser Theorem fuer Lambda-Kalkuele mit unendlich
		  langen Termen},
  volume	= {500},
  year		= {1975}
}

@Article{Maass:76,
  author	= {W. Maass},
  journal	= {Archive Math. Logik Grundlagen},
  pages		= {27--46},
  title		= {Eine {F}unktionalinterpretation der praedikativen
		  {A}nalysis},
  volume	= {18},
  year		= {1976}
}

@Article{Maass:77,
  author	= {W. Maass},
  journal	= {Archive Math. Logik Grundlagen},
  pages		= {169--186},
  title		= {On minimal pairs and minimal degrees in higher recursion
		  theory},
  volume	= {18},
  year		= {1977}
}

@Article{Maass:78,
  author	= {W. Maass},
  journal	= {Ann. of Math. Logic},
  pages		= {149--170},
  title		= {Inadmissibility, tame r.e. sets and the admissible
		  collapse},
  volume	= {13},
  year		= {1978}
}

@Article{Maass:78a,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {270-279},
  title		= {The uniform regular set theorem in alpha-recursion
		  theory},
  volume	= {43},
  year		= {1978}
}

@InProceedings{Maass:78b,
  author	= {W. Maass},
  booktitle	= {Higher Set Theory},
  editor	= {G. H. Mueller and D. Scott},
  pages		= {339--359},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Mathematics},
  title		= {Fine structure theory of the constructible universe in
		  alpha- and beta-recursion theory},
  volume	= {669},
  year		= {1978}
}

@MastersThesis{Maass:78c,
  author	= {W. Maass},
  note		= {Minerva Publikation (Muenchen)},
  school	= {Ludwig-Maximilians-Universitaet Muenchen},
  title		= {Contributions to alpha- and beta-recursion theory},
  type		= {Habilitationsschrift},
  year		= {1978}
}

@Conference{GuptaMaass:91,
  author	= {A. Gupta and W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {R. P. Lippmann and J. E. Moody and D. S. Touretzky},
  pages		= {825--831},
  publisher	= {Morgan Kaufmann, (San Mateo)},
  title		= {A method for the efficient design of {B}oltzmann machines
		  for classification problems},
  volume	= {3},
  year		= {1991}
}

@InCollection{Maass:78d,
  author	= {W. Maass},
  booktitle	= {Generalized Recursion Theory II},
  editor	= {E. Fenstad and R. O. Gandy and G. E. Sacks},
  pages		= {239--269},
  publisher	= {North-Holland (Amsterdam)},
  title		= {High alpha-recursively enumerable degrees},
  year		= {1978}
}

@Article{Maass:79,
  author	= {W. Maass},
  journal	= {Ann. of Math. Logic},
  pages		= {205--231},
  title		= {On alpha- and beta-recursively enumerable degrees},
  volume	= {16},
  year		= {1979}
}

@InProceedings{Maass:81,
  author	= {W. Maass},
  booktitle	= {Proceedings of the Conf. on Recursion Theory and
		  Computational Complexity},
  editor	= {G. Lolli},
  pages		= {229--236},
  publisher	= {Liguori editore (Napoli)},
  title		= {Recursively invariant beta-recursion theory -- a
		  preliminary survey},
  year		= {1981}
}

@Article{Maass:81a,
  author	= {W. Maass},
  journal	= {Proceedings Amer. Math. Soc.},
  pages		= {267--270},
  title		= {A countable basis for sigma-one-two sets and recursion
		  theory on aleph-one},
  volume	= {82},
  year		= {1981}
}

@Article{Maass:81b,
  author	= {W. Maass},
  journal	= {Ann. of Math. Logic},
  pages		= {27--73},
  title		= {Recursively invariant beta-recursion theory},
  volume	= {21},
  year		= {1981}
}

@Article{Maass:83,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {809--823},
  title		= {Recursively enumerable generic sets},
  volume	= {47},
  year		= {1983}
}

@Article{Maass:83a,
  author	= {W. Maass},
  journal	= {Trans. Amer. Math. Soc.},
  pages		= {311--336},
  title		= {Characterization of recursively enumerable sets with
		  supersets effectively isomorphic to all recursively
		  enumerable sets},
  volume	= {279},
  year		= {1983}
}

@Article{Maass:84,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {51--62},
  title		= {On the orbits of hyperhypersimple sets},
  volume	= {49},
  year		= {1984}
}

@InProceedings{Maass:84a,
  author	= {W. Maass},
  booktitle	= {Proceedings of 16th Annual ACM Symp. on Theory of
		  Computing},
  pages		= {401--408},
  title		= {Quadratic lower bounds for deterministic and
		  nondeterministic one-tape {T}uring machines},
  abstract	= {We introduce new techniques for proving quadratic lower
		  bounds for deterministic and nondeterministic i-tape Turing
		  machines (all considered Turing machines have an additional
		  oneway input tape). In particular we produce quadratic
		  lower bounds for the simulation of 2-tape TM's by l-tape
		  TM's and thus answer a rather old question (problem No.1
		  and No.7 in the l i s t of Duris, Galil, Paul, Reischuk
		  [3]). Further we demo6strate a substantial superiority of
		  nondeterminism over determinism and of co-nondeterminism
		  over nondeterminism for l-tape TM's.},
  year		= {1984}
}

@Article{Maass:85,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {138--148},
  title		= {Variations on promptly simple sets},
  volume	= {50},
  year		= {1985}
}

@Article{Maass:85a,
  author	= {W. Maass},
  journal	= {Proceedings of Symposia in Pure Mathematics},
  pages		= {21--32},
  title		= {Major subsets and automorphisms of recursively enumerable
		  sets},
  volume	= {42},
  year		= {1985}
}

@Article{Maass:85b,
  author	= {W. Maass},
  journal	= {Transactions of the American Mathematical Society},
  pages		= {675--693},
  title		= {Combinatorial lower bound arguments for deterministic and
		  nondeterministic {T}uring machines},
  abstract	= {We introduce new techniques for proving quadratic lower
		  bounds for deterministic and nondeterminisitc 1-tape
		  {T}uring machines (all considered {T}uring machines have an
		  additional one-way input tape). In particular, we derive
		  for the simulation of 2-tape {T}uring machines by 1-tape
		  {T}uring machines an optimal quadratic lower bound in the
		  deterministic case and a nearly optimal lower bound in the
		  nonderterministic case. This answers the rather old
		  question whether the computing power of the considered
		  types of {T}uring machines is significantly increased when
		  more than one tape is used (problem Nos. 1 and 7 in the
		  list of {D}uris, {G}alil, {P}aul, {R}eischuk [3]). Further,
		  we demonstrate a substantial superiority of nonderterminism
		  over determinism and of co-nondeterminism over
		  nondeterminism for 1-tage {T}uring machines},
  volume	= {292},
  number	= {2},
  year		= {1985},
  note		= {hard copy}
}

@InProceedings{Maass:86,
  author	= {W. Maass},
  booktitle	= {Proceedings of the International Conference on Logic,
		  Methodology and Philosphy of Science, Salzburg 1983},
  pages		= {141--158},
  publisher	= {North-Holland (Amsterdam)},
  title		= {Are recursion theoretic arguments useful in complexity
		  theory},
  year		= {1986}
}

@Article{Maass:86a,
  author	= {W. Maass},
  journal	= {SIAM J. Computing},
  pages		= {453--467},
  title		= {On the complexity of nonconvex covering},
  volume	= {15},
  year		= {1986}
}

@Article{Maass:88,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {1098--1109},
  title		= {On the use of inaccessible numbers and order
		  indiscernibles in lower bound arguments for random access
		  machines},
  volume	= {53},
  year		= {1988}
}

@InProceedings{Maass:91,
  author	= {W. Maass},
  booktitle	= {Proceedings of the 4th Annual ACM Workshop on
		  Computational Learning Theory},
  pages		= {167--175},
  publisher	= {Morgan Kaufmann (San Mateo)},
  title		= {On-line learning with an oblivious environment and the
		  power of randomization},
  year		= {1991}
}

@InProceedings{Maass:93,
  author	= {W. Maass},
  booktitle	= {Proceedings of the 25th Annual ACM Symposium on Theory
		  Computing},
  pages		= {335-344},
  title		= {Bounds for the computational power and learning complexity
		  of analog neural nets},
  year		= {1993}
}

@Article{Maass:93j,
  author	= {W. Maass},
  title		= {Bounds for the computational power and learning complexity
		  of analog neural nets},
  journal	= {SIAM J. on Computing},
  year		= 1997,
  volume	= 26,
  number	= 3,
  pages		= {708--732}
}

@InProceedings{Maass:94,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {311--318},
  title		= {Agnostic {PAC}-learning of functions on analog neural
		  nets},
  editors	= {G. Tesauro and D. S. Touretzky and T. K. Leen},
  volume	= {7},
  year		= {1995}
}

@InProceedings{Maass:94a,
  author	= {W. Maass},
  booktitle	= {Proceedings of the International Conference on Artificial
		  Neural Networks 1994 (ICANN'94)},
  pages		= {581--584},
  publisher	= {Springer (Berlin)},
  title		= {Neural nets with superlinear {VC}-dimension},
  year		= {1994}
}

@InCollection{Maass:94b,
  author	= {W. Maass},
  booktitle	= {Theoretical Advances in Neural Computation and Learning},
  editor	= {V. P. Roychowdhury and K. Y. Siu and A. Orlitsky},
  pages		= {295-336},
  publisher	= {Kluwer Academic Publishers (Boston)},
  title		= {Perspectives of current research about the complexity of
		  learning on neural nets},
  year		= {1994}
}

@InProceedings{Maass:94c,
  author	= {W. Maass},
  booktitle	= {Theoretical Andvances in Neural Computation and Learning},
  editor	= {V. P. Roychowdhury and K. Y. Siu and A. Orlitsky},
  pages		= {153--172},
  publisher	= {Kluwer Academics Publisher (Boston)},
  title		= {Computing on analog neural nets with arbitrary real
		  weights},
  year		= {1994}
}

@InProceedings{Maass:94d,
  author	= {W. Maass},
  booktitle	= {Proc. of the 7th Annual ACM Conference on Computational
		  Learning Theory},
  pages		= {67--75},
  title		= {Efficient agnostic {PAC}-learning with simple hypotheses},
  year		= {1994}
}

@InCollection{Maass:94e,
  author	= {W. Maass},
  booktitle	= {Computational Learning Theory: EuroColt'93},
  editor	= {J. Shawe-Taylor and M. Anthony},
  pages		= {1--17},
  publisher	= {Oxford University Press (Oxford)},
  title		= {On the complexity of learning on neural nets},
  year		= {1994}
}

@Article{Maass:94f,
  author	= {W. Maass},
  title		= {Agnostic {PAC}-learning of functions on analog neural
		  nets},
  journal	= {Neural Computation},
  year		= 1995,
  volume	= 7,
  pages		= {1054--1078}
}

@Article{Maass:94j,
  author	= {W. Maass},
  title		= {Neural nets with superlinear {VC}-dimension},
  journal	= {Neural Computation},
  year		= 1994,
  volume	= 6,
  pages		= {877--884}
}

@InProceedings{Maass:95,
  author	= {W. Maass},
  booktitle	= {Proc. of the 7th Italian Workshop on Neural Nets 1995},
  pages		= {99--104},
  publisher	= {World Scientific (Singapore)},
  title		= {Analog computations on networks of spiking neurons
		  (extended abstract)},
  year		= {1996}
}

@InCollection{Maass:95a,
  author	= {W. Maass},
  booktitle	= {The Handbook of Brain Theory and Neural Networks},
  editor	= {M.~A.~Arbib},
  pages		= {1000--1003},
  publisher	= {MIT Press (Cambridge)},
  title		= {Vapnik-{C}hervonenkis dimension of neural nets},
  year		= {1995}
}

@InProceedings{Maass:95b,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {G. Tesauro and D. S. Touretzky and T. K. Leen},
  pages		= {183--190},
  publisher	= {MIT Press (Cambridge)},
  title		= {On the computational complexity of networks of spiking
		  neurons},
  volume	= {7},
  year		= {1995}
}

@Article{Maass:95c,
  author	= {W. Maass},
  journal	= {Telematik},
  pages		= {53--60},
  volume	= {1},
  title		= {{N}euronale {N}etze und {M}aschinelles {L}ernen am
		  {I}nstitut fuer {G}rundlagen der {I}nformationsverarbeitung
		  an der {T}echnischen {U}niversitaet {G}raz},
  year		= {1995}
}

@Article{Maass:96,
  author	= {W. Maass},
  journal	= {Neural Computation},
  pages		= {1--40},
  title		= {Lower Bounds for the Computational Power of Networks of
		  Spiking Neurons},
  volume	= {8},
  number	= {1},
  year		= {1996}
}

@InProceedings{Maass:96a,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {D. Touretzky and M. C. Mozer and M. E. Hasselmo},
  pages		= {211--217},
  publisher	= {MIT Press (Cambridge)},
  title		= {On the computational power of noisy spiking neurons},
  volume	= {8},
  year		= {1996}
}

@InProceedings{Maass:96b,
  author	= {W. Maass},
  title		= {Networks of spiking neurons: the third generation of
		  neural network models},
  booktitle	= {Proc. of the 7th Australian Conference on Neural Networks
		  1996 in Canberra, Australia},
  pages		= {1-10},
  year		= {1996}
}

@Article{Maass:97a,
  author	= {W. Maass},
  journal	= {Neural Computation},
  pages		= {279--304},
  title		= {Fast sigmoidal networks via spiking neurons},
  volume	= {9},
  year		= {1997}
}

@Article{Maass:97b,
  author	= {W. Maass},
  howpublished	= {FTP-host: archive.cis.ohio-state.edu FTP-filename:
		  /pub/neuroprose/maass.third-generation.ps.Z},
  journal	= {Neural Networks},
  pages		= {1659--1671},
  title		= {Networks of spiking neurons: the third generation of
		  neural network models},
  volume	= {10},
  year		= {1997}
}

@InProceedings{Maass:97c,
  author	= {W. Maass},
  booktitle	= {Computational Neuroscience: Trends in research},
  editor	= {James Bower},
  pages		= {123--127},
  title		= {A model for fast analog computations with noisy spiking
		  neurons},
  year		= {1997}
}

@InCollection{Maass:97d,
  author	= {W. Maass},
  booktitle	= {Spatiotemporal Models in Biological and Artificial
		  Systems},
  editor	= {F. L. Silva},
  pages		= {97-104},
  publisher	= {IOS-Press},
  title		= {Analog computations with temporal coding in networks of
		  spiking neurons},
  year		= {1997}
}

@InProceedings{Maass:97e,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {M. Mozer and M. I. Jordan and T. Petsche},
  pages		= {211--217},
  publisher	= {MIT Press (Cambridge)},
  title		= {Noisy spiking neurons with temporal coding have more
		  computational power than sigmoidal neurons},
  volume	= {9},
  year		= {1997}
}

@InProceedings{Maass:97f,
  author	= {W. Maass},
  booktitle	= {Proc. of the 8th International Conference on Algorithmic
		  Learning Theory in Sendai (Japan)},
  editor	= {M. Li and A. Maruoka},
  pages		= {364--384},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {On the relevance of time in neural computation and
		  learning},
  volume	= {1316},
  year		= {1997}
}

@Article{Maass:97g,
  author	= {W. Maass},
  title		= {On the relevance of time in neural computation and
		  learning},
  journal	= {Theoretical Computer Science},
  year		= 2001,
  volume	= {261},
  pages		= {157-178}
}

@InProceedings{Maass:98a,
  author	= {W. Maass},
  booktitle	= {Proc. of the Federated Conference of CLS'98 and MFCS'98,
		  Mathematical Foundations of Computer Science 1998},
  title		= {On the role of time and space in neural computation},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  volume	= {1450},
  year		= {1998},
  pages		= {72-83},
  note		= {Invited talk}
}

@Article{Maass:98b,
  author	= {W. Maass},
  journal	= {Network: Computation in Neural Systems},
  number	= {3},
  pages		= {381-397},
  title		= {A simple model for neural computation with firing rates
		  and firing correlations},
  volume	= {9},
  year		= {1998}
}

@InProceedings{Maass:98d,
  author	= {W. Maass},
  title		= {Models for fast analog computation with spiking neurons},
  booktitle	= {Proc. of the International Conference on Neural
		  Information Processing 1998 (ICONIP'98) in Kytakyusyu,
		  Japan},
  pages		= {187--188},
  year		= 1998,
  publisher	= {IOS Press (Amsterdam)},
  note		= {Invited talk at the special session on ``Dynamic Brain''}
}

@InProceedings{Maass:98e,
  author	= {W. Maass},
  title		= {Spiking neurons},
  booktitle	= {Proceedings of the ICSC/IFAC Symposium on Neural
		  Computation 1998 (NC'98)},
  pages		= {16--20},
  year		= 1998,
  publisher	= {ICSC Academic Press (Alberta)},
  note		= {Invited talk}
}

@InCollection{Maass:98f,
  author	= {W. Maass},
  title		= {Computing with spiking neurons},
  booktitle	= {Pulsed Neural Networks},
  pages		= {55--85},
  publisher	= {MIT Press (Cambridge)},
  year		= {1999},
  editor	= {W. Maass and C.~M.~Bishop}
}

@InProceedings{Maass:99,
  author	= {W. Maass},
  title		= {Neural Computation with Winner-Take-All as the only
		  Nonlinear Operation},
  booktitle	= {Advances in Information Processing Systems},
  editor	= {Sara A. Solla and Todd K. Leen and Klaus-Robert Mueller},
  volume	= {12},
  publisher	= {MIT Press (Cambridge)},
  year		= {2000},
  pages		= {293--299}
}

@InCollection{Maass:99a,
  author	= {W. Maass},
  title		= {Das menschliche {G}ehirn -- nur ein {R}echner?},
  booktitle	= {Zur Kunst des Formalen Denkens},
  publisher	= {Passagen Verlag (Wien)},
  year		= 2000,
  pages		= {209-233},
  editor	= {R. E. Burkard and W. Maass and P. Weibel}
}

@InCollection{Maass:99c,
  author	= {W. Maass},
  title		= {Paradigms for computing with spiking neurons},
  booktitle	= {Models of Neural Networks. Early Vision and Attention},
  publisher	= {Springer (New York)},
  year		= {2002},
  editor	= {J. L. van Hemmen and J. D. Cowan and E. Domany},
  volume	= {4},
  chapter	= {9},
  pages		= {373--402}
}

@Article{Maass:99e,
  author	= {W. Maass},
  title		= {On the computational power of winner-take-all},
  year		= {2000},
  journal	= {Neural Computation},
  volume	= {12},
  number	= {11},
  pages		= {2519--2535}
}

@Article{MaassETAL:00,
  author	= {W. Maass and A. Pinz and R. Braunstingl and G. Wiesspeiner
		  and T. Natschlaeger and O. Friedl and H. Burgsteiner},
  title		= {Konstruktion von {L}ernfaehigen {R}obotern im
		  {S}tudentenwettbewerb ``{R}obotik 2000'' an der
		  {T}echnischen {U}niversitaet {G}raz},
  journal	= {in: Telematik},
  year		= {2000},
  pages		= {20--24}
}

@Article{MaassETAL:01a,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {Real-time Computing Without Stable States: A New Framework
		  for Neural Computation Based on Perturbations},
  journal	= {Neural Computation},
  volume	= 14,
  number	= 11,
  pages		= {2531-2560},
  year		= 2002,
  abstract	= {A key challenge for neural modeling is to explain how a
		  continuous stream of multi-modal input from a rapidly
		  changing environment can be processed by stereotypical
		  recurrent circuits of integrate-and-fire neurons in
		  real-time. We propose a new framework for neural
		  computation that provides an alternative to previous
		  approaches based on attractor neural networks. It is shown
		  that the inherent transient dynamics of the
		  high-dimensional dynamical system formed by a neural
		  circuit may serve as a universal source of information
		  about past stimuli, from which readout neurons can extract
		  particular aspects needed for diverse tasks in real-time.
		  Stable internal states are not required for giving a stable
		  output, since transient internal states can be transformed
		  by readout neurons into stable target outputs due to the
		  high dimensionality of the dynamical system. Our approach
		  is based on a rigorous computational model, the liquid
		  state machine, that unlike Turing machines, does not
		  require sequential transitions between discrete internal
		  states. Like the Turing machine paradigm it allows for
		  universal computational power under idealized conditions,
		  but for real-time processing of time-varying input. The
		  resulting new framework for neural computation has novel
		  implications for the interpretation of neural coding, for
		  the design of experiments and data-analysis in
		  neurophysiology, and for neuromorphic engineering.}
}

@InProceedings{MaassETAL:02,
  author	= {W. Maass and G. Steinbauer and R. Koholka},
  title		= {Autonomous fast learning in a mobile robot},
  booktitle	= {Sensor Based Intelligent Robots. International Workshop,
		  Dagstuhl Castle, Germany, October 15--25, 2000, Selected
		  Revised Papers },
  pages		= {345--356},
  year		= {2002},
  editor	= {G. D. Hager and H. I. Christensen and H. Bunke and R.
		  Klein},
  volume	= {2238},
  series	= lncs,
  publisher	= {<a
		  href="http://www.springer.de/comp/lncs/index.html">Springer</a>
		  (Berlin)}
}

@InProceedings{MaassETAL:02a,
  author	= {W. Maass and R. Legenstein and H. Markram},
  title		= {A New Approach towards Vision suggested by Biologically
		  Realistic Neural Microcircuit Models},
  booktitle	= {Biologically Motivated Computer Vision. Proc. of the
		  Second International Workshop, BMCV 2002, Tuebingen,
		  Germany, November 22--24, 2002},
  editor	= {H. H. Buelthoff and S. W. Lee and T. A. Poggio and C.
		  Wallraven},
  series	= {Lecture Notes in Computer Science},
  volume	= {2525},
  pages		= {282--293},
  year		= {2002},
  publisher	= {Springer (Berlin)},
  abstract	= {We propose an alternative paradigm for processing
		  time-varying visual inputs, in particular for tasks
		  involving temporal and spatial integration, which is
		  inspired by hypotheses about the computational role of
		  cortical microcircuits. Since detailed knowledge about the
		  precise structure of the microcircuit is not needed for
		  that, it can in principle also be implemented with
		  partially unknown or faulty analog hardware. In addition,
		  this approach supports parallel realtime processing of
		  time-varying visual inputs for diverse tasks, since
		  different readouts can be trained to extract concurrently
		  from the same microcircuit completely different information
		  components.}
}

@InProceedings{MaassETAL:02b,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {A Model for Real-Time Computation in Generic Neural
		  Microcircuits},
  booktitle	= {Proc. of NIPS 2002, Advances in Neural Information
		  Processing Systems},
  editor	= {S. Becker and S. Thrun and K. Obermayer},
  publisher	= {MIT Press},
  year		= {2003},
  volume	= {15},
  pages		= {229--236},
  abstract	= {A key challenge for neural modeling is to explain how a
		  continuous stream of multi-modal input from a rapidly
		  changing environment can be processed by stereotypical
		  recurrent circuits of integrate-and-fire neurons in
		  real-time. We propose a new computational model that does
		  not require a task-dependent construction of neural
		  circuits. Instead it is based on principles of high
		  dimensional dynamical systems in combination with
		  statistical learning theory, and can be implemented on
		  generic evolved or found recurrent circuitry.}
}

@Article{MaassETAL:02c,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {Fading Memory and Kernel Properties of Generic Cortical
		  Microcircuit Models},
  journal	= {Journal of Physiology -- Paris},
  year		= {2004},
  pages		= {315--330},
  volume	= {98},
  number	= {4--6},
  abstract	= {It is quite difficult to construct circuits of spiking
		  neurons that can carry out complex computational tasks. On
		  the other hand even randomly connected circuits of spiking
		  neurons can in principle be used for complex computational
		  tasks such as time-warp invariant speech recognition. This
		  is possible because such circuits have an inherent tendency
		  to integrate incoming information in such a way that simple
		  linear readouts can be trained to transform the current
		  circuit activity into the target output for a very large
		  number of computational tasks. Consequently we propose to
		  analyze circuits of spiking neurons in terms of their roles
		  as analog fading memory and nonlinear kernels, rather than
		  as implementations of specific computational operations and
		  algorithms. This article is a sequel to \cite{LSM}, and
		  contains new results about the performance of generic
		  neural microcircuit models for the recognition of speech
		  that is subject to linear and nonlinear time-warps, as well
		  as for computations on time-varying firing rates. These
		  computations rely, apart from general properties of generic
		  neural microcircuit models, just on capabilities of simple
		  linear readouts trained by linear regression. This article
		  also provides detailed data on the fading memory property
		  of generic neural microcircuit models, and a quick review
		  of other new results on the computational power of such
		  circuits of spiking neurons.}
}

@InCollection{MaassETAL:03,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {Computational Models for Generic Cortical Microcircuits},
  booktitle	= {Computational Neuroscience: A Comprehensive Approach},
  publisher	= {Chapman \& Hall/CRC},
  year		= {2004},
  editor	= {J. Feng},
  chapter	= {18},
  pages		= {575--605},
  address	= {Boca Raton},
  abstract	= {The human nervous system processes a continuous stream of
		  multi-modal input from a rapidly changing environment. A
		  key challenge for neural modeling is to explain how the
		  neural microcircuits (columns, minicolumns, etc.) in the
		  cerebral cortex whose anatomical and physiological
		  structure is quite similar in many brain areas and species
		  achieve this enormous computational task. We propose a
		  computational model that could explain the potentially
		  universal computational capabilities and does not require a
		  task-dependent construction of neural circuits. Instead it
		  is based on principles of high dimensional dynamical
		  systems in combination with statistical learning theory,
		  and can be implemented on generic evolved or found
		  recurrent circuitry. This new approach towards
		  understanding neural computation on the micro-level also
		  suggests new ways of modeling cognitive processing in
		  larger neural systems. In particular it questions
		  traditional ways of thinking about neural coding.}
}

@InProceedings{MaassETAL:04,
  author	= {W. Maass and R. Legenstein and N. Bertschinger},
  booktitle	= {Advances in Neural Information Processing Systems},
  title		= {Methods for Estimating the Computational Power and
		  Generalization Capability of Neural Microcircuits},
  year		= {2005},
  volume	= {17},
  pages		= {865--872},
  editor	= {L. K. Saul and Y. Weiss and L. Bottou},
  publisher	= {MIT Press},
  abstract	= {What makes a neural microcircuit computationally powerful?
		  Or more precisely, which measurable quantities could
		  explain why one microcircuit $C$ is better suited for a
		  particular family of computational tasks than another
		  microcircuit $C'$? We propose in this article quantitative
		  measures for evaluating the computational power and
		  generalization capability of a neural microcircuit, and
		  apply them to generic neural microcircuit models drawn from
		  different distributions. We validate the proposed measures
		  by comparing their prediction with direct evaluations of
		  the computational performance of these microcircuit models.
		  This procedure is applied first to microcircuit models that
		  differ with regard to the spatial range of synaptic
		  connections and with regard to the scale of synaptic
		  efficacies in the circuit, and then to microcircuit models
		  that differ with regard to the level of background input
		  currents and the level of noise on the membrane potential
		  of neurons. In this case the proposed method allows us to
		  quantify differences in the computational power and
		  generalization capability of circuits in different dynamic
		  regimes (UP- and DOWN-states) that have been demonstrated
		  through intracellular recordings in vivo.}
}

@InProceedings{MaassETAL:06,
  author	= {W. Maass and P. Joshi and E. D. Sontag},
  title		= {Principles of real-time computing with feedback applied to
		  cortical microcircuit models},
  booktitle	= {Advances in Neural Information Processing Systems},
  abstract	= {The network topology of neurons in the brain exhibits an
		  abundance of feedback connections, but the computational
		  function of these feedback connections is largely unknown.
		  We present a computational theory that characterizes the
		  gain in computational power achieved through feedback in
		  dynamical systems with fading memory. It implies that many
		  such systems acquire through feedback universal
		  computational capabilities for analog computing with a
		  non-fading memory. In particular, we show that feedback
		  enables such systems to process time-varying input streams
		  in diverse ways according to rules that are implemented
		  through internal states of the dynamical system. In
		  contrast to previous attractor-based computational models
		  for neural networks, these flexible internal states are
		  {\em high-dimensional} attractors of the circuit dynamics,
		  that still allow the circuit state to absorb new
		  information from online input streams. In this way one
		  arrives at novel models for working memory, integration of
		  evidence, and reward expectation in cortical circuits. We
		  show that they are applicable to circuits of
		  conductance-based Hodgkin-Huxley (HH) neurons with high
		  levels of noise that reflect experimental data on in-vivo
		  conditions. },
  year		= {2006},
  volume	= {18},
  pages		= {835--842},
  publisher	= {MIT Press},
  editor	= {Y. Weiss and B. Schoelkopf and J. Platt}
}

@Article{MaassETAL:06a,
  author	= {W. Maass and P. Joshi and E. D. Sontag},
  title		= {Computational aspects of feedback in neural circuits},
  journal	= {PLoS Computational Biology},
  year		= 2007,
  volume	= {3},
  number	= {1},
  pages		= {e165, 1--20}
}

@Article{MaassETAL:07,
  author	= {H. Jaeger and W. Maass and J. Principe},
  title		= {Introduction to the Special Issue on Echo State Networks
		  and Liquid State Machines},
  journal	= {Neural Networks},
  year		= {2007},
  volume	= {20},
  number	= {3},
  pages		= {287--289},
  note		= {}
}

@Article{MaassETAL:81,
  author	= {W. Maass and A. Shore and M. Stob},
  journal	= {Israel J. Math.},
  pages		= {210--224},
  title		= {Splitting properties and jump classes},
  volume	= {39},
  year		= {1981}
}

@InProceedings{MaassETAL:87,
  author	= {W. Maass and G. Schnitger and E. Szemeredi},
  booktitle	= {Proceedings of the 19th Annual ACM Symposium on Theory of
		  Computing},
  pages		= {94--100},
  title		= {Two tapes are better than one for off-line Turing
		  machines},
  year		= {1987}
}

@InProceedings{MaassETAL:91,
  author	= {W. Maass and G. Schnitger and E. Sontag},
  booktitle	= {Proc. of the 32nd Annual IEEE Symposium on Foundations of
		  Computer Science 1991},
  pages		= {767-776},
  title		= {On the computational power of sigmoid versus boolean
		  threshold circuits},
  year		= {1991}
}

@InCollection{MaassETAL:91a,
  author	= {W. Maass and G. Schnitger and E. Sontag},
  title		= {On the computational power of sigmoid versus boolean
		  threshold circuits},
  booktitle	= {Theoretical Advances in Neural Computation and Learning},
  pages		= {127--151},
  publisher	= {Kluwer Academic Publishers (Boston)},
  year		= {1994},
  editor	= {V.~P.~Roychowdhury and K.~Y.~Siu and A.~Orlitsky}
}

@Article{MaassETAL:93,
  author	= {W. Maass and G. Schnitger and E. Szemeredi and G. Turan},
  journal	= {Computational Complexity},
  pages		= {392--401},
  title		= {Two tapes versus one for off-line {T}uring machines},
  volume	= {3},
  year		= {1993}
}

@InProceedings{MaassLegenstein:01,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Foundations for a circuit complexity theory of sensory
		  processing},
  booktitle	= {Proc. of NIPS 2000, Advances in Neural Information
		  Processing Systems},
  editor	= {T. K. Leen and T. G. Dietterich and V. Tresp},
  year		= {2001},
  volume	= {13},
  pages		= {259--265},
  publisher	= {MIT Press},
  address	= {Cambridge},
  htmlnote	= {The poster presented at NIPS is available as <a
		  href="./psfiles/122_poster.ps.gz">gzipped Postscript</a>.},
  abstract	= {We introduce {\em total wire length} as salient complexity
		  measure for an analysis of the circuit complexity of
		  sensory processing in biological neural systems and
		  neuromorphic engineering. Furthermore we introduce a set of
		  basic computational problems that apparently need to be
		  solved by circuits for translation- and scale-invariant
		  sensory processing. Finally we exhibit a number of circuit
		  design strategies for these new benchmark functions that
		  can be implemented within realistic complexity bounds, in
		  particular with linear or almost linear total wire length.}
}

@Article{MaassLegenstein:01a,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Wire Length as a Circuit Complexity Measure},
  journal	= {Journal of Computer and System Sciences},
  year		= {2005},
  volume	= {70},
  pages		= {53--72},
  abstract	= { We introduce {\em wire length} as a salient complexity
		  measure for analyzing the circuit complexity of sensory
		  processing in biological neural systems. This new
		  complexity measure is applied in this paper to two basic
		  computational problems that arise in translation- and
		  scale-invariant pattern recognition, and hence appear to be
		  useful as benchmark problems for sensory processing. We
		  present new circuit design strategies for these benchmark
		  problems that can be implemented within realistic
		  complexity bounds, in particular with linear or almost
		  linear wire length. Finally we derive some general bounds
		  which provide information about the relationship between
		  new complexity measure wire length and traditional circuit
		  complexity measures.}
}

@Article{MaassLegenstein:01c,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Neural circuits for pattern recognition with small total
		  wire length},
  journal	= {Theoretical Computer Science},
  volume	= {287},
  pages		= {239--249},
  year		= {2002},
  abstract	= {One of the most basic pattern recognition problems is
		  whether a certain local feature occurs in some linear array
		  to the left of some other local feature. We construct in
		  this article circuits that solve this problem with an
		  asymptotically optimal number of threshold gates.
		  Furthermore it is shown that much fewer threshold gates are
		  needed if one employs in addition a small number of
		  winner-take-all gates. In either case the circuits that are
		  constructed have linear or almost linear total wire length,
		  and are therefore not unrealistic from the point of view of
		  physical implementations.}
}

@Article{MaassLegenstein:01d,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Optimizing the Layout of a Balanced Tree},
  journal	= {Technical Report},
  year		= {2001},
  abstract	= { It is shown that the total wire length of layouts of a
		  balanced binary tree on a 2-dimensional grid can be reduced
		  by 33% if one does not choose the obvious ``symmetric''
		  layout strategy. Furthermore it is shown that the more
		  efficient layout strategy that is presented in this article
		  is optimal, not only for binary trees but for m-ary trees
		  with any m >= 2.}
}

@Article{MaassMarkram:00,
  author	= {W. Maass and H. Markram},
  title		= {Synapses as dynamic memory buffers},
  journal	= {Neural Networks},
  volume	= {15},
  pages		= {155--161},
  year		= {2002}
}

@Article{MaassMarkram:02,
  author	= {W. Maass and H. Markram},
  title		= {On the Computational Power of Recurrent Circuits of
		  Spiking Neurons},
  journal	= {Journal of Computer and System Sciences},
  year		= {2004},
  volume	= {69},
  number	= {4},
  pages		= {593--616}
}

@InCollection{MaassMarkram:02a,
  author	= {W. Maass and H. Markram},
  title		= {Temporal Integration in Recurrent Microcircuits},
  booktitle	= {<a href="http://mitpress.mit.edu/0262011972">The Handbook
		  of Brain Theory and Neural Networks</a>},
  publisher	= {MIT Press (Cambridge)},
  year		= {2003},
  editor	= {M. A. Arbib},
  edition	= {2nd},
  pages		= {1159--1163}
}

@InProceedings{MaassMarkram:04,
  author	= {W. Maass and H. Markram},
  title		= {Theory of the Computational Function of Microcircuit
		  Dynamics},
  booktitle	= {The Interface between Neurons and Global Brain Function},
  editor	= {S. Grillner and A. M. Graybiel},
  year		= {2006},
  pages		= {371--390},
  chapter	= {18},
  publisher	= {MIT Press},
  series	= {Dahlem Workshop Report 93}
}

@Article{MaassNatschlaeger:97a,
  author	= {W. Maass and T. Natschlaeger},
  title		= {Networks of Spiking Neurons can Emulate Arbitrary
		  {H}opfield nets in Temporal Coding},
  journal	= {Network: Computation in Neural Systems},
  year		= 1997,
  volume	= 8,
  number	= 4,
  pages		= {355--371},
  keywords	= {spiking neurons, hopfield net, temporal coding},
  userlabel	= {4},
  abstract	= {A theoretical model for analog computation with temporal
		  coding is introduced and tested through simulations in
		  GENESIS. It turns out that the use of multiple synapses
		  yields very noise robust mechanisms for analog computations
		  with temporal coding in networks of detailed compartmental
		  neuron models. One arrives in this way at a method for
		  emulating arbitrary Hopfield nets with spiking neurons in
		  temporal coding, yielding new models for associative recall
		  of spatio-temporal firing patterns. A corresponding layered
		  architecture yields a refinement of the synfire-chain model
		  that can assume a fairly large set of different firing
		  patterns for different inputs.}
}

@InProceedings{MaassNatschlaeger:98,
  author	= {W. Maass and T. Natschlaeger},
  title		= {Associative Memory with Networks of Spiking Neurons in
		  Temporal Coding},
  booktitle	= {Neuromorphic Systems: Engineering Silicon from
		  Neurobiology},
  editor	= {L. S. Smith and A. Hamilton},
  year		= 1998,
  publisher	= {World Scientific},
  pages		= {21--32},
  keywords	= {spiking neurons, hopfield net, temporal coding},
  abstract	= {A theoretical model for analog computation with temporal
		  coding is introduced and tested through simulations in
		  GENESIS. It turns out that the use of multiple synapses
		  yields very noise robust mechanisms for analog computations
		  with temporal coding in networks of detailed compartmental
		  neuron models. One arrives in this way at a method for
		  emulating arbitrary Hopfield nets with spiking neurons in
		  temporal coding, yielding new models for associative recall
		  of spatio-temporal firing patterns.}
}

@InProceedings{MaassNatschlaeger:98b,
  author	= {W. Maass and T. Natschlaeger},
  title		= {Emulation of {H}opfield Networks with Spiking Neurons in
		  Temporal Coding},
  booktitle	= {Computational Neuroscience: Trends in Research},
  editor	= {J. M. Bower},
  year		= 1998,
  publisher	= {Plenum Press},
  pages		= {221--226},
  keywords	= {spiking neurons, hopfield net, temporal coding},
  abstract	= {A theoretical model for analog computation with temporal
		  coding is introduced and tested through simulations in
		  GENESIS. It turns out that the use of multiple synapses
		  yields very noise robust mechanisms for analog computations
		  with temporal coding in networks of detailed compartmental
		  neuron models. One arrives in this way at a method for
		  emulating arbitrary Hopfield nets with spiking neurons in
		  temporal coding, yielding new models for associative recall
		  of spatio-temporal firing patterns. A corresponding layered
		  architecture yields a refinement of the synfire-chain model
		  that can assume a fairly large set of different firing
		  patterns for different inputs.}
}

@Article{MaassNatschlaeger:99,
  author	= {W. Maass and T. Natschlaeger},
  title		= {A model for Fast Analog Computation Based on Unreliable
		  Synapses},
  journal	= {Neural Computation},
  year		= 2000,
  pages		= {1679--1704},
  volume	= {12},
  number	= {7},
  keywords	= {unreliable synapses, universal function approximation,
		  fast analog computation, time series, hebbian learning,
		  space rate coding, population activity},
  abstract	= {We investigate through theoretical analysis and computer
		  simulations the consequences of unreliable synapses for
		  fast analog computations in networks of spiking neurons,
		  with analog variables encoded by the current firing
		  activities of pools of spiking neurons. Our results suggest
		  that the known unreliability of synaptic transmission may
		  be viewed as a useful tool for analog computing, rather
		  than as a ``bug'' in neuronal hardware. We also investigate
		  computations on time series and Hebbian learning in this
		  context of space-rate coding.}
}

@InProceedings{MaassOrponen:97,
  author	= {W. Maass and P. Orponen},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {M. Mozer and M. I. Jordan and T. Petsche},
  pages		= {218--224},
  publisher	= {MIT Press (Cambridge)},
  title		= {On the effect of analog noise in discrete-time analog
		  computations},
  volume	= {9},
  year		= {1997}
}

@Article{MaassOrponen:97j,
  author	= {W. Maass and P. Orponen},
  title		= {On the effect of analog noise in discrete-time analog
		  computations},
  journal	= {Neural Computation},
  year		= 1998,
  volume	= 10,
  pages		= {1071--1095}
}

@InProceedings{MaassRuf:95,
  author	= {W. Maass and B. Ruf},
  address	= {Paris},
  booktitle	= {Proc. of the International Conference on Artificial Neural
		  Networks ICANN},
  pages		= {515--520},
  publisher	= {EC2\&Cie},
  title		= {On the Relevance of the Shape of Postsynaptic Potentials
		  for the Computational Power of Networks of Spiking
		  Neurons},
  year		= {1995},
  keywords	= {spiking neuron, postsynaptic potential, computational
		  complexity}
}

@Article{MaassRuf:97,
  author	= {W. Maass and B. Ruf},
  journal	= {Information and Computation},
  title		= {On computation with pulses},
  year		= {1999},
  volume	= {148},
  pages		= {202--218},
  keywords	= {spiking neuron, computational complexity, postsynaptic
		  potential}
}

@InProceedings{MaassSchmitt:97,
  author	= {W. Maass and M. Schmitt},
  booktitle	= {Proc. of the 10th Conference on Computational Learning
		  Theory 1997},
  note		= {See also Electronic Proc. of the Fifth International
		  Symposium on Artificial Intelligence and Mathematics
		  (http://rutcor.rutgers.edu/\~{}amai)},
  pages		= {54--61},
  publisher	= {ACM-Press (New York)},
  title		= {On the complexity of learning for a spiking neuron},
  year		= {1997}
}

@Article{MaassSchmitt:98,
  author	= {W. Maass and M. Schmitt},
  journal	= {Information and Computation},
  title		= {On the complexity of learning for spiking neurons with
		  temporal coding},
  year		= {1999},
  volume	= {153},
  pages		= {26--46}
}

@InProceedings{MaassSchnitger:86,
  author	= {W. Maass and G. Schnitger},
  booktitle	= {Proceedings of the Structure in Complexity Theory
		  Conference, Berkeley 1986},
  pages		= {249--264},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {An optimal lower bound for {T}uring machines with one Work
		  Tape and Two-Way Input Tape},
  year		= {1986},
  volume	= {223}
}

@Article{MaassSchorr:87,
  author	= {W. Maass and A. Schorr},
  journal	= {SIAM J. Comput.},
  pages		= {195--202},
  title		= {Speed-up of {T}uring machines with one work tape and a
		  two-way input tape},
  volume	= {16},
  year		= {1987}
}

@InProceedings{MaassSlaman:89,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the Logic Colloquium '88, Padova, Italy},
  editor	= {Ferro and Bonotto and Valentini and Zanardo},
  pages		= {79-89},
  publisher	= {Elsevier Science Publishers (North-Holland)},
  title		= {Some problems and results in the theory of actually
		  computable functions},
  year		= {1989}
}

@InProceedings{MaassSlaman:89a,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the 4th Annual Conference on Structure in
		  Complexity Theory},
  pages		= {231--239},
  publisher	= {IEEE Computer Society Press (Washington)},
  title		= {The complexity types of computable sets (extended
		  abstract)},
  year		= {1989}
}

@InProceedings{MaassSlaman:89b,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the 7th International Conference on
		  Fundamentals of Computation Theory},
  pages		= {318--326},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {Extensional properties of sets of time bounded complexity
		  (extended abstract)},
  volume	= {380},
  year		= {1989}
}

@InProceedings{MaassSlaman:90,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the 1989 Recursion Theory Week
		  Oberwolfach},
  pages		= {297--322},
  publisher	= {Springer (Berlin)},
  title		= {On the relationship between the complexity, the degree,
		  and the extension of a computable set},
  year		= {1990}
}

@InProceedings{MaassSlaman:91,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of a Workshop on Logic from Computer Science},
  editor	= {Y. N. Moschovakis},
  pages		= {359--372},
  publisher	= {Springer (Berlin)},
  title		= {Splitting and density for the recursive sets of a fixed
		  time complexity},
  year		= {1991}
}

@Article{MaassSlaman:92,
  author	= {W. Maass and T. A. Slaman},
  journal	= {J. Comput. Syst. Sci.},
  note		= {Invited paper for a special issue of the J. Comput. Syst.
		  Sci.},
  pages		= {168--192},
  title		= {The complexity types of computable sets},
  volume	= {44},
  year		= {1992}
}

@Article{MaassSontag:97,
  author	= {W. Maass and E. Sontag},
  journal	= {Neural Computation},
  title		= {Analog neural nets with {G}aussian or other common noise
		  distributions cannot recognize arbitrary regular
		  languages},
  year		= {1999},
  volume	= {11},
  pages		= {771--782}
}

@Article{MaassSontag:99a,
  author	= {W. Maass and E. D. Sontag},
  title		= {Neural systems as nonlinear filters},
  journal	= {Neural Computation},
  volume	= {12},
  number	= {8},
  year		= {2000},
  pages		= {1743--1772}
}

@InProceedings{MaassSontag:99b,
  author	= {W. Maass and E. D. Sontag},
  title		= {A precise characterization of the class of languages
		  recognized by neural nets under {G}aussian and other common
		  noise distributions},
  booktitle	= {Advances in Neural Information Processing Systems},
  year		= 1999,
  volume	= {11},
  pages		= {281--287},
  editor	= {M.~S.~Kearns and S.~S.~Solla and D.~A.~Cohn},
  publisher	= {MIT Press (Cambridge)}
}

@Article{MaassStob:83,
  author	= {W. Maass and M. Stob},
  journal	= {Ann. of Pure and Applied Logic},
  pages		= {189--212},
  title		= {Intervals of the lattice of recursively enumerable sets
		  determined by major subsets},
  volume	= {24},
  year		= {1983}
}

@Article{MaassSutner:88,
  author	= {W. Maass and K. Sutner},
  journal	= {Acta Informatica},
  pages		= {93-122},
  title		= {Motion planning among time dependent abstacles},
  volume	= {26},
  year		= {1988}
}

@InProceedings{MaassTuran:89,
  author	= {W. Maass and G. Turan},
  booktitle	= {Proceedings of the 30th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {262--267},
  title		= {On the complexity of learning from counterexamples
		  (extended abstract)},
  year		= {1989}
}

@InProceedings{MaassTuran:90,
  author	= {W. Maass and G. Turan},
  booktitle	= {Proceedings of the 31th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {203--210},
  title		= {On the complexity of learning from counterexamples and
		  membership queries},
  year		= {1990}
}

@Article{MaassTuran:92,
  author	= {W. Maass and G. Turan},
  journal	= {Machine Learning},
  note		= {Invited paper for a special issue of Machine Learning},
  pages		= {107--145},
  title		= {Lower bound methods and separation results for on-line
		  learning models},
  volume	= {9},
  year		= {1992}
}

@InCollection{MaassTuran:94,
  author	= {W. Maass and G. Turan},
  booktitle	= {Computational Learning Theory and Natural Learning System:
		  Constraints and Prospects},
  editor	= {S. J. Hanson and G. A. Drastal and R. L. Rivest},
  pages		= {381--414},
  publisher	= {MIT Press (Cambridge)},
  title		= {How fast can a threshold gate learn},
  year		= {1994}
}

@Article{MaassTuran:94a,
  author	= {W. Maass and G. Turan},
  journal	= {Machine Learning},
  pages		= {251--269},
  title		= {Algorithms and lower bounds for on-line learning of
		  geometrical concepts},
  volume	= {14},
  year		= {1994}
}

@InProceedings{MaassTuran:95,
  author	= {W. Maass and G. Turan},
  address	= {Jerusalem},
  booktitle	= {Proc. of the 4th Bar-Ilan Symposium on Foundations of
		  Artificial Intelligence (BISFAI'95)},
  title		= {On learnability and predicate logic (extended abstract)},
  year		= {1995},
  pages		= {75--85}
}

@InProceedings{MaassWarmuth:95,
  author	= {W. Maass and M. Warmuth},
  booktitle	= {Proc. of the 12th International Machine Learning
		  Conference, Tahoe City, USA},
  editor	= {Morgan Kaufmann (San Francisco)},
  pages		= {378-386},
  title		= {Efficient learning with virtual threshold gates},
  year		= {1995}
}

@Article{MaassWarmuth:95j,
  author	= {W. Maass and M. Warmuth},
  title		= {Efficient learning with virtual threshold gates},
  journal	= {Information and Computation},
  year		= 1998,
  volume	= 141,
  number	= 1,
  pages		= {66--83}
}

@InCollection{MaassWeibel:97,
  author	= {W. Maass and P. Weibel},
  booktitle	= {{Jenseits von Kunst}},
  editor	= {P. Weibel},
  pages		= {745--747},
  publisher	= {Passagen Verlag},
  title		= {{I}st die {V}ertreibung der {V}ernunft reversibel?
		  {U}eberlegungen zu einem {W}issenschafts- und
		  {M}edienzentrum},
  year		= {1997}
}

@InProceedings{MaassZador:98,
  author	= {W. Maass and A. M. Zador},
  booktitle	= {Advances in Neural Processing Systems},
  publisher	= {MIT Press (Cambridge)},
  title		= {Dynamic stochastic synapses as computational units},
  volume	= {10},
  pages		= {194--200},
  year		= {1998}
}

@InCollection{MaassZador:98a,
  author	= {W. Maass and A. Zador},
  booktitle	= {Pulsed Neural Networks},
  editor	= {W. Maass and C. Bishop},
  publisher	= {MIT-Press (Cambridge)},
  title		= {Computing and learning with dynamic synapses},
  year		= {1998},
  pages		= {321-336}
}

@Article{MaassZador:98j,
  author	= {W. Maass and A. M. Zador},
  title		= {Dynamic stochastic synapses as computational units},
  journal	= {Neural Computation},
  year		= 1999,
  volume	= 11,
  number	= 4,
  pages		= {903--917}
}

@Article{MelamedETAL:03,
  author	= {O. Melamed and W. Gerstner and W. Maass and M. Tsodyks and
		  H. Markram},
  title		= {Coding and Learning of Behavioral Sequences},
  abstract	= {A major challenge to understanding behavior is how the
		  nervous system allows the learning of behavioral sequences
		  that can occur over arbitrary timescales, ranging from
		  milliseconds up to seconds, using a fixed millisecond
		  learning rule. This article describes some potential
		  solutions, and then focuses on a study by Mehta et al. that
		  could contribute towards solving this puzzle. They have
		  discovered that an experience-dependent asymmetric shape of
		  hippocampal receptive fields combined with oscillatory
		  inhibition can serve to map behavioral sequences on a fixed
		  timescale.},
  journal	= {Trends in Neurosciences},
  volume	= 27,
  number	= 1,
  year		= {2004},
  pages		= {11--14}
}

@InProceedings{NatschlaegerETAL:01,
  author	= {T. Natschlaeger and W. Maass and E. D. Sontag and A.
		  Zador},
  title		= {Processing of Time Series by Neural Circuits with
		  Biologically Realistic Synaptic Dynamics},
  booktitle	= {Advances in Neural Information Processing Systems 2000
		  ({NIPS '2000})},
  editor	= {Todd K. Leen and Thomas G. Dietterich and Volker Tresp},
  year		= 2001,
  volume	= 13,
  pages		= {145--151},
  address	= {Cambridge},
  publisher	= {MIT Press},
  abstract	= {Experimental data show that biological synapses behave
		  quite differently from the symbolic synapses in common
		  artificial neural network models. Biological synapses are
		  dynamic, i.e., their weight changes on a short time scale
		  by several hundred percent in dependence of the past input
		  to the synapse. In this article we explore the consequences
		  that this synaptic dynamics entails for the computational
		  power of feedforward neural networks. It turns out that
		  even with just a single hidden layer such networks can
		  approximate a surprisingly large class of nonlinear
		  filters: all filters that can be characterized by Volterra
		  series. This result is robust with regard to various
		  changes in the model for synaptic dynamics. Furthermore we
		  show that simple gradient descent suffices to approximate a
		  given quadratic filter by a rather small neural system with
		  dynamic synapses. We demonstrate that with this approach
		  the nonlinear filter considered in (Back and Tsoi 93) can
		  be approximated even better than by their model.},
  htmlnote	= {The poster presented at NIPS is available as <a
		  href="./psfiles/dynsyn-poster.pdf">Acrobat PDF</a> file.}
}

@Article{NatschlaegerETAL:01a,
  author	= {T. Natschlaeger and W. Maass and A. Zador},
  title		= {Efficient Temporal Processing with Biologically Realistic
		  Dynamic Synapses},
  journal	= {Network: Computation in Neural Systems},
  year		= 2001,
  pages		= {75--87},
  volume	= 12,
  abstract	= {Experimental data show that biological synapses behave
		  quite differently from the symbolic synapses in common
		  artificial neural network models. Biological synapses are
		  dynamic, i.e., their ``weight'' changes on a short time
		  scale by several hundred percent in dependence of the past
		  input to the synapse. Here we describe a general model of
		  computation that exploits dynamic synapses, and use a
		  backpropagation-like algorithm to adjust the synaptic
		  parameters. We show that such gradient descent suffices to
		  approximate a given quadratic filter by a rather small
		  neural system with dynamic synapses. We demonstrate that
		  with this approach the nonlinear filter considered in (Back
		  and Tsoi, 1993) can be approximated slightly better than by
		  their model. Our numerical results are complemented by
		  theoretical analysis which show that even with just a
		  single hidden layer such networks can approximate a
		  surprisingly large class of nonlinear filters: all filters
		  that can be characterized by Volterra series. This result
		  is robust with regard to various changes in the model for
		  synaptic dynamics. }
}

@Article{NatschlaegerETAL:02,
  author	= {T. Natschlaeger and W. Maass and H. Markram},
  title		= {The "Liquid Computer": A Novel Strategy for Real-Time
		  Computing on Time Series},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  year		= {2002},
  pages		= {39--43},
  volume	= {8},
  number	= {1},
  abstract	= {We will discuss in this survey article a new framework for
		  analysing computations on time series and in particular on
		  spike trains, introduced in (Maass et. al. 2002). In
		  contrast to common computational models this new framework
		  does not require that information can be stored in some
		  stable states of a computational system. It has recently
		  been shown that such models where all events are transient
		  can be successfully applied to analyse computations in
		  neural systems and (independently) that the basic ideas can
		  also be used to solve engineering tasks such as the design
		  of nonlinear controllers. Using an illustrative example we
		  will develop the main ideas of the proposed model. This
		  illustrative example is generalized and cast into a
		  rigorous mathematical model: the Liquid State Machine. A
		  mathematical analysis shows that there are in principle no
		  computational limitations of liquid state machines in the
		  domain of time series computing. Finally we discuss several
		  successful applications of the framework in the area of
		  computational neuroscience and in the field of artificial
		  neural networks.}
}

@InCollection{NatschlaegerETAL:03,
  author	= {T. Natschlaeger and H. Markram and W. Maass},
  title		= {Computer Models and Analysis Tools for Neural
		  Microcircuits},
  booktitle	= {Neuroscience Databases. A Practical Guide},
  publisher	= {Kluwer Academic Publishers (Boston)},
  year		= {2003},
  editor	= {R. Koetter},
  chapter	= {9},
  pages		= {121--136},
  abstract	= {This chapter surveys web resources regarding computer
		  models and analysis tools for neural microcircuits. In
		  particular it describes the features of a new website
		  (www.lsm.tugraz.at) that facilitates the creation of
		  computer models for cortical neural microcircuits of
		  various sizes and levels of detail, as well as tools for
		  evaluating the computational power of these models in a
		  Matlabenvironment.}
}

@Article{NatschlaegerMaass:01a,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Computing the Optimally Fitted Spike Train for a Synapse},
  journal	= {Neural Computation},
  year		= 2001,
  volume	= 13,
  number	= 11,
  pages		= {2477--2494},
  abstract	= {Experimental data have shown that synapses are
		  heterogeneous: different synapses respond with different
		  sequences of amplitudes of postsynaptic responses to the
		  same spike train. Neither the role of synaptic dynamics
		  itself nor the role of the heterogeneity of synaptic
		  dynamics for computations in neural circuits is well
		  understood. We present in this article two computational
		  methods that make it feasible to compute for a given
		  synapse with known synaptic parameters the spike train that
		  is optimally fitted to the synapse in a certain sense. With
		  the help of these methods one can compute for example the
		  temporal pattern of a spike train (with a given number of
		  spikes) that produces the largest sum of postsynaptic
		  responses for a specific synapse. Several other
		  applications are also discussed. To our surprise we find
		  that most of these optimally fitted spike trains match
		  common firing patterns of specific types of neurons that
		  are discussed in the literature. Hence our analysis
		  provides a possible functional explanation for the
		  experimentally observed regularity in the combination of
		  specific types of synapses with specific types of neurons
		  in neural circuits. }
}

@InProceedings{NatschlaegerMaass:01b,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Finding the Key to a Synapse},
  booktitle	= {Advances in Neural Information Processing Systems ({NIPS
		  '2000})},
  editor	= {Todd K. Leen and Thomas G. Dietterich and Volker Tresp},
  year		= 2001,
  pages		= {138--144},
  volume	= 13,
  address	= {Cambridge},
  publisher	= {MIT Press},
  abstract	= {Experimental data have shown that synapses are
		  heterogeneous: different synapses respond with different
		  sequences of amplitudes of postsynaptic responses to the
		  same presynaptic spike train. Neither the role of synaptic
		  dynamics itself nor the role of the heterogeneity of
		  synaptic dynamics for computations in neural circuits is
		  well understood. We present in this article methods that
		  make it feasible to compute for a given synapse with known
		  synaptic parameters the spike train that is optimally
		  fitted to the synapse, in the sense that it produces the
		  largest sum of postsynaptic responses. To our surprise we
		  find that most of these optimally fitted spike trains match
		  common firing patterns of specific types of neurons that
		  are discussed in the literature. Hence our analysis
		  provides a possible functional explanation for the
		  experimentally observed regularity in the combination of
		  specific types of synapses with specific types of neurons
		  in neural circuits.},
  htmlnote	= {The poster presented at NIPS is available as <a
		  href="./psfiles/synkey-poster.pdf">Acrobat PDF</a> file.}
}

@InProceedings{NatschlaegerMaass:03,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Information Dynamics and Emergent Computation in Recurrent
		  Circuits of Spiking Neurons},
  booktitle	= {Proc. of NIPS 2003, Advances in Neural Information
		  Processing Systems},
  year		= {2004},
  volume	= {16},
  pages		= {1255--1262},
  editor	= {S. Thrun and L. Saul and B. Schoelkopf},
  publisher	= {MIT Press},
  address	= {Cambridge},
  abstract	= {An efficient method using Bayesian and linear classifiers
		  is presented for analyzing the dynamics of information in
		  high dimensional circuit states, and applied to investigate
		  emergent computation in generic cortical microcircuit
		  models. It is shown that such recurrent circuits of spiking
		  neurons have an inherent capability to carry out rapid
		  computations on complex spike patterns, merging information
		  contained in the order of spike arrival with previously
		  acquired context information.}
}

@Article{NatschlaegerMaass:04,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Dynamics of Information and Emergent Computation in
		  Generic Neural Microcircuit Models},
  journal	= {Neural Networks},
  volume	= {18},
  number	= {10},
  pages		= {1301--1308},
  year		= {2005},
  abstract	= {Numerous methods have already been developed to estimate
		  the information contained in single spike trains. In this
		  article we explore efficient methods for estimating the
		  information contained in the simultaneous firing activity
		  of hundreds of neurons. Obviously such methods are needed
		  to analyze data from multi-unit recordings. We test these
		  methods on generic neural microcircuit models consisting of
		  800 neurons, and analyze the temporal dynamics of
		  information about preceding spike inputs in such circuits.
		  It turns out that information spreads with high speed in
		  such generic neural microcircuit models, thereby supporting
		  -- without the postulation of any additional neural or
		  synaptic mechanisms -- the possibility of ultra-rapid
		  computations on the first input spikes.}
}

@Article{NatschlaegerMaass:2001,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Spiking Neurons and the Induction of Finite State
		  Machines},
  journal	= {Theoretical Computer Science: Special Issue on Natural
		  Computing},
  volume	= 287,
  year		= 2002,
  pages		= {251--265},
  abstract	= {We discuss in this short survey article some current
		  mathematical models from neurophysiology for the
		  computational units of biological neural systems: neurons
		  and synapses. These models are contrasted with the
		  computational units of common artificial neural network
		  models, which reflect the state of knowledge in
		  neurophysiology 50 years ago. We discuss the problem of
		  carrying out computations in circuits consisting of
		  biologically realistic computational units, focusing on the
		  biologically particularly relevant case of computations on
		  time series. Finite state machines are frequently used in
		  computer science as models for computations on time series.
		  One may argue that these models provide a reasonable common
		  conceptual basis for analyzing computations in computers
		  and biological neural systems, although the emphasis in
		  biological neural systems is shifted more towards
		  asynchronous computation on analog time series. In the
		  second half of this article some new computer experiments
		  and theoretical results are discussed, which address the
		  question whether a biological neural system can in
		  principle learn to behave like a given simple finite state
		  machine. }
}

@InProceedings{NatschlaegerMaass:99,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Fast Analog Computation in Networks of Spiking Neurons
		  Using Unreliable Synapses},
  booktitle	= {{ESANN'99} Proceedings of the European Symposium on
		  Artificial Neural Networks},
  year		= 1999,
  address	= {Bruges, Belgium},
  pages		= {417--422},
  keywords	= {unreliable synapses, universal function approximation,
		  fast analog computation, time series, hebbian learning,
		  space rate coding, population activity},
  abstract	= {We investigate through theoretical analysis and computer
		  simulations the consequences of unreliable synapses for
		  fast analog computations in networks of spiking neurons,
		  with analog variables encoded by the firing activities of
		  pools of spiking neurons. Our results suggest that the
		  known unreliability of synaptic transmission may be viewed
		  as a useful tool for analog computing, rather than as a
		  ``bug'' in neuronal hardware. We also investigate
		  computations on analog time series encoded by the firing
		  activities of pools of spiking neurons.}
}

@Article{NesslerETAL:08,
  author	= {B. Nessler and M. Pfeiffer and W. Maass},
  title		= {Hebbian learning of {B}ayes optimal decisions},
  journal	= {In Proc. of NIPS 2008: Advances in Neural Information
		  Processing Systems},
  year		= {2009},
  volume	= {21},
  number	= {},
  pages		= {},
  note		= {MIT Press},
  abstract	= {When we perceive our environment, make a decision, or take
		  an action, our brain has to deal with multiple sources of
		  uncertainty. The Bayesian framework of statistical
		  estimation provides computational methods for dealing
		  optimally with uncertainty. Bayesian inference however is
		  algorithmically quite complex, and learning of Bayesian
		  inference involves the storage and updating of probability
		  tables or other data structures that are hard to implement
		  in neural networks. Hence it is unclear how our nervous
		  system could acquire the capability to approximate optimal
		  Bayesian inference and action selection. This article shows
		  that the simplest and experimentally best supported type of
		  synaptic plasticity, Hebbian learning, can in principle
		  achieve this. Even inference in complex Bayesian networks
		  can be approximated by Hebbian learning in combination with
		  population coding and lateral inhibition
		  (``Winner-Take-All'') in cortical microcircuits that
		  produce a sparse encoding of complex sensory stimuli. We
		  also show that a corresponding reward-modulated Hebbian
		  plasticity rule provides a principled framework for
		  understanding how Bayesian inference could support fast
		  reinforcement learning in the brain. In particular we show
		  that recent experimental results by Yang and Shadlen on
		  reinforcement learning of probabilistic inference in
		  primates can be modeled in this way.}
}

@InProceedings{NesslerETAL:10,
  author	= {B. Nessler and M. Pfeiffer and W. Maass},
  title		= {{STDP} enables spiking neurons to detect hidden causes of
		  their inputs},
  booktitle	= {Proc. of NIPS 2009: Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2010},
  volume	= {22},
  pages		= {1357--1365},
  abstract	= {The principles by which spiking neurons contribute to the
		  astounding computational power of generic cortical
		  microcircuits, and how spike-timing-dependent plasticity
		  (STDP) of synaptic weights could generate and maintain this
		  computational function, are unknown. We show here that
		  STDP, in conjunction with a stochastic soft winner-take-all
		  (WTA) circuit, induces spiking neurons to generate through
		  their synaptic weights implicit internal models for
		  subclasses (or “causes”) of the high-dimensional spike
		  patterns of hundreds of pre-synaptic neurons. Hence these
		  neurons will fire after learning whenever the current input
		  best matches their internal model. The resulting
		  computational function of soft WTA circuits, a common
		  network motif of cortical microcircuits, could therefore be
		  a drastic dimensionality reduction of information streams,
		  together with the autonomous creation of internal models
		  for the probability distributions of their input patterns.
		  We show that the autonomous generation and maintenance of
		  this computational function can be explained on the basis
		  of rigorous mathematical principles. In particular, we show
		  that STDP is able to approximate a stochastic online
		  Expectation-Maximization (EM) algorithm for modeling the
		  input data. A corresponding result is shown for Hebbian
		  learning in artificial neural networks.}
}

@InProceedings{NeumannETAL:07,
  author	= {G. Neumann and M. Pfeiffer and W. Maass},
  booktitle	= {Proceedings of the 18th European Conference on Machine
		  Learning (ECML) and the 11th European Conference on
		  Principles and Practice of Knowledge Discovery in Databases
		  (PKDD) 2007, Warsaw (Poland)},
  title		= {Efficient Continuous-Time Reinforcement Learning with
		  Adaptive State Graphs},
  publisher	= {Springer (Berlin)},
  year		= {2007},
  pages		= {},
  note		= {in press},
  abstract	= {We present a new reinforcement learning approach for
		  deterministic continuous control problems in environments
		  with unknown, arbitrary reward functions. The difficulty of
		  finding solution trajectories for such problems can be
		  reduced by incorporating limited prior knowledge of the
		  approximative local system dynamics. The presented
		  algorithm builds an adaptive state graph of sample points
		  within the continuous state space. The nodes of the graph
		  are generated by an efficient principled exploration scheme
		  that directs the agent towards promising regions, while
		  maintaining good online performance. Global solution
		  trajectories are formed as combinations of local
		  controllers that connect nodes of the graph, thereby
		  naturally allowing continuous actions and continuous time
		  steps. We demonstrate our approach on various movement
		  planning tasks in continuous domains.}
}

@InProceedings{NeumannETAL:09,
  author	= {G. Neumann and W. Maass and J. Peters},
  title		= {Learning Complex Motions by Sequencing Simpler Motion
		  Templates},
  booktitle	= {Proc. of the Int. Conf. on Machine Learning (ICML 2009),
		  Montreal},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  note		= {in press},
  abstract	= {Abstraction of complex, longer motor tasks into simpler
		  elemental movements enables humans and animals to exhibit
		  motor skills which have not yet been matched by robots.
		  Humans intuitively decompose complex motions into smaller,
		  simpler segments. For example when describing simple
		  movements like drawing a triangle with a pen, we can easily
		  name the basic steps of this movement. Surprisingly, such
		  abstractions have rarely been used in artificial motor
		  skill learning algorithms. These algorithms typically
		  choose a new action (such as a torque or a force) at a very
		  fast time-scale. As a result, both policy and temporal
		  credit assignment problem become unnecessarily complex -
		  often beyond the reach of current machine learning methods.
		  We introduce a new framework for temporal abstractions in
		  reinforcement learning (RL), i.e. RL with motion templates.
		  We present a new algorithm for this framework which can
		  learn high-quality policies by making only few abstract
		  decisions. }
}

@Article{NikolicETAL:06,
  author	= {D. Nikolic and S. Haeusler and W. Singer and W. Maass},
  title		= {Temporal dynamics of informational contents carried by
		  neurons in the primary visual cortex},
  journal	= {submitted for publication},
  year		= 2006
}

@Article{NikolicETAL:07,
  author	= {D. Nikolic and S. Haeusler and W. Singer and W. Maass},
  title		= {Distributed fading memory for stimulus properties in the
		  primary visual},
  journal	= {submitted for publication},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {}
}

@InProceedings{NikolicETAL:07a,
  author	= {D. Nikoli\'{c} and S. Haeusler and W. Singer and W. Maass},
  title		= {Temporal dynamics of information content carried by
		  neurons in the primary visual cortex},
  booktitle	= {Proc. of NIPS 2006, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2007},
  volume	= {19},
  pages		= {1041--1048},
  abstract	= {We use multi-electrode recordings from cat primary visual
		  cortex and investigate whether a simple linear classifier
		  ca n extract information about the presented stimuli. We
		  find that information is extractable and that it even las
		  ts for several hundred milliseconds after the stimulus has
		  b een removed. In a fast sequence of stimulus presentation,
		  information about both new and old stimuli is present
		  simultaneously and nonlinear relations between these
		  stimuli can be extracted. These results suggest nonlinear
		  properties of cortical representat ions. The implications
		  of these properties for the nonlinear brain theory are
		  discussed.}
}

@Article{NikolicETAL:09,
  author	= {D. Nikolic and S. Haeusler and W. Singer and W. Maass},
  title		= {Distributed fading memory for stimulus properties in the
		  primary visual cortex},
  journal	= {PLoS Biology},
  year		= {2009},
  volume	= {7},
  number	= {12},
  pages		= {1--19},
  abstract	= {It is currently not known how distributed neuronal
		  responses in early visual areas carry stimulus-related
		  information. We made multi-electrode recordings from cat
		  primary visual cortex and applied methods from machine
		  learning in order to analyze the temporal evolution of
		  stimulus-related information in the spiking activity of
		  large ensembles of around 100 neurons. We used sequences of
		  up to three different visual stimuli (letters) presented
		  for 100 ms and with intervals of 100 ms or larger. Most of
		  the information about visual stimuli extractable by
		  sophisticated methods of machine learning, i.e. support
		  vector machines with non-linear kernel functions, was also
		  extractable by simple linear classification such as can be
		  achieved by individual neurons. New stimuli did not erase
		  information about previous stimuli. The responses to the
		  most recent stimulus contained about equal amounts of
		  information about both this and the preceding stimulus.
		  This information was encoded both in the discharge rates
		  (response amplitudes) of the ensemble of neurons and, when
		  using short time-constants for integration (e.g., 20 ms),
		  in the precise timing of individual spikes (<= $~20$ ms),
		  and persisted for several 100 ms beyond the offset of
		  stimuli. The results indicate that the network from which
		  we recorded is endowed with fading memory and is capable of
		  performing online computations utilizing information about
		  temporally sequential stimuli. This result challenges
		  models assuming frame-by-frame analyses of sequential
		  inputs.},
  note		= {}
}

@TechReport{PecevskiETAL:11,
  author	= {D. Pecevski and L. B\"using and W. Maass},
  title		= {Networks of spiking neurons are able to carry out
		  probabilistic inference in general graphical models through
		  their inherent stochastic dynamics},
  institution	= {Graz University of Technology},
  year		= {2011}
}

@Article{PfeifferETAL:09,
  author	= {M. Pfeiffer and B. Nessler and W. Maass and R. J.
		  Douglas},
  title		= {Reward-modulated {H}ebbian Learning of Optimal Decision
		  Making},
  journal	= {Neural Computation},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@Article{PfeifferETAL:09b,
  author	= {M. Pfeiffer and B. Nessler and W. Maass},
  title		= {{STDP} approximates Expectation Maximization in networks
		  of spiking neurons with lateral inhibition},
  journal	= {in preparation},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@Article{PfeifferETAL:09c,
  author	= {M. Pfeiffer and B. Nessler and R. Douglas and W. Maass},
  title		= {Reward-modulated {H}ebbian {L}earning of {D}ecision
		  {M}aking},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  pages		= {1399--1444},
  abstract	= {We introduce a framework for decision making in which the
		  learning of decision making is reduced to its simplest
		  andbiologically most plausible form: Hebbian learning on a
		  linear neuron. We cast our Bayesian-Hebb learning rule as
		  reinforcement learning in which certain decisions are
		  rewarded and prove that each synaptic weight will on
		  average converge exponentially fast to the log-odd of
		  receiving a reward when its pre- and postsynaptic neurons
		  are active. In our simple architecture, a particular action
		  is selected from the set of candidate actions by a
		  winner-takeall operation. The global reward assigned to
		  this action then modulates the update of each synapse.
		  Apart from this global reward signal, our reward-modulated
		  Bayesian Hebb rule is a pure Hebb update that depends only
		  on the coactivation of the pre- and postsynaptic neurons,
		  not on theweighted sum of all presynaptic inputs to the
		  postsynaptic neuron as in the perceptron learning rule or
		  the Rescorla-Wagner rule. This simple approach to
		  action-selection learning requires that information about
		  sensory inputs be presented to the Bayesian decision stage
		  in a suitably preprocessed form resulting from other
		  adaptive processes (acting on a larger timescale) that
		  detect salient dependencies among input features. Hence our
		  proposed framework for fast learning of decisions also
		  provides interesting new hypotheses regarding neural nodes
		  and computational goals of cortical areas that provide
		  input to the final decision stage.}
}

@TechReport{RaschETAL:06,
  author	= {M. Rasch and S. Haeusler and Z. Kisvarday and W. Maass and
		  N. Logothetis},
  title		= {Comparison of a detailed model for area {V}1 with
		  simultaneous recordings from {LGN} and {V}1},
  institution	= {Technische Universitaet Graz and MPI Tuebingen},
  year		= {2006}
}

@TechReport{RaschETAL:06b,
  author	= {M. Rasch and A. Gretton and Y. Murayama and W. Maass and
		  N. Logothetis},
  title		= {Interaction of local field potential and spiking activity
		  in area {V}1},
  institution	= {Technische Universitaet Graz and MPI Tuebingen},
  year		= {2006}
}

@Article{RaschETAL:07,
  author	= {M. J. Rasch and A. Gretton and Y. Murayama and W. Maass
		  and N. K. Logothetis},
  title		= {Inferring spike trains from local field potentials},
  journal	= {Journal of Neurophysiology},
  year		= {2008},
  volume	= {99},
  number	= {},
  pages		= {1461--1476},
  note		= {},
  abstract	= {We investigated whether it is possible to infer spike
		  trains solely on the basis of the underling local field
		  potentials ({LFP}s). Employing support vector machines and
		  linear regression models, we found that in the primary
		  visual cortex ({V}1) of monkeys, spikes can indeed be
		  inferred from {LFP}s, at least with moderate success.
		  Although there is a considerable degree of variation across
		  electrodes, the low-frequency structure in spike trains (in
		  the 100 ms range) can be inferred with reasonable accuracy,
		  whereas exact spike positions are not reliably predicted.
		  Two kinds of features of the {LFP} are exploited for
		  prediction: the frequency power of bands in the high
		  $\gamma$-range (40-90 {H}z), and information contained in
		  low-frequency oscillations (<10 {H}z), where both phase and
		  power modulations are informative. Information analysis
		  revealed that both features code (mainly) independent
		  aspects of the spike-to-{LFP} relationship, with the
		  low-frequency {LFP} phase coding for temporally clustered
		  spiking activity. Although both features and prediction
		  quality are similar during semi-natural movie stimuli and
		  spontaneous activity, prediction performance during
		  spontaneous activity degrades much more slowly with
		  increasing electrode distance. The general trend of data
		  obtained with anesthetized animals is qualitatively
		  mirrored in that of a more limited data set recorded in
		  {V}1 of awake monkeys. In contrast to the cortical field
		  potentials, thalamic {LFP}s (e.g. {LFP}s derived from
		  recordings in d{LGN}) hold no useful information for
		  predicting spiking activity.}
}

@Article{RaschETAL:09,
  author	= {M. J. Rasch and K. Schuch and N. K. Logothetis and W.
		  Maass},
  title		= {Statistical characterization of the spike response to
		  natural stimuli in monkey area {V}1 and in a detailed model
		  for a patch of {V}1},
  journal	= {submitted},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@Article{RaschETAL:10,
  author	= {M. J. Rasch and K. Schuch and N. K. Logothetis and W.
		  Maass},
  title		= {Statistical Comparision of Spike Responses to Natural
		  Stimuli in Monkey Area {V}1 With Simulated Responses of a
		  Detailed Laminar Network Model for a Patch of {V}1},
  journal	= {J Neurophysiol},
  year		= {2011},
  pages		= {757--778},
  volume	= {105},
  abstract	= {A major goal of computational neuroscience is the creation
		  of computer models for cortical areas whose response to
		  sensory stimuli resembles that of cortical areas in vivo in
		  important aspects. It is seldom considered whether the
		  simulated spiking activity is realistic (in a statistical
		  sense) in response to natural stimuli. Because certain
		  statistical properties of spike responses were suggested to
		  facilitate computations in the cortex, acquiring a
		  realistic firing regimen in cortical network models might
		  be a prerequisite for analyzing their computational
		  functions. We present a characterization and comparison of
		  the statistical response properties of the primary visual
		  cortex (V1) in vivo and in silico in response to natural
		  stimuli. We recorded from multiple electrodes in area V1 of
		  4 macaque monkeys and developed a large state-of-the-art
		  network model for a 5 x 5-mm patch of V1 composed of 35,000
		  neurons and 3.9 million synapses that integrates previously
		  published anatomical and physiological details. By
		  quantitative comparison of the model response to the
		  "statistical fingerprint" of responses in vivo, we find
		  that our model for a patch of V1 responds to the same movie
		  in a way which matches the statistical structure of the
		  recorded data surprisingly well. The deviation between the
		  firing regimen of the model and the in vivo data are on the
		  same level as deviations among monkeys and sessions. This
		  suggests that, despite strong simplifications and
		  abstractions of cortical network models, they are
		  nevertheless capable of generating realistic spiking
		  activity. To reach a realistic firing state, it was not
		  only necessary to include both N-methyl- D-aspartate and
		  GABAB synaptic conductances in our model, but also to
		  markedly increase the strength of excitatory synapses onto
		  inhibitory neurons (more than 2-fold) in comparison to
		  literature values, hinting at the importance to carefully
		  adjust the effect of inhibition for achieving realistic
		  dynamics in current network models.}
}

@Article{SteimerETAL:09,
  author	= {A. Steimer and W. Maass and R. Douglas},
  title		= {Belief-propagation in networks of spiking neurons},
  journal	= {Neural Computation},
  year		= {2009},
  volume	= {21},
  number	= {},
  pages		= {2502--2523},
  abstract	= {From a theoretical point of view, statistical inference is
		  an attractive model of brain operation. However, it is
		  unclear how to implement these inferential processes in
		  neuronal networks. We offer a solution to this problem by
		  showing in detailed simulations how the Belief-Propagation
		  algorithm on a factor graph can be embedded in a network of
		  spiking neurons. We use pools of spiking neurons as the
		  function nodes of the factor graph. Each pool gathers
		  'messages' in the form of population activities from its
		  input nodes and combines them through its network dynamics.
		  The various output messages to be transmitted over the
		  edges of the graph are each computed by a group of readout
		  neurons that feed in their respective destination pools. We
		  use this approach to implement two examples of factor
		  graphs. The first example is drawn from coding theory. It
		  models the transmission of signals through an unreliable
		  channel and demonstrates the principles and generality of
		  our network approach. The second, more applied example, is
		  of a psychophysical mechanism in which visual cues are used
		  to resolve hypotheses about the interpretation of an
		  object's shape and illumination. These two examples, and
		  also a statistical analysis, all demonstrate good agreement
		  between the performance of our networks and the direct
		  numerical evaluation of beliefpropagation.}
}

@Article{SteinbauerETAL:02,
  author	= {G. Steinbauer and R. Koholka and W. Maass},
  title		= {A very short story about autonomous robots},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  year		= {2002},
  volume	= {8},
  number	= {1},
  pages		= {26--29}
}

@Article{SussilloETAL:07,
  author	= {D. Sussillo and T. Toyoizumi and W. Maass},
  title		= {Self-tuning of neural circuits through short-term synaptic
		  plasticity},
  journal	= {Journal of Neurophysiology},
  year		= {2007},
  volume	= {97},
  pages		= {4079--4095},
  abstract	= {Circuits of neurons in the cortex have a remarkable
		  capability to maintain functional and dynamic stability in
		  spite of changes in the level of external inputs, synaptic
		  plasticity and changes in the circuit structure that occur
		  during development and adult learning. The source of this
		  characteristic stability of cortical circuits has remained
		  a mystery, especially since even stronglysimplified models
		  of such circuits do not exhibit similar stability
		  properties. One simplification that is usually made in such
		  models is that the empirically found nonlinear and diverse
		  inherent short term dynamics (paired-pulse facilitation and
		  depression) of biological synapses is replaced by static
		  and uniform linear synapse models. We show in this article
		  that this is a mistake, since the complex and diverse
		  nonlinear dynamics of biological synapses supports the
		  implementation of powerful control principles that endow
		  circuits of spiking neurons with almost in-vivo like
		  stability properties.}
}

@Article{UchizawaETAL:06,
  author	= {K. Uchizawa and R. Douglas and W. Maass},
  title		= {On the Computational Power of Threshold Circuits with
		  Sparse Activity},
  journal	= {Neural Computation},
  year		= 2006,
  volume	= 18,
  number	= 12,
  pages		= {2994--3008},
  abstract	= {Circuits composed of threshold gates (McCulloch-Pitts
		  neurons, or perceptrons) are simplified models of neural
		  circuits with the advantage that they are theoretically
		  more tractable than their biological counterparts. However,
		  when such threshold circuits are designed to perform a
		  specific computational task they usually differ in one
		  important respect from computations in the brain: they
		  require very high activity. On average every second
		  threshold gate fires (sets a ``1'' as output) during a
		  computation. By contrast, the activity of neurons in the
		  brain is much more sparse, with only about 1\% of neurons
		  firing. This mismatch between threshold and neuronal
		  circuits is due to the particular complexity measures
		  (circuit size and circuit depth) that have been minimized
		  in previous threshold circuit constructions. In this
		  article we investigate a new complexity measure for
		  threshold circuits, {\em energy complexity}, whose
		  minimization yields computations with sparse activity. We
		  prove that all computations by threshold circuits of
		  polynomial size with entropy $O(\log n)$ can be
		  restructured so that their energy complexity is reduced to
		  a level near the {\em entropy of circuit states}. This
		  entropy of circuit states is a novel circuit complexity
		  measure, which is of interest not only in the context of
		  threshold circuits, but for circuit complexity in general.
		  As an example of how this measure can be applied we show
		  that any polynomial size threshold circuit with entropy
		  $O(\log n)$ can be simulated by a polynomial size threshold
		  circuit of depth 3.
		  
		  Our results demonstrate that the structure of circuits
		  which result from a minimization of their energy complexity
		  is quite different from the structure which results from a
		  minimization of previously considered complexity measures,
		  and potentially closer to the structure of neural circuits
		  in the nervous system. In particular, different pathways
		  are activated in these circuits for different classes of
		  inputs. This article shows that such circuits with sparse
		  activity have a surprisingly large computational power.}
}

@InProceedings{UchizawaETAL:06a,
  author	= {K. Uchizawa and R. Douglas and W. Maass},
  booktitle	= {Proceedings of the 33rd International Colloquium on
		  Automata, Languages and Programming, ICALP (1) 2006,
		  Venice, Italy, July 10-14, 2006, Part I},
  title		= {Energy Complexity and Entropy of Threshold Circuits},
  year		= {2006},
  volume	= {4051},
  pages		= {631--642},
  publisher	= {Springer},
  series	= {Lecture Notes in Computer Science},
  editor	= {M. Bugliesi and B. Preneel and V. Sassone and I. Wegener},
  abstract	= {Circuits composed of threshold gates (McCulloch-Pitts
		  neurons, or perceptrons) are simplified models of neural
		  circuits with the advantage that they are theoretically
		  more tractable than their biological counterparts. However,
		  when such threshold circuits are designed to perform a
		  specific computational task they usually differ in one
		  important respect from computations in the brain: they
		  require very high activity. On average every second
		  threshold gate fires (sets a ``1'' as output) during a
		  computation. By contrast, the activity of neurons in the
		  brain is much more sparse, with only about 1\% of neurons
		  firing. This mismatch between threshold and neuronal
		  circuits is due to the particular complexity measures
		  (circuit size and circuit depth) that have been minimized
		  in previous threshold circuit constructions. In this
		  article we investigate a new complexity measure for
		  threshold circuits, {\em energy complexity}, whose
		  minimization yields computations with sparse activity. We
		  prove that all computations by threshold circuits of
		  polynomial size with entropy $O(\log n)$ can be
		  restructured so that their energy complexity is reduced to
		  a level near the {\em entropy of circuit states}. This
		  entropy of circuit states is a novel circuit complexity
		  measure, which is of interest not only in the context of
		  threshold circuits, but for circuit complexity in general.
		  As an example of how this measure can be applied we show
		  that any polynomial size threshold circuit with entropy
		  $O(\log n)$ can be simulated by a polynomial size threshold
		  circuit of depth 3.}
}

@TechReport{WinterETAL:06,
  author	= {M. Winter and H. Bischof and M. Rasch and W. Maass},
  title		= {Results and open problems regarding the role of local
		  feature descriptors for image classification in computer
		  vision, and image representations in primary visual
		  cortex},
  institution	= {Technische Universitaet Graz},
  year		= {2006}
}
