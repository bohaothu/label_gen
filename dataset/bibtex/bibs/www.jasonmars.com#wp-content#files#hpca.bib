@inproceedings{1410058,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {},
 year = {2004},
 pages = { x-- x},
 publisher = {IEEE},
 title = {Program Chair&#146;s Message},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10001},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410058},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410058.pdf?arnumber=1410058},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 abstract = {},
}

@inproceedings{1410059,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {},
 year = {2004},
 pages = { xi-- xi},
 publisher = {IEEE},
 title = {Reviewers},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10005},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410059},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410059.pdf?arnumber=1410059},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {IEEE, },
 abstract = {},
}

@inproceedings{1410057,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {},
 year = {2004},
 pages = { ix-- ix},
 publisher = {IEEE},
 title = {General Co-Chairs&#146; Message},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10006},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410057},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410057.pdf?arnumber=1410057},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 abstract = {},
}

@inproceedings{4798258,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Chaudhuri, M.},
 year = {2009},
 pages = {227--238},
 publisher = {IEEE},
 title = {PageNUCA: Selected policies for page-grain locality management in large shared chip-multiprocessor caches},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798258},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798258},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798258.pdf?arnumber=4798258},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Cache storage, Computer science, Data engineering, Delay, Energy storage, Engineering management, L2 cache banks, PageNUCA, Proposals, SDRAM, Switches, Technology management, cache block-grain dynamic migration, data locality, hardwired coarse-grain data migration, hardwired dynamic page migration, memory architecture, microprocessor chips, multiprocessing systems, non-uniform access latency, on-chip caches, on-chip data, page-grain locality management, page-grain migration, shared chip-multiprocessor caches, shared memory parallel application, storage management chips, },
 abstract = {As the last-level on-chip caches in chip-multiprocessors increase in size, the physical locality of on-chip data becomes important for delivering high performance. The non-uniform access latency seen by a core to different independent banks of a large cache spread over the chip necessitates active mechanisms for improving data locality. The central proposal of this paper is a fully hardwired coarse-grain data migration mechanism that dynamically monitors the access patterns of the cores at the granularity of a page to reduce the book-keeping overhead and decides when and where to migrate an entire page of data to amortize the performance overhead. The page-grain migration mechanism is compared against two variants of previously proposed cache block-grain dynamic migration mechanisms and two OS-assisted static locality management mechanisms. Our detailed execution-driven simulation of an eight-core chip-multiprocessor with a shared 16 MB L2 cache employing a bidirectional ring to connect the cores and the L2 cache banks shows that hardwired dynamic page migration, while using only 4.8\% of extra storage out of the total L2 cache and book-keeping budget, delivers the best performance and energy-efficiency across a set of shared memory parallel applications selected from the SPLASH-2, SPEC OMP, DARPA DIS, and FFTW suites and multiprogrammed workloads prepared out of the SPEC 2000 and BioBench suites. It reduces execution time by 18.7\% and 12.6\% on average (geometric mean) respectively for the shared memory applications and the multiprogrammed workloads compared to a baseline architecture that distributes the pages round-robin across the L2 cache banks. },
}

@inproceedings{4798259,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Guangyu Sun and Xiangyu Dong and Yuan Xie and Jian Li and Yiran Chen},
 year = {2009},
 pages = {239--249},
 publisher = {IEEE},
 title = {A novel architecture of the 3D stacked MRAM L2 cache for CMPs},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798259},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798259},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798259.pdf?arnumber=4798259},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {3D heterogeneous integrations, 3D stacked MRAM L2 cache, Clocks, Computational modeling, Degradation, Delay, Fabrication, MRAM devices, Magnetic cores, Magnetic tunneling, Random access memory, SRAM chips, SRAM counterparts, Stacking, Sun, cache storage, conventional chip multiprocessors, magnetic random access memory, memory architecture, read-preemptive write buffer, },
 abstract = {Magnetic random access memory (MRAM) is a promising memory technology, which has fast read access, high density, and non-volatility. Using 3D heterogeneous integrations, it becomes feasible and cost-efficient to stack MRAM atop conventional chip multiprocessors (CMPs). However, one disadvantage of MRAM is its long write latency and its high write energy. In this paper, we first stack MRAM-based L2 caches directly atop CMPs and compare it against SRAM counterparts in terms of performance and energy. We observe that the direct MRAM stacking might harm the chip performance due to the aforementioned long write latency and high write energy. To solve this problem, we then propose two architectural techniques: read-preemptive write buffer and SRAM-MRAM hybrid L2 cache. The simulation result shows that our optimized MRAM L2 cache improves performance by 4.91\% and reduces power by 73.5\%compared to the conventional SRAM L2 cache with the similar area. },
}

@inproceedings{4798256,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Greskamp, B. and Lu Wan and Karpuzcu, U.R. and Cook, J.J. and Torrellas, J. and Deming Chen and Zilles, C.},
 year = {2009},
 pages = {213--224},
 publisher = {IEEE},
 title = {Blueshift: Designing processors for timing speculation from the ground up.},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798256},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798256},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798256.pdf?arnumber=4798256},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {BlueShift, Clocks, Design methodology, Design optimization, Error correction, Frequency, Logic, Process design, Proposals, Timing, Voltage, circuit optimisation, clock frequency, clocks, design optimization algorithm, error-correcting support, logic design, microprocessor chips, on-demand selective biasing, path constraint tuning, processor design, static critical path, timing speculation, voltage scaling, },
 abstract = {Several recent processor designs have proposed to enhance performance by increasing the clock frequency to the point where timing faults occur, and by adding error-correcting support to guarantee correctness. However, such timing speculation (TS) proposals are limited in that they assume traditional design methodologies that are suboptimal under TS. In this paper, we present a new approach where the processor itself is designed from the ground up for TS. The idea is to identify and optimize the most frequently-exercised critical paths in the design, at the expense of the majority of the static critical paths, which are allowed to suffer timing errors. Our approach and design optimization algorithm are called BlueShift. We also introduce two techniques that, when applied under BlueShift, improve processor performance: on-demand selective biasing (OSB) and path constraint tuning (PCT). Our evaluation with modules from the OpenSPARC T1 processor shows that, compared to conventional TS, BlueShift with OSB speeds up applications by an average of 8\% while increasing the processor power by an average of 12\%. Moreover, compared to a high-performance TS design, BlueShift with PCT speeds up applications by an average of 6\% with an average processor power overhead of 23\% . providing a way to speed up logic modules that is orthogonal to voltage scaling. },
}

@inproceedings{4798257,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {225--226},
 publisher = {IEEE},
 title = {Session 4A NUCA and 3-D stacked memory hierarchies},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798257},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798257},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798257.pdf?arnumber=4798257},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798257.png" border="0"> },
}

@inproceedings{4798254,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Najaf-abadi, H.H. and Rotenberg, E.},
 year = {2009},
 pages = {189--200},
 publisher = {IEEE},
 title = {Architectural Contesting},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798254},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798254},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798254.pdf?arnumber=4798254},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Availability, Broadcasting, Delay, Impedance, Microarchitecture, Multicore processing, Process design, Robustness, Throughput, architectural contesting, computer architecture, constrained heterogeneous multicore design, fine-grain change, leader-follower arrangement, microarchitecture, single-thread performance enhancement, workload behavior, },
 abstract = {This paper presents results showing that workload behavior tends to vary considerably at granularities of less than a thousand instructions. If it were possible to adjust the microarchitecture to suit the workload behavior at such rates, significant single-thread performance enhancement would be achievable. However, previous techniques are too sluggish to be able to effectively respond to such fine-grain change. An approach is proposed that exploits the multi-core trend to enable swift adjustment in the employed microarchitecture upon variation in workload behavior. A number of cores that are each custom-designed for optimum performance under a class of workloads concurrently execute code in a leader-follower arrangement. In this manner, effective execution automatically and fluidly transfers to the most suitable microarchitecture as the workload behavior varies. We refer to this approach as architectural contesting. Two-way contesting yields an average speedup of 15\% (maximum speedup of 25\%) over a benchmark's own customized core. The paper also explores the interplay between contesting and the number of core types available in the heterogeneous multi-core. This exposes the broader issue of constrained heterogeneous multi-core design and how it influences, and may be influenced by, contesting. },
}

@inproceedings{4798255,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Stephenson, M. and Lixin Zhang and Rangan, R.},
 year = {2009},
 pages = {201--212},
 publisher = {IEEE},
 title = {Lightweight predication support for out of order processors},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798255},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798255},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798255.pdf?arnumber=4798255},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Art, Clocks, Floating-point arithmetic, Hardware, Lighting control, Out of order, Parallel processing, Pipelines, Process control, Registers, hammock predication, lightweight predication support, mutually exclusive groups, nonpredicated code, out of order processors, parallel architectures, pipeline processing, processor pipeline, unpredictable control flow, },
 abstract = {The benefits of Out of Order (OOO) processing are well known, as is the effectiveness of predicated execution for unpredictable control flow. However, as previous research has demonstrated, these techniques are at odds with one another. One common approach to reconciling their differences is to simplify the form of predication supported by the architecture. For instance, the only form of predication supported by modern OOO processors is a simple conditional move. We argue that it is the simplicity of conditional move that has allowed its widespread adoption, but we also show that this simplicity compromises its effectiveness as a compilation target. In this paper, we introduce a generalized form of hammock predication - called predicated mutually exclusive groups - that requires few modifications to an existing processor pipeline, yet presents the compiler with abundant predication opportunities. In comparison to non-predicated code running on an aggressively clocked baseline system, our technique achieves an 8\% speedup averaged across three important benchmark suites. },
}

@inproceedings{4798252,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Das, R. and Eachempati, S. and Mishra, A.K. and Narayanan, V. and Das, C.R.},
 year = {2009},
 pages = {175--186},
 publisher = {IEEE},
 title = {Design and evaluation of a hierarchical on-chip interconnect for next-generation CMPs},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798252},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798252},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798252.pdf?arnumber=4798252},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Delay, Energy consumption, Fabrics, Network topology, Network-on-a-chip, Power system interconnection, Spine, System performance, Telecommunication traffic, Throughput, bimodal traffic characteristics, cache bank, chip multiprocessor, circuit optimisation, clustered communication, data value locality, hierarchical on-chip interconnect design, integrated circuit design, integrated circuit interconnections, low-power electronics, low-power shared bus fabric, low-radix mesh, microprocessor chips, multiprocessing systems, network topology, network topology, network-on-chip, next-generation CMP, power consumption, router microarchitecture, },
 abstract = {Performance and power consumption of an on-chip interconnect that forms the backbone of chip multiprocessors (CMPs), are directly influenced by the underlying network topology. Both these parameters can also be optimized by application induced communication locality since applications mapped on a large CMP system will benefit from clustered communication, where data is placed in cache banks closer to the cores accessing it. Thus, in this paper, we design a hierarchical network topology that takes advantage of such communication locality. The two-tier hierarchical topology consists of local networks that are connected via a global network. The local network is a simple, high-bandwidth, low-power shared bus fabric, and the global network is a low-radix mesh. The key insight that enables the hybrid topology is that most communication in CMP applications can be limited to the local network, and thus, using a fast, low-power bus to handle local communication will improve both packet latency and power-efficiency. The proposed hierarchical topology provides up to 63\% reduction in energy-delay-product over mesh, 47\% over flattened butterfly, and 33\% with respect to concentrated mesh across network sizes with uniform and non-uniform synthetic traffic. For real parallel workloads, the hybrid topology provides up to 14\% improvement in system performance (IPC) and in terms of energy-delay-product, improvements of 70\%, 22\%, 30\% over the mesh, flattened butterfly, and concentrated mesh, respectively, for a 32-way CMP. Although the hybrid topology scales in a power- and bandwidth-efficient manner with network size, while keeping the average packet latency low in comparison to high radix topologies, it has lower throughput due to high concentration. To improve the throughput of the hybrid topology, we propose a novel router micro-architecture, called XShare, which exploits data value locality and bimodal traffic characteristics of CMP applications to transfer multiple small flits o- - ver a single channel. This helps in enhancing the network throughput by 35\%, providing a latency reduction of 14\% with synthetic traffic, and improving IPC on an average 4\% with application workloads. },
}

@inproceedings{4798253,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {187--188},
 publisher = {IEEE},
 title = {Session 3B processor microarchitecture-I},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798253},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798253},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798253.pdf?arnumber=4798253},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Microarchitecture, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798253.png" border="0"> },
}

@inproceedings{4798250,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Michelogiannakis, G. and Balfour, J. and Dally, W.J.},
 year = {2009},
 pages = {151--162},
 publisher = {IEEE},
 title = {Elastic-buffer flow control for on-chip networks},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798250},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798250},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798250.pdf?arnumber=4798250},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Buffer storage, Delay, Detectors, Frequency, Network-on-a-chip, Routing, System recovery, Telecommunication traffic, Throughput, UGAL routing, Virtual colonoscopy, buffer storage, deadlock prevention, distributed FIFO buffer, elastic-buffer flow control, microarchitecture, network on chip, network routing, network traffic, network-on-chip, pipelined channel, universal globally adaptive load-balancing, virtual-channel buffer, },
 abstract = {This paper presents elastic buffers (EBs), an efficient flow-control scheme that uses the storage already present in pipelined channels in place of explicit input virtual-channel buffers (VCBs). With this approach, the channels themselves act as distributed FIFO buffers. Without VCBs, and hence virtual channels (VCs), deadlock prevention is achieved by duplicating physical channels. We develop a channel occupancy detector to apply universal globally adaptive load-balancing (UGAL) routing to load balance traffic in networks using EBs. Using EBs results in up to 8\% (12\% for low-swing channels) improvement in peak throughput per unit power compared to a VC flow-control network. These gains allow for a wider network datapath to be used to offset the removal of VCBs and increase throughput for a fixed power budget. EB networks have identical zero-load latency to VC networks operating under the same frequency. The microarchitecture of an EB router is considerably simpler than a VC router because allocators and credits are not required. For 5 times 5 mesh routers, this results in an 18\% improvement in the cycle time. },
}

@inproceedings{4798251,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Grot, B. and Hestness, J. and Keckler, S.W. and Mutlu, O.},
 year = {2009},
 pages = {163--174},
 publisher = {IEEE},
 title = {Express Cube Topologies for on-Chip Interconnects},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798251},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798251},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798251.pdf?arnumber=4798251},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Costs, Delay, Energy efficiency, Moore's Law, Network topology, Network-on-a-chip, Scalability, Silicon, System-on-a-chip, Wires, express cube topologies, generalized express cubes, hypercube networks, multidrop express channels, multiple networks, network topologies, on-chip interconnect topologies, onchip interconnects, system-on-chip, systems-on-a-chip, },
 abstract = {Driven by continuing scaling of Moore's law, chip multi-processors and systems-on-a-chip are expected to grow the core count from dozens today to hundreds in the near future. Scalability of on-chip interconnect topologies is critical to meeting these demands. In this work, we seek to develop a better understanding of how network topologies scale with regard to cost, performance, and energy considering the advantages and limitations afforded on a die. Our contributions are three-fold. First, we propose a new topology, called Multidrop Express Channels (MECS), that uses a one-to-many communication model enabling a high degree of connectivity in a bandwidth-efficient manner. In a 64-terminal network, MECS enjoys a 9\% latency advantage over other topologies at low network loads, which extends to over 20\% in a 256-terminal network. Second, we demonstrate that partitioning the available wires among multiple networks and channels enables new opportunities for trading-off performance, area, and energy-efficiency that depend on the partitioning scheme. Third, we introduce Generalized Express Cubes - a framework for expressing the space of on-chip interconnects - and demonstrate how existing and proposed topologies can be mapped to it. },
}

@inproceedings{5416629,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Biswas, A. and Recchia, C. and Mukherjee, S.S. and Ambrose, V. and Chan, L. and Jaleel, A. and Papathanasiou, A.E. and Plaster, M. and Seifert, N.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Explaining cache SER anomaly using DUE AVF measurement},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416629},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416629},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416629.pdf?arnumber=5416629},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Alpha particles, Circuit faults, DUE AVF measurement, Equations, Error analysis, Error correction codes, Neutrons, Particle beams, Power measurement, Protection, Semiconductor device measurement, architectural vulnerability factor, cache SER anomaly, cache storage, detected unrecoverable error, proton beam irradiation, soft error rate, super-linear increase, write-back L2 cache, },
 abstract = {We have discovered that processors can experience a super-linear increase in detected unrecoverable errors (DUE) when the write-back L2 cache is doubled in size. This paper explains how an increase in the cache tag's Architectural Vulnerability Factor or AVF caused such a super-linear increase in the DUE rate. AVF expresses the fraction of faults that become user-visible errors. Our hypothesis is that this increase in AVF is caused by a super-linear increase in Â¿dirtyÂ¿ data residence times in the L2 cache. Using proton beam irradiation, we measured the DUE rates from the write-back cache tags and analyzed the data to show that our hypothesis holds. We utilized a combination of simulation and measurements to help develop and prove this hypothesis. Our investigation reveals two methods by which dirty line residency causes super-linear increases in the L2 cache tag's AVF. One is a reduction in the miss rates as we move to the larger cache part, resulting in fewer evictions of data required for architecturally correct execution. The second is the occurrence of strided cache access patterns, which cause a significant increase in the Â¿dirtyÂ¿ residency times of cache lines without increasing the cache miss rate. },
}

@inproceedings{5416628,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Dong Hyuk Woo and Nak Hee Seong and Lewis, D.L. and Lee, H.-H.S.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {An optimized 3D-stacked memory architecture by exploiting excessive, high-density TSV bandwidth},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416628},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416628},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416628.pdf?arnumber=5416628},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {3D chip stacks, Bandwidth, DRAM arrays, DRAM chips, DRAM density, DRAM interface, Delay, Frequency, L2 cache, L2 fetch, Memory architecture, Packaging, Pins, Proposals, Random access memory, SMART-3D architecture, Technological innovation, Through-silicon vias, cache storage, data transfer, energy consumption, energy consumption, false sharing problem, high frequency memory-bus interface, high-density TSV bandwidth, memory architecture, memory bandwidth, memory hierarchy, multi-threading, multicore processor, multiprocessing systems, multiprogram workload, multisocket processor, multisocket system, multithreaded workload, optimized 3D-stacked memory architecture, single-threaded memory-intensive application, system memory, through-silicon-vias, write-back network, },
 abstract = {Memory bandwidth has become a major performance bottleneck as more and more cores are integrated onto a single die, demanding more and more data from the system memory. Several prior studies have demonstrated that this memory bandwidth problem can be addressed by employing a 3D-stacked memory architecture, which provides a wide, high frequency memory-bus interface. Although previous 3D proposals already provide as much bandwidth as a traditional L2 cache can consume, the dense through-silicon-vias (TSVs) of 3D chip stacks can provide still more bandwidth. In this paper, we contest that we need to re-architect our memory hierarchy, including the L2 cache and DRAM interface, so that it can take full advantage of this massive bandwidth. Our technique, SMART-3D, is a new 3D-stacked memory architecture with a vertical L2 fetch/write-back network using a large array of TSVs. Simply stated, we leverage the TSV bandwidth to hide latency behind very large data transfers. We analyze the design trade-offs for the DRAM arrays, careful enough to avoid compromising the DRAM density because of TSV placement. Moreover, we propose an efficient mechanism to manage the false sharing problem when implementing SMART-3D in a multi-socket system. For single-threaded memory-intensive applications, the SMART-3D architecture achieves speedups from 1.53 to 2.14 over planar designs and from 1.27 to 1.72 over prior 3D designs. We achieve similar speedups for multi-program and multi-threaded workloads on multi-core and multi-socket processors. Furthermore, SMART-3D can even lower the energy consumption in the L2 cache and 3D DRAM for it reduces the total number of row buffer misses. },
}

@inproceedings{5416627,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Ware, M. and Rajamani, K. and Floyd, M. and Brock, B. and Rubio, J.C. and Rawson, F. and Carter, J.B.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Architecting for power management: The IBM&#x00AE; POWER7&#x2122; approach},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416627},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416627},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416627.pdf?arnumber=5416627},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Blades, Energy efficiency, Energy management, Environmental management, IBM POWER7 processor, Memory management, Power control, Power generation, Power system management, Scalability, Supercomputers, dynamic power management, microprocessor chips, peak energy-efficiency, server processor, },
 abstract = {The POWER7 processor is the newest member of the IBM POWER<sup>Â®</sup> family of server processors. With greater than 4X the peak performance and the same power budget as the previous generation POWER6<sup>Â®</sup>, POWER7 will deliver impressive energy-efficiency boosts. The improved peak energy-efficiency is accompanied by a wide array of new features in the processor and system designs that advance IBM's EnergyScale<sup>TM</sup> dynamic power management methodology. This paper provides an overview of these new features, which include better sensing, more advanced power controls, improved scalability for power management, and features to address the diverse needs of the full range of POWER servers from blades to supercomputers. We also highlight three challenges that need attention from a range of systems design and research teams: (i) power management in highly virtualized environments, (ii) power (in)efficiency of systems software and applications, and (iii) memory power costs, especially for servers with large memory footprints. },
}

@inproceedings{5416626,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Yan Pan and Kim, J. and Memik, G.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {FlexiShare: Channel sharing for an energy-efficient nanophotonic crossbar},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416626},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416626},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416626.pdf?arnumber=5416626},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Bandwidth, Delay, Energy consumption, Energy efficiency, FlexiShare, MineBench, Network-on-a-chip, Optical buffering, Optical losses, Performance loss, SPLASH-2, Scalability, Telecommunication traffic, channel arbitration, channel sharing, credit distribution, electrical power consumption, energy efficient nanophotonic crossbar architecture, extracted trace traffic, high static power consumption, low-power electronics, nanophotonics, network-on-chip, network-on-chip, photonic switching systems, photonic token-stream mechanism, router complexity, telecommunication channels, token-stream arbitrated conventional crossbar, },
 abstract = {On-chip network is becoming critical to the scalability of future many-core architectures. Recently, nanophotonics has been proposed for on-chip networks because of its low latency and high bandwidth. However, nanophotonics has relatively high static power consumption, which can lead to inefficient architectures. In this work, we propose FlexiShare - a nanophotonic crossbar architecture that minimizes static power consumption by fully sharing a reduced number of channels across the network. To enable efficient global sharing, we decouple the allocation of the channels and the buffers, and introduce novel photonic token-stream mechanism for channel arbitration and credit distribution The flexibility of FlexiShare introduces additional router complexity and electrical power consumption. However, with the reduced number of optical channels, the overall power consumption is reduced without loss in performance. Our evaluation shows that the proposed token-stream arbitration applied to a conventional crossbar design improves network throughput by 5.5Ã under permutation traffic. In addition, FlexiShare achieves similar performance as a token-stream arbitrated conventional crossbar using only half the amount of channels under balanced, distributed traffic. With the extracted trace traffic from MineBench and SPLASH-2, FlexiShare can further reduce the amount of channels by up to 87.5\%, while still providing better performance - resulting in up to 72\% reduction in power consumption compared to the best alternative. },
}

@inproceedings{4658639,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Chang, M.F. and Cong, J. and Kaplan, A. and Naik, M. and Reinman, G. and Socher, E. and Tam, S.-W.},
 year = {2008},
 pages = {191--202},
 publisher = {IEEE},
 title = {CMP network-on-chip overlaid with multi-band RF-interconnect},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658639},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658639},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658639.pdf?arnumber=4658639},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Aggregates, Bandwidth, CMOS technology, CMP, Costs, Delay, Integrated circuit interconnections, Network-on-a-chip, RF access points, RF signals, Radio frequency, System recovery, chip multiprocessor, integrated circuit interconnections, mesh topology, microprocessor chips, multiband RF-interconnect, network-on-chip, network-on-chip, packet latency, radio frequency interconnect, radiofrequency integrated circuits, signal propagation, traffic bottlenecks, },
 abstract = {In this paper, we explore the use of multi-band radio frequency interconnect (or RF-I) with signal propagation at the speed of light to provide shortcuts in a many core network-on-chip (NoC) mesh topology. We investigate the costs associated with this technology, and examine the latency and bandwidth benefits that it can provide. Assuming a 400 mm<sup>2</sup> die, we demonstrate that in exchange for 0.13\% of area overhead on the active layer, RF-I can provide an average 13\% (max 18\%) boost in application performance, corresponding to an average 22\% (max 24\%) reduction in packet latency. We observe that RF access points may become traffic bottlenecks when many packets try to use the RF at once, and conclude by proposing strategies that adapt RF-I utilization at runtime to actively combat this congestion. },
}

@inproceedings{4658638,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Hill, M.D.},
 year = {2008},
 pages = {187--187},
 publisher = {IEEE},
 title = {Amdahl&#x2019;s Law in the multicore era},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658638},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658638},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658638.pdf?arnumber=4658638},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Amdahl law, Biographies, Computer architecture, Hardware, Moore's Law, Multicore processing, Parallel processing, Turning, asymmetric core chip, microprocessor chips, multicore chip, multiprocessing systems, sequential execution, software model, symmetric core chip, },
 abstract = {Summary form only given. In this paper, we apply Amdahl's law to several multicore chips variants: symmetric cores, asymmetric cores and dynamic techniques that allow cores to work together on sequential execution. Starting with Amdahl's simple software model, we add a simple hardware model based on fixed chip resources. },
}

@inproceedings{4658633,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Wonyoung Kim and Gupta, M.S. and Gu-Yeon Wei and Brooks, D.},
 year = {2008},
 pages = {123--134},
 publisher = {IEEE},
 title = {System level analysis of fast, per-core DVFS using on-chip switching regulators},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658633},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658633},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658633.pdf?arnumber=4658633},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Costs, Digital systems, Dynamic voltage scaling, Embedded system, Frequency, Microprocessors, Performance analysis, Regulators, System-on-a-chip, Voltage control, chip-multiprocessors, dynamic voltage and frequency scaling, fast per-core DVFS, microprocessor chips, multi-threaded workloads, multi-threading, nanosecond-scale voltage switching, on-chip switching regulators, power aware computing, switching functions, system level analysis, voltage regulators, voltage regulators, },
 abstract = {Portable, embedded systems place ever-increasing demands on high-performance, low-power microprocessor design. Dynamic voltage and frequency scaling (DVFS) is a well-known technique to reduce energy in digital systems, but the effectiveness of DVFS is hampered by slow voltage transitions that occur on the order of tens of microseconds. In addition, the recent trend towards chip-multiprocessors (CMP) executing multi-threaded workloads with heterogeneous behavior motivates the need for per-core DVFS control mechanisms. Voltage regulators that are integrated onto the same chip as the microprocessor core provide the benefit of both nanosecond-scale voltage switching and per-core voltage control. We show that these characteristics provide significant energy-saving opportunities compared to traditional off-chip regulators. However, the implementation of on-chip regulators presents many challenges including regulator efficiency and output voltage transient characteristics, which are significantly impacted by the system-level application of the regulator. In this paper, we describe and model these costs, and perform a comprehensive analysis of a CMP system with on-chip integrated regulators. We conclude that on-chip regulators can significantly improve DVFS effectiveness and lead to overall system energy savings in a CMP, but architects must carefully account for overheads and costs when designing next-generation DVFS systems and algorithms. },
}

@inproceedings{4658632,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Ramos, L. and Bianchini, R.},
 year = {2008},
 pages = {111--122},
 publisher = {IEEE},
 title = {C-Oracle: Predictive thermal management for data centers},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658632},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658632},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658632.pdf?arnumber=4658632},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {C-Oracle, Disaster management, Dynamic voltage scaling, Energy management, Frequency, Internet services, Software performance, Temperature, Thermal loading, Thermal management, Web and internet services, Web server, computer centres, data centers, dynamic voltage/frequency scaling, load redistribution, multitier services, power-dense server clusters, predictive thermal management policy, software infrastructure, thermal management (packaging), },
 abstract = {Designing thermal management policies for todaypsilas power-dense server clusters is currently a challenge, since it is difficult to predict the exact temperature and performance that would result from trying to react to a thermal emergency. To address this challenge, in this paper we propose C-Oracle, a software infrastructure for Internet services that dynamically predicts the temperature and performance impact of different thermal management reactions into the future, allowing the thermal management policy to select the best reaction at each point in time. We experimentally evaluate C-Oracle for thermal management policies based on load redistribution and dynamic voltage/frequency scaling in both single-tier and multi-tier services. Our results show that, regardless of management policy or service organization, C-Oracle enables non-trivial decisions that effectively manage thermal emergencies, while avoiding unnecessary performance degradation. },
}

@inproceedings{4658631,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Xiaorui Wang and Ming Chen},
 year = {2008},
 pages = {101--110},
 publisher = {IEEE},
 title = {Cluster-level feedback power control for performance optimization},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658631},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658631},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658631.pdf?arnumber=4658631},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Control systems, Costs, Energy consumption, Energy management, Feedback, MIMO systems, Optimization, Power control, Power supplies, Power system management, Stability, cluster-level feedback power control, control system synthesis, control-theoretic techniques, feedback, operation cost reduction, optimal multiinput multioutput control theory, pattern clustering, performance optimization, power capacity overload, power consumption, power control, stability, system stability, },
 abstract = {Power control is becoming a key challenge for effectively operating a modern data center. In addition to reducing operation costs, precisely controlling power consumption is an essential way to avoid system failures caused by power capacity overload or overheating due to increasing high-density. Control-theoretic techniques have recently shown a lot of promise on power management thanks to their better control performance and theoretical guarantees on control accuracy and system stability. However, existing work over-simplifies the problem by controlling a single server independently from others. As a result, at the cluster level where multiple servers are correlated by common workloads and share common power supplies, power cannot be shared to improve application performance. In this paper, we propose a cluster-level power controller that shifts power among servers based on their performance needs, while controlling the total power of the cluster to be lower than a constraint. Our controller features a rigorous design based on an optimal multi-input-multi-output control theory. Empirical results demonstrate that our controller outperforms two state-of-the-art controllers, by having better application performance and more accurate power control. },
}

@inproceedings{4658630,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Useche, L. and Guerra, J. and Bhadkamkar, M. and Alarcon, M. and Rangaswami, R.},
 year = {2008},
 pages = {89--100},
 publisher = {IEEE},
 title = {EXCES: External caching in energy saving storage systems},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658630},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658630},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658630.pdf?arnumber=4658630},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Degradation, Disk drives, Energy consumption, Energy efficiency, Energy storage, Kernel, Linux, Nonvolatile memory, Prefetching, Scheduling, cache storage, disk data buffering, disk-based storage subsystem, energy saving storage system, external caching device, power aware computing, power consumption, prefetching, },
 abstract = {Power consumption within the disk-based storage subsystem forms a substantial portion of the overall energy footprint in commodity systems. Researchers have proposed external caching on a persistent, low-power storage device, which we term external caching device (ECD), to minimize disk activity and conserve energy. While recent simulation-based studies have argued in favor of this approach, the lack of an actual system implementation has precluded answering several key questions about external caching systems. We present the design and implementation of EXCES, an external caching system that employs prefetching, caching, and buffering of disk data for reducing disk activity. EXCES addresses important questions related to external caching, including the estimation of future data popularity, I/O indirection, continuous reconfiguration of the ECD contents, and data consistency. We evaluated EXCES with both micro- and macro-benchmarks that address idle, I/O intensive, and real-world workloads. Overall system energy savings was found to lie in the modest 2-14\% range, depending on the workload, in somewhat of a contrast to the higher values predicted by earlier studies. Furthermore, while the CPU and memory overheads of EXCES were well within acceptable limits, we found that flash-based external caching can substantially degrade I/O performance. We believe that external caching systems hold promise. Further improvements in ECD technology, both in terms of their power consumption and performance characteristics can help realize the full potential of such systems. },
}

@inproceedings{4658637,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Venkataramani, G. and Doudalis, I. and Solihin, Y. and Prvulovic, M.},
 year = {2008},
 pages = {173--184},
 publisher = {IEEE},
 title = {FlexiTaint: A programmable accelerator for dynamic taint propagation},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658637},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658637},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658637.pdf?arnumber=4658637},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Acceleration, Application software, Computer bugs, Costs, Filters, FlexiTaint cache, FlexiTaint programmable hardware accelerator, Hardware, Pipelines, Runtime, Security, Software debugging, cache storage, common-case optimization, data flow analysis, dynamic taint propagation cache, out-of-order dataflow engine, packed array memory location, performance-critical front-end pipeline, pipeline processing, program debugging, program verification, runtime checking, runtime tracking approach, software debugging, software handler, software verification, },
 abstract = {This paper presents FlexiTaint, a hardware accelerator for dynamic taint propagation. FlexiTaint is implemented as an in-order addition to the back-end of the processor pipeline, and the taints for memory locations are stored as a packed array in regular memory. The taint propagation scheme is specified via a software handler that, given the operation and the sourcespsila taints, computes the new taint for the result. To keep performance overheads low, FlexiTaint caches recent taint propagation lookups and uses a filter to avoid lookups for simple common-case behavior. We also describe how to implement consistent taint propagation in a multi-core environment. Our experiments show that FlexiTaint incurs average performance overheads of only 1\% for SPEC2000 benchmarks and 3.7\% for Splash-2 benchmarks, even when simultaneously following two different taint propagation policies. },
}

@inproceedings{4658636,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Rogers, B. and Chenyu Yan and Chhabra, S. and Prvulovic, M. and Solihin, Y.},
 year = {2008},
 pages = {161--172},
 publisher = {IEEE},
 title = {Single-level integrity and confidentiality protection for distributed shared memory multiprocessors},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658636},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658636},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658636.pdf?arnumber=4658636},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Application software, Computer security, Credit cards, Cryptography, Data security, Distributed computing, Hardware, Large-scale systems, Multiprocessing systems, Protection, SPLASH-2 application, architectural support, confidentiality protection, credit card number, cryptographic operation, cryptography, customer record, data integrity, data integrity, data privacy, data stealing, data tampering, distributed shared memory multiprocessors, distributed shared memory systems, financial data, hardware-based attack, multiprocessor computer systems, multiprocessor interconnection networks, performance overhead, point-to-point based interconnection network, sensitive data, single-level integrity, software-based attack, },
 abstract = {Multiprocessor computer systems are currently widely used in commercial settings to run critical applications. These applications often operate on sensitive data such as customer records, credit card numbers, and financial data. As a result, these systems are the frequent targets of attacks because of the potentially significant gain an attacker could obtain from stealing or tampering with such data. This provides strong motivation to protect the confidentiality and integrity of data in commercial multiprocessor systems through architectural support. Architectural support is able to protect against software-based attacks, and is necessary to protect against hardware-based attacks. In this work, we propose architectural mechanisms to ensure data confidentiality and integrity in Distributed Shared Memory multiprocessors which utilize a point-to-point based interconnection network. Our approach improves upon previous work in this area, mainly in the fact that our approach reduces performance overheads by significantly reducing the amount of cryptographic operations required. Evaluation results show that our approach can protect data confidentiality and integrity in a 16-processor DSM system with an average overhead of 1.6\% and a maximum of only 7\% across all SPLASH-2 applications. },
}

@inproceedings{4658635,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Ramirez, T. and Pajuelo, A. and Santana, O.J. and Valero, M.},
 year = {2008},
 pages = {149--158},
 publisher = {IEEE},
 title = {Runahead Threads to improve SMT performance},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658635},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658635},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658635.pdf?arnumber=4658635},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Degradation, Dynamic scheduling, Multithreading, Prefetching, Processor scheduling, Registers, Resource management, SMT architecture, SMT performance, Surface-mount technology, Throughput, Yarn, blocking memory operation, data prefetching, dynamic resource control, memory-bound thread, memory-level parallelism, multi-threading, resource allocation, resource contention, runahead threads, simultaneous multithreaded processors, software architecture, static fetch, storage management, },
 abstract = {In this paper, we propose runahead threads (RaT) as a valuable solution for both reducing resource contention and exploiting memory-level parallelism in simultaneous multithreaded (SMT) processors. Our technique converts a resource intensive memory-bound thread to a speculative light thread under long-latency blocking memory operations. These speculative threads prefetch data and instructions with minimal resources, reducing critical resource conflicts between threads. We compare an SMT architecture using RaT to both state-of-the-art static fetch policies and dynamic resource control policies. In terms of throughput and fairness, our results show that RaT performs better than any other policy. The proposed mechanism improves average throughput by 37\% regarding previous static fetch policies and by 28\% compared to previous dynamic resource scheduling mechanisms. RaT also improves fairness by 36\% and 30\% respectively. In addition, the proposed mechanism permits register file size reduction of up to 60\% in a SMT processor without performance degradation. },
}

@inproceedings{4658634,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Subramaniam, S. and Prvulovic, M. and Loh, G.H.},
 year = {2008},
 pages = {137--148},
 publisher = {IEEE},
 title = {PEEP: Exploiting predictability of memory dependences in SMT processors},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658634},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658634},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658634.pdf?arnumber=4658634},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Delay, Dynamic scheduling, Educational institutions, Microarchitecture, Multithreading, Processor scheduling, Resource management, SMT processor dynamic resource scheduling, Surface-mount technology, Throughput, Yarn, early parole mechanism, front-end pipeline, memory dependence predictability, memory fetch policy, multi-threading, pipeline processing, proactive exclusion technique, processor scheduling, resource allocation, simultaneous multithreading, speculative memory disambiguation, storage management, system throughput, thread long-latency stall, },
 abstract = {Simultaneous multithreading (SMT) attempts to keep a dynamically scheduled processorpsilas resources busy with work from multiple independent threads. Threads with long-latency stalls, however, can lead to a reduction in overall throughput because they occupy many of the critical processor resources. In this work, we first study the interaction between stalls caused by ambiguous memory dependences and SMT processing. We then propose the technique of proactive exclusion (PE) where the SMT fetch unit stops fetching from a thread when a memory dependence is predicted to exist. However, after the dependence has been resolved, the thread is delayed waiting for new instructions to be fetched and delivered down the front-end pipeline. So we introduce an early parole (EP) mechanism that exploits the predictability of dependence-resolution delays to restart fetch of an excluded thread so that the instructions reach the execution core just as the original dependence resolves. We show that combining these two techniques (PEEP) yields a 16.9\% throughput improvement on a 4-way SMT processor that supports speculative memory disambiguation. These strong results indicate that a fetch policy that is cognizant of future stalls considerably improves the throughput of an SMT machine. },
}

@inproceedings{501183,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Kontothanassis, L.I. and Scott, M.L.},
 year = {1996},
 pages = {166--177},
 publisher = {IEEE},
 title = {Using memory-mapped network interfaces to improve the performance of distributed shared memory},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501183},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501183},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501183.pdf?arnumber=501183},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Access protocols, Coherence, Costs, Delay, Hardware, Merging, Message passing, Network interfaces, Parallel algorithms, Parallel programming, bandwidth, cache fills, distributed memory systems, distributed shared memory, fine-grain access faults, latency, memory-mapped, memory-mapped network interfaces, message passing, network interfaces, network interfaces, parallel algorithms, protocols, shared memory systems, },
 abstract = {Shared memory is widely believed to provide an easier programming model than message passing for expressing parallel algorithms. Distributed Shared Memory (DSM) systems provide the illusion of shared memory on top of standard message passing hardware at very low implementation cost, but provide acceptable performance for only a limited class of applications. We argue that the principal sources of overhead overhead in DSM systems can be dramatically reduced with modest amounts of hardware support (substantially less than is required for hardware cache coherence). Specifically, we present and evaluate a family of protocols designed to exploit hardware support for a global, but non-coherent, physical address space. We consider systems both with and without remote cache fills, fine-grain access faults, ``doubled" writes to local and remote memory, and merging write buffers. We also consider varying levels of latency and bandwidth. We evaluate our protocols using execution driven simulation, comparing them to each other and to a state-of-the-art protocol for traditional message-based networks. For the programs in our application suite, protocols taking advantage of the global address space improve performance by a minimum of 50\% and sometimes by as much as an order of magnitude },
}

@inproceedings{501182,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Blumrich, M.A. and Dubnicki, C. and Felten, E.W. and Kai Li},
 year = {1996},
 pages = {154--165},
 publisher = {IEEE},
 title = {Protected, user-level DMA for the SHRIMP network interface},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501182},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501182},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501182.pdf?arnumber=501182},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Control systems, Costs, DMA, DMA transfers, Drives, Hardware, Kernel, Memory, Network interfaces, Operating systems, Permission, Protection, SHRIMP network interface, User-level Direct Memory Access, address translation, computer interfaces, computer networks, file organisation, network interfaces, operating system, permission checking, virtual memory translation, },
 abstract = {Traditional DMA requires the operating system to perform many tasks to initiate a transfer, with overhead on the order of hundreds or thousands of CPU instructions. This paper describes a mechanism, called User-level Direct Memory Access (UDMA), for initiating DMA transfers of input/output data, with full protection, at a cost of only two user-level memory references. The UDMA mechanism uses existing virtual memory translation hardware to perform permission checking and address translation without kernel involvement. The implementation of the UDMA mechanism is simple, requiring a small extension to the traditional DMA controller and minimal operating system kernel support. The mechanism can be used with a wide variety of I/O devices including network interfaces, data storage devices such as disks and tape drives, and memory-mapped devices such as graphics frame-buffers. As an illustration, we describe how we used UDMA in building network interface hardware for the SHRIMP multicomputer },
}

@inproceedings{501181,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Markatos, E.P. and Katevenis, M.G.H.},
 year = {1996},
 pages = {144--153},
 publisher = {IEEE},
 title = {Telegraphos: high-performance networking for parallel processing on workstation clusters},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501181},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501181},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501181.pdf?arnumber=501181},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Aggregates, Computational modeling, Microcomputers, Multimedia computing, Network interfaces, Parallel processing, Power engineering and energy, Prototypes, Switches, Telegraphos, Workstations, computer networks, distributed system, high-performance networking, network interface, network interfaces, network operating systems, parallel processing, parallel processing, remote atomic operations, remote reads, remote writes, shared memory systems, shared-memory support, workstation cluster, workstation clusters, },
 abstract = {Networks of workstations and high-performance microcomputers have been rarely used for running high-performance applications like multimedia, simulations, scientific and engineering applications, because, although they have significant aggregate computing power, they lack the support for efficient message-passing and shared-memory communication. In this paper we present Telegraphos, a distributed system that provides efficient shared-memory support on top of a workstation cluster. We focus on the network interface of Telegraphos that provides a variety of shared-memory operations like remote reads, remote writes, remote atomic operations, all launched from user level without any intervention of the operating system. Telegraphos I, the first Telegraphos prototype has been implemented. Emphasis was put on rapid prototyping, so the technology used was conservative: FPGA's, SRAM's, and TTL buffers. Telegraphos II, is the single-chip version of the Telegraphos architecture; its switch was implemented and its network interface is being debugged },
}

@inproceedings{501180,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Guihai Chen and Lau, F.C.M.},
 year = {1996},
 pages = {130--138},
 publisher = {IEEE},
 title = {Shuffle-Ring: overcoming the increasing degree of hypercube},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501180},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501180},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501180.pdf?arnumber=501180},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {ASCEND/DESCEND, Computer networks, Computer science, Concurrent computing, DeBruijn Network, Fault tolerance, Fault tolerant systems, Hypercubes, Lead, Microwave integrated circuits, Multiprocessor interconnection networks, Routing, Shuffle-Ring, easy routing, fault tolerance, hypercube, interconnection network, logarithmic diameter, multiprocessor interconnection networks, optimal fault tolerance, parallel architectures, r-Shuffle, },
 abstract = {The hypercube as a parallel interconnection network has been studied by many for tens of years due to its many merits. However, its increasing node degree is an obvious weakness. Some networks such as the Cube-Connected Circle and the DeBruijn Network have been proposed to overcome the increasing degree of the hypercube. In this paper, we present a new cost-effective network which outperforms the cube network. It can overcome the increasing degree of the cube network while keeping the advantages of the cube network such as logarithmic diameter, easy routing, optimal fault tolerance, and suitability for the ASCEND/DESCEND class of parallel problems. Furthermore, the proposed network achieves the logarithmic diameter with a very small constant node degree, 3 or 4 },
}

@inproceedings{569581,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Yang, L. and Torrellas, J.},
 year = {1997},
 pages = {4--13},
 publisher = {IEEE},
 title = {Speeding up the memory hierarchy in Flat COMA multiprocessors},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569581},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569581},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569581.pdf?arnumber=569581},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Contracts, Cost function, Delay, Flat COMA multiprocessors, Hardware, Memory architecture, Operating systems, Programming profession, Random access memory, Research and development, World Wide Web, cache storage, direct-mapped cache, invalidation cache, memory access latencies, memory architecture, memory hierarchy, multiprocessing systems, network hops, operating system, optimisation, page migration, page-mode DRAMs, scalable Flat cache only memory architectures, },
 abstract = {Scalable Flat Cache Only Memory Architectures (Flat COMA) are designed for reduced memory access latencies while minimizing programmer and operating system involvement. Indeed, to keep memory access latencies low, neither the programmer needs to perform clever data placement nor the operating system needs to perform page migration. The hardware automatically replicates the data and migrates it to the attraction memories of the nodes that use it. Unfortunately, part of the latency of memory accesses is superfluous. In particular, reads often perform unnecessary attraction memory accesses, require too many network hops, or perform necessary attraction memory accesses inefficiently. In this paper, we propose relatively inexpensive schemes that address these three problems. To eliminate unnecessary attraction memory accesses, we propose a small direct-mapped cache called Invalidation Cache (IVC). To reduce the number of network hops, the IVC is augmented with hint pointers to processors. These hint pointers are faster and have more applicability than in older hint schemes. Finally, to speed up necessary accesses to set-associative attraction memories, we optimize the locality of windows in page-mode DRAMs. We evaluate these optimizations with 32-processor simulations of 8 Splash and Perfect Suite applications. We show that these optimizations speed up the applications by an average of 20\% at a modest cost },
}

@inproceedings{501186,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Chalasani, S. and Boppana, R.V.},
 year = {1996},
 pages = {201--210},
 publisher = {IEEE},
 title = {Fault-tolerance with multimodule routers},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501186},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501186},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501186.pdf?arnumber=501186},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {2-dimensional torus, Communication switching, Computer science, Cray T3D, Fault tolerance, Hardware, Logic, Mesh networks, Network topology, Pins, Routing, System recovery, fault tolerant computing, fault-tolerance, fault-tolerant PDRs, fault-tolerant routing, interprocessor communication, mesh networks, multimodule routers, multiprocessor interconnection networks, multiprocessors, network routing, partitioned dimension-order routers, routing logic, switching hardware, },
 abstract = {The current multiprocessors such as Cray T3D support interprocessor communication using partitioned dimension-order routers (PDRs). In a PDR implementation, the routing logic and switching hardware is partitioned into multiple modules, with each module suitable for implementation as a chip. This paper proposes a method to incorporate fault-tolerance into such routers with simple changes to the router structure and logic. The previously known fault-tolerant routing methods assume centralized crossbar based routers and are not applicable to multiprocessors with PDRs. The proposed technique works for convex fault model, using only local knowledge of faults. Using the proposed techniques and as few as four virtual channels per physical channel, torus networks with PDRs can handle faults without compromising deadlock- and livelock-freedom. Simulations for 2-dimensional torus and mesh networks show that the resulting fault-tolerant PDRs have performances similar to those of the crossbar based routers },
}

@inproceedings{5463058,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Kundu, S. and Rangaswami, R. and Dutta, K. and Zhao, M.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Application performance modeling in a virtualized environment},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5463058},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5463058},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05463058.pdf?arnumber=5463058},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Application virtualization, Artificial neural networks, Availability, Capacity planning, Hardware, Predictive models, Resource management, Resource virtualization, Virtual machining, Virtual prototyping, },
 abstract = {Performance models provide the ability to predict application performance for a given set of hardware resources and are used for capacity planning and resource management. Traditional performance models assume the availability of dedicated hardware for the application. With growing application deployment on virtualized hardware, hardware resources are increasingly shared across multiple virtual machines. In this paper, we build performance models for applications in virtualized environments. We identify a key set of virtualization architecture independent parameters that influence application performance for a diverse and representative set of applications. We explore several conventional modeling techniques and evaluate their effectiveness in modeling application performance in a virtualized environment. We propose an iterative model training technique based on artificial neural networks which is found to be accurate across a range of applications. The proposed approach is implemented as a prototype in Xen-based virtual machine environments and evaluated for accuracy, sensitivity to the training process, and overhead. Median modeling error in the range 1.16-6.65\% across a diverse application set and low modeling overhead suggest the suitability of our approach in production virtualized environments. },
}

@inproceedings{501184,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Libeskind-Hadas, R. and Watkins, K. and Hehre, T.},
 year = {1996},
 pages = {180--190},
 publisher = {IEEE},
 title = {Fault-tolerant multicast routing in the mesh with no virtual channels},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501184},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501184},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501184.pdf?arnumber=501184},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Computer science, Delay, Educational institutions, Fault tolerance, Multicast algorithms, Radio access networks, Routing, Synchronization, System recovery, Topology, deadlock-free, distributed memory systems, fault tolerant computing, fault-tolerant, fault-tolerant multicast routing, multicast routing, multicomputers, multiprocessor interconnection networks, path-based routing, performance degradation, pseudo-Hamiltonian, virtual channels, },
 abstract = {This paper addresses the problem of fault-tolerant multicast routing in wormhole-routed multicomputers. We present a new pseudo-Hamiltonian path-based routing methodology for constructing deadlock-free multicast routing algorithms requiring no virtual channels. This technique is applied to construct the first fault-tolerant multicast routing algorithm for the mesh that requires no virtual channels. Simulation results indicate that this technique results in minimal performance degradation in the presence of a large number of node and channel faults },
}

@inproceedings{5463056,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Vasan, A. and Sivasubramaniam, A. and Shimpi, V. and Sivabalan, T. and Subbiah, R.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Worth their watts? - an empirical study of datacenter servers},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5463056},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5463056},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05463056.pdf?arnumber=5463056},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Aggregates, Cooling, Costs, Energy consumption, Network servers, Power generation, Power measurement, Production, Technological innovation, Web server, },
 abstract = {The management of power consumption in datacenters has become an important problem. This needs a systematic evaluation of the as-is scenario to identify potential areas for improvement and quantify the impact of any strategy. We present a measurement study of a production datacenter from a joint perspective of power and performance at the individual server level. Our observations help correlate power consumption of production servers with their activity, and identify easily implementable improvements. We find that production servers are underutilized from an activity perspective; are overrated from a power perspective; execute temporally similar workloads over a granularity of weeks; do not idle efficiently; and have power consumptions that are well tracked by their CPU utilizations. Our measurements suggest the following steps for improvement: staggering periodic activities on servers; enabling deeper sleep states; and provisioning based on measurement. },
}

@inproceedings{5463057,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Vujic, N. and Gonzalez, M. and CabarcasÂ¿, F. and Ramirez, A. and Martorell, X. and Ayguade, E.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {DMA++: on the fly data realignment for on-chip memories},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5463057},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5463057},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05463057.pdf?arnumber=5463057},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Bandwidth, Computer architecture, Digital signal processing, Hardware, Instruction sets, Memory management, Multicore processing, Multimedia computing, Random access memory, Software performance, },
 abstract = {Multimedia extensions based on Single-Instruction Multiple-Data (SIMD) units are widespread. They are used both in processors and accelerators (e.g., the Cell SPEs), since some time ago. SIMD units have usually big memory alignment constraints in order to meet power requirements and design simplicity. This increases the complexity of the code generated by the compiler, as in the general case, the compiler cannot be sure of the proper alignment of data. For that, the ISA provides either unaligned memory load and store instructions, or a special set of instructions to perform the realignments in software. In this paper, we propose a hardware realignment unit that takes advantage of the DMA transfers needed in accelerators with local memories. While the data is being transferred, it is realigned on the fly by our realignment unit, and stored with the proper alignment in the accelerator memory. The accelerator can then access the data with no special instructions. Finally, the data is realigned properly also when put back to main memory. Our experiments with four applications show that with our approach, the bandwidth of the DMA transfers is not penalized. And the performance of the synthetic benchmarks shows that aligned code is 1.5 to 2 times better with respect using unaligned code. },
}

@inproceedings{501189,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Wen-Jann Yang and Sridhar, R. and Demjanenko, V.},
 year = {1996},
 pages = {232--241},
 publisher = {IEEE},
 title = {Parallel intersecting compressed bit vectors in a high speed query server for processing postal addresses},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501189},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501189},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501189.pdf?arnumber=501189},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Computational modeling, Computer architecture, Costs, Databases, Embedded computing, Hardware, High performance computing, Parallel architectures, Parallel processing, System performance, file servers, general purpose computer, high speed query server, offset addresses, parallel architecture, parallel architectures, parallel intersecting compressed bit vectors, postal addresses processing, query processing, specialized hardware, },
 abstract = {A parallel architecture is proposed for a high speed query server to process postal addresses with several fields. For a given component in a field, the offset addresses of records which contain the component in a postal address database are coded into a Compressed Bit Vector (CBV). Finding the appropriate CBVs and performing intersections to get matching offset addresses are key bottleneck for the performance in the query server. They are accomplished by a specialized hardware embedded in a general purpose computer for a cost effective solution. This hardware directly operates on the CBVs using parallel schemes. The architecture and algorithms for expanding a CBV, for synchronizing the parallel processing of the processing units, and for balancing the load in the pipelined stages are presented with simulation results },
}

@inproceedings{501188,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Reisner, J.A. and Wailes, T.S.},
 year = {1996},
 pages = {222--231},
 publisher = {IEEE},
 title = {A cache coherency protocol for optically connected parallel computer systems},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501188},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501188},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501188.pdf?arnumber=501188},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Computational modeling, Computer architecture, Concurrent computing, Fiber lasers, Laser tuning, MCA, Multiple Channel Architecture, Optical computing, Optical receivers, Optical transmitters, Protocols, Tunable circuits and devices, cache coherency protocol, cache storage, massively parallel systems, memory protocols, optical interconnection network, optical interconnections, optically connected, parallel architectures, parallel computer systems, scalability, shared cache, shared data, shared memory systems, system performanc, },
 abstract = {A cache coherency protocol was developed for the Multiple Channel Architecture (MCA), a proposed computer architecture that uses an optical interconnection network to overcome many of the problems associated with internode communication in massively parallel systems. A directory-based protocol was attempted, but testing revealed a serious scalability problem associated with the collection of acknowledgement packets. An alternative scheme, which employs snooping on a special-purpose, time-division multiplexed optical channel performed well. Additionally, the benefits of a separate cache for shared data were examined. Simulations demonstrate that the coherency protocol improves system performance and scalability, and that additional performance gains may be attained by customizing shared cache parameters },
}

@inproceedings{569588,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Dahlgren, F. and Landin, A.},
 year = {1997},
 pages = {14--23},
 publisher = {IEEE},
 title = {Reducing the replacement overhead in bus-based COMA multiprocessors },
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569588},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569588},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569588.pdf?arnumber=569588},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Aggregates, Application software, Cache memory, Computer science, Costs, attraction memories, bus-based COMA multiprocessors, cache blocks replication, cache storage, cache-only memory architecture, digital simulation, loose-inclusion, memory architecture, memory overhead, memory space, multiprocessing systems, no-inclusion, replacement traffic, shared memory accesses, simulation results, },
 abstract = {In a multiprocessor with a Cache-Only Memory Architecture (COMA) all available memory is used to form large cache memories called attraction memories. These large caches help to satisfy shared memory accesses locally, reducing the need for node-external communication. However since a COMA has no back-up main memory, blocks replaced from one attraction memory must be relocated into another attraction memory. To keep memory overhead low, it is desirable to have most of the memory space filled with unique data. This leaves little space left for replication of cache blocks, resulting in that replacement traffic may become excessive. We have studied two schemes for removing the traditional demand for full inclusion between the lower-level caches and the attraction memory: the loose-inclusion and no-inclusion schemes. They differ in efficiency but also in implementation cost. Detailed simulation results show that the replacement traffic is reduced substantially for both approaches, indicating that breaking inclusion is an efficient way to bound the sensitivity for high memory pressure in COMA machines },
}

@inproceedings{1598123,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Manovit, C. and Hangal, S.},
 year = {2006},
 pages = { 166-- 175},
 publisher = {IEEE},
 title = {Completely verifying memory consistency of test program executions},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598123},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598123},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598123.pdf?arnumber=1598123},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { NP-complete,  Sun Microsystems,  commercial-grade shared memory multiprocessor,  data race,  memory consistency verification,  microprogramming,  parallel architectures,  performance evaluation,  pseudo-random test program execution,  shared memory systems, Algorithm design and analysis, Computer bugs, Lamps, Multiprocessing systems, Performance analysis, Software testing, Spine, Sun, System testing, Vehicle crash testing, },
 abstract = {An important means of validating the design of commercial-grade shared memory multiprocessors is to run a large number of pseudo-random test programs on them. However, when intentional data races are placed in a test program, there may be many correct results according to the memory consistency model supported by the system. For popular memory models like SC and TSO, the problem of verifying correctness of an execution is known to be NP-complete. As a result, analysis techniques implemented in the past have been incomplete: violations of the memory model are flagged if provable, otherwise the result is inconclusive and it is assumed optimistically that the machine's results are correct. In this paper, we describe for the first time a practical, new algorithm which can solve this problem with certainty, thus ensuring that incorrect behavior of a large, complex multiprocessor cannot escape. We present results of our analysis algorithm on test programs run on a newly designed multiprocessor system built by Sun Microsystems. We show that our algorithm performs very well, typically analyzing a program with 512 K memory operations distributed across 60 processors within a few minutes. Our algorithm runs in less than 2.6 times the time taken by an incomplete baseline algorithm which may miss errors. Our approach greatly increases the confidence in the correctness of the results generated by the multiprocessor, and allows us to potentially uncover more bugs in the design than was previously possible. },
}

@inproceedings{1598122,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Venkatesan, R.K. and Herr, S. and Rotenberg, E.},
 year = {2006},
 pages = { 155-- 165},
 publisher = {IEEE},
 title = {Retention-aware placement in DRAM (RAPID): software methods for quasi-non-volatile DRAM},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598122},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598122},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598122.pdf?arnumber=1598122},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { DRAM chips,  off-the-shelf DRAM chip,  paged storage,  power consumption,  quasinonvolatile DRAM,  refresh power reduction,  retention-aware placement,  shortest retention time,  software methods,  worst-case refresh period, Embedded software, Embedded system, Energy states, Nonvolatile memory, Random access memory, Semiconductor device measurement, Software measurement, Temperature, Time measurement, Wireless sensor networks, },
 abstract = {Measurements of an off-the-shelf DRAM chip confirm that different cells retain information for different amounts of time. This result extends to DRAM rows, or pages (retention time of a page is defined as the shortest retention time among its constituent cells). Currently, a single worst-case refresh period is selected based on the page with the shortest retention time. Even with refresh optimized for room temperature, the worst page limits the safe refresh period to no longer than 500 ms. Yet, 99\% and 85\% of pages have retention times above 3 seconds and 10 seconds, respectively. We propose retention-aware placement in DRAM (RAPID), novel software approaches that can exploit off-the-shelf DRAMs to reduce refresh power to vanishingly small levels approaching non-volatile memory. The key idea is to favor longer-retention pages over shorter-retention pages when allocating DRAM pages. This allows selecting a single refresh period that depends on the shortest-retention page among populated pages, instead of the shortest-retention page overall. We explore three versions of RAPID and observe refresh energy savings of 83\%, 93\%, and 95\%, relative to the best temperature-compensated refresh. RAPID with off-the-shelf DRAM also approaches the energy levels of idealized techniques that require custom DRAM support. },
}

@inproceedings{1598121,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Pujara, P. and Aggarwal, A.},
 year = {2006},
 pages = { 145-- 154},
 publisher = {IEEE},
 title = {Increasing the cache efficiency by eliminating noise},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598121},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598121},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598121.pdf?arnumber=1598121},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { L1 data cache,  bandwidth requirement,  cache efficiency,  cache miss rate,  cache noise prediction,  cache storage,  cache utilization,  code-context predictor,  energy consumption,  noise elimination,  power consumption,  spatial locality,  subblocked cache,  to-be-referenced data fetch, Bandwidth, Energy consumption, Hardware, History, Noise level, Noise reduction, Prefetching, },
 abstract = {Caches are very inefficiently utilized because not all the excess data fetched into the cache, to exploit spatial locality, is utilized. We define cache utilization as the percentage of data brought into the cache that is actually used. Our experiments showed that Level 1 data cache has a utilization of only about 57\%. In this paper, we show that the useless data in a cache block (cache noise) is highly predictable. This can be used to bring only the to-be-referenced data into the cache on a cache miss, reducing the energy, cache space, and bandwidth wasted on useless data. Cache noise prediction is based on the last words usage history of each cache block. Our experiments showed that a code-context predictor is the best performing predictor and has a predictability of about 95\%. In a code context predictor, each cache block belongs to a code context determined by the upper order PC bits of the instructions that fetched the cache block. When applying cache noise prediction to L1 data cache, we observed about 37\% improvement in cache utilization, and about 23\% and 28\% reduction in cache energy consumption and bandwidth requirement, respectively. Cache noise mispredictions increased the miss rate by 0.1\% and had almost no impact on instructions per cycle (IPC) count. When compared to a sub-blocked cache, fetching the to-be-referenced data resulted in 97\% and 44\% improvement in miss rate and cache utilization, respectively. The sub-blocked cache had a bandwidth requirement about 35\% of the cache noise prediction based approach. },
}

@inproceedings{1598120,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Vivek Pandey and Jiang, W. and Zhou, Y. and Bianchini, R.},
 year = {2006},
 pages = { 133-- 144},
 publisher = {IEEE},
 title = {DMA-aware memory energy management},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598120},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598120},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598120.pdf?arnumber=1598120},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { DMA,  computer architecture,  data server,  database server trace,  main memory energy consumption,  memory energy management,  power consumption,  storage management,  storage server,  trace-driven simulator, Bandwidth, Bridges, Computer science, Concurrent computing, Databases, Energy consumption, Energy management, File servers, Memory management, Network servers, },
 abstract = {As increasingly larger memories are used to bridge the widening gap between processor and disk speeds, main memory energy consumption is becoming increasingly dominant. Even though much prior research has been conducted on memory energy management, no study has focused on data servers, where main memory is predominantly accessed by DMAs instead of processors. In this paper, we study DMA-aware techniques for memory energy management in data servers. We first characterize the effect of DMA accesses on memory energy and show that, due to the mismatch between memory and I/O bus band-widths, significant energy is wasted when memory is idle but still active during DMA transfers. To reduce this waste, we propose two novel performance-directed energy management techniques that maximize the utilization of memory devices by increasing the level of concurrency between multiple DMA transfers from different I/O buses to the same memory device. We evaluate our techniques using a detailed trace-driven simulator, and storage and database server traces. The results show that our techniques can effectively minimize the amount of idle energy waste during DMA transfers and, consequently, conserve up to 38.6\% more memory energy than previous approaches while providing similar performance. },
}

@inproceedings{1598127,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Recio, R.},
 year = {2006},
 pages = { 201-- 201},
 publisher = {IEEE},
 title = {Industrial Perspectives: System IO Network Evolution - Closing Requirement Gaps},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598127},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598127},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598127.pdf?arnumber=1598127},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = {Bandwidth, Energy management, Power system management, Power system reliability, System-on-a-chip, },
 abstract = {},
}

@inproceedings{1598126,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Yavatkar, R.},
 year = {2006},
 pages = { 201-- 201},
 publisher = {IEEE},
 title = {Industrial Perspectives: Platform Design Challenges with Many cores},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598126},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598126},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598126.pdf?arnumber=1598126},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = {Bandwidth, Energy management, Power system management, Power system reliability, System-on-a-chip, },
 abstract = {},
}

@inproceedings{1598125,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Yu, H. and Sahoo, R.K. and Howson, C. and Almasi, G. and Castanos, J.G. and Gupta, M. and Moreira, J.E. and Parker, J.J. and Engelsiepen, T.E. and Ross, R.B. and Thakur, R. and Latham, R. and Gropp, W.D.},
 year = {2006},
 pages = { 187-- 196},
 publisher = {IEEE},
 title = {High performance file I/O for the Blue Gene/L supercomputer},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598125},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598125},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598125.pdf?arnumber=1598125},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { Blue Gene/L supercomputer,  General Parallel File System,  MPI,  application program interfaces,  benchmark testing,  data-intensive application,  file organisation,  functional partitioning design,  hierarchical partitioning,  high performance file I/O,  message passing,  parallel HDF5,  parallel I/O benchmark,  parallel NetCDF,  parallel architectures,  parallel file I/O architecture,  parallel machines, Application software, Bandwidth, Computer architecture, Concurrent computing, File systems, Large-scale systems, Parallel programming, Scalability, Supercomputers, System software, },
 abstract = {Parallel I/O plays a crucial role for most data-intensive applications running on massively parallel systems like Blue Gene/L that provides the promise of delivering enormous computational capability. We designed and implemented a highly scalable parallel file I/O architecture for Blue Gene/L, which leverages the benefit of the hierarchical and functional partitioning design of the system software with separate computational and I/O cores. The architecture exploits the scalability aspect of GPFS (General Parallel File System) at the backend, while using MPI I/O as an interface between the application I/O and the file system. We demonstrate the impact of our high performance I/O solution for Blue Gene/L with a comprehensive evaluation that consists of a number of widely used parallel I/O benchmarks and I/O intensive applications. Our design and implementation is not only able to deliver at least one order of magnitude speed up in terms of I/O bandwidth for a real-scale application HOMME (achieving aggregate bandwidth of 1.8 GB/Sec and 2.3 GB/Sec for write and read accesses, respectively), but also supports high-level parallel I/O data interfaces such as parallel HDF5 and parallel NetCDF scaling up to a large number of processors. },
}

@inproceedings{1598124,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Youngjae Kim and Gurumurthi, S. and Sivasubramaniam, A.},
 year = {2006},
 pages = {176--186},
 publisher = {IEEE},
 title = {Understanding the performance-temperature interactions in disk I/O of server workloads},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598124},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598124},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598124.pdf?arnumber=1598124},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = {Computer science, Cooling, Disk drives, Energy management, Environmental management, Performance analysis, Power system management, Temperature distribution, Thermal engineering, Thermal management, disc drives, disc storage, disk I/O characteristics, disk drive, dynamic thermal management, microbenchmark, performance evaluation, performance-temperature interaction, server workload, storage system, temperature optimization, thermal profile, },
 abstract = {This paper describes the first infrastructure for integrated studies of the performance and thermal behavior of storage systems. Using microbenchmarks running on this infrastructure, we first gain insight into how I/O characteristics can affect the temperature of disk drives. We use this analysis to identify the most promising, yet simple, "knobs" for temperature optimization of high speed disks, which can be implemented on existing disks. We then analyze the thermal profiles of real workloads that use such disk drives in their storage systems, pointing out which knobs are most useful for dynamic thermal management when pushing the performance envelope. },
}

@inproceedings{1598129,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Nakano, J. and Montesinos, P. and Gharachorloo, K. and Torrellas, J.},
 year = {2006},
 pages = { 200-- 211},
 publisher = {IEEE},
 title = {ReViveI/O: efficient handling of I/O in highly-available rollback-recovery servers},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598129},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598129},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598129.pdf?arnumber=1598129},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { I/O handling,  Linux,  Linux-based prototype,  ReViveI/O,  database server,  fault tolerant computing,  hardware-assisted rollback-recovery server,  mean time to repair,  shared memory systems,  system recovery, Application software, Checkpointing, Databases, Delay, Frequency, Hardware, Interleaved codes, Proposals, Prototypes, Web server, },
 abstract = {The increasing demand for reliable computers has led to proposals for hardware-assisted rollback of memory state. Such approach promises major reductions in mean time to repair (MTTR). The benefits are especially compelling for database servers, where existing recovery software typically leads to downtimes of tens of minutes. Unfortunately, adoption of such proposals is hindered by the lack of efficient mechanisms for I/O recovery. This paper presents and evaluates ReViveI/O, a scheme for I/O undo and redo that is compatible with mechanisms for hardware-assisted rollback of memory state. We have fully implemented a Linux-based prototype that shows that low-overhead, low-MTTR recovery of I/O is feasible. For 20-120 ms between checkpoints, a throughput-oriented workload such as TPC-C has negligible overhead. Moreover, for 50 ms or less between checkpoints, the response time of a latency-bound workload such as WebStone remains tolerable. In all cases, the recovery time of ReViveI/O is practically negligible. The result is a cost-effective highly-available server. },
}

@inproceedings{1598128,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Emma, P.},
 year = {2006},
 pages = { 201-- 201},
 publisher = {IEEE},
 title = {Industrial Perspectives: The Next Roadblocks in SOC Evolution: On-Chip Storage Capacity and Off-Chip Bandwidth},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598128},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598128},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598128.pdf?arnumber=1598128},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = {Bandwidth, Energy management, Power system management, Power system reliability, System-on-a-chip, },
 abstract = {},
}

@inproceedings{744368,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Pirvu, M. and Bhuyan, L. and Ni, N.},
 year = {1999},
 pages = {228--235},
 publisher = {IEEE},
 title = {The impact of link arbitration on switch performance},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744368},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744368},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744368.pdf?arnumber=744368},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Computer networks, Computer science, Context modeling, Delay, Electronic mail, Multiprocessing systems, Multiprocessor interconnection networks, Switches, Tellurium, Throughput, computational complexity, computer networks, computer networks, execution driven simulations, hot-spot problem, interconnection networks, link arbitration, look-ahead arbitration, multiprocessor interconnection networks, multiprocessors, performance, performance evaluation, switch performance, synthetic workload, virtual channel reservation, virtual channels, },
 abstract = {Switch design for interconnection networks plays an important role in the overall performance of multiprocessors and computer networks. In this paper we study the impact of one parameter in the switch design space, link arbitration. We demonstrate that link arbitration can be a determining factor in the performance of current networks. Moreover, we expect increased research focus on arbitration techniques to become a trend in the future, as switch architectures evolve towards increasing the number of virtual channels and input ports. In the context of a state-of-the-art switch design we use both synthetic workload and execution driven simulations to compare several arbitration policies. Furthermore, we devise a new arbitration method, Look-Ahead arbitration. Under heavy traffic conditions the Look-Ahead policy provides important improvements over traditional arbitration schemes without a significant increase in hardware complexity. Also, we propose a priority based policy that is capable of reducing the execution time of parallel applications. Lastly, we enhance the arbitration policies by a supplemental mechanism, virtual channel reservation, intended to alleviate the hot-spot problem },
}

@inproceedings{386526,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {John, L.K. and Reddy, V. and Hulina, P.T. and Coraor, L.D.},
 year = {1995},
 pages = {370--379},
 publisher = {IEEE},
 title = {Program balance and its impact on high performance RISC architectures},
 date = {1995},
 doi = {10.1109/HPCA.1995.386526},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386526},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386526.pdf?arnumber=386526},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Bandwidth, Circuit synthesis, Computer aided instruction, Computer architecture, Computer science, Kernel, MIPS, Parallel processing, Pipelines, Power engineering computing, Reduced instruction set computing, computation tasks, computer architects, computer architecture, floating point, functional units, high performance RISC architectures, integer computation units, memory system, performance evaluation, program balance, reduced instruction set computing, single instruction stream parallelism, supercomputing applications, superscalar processors, },
 abstract = {Information on the behavior of programs is essential for deciding the number and nature of functional units in high performance architectures. In this paper, we present studies on the balance of access and computation tasks on a typical RISC architecture, the MIPS. The MIPS programs are analyzed to find the demands they place on the memory system and the floating point or integer computation units. A balance metric that indicates the match of accessing power to computation power is calculated. It is observed that many of the SPEC floating point programs and kernels from supercomputing applications typically considered as computation intensive programs, place extensive demands on the memory system in terms of memory bandwidth. Access related instructions are seen to dominate most instruction streams. We discuss how these instruction stream characteristics can limit the instruction issue in superscalar processors. The properties of the dynamic instruction mix are used to alert computer architects to the importance of memory bandwidth. Single instruction stream parallelism will not be much greater than two if memory bandwidth is only one. A decoupled access/execute architecture with multiple load/store units and queues which alleviate the balance problem is presented },
}

@inproceedings{386527,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Torrellas, J. and Chun Xia and Daigle, R.},
 year = {1995},
 pages = {360--369},
 publisher = {IEEE},
 title = {Optimizing instruction cache performance for operating system intensive workloads},
 date = {1995},
 doi = {10.1109/HPCA.1995.386527},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386527},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386527.pdf?arnumber=386527},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Central Processing Unit, Contracts, Control systems, Delay, Interference, NASA, Operating systems, Optimizing compilers, Research and development, Size control, application code, cache interference, cache storage, interference, locality patterns, operating system intensive workloads, optimising compilers, optimizing compiler, optimizing instruction cache performance, program compilers, temporal locality, total instruction miss rates, },
 abstract = {High instruction cache hit rates are key to high performance. One known technique to improve the hit rate of caches is to use an optimizing compiler to minimize cache interference via an improved layout of the code. This technique, however, has been applied to application code only, even though there is evidence that the operating system often uses the cache heavily and with less uniform patterns than applications. Therefore, it is unknown how well existing optimizations perform for systems code and whether better optimizations can be found. We address this problem in this paper. This paper characterizes in detail the locality patterns of the operating system code and shows that there is substantial locality. Unfortunately, caches are not able to extract much of it: rarely-executed special-case code disrupts spatial locality, loops with few iterations that call routines make loop locality hard to exploit, and plenty of loop-less code hampers temporal locality. As a result, interference within popular execution paths dominates instruction cache misses. Based on our observations, we propose an algorithm to expose these localities and reduce interference. For a range of cache sizes, associativities, lines sizes, and other organizations we show that we reduce total instruction miss rates by 31-86\% (up to 2.9 absolute points). Using a simple model this corresponds to execution time reductions in the order of 12-26\%. In addition, our optimized operating system combines well with optimized or unoptimized applications },
}

@inproceedings{386525,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {De-Lei Lee},
 year = {1995},
 pages = {380--389},
 publisher = {IEEE},
 title = {Memory access reordering in vector processors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386525},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386525},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386525.pdf?arnumber=386525},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Bandwidth, Clocks, Computer science, Councils, Degradation, Hardware, Interference elimination, Pipelines, Vector processors, hardware implementations, interference, interference, memory access reordering, multiple vector streams, performance degradation, storage allocation, vector processor systems, vector processors, },
 abstract = {Interference among multiple vector streams that access memory concurrently is the major source of performance degradation in main memory of pipelined vector processors. While totally eliminating interference appears to be impossible, little is known on how to design a memory system that can reduce it. In this paper, we introduce a concept called memory access reordering for reducing interference. This technique reduces interference by means of making the multiple vector streams access memory in an orderly fashion. Effective algorithms for memory access reordering are presented and their efficient hardware implementations are described },
}

@inproceedings{744362,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Falsafi, B. and Wood, D.A.},
 year = {1999},
 pages = {182--192},
 publisher = {IEEE},
 title = {Parallel Dispatch Queue: a queue-based programming abstraction to parallelize fine-grain communication protocols},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744362},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744362},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744362.pdf?arnumber=744362},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Access protocols, Application software, Computer networks, Concurrent computing, Data structures, Embedded software, Monitoring, Parallel programming, Protection, Software performance, access synchronization, data structures, distributed shared memory, distributed shared memory systems, fine-grain communication protocols, fine-grain software communication protocols, network message, parallel dispatch queue, parallel execution, protocols, queue-based programming abstraction, synchronisation, synchronization primitives, },
 abstract = {This paper proposes a novel queue-based programming abstraction, Parallel Dispatch Queue (PDQ), that enables efficient parallel execution of fine-grain software communication protocols. Parallel systems often use fine-grain software handlers to integrate a network message into computation. Executing such handlers in parallel requires access synchronization around resources. Much as a monitor construct in a concurrent language protects accesses to a set of data structures, PDQ allows messages to include a synchronization key protecting handler accesses to a group of protocol resources. By simply synchronizing messages in a queue prior to dispatch, PDQ not only eliminates the overhead of acquiring/releasing synchronization primitives but also prevents busy-waiting within handlers. In this paper, we study PDQ's impact on software protocol performance in the context of fine-grain distributed shared memory (DSM) on an SMP cluster. Simulation results running shared-memory applications indicate that: (i) parallel software protocol execution using PDQ significantly improves performance in fine-grain DSM, (ii) tight integration of PDQ and embedded processors into a single custom device can offer performance competitive or better than an all-hardware DSM, and (iii) PDQ best benefits cost-effective systems that use idle SMP processors (rather than custom embedded processors) to execute protocols. On a cluster of 4 16-way SMPs, a PDQ-based parallel protocol running on idle SMP processors improves application performance by a factor of 2.6 over a system running a serial protocol on a single dedicated processor },
}

@inproceedings{744363,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Bilas, A. and Dongming Jiang and Yuanyuan Zhou and Singh, J.P.},
 year = {1999},
 pages = {193--202},
 publisher = {IEEE},
 title = {Limits to the performance of software shared memory: a layered approach},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744363},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744363},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744363.pdf?arnumber=744363},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Application software, Computer science, Cost function, Hardware, Parallel programming, Protocols, Software libraries, Software performance, Software systems, Support vector machines, application layer, application restructuring, bottlenecks, distributed shared memory systems, fine-grained software systems, optimisation, page-based shared virtual memory, paged storage, parallel performance, performance, performance characteristics, protocols, protocols, software performance evaluation, software shared memory, workstation clusters, },
 abstract = {Much research has been done in fast communication on clusters and in protocols for supporting software shared memory across them. However, the end performance of applications that were written for the more proven hardware-coherent shared memory is still not very good on these systems. Three major layers of software (and hardware) stand between the end user and parallel performance, each with its own functionality and performance characteristics. They include the communication layer, the software protocol layer that supports the programming model, and the application layer. These layers provide a useful framework to identify the key remaining limitations and bottlenecks in software shared memory systems, as well as the areas where optimization efforts might yield the greatest performance improvements. This paper performs such an integrated study, using this layered framework, for two types of software distributed shared memory systems: page-based shared virtual memory (SVM) and fine-grained software systems (FG). For the two system layers (communication and protocol), we focus on the performance costs of basic operations in the layers rather than on their functionalities. This is possible because their functionalities are now fairly mature. The less mature applications layer is treated through application restructuring. We examine the layers individually and in combination, understanding their implications for the two types of protocols and exposing the synergies among layers },
}

@inproceedings{386528,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Garg, V. and Schimmel, D.E.},
 year = {1995},
 pages = {348--357},
 publisher = {IEEE},
 title = {Architectural support for inter-stream communication in a MSIMD system},
 date = {1995},
 doi = {10.1109/HPCA.1995.386528},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386528},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386528.pdf?arnumber=386528},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {CMOS process, CMOS process, Circuit simulation, Communication system control, Costs, Hardware, MSIMD system, Parallel algorithms, Parallel architectures, Registers, SIMD architectures, System performance, Timing, access times, architectural support, bit-interleaved register file structure, circuit simulation, control parallel structure, control parallelism, data dependency, data parallel algorithms, data parallel architectures, hardware support, inter-stream communication, parallel algorithms, parallel architectures, scalar variables, sharing mechanism, synchronization, synchronization issues, system performance, },
 abstract = {This paper considers hardware support for the exploitation of control parallelism on data parallel architectures. It is well known that data parallel algorithms may also possess control parallel structure. However the splitting of control leads to data dependency and synchronization issues that were implicitly handled in conventional SIMD architectures. These include synchronization of access to scalar and parallel variables, and synchronization for parallel communication operations. We propose a sharing mechanism for scalar variables and identify a strategy which allows synchronization of scalar variables between multiple streams. The techniques considered are based on a bit-interleaved register file structure which allows fast copy between register sets. Hardware cost estimates and timing analyses are provided, and comparison with an alternate scheme is presented. The register file structure has been designed and simulated for the HP 0.8 \&mu;m CMOS process, and circuit simulation indicates that access times are less than six nanoseconds. In addition, the impact of this structure on system performance is also studied },
}

@inproceedings{744361,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Hagersten, E. and Koster, M.},
 year = {1999},
 pages = {172--181},
 publisher = {IEEE},
 title = {WildFire: a scalable path for SMPs},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744361},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744361},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744361.pdf?arnumber=744361},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Backplanes, Bandwidth, Coherent Memory Replication, Hierarchical Affinity Scheduling, Memory architecture, Memory management, Moore's Law, NUMA implementation, Non-Uniform Memory Architecture, OLTP benchmark, Prototypes, SMP architecture, Simple COMA/Reactive NUMA, Sun, Switched-mode power supply, Switches, Time measurement, WildFire, application-transparent locality, distributed shared memory prototype implementation, distributed shared memory systems, local memory access time, memory architecture, node locality, performance characteristics, scalable path, scalable technologies, storage management, symmetric multiprocessor, },
 abstract = {Researchers have searched for scalable alternatives to the symmetric multiprocessor (SMP) architecture since it was first introduced in 1982. The paper introduces an alternative view of the relationship between scalable technologies and SMPs. Instead of replacing large SMPs with scalable technology, we propose new scalable techniques that allow large SMPs to be tied together efficiently, while maintaining the compatibility with, and performance characteristics of, an SMP. The trade-offs of such an architecture differ from those of traditional, scalable, Non-Uniform Memory Architecture (cc-NUMA) approaches. WildFire is a distributed shared memory (DSM) prototype implementation based on large SMPs. It relies on two techniques for creating application-transparent locality: Coherent Memory Replication (CMR), which is a variation of Simple COMA/Reactive NUMA, and Hierarchical Affinity Scheduling (HAS). These two optimizations create extra node locality, which blurs the node boundaries to an application such that SMP-like performance can be achieved with no NUMA-specific optimizations. We present a performance study of a large OLTP benchmark running on DSMs built from various sized nodes and with varying amounts of application-transparent locality. WildFire's measured performance is shown to be more than two times that of an unoptimized NUMA implementation built from small nodes and within 13\% of the performance of the ideal implementation: a large SMP with the same access time to its entire shared memory as the local memory access time of WildFire },
}

@inproceedings{744366,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Inoue, K. and Kai, K. and Murakami, K.},
 year = {1999},
 pages = {218--222},
 publisher = {IEEE},
 title = {Dynamically variable line-size cache exploiting high on-chip memory bandwidth of merged DRAM/logic LSIs},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744366},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744366},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744366.pdf?arnumber=744366},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Bandwidth, DRAM chips, Logic, Random access memory, Terminology, cache storage, dynamically variable line-size cache, high on-chip memory bandwidth, large scale integration, memory architecture, merged DRAM/logic LSIs, performance evaluation, },
 abstract = {This paper proposes a novel cache architecture suitable for merged DRAM/logic LSIs, which is called ``dynamically variable line-size cache (D-VLS cache)". The D-VLS cache can optimize its line-size according to the characteristic of programs, and attempts to improve the performance by exploiting the high on-chip memory bandwidth. In our evaluation, it is observed that the performance improvement achieved by a direct-mapped D-VLS cache is about 27\%, compared to a conventional direct-mapped cache with fixed 32-byte lines },
}

@inproceedings{744367,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Yunseok Rhee and Joonwon Lee},
 year = {1999},
 pages = {223--226},
 publisher = {IEEE},
 title = {A scalable cache coherent scheme exploiting wormhole routing networks},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744367},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744367},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744367.pdf?arnumber=744367},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Access protocols, Broadcasting, Computer science, Computer worms, Degradation, Delay, N-node system, Routing, Space technology, Telecommunication traffic, Watches, complexity, computational complexity, full-mapped vector, generic router, k-ary n-cube networks, mesh interconnection, multiprocessor interconnection networks, scalability, scalable cache coherent scheme, shared memory multiprocessors, wormhole routing networks, },
 abstract = {Large scale shared memory multiprocessors favor a directory-based cache coherence scheme for its scalability. The directory space needed to record the information for sharers has a complexity of \&Theta;(N<sup>2</sup>) when a full-mapped vector is used for an N-node system. Though this overhead can be reduced by limiting the directory size assuming that the sharing degree is small, it will experience significant inefficiency when a data is widely shared. In this paper, we propose a new directory scheme and a cache coherence scheme based on it for a mesh interconnection. Deterministic and wormhole routing enables a pointer to represent a set of nodes. Also a message traversing on the mesh performs a broadcast mission to a set of nodes without extra traffic, which can be utilized for the cache coherence problem. Only a slight change on a generic router is needed to implement our scheme. This scheme is also applicable to any k-ary n-cube networks including a mesh },
}

@inproceedings{744364,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Yiming Hu and Qing Yang and Nightingale, T.},
 year = {1999},
 pages = {204--213},
 publisher = {IEEE},
 title = {RAPID-Cache-a reliable and inexpensive write cache for disk I/O systems},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744364},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744364},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744364.pdf?arnumber=744364},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Analytical models, Computer architecture, Concurrent computing, Costs, Delay, Electronic mail, NVRAM, Nonvolatile memory, RAM, RAPID-Cache, Random access memory, Read-write memory, Reliability engineering, backup cache, buffers, cache storage, disk I/O systems, memory architecture, performance evaluation, random-access storage, redundant write buffers, write cache, write performance, },
 abstract = {This paper presents a new cache architecture called RAPID-Cache for Redundant, Asymmetrically Parallel, and Inexpensive Disk Cache. A typical RAPID-Cache consists of two redundant write buffers on top of a disk system. One of the buffers is a primary cache made of RAM or NVRAM and the other is a backup cache containing a two level hierarchy: a small NVRAM buffer on top of a log disk. The backup cache has nearly equivalent write performance as the primary RAM cache, while the read performance of the backup cache is not as critical because normal read operations are performed through the primary RAM cache and reads from the backup cache happen only during error recovery periods. The RAPID-Cache presents an asymmetric architecture with a fast-write-fast-read RAM being a primary cache and a fast-write-slow-read NVRAM-disk hierarchy being a backup cache. The asymmetric cache architecture allows cost-effective designs for very large write caches for high-end disk I/O systems that would otherwise have to use dual-copy, costly NVRAM caches. It also makes it possible to implement reliable write caching for low-end disk I/O systems since the RAPID-Cache makes use of inexpensive disks to perform reliable caching. Our analysis and trace-driven simulation results show that the RAPID-Cache has significant reliability/cost advantages over conventional single NVRAM write caches and has great cost advantages over dual-copy NVRAM caches. The RAPID-Cache architecture opens a new dimension for disk system designers to exercise trade-offs among performance, reliability and cost },
}

@inproceedings{744365,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Schwarz, T.J.E. and Steinberg, J. and Burkhard, W.A.},
 year = {1999},
 pages = {214--217},
 publisher = {IEEE},
 title = {Permutation development data layout (PDDL)},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744365},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744365},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744365.pdf?arnumber=744365},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Availability, Computer science, Costs, Electronic switching systems, Laboratories, Optical arrays, PDDL, Parallel processing, RAID, Reactive power, Throughput, access parallelism, data layout, declustered data organizations, disk arrays, magnetic disc storage, permutation development data layout, run-time performance, },
 abstract = {Declustered data organizations in disk arrays (RAIDs) achieve less-intrusive reconstruction of data after a disk failure. We present PDDL, a new data layout for declustered disk arrays. PDDL layouts exist for a large variety of disk array configurations with a distributed spare disk. PDDL declustered disk arrays have excellent run-time performance under light and heavy workloads. PDDL maximizes access parallelism in the most critical circumstances, namely during reconstruction of data on the spare disk. PDDL occurs minimum address translation overhead compared to all other proposed declustering layouts },
}

@inproceedings{4798285,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {459--460},
 publisher = {IEEE},
 title = {Author index},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798285},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798285},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798285.pdf?arnumber=4798285},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798285.png" border="0"> },
}

@inproceedings{4798284,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Dennis, J.},
 year = {2009},
 pages = {457--458},
 publisher = {IEEE},
 title = {How to build programmable multi-core chips},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798284},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798284},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798284.pdf?arnumber=4798284},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Buildings, Computer architecture, Computer languages, Concurrent computing, Lifting equipment, Mathematical programming, Parallel programming, Quadratic programming, Robustness, Writing, composable parallel software, computer architecture, microprocessor chips, parallel architectures, parallel program, parallel programming, parallel programming, programmable multicore chips, software structuring, },
 abstract = {The arrival of multi-core chips has heightened interest in the discipline of parallel programming, a topic that has received much attention for many years. Computer architects have much to learn from sound principles for structuring software and expressing parallel computation. This talk will cover principles for the design of computer systems to support composable parallel software - the idea that any parallel program is usable, without change, as a component of larger parallel programs. By following these principles, a revolution in the ease of building robust and high-performance parallel software can be achieved. The principles suggest interesting directions for computer architecture; the tools to experiment with new architecture concepts are ready and waiting for the savvy and ambitious researcher. },
}

@inproceedings{4798281,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Hilton, A. and Nagarakatte, S. and Roth, A.},
 year = {2009},
 pages = {431--442},
 publisher = {IEEE},
 title = {iCFP: Tolerating all-level cache misses in in-order processors},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798281},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798281},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798281.pdf?arnumber=4798281},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Buffer storage, Delay, Information science, Merging, Out of order, Parallel processing, Pipeline processing, Processor scheduling, Proposals, Registers, Runahead execution, all-level cache, in-order continual flow pipeline, in-order pipelines, in-order processors, miss-independent instructions, multipass pipelining, multiprocessing systems, pipeline processing, register dependence tracking scheme, register file, },
 abstract = {Growing concerns about power have revived interest in in-order pipelines. In-order pipelines sacrifice single-thread performance. Specifically, they do not allow execution to flow freely around data cache misses. As a result, they have difficulties overlapping independent misses with one another. Previously proposed techniques like Runahead execution and Multipass pipelining have attacked this problem. In this paper, we go a step further and introduce iCFP (in-order Continual Flow Pipeline), an adaptation of the CFP concept to an in-order processor. When iCFP encounters a primary data cache or 12 miss, it checkpoints the register file and transitions into an "advance " execution mode. Miss-independent instructions execute as usual and even update register state. Miss- dependent instructions are diverted into a slice buffer, un-blocking the pipeline latches. When the miss returns, iCFP "rallies" and executes the contents of the slice buffer, merging miss-dependent state with miss- independent state along the way. An enhanced register dependence tracking scheme and a novel store buffer design facilitate the merging process. Cycle-level simulations show that iCFP out-performs Runahead, Multipass, and SLTP, another non-blocking in-order pipeline design. },
}

@inproceedings{4798280,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Subramaniam, S. and Bracy, A. and Hong Wang and Loh, G.H.},
 year = {2009},
 pages = {419--430},
 publisher = {IEEE},
 title = {Criticality-based optimizations for efficient load processing},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798280},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798280},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798280.pdf?arnumber=4798280},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Accuracy, Buffer storage, Computer aided instruction, Costs, Delay, Educational institutions, Hardware, Investments, Laboratories, Microarchitecture, criticality information, criticality predictor, criticality-based optimization, dual-read-port cache, efficient load processing, load value prediction, performance evaluation, processor enhancements, resource allocation, single-read-port data cache, storage management, },
 abstract = {Some instructions have more impact on processor performance than others. Identification of these critical instructions can be used to modify and improve instruction processing. Previous work has shown that the criticality of instructions can be dynamically predicted with high accuracy, and that this information can be leveraged to optimize the performance of load value prediction and instruction steering for clustered architectures. In this work, we revisit the idea of criticality, but we propose several processor enhancements that can exploit criticality information and can be directly applied to modern times86 microarchitectures. For the investment of a small (less than 1 KB) criticality predictor, we can make a conventional single-read-port data cache achieve the performance of an ideal dual-read-port cache, yielding an average 10\% performance improvement. Our remaining techniques can reuse the predictor (i.e., no additional overhead) to further optimize other aspects of load processing (e.g., caching decisions, store-to-load forwarding, etc.), yielding an overall performance improvement of 16\% over a conventional processor. Some of these techniques also allow us to decrease power and area costs for several related hardware structures. },
}

@inproceedings{4798283,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {455--456},
 publisher = {IEEE},
 title = {Keynote III (joint with PPoPP)},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798283},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798283},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798283.pdf?arnumber=4798283},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798283.png" border="0"> },
}

@inproceedings{4798282,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Hur, I. and Lin, C.},
 year = {2009},
 pages = {443--454},
 publisher = {IEEE},
 title = {Feedback mechanisms for improving probabilistic memory prefetching},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798282},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798282},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798282.pdf?arnumber=4798282},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Computer applications, DRAM chips, DRAM energy consumption, Energy consumption, Feedback, Histograms, Prefetching, Random access memory, Timing, Variable speed drives, adaptive stream detection prefetching, buffer storage, epoch length, memory architecture, probabilistic feedback, probabilistic memory prefetching, probability, similarity metric, stream buffer, variable-length prefetching, },
 abstract = {This paper presents three techniques for improving the effectiveness of the recently proposed adaptive stream detection (ASD) prefetching mechanism. The ASD prefetcher is a standard stream buffer that takes a probabilistic feedback-based probabilistic approach to identifying streams. Its strength lies in its ability to effectively prefetch streams that are as short as two consecutive cache lines, which allows it to exploit spatial locality even for programs that have irregular access patterns. The first technique improves a stream buffer's ability to detect short streams, which significantly increases the potential of stream-based prefetching. For example, for the SPECFfp mile benchmark, this new technique doubles the number of detectable streams from 33\% to 67\%. The second technique improves the quality of the ASD prefetcher's feedback mechanism by adaptively adjusting the epoch length - the length of time used to represent the recent past behavior - according to a simple similarity metric. The third technique improves the timing of prefetches by supporting variable-length prefetching of multiple cache lines. Collectively, these techniques almost double the effectiveness of the ASD prefetcher, improving the performance of the ASD prefetcher by 11.2\% for the SPECfp benchmarks, by 10.3\% for the NAS benchmarks, and by 13.2\% for a set of commercial benchmarks that exhibit poor spatial locality. The improved performance in turn decreases DRAM energy consumption by 7.3\%, 8.3\%, and 9.4\%, respectively, for the same three benchmark suites. },
}

@inproceedings{903248,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Wang, P.H. and Hong Wang and Kling, R.M. and Ramakrishnan, K. and Shen, J.P.},
 year = {2001},
 pages = {15--25},
 publisher = {IEEE},
 title = {Register renaming and scheduling for dynamic execution of predicated code},
 date = {2001},
 doi = {10.1109/HPCA.2001.903248},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903248},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903248.pdf?arnumber=903248},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Availability, Dynamic scheduling, Hardware, Intel Itanium processor pipeline, Microarchitecture, Microprocessors, Modems, Process design, Processor scheduling, Registers, Runtime, compiler techniques, dynamic microarchitecture, hardware optimizations, instruction level parallelism, parallel architectures, performance evaluation, predicated execution, processor performance, processor scheduling, program compilers, register renaming, scheduling, },
 abstract = {To achieve higher processor performance requires greater synergy between advanced hardware features and innovative compiler techniques. Recent advancement in compilation techniques for predicated execution has provided significant opportunity in exploiting instruction level parallelism. However, little research has been done on how to efficiently execute predicated code in a dynamic microarchitecture. In this paper, we evaluate hardware optimizations for executing predicated code on a dynamically scheduled microarchitecture. We provide two novel ideas to improve the efficiency of executing predicated code. On a generic Intel Itanium processor pipeline model, we demonstrate that, with some microarchitecture enhancements, a dynamic execution processor can achieve about 16\% performance improvement over an equivalent static execution processor },
}

@inproceedings{1183552,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Terechko, A. and Le Thenaff, E. and Garg, M. and van Eijndhoven, J. and Corporaal, H.},
 year = {2003},
 pages = { 354-- 364},
 publisher = {IEEE},
 title = {Inter-cluster communication models for clustered VLIW processors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183552},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183552},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183552.pdf?arnumber=1183552},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { broadcasting,  clustered VLIW processors,  copy operations,  dedicated issue slots,  extended operands,  extended results,  inter-cluster communication models,  parallel architectures,  performance,  performance evaluation,  processor scheduling,  scheduling freedom,  single register file, Broadcasting, Computer architecture, Costs, Hardware, Radio frequency, Registers, Scalability, Scheduling, Streaming media, VLIW, },
 abstract = {Clustering is a well-known technique to improve the implementation of single register file VLIW processors. Many previous studies in clustering adhere to an inter-cluster communication means in the form of copy operations. This paper, however, identifies and evaluates five different inter-cluster communication models, including copy operations, dedicated issue slots, extended operands, extended results, and broadcasting. Our study reveals that these models have a major impact on performance and implementation of the clustered VLIW. We found that copy operations executed in regular VLIW issue slots significantly constrain the scheduling freedom of regular operations. For example, in the dense code for our four cluster machine the total cycle count overhead reached 46.8\% with respect to the unicluster architecture, 56\% of which are caused by the copy operation constraint. Therefore, we propose to use other models (e.g. extended results or broadcasting), which deliver higher performance than the copy operation model at the same hardware cost. },
}

@inproceedings{1183553,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Ming Hao and Heinrich, M.},
 year = {2003},
 pages = { 365-- 376},
 publisher = {IEEE},
 title = {Active I/O switches in system area networks},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183553},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183553},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183553.pdf?arnumber=1183553},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { SAN subsystem,  active I/O switches,  active switch architecture,  application-level code,  benchmark applications,  data-intensive applications,  flexible packet routing,  local area networks,  microarchitecture,  multi-programmed servers,  parallel processor,  performance,  performance evaluation,  programming model,  simulation results,  speed up,  switches,  system area networks,  telecommunication network routing,  virtual machines, Bandwidth, Computer architecture, Computer networks, Intelligent networks, Laboratories, Microarchitecture, Packet switching, Prototypes, Storage area networks, Switches, },
 abstract = {We present an active switch architecture to improve the performance of systems connected via system area networks. Our programmable active switches not only flexibly route packets between any combination of hosts and I/O devices, but also have the capability of running application-level code, forming a parallel processor in the SAN subsystem. By replacing existing SAN-based switches with a new active switch architecture, we can design a prototype system with otherwise commercially available, commodity parts that can dramatically speed up data-intensive applications and workloads on modern multi-programmed servers. We explain the programming model and detail the microarchitecture of our active switch, and analyze simulation results for nine benchmark applications that highlight various advantages of active switch-based systems. },
}

@inproceedings{1183550,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Jeong, J. and Dubois, M.},
 year = {2003},
 pages = { 327-- 337},
 publisher = {IEEE},
 title = {Cost-sensitive cache replacement algorithms},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183550},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183550},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183550.pdf?arnumber=1183550},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { LRU extensions,  cache misses,  cache storage,  cost function,  cost-sensitive cache replacement algorithms,  delays,  execution time,  miss latency,  multiprocessing systems,  multiprocessor,  nonuniform miss costs,  parallel applications,  parallel architectures,  performance evaluation,  second-level cache,  static miss costs,  superscalar processors,  trace-driven simulations, Aggregates, Bandwidth, Computer architecture, Cost function, Delay, Energy consumption, Heuristic algorithms, Multiprocessing systems, Switches, },
 abstract = {Cache replacement algorithms originally developed in the context of simple uniprocessor systems aim to reduce the miss count. However, in modern systems, cache misses have different costs. The cost may be latency, penalty, power consumption, bandwidth consumption, or any other ad-hoc numerical property attached to a miss. In many practical situations, it is desirable to inject the cost of a miss into the replacement policy. In this paper, we propose several extensions of LRU which account for nonuniform miss costs. These LRU extensions have simple implementations, yet they are very effective in various situations. We first explore the simple case of two static miss costs using trace-driven simulations to understand when cost-sensitive replacements are effective. We show that very large improvements of the cost function are possible in many practical cases. As an example of their effectiveness, we apply the algorithms to the second-level cache of a multiprocessor with superscalar processors, using the miss latency as the cost function. By applying our simple replacement policies sensitive to the latency of misses we can improve the execution time of some parallel applications by up to 18\%. },
}

@inproceedings{903249,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Michaud, P. and Seznec, A.},
 year = {2001},
 pages = {27--36},
 publisher = {IEEE},
 title = {Data-flow prescheduling for large instruction windows in out-of-order processors},
 date = {2001},
 doi = {10.1109/HPCA.2001.903249},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903249},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903249.pdf?arnumber=903249},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Accuracy, Clocks, Delay, Logic, Out of order, Process design, Processor scheduling, Proposals, Registers, Silicon, complexity, data flow computing, data flow prescheduling, instruction window, instruction windows, out-of-order processors, prescheduling, processor scheduling, time-critical operation, },
 abstract = {The performance of out-of-order processors increases with the instruction window size, In conventional processors, the effective instruction window cannot be larger than the issue buffer. Determining which instructions from the issue buffer can be launched to the execution units is a time-critical operation which complexity increases with the issue buffer size. We propose to relieve the issue stage by reordering instructions before they enter the issue buffer. This study introduces the general principle of data flow prescheduling. Then we describe a possible implementation. Our preliminary results show that data-flow prescheduling makes it possible to enlarge the effective instruction window while keeping the issue buffer small },
}

@inproceedings{1183554,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Wai Hong Ho and Pinkston, T.M.},
 year = {2003},
 pages = { 377-- 388},
 publisher = {IEEE},
 title = {A methodology for designing efficient on-chip interconnects on well-behaved communication patterns},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183554},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183554},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183554.pdf?arnumber=1183554},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { blocking performance,  contention-free communication,  irregular topology,  multiprocessor interconnection networks,  network topology,  on-chip interconnects,  parallel architectures,  parallel system,  performance evaluation,  recursive bisection technique,  systematic partitioning,  temporal spatial model,  well-behaved communication patterns, Communication switching, Computer networks, Design methodology, Design optimization, Network topology, Network-on-a-chip, Resource management, Sufficient conditions, Switches, Wires, },
 abstract = {As the level of chip integration continues to advance at a fast pace, the desire for efficient interconnects - whether on-chip or off-chip - is rapidly increasing. Traditional interconnects like buses, point-to-point wires and regular topologies may suffer from poor resource sharing in the time and space domains, leading to high contention or low resource utilization. In this paper, we propose a design methodology for constructing networks for special-purpose computer systems with well-behaved (known) communication characteristics. A temporal and spatial model is proposed to define the sufficient condition for contention-free communication. Based upon this model, a design methodology using a recursive bisection technique is applied to systematically partition a parallel system such that the required number of links and switches is minimized while achieving low contention. Results show that the design methodology can generate more optimized on-chip networks with up to 60\% fewer resources than meshes or tori while providing blocking performance closer to that of a fully connected crossbar. },
}

@inproceedings{1183555,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {},
 year = {2003},
 pages = { 389-- 390},
 publisher = {IEEE},
 title = {Author index},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183555},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183555},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183555.pdf?arnumber=1183555},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01183555.png" border="0"> },
}

@inproceedings{4147654,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Pin Zhou and Teodorescu, R. and Yuanyuan Zhou},
 year = {2007},
 pages = {121--132},
 publisher = {IEEE},
 title = {HARD: Hardware-Assisted Lockset-based Race Detection},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346191},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147654},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147654.pdf?arnumber=4147654},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Application software, Computer architecture, Computer bugs, Hardware, Interleaved codes, Monitoring, Multicore processing, Proposals, SPLASH-2 applications, Software performance, Yarn, data races, fast bitwise logic operation, hardware bloom filters, hardware implementation, hardware-assisted lockset-based race detection, lockset algorithm, multi-threading, multicore architecture, multithreaded application, parallel architectures, program debugging, },
 abstract = {The emergence of multicore architectures will lead to an increase in the use of multithreaded applications that are prone to synchronization bugs, such as data races. Software solutions for detecting data races generally incur large overheads. Hardware support for race detection can significantly reduce that overhead. However, all existing hardware proposals for race detection are based on the happens-before algorithm which is sensitive to thread interleaving and cannot detect races that are not exposed during the monitored run. The lockset algorithm addresses this limitation. Unfortunately, due to the challenging issues such as storing the lockset information and performing complex set operations, so far it has been implemented only in software with 10-30 times performance hit. This paper proposes the first hardware implementation (called HARD) of the lockset algorithm to exploit the race detection capability of this algorithm with minimal overhead. HARD efficiently stores lock sets in hardware bloom filters and converts the expensive set operations into fast bitwise logic operations with negligible overhead. We evaluate HARD using six SPLASH-2 applications with 60 randomly injected bugs. Our results show that HARD can detect 54 out of 60 tested bugs, 20\% more than happens-before, with only 0.1-2.6\% of execution overhead. We also show our hardware design is cost-effective by comparing with the ideal lockset implementation, which would require a large amount of hardware resources },
}

@inproceedings{903246,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {},
 year = {2001},
 publisher = {IEEE},
 title = {Proceedings HPCA Seventh International Symposium on High-Performance Computer Architecture},
 date = {2001},
 doi = {10.1109/HPCA.2001.903246},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903246},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903246.pdf?arnumber=903246},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {application specific designs, code generation, computer architecture, latency tolerance, memory architecture, memory architecture, microarchitecture, multiprocessing systems, multiprocessor systems, performance evaluation, performance modelling, prediction techniques, thermal management, },
 abstract = {The following topics were dealt with: microarchitecture; memory architectures; multiprocessor systems; code generation; energy and thermal management; prediction techniques; application specific designs; performance modelling and analysis; and latency tolerance },
}

@inproceedings{4147656,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Meixner, A. and Sorin, D.J.},
 year = {2007},
 pages = {145--156},
 publisher = {IEEE},
 title = {Error Detection via Online Checking of Cache Coherence with Token Coherence Signatures},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346193},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147656},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147656.pdf?arnumber=4147656},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Bandwidth, Communication system traffic control, Costs, Error correction, Fault detection, Hardware, History, Multiprocessor interconnection networks, Power capacitors, Thyristors, cache coherence, cache controller, cache storage, coherence events, error detection, interconnect bandwidth overhead, memory controller, multi-threading, multithreaded system, online checking, protocols, shared memory system, shared memory systems, token coherence signature checker, },
 abstract = {To provide high dependability in a multithreaded system despite hardware faults, the system must detect and correct errors in its shared memory system. Recent research has explored dynamic checking of cache coherence as a comprehensive approach to memory system error detection. However, existing coherence checkers are costly to implement, incur high interconnection network traffic overhead, and do not scale well. In this paper, we describe the token coherence signature checker (TCSC), which provides comprehensive, low-cost, scalable coherence checking by maintaining signatures that represent recent histories of coherence events at all nodes (cache and memory controllers). Periodically, these signatures are sent to a verifier to determine if an error occurred. TCSC has a small constant hardware cost per node, independent of cache and memory size and the number of nodes. TCSC's interconnect bandwidth overhead has a constant upper bound and never exceeds 7\% in our experiments. TCSC has negligible impact on system performance },
}

@inproceedings{4147657,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Fernandez-Pascual, R. and Garcia, J.M. and Acacio, M.E. and Duato, J.},
 year = {2007},
 pages = {157--168},
 publisher = {IEEE},
 title = {A Low Overhead Fault Tolerant Coherence Protocol for CMP Architectures},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346194},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147657},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147657.pdf?arnumber=4147657},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {CMP architecture, Computer architecture, Electromagnetic interference, Electromagnetic radiation, Electromagnetic transients, Electronic components, Energy consumption, Fault tolerance, GEMS full system simulator, Multiprocessor interconnection networks, Proposals, Protocols, TOKENCMP, cache storage, chip-multiprocessors, fault tolerance, fault tolerant coherence protocol, interconnection network, microprocessor chips, multiprocessing systems, parallel architectures, protocols, single chip, token-based cache coherence protocol, },
 abstract = {It is widely accepted that transient failures will appear more frequently in chips designed in the near future due to several factors such as the increased integration scale. On the other hand, chip-multiprocessors (CMP) that integrate several processor cores in a single chip are nowadays the best alternative to more efficient use of the increasing number of transistors that can be placed in a single die. Hence, it is necessary to design new techniques to deal with these faults to be able to build sufficiently reliable chip multiprocessors (CMPs). In this work, we present a coherence protocol aimed at dealing with transient failures that affect the interconnection network of a CMP, thus assuming that the network is no longer reliable. In particular, our proposal extends a token-based cache coherence protocol so that no data can be lost and no deadlock can occur due to any dropped message. Using GEMS full system simulator, we compare our proposal against a similar protocol without fault tolerance (TOKENCMP). We show that in absence of failures our proposal does not introduce overhead in terms of increased execution time over TOKENCMP. Additionally, our protocol can tolerate message loss rates much higher than those likely to be found in the real world without increasing execution time more than 15\% },
}

@inproceedings{4147650,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Weifeng Zhang and Tullsen, D.M. and Calder, B.},
 year = {2007},
 pages = {85--95},
 publisher = {IEEE},
 title = {Accelerating and Adapting Precomputation Threads for Effcient Prefetching},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346187},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147650},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147650.pdf?arnumber=4147650},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Acceleration, Aerodynamics, Computer architecture, Computer science, Delay, Hardware, Monitoring, Prefetching, Runtime, Yarn, adaptive prefetching, dynamic stride prediction, event-driven dynamic optimization, memory behavior, multi-threading, p-thread prefetching, precomputation threads, prefetching threads, program compilers, runtime compiler, storage management, },
 abstract = {Speculative precomputation enables effective cache prefetching for even irregular memory access behavior, by using an alternate thread on a multithreaded or multi-core architecture. This paper describes a system that constructs and runs precomputation based prefetching threads via event-driven dynamic optimization. Precomputation threads are dynamically constructed by a runtime compiler from the program's frequently executed hot traces, and are adapted to the memory behavior automatically. Both construction and execution of the prefetching threads happen in another thread, imposing little overhead on the main thread. This paper also presents several techniques to accelerate the precomputation threads, including colocation of p-threads with hot traces, dynamic stride prediction, and automatic adaptation of runahead and jumpstart distance. The adaptive prefetching achieves 42\% speedup, a 17\% improvement over existing p-thread prefetching schemes },
}

@inproceedings{4147651,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Pawlowski, S.},
 year = {2007},
 pages = {96--96},
 publisher = {IEEE},
 title = {Petascale Computing Research Challenges - A Manycore Perspective},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346188},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147651},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147651.pdf?arnumber=4147651},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Bandwidth, Computer architecture, Feeds, Geometry, High performance computing, Microprocessors, Moore's Law, Petascale computing, Petascale computing, Technology management, Transistors, high performance computing, manycore processor, memory bandwidth, multicore processor, multiprocessing systems, parallel architectures, personal computing, processor architecture, },
 abstract = {Summary form only given. Future high performance computing will undoubtedly reach Petascale and beyond. Today's HPC is tomorrow's personal computing. What are the evolving processor architectures towards multi-core and many-core for the best performance per watt; memory bandwidth solutions to feed the ever more powerful processors; intra-chip interconnect options for optimal bandwidth vs. power? With Moore's Law continuing to prove its viability and shrinking transistors' geometry, improving reliability is even more challenging. Intel Senior Fellow and Chief Technology Officer of Intel's Digital Enterprise Group, Steve Pawlowski, will provide his technology vision, insight and research challenges to achieve the vision of Petascale computing and beyond },
}

@inproceedings{4147652,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Chafi, H. and Casper, J. and Carlstrom, B.D. and McDonald, A. and Chi Cao Minh and Woongki Baek and Kozyrakis, C. and Olukotun, K.},
 year = {2007},
 pages = {97--108},
 publisher = {IEEE},
 title = {A Scalable, Non-blocking Approach to Transactional Memory},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346189},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147652},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147652.pdf?arnumber=4147652},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Coherence, Concurrency control, Content management, Filters, Large-scale systems, NUMA system, Parallel programming, Programming profession, Proposals, Protocols, System recovery, cache storage, coherence message filtering, concurrency control, directory-based TCC design, directory-based distributed shared memory systems, distributed shared memory systems, fault isolation, livelock free, optimistic concurrency control, parallel programming, parallel programming, performance evaluation, performance evaluation, transactional coherence and consistency, transactional memory, two-phase commit protocol, write-back caches, },
 abstract = {Transactional memory (TM) provides mechanisms that promise to simplify parallel programming by eliminating the need for locks and their associated problems (deadlock, livelock, priority inversion, convoying). For TM to be adopted in the long term, not only does it need to deliver on these promises, but it needs to scale to a high number of processors. To date, proposals for scalable TM have relegated livelock issues to user-level contention managers. This paper presents the first scalable TM implementation for directory-based distributed shared memory systems that is livelock free without the need for user-level intervention. The design is a scalable implementation of optimistic concurrency control that supports parallel commits with a two-phase commit protocol, uses write-back caches, and filters coherence messages. The scalable design is based on transactional coherence and consistency (TCC), which supports continuous transactions and fault isolation. A performance evaluation of the design using both scientific and enterprise benchmarks demonstrates that the directory-based TCC design scales efficiently for NUMA systems up to 64 processors },
}

@inproceedings{903247,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Lee, H.H.-S. and Smelyanskiy, M. and Newburn, C.J. and Tyson, G.S.},
 year = {2001},
 pages = {5--14},
 publisher = {IEEE},
 title = {Stack value file: custom microarchitecture for the stack},
 date = {2001},
 doi = {10.1109/HPCA.2001.903247},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903247},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903247.pdf?arnumber=903247},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Computer architecture, Delay, History, Microarchitecture, Parallel processing, Performance evaluation, Registers, Time factors, caches, custom microarchitecture, data references, instruction-level parallelism, memory architecture, memory organization, memory system, processor performance, stack, stack value file, storage management, },
 abstract = {As processor performance increases, there is a corresponding increase in the demands on the memory system, including caches. Research papers have proposed partitioning the cache into instruction/data, temporal/non-temporal, and/or stack/non-stack regions. Each of these designs can improve performance by constructing two separate structures which can be probed in parallel while reducing contention. In this paper, we propose a new memory organization that partitions data references into stack and nonstack regions. Non-stack references are routed to a conventional cache. Stack references, on the other hand, are shown to have several characteristics that can be leveraged to improve performance using a less conventional storage organization. This paper enumerates those characteristics and proposes a new microarchitectural feature, the stack value file (SVF), which exploits them to improve instruction-level parallelism, reduce stack access latencies, reduce demand on the first-level cache, and reduce data bus traffic. Our results show that the SVF can improve execution performance by 29 to 65\% while reducing overhead traffic for the stack region by many orders of magnitude over cache structures of the same size },
}

@inproceedings{903273,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {},
 year = {2001},
 pages = {317--318},
 publisher = {IEEE},
 title = {Author Index},
 date = {2001},
 doi = {10.1109/HPCA.2001.903273},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903273},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903273.pdf?arnumber=903273},
 issn = {1530-0897},
 isbn = {0-7695-1019-1},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00903273.png" border="0"> },
}

@inproceedings{4147658,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Racunas, P. and Constantinides, K. and Manne, S. and Mukherjee, S.S.},
 year = {2007},
 pages = {169--180},
 publisher = {IEEE},
 title = {Perturbation-based Fault Screening},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346195},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147658},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147658.pdf?arnumber=4147658},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {CMOS technology, Circuit faults, Costs, Error analysis, Fault diagnosis, Fault tolerance, Hardware, Microprocessors, Pipelines, Protection, SpecInt, data inconsistencies, error detection, fault identification, fault location, fault tolerance, microprocessor chips, perturbation-based fault screening, screening algorithm, simulated Pentium-III-like processor, transient fault, },
 abstract = {Fault screeners are a new breed of fault identification technique that can probabilistically detect if a transient fault has affected the state of a processor. We demonstrate that fault screeners function because of two key characteristics. First, we show that much of the intermediate data generated by a program inherently falls within certain consistent bounds. Second, we observe that these bounds are often violated by the introduction of a fault. Thus, fault screeners can identify faults by directly watching for any data inconsistencies arising in an application's behavior. We present an idealized algorithm capable of identifying over 85\% of injected faults on the SpecInt suite and over 75\% overall. Further, in a realistic implementation on a simulated Pentium-III-like processor, about half of the errors due to injected faults are identified while still in speculative state. Errors detected this early can be eliminated by a pipeline flush. In this paper, we present several hardware-based versions of this screening algorithm and show that flushing the pipeline every time the hardware screener triggers reduces overall performance by less than 1\% },
}

@inproceedings{4147659,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Xuanhua Li and Yeung, D.},
 year = {2007},
 pages = {181--192},
 publisher = {IEEE},
 title = {Application-Level Correctness and its Impact on Fault Tolerance},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346196},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147659},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147659.pdf?arnumber=4147659},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Artificial intelligence, Computer architecture, Costs, Educational institutions, Engineering profession, Error correction, Fault tolerance, Government, Hardware, Resilience, application-level correctness, application-level fidelity metrics, artificial intelligence workloads, enhanced fault resilience, error resilience, fault recovery mechanism, fault tolerance, multimedia workloads, program correctness, program verification, programs soft computation, software architecture, software fault tolerance, software metrics, user satisfaction, user-perceived program solution quality, },
 abstract = {Traditionally, fault tolerance researchers have required architectural state to be numerically perfect for program execution to be correct. However, in many programs, even if execution is not 100\% numerically correct, the program can still appear to execute correctly from the user's perspective. Hence, whether a fault is unacceptable or benign may depend on the level of abstraction at which correctness is evaluated, with more faults being benign at higher levels of abstraction, i.e. at the user or application level, compared to lower levels of abstraction, i.e. at the architecture level. The extent to which programs are more fault resilient at higher levels of abstraction is application dependent. Programs that produce inexact and/or approximate outputs can be very resilient at the application level. We call such programs soft computations, and we find they are common in multimedia workloads, as well as artificial intelligence (AI) workloads. Programs that compute exact numerical outputs offer less error resilience at the application level. However, we find all programs studied in this paper exhibit some enhanced fault resilience at the application level, including those that are traditionally considered exact computations - e.g., SPECInt CPU2000. This paper investigates definitions of program correctness that view correctness from the application's standpoint rather than the architecture's standpoint. Under application-level correctness, a program's execution is deemed correct as long as the result it produces is acceptable to the user. To quantify user satisfaction, we rely on application-level fidelity metrics that capture user-perceived program solution quality. We conduct a detailed fault susceptibility study that measures how much more fault resilient programs are when defining correctness at the application level compared to the architecture level. Our results show for 6 multimedia and AI benchmarks that 45.8\% of architecturally incorrect faults are corre- ct at the application level. For 3 SPECInt CPU2000 benchmarks, 17.6\% of architecturally incorrect faults are correct at the application level. We also present a lightweight fault recovery mechanism that exploits the relaxed requirements on numerical integrity provided by application-level correctness to reduce checkpoint cost. Our lightweight fault recovery mechanism successfully recovers 66.3\% of program crashes in our multimedia and AI workloads, while incurring minimum runtime overhead },
}

@inproceedings{903272,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Wei-Fen Lin and Reinhardt, S.K. and Burger, D.},
 year = {2001},
 pages = {301--312},
 publisher = {IEEE},
 title = {Reducing DRAM latencies with an integrated memory hierarchy design },
 date = {2001},
 doi = {10.1109/HPCA.2001.903272},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903272},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903272.pdf?arnumber=903272},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Banking, Clocks, Computer science, DRAM accesses, Degradation, Delay, Dynamic scheduling, Frequency, High performance computing, Prefetching, Rambus channels, Random access memory, benchmarks, cache blocks, cache storage, integrated memory hierarchy, memory architecture, next-generation memory system, performance, performance evaluation, performance gap, },
 abstract = {In this paper we address the severe performance gap caused by high processor clock rates and slow DRAM accesses. We show that even with an aggressive, next-generation memory system using four Direct Rambus channels and an integrated one-megabyte level-two cache, a processor still spends over half of its time stalling for L2 misses. Large cache blocks can improve performance, but only when coupled with wide memory channels. DRAM address mappings also affect performance significantly. We evaluate an aggressive prefetch unit integrated with the L2 cache and memory, controllers. By issuing prefetches only when the Rambus channels are idle, prioritizing them to maximize DRAM row buffer hits, and giving them low replacement priority, we achieve a 43\% speedup across 10 of the 26 SPEC2000 benchmarks, without degrading performance an the others. With eight Rambus channels, these ten benchmarks improve to within 10\% of the performance of a perfect L2 cache },
}

@inproceedings{501178,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {ElGindy, H. and Schroder, H. and Spray, A. and Somani, A.K. and Schmeck, H.},
 year = {1996},
 pages = {108--117},
 publisher = {IEEE},
 title = {RMB-a reconfigurable multiple bus network},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501178},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501178},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501178.pdf?arnumber=501178},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Australia, Computer architecture, Computer networks, Computer science, Concurrent computing, Integrated circuit interconnections, Multiprocessor interconnection networks, RMB, Routing protocols, Spraying, Topology, circuit switching, circuit switching, full utilization, interconnection network, massively parallel computer, modularity, multiprocessor interconnection networks, reconfigurable architectures, reconfigurable multiple bus network, routing hardware, },
 abstract = {The heart of a massively parallel computer is its interconnection network. In this article we present a reconfigurable multiple bas network to support circuit switching as means of communication between processors of a multiprocessor machine. The main contribution of the papers is in demonstrating the simplicity of the routing hardware whilst still providing modularity and full utilization of the multiple bus system. A comparison with major interconnection network is also presented },
}

@inproceedings{5416648,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Arvind},
 year = {2010},
 pages = {1--1},
 publisher = {IEEE},
 title = {Is hardware innovation over?},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416648},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416648},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416648.pdf?arnumber=5416648},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {ASIC, Application specific integrated circuits, Artificial intelligence, Collaborative software, Computer architecture, Computer science, Hardware, Integrated circuit technology, Intel IA-32, Laboratories, SoC, System-on-a-chip, Technological innovation, application-specific integrated circuit, logic design, system-on-a-chip, system-on-chip, },
 abstract = {My colleagues, promotion committees, research funding agencies and business people often wonder if there is need for any architecture research. There seems to be no room to dislodge Intel IA-32. Even the number of new Application-Specific Integrated Circuits (ASICs) seems to be declining each year, because of the ever-increasing development cost. This viewpoint ignores another reality which is that the future will be dominated by mobile devices such as smart phones and the infrastructure needed to support consumer services on these devices. This is already restructuring the IT industry. To the first-order, in the mobile world functionality is determined by what can be supported within a 3W power budget. The only way to reduce power by one to two orders of magnitude is via functionally specialized hardware blocks. A fundamental shift is needed in the current design flow of systems-on-a-chip (SoCs) to produce them in a less-risky and cost-effective manner. In this talk we will present, via examples, a method of designing systems that facilitates the synthesis of complex SoCs from reusable Â¿IPÂ¿ modules. The technical challenge is to provide a method for connecting modules in a parallel setting so that the functionality and the performance of the composite are predictable. },
}

@inproceedings{824350,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Chatterjee, S. and Sen, S.},
 year = {2000},
 pages = {195--205},
 publisher = {IEEE},
 title = {Cache-efficient matrix transposition},
 date = {2000},
 doi = {10.1109/HPCA.2000.824350},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824350},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824350.pdf?arnumber=824350},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Computer science, Degradation, Educational programs, Educational technology, Electrical capacitance tomography, Electronic switching systems, Fast Fourier transforms, Multidimensional systems, Read only memory, Registers, array alignment, array layout function, data cache, limited associativity, matrix algebra, matrix transposition, memory architecture, memory system performance, optimal algorithms, register tiling, translation lookaside buffer, },
 abstract = {We investigate the memory system performance of several algorithms for transposing an N\&times;N matrix in-place, where N is large. Specifically, we investigate the relative contributions of the data cache, the translation lookaside buffer, register tiling, and the array layout function to the overall running time of the algorithms. We use various memory models to capture and analyze the effect of various facets of cache memory architecture that guide the choice of a particular algorithm, and attempt to experimentally validate the predictions of the model. Our major conclusions are as follows: limited associativity in the mapping from main memory addresses to cache sets can significantly degrade running time; the limited number of TLB entries can easily lead to thrashing; the fanciest optimal algorithms are not competitive on real machines even at fairly large problem sizes unless cache miss penalties are quite high: low-level performance tuning ``hacks", such as register tiling and array alignment, can significantly distort the effects of improved algorithms; and hierarchical non-linear layouts are inherently superior to the standard canonical layouts (such as row- or column-major) for this problem },
}

@inproceedings{501176,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Chun Xia and Torrellas, J.},
 year = {1996},
 pages = {85--94},
 publisher = {IEEE},
 title = {Improving the data cache performance of multiprocessor operating systems},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501176},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501176},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501176.pdf?arnumber=501176},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {4-processor machine, Computer applications, Condition monitoring, Contracts, DMA-like scheme, Hardware, Operating systems, Pipelines, Prefetching, Privatization, Research and development, Stress, cache storage, coherence activity, coherent caches, data cache performance, data transfer, multiprocessor operating systems, performance evaluation, performance measurements, performance monitor, shared memory systems, shared-memory multiprocessors, },
 abstract = {Bus-based shared-memory multiprocessors with coherent caches have recently become very popular. To achieve high performance, these systems rely on increasingly sophisticated cache hierarchies. However, while these machines often run loads with substantial operating system activity, performance measurements have consistently indicated that the operating system uses the data cache hierarchy poorly. In this paper, we address the issue of how to eliminate most of the data cache misses in a multiprocessor operating system while still using off-the-shelf processors. We use a performance monitor to examine traces of a 4-processor machine running four system-intensive loads under UNIX. Based on our observations, we propose hardware and software support that targets block operations, coherence activity, and cache conflicts. For block operations, simple cache bypassing or prefetching schemes are undesirable. Instead, it is best to use a DMA-like scheme that pipelines the data transfer in the bus without involving the processor. Coherence misses are handled with data, privatization and relocation, and the use of updates for a small core of shared variables. Finally, the remaining miss hot spots are handled with data prefetching. Overall, our simulations show that all these optimizations combined eliminate or hide 75\% of the operating system data misses in 32-Kbyte primary caches. Furthermore, they speed up the operating system by 19\% },
}

@inproceedings{824352,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Lefurgy, C. and Piccininni, E. and Mudge, T.},
 year = {2000},
 pages = {218--228},
 publisher = {IEEE},
 title = {Reducing code size with run-time decompression},
 date = {2000},
 doi = {10.1109/HPCA.2000.824352},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824352},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824352.pdf?arnumber=824352},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Computer architecture, Degradation, Dictionaries, Jacobian matrices, Runtime, Software performance, Table lookup, VLIW, Writing, cache management instruction, cache miss profiles, cache storage, embedded systems, run-time decompression, selective compression, software-managed instruction cache, },
 abstract = {Compressed representations of programs can be used to improve the code density in embedded systems. Several hardware decompression architectures have been proposed recently. In this paper, we present a method of decompressing programs using software. It relies on using a software-managed instruction cache under control of the decompressor. This is achieved by employing a simple cache management instruction that allows explicit writing into a cache line. We also consider selective compression (determining which procedures in a program should be compressed) and show that selection based on cache miss profiles can substantially outperform the usual execution time based profiles for some benchmarks },
}

@inproceedings{824353,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Sang-Jeong Lee and Yuan Wang and Pen-Chung Yew},
 year = {2000},
 pages = {231--240},
 publisher = {IEEE},
 title = {Decoupled value prediction on trace processors},
 date = {2000},
 doi = {10.1109/HPCA.2000.824353},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824353},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824353.pdf?arnumber=824353},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Clocks, Computer science, Decoding, Electronic switching systems, Hardware, Parallel processing, Performance gain, Registers, dynamic classification, execution-driven simulation, hybrid predictor, instruction fetch stage, parallel architectures, performance, performance evaluation, superscalar architectures, trace processor, trace processors, value prediction, },
 abstract = {Value prediction is a technique that breaks true data dependences by predicting the outcome of an instruction, and executes speculatively its data-dependent instructions based on the predicted outcome. In this paper, we address several implementation issues for value prediction which are important on wide-issue superscalar architectures, and present a value prediction scheme based on the trace processor. The scheme decouples the value prediction from the instruction fetch stage and uses a hybrid predictor with dynamic classification. We use execution-driven simulation to study the performance of such a scheme using SPECint95 benchmarks },
}

@inproceedings{824354,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Haungs, M. and Sallee, P. and Farrens, M.},
 year = {2000},
 pages = {241--250},
 publisher = {IEEE},
 title = {Branch transition rate: a new metric for improved branch classification analysis},
 date = {2000},
 doi = {10.1109/HPCA.2000.824354},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824354},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824354.pdf?arnumber=824354},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Accuracy, Electronic mail, History, branch behavior, branch classification analysis, branch prediction, branch transition rate, computer architecture, instruction sets, metric, performance evaluation, },
 abstract = {Recent studies have shown significantly improved branch prediction through the use of branch classification. By separating static branches into groups, or classes, with similar dynamic behavior predictors may be selected that are best suited for each class. Previous methods have classified branches according to taken rate (or bias). We propose a new metric for branch classification: branch transition rate, which is defined as the number of times a branch changes direction between taken and not taken during execution. We show that transition rate is a more appropriate indicator of branch behavior than taken rate for determining predictor performance. When both metrics are combined, an even clearer picture of dynamic branch behavior emerges, in which expected predictor performance for a branch is closely correlated with its combined taken and transition rate class. Using this classification, a small group of branches is identified for which two-level predictors are ineffective },
}

@inproceedings{824355,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Patil, H. and Emer, J.},
 year = {2000},
 pages = {251--262},
 publisher = {IEEE},
 title = {Combining static and dynamic branch prediction to reduce destructive aliasing},
 date = {2000},
 doi = {10.1109/HPCA.2000.824355},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824355},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824355.pdf?arnumber=824355},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Accuracy, Degradation, Feedback, Runtime, computer architecture, destructive aliasing, dynamic branch prediction, feedback, hybrid predictor, instruction sets, mispredictions, performance evaluation, profile-directed feedback, run-time behavior, state prediction, static branch prediction, },
 abstract = {Dynamic branch predictor accuracy is known to be degraded by the problem of aliasing that occurs when two branches with different run-time behavior share an entry in the dynamic predictor and that sharing results in mispredictions for the branches. In this paper, we analyze the use of state prediction of certain branches to relieve the aliasing problem in dynamic predictors. We report on our experience with using profile-directed feedback to select branches that can profitably be predicted statically in combination with some well known dynamic branch predictors. We found prediction rate improvements of up to 75\% for a simple branch predictor (ghist) and up to 14\% for a very aggressive hybrid predictor (2bcgskew) for certain programs },
}

@inproceedings{824356,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Stets, R. and Dwarkadas, S. and Kontothanassis, L. and Rencuzogullari, U. and Scott, M.L.},
 year = {2000},
 pages = {265--276},
 publisher = {IEEE},
 title = {The effect of network total order, broadcast, and remote-write capability on network-based shared memory computing},
 date = {2000},
 doi = {10.1109/HPCA.2000.824356},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824356},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824356.pdf?arnumber=824356},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Access protocols, Broadcasting, Cashmere protocol, Cashmere system, Computer networks, Cost accounting, Delay, Electronic switching systems, Protection, Random access memory, Read only memory, Space technology, broadcast, distributed shared memory systems, home node migration, low-latency messages, network total order, network-based shared memory computing, performance evaluation, protocols, remote-write capability, software distributed shared memory, system-area networks, },
 abstract = {Emerging system-area networks provide a variety of features that can dramatically reduce network communication overhead. In this paper, we evaluate the impact of such features on the implementation of Software Distributed Shared Memory (SDSM), and on the Cashmere system in particular. Cashmere has been implemented on the Compaq Memory Channel network, which supports low-latency messages, protected remote memory writes, in-expensive broadcast, and total ordering of network packets. Our evaluation is based on several Cashmere protocol variants, ranging from a protocol that fully leverages the Memory Channel's special features to one that uses the network only for fast messaging. We find that the special features improve performance by 18-44\% for three of our applications, but less than 12\% for our other seven applications. We also find that home node migration, an optimization available only in the message-based protocol, can improve performance by as much as 67\%. These results suggest that for systems of modest size, low latency is much more important for SDSM performance than are remote writes, broadcast, or total ordering. At the same time, results on an emulated 32-node system indicate that broadcast based on remote writes of widely-shared data may improve performance by up to 51\% for some applications. If hardware broadcast or multicast facilities can be made to scale, they can be beneficial in future system-area networks },
}

@inproceedings{824357,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Behr, P.M. and Pletner, S. and Sodan, A.C.},
 year = {2000},
 pages = {277--286},
 publisher = {IEEE},
 title = {PowerMANNA: a parallel architecture based on the PowerPC MPC620},
 date = {2000},
 doi = {10.1109/HPCA.2000.824357},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824357},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824357.pdf?arnumber=824357},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Application software, Bandwidth, Benchmark testing, Computer architecture, Concrete, Parallel architectures, Power system interconnection, PowerMANNA, PowerMANNA node architecture, PowerPC MPC620, Predictive models, Scalability, Throughput, architectural concepts, distributed memory systems, distributed-memory parallel computer system, parallel architecture, parallel architectures, performance evaluation, shared-memory machines, superscale microprocessor, },
 abstract = {The paper presents PowerMANNA, a distributed-memory parallel computer system based on the 64-Bit PowerPC processor MPC620. The PowerMANNA node architecture supports all the sophisticated features of the MPC620 and incorporates important architectural concepts that allow us to exploit the performance of modern superscale microprocessor in the context of massively parallel supercomputing. The two-way processor nodes of PowerMANNA are embedded in a powerful communication system supporting low-latency communication and maximum connectivity. Processing and communication performance of an eight-node prototype are shown and compared with shared-memory machines and clusters. In the course of the presentation, experience gained with the PowerPC MP620 processor is discussed },
}

@inproceedings{824358,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Hosomi, T. and Kanoh, Y. and Nakamura, M. and Hirose, T.},
 year = {2000},
 pages = {287--298},
 publisher = {IEEE},
 title = {A DSM architecture for a parallel computer Cenju-4},
 date = {2000},
 doi = {10.1109/HPCA.2000.824358},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824358},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824358.pdf?arnumber=824358},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Computer architecture, Concurrent computing, DSM architecture, Decoding, bit-pattern directory, cache-coherent non-uniform memory access multiprocessor, ccNUMA multiprocessor, centralized directory scheme, deadlock, distributed shared memory systems, invalidation request messages, parallel architectures, parallel computer Cenju-4, performance evaluation, scalability, starvation, store access latency, },
 abstract = {A parallel computer Cenju-4 is a cache-coherent non-uniform memory access (ccNUMA) multiprocessor and designed to be scalable up to 1024 nodes. For scalability, Cenju-4 adopts a bit-pattern directory. This scheme enables more precise representation than other imprecise schemes, such as a coarse vector scheme. Cenju-4 utilizes multicast and gathering functions of the network for delivering invalidation request messages and for collecting replies. This enables store access latency to be scalable, even when the block is shared among all nodes. Cenju-4 also prevents starvation and deadlock by queuing certain types of messages in the main memory. This enables a full solution to the starvation problem with centralized directory scheme, and to the deadlock problem with one physical or virtual network. The buffer sizes required for queuing messages at each node are only 32K bytes and two 64K bytes on a 2024-node system. In this paper, we present the design of the DSM architecture and some performance results },
}

@inproceedings{824359,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Moshovos, A. and Sohi, G.S.},
 year = {2000},
 pages = {301--312},
 publisher = {IEEE},
 title = {Memory dependence speculation tradeoffs in centralized, continuous-window superscalar processors},
 date = {2000},
 doi = {10.1109/HPCA.2000.824359},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824359},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824359.pdf?arnumber=824359},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Clocks, Inspection, Processor scheduling, Registers, complexity, computational complexity, continuous-window superscalar processors, distributed window processor models, hardware-based methods, latency, load/store parallelism, memory dependence speculation, memory dependence speculation tradeoffs, parallel processing, performance evaluation, },
 abstract = {We consider a variety of dynamic, hardware-based methods for exploiting load/store parallelism, including mechanisms that use memory dependence speculation. While previous work has also investigated such methods, this has been done primarily for split, distributed window processor models. We focus on centralized continuous-window processor models (the common configuration today). We confirm that exploiting load/store parallelism can greatly improve performance. Moreover, we show that much of this performance potential can be captured if addresses of the memory locations accessed by both loads and stores can be used to schedule loads. However, using addresses to schedule load execution may not always be an option due to complexity, latency, and cost considerations. For this reason, we also consider configurations that use just memory dependence speculation to guide load execution. We consider a variety of methods and show that speculation/synchronization can be used to effectively exploit virtually all load/store parallelism. We demonstrate that this technique is competitive to or better than the one that uses addresses for scheduling loads. We conclude by discussing why our findings differ, in part, from those reported for split, distributed window processor models },
}

@inproceedings{5416645,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Qureshi, M.K. and Franceschini, M.M. and Lastras-Montano, L.A.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Improving read performance of Phase Change Memories via Write Cancellation and Write Pausing},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416645},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416645},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416645.pdf?arnumber=5416645},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Costs, Crystallization, Delay, Electric resistance, PCM controller, Phase change materials, Phase change memory, Random access memory, Read-write memory, Switches, System performance, baseline PCM system, large-scale main memory systems, phase change memories, random-access storage, read latency, read-priority scheduling, storage management, write cancellation, write pausing, },
 abstract = {Phase Change Memory (PCM) is emerging as a promising technology to build large-scale main memory systems in a cost-effective manner. A characteristic of PCM is that it has write latency much higher than read latency. A higher write latency can typically be tolerated using buffers. However, once a write request is scheduled for service to a bank, it can still cause increased latency for later arriving read requests to the same bank. We show that for the baseline PCM system with read-priority scheduling, the write requests increase the effective read latency to 2.3x (on average), causing significant performance degradation. To reduce the read latency of PCM devices under such scenarios, we propose adaptive Write Cancellation policies. Such policies can abort the processing of a scheduled write requests if a read request arrives to the same bank within a predetermined period. We also propose Write Pausing, which exploits the iterative write algorithms used in PCM to pause at the end of each write iteration to service any pending reads. For the baseline system, the proposed technique removes 75\% of the latency increase incurred by read requests and improves overall system performance by 46\% (on average), while requiring negligible hardware and simple extensions to PCM controller. },
}

@inproceedings{5416644,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Hyunjin Lee and Sangyeun Cho and Childers, B.R.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {StimulusCache: Boosting performance of chip multiprocessors with excess cache},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416644},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416644},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416644.pdf?arnumber=5416644},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Boosting, Circuit faults, Computer science, Degradation, Error correction codes, L2 cache, Logic, Multicore processing, Redundancy, Semiconductor device manufacture, StimulusCache, Sun, cache storage, chip multiprocessors, compute cores, design strategy, integrated circuit design, logic complexity, microprocessor chips, multi-threading, multicore chip, multithreaded workloads, on-chip devices, on-chip memory utilization scheme, single-threaded workloads, },
 abstract = {Technology advances continuously shrink on-chip devices. Consequently, the number of cores in a single chip multiprocessor (CMP) is expected to grow in coming years. Unfortunately, with smaller device size and greater integration, chip yield degrades significantly. Guaranteeing that all chip components function correctly leads to an unrealistically low yield. Chip vendors have adopted a design strategy to market partially functioning processor chips to combat this problem. The two major components in a multicore chip are compute cores and on-chip memory such as L2 cache. From the viewpoint of the chip yield, the compute cores have a much lower yield than the on-chip memory due to their logic complexity and well-established memory yield enhancing techniques. Therefore, future CMPs are expected to have more available on-chip memories than working cores. This paper introduces a novel on-chip memory utilization scheme called StimulusCache, which decouples the L2 caches of faulty compute cores and employs them to assist applications on other working cores. Our extensive experimental evaluation demonstrates that StimulusCache significantly improves the performance of both single-threaded and multithreaded workloads. },
}

@inproceedings{5416643,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Romanescu, B.F. and Lebeck, A.R. and Sorin, D.J. and Bracy, A.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {UNified Instruction/Translation/Data (UNITD) coherence: One protocol to rule them all},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416643},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416643},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416643.pdf?arnumber=5416643},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Computer architecture, Hardware, Local activities, Memory management, Microarchitecture, Multicore processing, Multiprocessing systems, Protocols, Software maintenance, Software performance, UNITD coherence protocol, cache coherence protocol, cache storage, multicore processor, multiprocessing systems, unified hardware coherence framework, unified instruction-translation-data, },
 abstract = {We propose UNITD, a unified hardware coherence framework that integrates translation coherence into the existing cache coherence protocol. In UNITD coherence protocols, the TLBs participate in the cache coherence protocol just like the instruction and data caches, without requiring any changes to the existing coherence protocol. UNITD eliminates the need for the software TLB shootdown routine, a procedure known to be performance costly and non-scalable. We evaluate snooping and directory UNITD coherence protocols on multicore processors with 2-16 cores, and we demonstrate that UNITD reduces the performance penalty associated with TLB coherence to almost zero. },
}

@inproceedings{5416642,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Xiaowei Jiang and Madan, N. and Li Zhao and Upton, M. and Iyer, R. and Makineni, S. and Newell, D. and Solihin, Y. and Balasubramonian, R.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {CHOP: Adaptive filter-based DRAM caching for CMP server platforms},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416642},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416642},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416642.pdf?arnumber=5416642},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Adaptive filters, Bandwidth, CHOP, CHOP, CMP server platforms, Computer architecture, Concurrent computing, Costs, DRAM cache, DRAM chips, Delay, Multicore processing, Organizing, Random access memory, Throughput, adaptive filter, adaptive filter based DRAM caching, caching hot pages, filter cache, hot page, manycore architectures, on-chip tag area, },
 abstract = {As manycore architectures enable a large number of cores on the die, a key challenge that emerges is the availability of memory bandwidth with conventional DRAM solutions. To address this challenge, integration of large DRAM caches that provide as much as 5Ã higher bandwidth and as low as 1/3rd of the latency (as compared to conventional DRAM) is very promising. However, organizing and implementing a large DRAM cache is challenging because of two primary tradeoffs: (a) DRAM caches at cache line granularity require too large an on-chip tag area that makes it undesirable and (b) DRAM caches with larger page granularity require too much bandwidth because the miss rate does not reduce enough to overcome the bandwidth increase. In this paper, we propose CHOP (Caching HOt Pages) in DRAM caches to address these challenges. We study several filter-based DRAM caching techniques: (a) a filter cache (CHOP-FC) that profiles pages and determines the hot subset of pages to allocate into the DRAM cache, (b) a memory-based filter cache (CHOP-MFC) that spills and fills filter state to improve the accuracy and reduce the size of the filter cache and (c) an adaptive DRAM caching technique (CHOP-AFC) to determine when the filter cache should be enabled and disabled for DRAM caching. We conduct detailed simulations with server workloads to show that our filter-based DRAM caching techniques achieve the following: (a) on average over 30\% performance improvement over previous solutions, (b) several magnitudes lower area overhead in tag space required for cache-line based DRAM caches, (c) significantly lower memory bandwidth consumption as compared to page-granular DRAM caches. },
}

@inproceedings{5416641,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Merino, J. and Puente, V. and Gregorio, J.A.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {ESP-NUCA: A low-cost adaptive Non-Uniform Cache Architecture},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416641},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416641},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416641.pdf?arnumber=5416641},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Automatic speech recognition, Computer architecture, Cooperative caching, Costs, Degradation, Delay, Interference, Proposals, Stability, Yarn, average on-chip access latency, cache storage, chip multiprocessors, computer architecture, enhanced shared-private non-uniform cache architecture, inter-core interference, multiprocessing systems, private cache blocks, },
 abstract = {This paper introduces a cost effective cache architecture called Enhanced Shared-Private Non-Uniform Cache Architecture (ESP-NUCA), which is suitable for highperformance Chip MultiProcessors (CMPs). This architecture enhances system stability by combining the advantages of private and shared caches. Starting from a shared NUCA, ESP-NUCA introduces a low-cost mechanism to dynamically allocate private cache blocks closer to their owner processor. In this way, average on-chip access latency is reduced and inter-core interference minimized. ESP-NUCA synergistically integrates victims and replicas thus making it possible to take advantage of multiple-readers for shared data, and to maximize cache usage under unbalanced core utilization. This architecture leads to stable behavior within the whole system across a broad spectrum of working scenarios. ESP-NUCA not only outperforms architectures with similar implementation costs such as private and shared caches by up to 20\% and 40\% respectively, but even outperforms much costlier architectures such as D-NUCA [13] by up to 28\%, Adaptive Selective Replication [3] by up to 19\%, and Cooperative Caching [5] by up to 15\%. Moreover, performance variance throughout the set of benchmarks is 37\% lower than with ASR, 87\% lower than with D-NUCA, and 43\% lower than with Cooperative Caching. },
}

@inproceedings{1183529,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Hai Li and Bhunia, S. and Chen, Y. and Vijaykumar, T.N. and Roy, K.},
 year = {2003},
 pages = { 113-- 122},
 publisher = {IEEE},
 title = {Deterministic clock gating for microprocessor power reduction},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183529},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183529},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183529.pdf?arnumber=1183529},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { D-Cache wordline decoders,  cache storage,  deterministic clock gating,  execution units,  microprocessor chips,  microprocessor designs,  microprocessor power reduction,  nonpredictive methodologies,  out-of-order superscalar processor,  performance,  performance evaluation,  pipeline balancing,  pipeline latches,  pipeline processing,  power consumption,  power control,  power dissipation,  result bus drivers, Circuits, Clocks, Decoding, Latches, Microprocessors, Out of order, Performance loss, Pipelines, Power dissipation, Power system reliability, },
 abstract = {With the scaling of technology and the need for higher performance and more functionality, power dissipation is becoming a major bottleneck for microprocessor designs. Pipeline balancing (PLB), a previous technique, is essentially a methodology to clock-gate unused components whenever a program's instruction-level parallelism is predicted to be low. However, no nonpredictive methodologies are available in the literature for efficient clock gating. This paper introduces deterministic clock gating (DCG) based on the key observation that for many of the stages in a modern pipeline, a circuit block's usage in a specific cycle in the near future is deterministically known a few cycles ahead of time. Our experiments show an average of 19.9\% reduction in processor power with virtually no performance loss for an 8-issue, out-of-order superscalar processor by applying DCG to execution units, pipeline latches, D-Cache wordline decoders, and result bus drivers. In contrast, PLB achieves 9.9\% average power savings at 2.9\% performance loss. },
}

@inproceedings{1183528,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Aragon, J.L. and Gonzalez, J. and Gonzalez, A.},
 year = {2003},
 pages = { 103-- 112},
 publisher = {IEEE},
 title = {Power-aware control speculation through selective throttling},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183528},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183528},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183528.pdf?arnumber=1183528},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { branch mispredictions,  branch prediction confidence level,  decode throttling,  delays,  energy reduction,  energy-delay improvement,  fetch bandwidth reduction,  fetch throttling,  high-performance processors,  mis-speculated instructions,  parallel architectures,  performance evaluation,  pipeline processing,  pipelines,  power consumption,  power control,  power dissipation,  power-aware control speculation,  selection logic disabling,  selective throttling, Bandwidth, Clocks, Decoding, Energy consumption, Engines, Frequency, Logic, Pipelines, Power dissipation, Transistors, },
 abstract = {With the constant advances in technology that lead to the increasing of the transistor count and processor frequency, power dissipation is becoming one of the major issues in high-performance processors. These processors increase their clock frequency by lengthening the pipeline, which puts more pressure on the branch prediction engine since branches take longer to be resolved. Branch mispredictions are responsible for around 28\% of the power dissipated by a typical processor due to the useless activities performed by instructions that are squashed. This work focuses on reducing the power dissipated by mis-speculated instructions. We propose selective throttling as an effective way of triggering different power-aware techniques (fetch throttling, decode throttling or disabling the selection logic). The particular set of techniques applied to each branch is dynamically chosen depending on the branch prediction confidence level. For branches with a low confidence on the prediction, the most aggressive throttling mechanism is used whereas high confidence branch predictions trigger the least aggressive techniques. Results show that combining fetch bandwidth reduction along with select logic disabling provides the best performance both in terms of energy reduction and energy-delay improvement (14\% and 9\% respectively for 14 stages, and 17\% and 12\% respectively for 28 stages). },
}

@inproceedings{1183527,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Li Shang and Li-Shiuan Peh and Jha, N.K.},
 year = {2003},
 pages = { 91-- 102},
 publisher = {IEEE},
 title = {Dynamic voltage scaling with links for power optimization of interconnection networks},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183527},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183527},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183527.pdf?arnumber=1183527},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { average latency,  communication links,  delays,  dynamic voltage scaling,  frequency adjustment,  frequency control,  history-based DVS policy,  interconnection networks,  multiprocessor interconnection networks,  network saturation,  performance,  performance evaluation,  power consumption,  power consumption minimization,  power efficiency,  power optimization,  throughput reduction,  voltage control, Bandwidth, Circuits, Dynamic voltage scaling, Fabrics, Frequency, IP networks, Microprocessors, Multiprocessing systems, Multiprocessor interconnection networks, Voltage control, },
 abstract = {Originally developed to connect processors and memories in multicomputers, prior research and design of interconnection networks have focused largely on performance. As these networks get deployed in a wide range of new applications, where power is becoming a key design constraint, we need to seriously consider power efficiency in designing interconnection networks. As the demand for network bandwidth increases, communication links, already a significant consumer of power now, will take up an ever larger portion of total system power budget. In this paper we motivate the use of dynamic voltage scaling (DVS) for links, where the frequency and voltage of links are dynamically adjusted to minimize power consumption. We propose a history-based DVS policy that judiciously adjusts link frequencies and voltages based on past utilization. Our approach realizes up to 6.3\&times; power savings (4.6\&times; on average). This is accompanied by a moderate impact on performance (15.2\% increase in average latency before network saturation and 2.5\% reduction in throughput.) To the best of our knowledge, this is the first study that targets dynamic power optimization of interconnection networks. },
}

@inproceedings{1183526,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Joseph, R. and Brooks, D. and Martonosi, M.},
 year = {2003},
 pages = { 79-- 90},
 publisher = {IEEE},
 title = {Control techniques to eliminate voltage emergencies in high performance processors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183526},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183526},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183526.pdf?arnumber=1183526},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { ITRS roadmap,  chip packaging techniques,  computer architecture,  controller error,  controllers,  dI/dt stressmark,  delay,  delays,  high performance processors,  integrated circuit packaging,  microarchitectural control mechanisms,  microprocessor chips,  microprocessors,  performance evaluation,  power consumption,  power dissipation,  resonant frequencies,  supply voltage fluctuations,  voltage control, Clocks, Microarchitecture, Microprocessors, Packaging, Power dissipation, Power supplies, Proposals, Resonant frequency, Voltage control, Voltage fluctuations, },
 abstract = {Increasing focus on power dissipation issues in current microprocessors has led to a host of proposals for clock gating and other power-saving techniques. While generally effective at reducing average power, many of these techniques have the undesired side-effect of increasing both the variability of power dissipation and the variability of current drawn by the processor This increase in current variability, often referred to as the dI/dt problem, can cause supply voltage fluctuations. Such voltage fluctuations lead to unreliable circuits if not addressed, and increasingly expensive chip packaging techniques are needed to mitigate them. This paper proposes and evaluates a methodology for augmenting packaging techniques for dI/dt with microarchitectural control mechanisms. We discuss the resonant frequencies most relevant to current microprocessor packages, produce and evaluate a "dI/dt stressmark" that exercises the system at its resonant frequency, and characterize the behavior of more mainstream applications. Based on these results plus evaluations of the impact of controller error and delay, our microarchitectural control proposals offer bounds on supply voltage fluctuations, with nearly negligible impact on performance and energy. With the ITRS roadmap predicting aggressive drops in supply voltage and power supply impedances in coming chip generations, novel voltage control techniques will be required to stay on track. Our microarchitectural dI/dt controllers represent a step in this direction. },
}

@inproceedings{1183525,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Lei Chen and Dropsho, S. and Albonesi, D.H.},
 year = {2003},
 pages = { 65-- 76},
 publisher = {IEEE},
 title = {Dynamic data dependence tracking and its application to branch prediction},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183525},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183525},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183525.pdf?arnumber=1183525},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { ARVI,  ILP,  SPEC95 integer benchmark suite,  available register value information,  cycle-by-cycle basis,  dynamic data dependence tracking,  efficient hardware mechanism,  instruction level parallelism,  microengine,  microprocessor chips,  parallel architectures,  performance evaluation,  processor performance,  value-based branch prediction, Accuracy, Application software, Computer architecture, Computer science, Microprocessors, Out of order, Parallel processing, Read-write memory, Registers, Runtime, },
 abstract = {To continue to improve processor performance, microarchitects seek to increase the effective instruction level parallelism (ILP) that can be exploited in applications. A fundamental limit to improving ILP is data dependences among instructions. If data dependence information is available at run-time, there are many uses to improve ILP. Prior published examples include decoupled branch execution architectures and critical instruction detection. In this paper, we describe an efficient hardware mechanism to dynamically track the data dependence chains of the instructions in the pipeline. This information is available on a cycle-by-cycle basis to the microengine for optimizing its performance. We then use this design in a new value-based branch prediction design using available register value information (ARVI). From the use of data dependence information, the ARVI branch predictor has better prediction accuracy over a comparably sized hybrid branch predictor With ARVI used as the second-level branch predictor the improved prediction accuracy results in a 12.6\% performance improvement on average across the SPEC95 integer benchmark suite. },
}

@inproceedings{1183524,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Simon, B. and Calder, B. and Ferrante, J.},
 year = {2003},
 pages = { 53-- 64},
 publisher = {IEEE},
 title = {Incorporating predicate information into branch predictors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183524},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183524},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183524.pdf?arnumber=1183524},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { conditional branch,  intelligent branch prediction,  mispredicted branches,  parallel architectures,  performance evaluation,  predicate global update branch predictor,  predicate information,  predicated code,  predicated execution,  recently calculated predicate definitions,  region-based branches,  squash false path filter, Computer aided instruction, Computer architecture, Computer science, Concurrent computing, Costs, Delay, Filters, Mathematics, Parallel processing, VLIW, },
 abstract = {Predicated execution can be used to alleviate the costs associated with frequently mispredicted branches. This is accomplished by trading the cost of a mispredicted branch for execution of both paths following the conditional branch. In this paper we examine two enhancements for branch prediction in the presence of predicated code. Both of the techniques use recently calculated predicate definitions to provide a more intelligent branch prediction. The first branch predictor, called the squash false path filter, recognizes fetched branches known to be guarded with a false predicate and predicts them as not-taken with 100\% accuracy. The second technique, called the predicate global update branch predictor, improves prediction by incorporating recent predicate information into the branch predictor. We use these techniques to aid the prediction of region-based branches. A region-based branch is a branch that is left in a predicated region of code. A region-based branch may be correlated with predicate definitions in the region in addition to those that define the branch's guarding predicate. },
}

@inproceedings{1183523,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Jimenez, D.A.},
 year = {2003},
 pages = { 43-- 52},
 publisher = {IEEE},
 title = {Reconsidering complex branch predictors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183523},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183523},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183523.pdf?arnumber=1183523},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { IPC,  aggressively clocked microarchitectures,  complex branch predictors,  delays,  instruction throughput rates,  latency hiding,  parallel architectures,  performance,  performance evaluation,  pipeline processing,  pipelines,  predictor latency, Arithmetic, Clocks, Computer science, Delay, Hardware, Microarchitecture, Microprocessors, Pipelines, Random access memory, Throughput, },
 abstract = {To sustain instruction throughput rates in more aggressively clocked microarchitectures, microarchitects have incorporated larger and more complex branch predictors into their designs, taking advantage of the increasing numbers of transistors available on a chip. Unfortunately, because of penalties associated with their implementations, the extra accuracy provided by many branch predictors does not produce a proportionate increase in performance. Specifically, we show that the techniques used to hide the latency of a large and complex branch predictor do not scale well and will be unable to sustain IPC for deeper pipelines. We investigate a different way to build large branch predictors. We propose an alternative predictor design that completely hides predictor latency so that accuracy and hardware budget are the only factors that affect the efficiency of the predictor. Our simple design allows the predictor to be pipelined efficiently by avoiding difficulties introduced by complex predictors. Because this predictor eliminates the penalties associated with complex predictors, overall performance exceeds that of even the most accurate known branch predictors in the literature at large hardware budgets. We conclude that as chip densities increase in the next several years, the accuracy of complex branch predictors must be weighed against the performance benefits of simple branch predictors. },
}

@inproceedings{1183522,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {El-Moursy, A. and Albonesi, D.H.},
 year = {2003},
 pages = { 31-- 40},
 publisher = {IEEE},
 title = {Front-end policies for improved issue efficiency in SMT processors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183522},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183522},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183522.pdf?arnumber=1183522},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { SMT processors,  dynamic superscalar microprocessors,  floating point queue size,  front-end policies,  hardware simplification,  improved issue efficiency,  integer queue size,  issue queue occupancy,  issue queue simplification,  microprocessor chips,  multi-threading,  parallel architectures,  parallelism,  performance,  performance evaluation,  power consumption,  power optimization,  simultaneous multi-threading processor, Clocks, Costs, Delay, Hardware, Microprocessors, Parallel processing, Surface-mount technology, Technological innovation, Timing, Yarn, },
 abstract = {The performance and power optimization of dynamic superscalar microprocessors requires striking a careful balance between exploiting parallelism and hardware simplification. Hardware structures which are needlessly complex may exacerbate critical timing paths and dissipate extra power. One such structure requiring careful design is the issue queue. In a simultaneous multi-threading (SMT) processor it is particularly challenging to achieve issue queue simplification due to the increased utilization of the queue afforded by multi-threading. In this paper we propose new front-end policies that reduce the required integer and floating point issue queue sizes in SMT processors. We explore both general policies as well as those directed towards alleviating a particular cause of issue queue inefficiency. For the same level of performance, the most effective policies reduce the issue queue occupancy by 33\% for an SMT processor with appropriately sized issue queue resources. },
}

@inproceedings{1183521,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Redstone, J. and Eggers, S. and Levy, H.},
 year = {2003},
 pages = { 19-- 30},
 publisher = {IEEE},
 title = {Mini-threads: increasing TLP on small-scale SMT processors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183521},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183521},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183521.pdf?arnumber=1183521},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { TLP,  architectural register set,  microprocessor chips,  mini-threads,  multi-CPU chips,  multi-threading,  parallel architectures,  performance,  performance evaluation,  register file size,  simultaneous-multithreaded processors,  small-scale SMT processors,  thread-level parallelism, Application software, Hardware, Impedance, Manufacturing processes, Parallel processing, Pipelines, Registers, Surface-mount technology, Throughput, Yarn, },
 abstract = {Several manufacturers have recently announced the first simultaneous-multithreaded processors, both as single CPU and as components of multi-CPU chips. All are small scale, comprising only two to four thread contexts. A significant impediment to the construction of larger-scale SMT is the register file size required by a large number of contexts. This paper introduces and evaluates mini-threads, a simple extension to SMT that increases thread-level parallelism without the commensurate increase in register file size. A mini-threaded SMT CPU adds additional per-thread state to each hardware context; an application executing in a context can create mini-threads that will utilize its own per-thread state, but share the context's architectural register set. The resulting performance will depend on the benefits of additional TLP compared to the costs of executing mini-threads with fewer registers. Our results quantify these factors in detail and demonstrate that mini-threads can improve performance significantly, particularly on small-scale, space-sensitive CPU designs. },
}

@inproceedings{1183520,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Alameldeen, A.R. and Wood,D.A.},
 year = {2003},
 pages = {7--18},
 publisher = {IEEE},
 title = {Variability in architectural simulations of multi-threaded workloads},
 date = {12-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183520},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183520},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183520.pdf?arnumber=1183520},
 issn = {1530-0897},
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = {Application software, Computational modeling, Computer architecture, Current measurement, Databases, Internet services, Multiprocessing systems, Probability, Timing, Web and internet services, Web server, architectural simulations, multi-threaded workloads, multi-threading, multiple simulations, multiprocessing systems, multiprocessor system, parallel architectures, performance evaluation, performance evaluation, performance variability, statistical analysis, statistical techniques, uniprocessor system, virtual machines, },
 abstract = {Multi-threaded commercial workloads implement many important Internet services. Consequently, these workloads are increasingly used to evaluate the performance of uniprocessor and multiprocessor system designs. This paper identifies performance variability as a potentially major challenge for architectural simulation studies using these workloads. Variability refers to the differences between multiple estimates of a workload's performance. Time variability occurs when a workload exhibits different characteristics during different phases of a single run. Space variability occurs when small variations in timing cause runs starting from the same initial condition to follow widely different execution paths. Variability is a well-known phenomenon in real systems, but is nearly universally ignored in simulation experiments. In a central result of this paper we show that variability in multi-threaded commercial workloads can lead to incorrect architectural conclusions (e.g., 31\% of the time in one experiment). We propose a methodology, based on multiple simulations and standard statistical techniques, to compensate for variability. Our methodology greatly reduces the probability of reaching incorrect conclusions, while enabling simulations to finish within reasonable time limits. },
}

@inproceedings{5749749,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Robatmili, Behnam and Govindan, Sibi and Burger, Doug and Keckler, Stephen W.},
 year = {2011},
 pages = {431--442},
 publisher = {IEEE},
 title = {Exploiting criticality to reduce bottlenecks in distributed uniprocessors},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749749},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749749},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749749.pdf?arnumber=5749749},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Composable multicore systems merge multiple independent cores for running sequential single-threaded workloads. The performance scalability of these systems, however, is limited due to partitioning overheads. This paper addresses two of the key performance scalability limitations of composable multicore systems. We present a critical path analysis revealing that communication needed for cross-core register value delivery and fetch stalls due to misspeculation are the two worst bottlenecks that prevent efficient scaling to a large number of fused cores. To alleviate these bottlenecks, this paper proposes a fully distributed framework to exploit criticality in these architectures at different granularities. A coordinator core exploits different types of block-level communication criticality information to fine-tune critical instructions at decode and register forward pipeline stages of their executing cores. The framework exploits the fetch criticality information at a coarser granularity by reissuing all instructions in the blocks previously fetched into the merged cores. This general framework reduces competing bottlenecks in a synergic manner and achieves scalable performance/power efficiency for sequential programs when running across a large number of cores. },
}

@inproceedings{5749748,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Anderson, Owen and Fortuna, Emily and Ceze, Luis and Eggers, Susan},
 year = {2011},
 pages = {419--430},
 publisher = {IEEE},
 title = {Checked Load: Architectural support for JavaScript type-checking on mobile processors},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749748},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749748},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749748.pdf?arnumber=5749748},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Dynamic languages such as Javascript are the de-facto standard for web applications. However, generating efficient code for dynamically-typed languages is a challenge, because it requires frequent dynamic type checks. Our analysis has shown that some programs spend upwards of 20\% of dynamic instructions doing type checks, and 12.9\% on average. In this paper we propose Checked Load, a low-complexity architectural extension that replaces software-based, dynamic type checking. Checked Load is comprised of four new ISA instructions that provide flexible and automatic type checks for memory operations, and whose implementation requires minimal hardware changes. We also propose hardware support for dynamic type prediction to reduce the cost of failed type checks. We show how to use Checked Load in the Nitro JavaScript just-in-time compiler (used in the Safari 5 browser). Speedups on a typical mobile processor range up to 44.6\% (with a mean of 11.2\%) in popular JavaScript benchmarks. While we have focused our work on JavaScript, Checked Load is sufficiently general to support other dynamically-typed languages, such as Python or Ruby. },
}

@inproceedings{5749743,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Lee, Dongyoon and Said, Mahmoud and Narayanasamy, Satish and Yang, Zijiang},
 year = {2011},
 pages = {357--358},
 publisher = {IEEE},
 title = {Offline symbolic analysis to infer Total Store Order},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749743},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749743},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749743.pdf?arnumber=5749743},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Ability to record and replay an execution can significantly help programmers debug their programs, especially parallel programs. De-terministically replaying a multiprocessor's execution under a relaxed memory model has remained a challenging problem. This is an important problem as most modern processors only support a relaxed memory model to enable many performance critical optimizations. The most common consistency model implemented in processors is the Total Store Order (TSO). We present an efficient and low-complexity processor based solution for recording and replaying under the Total Store Order (TSO) memory model. Processor provides support for logging data fetched on cache misses. Using this information each thread can be de-terministically replayed. A TSO-compliant casual order between the shared-memory accesses executed in different threads is then inferred using an offline algorithm based on Satisfiability Modulo Theory (SMT) solver. We also discuss methods to bound the search space during offline analysis and several optimizations to reduce the offline analysis time. },
}

@inproceedings{5749742,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Joshi, Madhura and Zhang, Wangyuan and Li, Tao},
 year = {2011},
 pages = {345--356},
 publisher = {IEEE},
 title = {Mercury: A fast and energy-efficient multi-level cell based Phase Change Memory system},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749742},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749742},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749742.pdf?arnumber=5749742},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Phase Change Memory (PCM) is one of the most promising technologies among emerging non-volatile memories. PCM stores data in crystalline and amorphous phases of the GST material using large differences in their electrical resistivity. Although it is possible to design a high capacity memory system by storing multiple bits at intermediate levels between the highest and lowest resistance states of PCM, it is difficult to obtain the tight distribution required for accurate reading of the data. Moreover, the required programming latency and energy for a Multiple Level PCM (MLC-PCM) cell is not trivial and can act as a major hurdle in adopting multilevel PCM in a high-density memory architecture design. Furthermore, the effect of process variation (PV) on PCM cell exacerbates the variability in necessary programming current and hence the target resistance spread, leading to the demand for high-latency, multi-iteration-based programming-and-verify write schemes for MLC-PCM. PV-aware control of programming current, programming using staircase down current pulses and programming using increasing reset current pulses are some of the traditional techniques used to achieve optimum programming energy, write latency and accuracy, but they usually target on optimizing only one aspect of the design. In this paper, we address the high-write latency and process variation issues of MLC-PCM by introducing Mercury: A fast and energy efficient multi-level cell based phase change memory architecture. Mercury adapts the programming scheme of a multi-level PCM cell by taking into consideration the initial state of the cell, the target resistance to be programmed and the effect of process variation on the programming current profile of the cell. The proposed techniques act at circuit as well as microarchitecture levels. Simulation results show that Mercury achieves 10\% saving in programming latency and 25\% saving in programming energy for the PCM memory system compared to that of the tradit- - ional methods. },
}

@inproceedings{5749741,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Hower, Derek R and Dudnik, Polina and Hill, Mark D. and Wood, David A.},
 year = {2011},
 pages = {333--334},
 publisher = {IEEE},
 title = {Calvin: Deterministic or not? Free will to choose},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749741},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749741},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749741.pdf?arnumber=5749741},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Most shared memory systems maximize performance by unpredictably resolving memory races. Unpredictable memory races can lead to nondeterminism in parallel programs, which can suffer from hard-to-reproduce hiesenbugs. We introduce Calvin, a shared memory model capable of executing in a conventional nondeterministic mode when performance is paramount and a deterministic mode when execution repeatability is important. Unlike prior hardware proposals for deterministic execution, Calvin exploits the flexibility of a memory consistency model weaker than sequential consistency. Specifically, Calvin logically orders memory operations into strata that are compatible with the Total Store Order (TSO). Calvin is also designed with the needs of future power-aware processors in mind, and does not require any speculation support. We develop a Calvin-MIST implementation that uses an unordered coalescing write cache, multiple-write coherence protocol, and delayed (timebomb) invalidations while maintaining TSO compatibility. Results show that Calvin-MIST can execute workloads in conventional mode at speeds comparable to a conventional system (providing compatibility) or execute deterministically for a modest average slowdown of less than 20\% (when determinism is valued). },
}

@inproceedings{5749740,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Carretero, Javier and Vera, Xavier and Abella, Jaume and Ramirez, Tanausu and Monchiero, Matteo and Gonzalez, Antonio},
 year = {2011},
 pages = {321--331},
 publisher = {IEEE},
 title = {Hardware/software-based diagnosis of load-store queues using expandable activity logs},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749740},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749740},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749740.pdf?arnumber=5749740},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The increasing device count and design complexity are posing significant challenges to post-silicon validation. Bug diagnosis is the most difficult step during post-silicon validation. Limited reproducibility and low testing speeds are common limitations in current testing techniques. Moreover, low observability defies full-speed testing approaches. Modern solutions like on-chip trace buffers alleviate these issues, but are unable to store long activity traces. As a consequence, the cost of post-Si validation now represents a large fraction of the total design cost. This work describes a hybrid post-Si approach to validate a modern load-store queue. We use an effective error detection mechanism and an expandable logging mechanism to observe the microarchitectural activity for long periods of time, at processor full-speed. Validation is performed by analyzing the log activity by means of a diagnosis algorithm. Correct memory ordering is checked to root the cause of errors. },
}

@inproceedings{5749747,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Pellauer, Michael and Adler, Michael and Kinsy, Michel and Parashar, Angshuman and Emer, Joel},
 year = {2011},
 pages = {406--417},
 publisher = {IEEE},
 title = {HAsim: FPGA-based high-detail multicore simulation using time-division multiplexing},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749747},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749747},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749747.pdf?arnumber=5749747},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 keywords = {FPGA, Field-Programmable Gate Arrays, Modeling, On-Chip Networks, Simulation, },
 abstract = {In this paper we present the HAsim FPGA-accelerated simulator. HAsim is able to model a shared-memory multicore system including detailed core pipelines, cache hierarchy, and on-chip network, using a single FPGA. We describe the scaling techniques that make this possible, including novel uses of time-multiplexing in the core pipeline and on-chip network. We compare our time-multiplexed approach to a direct implementation, and present a case study that motivates why high-detail simulations should continue to play a role in the architectural exploration process. },
}

@inproceedings{5749746,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Jacobson, Hans and Buyuktosunoglu, Alper and Bose, Pradip and Acar, Emrah and Eickemeyer, Richard},
 year = {2011},
 pages = {394--405},
 publisher = {IEEE},
 title = {Abstraction and microarchitecture scaling in early-stage power modeling},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749746},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749746},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749746.pdf?arnumber=5749746},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Early-stage, microarchitecture-level power modeling methodologies have been used in industry and academic research for a decade (or more). Such methods use cycle-accurate performance simulators and deduce active power based on utilization markers. A key question faced in this context is: what key utilization metrics to monitor, and how many are needed for accuracy? Is there a systematic way to select the \&#x201C;best\&#x201D; markers? We also pose a key follow-on question: is it possible to perform accurate scaling of an abstracted model to enable exploration of new microarchitecture features? In this paper, we address these particular questions and examine the results for a range of abstraction levels. We highlight innovative insights for intelligent abstraction and microarchitecture scaling, and point out the pitfalls of abstractions that are not based on a systematic methodology or sound theory. },
}

@inproceedings{5749745,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Zhang, Yao and Owens, John D.},
 year = {2011},
 pages = {382--393},
 publisher = {IEEE},
 title = {A quantitative performance analysis model for GPU architectures},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749745},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749745},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749745.pdf?arnumber=5749745},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {We develop a microbenchmark-based performance model for NVIDIA GeForce 200-series GPUs. Our model identifies GPU program bottlenecks and quantitatively analyzes performance, and thus allows programmers and architects to predict the benefits of potential program optimizations and architectural improvements. In particular, we use a microbenchmark-based approach to develop a throughput model for three major components of GPU execution time: the instruction pipeline, shared memory access, and global memory access. Because our model is based on the GPU's native instruction set, we can predict performance with a 5\&#x2013;15\% error. To demonstrate the usefulness of the model, we analyze three representative real-world and already highly-optimized programs: dense matrix multiply, tridiagonal systems solver, and sparse matrix vector multiply. The model provides us detailed quantitative analysis on performance, allowing us to understand the configuration of the fastest dense matrix multiply implementation and to optimize the tridiagonal solver and sparse matrix vector multiply by 60\% and 18\% respectively. Furthermore, our model applied to analysis on these codes allows us to suggest architectural improvements on hardware resource allocation, avoiding bank conflicts, block scheduling, and memory transaction granularity. },
}

@inproceedings{5749744,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Bobba, Jayaram and Lupon, Marc and Hill, Mark D. and Wood, David A.},
 year = {2011},
 pages = {369--380},
 publisher = {IEEE},
 title = {Safe and efficient supervised memory systems},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749744},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749744},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749744.pdf?arnumber=5749744},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Supervised Memory systems use out-of-band metabits to control and monitor accesses to normal data memory for such purposes as transactional memory and memory typestate trackers. Previous proposals demonstrate the value of supervised memory systems, but have typically (1) assumed sequential consistency (while most deployed systems use weaker models), and (2) used ad hoc, informal memory specifications (that can be ambiguous and/or incorrect). This paper seeks to make many previous proposals more practical. This paper builds a foundation for future supervised memory systems which (1) operate with the TSO and \&#x00D7;86 memory models, and (2) are formally specified using two supervised memory models. The simpler TSO<inf>all</inf> model requires all metadata and data accesses to obey TSO, but precludes using store buffers for supervised accesses. The more complex TSO<inf>data</inf> model relaxes some ordering constraints (allowing store buffer use) but makes programmer reasoning more difficult. To get the benefits of both models, we propose Safe Supervision, which asks programmers to avoid using metabits from one location to order accesses to another. Programmers that obey safe supervision can reason with the simpler semantics of TSO<inf>all</inf> while obtaining the higher performance of TSO<inf>data</inf>. Our approach is similar to how data-race-free programs can run on relaxed systems and yet appear sequentially consistent. Finally, we show that TSO<inf>data</inf> can (a) provide significant performance benefit (up to 22\%) over TSO<inf>all</inf> and (b) can be incorporated correctly and with low overhead into the RTL of an industrial multi-core chip design (OpenSPARC T2). },
}

@inproceedings{4798267,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {323--324},
 publisher = {IEEE},
 title = {Industrial perspectives panel (joint with PPoPP)},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798267},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798267},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798267.pdf?arnumber=4798267},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798267.png" border="0"> },
}

@inproceedings{4798266,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Fan, K. and Kudlur, M. and Dasika, G. and Mahlke, S.},
 year = {2009},
 pages = {313--322},
 publisher = {IEEE},
 title = {Bridging the computation gap between programmable processors and hardwired accelerators},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798266},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798266},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798266.pdf?arnumber=4798266},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {ASIC, Acceleration, Accelerator architectures, Application software, Application specific integrated circuits, Computer architecture, Concurrent computing, Energy efficiency, Hardware, High performance computing, Laboratories, OpenRISC 1200 general purpose processor, application specific integrated circuits, application-specific integrated circuits, compiler mapping phase, customized semiprogrammable loop accelerator architecture, general-purpose processors, hardware programmability, hardware reusability, microprocessor chips, programmable accelerator, programmable processors, reduced instruction set computing, },
 abstract = {New media and signal processing applications demand ever higher performance while operating within the tight power constraints of mobile devices. A range of hardware implementations is available to deliver computation with varying degrees of area and power efficiency, from general-purpose processors to application-specific integrated circuits (ASICs). The tradeoff of moving towards more efficient customized solutions such as ASICs is the lack of flexibility in terms of hardware reusability and programmability. In this paper, we propose a customized semi-programmable loop accelerator architecture that exploits the efficiency gains available through high levels of customization, while maintaining sufficient flexibility to execute multiple similar loops. A customized instance of the loop accelerator architecture is generated for a particular loop and then the data and control paths are proactively generalized in an efficient manner to increase flexibility. A compiler mapping phase is then able to map other loops onto the same hardware. The efficiency of the programmable accelerator is compared with non-programmable accelerators and with the OpenRISC 1200 general purpose processor. The programmable accelerator is able to achieve up to 34x better power efficiency and 30x better area efficiency than a simple general purpose processor, while trading off as little as 2x power and area efficiency to the non-programmable accelerator. },
}

@inproceedings{4798265,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Herbert, S. and Marculescu, D.},
 year = {2009},
 pages = {301--312},
 publisher = {IEEE},
 title = {Variation-aware dynamic voltage/frequency scaling},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798265},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798265},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798265.pdf?arnumber=4798265},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Aggregates, DVFS controllers, Dynamic voltage scaling, Energy efficiency, Energy management, Frequency control, Fuses, Hardware, Manufacturing processes, Microprocessors, Throughput, chip-multiprocessors, core-to-core variations, microprocessor chips, multi-threading, power aware computing, variation-aware dynamic voltage/frequency scaling, },
 abstract = {Fine-grained dynamic voltage/frequency scaling (DVFS) is an important tool in managing the balance between power and performance in chip-multiprocessors. Although manufacturing process variations are giving rise to significant core-to-core variations in power and performance, traditional DVFS controllers are unaware of these variations. Exploiting the different power/performance profiles of the cores can significantly improve energy-efficiency. Two hardware DVFS control algorithms are considered and the gains enabled by incorporating variability-awareness are demonstrated on multithreaded commercial workloads. For a design with per-core voltage/frequency islands (VFIs), the mean power per unit throughput for a simple threshold-based controller is reduced by 8.0\% when variability-awareness is added. A complex greedy-search controller sees an even larger reduction of 15.4\%. The variability-aware versions of the two controllers achieve power/throughput reductions of 2.1\% and 9.9\% relative to LinOpt, a recent software variability-aw are DVFS scheme. Designs which apply DVFS at a coarser granularity are also considered, and the variability-aware schemes maintain significant improvement over the -unaware ones. With four cores per VFI, variability-awareness reduces power/throughput by 6.5\% and 9.2\% for the threshold- based and greedy-search controllers, respectively. },
}

@inproceedings{4798264,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Powell, M.D. and Biswas, A. and Emer, J.S. and Mukherjee, S.S. and Sheikh, B.R. and Yardi, S.},
 year = {2009},
 pages = {289--300},
 publisher = {IEEE},
 title = {CAMP: A technique to estimate per-structure power at run-time using a few simple parameters},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798264},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798264},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798264.pdf?arnumber=4798264},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {CAMP, Energy management, Equations, Frequency estimation, Hardware, Knowledge management, Microarchitecture, Microprocessors, Pipelines, Runtime, Statistics, common activity-based model for power, dynamic voltage/frequency scaling, fine-grain clock gating, first-order constraint, linear-regression-based model, microprocessor chips, microprocessor utilization statistics, per-structure power, power aware computing, power consumption, power gating, power management, run-time microprocessor power consumption, },
 abstract = {Microprocessor power has become a first-order constraint at run-time. Designers must employ aggressive power-management techniques at run-time to keep a processor's ballooning power requirements under control. Effective power management benefits from knowledge of run-time microprocessor power consumption in both the core and individual microarchitectural structures, such as caches, queues, and execution units. Increasingly feasible per-structure power-control techniques, such as fine-grain clock gating, power gating, and dynamic voltage/frequency scaling (DVFS), become more effective from run-time estimates of per-structure power. However, run-time computation of per-structure power estimates based on utilization requires daunting numbers of input statistics, which makes per-structure monitoring of run-time power a challenging problem. To address the challenges of estimating per-structure power in hardware, we propose a new technique, called Common Activity-based Model for Power (CAMP), to estimate activity factors and power for microarchitectural structures. Despite using a relatively few input parameters-specifically nine-based on general microprocessor utilization statistics (e.g., IPC and load rate), our linear-regression-based model estimates activity and dynamic power for over 100 structures in an out-of-order x86 pipeline and core power with an average error of 8\%. Because the computations utilize few inputs, CAMP is simple enough to implement in hardware, providing run-time structure and core power estimates for dynamic power management. Because the input statistics are generic in nature and the model remains accurate across incremental microarchitectural refinements, CAMP provides simple intuitive equations relating global microarchitectural statistics to structure activity and power. These equations provide a simple technique that can equate changes in one structure's activity to power variations in other structures across the pipeline. },
}

@inproceedings{4798263,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Yehia, S. and Girbal, S. and Berry, H. and Temam, O.},
 year = {2009},
 pages = {277--288},
 publisher = {IEEE},
 title = {Reconciling specialization and flexibility through compound circuits},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798263},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798263},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798263.pdf?arnumber=4798263},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Aggregates, Circuit synthesis, Costs, Coupling circuits, Embedded system, FPGA, Field programmable gate arrays, Flexible printed circuits, Proportional control, Routing, Scalability, UTDSP benchmarks, accelerators, aggregation method, benchmark testing, compound customization circuit, control-flow patterns, data-flow patterns, embedded PowerPC405 processor, embedded systems, embedded systems, field programmable gate arrays, individual customization circuits, microprocessor chips, parallelizable programs, program sections, },
 abstract = {While parallelism and multi-cores are receiving much attention as a major scalability path, customization is another, orthogonal and complementary, scalability path which can target not easily parallelizable programs or program sections. The key assets of customization are cost and power efficiency. The key limitation of customization is flexibility. However, we argue that there is no perfect balance between efficiency and flexibility, each system vendor may want to strike a different such balance. In this article, we present a method for achieving any desired balance between flexibility and efficiency by automatically combining any set of individual customization circuits into a larger compound circuit. This circuit is significantly more cost efficient than the simple union of all target circuits, and is configurable to behave as any of the target circuits, while avoiding the routing and configuration cost overhead of FPGAs. The more individual circuits are included, the larger the number of applications which can potentially benefit from this compound customization circuit, realizing flexibility at a minimal cost. Moreover, we observe that the compound circuit cost does not increase in proportion to the number of target applications, due to the wide range of common data-flow and control-flow patterns in programs. Currently, the target individual circuits correspond to loops, like most accelerators in embedded systems, but the aggregation method can accommodate circuits of any size. Using the UTDSP benchmarks and accelerators coupled with an embedded PowerPC405 processor, we show that this approach can yield an average performance improvement of 2.97, while the corresponding synthesized aggregate accelerator is 3 time smaller than the sum of individual accelerators for each target benchmark. },
}

@inproceedings{4798262,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {275--276},
 publisher = {IEEE},
 title = {Session 4B Power/performance-efficient architectures and accelerators},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798262},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798262},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798262.pdf?arnumber=4798262},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Acceleration, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798262.png" border="0"> },
}

@inproceedings{4798261,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Madan, N. and Li Zhao and Muralimanohar, N. and Udipi, A. and Balasubramonian, R. and Iyer, R. and Makineni, S. and Newell, D.},
 year = {2009},
 pages = {262--274},
 publisher = {IEEE},
 title = {Optimizing communication and capacity in a 3D stacked reconfigurable cache hierarchy},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798261},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798261},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798261.pdf?arnumber=4798261},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {3D stacked reconfigurable cache hierarchy, Bandwidth, Chip scale packaging, DRAM chip, DRAM chips, Delay, Network topology, Network-on-a-chip, OS-based page coloring, Proposals, Random access memory, SRAM chip, SRAM chips, SRAM/DRAM cache reconfiguration, Space technology, Stacking, Technological innovation, cache and memory hierarchy, cache storage, chip design, horizontal communication, integrated circuit design, multi-core processors, multicore processor, multiprocessing systems, network topology, network-on-chip, network-on-chip, non-uniform cache architecture (NUCA), on-chip networks, page coloring, reconfigurable architectures, tree topology, trees (mathematics), },
 abstract = {Cache hierarchies in future many-core processors are expected to grow in size and contribute a large fraction of overall processor power and performance. In this paper, we postulate a 3D chip design that stacks SRAM and DRAM upon processing cores and employs OS-based page coloring to minimize horizontal communication of cache data. We then propose a heterogeneous reconfigurable cache design that takes advantage of the high density of DRAM and the superior power/delay characteristics of SRAM to efficiently meet the working set demands of each individual core. Finally, we analyze the communication patterns for such a processor and show that a tree topology is an ideal fit that significantly reduces the power and latency requirements of the on-chip network. The above proposals are synergistic: each proposal is made more compelling because of its combination with the other innovations described in this paper. The proposed reconfigurable cache model improves performance by up to 19\% along with 48\% savings in network power. },
}

@inproceedings{4798260,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Awasthi, M. and Sudan, K. and Balasubramonian, R. and Carter, J.},
 year = {2009},
 pages = {250--261},
 publisher = {IEEE},
 title = {Dynamic hardware-assisted software-controlled page placement to manage capacity allocation and sharing within large caches},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798260},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798260},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798260.pdf?arnumber=4798260},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Computer architecture, Delay, Energy management, Hardware, Network-on-a-chip, OS-based page coloring, Technological innovation, Throughput, Tiles, Wires, Yarn, cache capacity allocation, cache storage, capacity allocation, capacity sharing, complex data search mechanism, data/page migration, dynamic hardware-assisted software-controlled page placement, large L2 cache, large L3 cache, last level caches, multi-threading, multithreading, non-uniform cache architectures (NUCA), nonuniform cache architecture, operating systems (computers), page coloring, paged storage, shadow address space, shadow-memory addresses, storage allocation, storage management, },
 abstract = {In future multi-cores, large amounts of delay and power will be spent accessing data in large L2/L3 caches. It has been recently shown that OS-based page coloring allows a non-uniform cache architecture (NUCA) to provide low latencies and not be hindered by complex data search mechanisms. In this work, we extend that concept with mechanisms that dynamically move data within caches. The key innovation is the use of a shadow address space to allow hardware control of data placement in the L2 cache while being largely transparent to the user application and off-chip world. These mechanisms allow the hardware and OS to dynamically manage cache capacity per thread as well as optimize placement of data shared by multiple threads. We show an average IPC improvement of 10-20\% for multi-programmed workloads with capacity allocation policies and an average IPC improvement of 8\% for multi-threaded workloads with policies for shared page placement. },
}

@inproceedings{4798269,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {327--328},
 publisher = {IEEE},
 title = {Session 5A Performance modeling and analysis},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798269},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798269},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798269.pdf?arnumber=4798269},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Performance analysis, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798269.png" border="0"> },
}

@inproceedings{4798268,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Ranganathan, Parthasarathy},
 year = {2009},
 pages = {325--326},
 publisher = {IEEE},
 title = {Industrial perspectives panel},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798268},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798268},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798268.pdf?arnumber=4798268},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Concurrent computing, Delay, Graphics, Hardware, Multicore processing, Parallel programming, Phase change materials, Random access memory, Sun, Supercomputers, },
 abstract = {This year, we have a different format for HPCA's industrial session. We will have an \&#x201C;industrial perspectives panel\&#x201D; with practitioners from industry presenting their perspectives on interesting future technical challenges and opportunities for research. We have panelists from IBM, Microsoft, NVIDIA, and Sun exploring a spectrum of interesting areas from consumer to enterprise markets including \&#x201C;hot\&#x201D; topics around multicores, graphics accelerators, and solid-state memory. },
}

@inproceedings{650548,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {O'Hallaron, D. and Shewchuk, J.R. and Gross, T.},
 year = {1998},
 pages = {80--89},
 publisher = {IEEE},
 title = {Architectural implications of a family of irregular applications },
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650548},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650548},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650548.pdf?arnumber=650548},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Application software, Bandwidth, Communication networks, Computer science, Delay, Earthquakes, Finite element methods, Ice, Sparse matrices, Stress, architectural implications, bisection bandwidth, block latencies, block transfers, burst bandwidth, communication networks, distributed systems, family of irregular applications, parallel architectures, parallel systems, performance evaluation, processing element, scientific computations, sparse matrices, sparse matrices, sparse matrix vector product, sustained bandwidth, },
 abstract = {Irregular applications based on sparse matrices are at the core of many important scientific computations. Since the importance of such applications is likely to increase in the future, high-performance parallel and distributed systems must provide adequate support for such applications. We characterize a family of irregular scientific applications and derive the demands they will place on the communication systems of future parallel systems. Running time of these applications is dominated by repeated sparse matrix vector product (SMVP) operations. Using simple performance models of the SMVP, we investigate requirements for bisection bandwidth, sustained bandwidth on each processing element (PE), burst bandwidth during block transfers, and block latencies for PEs under different assumptions about sustained computational throughput. Our model indicates that block latencies are likely to be the most problematic engineering challenge for future communication networks },
}

@inproceedings{650549,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Arpaci-Dusseau, R.H. and Arpaci-Dusseau, A.C. and Culler, D.E. and Hellerstein, J.M. and Patterson, D.A.},
 year = {1998},
 pages = {90--101},
 publisher = {IEEE},
 title = {The architectural costs of streaming I/O: A comparison of workstations, clusters, and SMPs},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650549},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650549},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650549.pdf?arnumber=650549},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Bandwidth, Computer architecture, Computer science, Costs, I/O bus, Peak to average power ratio, Performance gain, SMPs, Switched-mode power supply, System performance, Thumb, Workstations, architectural costs, cluster workstation, clusters, data locality, local area networks, memory bus, parallel architectures, peak performance, performance evaluation, processor performance, resource usage, shared memory systems, shared-memory system, streaming I/O, workstations, },
 abstract = {We investigate resource usage while performing streaming I/O by contrasting three architectures, a single workstation, a cluster, and an SMP, under various I/O benchmarks. We derive analytical and empirically-based models of resource usage during data transfer, examining the I/O bus, memory bus, network, and processor of each system. By investigating each resource in detail, we assess what comprises a well-balanced system for these workloads. We find that the architectures we study are not well balanced for streaming I/O applications. Across the platforms, the main limitation to attaining peak performance is the CPU, due to lack of data locality. Increasing processor performance (especially with improved block operation performance) will be of great aid for these workloads in the future. For a cluster workstation, the I/O bus is a major system bottleneck, because of the increased load placed on it from network communication. A well-balanced cluster workstation should have copious I/O bus bandwidth, perhaps via multiple I/O busses. The SMP suffers from poor memory-system performance; even when there is true parallelism in the benchmark, contention in the shared-memory system leads to reduced performance. As a result, the clustered workstations provide higher absolute performance for streaming I/O workloads },
}

@inproceedings{650540,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {},
 year = {1998},
 publisher = {IEEE},
 title = {Proceedings 1998 Fourth International Symposium on High-Performance Computer Architecture},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650540},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650540},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650540.pdf?arnumber=650540},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {DSM systems, SMP clusters, application performance, communication mechanisms, compiler, distributed memory systems, multithreading, network interface design, network operating systems, operating system, parallel architectures, performance evaluation, performance evaluation, processor design, register renaming, routing, shared memory systems, shared-memory multiprocessors, },
 abstract = {The following topics were covered: multithreading; routing and communication mechanisms; communication impact on application performance; SMP clusters; shared-memory multiprocessors; speculation and register renaming; network interface design; compiler and operating system issues; enhancements for DSM systems; and processor design and performance evaluation },
}

@inproceedings{650541,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Steffan, J.G. and Mowry, T.C.},
 year = {1998},
 pages = {2--13},
 publisher = {IEEE},
 title = {The potential for using thread-level data speculation to facilitate automatic parallelization},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650541},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650541},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650541.pdf?arnumber=650541},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Application software, Computer science, Face detection, Hip, Integrated circuit technology, Microprocessors, Ores, Parallel processing, Transistors, Yarn, automatic parallelization, buffer speculative state, cache coherence scheme, compilers, microprocessor chips, microprocessors, multiple parallel threads, parallelising compilers, thread-level data speculation, },
 abstract = {As we look to the future, and the prospect of a billion transistors on a chip, it seems inevitable that microprocessors will exploit having multiple parallel threads. To achieve the full potential of these ``single-chip multiprocessors", however, we must find a way to parallelize non-numeric applications. Unfortunately, compilers have had little success in parallelizing non-numeric codes due to their complex access patterns. This paper explores the potential for using thread-level data speculation (TLDS) to overcome this limitation by allowing the compiler to view parallelization solely as a cost/benefit tradeoff rather than something which is likely to violate program correctness. Our experimental results demonstrate that with realistic compiler support, TLDS can offer significant program speedups. We also demonstrate that through modest hardware extensions, a generic single-chip multiprocessor could support TLDS by augmenting its cache coherence scheme to detect dependence violations, and by using the primary data caches to buffer speculative state },
}

@inproceedings{650542,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Tubella, J. and Gonzalez, A.},
 year = {1998},
 pages = {14--23},
 publisher = {IEEE},
 title = {Control speculation in multithreaded processors through dynamic loop detection},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650542},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650542},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650542.pdf?arnumber=650542},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Concrete, Electronic mail, Hardware, History, Microprocessors, Parallel processing, Performance analysis, Program processors, Proposals, Yarn, coarse grain parallelism, control speculation, dynamic loop detection, instruction sequence, instruction sets, multithreaded processors, parallel architectures, },
 abstract = {This paper presents a mechanism to dynamically detect the loops that are executed in a program. This technique detects the beginning and the termination of the iterations and executions of the loops without compiler/user intervention. We propose to apply this dynamic loop detection to the speculation of multiple threads of control dynamically obtained from a sequential program. Based an the highly predictable behavior of the loops, the history of the past executed loops is used to speculate the future instruction sequence. The overall objective is to dynamically obtain coarse grain parallelism (at the thread level) that can be exploited by a multithreaded architecture. We show that for a 4-context multithreaded processor the speculation mechanism provides around 2.6 concurrent threads in average },
}

@inproceedings{650543,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Jenn-Yuan Tsai and Zhenzhen Jiang and Ness, E. and Pen-Chung Yew},
 year = {1998},
 pages = {24--35},
 publisher = {IEEE},
 title = {Performance study of a concurrent multithreaded processor},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650543},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650543},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650543.pdf?arnumber=650543},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Bandwidth, Cache memory, Computer science, Contracts, Hardware, Parallel processing, Pipeline processing, Runtime, Scalability, Yarn, concurrent multithreaded architectural model, concurrent multithreaded processor, data dependence enforcement, discrete event simulation, instruction sets, instruction-level parallelism, memory buffer, optimizing compilation, parallel architectures, performance evaluation, performance study, run-time data dependence checking, run-time hardware support, superscalars, superthreading, thread pipelining execution model, trace-driven simulator, },
 abstract = {The performance of a concurrent multithreaded architectural model, called superthreading, is studied in this paper. It tries to integrate optimizing compilation techniques and run-time hardware support to exploit both thread-level and instruction-level parallelism, as opposed to exploiting only instruction-level parallelism in existing superscalars. The superthreaded architecture uses a thread pipelining execution model to enhance the overlapping between, threads, and to facilitate data dependence enforcement between threads through compiler-directed, hardware-supported, thread-level control speculation and run-time data dependence checking. We also evaluate the performance of the superthreaded processor through a detailed trace-driven simulator. Our results show that the superthreaded execution model can obtain good performance by exploiting both thread-level and instruction-level parallelism in programs. We also study the design parameters of its main system components, such as the size of the memory buffer, the bandwidth requirement of the communication links between thread processing units, and the bandwidth requirement of the shared data cache },
}

@inproceedings{650544,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Chong, F.T. and Barua, R. and Dahlgren, F. and Kubiatowicz, J.D. and Agarwal, A.},
 year = {1998},
 pages = {37--46},
 publisher = {IEEE},
 title = {The sensitivity of communication mechanisms to bandwidth and latency},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650544},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650544},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650544.pdf?arnumber=650544},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Bandwidth, Clocks, Contracts, DMA, Delay, MIT Alewife multiprocessor, Magnetic heads, Message passing, Performance gain, Prefetching, bandwidth, bisection bandwidth, bulk transfer, communication mechanisms sensitivity, interrupts, interrupts, latency, little data-reuse, message passing, message passing, network latency, performance evaluation, prefetching, processor clock speeds, shared memory systems, },
 abstract = {The goal of this paper is to gain insight into the relative performance of communication mechanisms as bisection bandwidth and network latency vary. We compare shared memory with and without prefetching, message passing with interrupts and with polling, and bulk transfer via DMA. We present two sets of experiments involving four irregular applications on the MIT Alewife multiprocessor. First, we introduce I/O cross-traffic to vary bisection bandwidth. Second, we change processor clock speeds to vary relative network latency. We establish a framework from which to understand a range of results. On Alewife, shared memory provides good performance, even on producer-consumer applications with little data-reuse. On machines with lower bisection bandwidth and higher network latency, however, message-passing mechanisms become important. In particular, the high communication volume of shared memory threatens to become difficult to support on future machines without expensive, high-dimensional networks. Furthermore, the round-trip nature of shared memory may not be able to tolerate the latencies of future networks },
}

@inproceedings{650545,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Katevenis, M. and Serpanos, D. and Spyridakis, E.},
 year = {1998},
 pages = {47--56},
 publisher = {IEEE},
 title = {Credit-flow-controlled ATM for MP interconnection: The ATLAS I single-chip ATM switch},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650545},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650545},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650545.pdf?arnumber=650545},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {ATLAS I single-chip ATM switch, Asynchronous transfer mode, Computer architecture, Computer networks, Delay, LAN interconnection, MP interconnection, Multiprocessor interconnection networks, Protocols, Routing, Switches, Throughput, Workstations, asynchronous transfer mode, backpressure, bursty traffic, clock-cycle granularity, credit flow control, credit-flow-controlled ATM, fixed-size cells, fixed-size flits, high-performance computing architecture, hot-spot configurations, multiprocessing, multiprocessor interconnection networks, networks of workstations, protocol, protocols, quantum flow control, simulation, switch models, wormhole routing interconnection networks, },
 abstract = {Multiprocessing (MP) on networks of workstations (NOW) is a high-performance computing architecture of growing importance. In traditional MP's, wormhole routing interconnection networks use fixed-size flits and backpressure. In NOW's, ATM-one of the major contending interconnection technologies-uses fixed-size cells, while backpressure can be added to it. We argue that ATM with backpressure has interesting similarities with wormhole routing. We are implementing ATLAS I, a single-chip gigabit ATM switch, which includes credit flow control (backpressure), according to a protocol resembling Quantum Flow Control (QFC). We show by simulation that this protocol performs better than the traditional multi-lane wormhole protocol: high throughput and low latency are provided with less buffer space. Also, ATLAS I demonstrates little sensitivity to bursty traffic, and, unlike wormhole, it is fair in terms of latency in hot-spot configurations. We use detailed switch models, operating at clock-cycle granularity },
}

@inproceedings{650546,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Lopez, P. and Martinez, J.M. and Duato, J.},
 year = {1998},
 pages = {57--66},
 publisher = {IEEE},
 title = {A very efficient distributed deadlock detection mechanism for wormhole networks},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650546},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650546},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650546.pdf?arnumber=650546},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Bandwidth, Degradation, Frequency measurement, Liver, Multiprocessor interconnection networks, Packet switching, Routing, Switching circuits, System recovery, deadlock avoidance strategies, deadlock recovery strategies, deadlock recovery techniques, distributed deadlock detection mechanism, false deadlock detection, local information, message destination distribution, message length, multiprocessor interconnection networks, performance degradation, performance evaluation, routing algorithms, system recovery, wormhole networks, wormhole switching, },
 abstract = {Networks using wormhole switching have traditionally relied upon deadlock avoidance strategies for the design of routing algorithms. More recently, deadlock recovery strategies have begun to gain acceptance. Progressive deadlock recovery techniques are very attractive because they allocate a few dedicated resources to quickly deliver deadlocked messages, instead of killing them. However, the distributed deadlock detection techniques proposed up to now detect many false deadlocks, especially when the network is heavily loaded and messages have different lengths. As a consequence, messages detected as deadlocked may saturate the bandwidth offered by recovery resources, thus degrading performance considerably. In this paper we propose an improved distributed deadlock detection mechanism that uses only local information, detects all the deadlocks, considerably reduces the probability of false deadlock detection and is not strongly affected by variations in message length and message destination distribution },
}

@inproceedings{650547,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Langendoen, K. and Hofman, R. and Bal, H.},
 year = {1998},
 pages = {68--79},
 publisher = {IEEE},
 title = {Challenging applications on fast networks},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650547},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650547},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650547.pdf?arnumber=650547},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {1.28 Gbit/s Myrinet, 100 Mbit/s Fast Ethernet, 155 Mbit/s ATM, Application software, Costs, Ethernet networks, LAN interconnection, Local area networks, Next generation networking, Parallel processing, Software design, Software performance, Workstations, clusters of workstations, communication software, local area network, local area networks, modern interconnects, parallel computing, parallel processing, performance, performance evaluation, },
 abstract = {Parallel computing on clusters of workstations is attractive because of the low costs in comparison to MPPs, but the speed of the local area network limits the class of applications that can be run efficiently. Fortunately, faster network technology is becoming available for the next generation of workstation clusters. This paper studies the effect of running challenging applications that communicate heavily on three types of modern interconnects: 100 Mbit/s Fast Ethernet, 155 Mbit/s ATM, and 1.28 Gbit/s Myrinet. Experimental results show that even challenging communication-intensive applications can achieve acceptable performance on workstation clusters, but only if the communication software has been designed and tuned for high performance },
}

@inproceedings{1385938,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Hasan, J. and Jalote, A. and Vijaykumar, T.N. and Brodley, C.E.},
 year = {2005},
 pages = { 166-- 177},
 publisher = {IEEE},
 title = {Heat stroke: power-density-based denial of service in SMT},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.16},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385938},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385938.pdf?arnumber=1385938},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { SMT pipeline,  heat stroke,  multi-threading,  pipeline processing,  pipeline resources,  power-density-based denial of service attack,  resource allocation,  shared resource, Clocks, Computer crime, Degradation, Microprocessors, Physics computing, Pipelines, Surface-mount technology, Throughput, Voltage, Yarn, },
 abstract = {In the past, there have been several denial of service (DOS) attacks which exhaust some shared resource (e.g., physical memory, process table, file descriptors, TCP connections) of the targeted machine. Though these attacks have been addressed, it is important to continue to identify and address new attacks because DOS is one of most prominent methods used to cause significant financial loss. A recent paper shows how to prevent attacks that exploit the sharing of pipeline resources (e.g., shared trace cache) in SMT to degrade the performance of normal threads. In this paper, we show that power density can be exploited in SMT to launch a novel DOS attack, called heat stroke. Heat stroke repeatedly accesses a shared resource to create a hot spot at the resource. Current solutions to hot spots inevitably involve slowing down the pipeline to let the hot spot cool down. Consequently, heat stroke slows down the entire SMT pipeline and severely degrades normal threads. We present a solution to heat stroke by identifying the thread that causes the hot spot and selectively slowing down the malicious thread while minimally affecting normal threads. },
}

@inproceedings{1385939,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Qiang Wu and Juang, P. and Martonosi, M. and Clark, D.W.},
 year = {2005},
 pages = { 178-- 189},
 publisher = {IEEE},
 title = {Voltage and frequency control with adaptive reaction time in multiple-clock-domain processors},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.43},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385939},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385939.pdf?arnumber=1385939},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { DVFS control,  MediaBench benchmark,  SPEC2000 benchmark,  adaptive reaction time,  computer architecture,  computer power supplies,  cycle-accurate simulation,  decision process,  energy-efficient computing,  fixed time interval,  formal stability analysis,  frequency control,  frequency control,  intratask online DVFS scheme,  microprocessor chips,  multiple-clock-domain processors,  performance evaluation,  stability margin,  voltage control,  voltage control, Adaptive control, Clocks, Computer science, Dynamic voltage scaling, Energy efficiency, Frequency control, Hardware, Programmable control, Stability analysis, Statistics, },
 abstract = {Dynamic voltage and frequency scaling (DVFS) is a widely used method for energy-efficient computing. In this paper, we present a new intra-task online DVFS scheme for multiple clock domain (MCD) processors. Most existing online DVFS schemes for MCD processors use a fixed time interval between possible voltage/frequency changes. The downside to this approach is that the interval boundaries are predetermined and independent of workload changes. Thus, they can be late in responding to large, severe activity swings. In this work, we propose an alternative online DVFS scheme in which the reaction time is self-tuned and adaptive to application and work-load changes. In addition to designing such a scheme, we model the proposed DVFS control and use the derived model in a formal stability analysis. The obtained analytical insight is then used to guide and improve the design in terms of stability margin and control effectiveness. We evaluate our DVFS scheme through cycle-accurate simulation over a wide set of MediaBench and SPEC2000 benchmarks. Compared to the best-known prior fixed-interval DVFS schemes for MCD processors, the proposed DVFS scheme has a simpler decision process, which leads to smaller and cheaper hardware. Our scheme has achieved significant energy savings over all studied benchmarks (19\% energy savings with 3\% performance degradation on average, which is close to the best results from existing fixed-interval DVFS schemes). For a group of applications with fast workload variations, our scheme outperforms existing fixed-interval DVFS schemes significantly due to its adaptive nature. Overall, we feel the proposed adaptive online DVFS scheme is an effective and promising alternative to existing fixed-interval DVFS schemes. Designers may choose the new scheme for processors with limited hardware budget, or if the anticipated work-load behavior is variable. In addition, the modeling and analysis techniques in this work serve as examples of using stability analysis in other aspects of high-performance CPU design and control. },
}

@inproceedings{1385934,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Xuning Chen and Li-Shiuan Peh and Gu-Yeon Wei and Yue-Kai Huang and Prucnal, P.},
 year = {2005},
 pages = { 120-- 131},
 publisher = {IEEE},
 title = {Exploring the design space of power-aware opto-electronic networked systems},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.15},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385934},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385934.pdf?arnumber=1385934},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { high speed optoelectronic links,  integrated optoelectronics,  interconnection network,  link bandwidth,  link circuitry,  link components,  microprocessor interconnection,  multiprocessor interconnection networks,  network simulator,  network traffic,  optical link power dissipation,  optical links,  power consumption,  power consumption,  power control,  power-aware optical links,  power-aware opto-electronic networked systems, Bandwidth, Circuit simulation, Energy consumption, Integrated circuit interconnections, Microprocessors, Multiprocessor interconnection networks, Optical fiber communication, Power dissipation, Power system interconnection, Space exploration, },
 abstract = {As microprocessors become increasingly interconnected, the power consumed by the interconnection network can no longer be ignored. Moreover, with demand for link bandwidth increasing, optical links are replacing electrical links in inter-chassis and inter-board environments. As a result, the power dissipation of optical links is becoming as critical as their speed. In this paper, we first explore options for building high speed optoelectronic links and discuss the power characteristics of different link components. Then, we propose circuit and network mechanisms that can realize power-aware optical links -links whose power consumption can be tuned dynamically in response to changes in network traffic. Finally, we incorporate power control policies along with the power characterization of link circuitry into a detailed network simulator to evaluate the performance cost and power savings of building power aware optoelectronic networked systems. Simulation results show that more than 75\% savings in power consumption can be achieved with the proposed power aware optoelectronic network. },
}

@inproceedings{1385935,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Jung Ho Ahn and Erez, M. and Dally, W.J.},
 year = {2005},
 pages = { 132-- 142},
 publisher = {IEEE},
 title = {Scatter-add in data parallel architectures},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.30},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385935},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385935.pdf?arnumber=1385935},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { SIMD style memory system,  computer systems,  data parallel architectures,  data-parallel atomic update computations,  memory addresses,  multimedia application,  multiprocessor SIMD data-parallel system,  parallel architectures,  parallel programming,  parallel programming languages,  referenced memory location,  scatter-add microarchitecture,  scientific application,  single-processor SIMD data-parallel system,  storage allocation,  stream architecture,  stream style memory system,  vector style memory system, Application software, Computer architecture, Concurrent computing, Histograms, Parallel architectures, Parallel processing, Parallel programming, Quantum computing, Scattering, Scientific computing, },
 abstract = {Many important applications exhibit large amounts of data parallelism, and modern computer systems are designed to take advantage of it. While much of the computation in the multimedia and scientific application domains is data parallel, certain operations require costly serialization that increase the run time. Examples include superposition type updates in scientific computing and histogram computations in media processing. We introduce scatter-add, which is the data-parallel form of the well-known scalar fetch-and-op, specifically tuned for SIMD/vector/stream style memory systems. The scatter-add mechanism scatters a set of data values to a set of memory addresses and adds each data value to each referenced memory location instead of overwriting it. This novel architecture extension allows us to efficiently support data-parallel atomic update computations found in parallel programming languages such as HPF, and applies both to single-processor and multiprocessor SIMD data-parallel systems. We detail the microarchitecture of a scatter-add implementation on a stream architecture, which requires less than 2\% increase in die area yet shows performance speedups ranging from 1.45 to over 11 on a set of applications that require a scatter-add computation. },
}

@inproceedings{1385936,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Jones, T.M. and O'Boyle, M.F.P. and Abella, J. and Gonzalez, A.},
 year = {2005},
 pages = { 144-- 153},
 publisher = {IEEE},
 title = {Software directed issue queue power reduction},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.32},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385936},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385936.pdf?arnumber=1385936},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { 2.2 percent,  30 percent,  31 percent,  45 percent,  47 percent,  compiler analysis,  cooling systems,  dynamic power,  parallel architectures,  program compilers,  queueing theory,  software assisted approach,  software directed issue queue power reduction,  static power,  superscalar processor, Computer architecture, Cooling, Delay, Hardware, Informatics, Logic, Performance loss, Power dissipation, Registers, Turning, },
 abstract = {The issue logic of a superscalar processor dissipates a large amount of static and dynamic power. Furthermore, its power density makes it a hot-spot requiring expensive cooling systems and additional packaging. In this paper we present a novel software assisted approach to power reduction where the processor dynamically resizes the issue queue based on compiler analysis. The compiler passes information to the processor about the number of entries needed which limits the number of instructions dispatched and resident in the queue. This saves power without adversely affecting performance. Compared with recently proposed hardware techniques, our approach is faster, simpler and saves more power. Using a simplistic scheme we achieve 47\% dynamic and 31\% static power savings in the issue queue with only a 2.2\% performance loss. We then show that the performance loss can be reduced to less than 1.3\% with 45\% dynamic and 30\% static power savings, outperforming all current approaches. },
}

@inproceedings{1385937,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Yan Meng and Sherwood, T. and Kastner, R.},
 year = {2005},
 pages = { 154-- 165},
 publisher = {IEEE},
 title = {On the limits of leakage power reduction in caches},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.23},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385937},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385937.pdf?arnumber=1385937},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { address trace,  architecture technologies,  cache storage,  circuit technologies,  data cache,  electrical faults,  instruction cache,  leakage currents,  leakage power dissipation,  leakage power reduction,  on-chip transistors,  power consumption,  total leakage power, Circuits, Cooling, Dynamic voltage scaling, Energy consumption, Energy dissipation, Packaging, Power dissipation, Prefetching, Process design, Threshold voltage, },
 abstract = {If current technology scaling trends hold, leakage power dissipation soon becomes the dominant source of power consumption. Caches, due to the fact that they account for the largest fraction of on-chip transistors in most modern processors, are a primary candidate for attacking the leakage problem. While there has been a flurry of research in this area over the last several years, a major question remains unanswered. What is the total potential of existing architectural and circuit techniques to address this important design concern? In this paper, we explore the limits in which existing circuit and architecture technologies may address this growing problem. We find that by using perfect knowledge of the address trace to carefully apply sleep and drowsy modes, the total leakage power from the instruction cache may be reduced to mere 3.6\% of the unoptimized case, and the total from the data cache reduced to only 0.9\%. We also present a complete parameterized model to determine the optimal leakage savings while the implementation technology changes over time. We further suggest how such limits might be approached using a form of prefetching for low power. },
}

@inproceedings{1385930,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Yingmin Li and Skadron, K. and Brooks, D. and Zhigang Hu},
 year = {2005},
 pages = { 71-- 82},
 publisher = {IEEE},
 title = {Performance, energy, and thermal considerations for SMT and CMP architectures},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.25},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385930},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385930.pdf?arnumber=1385930},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { CMP,  CPU-bound benchmarks,  HotSpot,  POWER4-like microarchitecture,  POWER5-like microarchitecture,  PowerTimer,  SMT,  Turandot,  cache storage,  chip multiprocessing,  heat up machanism,  leakage power,  localized heating,  memory-bound benchmarks,  microprocessor chips,  multi-threading,  multiprocessing systems,  peak operating temperatures,  performance evaluation,  simultaneous multithreading,  thermal management (packaging),  thermal management overheads, Energy efficiency, Heating, Multithreading, Space exploration, Subthreshold current, Surface-mount technology, Temperature, Thermal management, Throughput, Voltage control, },
 abstract = {Simultaneous multithreading (SMT) and chip multiprocessing (CMP) both allow a chip to achieve greater throughput, but their relative energy-efficiency and thermal properties are still poorly understood. This paper uses Turandot, PowerTimer, and HotSpot to explore this design space for a POWER4/POWER5-like core. For an equal-area comparison with this style of core, we find CMP to be superior in terms of performance and energy-efficiency for CPU-bound benchmarks, but SMT to be superior for memory-bound benchmarks due to a larger L2 cache. Although both exhibit similar peak operating temperatures and thermal management overheads, the mechanism by which SMT and CMP heat up are quite different. More specifically, SMT heating is primarily caused by localized heating in certain key structures, CMP heating is mainly caused by the global impact of increased energy output. Because of this difference in heat up mechanism, we found that the best thermal management technique is also different for SMT and CMP Indeed, non-DVS localized thermal-management can outperform DVS for SMT. Finally, we show that CMP and SMT scales differently as the contribution of leakage power grows, with CMP suffering from higher leakage due to the second core's higher temperature and the exponential temperature-dependence of subthreshold leakage. },
}

@inproceedings{1385931,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Venkatesan, R. and Al-Zawawi, A.S. and Rotenberg, E.},
 year = {2005},
 pages = { 83-- 94},
 publisher = {IEEE},
 title = {Tapping ZettaRAM&trade; for low-power memory systems},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.35},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385931},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385931.pdf?arnumber=1385931},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { DRAM,  DRAM chips,  ZettaCore,  ZettaRAM,  cache storage,  capacitors,  charge-storage molecules,  delayed writebacks,  discrete threshold voltage,  dual-speed writes,  eager writebacks,  extended molecule latency,  hybrid write policy,  low-power electronics,  low-power memory systems,  memory architectural management,  memory architecture,  memory controller,  memory technology,  molecular capacitor,  molecular electronics,  performance limiter,  peripheral circuitry,  row buffer misses,  scheduling flexibility,  storage management, Capacitors, Circuits, Delay, Energy management, Manufacturing, Memory management, Potential energy, Random access memory, Technological innovation, Threshold voltage, },
 abstract = {ZettaRAM\&trade; is a new memory technology under development by ZettaCore\&trade; as a potential replacement for conventional DRAM. The key innovation is replacing the conventional capacitor in each DRAM cell with "charge-storage" molecules - a molecular capacitor. We look beyond ZettaRAM's manufacturing benefits, and approach it from an architectural viewpoint to discover benefits within the domain of architectural metrics. The molecular capacitor is unusual because the amount of charge deposited (critical for reliable sensing) is independent of write voltage, i.e., there is a discrete threshold voltage above/below which the device is fully charged/discharged. Decoupling charge from voltage enables manipulation via arbitrarily small bitline swings, saving energy. However, while charge is voltage-independent, speed is voltage-dependent. Operating too close to the threshold causes molecules to overtake peripheral circuitry as the overall performance limiter. Nonetheless, ZettaRAM offers a speed/energy trade-off whereas DRAM is inflexible, introducing new dimensions for architectural management of memory. We apply architectural insights to tap the full extent of ZettaRAM's power savings without compromising performance. Several factors converge nicely to direct focus on L2 writebacks: (i) they account for 80\% of row buffer misses in the main memory, thus most of the energy savings potential, and (ii) they do not directly stall the processor and thereby offer scheduling flexibility for tolerating extended molecule latency. Accordingly, slow writes (low energy) are applied to non-critical writebacks and fast writes (high energy) to critical fetches. The hybrid write policy is combined with two options for tolerating delayed writebacks: large buffers with access reordering or L2-cache eager writebacks. Eager writebacks are remarkably synergistic with ZettaRAM: initiating writebacks early in the L2 cache compensates for delaying them at the memory controller. Dual-speed writes coupled with eager writebacks yields energy savings of 34\% (out of 41\% with uniformly slow writes), with less than 1\% performance degradation. },
}

@inproceedings{1385932,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Willmann, P. and Hyong-youb Kim and Rixner, S. and Pai, V.S.},
 year = {2005},
 pages = { 96-- 107},
 publisher = {IEEE},
 title = {An efficient programmable 10 gigabit Ethernet network interface card},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.6},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385932},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385932.pdf?arnumber=1385932},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { 10 Gbit/s,  166 MHz,  500 MHz,  Ethernet network interface card,  SRAM chips,  atomic read-modify-write instructions,  distributed task-queue mechanism,  external SDRAM,  frame data,  frame metadata,  frame ordering,  hardware mechanisms,  high frame rate processing,  high-bandwidth access,  high-capacity memory,  local area networks,  low-frequency cores,  low-latency access,  network interfaces,  on-chip SRAM,  partitioned memory organization,  programmable network interface,  software mechanisms,  storage management, Bandwidth, Communication system control, Delay, Ethernet networks, Hardware, IP networks, Network interfaces, Network servers, Parallel processing, Software maintenance, },
 abstract = {This paper explores the hardware and software mechanisms necessary for an efficient programmable 10 Gigabit Ethernet network interface card. Network interface processing requires support for the following characteristics: a large volume of frame data, frequently accessed frame metadata, and high frame rate processing. This paper proposes three mechanisms to improve programmable network interface efficiency. First, a partitioned memory organization enables low-latency access to control data and high-bandwidth access to frame contents from a high-capacity memory. Second, a distributed task-queue mechanism enables parallelization of frame processing across many low-frequency cores, while using software to maintain total frame ordering. Finally, the addition of two new atomic read-modify-write instructions reduces frame ordering overheads by 50\%. Combining these hardware and software mechanisms enables a network interface card to saturate a full-duplex 10 Gb/s Ethernet link by utilizing 6 processor cores and 4 banks of on-chip SRAM operating at 166 MHz, along with external 500 MHz GDDR SDRAM. },
}

@inproceedings{1385933,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Duato, J. and Johnson, I. and Flich, J. and Naven, F. and Garcia, P. and Nachiondo, T.},
 year = {2005},
 pages = { 108-- 119},
 publisher = {IEEE},
 title = {A new scalable and cost-effective congestion management strategy for lossless multistage interconnection networks},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.1},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385933},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385933.pdf?arnumber=1385933},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { HOL blocking,  computer network management,  congestion management,  congestion trees,  lossless multistage interconnection networks,  multistage interconnection networks,  network queue,  queueing theory,  telecommunication congestion control,  trees (mathematics), Communication switching, Costs, Degradation, Delay, Dynamic voltage scaling, Energy consumption, Multiprocessor interconnection networks, Switches, Telecommunication traffic, Traffic control, },
 abstract = {In this paper, we propose a new congestion management strategy for lossless multistage interconnection networks that scales as network size and/or link bandwidth increase. Instead of eliminating congestion, our strategy avoids performance degradation beyond the saturation point by eliminating the HOL blocking produced by congestion trees. This is achieved in a scalable manner by using separate queues for congested flows. These are dynamically allocated only when congestion arises, and deallocated when congestion subsides. Performance evaluation results show that our strategy responds to congestion immediately and completely eliminates the performance degradation produced by HOL blocking while using only a small number of additional queues. },
}

@inproceedings{569673,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Jacobson, Q. and Bennett, S. and Sharma, N. and Smith, J.E.},
 year = {1997},
 pages = {218--229},
 publisher = {IEEE},
 title = {Control flow speculation in multiscalar processors},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569673},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569673},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569673.pdf?arnumber=569673},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Automatic control, Computer architecture, Electric variables measurement, Hardware, Jacobian matrices, Parallel processing, Program processors, Programming profession, VLIW, Yarn, compiler, control flow speculation, global sequencer, instruction level parallelism, instruction sets, multiscalar processors, parallel architectures, performance characteristics, performance evaluation, prediction automata, program's control flow graph, sequential program, single sequential program, target buffers, },
 abstract = {The multiscalar architecture executes a single sequential program following multiple flows of control. In the multiscalar hardware, a global sequencer with help from the compiler takes large steps through the program's control flow graph (CFG) speculatively, starting a new thread of control (task) at each step. This is inter-task control flow speculation. Within a task, traditional control flow speculation is used to extract instruction level parallelism. This is intra-task control flow speculation. This paper focuses on mechanisms to implement inter-task control flow speculation (task prediction) in a multi-scalar implementation. This form of speculation has fundamental differences from traditional branch prediction. We look in detail at the issues of prediction automata, history generation and target buffers. We present implementations in each of these areas that offer good accuracy, size and performance characteristics },
}

@inproceedings{569677,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Espasa, R. and Valero, M.},
 year = {1997},
 pages = {237--248},
 publisher = {IEEE},
 title = {Multithreaded vector architectures},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569677},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569677},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569677.pdf?arnumber=569677},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Computer aided instruction, Computer architecture, Costs, Delay, Multithreading, Parallel processing, Perfect Club, Registers, Resource management, Specfp92 programs, Throughput, Yarn, discrete event simulation, main memory latency, multithreaded vector architectures, parallel architectures, performance advantage, performance evaluation, processor throughput, resource utilization, trace driven approach, vector processor, vector processor systems, vector register file, },
 abstract = {The purpose of this paper is to show that multi-threading techniques can be applied to a vector processor to greatly increase processor throughput and maximize resource utilization. Using a trace driven approach, we simulate a selection of the Perfect Club and Specfp92 programs and compare their execution time on a conventional vector architecture with a single memory port and on a multithreaded vector architecture. We devote an important part of this paper to study the interaction between multi-threading and main memory latency. This paper focuses on maximizing the usage of the memory port, the most expensive resource is typical vector computers. A study of the cost associated with the duplication of the vector register file is also carried out. Overall, multithreading provides for this architecture a performance advantage of more than a factor of 1.4 for realistic memory latencies, and can drive the utilization of the single memory port as high as 95\% },
}

@inproceedings{569675,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Janik, K.J. and Lu, S.-L. and Miller, M.F.},
 year = {1997},
 pages = {230--236},
 publisher = {IEEE},
 title = {Advances of the counterflow pipeline microarchitecture},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569675},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569675},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569675.pdf?arnumber=569675},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Asynchronous circuits, Clocks, Hardware, Helium, Microarchitecture, Pipelines, Processor scheduling, Registers, Throughput, VHDL model, VLIW, asynchronous circuits, clocking, counterflow pipeline microarchitecture, data locality, design tradeoffs, high level C++ simulator, instruction sets, parallel architectures, pipeline processing, scalable architecture, throughput, },
 abstract = {The counterflow pipeline concept was originated by R.F. Sproull et al. (1994) to demonstrate the concept of asynchronous circuits. This architecture provides better throughput via clocking and data locality within the pipeline. We have taken these ideas and reformulated them into a scalable architecture that has the same locality for clocking and data, but adds aggressive speculation, fewer pipeline stalls, and a much faster startup. A high level C++ simulator has been built to explain the design tradeoffs. A VHDL model of an implementation of CFPP has been designed to validate the concept },
}

@inproceedings{995715,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Martin, M.M.K. and Sorin, D.J. and Hill, M.D. and Wood, D.A.},
 year = {2002},
 pages = { 251-- 262},
 publisher = {IEEE},
 title = {Bandwidth adaptive snooping},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995715},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995715},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995715.pdf?arnumber=995715},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Bandwidth Adaptive Snooping Hybrid,  cache coherence protocols,  cache storage,  directory protocol,  hybrid protocol,  interconnection network utilization,  microbenchmark,  shared memory systems,  transport protocols,  varied system configurations,  workload behaviors, Bandwidth, Broadcasting, Costs, Delay, Hardware, Logic, Multiprocessor interconnection networks, Protocols, Robustness, Unicast, },
 abstract = {This paper advocates that cache coherence protocols use a bandwidth adaptive approach to adjust to varied system configurations (e.g., number of processors) and workload behaviors. We propose Bandwidth Adaptive Snooping Hybrid (BASH), a hybrid protocol that ranges from behaving like snooping (by broadcasting requests) when excess bandwidth is available to behaving like a directory protocol (by unicasting requests) when bandwidth is limited. BASH adapts dynamically by probabilistically deciding to broadcast or unicast on a per request basis using a local estimate of recent interconnection network utilization. Simulations of a microbenchmark and commercial and scientific workloads show that BASH robustly performs as well or better than the best of snooping and directory protocols as available bandwidth is varied. By mixing broadcasts and unicasts, BASH outperforms both snooping and directory protocols in the mid-range where a static choice of either is inefficient. },
}

@inproceedings{744334,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Carter, J. and Hsieh, W. and Stoller, L. and Swanson, M. and Lixin Zhang and Brunvand, E. and Davis, A. and Chen-Chi Kuo and Kuramkote, R. and Parker, M. and Schaelicke, L. and Tateyama, T.},
 year = {1999},
 pages = {70--79},
 publisher = {IEEE},
 title = {Impulse: building a smarter memory controller},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744334},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744334},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744334.pdf?arnumber=744334},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Bandwidth, Cities and towns, Computer science, DRAM access latency hiding, Databases, Delay, Electronic switching systems, Impulse memory system architecture, Microprocessors, NAS conjugate gradient benchmark, Prefetching, Random access memory, Sparse matrices, application-specific optimization, bus design, cache design, cache storage, configurable physical address remapping, conjugate gradient methods, data access, data caching, database management systems, database programs, memory architecture, memory controller, memory-bound program performance, multimedia computing, multimedia programs, performance, prefetching, processor design, scientific applications, },
 abstract = {Impulse is a new memory system architecture that adds two important features to a traditional memory controller. First, Impulse supports application-specific optimizations through configurable physical address remapping. By remapping physical addresses, applications control how their data is accessed and cached, improving their cache and bus utilization. Second, Impulse supports prefetching at the memory controller, which can hide much of the latency of DRAM accesses. In this paper we describe the design of the Impulse architecture, and show how an Impulse memory system can be used to improve the performance of memory-bound programs. For the NAS conjugate gradient benchmark, Impulse improves performance by 67\%. Because it requires no modification to processor, cache, or bus designs, Impulse can be adopted in conventional systems. In addition to scientific applications, we expect that Impulse will benefit regularly strided memory-bound applications of commercial importance, such as database and multimedia programs },
}

@inproceedings{744337,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Hong, S.I. and McKee, S.A. and Salinas, M.H. and Klenke, R.H. and Aylor, J.H. and Wulf, W.A.},
 year = {1999},
 pages = {80--89},
 publisher = {IEEE},
 title = {Access order and effective bandwidth for streams on a Direct Rambus memory},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744337},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744337},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744337.pdf?arnumber=744337},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Bandwidth, Cities and towns, Clocks, Computer science, Content addressable storage, Counting circuits, DRAM transactions, Decoding, Direct RDRAM device, Direct Rambus memory, Performance analysis, Random access memory, Timing, access order, access ordering scheme, effective bandwidth, inner loops, memory architecture, memory bottleneck, memory controllers, memory devices, memory speeds, performance analysis, performance evaluation, processor speeds, random cacheline accesses, random-access storage, streaming computations, streaming hardware, unit-stride stream access, },
 abstract = {Processor speeds are increasing rapidly and memory speeds are not keeping up. Streaming computations (such as multimedia or scientific applications) are among those whose performance is most limited by the memory bottleneck. Rambus hopes to bridge the processor/memory performance gap with a recently introduced DRAM that can deliver up to 1.6 Gbytes/sec. We analyze the performance of these interesting new memory devices on the inner loops of streaming computations, both for traditional memory controllers that treat all DRAM transactions as random cacheline accesses, and for controllers augmented with streaming hardware. For our benchmarks, we find that accessing unit-stride streams in cacheline bursts in the natural order of the computation exploits from 44-76\% of the peak bandwidth of a memory system composed of a single Direct RDRAM device, and that accessing streams via a streaming mechanism with a simple access ordering scheme can improve performance by factors of 1.18 to 2.25 },
}

@inproceedings{744331,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Hily, S. and Seznec, A.},
 year = {1999},
 pages = {64--67},
 publisher = {IEEE},
 title = {Out-of-order execution may not be cost-effective on processors featuring simultaneous multithreading},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744331},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744331},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744331.pdf?arnumber=744331},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Application software, Hardware, Microprocessors, Multithreading, Operating systems, Out of order, Pipelines, Surface-mount technology, Throughput, Yarn, desktop applications, high performance, in-order execution, multi-threading, multiprocessing systems, operating systems, out-of-order execution, performance throughput, pipeline processing, simultaneous multithreading, speculative execution, superscalar pipelines, superscalar processors, time-shared multiprocess environments, },
 abstract = {To achieve high performance on a single process, superscalar processors now rely on very complex out-of-order execution. Using more and more speculative execution (e.g. value prediction) will be needed for further improvements. On the other hand, most operating systems now offer time-shared multiprocess environments. For the moment most of the time is spent in a single thread, but this should change, as the computer will perform more and more independent tasks. Moreover, desktop applications tend to be multithreaded. A lot of users should then be more concerned with the performance throughput on the workload than with the performance of the processor on a single process. Simultaneous multithreading (SMT) is a promising approach to deliver high throughput from superscalar pipelines. In this paper, we show that when executing 4 threads on an SMT processor, out-of-order execution induces small performance benefits over in-order execution. Then, for application domains where performance throughput is more important than ultimate performance on a single application, SMT combined with in-order execution may be a more cost-effective alternative than ultimate aggressive out-of-order superscalar processors or out-of-order execution SMT },
}

@inproceedings{569611,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Pai, V.S. and Ranganathan, P. and Adve, S.V.},
 year = {1997},
 pages = {72--83},
 publisher = {IEEE},
 title = {The impact of instruction-level parallelism on multiprocessor performance and simulation methodology},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569611},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569611},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569611.pdf?arnumber=569611},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Analytical models, Computational modeling, Computer simulation, Delay, Dynamic scheduling, Microprocessors, Out of order, Parallel processing, Performance analysis, Processor scheduling, direct-execution simulation, direct-execution simulators, discrete event simulation, dynamic scheduling, execution-driven simulator, instruction sets, instruction-level parallelism, multiprocessor performance, nonblocking reads, processor models, processor scheduling, shared memory systems, shared-memory multiprocessors, simulation methodology, },
 abstract = {Current microprocessors exploit high levels of instruction-level parallelism (ILP) through techniques such as multiple issue, dynamic scheduling, and non-blocking reads. This paper presents the first detailed analysis of the impact of such processors on shared-memory multiprocessors using a detailed execution-driven simulator. Using this analysis, we also examine the validity of common direct-execution simulation techniques that employ previous-generation processor models to approximate ILP-based multiprocessors. We find that ILP techniques substantially reduce CPU time in multiprocessors, but are less effective in reducing memory stall time. Consequently, despite the presence of inherent latency-tolerating techniques in ILP processors, memory stall time becomes a larger component of execution time and parallel efficiencies are generally poorer in ILP-based multiprocessors than in previous-generation multiprocessors. Examining the validity of direct-execution simulators with previous-generation processor models, we find that, with appropriate approximations, such simulators can reasonably characterize the behavior of applications with poor overlap of read misses. However, they can be highly inaccurate for applications with high overlap of read misses. For our applications, the errors in execution time with these simulators range from 26\% to 192\% for the most commonly used model, and from -8\% to 73\% for the most accurate model },
}

@inproceedings{744339,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Tanaka, K. and Matsumoto, T. and Hiraki, K.},
 year = {1999},
 pages = {90--99},
 publisher = {IEEE},
 title = {Lightweight hardware distributed shared memory supported by generalized combining},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744339},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744339},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744339.pdf?arnumber=744339},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Concurrent computing, Costs, Degradation, Hardware, Large-scale systems, Memory management, OCHANOMIZ-5, Protocols, Prototypes, Software maintenance, Software performance, access costs, accidental events, directory memory, distributed shared memory systems, generalized combining, hierarchical coherence management, hierarchical management technique, high cost factors, large scale parallel computer system, lightweight hardware distributed shared memory, lightweight method, massively parallel system, memory block, memory component, parallel machines, parallelization, programming environment, protocol processor, prototype parallel computer, shared memory system, storage management, switching node, tag/state information, },
 abstract = {On a large scale parallel computer system, shared memory provides a general and convenient programming environment. The paper describes a lightweight method for constructing an efficient shared memory system supported by hierarchical coherence management and generalized combining. The hierarchical management technique and generalized combining cooperate with each other. We eliminate the following heavyweight and high cost factors: a large amount of directory memory which is proportional to the number of processors, a separate memory component for the directory, tag/state information, and a protocol processor. In our method, the amount of memory required for the directory is proportional to the logarithm of the number of processors. This implies that a single word for each memory block is sufficient for covering a massively parallel system and that the access costs of the directory are small. Moreover, our combining technique, generalized combining, does not expect the accidental events which existing combining networks do, that is, events that messages meet each other at a switching node. A switching node can combine succeeding messages with a preceding one even after the preceding message leaves the node. This can increase the rate of successful combining. We have developed a prototype parallel computer OCHANOMIZ-5, that implements this lightweight distributed shared memory and generalized combining with simple hardware. The results of evaluating the prototype's performance using several programs show that our methodology provides the advantages of parallelization },
}

@inproceedings{569606,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Ravindran, G. and Stumm, M.},
 year = {1997},
 pages = {58--69},
 publisher = {IEEE},
 title = {A performance comparison of hierarchical ring- and mesh-connected multiprocessor networks},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569606},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569606},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569606.pdf?arnumber=569606},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Analytical models, Backplanes, Bandwidth, Broadcasting, Clocks, Large-scale systems, Mesh networks, Multicast protocols, Network topology, Scalability, application memory access patterns, digital simulation, hierarchical ring-connected multiprocessor networks, hierarchical rings, mesh router buffers, mesh-connected multiprocessor networks, multiprocessor interconnection networks, performance, performance comparison, performance evaluation, shared memory multiprocessor networks, shared memory systems, simulation study, spatial locality, },
 abstract = {This paper compares the performance of hierarchical ring- and mesh-connected wormhole routed shared memory multiprocessor networks in a simulation study. Hierarchical rings are interesting alternatives to meshes since (i) they can be clocked at faster rates, (ii) they can have wider data paths and hence shorter message sates, (iii) they allow addition and removal of processing nodes at arbitrary locations, (iv) their topology allows natural exploitation in the spatial locality of application memory access patterns, and (v) their topology allows efficient implementation of broadcasts. Our study shows that for workloads with little locality, meshes scale better than ring networks because ring-based systems have limited bisection bandwidth. However, for workloads with some memory access locality hierarchical rings outperform meshes by 20-40\% for system sizes of up to 128 processors. Even with poor access locality, hierarchical rings will outperform meshes for these system sizes if the mesh router buffers are only 1-flit large, and they will outperform meshes an systems with less than 36 processors regardless of mesh router buffer size },
}

@inproceedings{569602,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Kesavan, R. and Bondalapati, K. and Panda, D.K.},
 year = {1997},
 pages = {48--57},
 publisher = {IEEE},
 title = {Multicast on irregular switch-based networks with wormhole routing },
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569602},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569602},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569602.pdf?arnumber=569602},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Bonding, Communication switching, Delay, Engineering profession, Information science, Multicast algorithms, NOW systems, Network topology, Routing, Switches, System recovery, adaptive routing, arbitrary irregular network, binomial tree-based communication pattern, chain concatenation ordering, irregular interconnection, irregular switch-based networks, message passing, multicast systems, multiprocessor interconnection networks, naive random ordering, node orderings, reduced contention, switch-based hierarchical ordering, switch-based ordering, switch-based wormhole interconnection, unicast message passing, wormhole routing, },
 abstract = {This paper presents efficient multicasting with reduced contention on irregular networks with switch-based wormhole interconnection and unicast message passing. First, it is proved that for an arbitrary irregular network with a typical deadlock-free, adaptive routing, it may not be possible to create an ordered list of nodes to implement an arbitrary multicast in a contention-free manner with minimal number of communication steps. Next, three different multicast algorithms are proposed with their respective node orderings to reduce contention: switch-based ordering (SO), switch-based hierarchical ordering (SHO), and chain concatenation ordering (CCO). A variation of a binomial tree-based communication pattern with unicast message passing is used on the above ordered lists to implement multicast. The proposed multicast algorithms are compared with each other as well as with the naive random ordering (RO) algorithm for a range of system sizes, switch sizes, message lengths, degrees of connectivity, destination set sizes, and communication start-up times. The CCO algorithm is shown to be the best to implement multicast with reduced contention and minimum latency. Such results related to multicast on irregular networks are the first of their kind in the wormhole literature. Thus, these demonstrate significant potential to be applied to current and future generation NOW systems with irregular interconnection },
}

@inproceedings{995708,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Vera, X. and Jingling Xue},
 year = {2002},
 pages = { 175-- 186},
 publisher = {IEEE},
 title = {Let's study whole-program cache behaviour analytically},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995708},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995708},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995708.pdf?arnumber=995708},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { SPECfP95,  cache behaviour,  cache simulation performance,  cache storage,  compiler locality optimisations,  data reuse,  multiple loop nests,  optimising compilers,  program control structures,  prototyping implementation,  regular computations, Boolean functions, Computer architecture, Data structures, },
 abstract = {Based on a new characterisation of data reuse across multiple loop nests, we preset a method, a prototyping implementation and some experimental results for analysing the cache behaviour of whole programs with regular computations. Validation against cache simulation using real codes shows the efficiency and accuracy of our method. The largest program, we have analysed, Applu from SPECfP95, has 3868 lines, 16 subroutines and 2565 references. In the case of a 32KB cache with a 32B line size, our method obtains the miss ratio with an absolute error of about 0.80\% in about 128 seconds while the simulator used runs for nearly 5 hours on a 933MHz Pentium. III PC. Our method can be used to guide compiler locality optimisations and improve cache simulation performance. },
}

@inproceedings{995709,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Wang, P.H. and Hong Wang and Collins, J.D. and Grochowski, E. and Kling, R.M. and Shen, J.P.},
 year = {2002},
 pages = { 187-- 196},
 publisher = {IEEE},
 title = {Memory latency-tolerance approaches for Itanium processors: out-of-order execution vs. speculative precomputation},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995709},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995709},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995709.pdf?arnumber=995709},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Itanium processors,  cache misses,  cache prefetching,  cache storage,  loop control code,  memory latency tolerance,  memory latency-tolerance approaches,  microprocessor chips,  multi-threading,  multithreading support,  out-of-order execution,  speculative precomputation, Delay, Educational institutions, Microarchitecture, Microprocessors, Out of order, Prefetching, Process design, Scheduling, Surface-mount technology, Yarn, },
 abstract = {The performance of in-order execution Itanium<sup>TM</sup> processors can suffer significantly due to cache misses. Two memory latency tolerance approaches can be applied for the Itanium processors. One uses an out-of-order (OOO) execution core; the other assumes multithreading support and exploits cache prefetching via speculative precomputation (SP). This paper evaluates and contrasts these two approaches. In addition, this paper assesses the effectiveness of combining the two approaches. For a select set of memory-intensive programs, an in-order SMT Itanium processor using speculative precomputation can achieve performance improvement (92\%) comparable to that of an out-of-order design (87\%). Applying both 000 and SP yields a total performance improvement of 141\% over the baseline in-order machine. OOO tends to be effective in prefetching-for L1 misses; whereas SP is primarily good at covering L2 and L3 misses. Our analysis indicates that the two approaches can be redundant or complementary depending on the type of delinquent loads that each targets. Both approaches are effective on delinquent loads in the loop body; however only SP is effective on delinquent loads found in loop control code. },
}

@inproceedings{995700,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Sakamoto, M. and Brisson, L. and Katsuno, A. and Inoue, A. and Kimura, Y.},
 year = {2002},
 pages = { 81-- 91},
 publisher = {IEEE},
 title = {Reverse Tracer: a software tool for generating realistic performance test programs},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995700},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995700},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995700.pdf?arnumber=995700},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Reverse Tracer,  accurate performance measurement,  application code,  application generators,  automatic test software,  errors,  executable performance tests,  high-performance processors,  input-output programs,  instruction trace,  large interactive workloads,  logic simulation,  logic simulators,  multi-user I/O-intensive workloads,  noncycle-accurate models,  operating system code,  performance estimates,  performance evaluation,  realistic-performance test program generation,  server systems,  software performance models,  software tool,  software tools,  virtual machines, Computer architecture, Software testing, Software tools, },
 abstract = {During the development of high-performance processors, software performance models are used to obtain performance estimates. These models are not cycle-accurate, so their results can have significant errors, leading to performance surprises after the hardware is built. Some performance tests can run directly on the logic simulators, to get more accurate results, but those simulators cannot run large interactive workloads with I/O and much operating system code. So the accurate performance estimates from logic simulators are only available for application code, and are not adequate for the evaluation of powerful server systems that are primarily intended to run large interactive workloads. We discuss a software tool system, the "Reverse Tracer", that generates executable performance tests from an instruction trace of the workload. The generated performance tests retain the essential performance characteristics of multi-user I/O-intensive workloads without doing any real I/O, so they can run in logic simulation to measure performance accurately before the hardware is built. },
}

@inproceedings{995701,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Chen, G. and Shetty, R. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M.J. and Wolczko, M.},
 year = {2002},
 pages = { 92-- 103},
 publisher = {IEEE},
 title = {Tuning garbage collection in an embedded Java environment},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995701},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995701},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995701.pdf?arnumber=995701},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Java,  Java virtual machine,  battery-operated architecture,  embedded Java environment,  embedded systems,  embedded systems,  energy consumption,  garbage collector,  leakage energy,  limited memory systems,  memory architecture,  object allocation,  optimisation,  optimization,  storage management, Compaction, Embedded system, Energy consumption, Engineering profession, Java, Libraries, Personal digital assistants, Radio spectrum management, Sun, Virtual machining, },
 abstract = {Traditionally, the Java virtual machine (JVM), the cornerstone of Java technology, is tuned for performance, taking into account that the energy consumption requires re-evaluation, and possibly re-design of the virtual machine. This motivates us to tune specific components of the virtual machine for a battery-operated architecture. As embedded JVMs are designed to run for long periods of time on limited-memory embedded systems, creating and managing Java objects is of critical importance. The garbage collector (GC) is an important part of the JVM responsible for the automatic reclamation of unused memory. This paper shows that the GC is not only important for limited-memory systems but also for energy-constrained architectures. In particular, we present a GC-controlled leakage energy optimization technique that shuts off memory banks that do not hold live data. A variety of parameters, such as bank size, the garbage collection frequency, object allocation style, compaction style, and compaction frequency are tuned for energy saving. },
}

@inproceedings{995702,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Zhichun Zhu and Zhao Zhang and Xiaodong Zhang},
 year = {2002},
 pages = { 107-- 116},
 publisher = {IEEE},
 title = {Fine-grain priority scheduling on multi-channel memory systems},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995702},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995702},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995702.pdf?arnumber=995702},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { DRAM chips,  DRAM memory,  SPEC2000 programs,  direct rambus DRAM,  fine-grain priority scheduling,  granularity,  memory access scheduling,  multichannel memory systems,  performance evaluation,  scheduling,  storage management,  workload independent configuration, Application software, Bandwidth, Computer science, Concurrent computing, Delay, Educational institutions, Processor scheduling, Random access memory, Resumes, System performance, },
 abstract = {Configurations of contemporary DRAM memory systems become increasingly complex. A recent study shows that the application performance is highly sensitive to choices of configurations. In this study we show that, by utilizing fine-grain priority access scheduling, we are able to find a workload independent configuration that achieves optimal performance on a multichannel memory system. Our approach can well utilize the available high concurrency and high bandwidth on such memory systems, and effectively reduce the memory stall time of memory-intensive applications. Conducting execution-driven simulation of a 4-way issue, a 2 GHz processor, we show that the average performance improvement for fifteen memory-intensive SPEC2000 programs by using an optimized fine-grain priority scheduling is about 13\% and 8\% for a 2-channel and a 4-channel Direct Rambus DRAM memory system, respectively, compared with gang scheduling. Compared with burst scheduling, the average performance improvement is 16\% and 14\% for the 2-channel and 4-channel memory systems, respectively. },
}

@inproceedings{995703,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Suh, G.E. and Devadas, S. and Rudolph, L.},
 year = {2002},
 pages = { 117-- 128},
 publisher = {IEEE},
 title = {A new memory monitoring scheme for memory-aware scheduling and partitioning},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995703},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995703},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995703.pdf?arnumber=995703},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { LRU replacement policy,  cache hits,  cache miss-rate,  cache size,  cache storage,  marginal-gain counters,  memory monitoring,  memory-aware scheduling,  monitoring,  partitioning,  real-time systems,  scheduling,  storage management, Analytical models, Computer architecture, Computer science, Computerized monitoring, Counting circuits, Hardware, Laboratories, Processor scheduling, Runtime, Yarn, },
 abstract = {We propose a low overhead, online memory monitoring scheme utilizing a set of novel hardware counters. The counters indicate the marginal gain in cache hits as the size of the cache is increased, which gives the cache miss-rate as a function of cache size. Using the counters, we describe a scheme that enables an accurate estimate of the isolated miss-rates of each process as a function of cache size under the standard LRU replacement policy. This information can be used to schedule jobs or to partition the cache to minimize the overall miss-rate. The data collected by the monitors can also be used by an analytical model of cache and memory behavior to produce a more accurate overall miss-rate for the collection of processes sharing a cache in both time and space. This overall miss-rate can be used to improve scheduling and partitioning schemes. },
}

@inproceedings{995704,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Unsal, O.S. and Koren, I. and Mani Krishna, C. and Moritz, C.A.},
 year = {2002},
 pages = { 131-- 140},
 publisher = {IEEE},
 title = {The minimax cache: an energy-efficient framework for media processors},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995704},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995704},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995704.pdf?arnumber=995704},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { cache memory,  cache storage,  media processors,  memory accesses,  memory architecture,  mini cache,  minimax cache,  multipartitioned memory systems,  power consumption,  program compilers,  storage management, Application software, Control systems, Energy efficiency, Hardware, Interference, Memory management, Minimax techniques, Power engineering computing, Power system management, Registers, },
 abstract = {This work is based on our philosophy of providing interlayer system-level power awareness in computing systems. Here, we couple this approach with our vision of multi-partitioned memory systems, where memory accesses are separated based on their static predictability and memory footprint and managed with various compiler controlled techniques. We show that media applications are mapped more efficiently when scalar memory accesses are redirected to a mini-cache. Our results indicate that a partitioned 8K cache with the scalars being mapped to a 512 byte mini-cache can be more efficient than a 16K monolithic cache from both performance and energy point of view for most applications. In extensive experiments, we report 30\% to 60\% energy-delay product savings over a range of system configurations and different cache sizes. },
}

@inproceedings{995705,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Gurumurthi, S. and Sivasubramaniam, A. and Irwin, M.J. and Vijaykrishnan, N. and Kandemir, M.},
 year = {2002},
 pages = { 141-- 150},
 publisher = {IEEE},
 title = {Using complete machine simulation for software power estimation: the SoftWatt approach},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995705},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995705},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995705.pdf?arnumber=995705},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { CPU,  SimOS,  SoftWatt,  Spec JVM98 benchmark,  computer power supplies,  digital simulation,  memory,  operating system,  performance evaluation,  power consumption,  power optimization,  power profile,  simulation tools,  software power estimation,  utility programs, Application software, Computational modeling, Computer architecture, Design optimization, Hardware, Kernel, Operating systems, Power dissipation, Power system modeling, Technological innovation, },
 abstract = {Power dissipation has become one of the most critical factors for the continued development of both high-end and low-end computer systems. We present a complete system power simulator, called SoftWatt, that models the CPU, memory hierarchy, and a low-power disk subsystem and quantifies the power behavior of both the application and operating system. This tool, built on top of the SimOS infrastructure, uses validated analytical energy models to identify the power hotspots in the system components, capture relative contributions of the user and kernel code to the system power profile, identify the power-hungry operating system services and characterize the variance in kernel power profile with respect to workload. Our results using Spec JVM98 benchmark suite emphasize the importance of complete system simulation to understand the power impact of architecture and operating system on application execution. },
}

@inproceedings{995706,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Se-Hyun Yang and Powell, M.D. and Falsafi, B. and Vijaykumar, T.N.},
 year = {2002},
 pages = { 151-- 161},
 publisher = {IEEE},
 title = {Exploiting choice in resizable cache design to optimize deep-submicron processor energy-delay},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995706},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995706},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995706.pdf?arnumber=995706},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { CMOS digital integrated circuits,  cache memories,  cache organization,  cache storage,  d-cache,  deep-submicron processor energy-delay,  dynamic resizing,  energy dissipation,  hybrid selective-sets-andways cache organization,  i-cache,  microprocessor chips,  resizable cache design,  resizing granularity,  selective-sets,  selective-ways,  set-associativity, Application software, CMOS technology, Cache memory, Circuits, Computer architecture, Design optimization, Energy dissipation, Hardware, Microprocessors, Proposals, },
 abstract = {Cache memories account for a significant fraction of a chip's overall energy dissipation. Recent research advocates using "resizable" caches to exploit cache requirement variability in applications to reduce cache size and eliminate energy dissipation in the cache's unused sections with minimal impact on performance. Current proposals for resizable caches fundamentally vary in two design aspects: (1) cache organization, where one organization, referred to as selective-ways, varies the cache's set-associativity, while the other, referred to as selective-sets, varies the number of cache sets, and (2) resizing strategy, where one proposal statically sets the cache size prior to an application's execution, while the other allows for dynamic resizing both within and across applications. In this paper, we compare and contrast, for the first time, the proposed design choices for resizable caches, and evaluate the effectiveness of cache resizings in reducing the overall energy-delay in deep-submicron processors. In addition, we propose a hybrid selective-sets-and-ways cache organization that always offers equal or better resizing granularity than both of previously proposed organizations. We also investigate the energy savings from resizing d-cache and i-cache together to characterize the interaction between d-cache and i-cache resizings. },
}

@inproceedings{995707,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Rakvic, T. and Black, T. and Limaye, D. and Shen, T.P.},
 year = {2002},
 pages = { 165-- 174},
 publisher = {IEEE},
 title = {Non-vital loads},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995707},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995707},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995707.pdf?arnumber=995707},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Vital Cache,  cache storage,  load instruction behavior,  microprocessor chips,  on-chip caches,  selective data caching, Bandwidth, Cache memory, Classification algorithms, Computer architecture, Delay, Prefetching, },
 abstract = {As the frequency gap between main memory and modern microprocessor grows, the implementation and efficiency of on-chip caches become more important. The growing latency to memory is motivating new research into load instruction behavior and selective data caching. This work investigates the classification of load instruction behavior. A new load classification method is proposed that classifies loads into those vital to performance and those not vital to performance. A limit study is presented to characterize different types of non-vital loads and to quantify the percentage of loads that are non-vital. Finally, a realistic implementation of the non-vital load classification method is presented and a new cache structure called the Vital Cache is proposed to take advantage of non-vital loads. The Vital Cache caches data for vital loads only, deferring non-vital loads to slower caches. Results: The limit study shows 75\% of all loads are non-vital with only 35\% of the accessed data space being vital for caching. The Vital Cache improves the efficiency of the cache hierarchy and the hit rate for vital loads. The Vital Cache increases performance by 17\%. },
}

@inproceedings{569597,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Yuan, X. and Melhem, R. and Gupta, R.},
 year = {1997},
 pages = {38--47},
 publisher = {IEEE},
 title = {Distributed path reservation algorithms for multiplexed all-optical interconnection networks},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569597},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569597},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569597.pdf?arnumber=569597},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {2-dimensional torus interconnection networks, Bandwidth, Multiprocessor interconnection networks, Optical buffering, Optical fiber networks, Optical wavelength conversion, Protocols, Routing, Time division multiplexing, WDM networks, Wavelength division multiplexing, access protocols, backward reservation protocols, communication delay, distributed path reservation algorithms, distributed path reservation protocols, forward reservation protocols, multiplexed all-optical interconnection networks, multiprocessor interconnection networks, optical information processing, time division multiplexing, time division multiplexing, wavelength division multiplexing, wavelength division multiplexing, },
 abstract = {In this paper, we study distributed path reservation protocols for multiplexed all-optical interconnection networks. In such networks, a path for a connection is reserved such that transmitted data remains in the optical domain until it reaches its destination. The path reservation protocols negotiate the reservation and establishment of connections that arrive dynamically to the network. They can be applied to both wavelength division multiplexing (WDM) and time division multiplexing (TDM), which are two techniques that allow the large optical bandwidth to be shared among multiple connections. Two classes of protocols are discussed: forward reservation protocols and backward reservation protocols. Simulations of multiplexed 2-dimensional torus interconnection networks are used to evaluate and compare the performance of the protocols, and to study the impact of system parameters on both network throughput and communication delay. The simulation results show that the backward reservation schemes provide better performance than their forward reservation counterparts },
}

@inproceedings{1410069,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Kalogeropulos, S. and Rajagopalan, M. and Vikram Rao and Yonghong Song and Tirumalai, P.},
 year = {2004},
 pages = { 106-- 106},
 publisher = {IEEE},
 title = {Processor Aware Anticipatory Prefetching in Loops},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10029},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410069},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410069.pdf?arnumber=1410069},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Bandwidth, Costs, Delay, Electronic mail, Hardware, Intelligent networks, Performance loss, Prefetching, Sun, },
 abstract = {As microprocessor speeds increase, a large fraction of the execution time is often lost to cache miss penalties. This loss can be particularly severe in processors such as the UltraSPARC-IIICu which have in-order execution and block on cache misses. Such processors rely greatly on the compiler to reduce stalls and achieve high performance. This paper describes a compiler technique for software prefetching that is aware of the specific prefetch behaviors of the target processor. The implementation targets loops containing control-flow and strided or irregular memory access patterns. A two phase locality analysis, capable of handling complex subscript expressions, is used for enhanced identification of prefetch candidates. Prefetch instructions are scheduled with careful consideration of the prefetch behaviors in the target system. Compared to a previous implementation, our technique produced performance improvements of 9\% on the geometric mean, and up to 44\% on individual tests, in Sun\&#146;s first UltraSPARC-IIICu based SPEC CPU2000 submission [5] and has been used in all later submissions to date. },
}

@inproceedings{1410068,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Nesbit, K.J. and Smith, J.E.},
 year = {2004},
 pages = { 96-- 96},
 publisher = {IEEE},
 title = {Data Cache Prefetching Using a Global History Buffer},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10030},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410068},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410068.pdf?arnumber=1410068},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Analytical models, Cache memory, Clocks, Computational modeling, Computer simulation, Delay, History, Microarchitecture, Microprocessors, Prefetching, },
 abstract = {A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order. Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction. The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones. Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods. Global History Buffer prefetching can increase correlation prefetching performance by 20\% and cut its memory traffic by 90\%. Furthermore, the Global History Buffer can make correlations within a load\&#146;s address stream, which can increase stride prefetching performance by 6\%. Collectively, the Global History Buffer prefetching methods perform as well or better than the conventional prefetching methods studied on 14 of 15 benchmarks. },
}

@inproceedings{1410061,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Li, J. and Martinez, J.F. and Huang, M.C.},
 year = {2004},
 pages = { 14-- 23},
 publisher = {IEEE},
 title = {The thrifty barrier: energy-aware synchronization in shared-memory multiprocessors},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10018},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410061},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410061.pdf?arnumber=1410061},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { energy conservation,  energy-aware synchronization,  hardware-software approach,  hardware-software codesign,  multi-threading,  multithreading,  parallel processing,  power electronics,  protocols,  shared memory multiprocessor,  shared memory systems,  synchronisation,  thrifty barrier synchronization, Degradation, Energy consumption, Energy efficiency, Hardware, Laboratories, Microprocessors, Protocols, Sleep, State estimation, Yarn, },
 abstract = {Much research has been devoted to making microprocessors energy-efficient. However, little attention has been paid to multiprocessor environments where, due to the cooperative nature of the computation, the most energy-efficient execution in each processor may not translate into the most energy-efficient overall execution. We present the thrifty barrier, a hardware-software approach to saving energy in parallel applications that exhibit barrier synchronization imbalance. Threads that arrive early to a thrifty barrier pick among existing low-power processor sleep states based on predicted barrier stall time and other factors. We leverage the coherence protocol and propose small hardware extensions to achieve timely wake-up of these dormant threads, maximizing energy savings while minimizing the impact on performance. },
}

@inproceedings{1410060,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Wen, V. and Whitney, M. and Patel, Y. and Kubiatowicz, J.D.},
 year = {2004},
 pages = { 2-- 13},
 publisher = {IEEE},
 title = {Exploiting prediction to reduce power on buses},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10025},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410060},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410060.pdf?arnumber=1410060},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { SPEC benchmarking,  SPICE,  SPICE simulation,  SimpleScalar simulator,  coding technique,  decoder circuit,  decoding,  encoding,  energy consumption reduction,  microprocessor chip,  microprocessor chips,  on-chip bus,  power consumption,  power electronics,  system buses, Capacitance, Circuit simulation, Computational modeling, Decoding, Encoding, Integrated circuit interconnections, Latches, Repeaters, Transcoding, Wire, },
 abstract = {We investigate coding techniques to reduce the energy consumed by on-chip buses in a microprocessor. We explore several simple coding schemes and simulate them using a modified SimpleScalar simulator and SPEC benchmarks. We show an average of 35\% savings in transitions on internal buses. To quantify actual power savings, we design a dictionary based encoder/decoder circuit in a 0.13 \&mu;m process, extract it as a netlist, and simulate its behavior under SPICE. Utilizing a realistic wire model with repeaters, we show that we can break even at median wire length scales of less than 11.5 mm at 0.13 \&mu; and project a break-even point of 2.7 mm for a larger design at 0.07 \&mu;. },
}

@inproceedings{1410063,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Joseph, R. and Hu, Z. and Martonosi, M.},
 year = {2004},
 pages = { 36-- 46},
 publisher = {IEEE},
 title = {Wavelet analysis for microprocessor design: experiences with wavelet-based dI/dt characterization},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10027},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410063},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410063.pdf?arnumber=1410063},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { computer architecture,  dl/dt estimation,  microprocessor chips,  microprocessor design,  offline wavelet-based estimation,  online wavelet-based control,  time-frequency analysis,  time-frequency analysis,  voltage control,  voltage control,  wavelet analysis,  wavelet transforms, Central Processing Unit, Fluctuations, Frequency, Hardware, Microarchitecture, Microprocessors, Monitoring, Power supplies, Voltage control, Wavelet analysis, },
 abstract = {As microprocessors become increasingly complex, the techniques used to analyze and predict their behavior must become increasingly rigorous. We apply wavelet analysis techniques to the problem of dl/dt estimation and control in modern microprocessors. While prior work has considered Bayesian phase analysis, Markov analysis, and other techniques to characterize hardware and software behavior, we know of no prior work using wavelets for characterizing computer systems. The dl/dt problem has been increasingly vexing in recent years, because of aggressive drops in supply voltage and increasingly large relative fluctuations in CPU current dissipation. Because the dl/dt problem has natural frequency dependence (it is worst in the mid-frequency range of roughly 50-200 MHz) it is natural to apply frequency-oriented techniques like wavelets to understand it. Our work proposes (i) an offline wavelet-based estimation technique that can accurately predict a benchmark's likelihood of causing voltage emergencies, and (ii) an online wavelet-based control technique that uses key wavelet coefficients to predict and avert impending voltage emergencies. The offline estimation technique works with roughly 0.94\% error. The online control technique reduces false positives in dl/dt prediction, allowing, voltage control to occur with less than 2.5\% performance overhead on the SPEC benchmark suite. },
}

@inproceedings{1410062,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Gniady, C. and Hu, Y.C. and Lu, Y.-H.},
 year = {2004},
 pages = { 24-- 35},
 publisher = {IEEE},
 title = {Program counter based techniques for dynamic power management},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10021},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410062},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410062.pdf?arnumber=1410062},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { I/O activity prediction,  dynamic power management,  energy consumption reduction,  hard discs,  idle period prediction,  microprocessor chips,  operating system,  path-based correlation,  power consumption,  power electronics,  program-counter access predictor design, Counting circuits, Design engineering, Energy consumption, Energy management, Hard disks, Power engineering and energy, Power engineering computing, Power system management, Space technology, Wireless networks, },
 abstract = {Reducing energy consumption has become one of the major challenges in designing future computing systems. We propose a novel idea of using program counters to predict I/O activities in the operating system. We present a complete design of program-counter access predictor (PCAP) that dynamically learns the access patterns of applications and predicts when an I/O device can be shut down to save energy. PCAP uses path-based correlation to observe a particular sequence of program counters leading to each idle period, and predicts future occurrences of that idle period. PCAP differs from previously proposed shutdown predictors in its ability to: (1) correlate I/O operations to particular behavior of the applications and users, (2) carry prediction information across multiple executions of the applications, and (3) attain better energy savings while incurring low mispredictions. },
}

@inproceedings{1410065,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Jayasena, N. and Erez, M. and Ahn, J.H. and Dally, W.J.},
 year = {2004},
 pages = { 60-- 72},
 publisher = {IEEE},
 title = {Stream register files with indexed access},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10007},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410065},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410065.pdf?arnumber=1410065},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { data access pattern,  data parallelism,  data structures,  indexed SRF access,  parallel architectures,  programmable architecture,  storage allocation,  stream processor,  stream register file,  vector processor, Application software, Bandwidth, Computer architecture, Concurrent computing, Cryptography, Delay, Graphics, Microprocessors, Parallel processing, Registers, },
 abstract = {Many current programmable architecture designed to exploit data parallelism require computation to be structured to operate on sequentially accessed vectors or streams of data. Applications with less regular data access patterns perform sub-optimally on such architectures. We present a register file for streams (SRF) that allows arbitrary, indexed accesses. Compared to sequential SRF access, indexed access captures more temporal locality, reduces data replication in the SRF, and provides efficient support for certain types of complex access patterns. Our simulations show that indexed SRF access provides speedups of 1.03x to 4.1x and memory bandwidth reductions of up to 95\% over sequential SRF access for a set of benchmarks representative of data-parallel applications with irregular accesses. Indexed SRF access also provides greater speedups than caches for a number of application classes despite significantly lower hardware costs. The area overhead of our indexed SRF implementation is 11\%-22\% over a sequentially accessed SRF, which corresponds to a modest 1.5\%-3\% increase in the total die area of a typical stream processor. },
}

@inproceedings{1410064,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Cristal, A. and Ortega, D. and Llosa, J. and Valero, M.},
 year = {2004},
 pages = { 48-- 59},
 publisher = {IEEE},
 title = {Out-of-order commit processors},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10008},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410064},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410064.pdf?arnumber=1410064},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { branch speculation,  buffer storage,  cache hierarchy,  checkpointing,  in-flight instruction,  instruction sets,  microarchitectural structure,  out-of-order commit processor,  parallel architectures,  queuing mechanism,  reorder buffer,  system recovery, Bars, Buffer storage, Checkpointing, Computer aided instruction, Costs, Delay, Energy consumption, Microarchitecture, Out of order, Registers, },
 abstract = {Modern out-of-order processors tolerate long latency memory operations by supporting a large number of in-flight instructions. This is particularly useful in numerical applications where branch speculation is normally not a problem and where the cache hierarchy is not capable of delivering the data soon enough. In order to support more in-flight instructions, several resources have to be up-sized, such as the reorder buffer (ROB), the general purpose instructions queues, the load/store queue and the number of physical registers in the processor. However, scaling-up the number of entries in these resources is impractical because of area, cycle time, and power consumption constraints. We propose to increase the capacity of future processors by augmenting the number of in-flight instructions. Instead of simply up-sizing resources, we push for new and novel microarchitectural structures that achieve the same performance benefits but with a much lower need for resources. Our main contribution is a new checkpointing mechanism that is capable of keeping thousands of in-flight instructions at a practically constant cost. We also propose a queuing mechanism that takes advantage of the differences in waiting time of the instructions in the flow. Using these two mechanisms our processor has a performance degradation of only 10\% for SPEC2000fp over a conventional processor requiring more than an order of magnitude additional entries in the ROB and instruction queues, and about a 200\% improvement over a current processor with a similar number of entries. },
}

@inproceedings{1410067,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Aamodt, T.M. and Chow, P. and Hammarlund, P. and Hong Wang and Shen, J.P.},
 year = {2004},
 pages = { 84-- 84},
 publisher = {IEEE},
 title = {Hardware Support for Prescient Instruction Prefetch},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10028},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410067},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410067.pdf?arnumber=1410067},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Application software, Automata, Databases, Delay, Hardware, Multithreading, Performance evaluation, Prefetching, Surface-mount technology, Yarn, },
 abstract = {This paper proposes and evaluates hardware mechanisms for supporting prescient instruction prefetch \&#8212; an approach to improving single-threaded application performance by using helper threads to perform instruction prefetch. We demonstrate the need for enabling store-to-load communication and selective instruction execution when directly pre-executing future regions of an application that suffer I-cache misses. Two novel hardware mechanisms, safe-store and YAT-bits, are introduced that help satisfy these requirements. This paper also proposes and evaluates .nite state machine recall, a technique for limiting pre-execution to branches that are hard to predict by leveraging a counted I-prefetch mechanism. On a research Itanium\&#174;SMT processor with next line and streaming I-prefetch mechanisms that incurs latencies representative of next generation processors, prescient instruction prefetch can improve performance by an average of 10.0\% to 22\% on a set of SPEC 2000 benchmarks that suffer significant I-cache misses. Prescient instruction prefetch is found to be competitive against even the most aggressive research hardware instruction prefetch technique: fetch directed instruction prefetch. },
}

@inproceedings{1410066,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Abella, J. and Gonzalez, A.},
 year = {2004},
 pages = { 73-- 82},
 publisher = {IEEE},
 title = {Low-complexity distributed issue queue},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10013},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410066},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410066.pdf?arnumber=1410066},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { computational complexity,  energy-delay reduction,  file organisation,  formal logic,  instruction classification,  instruction queue,  instruction sets,  low-complexity distributed issue queue,  parallel architectures,  storage allocation, CADCAM, Clocks, Computer aided manufacturing, Computer architecture, Cooling, Delay, Dispatching, Logic, MB&#095, distr issue logic, Pipelines, Power dissipation, },
 abstract = {As technology evolves, power density significantly increases and cooling systems become more complex and expensive. The issue logic is one of the processor hotspots and, at the same time, its latency is crucial for the processor performance. We present a low-complexity FP issue logic (MB\&#095;distr) that achieves high performance with small energy requirements. The MB\&#095;distr scheme is based on classifying instructions and dispatching them into a set of queues depending on their data dependences. These instructions are selected for issuing based on an estimation of when their operands will be available, so the conventional wakeup activity is not required. Additionally, the functional units are distributed across the different queues. The energy required by the proposed scheme is substantially lower than that required by a conventional issue design, even if the latter has the ability of waking-up only unready operands. MB\&#095;distr scheme reduces the energy-delay product by 35\% and the energy-delay product by 18\% with respect to a state-of-the-art approach. },
}

@inproceedings{4798229,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {1--2},
 publisher = {IEEE},
 title = {Keynote I},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798229},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798229},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798229.pdf?arnumber=4798229},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798229.png" border="0"> },
}

@inproceedings{4798228,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {i--xiv},
 publisher = {IEEE},
 title = {Frontal},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798228},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798228},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798228.pdf?arnumber=4798228},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {3D stacked memory hierarchies, NUCA, cache storage, computer architecture, industrial perspectives panel, multicore cache architectures, onchip networks, power-performance-efficient architectures, processor microarchitecture, },
 abstract = {The following topics are dealt with: multicore cache architectures; on-chip networks; processor microarchitecture; NUCA; 3D stacked memory hierarchies; power-performance-efficient architectures; and industrial perspectives panel. },
}

@inproceedings{903259,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Yang, S. and Powell, M.D. and Falsafi, B. and Roy, K. and Vijaykumar, T.N.},
 year = {2001},
 pages = {147--157},
 publisher = {IEEE},
 title = {An integrated circuit/architecture approach to reducing leakage in deep-submicron high-performance I-caches},
 date = {2001},
 doi = {10.1109/HPCA.2001.903259},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903259},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903259.pdf?arnumber=903259},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {CMOS designs, CMOS integrated circuits, CMOS integrated circuits, Computer architecture, Design engineering, Energy dissipation, High speed integrated circuits, Maintenance engineering, Power engineering and energy, SRAM cells, SRAM chips, Subthreshold current, Switching circuits, Threshold voltage, cache storage, circuit level approach, deep-submicron high-performance I-caches, high transistor switching speeds, integrated circuit/architecture approach, integrated memory circuits, leakage energy dissipation, microarchirectures, on-chip cache memory structures, transistor threshold voltage, },
 abstract = {Deep-submicron CMOS designs maintain high transistor switching speeds by scaling down the supply voltage and proportionately, reducing the transistor threshold voltage. Lowering the threshold voltage increases leakage energy dissipation due to subthreshold leakage current even when the transistor is for switching. Estimates suggest a five-fold increase in leakage energy in every future generation. In modern microarchirectures, much of the leakage energy is dissipated in large on-chip cache memory structures with high transistor densities. While cache utilization varies both within and across applications, modern cache designs are fixed in size resulting in transistor leakage inefficiencies. This paper explores an integrated architectural and circuit level approach to reducing leakage energy in instruction caches (i-caches). At the architectural level, we propose the Dynamically Resizable i-cache (DRI i-cache), a novel i-cache design that dynamically resizes and adapts to an application's required size. At the circuit-level, we use gated-V<sub>dd</sub>, a mechanism that effectively turns of the supply voltage to, and eliminates leakage in, the SRAM cells in a DRI i-cache's unused sections. Architectural and circuit-level simulation results indicate that a DRI i-cache successfully and robust exploits the cache size variability both within and across applications. Compared to a conventional i-cache using an aggressively-scaled threshold voltage a 64K DRI i-cache reduces on average both the leakage energy-delay product and cache size 62\%, with less than 4\% impact on execution time },
}

@inproceedings{903258,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Kailas, K. and Ebcioglu, K. and Agrawala, A.},
 year = {2001},
 pages = {133--143},
 publisher = {IEEE},
 title = {CARS: a new code generation framework for clustered ILP processors },
 date = {2001},
 doi = {10.1109/HPCA.2001.903258},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903258},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903258.pdf?arnumber=903258},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {CARS, Clustering algorithms, Computer science, Dynamic scheduling, Educational institutions, Iterative algorithms, Microarchitecture, Microprocessors, Processor scheduling, Registers, Scheduling algorithm, cluster assignment, clustered ILP processors, code generation, code generation framework, instruction scheduling, parallel architectures, parallel programming, performance evaluation, program compilers, register allocation, },
 abstract = {Clustered ILP processors are characterized by a large number of non-centralized on-chip resources grouped into clusters. Traditional code generation schemes for these processors consist of multiple phases for cluster assignment, register allocation and instruction scheduling. Most of these approaches need additional re-scheduling phases because they often do not impose finite resource constraints in all phases of code generation. These phase-ordered solutions have several drawbacks, resulting in the generation of poor performance code. Moreover the iterative/back-tracking algorithms used in some of these schemes have large turning times. In this paper we present CARS, a code generation framework for Clustered ILP processors, which combines the cluster assignment, register allocation, and instruction scheduling phases into a single code generation phase, thereby eliminating the problems associated with phase-ordered solutions. The CARS algorithm explicitly takes into account all the resource constraints at each cluster scheduling step to reduce spilling and to avoid iterative re-scheduling steps. We also present a new on-the-fly register allocation scheme developed for CARS. We describe an implementation of the proposed code generation framework and the results of a performance evaluation study using the SPEC95/2000 and MediaBench benchmarks },
}

@inproceedings{903255,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Acacio, M.E. and Gonzalez, J. and Garcia, J.M. and Duato, J.},
 year = {2001},
 pages = {97--106},
 publisher = {IEEE},
 title = {A new scalable directory architecture for large-scale multiprocessors},
 date = {2001},
 doi = {10.1109/HPCA.2001.903255},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903255},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903255.pdf?arnumber=903255},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Access protocols, Bandwidth, Broadcasting, Computer architecture, Degradation, Delay, Large-scale systems, Multiprocessor interconnection networks, Nonhomogeneous media, Scalability, large-scale multiprocessors, memory overhead, multilayer clustering, parallel architectures, performance evaluation, scalability, scalable directory architecture, shared memory systems, shared-memory multiprocessors, },
 abstract = {The memory overhead introduced by directories constitutes a major hurdle in the scalability of cc-NUMA architectures, which makes the shared-memory paradigm unfeasible for very large-scale systems. This work is focused on improving the scalability of shared-memory multiprocessors by significantly reducing the size of the directory. We propose multilayer clustering as an effective approach to reduce the directory-entry width. Detailed evaluation for 64 processors shows that using this approach we can drastically reduce the memory overhead, while suffering a performance degradation we similar to previous compressed schemes (such as Coarse Vector). In addition, a novel two-level directory architecture is proposed in order to eliminate the penalty caused by these compressed directories. This organization consists of a small Full-Map first-level directory (which provides precise information for the most recently referenced lines) and a compressed second-level directory (which provides in-excess information). Results show that a system with this directory architecture can achieve the same performance as a multiprocessor with a big and non-scalable Full-Map directory with a very significant reduction of the memory overhead },
}

@inproceedings{903254,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Moshovos, A. and Memik, G. and Falsafi, B. and Choudhary, A.},
 year = {2001},
 pages = {85--96},
 publisher = {IEEE},
 title = {JETTY: filtering snoops for reduced energy consumption in SMP servers},
 date = {2001},
 doi = {10.1109/HPCA.2001.903254},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903254},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903254.pdf?arnumber=903254},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {CMOS memory circuits, Energy consumption, Filtering, JETTY, Network servers, Power dissipation, Power engineering and energy, Power engineering computing, SMP server, Switches, Switching circuits, Telecommunication network reliability, cache storage, cache-like structure, coherence protocol, file servers, memory architecture, multiprocessing systems, reduced energy consumption, snoop requests, symmetric multiprocessor, },
 abstract = {We propose methods for reducing the energy consumed by snoop requests in snoopy bus-based symmetric multiprocessor (SMP) systems. Observing that a large fraction of snoops do not find copies in many of the other caches, we introduce JETTY, a small, cache-like structure. A JETTY is introduced in-between the bus and the L2 backside of each processor. There it filters the vast majority of snoops that would not find a locally cached copy. Energy is reduced as accesses to the much more energy demanding L2 tag arrays are decreased. No changes in the existing coherence protocol are required and no performance loss is experienced. We evaluate our method on a 4-way SMP server using a set of shared-memory applications. We demonstrate that a very small JETTY filters 74\% (average) of all snoop-induced tag accesses that would miss. This results in an average energy reduction of 29\% (range: 12\% to 40\%) measured as a fraction of the energy required by all L2 accesses (both tag and data arrays) },
}

@inproceedings{903257,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Jaejin Lee and Solihin, Y. and Torrettas, J.},
 year = {2001},
 pages = {121--132},
 publisher = {IEEE},
 title = {Automatically mapping code on an intelligent memory architecture },
 date = {2001},
 doi = {10.1109/HPCA.2001.903257},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903257},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903257.pdf?arnumber=903257},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, Computer architecture, Delay, Intelligent systems, Laboratories, Memory architecture, Multiprocessing systems, Partitioning algorithms, Processor scheduling, Proposals, generic intelligent memory system, high performance, intelligent memory architecture, intelligent memory systems, mapping code, memory architecture, },
 abstract = {This paper presents an algorithm to automatically map code on a generic intelligent memory system that consists of a host processor and a simpler memory processor. To achieve high performance with this type of architecture, code needs to be partitioned and scheduled such that each section is assigned to the processor on which it runs most efficiently. In addition, the two processors should overlap their execution as much as possible. With our algorithm, applications are mapped fully automatically using both static and dynamic information. Using a set of standard applications and a simulated architecture, we show average speedups of 1.7 for numerical applications and 1.3 for nonnumerical applications over a single host with plain memory. The speedups are very close and often higher than ideal speedups on a more expensive multiprocessor system composed of two identical host processors. Our work shows that heterogeneity can be cost-effectively exploited and represents one step toward effectively mapping code on intelligent memory systems },
}

@inproceedings{903256,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Thottethodi, M. and Lebeck, A.R. and Mukherjee, S.S.},
 year = {2001},
 pages = {107--118},
 publisher = {IEEE},
 title = {Self-tuned congestion control for multiprocessor networks},
 date = {2001},
 doi = {10.1109/HPCA.2001.903256},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903256},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903256.pdf?arnumber=903256},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, Bandwidth, Communication system control, Computer science, Degradation, Delay, Feedback, Multiprocessor interconnection networks, Network topology, Throughput, congestion control, global-knowledge-based, multiprocessing systems, multiprocessor networks, performance evaluation, source throttling, telecommunication congestion control, tightly-coupled multiprocessors, },
 abstract = {One-track performance in tightly-coupled multiprocessors typically, degrades rapidly beyond network saturation. Consequently, designers must keep a network below its saturation point by reducing the load on the network. Congestion control via source throttling-a common technique to reduce the network load-presents new packets from entering the network in the presence of congestion. Unfortunately, prior schemes to implement source throttling either lack vital global information about the network to make the correct decision (whether to throttle or not) or depend on specific network parameters, network topology or communication pattern. This paper presents a global-knowledge-based, self-tuned, congestion control technique that prevents saturation at high loads across different network configurations and commutation pattern. Our design is composed of two key components. First, we use global information about a network to obtain a timely estimate of network congestion. We compare this estimate to a threshold value to determine when to throttle packet injection. The second component is a self-tuning mechanism that automatically determines appropriate threshold values based on throughput feedback. A combination of these two techniques provides high performance under heavy load does not penalize performance under light load, and gracefully adapts to changes in communication patterns },
}

@inproceedings{903251,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Xiaogang Qiu and Dubois, M.},
 year = {2001},
 pages = {51--62},
 publisher = {IEEE},
 title = {Towards virtually-addressed memory hierarchies},
 date = {2001},
 doi = {10.1109/HPCA.2001.903251},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903251},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903251.pdf?arnumber=903251},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, Bandwidth, Clocks, Computer aided instruction, Computer architecture, Concurrent computing, Costs, Delay, Hardware, Sun, Synonym Lookaside Buffer, cache hierarchies, cache organizations, cache storage, concurrency, memory architecture, memory hierarchy, scalable multiprocessor extensions, virtual addresses, virtual storage, },
 abstract = {Current cache hierarchies are indexed in parallel with a TLB but their tags are part of the physical address so that the memory hierarchy is physically addressed. This design faces problems as more concurrency is exploited in the processor core and as the memory demand of emerging applications is growing fast. The traditional TLB does not scale well inside the processor core and its hit rate call be poor for data-intensive applications or scientific applications without much locality. At the same time, given current trends towards computing in memory and in communication interfaces, virtual addresses are needed not just inside the processor but throughout the memory hierarchy. These observations have prompted us to result the problem of moving virtual address translation away from the processor. This paper introduces new ideas to enable the use of virtual addresses throughout the memory hierarchy. The major idea is the replacement of the TLB with a small Synonym Lookaside Buffer (SLB), which scales well because its size depends on the number of addresses, and not on the size of the application or of the physical memory. We also characterize synonym usage, evaluate the amount of cache and SLB flushing due to remapping of addresses, and compare the miss rate of various virtual physical cache organizations for several application domains. These evaluations show that virtually addressed memory hierarchies overall have better performance behavior than physically-addressed memory hierarchies. Finally, we also show how virtually-addressed memory hierarchies facilitate natural, scalable multiprocessor extensions, as well as computing-in-memory in the context of general-purpose computers },
}

@inproceedings{903250,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Roth, A. and Sohi, G.S.},
 year = {2001},
 pages = {37--48},
 publisher = {IEEE},
 title = {Speculative data-driven multithreading},
 date = {2001},
 doi = {10.1109/HPCA.2001.903250},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903250},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903250.pdf?arnumber=903250},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Computational modeling, Computer aided instruction, Concurrent computing, Delay, Microarchitecture, Multithreading, Retirement, Surface-mount technology, Throughput, Yarn, data-driven multithreading, multi-threading, multithreading, parallel architectures, sequential processor, simultaneous multithreading, },
 abstract = {Mispredicted branches and loads that miss in the cache cause the majority of retirement stalls experienced by sequential processors; we call these critical instructions. Despite their importance, a sequential processor has difficulty prioritizing critical computations (computations of critical instructions), because it must fetch all computations sequentially, regardless of their contribution to performance. Speculative data-driven multithreading (DDMT) is a general-purpose mechanism for overcoming this limitation. In DDAT critical computations are annotated so that they can execute standalone. When the processor predicts an upcoming instance of a critical instruction, it microarchiturally forks a copy of its computation as a new kind of speculative thread: a data-driven thread (DDT). The DDT executes in parallel with the main program thread, but typically generates the critical result much faster since it fetches and executes only the critical computation and not the whole program. A DDT ``pre-executes" a critical computation and effectively ``consumes" its latency on behalf of the main thread. A DDMT component called integration incorporates results completed in DDTs directly, into the main thread, sparing it from having to repent the work. We simulate an implementation of DDMT on top of a simultaneous multithreading (SMT) processor and use program profiles to create DDTs and annotate them into the executable. Our experiments show that DDMT pre-execution of critical loads and branches can improve performance significantly },
}

@inproceedings{903253,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Abali, B. and Franke, H. and Xiaowei Shen and Poff, D.E. and Smith, T.B.},
 year = {2001},
 pages = {73--81},
 publisher = {IEEE},
 title = {Performance of hardware compressed main memory},
 date = {2001},
 doi = {10.1109/HPCA.2001.903253},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903253},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903253.pdf?arnumber=903253},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, Bandwidth, Costs, Data compression, Databases, Hardware, Linux, Memory Expansion Technology, Operating systems, Performance analysis, SPEC2000 benchmarks, Web server, data compression, database benchmark, hardware compressed main memory, memory architecture, memory compression, performance evaluation, performance impact, },
 abstract = {A new memory subsystem called Memory Expansion Technology (MXT) has been built for compressing main memory contents. MXT effectively doubles the physically available memory. This paper provides an analysis of the performance impact of memory compression using the SPEC2000 benchmarks and a database benchmark. Results show that the hardware compression of memory has a negligible performance penalty compared to a standard memory. We also show that many applications' memory contents can be compressed usually by a factor of two to one. We demonstrate this using industry benchmarks, web server benchmarks, and contents of popular web sites },
}

@inproceedings{903252,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Zhen Fang and Lixin Zhang and Carter, J.B. and Hsieh, W.C. and McKee, S.A.},
 year = {2001},
 pages = {63--72},
 publisher = {IEEE},
 title = {Reevaluating online superpage promotion with hardware support},
 date = {2001},
 doi = {10.1109/HPCA.2001.903252},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903252},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903252.pdf?arnumber=903252},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Analytical models, Costs, Decision making, Hardware, Out of order, Performance analysis, Pipelines, US Government, World Wide Web, copying-based promotion, execution-driven simulation, memory architecture, multiple adjacent virtual memory pages, paged storage, performance, performance evaluation, superpages, translation lookaside buffers, },
 abstract = {Typical translation lookaside buffers (TLBs) can map a far smaller region of memory than application footprints demand, and the cost of handling TLB misses therefore limits the performance of an increasing number of applications. This bottleneck can be mitigated by the use of superpages, multiple adjacent virtual memory pages that can be mapped with a single TLB entry that extend TLB reach without significantly increasing size or cost. We analyze hardware/software tradeoff for dynamically creating superpages. This study extends previous work by using execution-driven simulation to compare creating superpages via copying with remapping pages within the memory controller and by examining how the tradeoffs change when moving front a single-issue to a superscalar processor model. We find that remapping-based promotion outperforms copying-based promotion, often significantly. Copying-based promotion is slightly more effective on superscalar processors than on single-issue processors, and the relative performance of remapping-based promotion on the two platform is application-dependent },
}

@inproceedings{5749714,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Fung, Wilson W. L. and Aamodt, Tor M.},
 year = {2011},
 pages = {25--36},
 publisher = {IEEE},
 title = {Thread block compaction for efficient SIMT control flow},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749714},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749714},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749714.pdf?arnumber=5749714},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Manycore accelerators such as graphics processor units (GPUs) organize processing units into single-instruction, multiple data \&#x201C;cores\&#x201D; to improve throughput per unit hardware cost. Programming models for these accelerators encourage applications to run kernels with large groups of parallel scalar threads. The hardware groups these threads into warps/wavefronts and executes them in lockstep-dubbed single-instruction, multiple-thread (SIMT) by NVIDIA. While current GPUs employ a per-warp (or per-wavefront) stack to manage divergent control flow, it incurs decreased efficiency for applications with nested, data-dependent control flow. In this paper, we propose and evaluate the benefits of extending the sharing of resources in a block of warps, already used for scratchpad memory, to exploit control flow locality among threads (where such sharing may at first seem detrimental). In our proposal, warps within a thread block share a common block-wide stack for divergence handling. At a divergent branch, threads are compacted into new warps in hardware. Our simulation results show that this compaction mechanism provides an average speedup of 22\% over a baseline per-warp, stack-based reconvergence mechanism, and 17\% versus dynamic warp formation on a set of CUDA applications that suffer significantly from control flow divergence. },
}

@inproceedings{5749715,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Ghasemi, Hamid Reza and Draper, Stark C. and Kim, Nam Sung},
 year = {2011},
 pages = {38--49},
 publisher = {IEEE},
 title = {Low-voltage on-chip cache architecture using heterogeneous cell sizes for high-performance processors},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749715},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749715},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749715.pdf?arnumber=5749715},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {To date dynamic voltage/frequency scaling (DVFS) has been one of the most successful power-reduction techniques. However, ever-increasing process variability reduces the reliability of static random access memory (SRAM) at low voltages. This limits voltage scaling to a minimum operating voltage (V<inf>DDMIN</inf>). Larger SRAM cells, that are less sensitive to process variability, allow the use of lower V<inf>DDMIN</inf>. However, large-scale memory structures, e.g., the last-level cache (LLC) (that often determines the V<inf>DDMIN</inf> of the processor), cannot afford to use such large SRAM cells due to the die area constraint. In this paper we propose low-voltage LLC architectures that exploit 1) the DVFS characteristics of workloads running on high-performance processors, 2) the trade-off between SRAM cell size and V<inf>DDMIN</inf>, and 3) the fact that at lower voltage/frequency operating states the negative performance impact of having a smaller LLC capacity is reduced. Our proposed LLC architectures provide the same maximum performance and V<inf>DDMIN</inf> as the conventional architecture, while reducing the total LLC cell area by 15\%\&#x2013;19\% with negligible average runtime increase. },
}

@inproceedings{5749716,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Smullen, Clinton W. and Mohan, Vidyabhushan and Nigam, Anurag and Gurumurthi, Sudhanva and Stan, Mircea R.},
 year = {2011},
 pages = {50--61},
 publisher = {IEEE},
 title = {Relaxing non-volatility for fast and energy-efficient STT-RAM caches},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749716},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749716},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749716.pdf?arnumber=5749716},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Spin-Transfer Torque RAM (STT-RAM) is an emerging non-volatile memory technology that is a potential universal memory that could replace SRAM in processor caches. This paper presents a novel approach for redesigning STT-RAM memory cells to reduce the high dynamic energy and slow write latencies. We lower the retention time by reducing the planar area of the cell, thereby reducing the write current, which we then use with CACTI to design caches and memories. We simulate quad-core processor designs using a combination of SRAM- and STT-RAM-based caches. Since ultra-low retention STT-RAM may lose data, we also provide a preliminary evaluation for a simple, DRAMstyle refresh policy. We found that a pure STT-RAM cache hierarchy provides the best energy efficiency, though a hybrid design of SRAM-based L1 caches with reduced-retention STT-RAM L2 and L3 caches eliminates performance loss while still reducing the energy-delay product by more than 70\%. },
}

@inproceedings{5749717,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Bhattacharjee, Abhishek and Lustig, Daniel and Martonosi, Margaret},
 year = {2011},
 pages = {62--63},
 publisher = {IEEE},
 title = {Shared last-level TLBs for chip multiprocessors},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749717},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749717},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749717.pdf?arnumber=5749717},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Translation Lookaside Buffers (TLBs) are critical to processor performance. Much past research has addressed uniprocessor TLBs, lowering access times and miss rates. However, as chip multiprocessors (CMPs) become ubiquitous, TLB design must be re-evaluated. This paper is the first to propose and evaluate shared last-level (SLL) TLBs as an alternative to the commercial norm of private, per-core L2 TLBs. SLL TLBs eliminate 7\&#x2013;79\% of system-wide misses for parallel workloads. This is an average of 27\% better than conventional private, per-core L2 TLBs, translating to notable runtime gains. SLL TLBs also provide benefits comparable to recently-proposed Inter-Core Cooperative (ICC) TLB prefetchers, but with considerably simpler hardware. Furthermore, unlike these prefetchers, SLL TLBs can aid sequential applications, eliminating 35\&#x2013;95\% of the TLB misses for various multiprogrammed combinations of sequential applications. This corresponds to a 21\% average increase in TLB miss eliminations compared to private, per-core L2 TLBs. Because of their benefits for parallel and sequential applications, and their readily-implementable hardware, SLL TLBs hold great promise for CMPs. },
}

@inproceedings{5749711,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Larus, James},
 year = {2011},
 pages = {1--1},
 publisher = {IEEE},
 title = {Keynote address I: Programming the cloud},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749711},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749711},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749711.pdf?arnumber=5749711},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Client \&#x002B; cloud computing is a disruptive, new computing platform, combining diverse client devices \&#x2014; PCs, smartphones, sensors, and single-function and embedded devices \&#x2014; with the unlimited, on-demand computation and data storage offered by cloud computing services such as Amazon's AWS or Microsoft's Windows Azure. As with every advance in computing, programming is a fundamental challenge as client \&#x002B; cloud computing combines many difficult aspects of software development. Systems built for this world are inherently parallel and distributed, run on unreliable hardware, and must be continually available \&#x2014; a challenging programming model for even the most skilled programmers. How then do ordinary programmers develop software for the Cloud? },
}

@inproceedings{5749712,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Rangan, Krishna K. and Powell, Michael D. and Wei, Gu-Yeon and Brooks, David},
 year = {2011},
 pages = {3--14},
 publisher = {IEEE},
 title = {Achieving uniform performance and maximizing throughput in the presence of heterogeneity},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749712},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749712},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749712.pdf?arnumber=5749712},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Continued scaling of process technologies is critical to sustaining improvements in processor frequencies and performance. However, shrinking process technologies exacerbates process variations \&#x2014; the deviation of process parameters from their target specifications. In the context of multi-core CMPs, which are implemented to feature homogeneous cores, within-die process variations result in substantially different core frequencies. Exposing such process-variation induced heterogeneity interferes with the norm of marketing chips at a single frequency. Further, application performance is undesirably dictated by the frequency of the core it is running on. To work around these challenges, a single uniform frequency, dictated by the slowest core, is currently chosen as the chip frequency sacrificing the increased performance capabilities of cores that could operate at higher frequencies. In this paper, we propose choosing the mean frequency across all cores, in lieu of the minimum frequency, as the single-frequency to use as the chip sales frequency. We examine several scheduling algorithms implemented below the O/S in hardware/firmware that guarantee minimum application performance near that of the average frequency, by masking process-variation induced heterogeneity from the end-user. We show that our Throughput-Driven Fairness (TDF) scheduling policy improves throughput by an average of 12\% compared to a naive fairness scheme (round-robin) for frequency-sensitive applications. At the same time, TDF allows 98\% of chips to maintain minimum performance at or above 90\% of that expected at the mean frequency, providing a single uniform performance level to present for the chip. },
}

@inproceedings{5749713,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Ranjan, Rakesn and Latorre, Fernando and Marcuello, Pedro and Gonzalez, Antonio},
 year = {2011},
 pages = {15--24},
 publisher = {IEEE},
 title = {Fg-STP: Fine-Grain Single Thread Partitioning on Multicores},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749713},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749713},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749713.pdf?arnumber=5749713},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Power and complexity issues have led the microprocessor industry to shift to Chip Multiprocessors in order to be able to better utilize the additional transistors ensured by Moore's law. While parallel programs are going to be able to take most of the advantage of these CMPs, single thread applications are not equipped to benefit from them. In this paper we propose Fine-Grain Single-Thread Partitioning (Fg-STP), a hardware-only scheme that takes advantage of CMP designs to speedup single-threaded applications. Our proposal improves single thread performance by reconfiguring two cores with the aim of collaborating on the fetching and execution of the instructions. These cores are basically conventional out-of-order cores in which execution is orchestrated using a dedicated hardware that has minimum and localized impact on the original design of the cores. This approach partitions the code at instruction granularity and differs from previous proposals on the extensive use of dependence speculation, replication and communication. These features are combined with the ability to look for parallelism on large instruction windows without any software intervention (no re-compilation or profiling hints are needed). These characteristics allow Fg-STP to speedup single thread by 18\% and 7\% on average over similar hardware-only approaches like Core Fusion, on medium sized and small sized 2-core CMP respectively for Spec 2006 benchmarks. },
}

@inproceedings{5416632,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Xekalakis, P. and Cintra, M.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Handling branches in TLS systems with Multi-Path Execution},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416632},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416632},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416632.pdf?arnumber=5416632},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Hardware, History, Informatics, Microarchitecture, Moore's Law, Pollution, Program processors, Programming profession, Protocols, SPEC2000 Int benchmark suite, Yarn, branch prediction, code partitioning, hard-to-predict conditional branches, microarchitectural feature, multi-threading, multipath execution, parallel processing, parallel thread extraction, sequential applications, squashed threads reexecution, thread-level speculation, },
 abstract = {Thread-Level Speculation (TLS) has been proposed to facilitate the extraction of parallel threads from sequential applications. Most prior work on TLS has focused on architectural features directly related to supporting the main TLS operations. In this work we, instead, investigate how a common microarchitectural feature, namely branch prediction, interacts with TLS. We show that branch prediction for TLS is even more important than it is for sequential execution. Unfortunately, branch prediction for TLS systems is also inherently harder. Code partitioning and re-executions of squashed threads pollute the branch history making it harder for predictors to be accurate. We thus propose to augment the hardware, so as to accommodate Multi-Path Execution (MP) within the existing TLS protocol. Under the MP execution model, all paths following a number of hard-to-predict conditional branches are followed simultaneously. MP execution thus removes branches that would have been otherwise mispredicted, helping in this way the core to exploit more ILP. We show that, with only minimal hardware support, one can combine these two execution models into a unified one. Experimental results show that our combined execution model achieves speedups of up to 23.2\%, with an average of 9.2\%, over an existing state-of-the-art TLS system and speedups of up to 138 \%, with an average of 28.2\%, when compared with MP execution for a subset of the SPEC2000 Int benchmark suite. },
}

@inproceedings{5416633,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Jaejin Lee and Jun Lee and Sangmin Seo and Jungwon Kim and Seungkyun Kim and Sura, Z.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {COMIC&#x002B;&#x002B;: A software SVM system for heterogeneous multicore accelerator clusters},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416633},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416633},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416633.pdf?arnumber=5416633},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {APE, Ethernet networks, GPE, Hardware, Memory management, Multicore processing, Multiprocessor interconnection networks, Performance analysis, Proposals, Protocols, Software systems, Support vector machines, cache storage, general purpose processor element, hardware cache coherence, heterogeneous multicore accelerator clusters, heterogeneous multicore processor, hierarchical centralized release consistency, local memory, main memory, on-chip cache hierarchy, shared memory systems, software SVM system, software shared virtual memory system, },
 abstract = {In this paper, we propose a software shared virtual memory (SVM) system for heterogeneous multicore accelerator clusters with explicitly managed memory hierarchies. The target cluster consists of a single manager node and many compute nodes. The manager node contains a generalpurpose processor and larger main memory, and each compute node contains a heterogeneous multicore processor and smaller main memory. These nodes are connected with an interconnection network, such as Gigabit Ethernet. The heterogeneous multicore processor in each compute node consists of a general-purpose processor element (GPE) and multiple accelerator processor elements (APEs). The GPE runs an OS and the multiple APEs are dedicated to compute-intensive workloads. The GPE is typically backed by a deep on-chip cache hierarchy and hardware cache coherence. On the other hand, the APEs have small explicitly-addressed local memory instead of caches. This APE local memory is not coherent with the main memory. Different main and local memory units in the accelerator cluster can be viewed as an explicitly managed memory hierarchy: global memory, node local memory, and APE local memory. Since coherence protocols of previous software SVM proposals cannot effectively handle such a memory hierarchy, we propose a new coherence and consistency protocol, called hierarchical centralized release consistency (HCRC). Our software SVM system is built on top of HCRC and software-managed caches. We evaluate the effectiveness and analyze the performance of our software SVM system on a 32-node heterogeneous multicore cluster (a total of 192 APEs). },
}

@inproceedings{5416630,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Abella, J. and Chaparro, P. and Vera, X. and Carretero, J. and Gonzalez, A.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {High-Performance low-vcc in-order core},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416630},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416630},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416630.pdf?arnumber=5416630},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Batteries, CMOS process, Clocks, Delay estimation, Energy efficiency, Frequency, Intel Silverthorne in-order core, Mobile handsets, Phase estimation, Random access memory, Voltage, energy-delay product, immediate read after write avoidance, low-Vcc in-order core, microprocessor chips, mobile platforms, operating frequency, power density, voltage 400 mV, voltage 500 mV, },
 abstract = {Power density grows in new technology nodes, thus requiring Vcc to scale especially in mobile platforms where energy is critical. This paper presents a novel approach to decrease Vcc while keeping operating frequency high. Our mechanism is referred to as immediate read after write (IRAW) avoidance. We propose an implementation of the mechanism for an Intel<sup>Â®</sup> Silverthorne<sup>TM</sup> in-order core. Furthermore, we show that our mechanism can be adapted dynamically to provide the highest performance and lowest energy-delay product (EDP) at each Vcc level. Results show that IRAW avoidance increases operating frequency by 57\% at 500mV and 99\% at 400mV with negligible area and power overhead (below 1\%), which translates into large speedups (48\% at 500mV and 90\% at 400mV) and EDP reductions (0.61 EDP at 500mV and 0.33 at 400mV). },
}

@inproceedings{5416631,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Libo Huang and Li Shen and Zhiying Wang and Wei Shi and Nong Xiao and Sheng Ma},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {SIF: Overcoming the limitations of SIMD devices via implicit permutation},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416631},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416631},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416631.pdf?arnumber=5416631},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Application software, Control systems, Data flow computing, Hardware, ISA, Instruction sets, Microprocessors, Pipelines, Process design, Registers, SIF, SIMD datapath, SIMD devices, SIMD interface framework, SIMD pipeline, Virtual colonoscopy, control flow, data reorganization, general purpose computer systems, general purpose computers, implicit permutation, logic design, memory alignment, microprocessor chips, microprocessor designs, multimedia applications, parallel processing, permutation vector register file, pipeline processing, scalar pipeline, single instruction multiple data, state setting instructions, },
 abstract = {SIMD devices have gained widespread acceptance in modern microprocessor designs for their superior performance for multimedia applications. However, there are three remaining limitations to the efficient utilization of SIMD devices in general-purpose computer systems: memory alignment, data reorganization and control flow. This paper presents SIF, an efficient SIMD interface framework that addresses these three shortcomings without modifying existing ISA. It is designed around a permutation vector register file (PVRF) and it adds new extended instructions to set internal permutation state in SIMD datapath rather than putting the permutation state setting bits in every instruction. The implicit permutation capability provided by PVRF results in zero overhead, which frees the handling of three limitations by using permutation instructions. To further reduce the state setting instructions in SIMD datapath, a technique that moves the workloads from SIMD pipeline into scalar pipeline is also introduced. With the help of proposed compilation algorithm, SIF can efficiently transform regular SIMD codes into SIF codes which make it easily integrated in all existing SIMD devices. We implemented these techniques in a vectorizing compiler and experimental results show that most of the permutation overhead instructions can be eliminated and distinct performance speedup can be achieved, which is 37\% higher than current SIMD techniques on average. },
}

@inproceedings{5416636,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Genbrugge, D. and Eyerman, S. and Eeckhout, L.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Interval simulation: Raising the level of abstraction in architectural simulation},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416636},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416636},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416636.pdf?arnumber=5416636},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Analytical models, Coherence, Discrete event simulation, FPGA, Field programmable gate arrays, M5 multicore simulator, Multicore processing, PARSEC benchmark suites, Performance analysis, Predictive models, Protocols, SPEC CPU2000, Sampling methods, Timing, abstraction level, architectural simulation, branch predictor, coherence protocol, core-level cycle-accurate simulation model, field programmable gate arrays, interconnection network, interval simulation, mechanistic analytical model, memory hierarchy, microarchitecture tradeoffs, microprocessor chips, multicore processor, simulated instruction stream sampling, },
 abstract = {Detailed architectural simulators suffer from a long development cycle and extremely long evaluation times. This longstanding problem is further exacerbated in the multi-core processor era. Existing solutions address the simulation problem by either sampling the simulated instruction stream or by mapping the simulation models on FPGAs; these approaches achieve substantial simulation speedups while simulating performance in a cycle-accurate manner. This paper proposes interval simulation which takes a completely different approach: interval simulation raises the level of abstraction and replaces the core-level cycle-accurate simulation model by a mechanistic analytical model. The analytical model estimates core-level performance by analyzing intervals, or the timing between two miss events (branch mispredictions and TLB/cache misses); the miss events are determined through simulation of the memory hierarchy, cache coherence protocol, interconnection network and branch predictor. By raising the level of abstraction, interval simulation reduces both development time and evaluation time. Our experimental results using the SPEC CPU2000 and PARSEC benchmark suites and the M5 multi-core simulator, show good accuracy up to eight cores (average error of 4.6\% and max error of 11\% for the multi-threaded full-system workloads), while achieving a one order of magnitude simulation speedup compared to cycle-accurate simulation. Moreover, interval simulation is easy to implement: our implementation of the mechanistic analytical model incurs only one thousand lines of code. Its high accuracy, fast simulation speed and ease-of-use make interval simulation a useful complement to the architect's toolbox for exploring system-level and high-level micro-architecture trade-offs. },
}

@inproceedings{5749719,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Mehrara, Mojtaba and Hsu, Po-Chun and Samadi, Mehrzad and Mahlke, Scott},
 year = {2011},
 pages = {87--98},
 publisher = {IEEE},
 title = {Dynamic parallelization of JavaScript applications using an ultra-lightweight speculation mechanism},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749719},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749719},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749719.pdf?arnumber=5749719},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {As the web becomes the platform of choice for execution of more complex applications, a growing portion of computation is handed off by developers to the client side to reduce network traffic and improve application responsiveness. Therefore, the client-side component, often written in JavaScript, is becoming larger and more compute-intensive, increasing the demand for high performance JavaScript execution. This has led to many recent efforts to improve the performance of JavaScript engines in the web browsers. Furthermore, considering the wide-spread deployment of multi-cores in today's computing systems, exploiting parallelism in these applications is a promising approach to meet their performance requirement. However, JavaScript has traditionally been treated as a sequential language with no support for multithreading, limiting its potential to make use of the extra computing power in multicore systems. In this work, to exploit hardware concurrency while retaining traditional sequential programming model, we develop ParaScript, an automatic runtime parallelization system for JavaScript applications on the client's browser. First, we propose an optimistic runtime scheme for identifying parallelizable regions, generating the parallel code on-the-fly, and speculatively executing it. Second, we introduce an ultra-lightweight software speculation mechanism to manage parallel execution. This speculation engine consists of a selective checkpointing scheme and a novel runtime dependence detection mechanism based on reference counting and range-based array conflict detection. Our system is able to achieve an average of 2.18\&#x00D7; speedup over the Firefox browser using 8 threads on commodity multi-core systems, while performing all required analyses and conflict detection dynamically at runtime. },
}

@inproceedings{5416634,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Hilton, A. and Roth, A.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {BOLT: Energy-efficient Out-of-Order Latency-Tolerant execution},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416634},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416634},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416634.pdf?arnumber=5416634},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {BOLT, Delay, Energy efficiency, Fasteners, ILP, Information science, LLC, MLP, Microarchitecture, Out of order, Performance gain, Software performance, Surface-mount technology, Yarn, better out-of-order latency-tolerance, cache performance, cache storage, candidate technique, energy-efficient out-of-order latency-tolerant execution, forward slices, future out-of-order cores, instruction level parallelism, last-level cache, memory architecture, memory-level parallelism, microarchitecture, parallel architectures, parallel memories, performance evaluation, performance gains, redundant LT, register file, slice buffer, slice buffer organization, software threads, tolerance analysis, },
 abstract = {LT (latency tolerant) execution is an attractive candidate technique for future out-of-order cores. LT defers the forward slices of LLC (last-level cache) misses to a slice buffer and re-executes them when the misses return. An LT core increases ILP without physically scaling the issue queue and register file and increases MLP without additional software threads that can reduce cache performance. Unfortunately, proposed LT designs are not energy efficient. They require too many additional structures and they defer and re-execute too many instructions to justify their performance gains. In this paper, we address these inefficiencies. We introduce a microarchitecture called BOLT (Better Out-of-Order Latency-Tolerance) that implements LT as an alternative use of SMT (Simultaneous Multi-Threading). We also present a new slice buffer organization and traversal scheme that increases performance and reduces overhead by pruning instances of useless and redundant LT. Collectively, these modifications turn out-of-order LT into a technique that improves performance in an energy-efficient way. },
}

@inproceedings{5416635,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Miller, J.E. and Kasture, H. and Kurian, G. and Gruenwald, C. and Beckmann, N. and Celio, C. and Eastep, J. and Agarwal, A.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Graphite: A distributed parallel simulator for multicores},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416635},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416635},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416635.pdf?arnumber=5416635},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Acceleration, Application software, Computational modeling, Computer architecture, Graphite, Hardware, Linux, Linux, Multicore processing, Open source software, Performance evaluation, Programming, fast design space exploration, lax synchronization, multicore processors, multimachine distribution, multiple commodity Linux machines, multiprocessing systems, open-source distributed parallel multicore simulator infrastructure, parallel processing, software development, source code modification, synchronisation, },
 abstract = {This paper introduces the Graphite open-source distributed parallel multicore simulator infrastructure. Graphite is designed from the ground up for exploration of future multi-core processors containing dozens, hundreds, or even thousands of cores. It provides high performance for fast design space exploration and software development. Several techniques are used to achieve this including: direct execution, seamless multicore and multi-machine distribution, and lax synchronization. Graphite is capable of accelerating simulations by distributing them across multiple commodity Linux machines. When using multiple machines, it provides the illusion of a single process with a single, shared address space, allowing it to run off-the-shelf pthread applications with no source code modification. Our results demonstrate that Graphite can simulate target architectures containing over 1000 cores on ten 8-core servers. Performance scales well as more machines are added with near linear speedup in many cases. Simulation slowdown is as low as 41Ã versus native execution. },
}

@inproceedings{569569,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {},
 year = {1997},
 publisher = {IEEE},
 title = {Proceedings Third International Symposium on High-Performance Computer Architecture},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569569},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569569},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569569.pdf?arnumber=569569},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {branch handling, communication-efficient cache architectures, computer architecture, computer architecture, high performance processors, instruction level parallelism, memory architecture, memory architecture, multiprocessing systems, network interfaces, network interfaces, performance evaluation, performance evaluation, routing, shared memory multiprocessors, },
 abstract = {The following topics were dealt with: novel memory architecture; routing and networks; instruction level parallelism and branch handling; shared memory multiprocessors; communication-efficient cache architectures; high performance processors; computer architecture research; performance evaluation and characterisation; and network interfaces },
}

@inproceedings{4658628,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Malik, K. and Agarwal, M. and Stone, S.S. and Woley, K.M. and Frank, M.I.},
 year = {2008},
 pages = {62--73},
 publisher = {IEEE},
 title = {Branch-mispredict level parallelism (BLP) for control independence},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658628},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658628},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658628.pdf?arnumber=4658628},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Data mining, Feedback, Flow graphs, Out of order, Process control, branch-mispredict level parallelism, control independence architecture, data dependence handling policy, dynamic execution trace, multi-threading, multithreading, parallelising compilers, program control structures, program diagnostics, spawn selection, },
 abstract = {A microprocessorpsilas performance is fundamentally limited by the rate at which it can resolve branch mispredictions. Control independence (CI) architectures look for useful control and data independent instructions to fetch and execute in the shadow of a branch misprediction. This paper demonstrates that CI architectures can be guided to exploit substantial branch-mispredict level parallelism (BLP) in existing control intensive applications. A program has branch-mispredict level parallelism when its dynamic execution trace contains hard-to-predict branches that are both control and data independent, and thus could, potentially, be resolved in parallel. Although applications have a high degree of inherent BLP, we find that the amount of BLP exploited by naive CI architectures tends to be quite small. We show that spawn selection and data dependence handling policies in a CI architecture should make choices that explicitly aim to maximize branch-mispredict level parallelism. We demonstrate that with BLP-focussed policies, CI architectures can expose high amounts of branch-mispredict level parallelism and achieve 50\% to 90\% improvements in IPC on several of the SPEC 2000 Integer benchmarks. },
}

@inproceedings{4658629,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Hongliang Gao and Yi Ma and Dimitrov, M. and Huiyang Zhou},
 year = {2008},
 pages = {74--85},
 publisher = {IEEE},
 title = {Address-branch correlation: A novel locality for long-latency hard-to-predict branches},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658629},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658629},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658629.pdf?arnumber=4658629},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Accuracy, Computer science, Costs, Data structures, Energy consumption, History, Microprocessors, Pattern analysis, Performance analysis, Yarn, address-branch correlation, data structures, data structures, instruction window processors, long-latency hard-to-predict branches, memory-intensive benchmarks, microprocessor chips, microprocessors, program execution behavior, substantial power-energy, widening speed gap, },
 abstract = {Hard-to-predict branches depending on long-latency cache-misses have been recognized as a major performance obstacle for modern microprocessors. With the widening speed gap between memory and microprocessors, such long-latency branch mispredictions also waste substantial power/energy in executing instructions on wrong paths, especially for large instruction window processors. This paper presents a novel program locality that can be exploited to handle long-latency hard-to-predict branches. The locality is a result of an interesting program execution behavior: for some applications, major data structures or key components of the data structures tend to remain stable for a long time. If a hard-to-predict branch depends on such stable data, the address of the data rather than the data value is sufficient to determine the branch outcome. This way, a misprediction can be resolved much more promptly when the data access results in a long-latency cache miss. We call such locality address-branch correlation and we show that certain memory-intensive benchmarks, especially those with heavy pointer chasing, exhibit this locality. We then propose a low-cost auxiliary branch predictor to exploit address-branch correlation. Our experimental results show that the proposed scheme reduces the execution time by 6.3\% (up to 27\%) and energy consumption by 5.2\% (up to 24\%) for a set of memory-intensive benchmarks with a 9 kB prediction table when used with a state-of-art 16 kB TAGE predictor. },
}

@inproceedings{4658620,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {},
 year = {2008},
 pages = {x--xii},
 publisher = {IEEE},
 title = {Information},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658620},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658620},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658620.pdf?arnumber=4658620},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Computer architecture, Energy storage, Irrigation, Jacobian matrices, Organizing, Program processors, Rivers, Sun, },
 abstract = {},
}

@inproceedings{4658621,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {},
 year = {2008},
 pages = {ix--ix},
 publisher = {IEEE},
 title = {Message from the General and Program Chairs},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658621},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658621},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658621.pdf?arnumber=4658621},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 abstract = {},
}

@inproceedings{4658622,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Rattner, J.},
 year = {2008},
 pages = {1--1},
 publisher = {IEEE},
 title = {Intel&#x2019;s Tera-scale Computing Project: The first five years, the next five years},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658622},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658622},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658622.pdf?arnumber=4658622},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Application software, Bandwidth, Computer architecture, Energy efficiency, Hardware, Intel, Parallel programming, Programming profession, Scalability, Software tools, Throughput, data mining, fixed-function accelerators, flexible cache, memory hierarchy, multicore architectures, parallel architectures, parallel programming, parallel programming, programmable cores, tera-scale computing, thread-aware execution environments, },
 abstract = {The Intel tera-scale computing research project is an effort to advance computing technology for the next decade. By scaling todaypsilas multi-core architectures to 10s and 100s of cores and embracing a shift to parallel programming, the goal is to enable applications and capabilities only dreamed of today. In his keynote Justin Rattner will talk about the hardware and software research vision for the program. He will address hardware challenges with scaling multi-core architectures to integrate programmable cores and fixed-function accelerators, flexible cache and memory hierarchy, and high bandwidth on-die networks to ensure high throughput. On the software front, he will talk about thread-aware execution environments to provide high scalability and energy-efficiency across the cores and parallel programming tools for mere mortal programmers. The talk will also highlight future applications like integrated real-time physics and visualization and non-textual media mining which along with many others benefit from high degrees of concurrency. },
}

@inproceedings{4658623,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Salapura, V. and Blumrich, M. and Gara, A.},
 year = {2008},
 pages = {5--14},
 publisher = {IEEE},
 title = {Design and implementation of the blue gene/P snoop filter},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658623},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658623},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658623.pdf?arnumber=4658623},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Degradation, Delay, Filtering, Filters, Frequency, Hardware, Multicore processing, Registers, SPLASH-2 benchmarks, Supercomputers, Wire, blue gene-P snoop filter, coherence traffic negative effects, computer architecture, filters, mainframes, multicore processors, parallel machines, snoop lookups, },
 abstract = {As multi-core processors evolve, coherence traffic between cores is becoming problematic, both in terms of performance and power. The negative effects of coherence (snoop) traffic can be significantly mitigated through snoop filtering. Shielding each cache with a device that can squash snoop requests for addresses known not to be in cache improves performance significantly for caches that cannot perform normal load and snoop lookups simultaneously. In addition, reducing snoop lookups yields power savings. This paper describes the design of the Blue Gene/P snoop filters, and presents hardware measurements to demonstrate their effectiveness. The Blue Gene/P snoop filters combine stream registers and snoop caches to capture both the locality of snoop addresses and their streaming behavior. Simulations of SPLASH-2 benchmarks illustrate tradeoffs and strengths of these two techniques. Their combination is shown to be most effective, eliminating 94-99\% of all snoop requests using very few stream registers and snoop cache lines. This translates into an average performance improvement of almost 20\% for the NAS benchmarks running on an actual Blue Gene/P system. },
}

@inproceedings{4658624,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Leigh, K. and Ranganathan, P. and Subhlok, J.},
 year = {2008},
 pages = {15--26},
 publisher = {IEEE},
 title = {Fabric convergence implications on systems architecture},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658624},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658624},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658624.pdf?arnumber=4658624},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Backplanes, Bandwidth, Blades, Convergence, Costs, Fabrics, I-O consolidation, Instruments, LAN interconnection, Protocols, Resource management, application-agnostic designs, cluster networking, fabric convergence implications, leverage ensemble-level resource sharing, pattern clustering, physical-layer similarities, protocols, systems architecture, },
 abstract = {Converged fabrics that support data, storage, and cluster networking in a unified fashion are desirable for their cost and manageability advantages. Recent trends towards higher-bandwidths in commodity networks, physical-layer similarities across different communication protocols, and the adoption of blade servers along with the corresponding availability of dasiabackplanespsila to implement new networking methods, motivate revisiting this idea. We discuss various aspects of fabric convergence, and present some evaluation results from our experiments in the context of a specific I/O consolidation case study. Based on the insights from these experiments, we discuss opportunities for future research - in new instrumentation and evaluation methods, new cross-layer and application-agnostic designs for fabric convergence solutions, and new system architectures that leverage ensemble-level resource sharing. Our goal, through the discussions in this position paper, is to initiate a more general examination of these issues in the broader academic community. },
}

@inproceedings{4658625,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Qian Diao and Song, J.},
 year = {2008},
 pages = {27--36},
 publisher = {IEEE},
 title = {Prediction of CPU idle-busy activity pattern},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658625},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658625},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658625.pdf?arnumber=4658625},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {CPU idle-busy activity pattern, Delay, Hardware, Hidden Markov models, Inference algorithms, Machine learning algorithms, Multicore processing, Packaging, Power system modeling, Prediction methods, Predictive models, dual-core processor, learning (artificial intelligence), machine learning prediction method, multicore processor, multiprocessing systems, power consumption reduction, processor idle time, quad-core processor, },
 abstract = {Real-world workloads rarely saturate multi-core processor. CPU C-states can be used to reduce power consumption during processor idle time. The key unsolved problem is: when and how to use which C-state. We propose a machine learning prediction method and usage model. We evaluate this model with idle traces collected on dual-core and quad-core processor, and find this method can well predict CPUpsilas activity pattern at the error level not exceeding 4\%. Compared with existing OS C-state policy, it results in 12\% additional CPU power saving and 2\% performance improvement. In industry, 12\% power saving for any processor is very significant improvement. SPECWeb (which we used consists of 3 different benchmarks - We consistently see doubledigit power saving) is representative ldquofront-endrdquo server workload - it takes \&gt;60\% DP server market segment share. },
}

@inproceedings{4658626,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Chang Joo Lee and Hyesoon Kim and Mutlu, O. and Patt, Y.N.},
 year = {2008},
 pages = {39--49},
 publisher = {IEEE},
 title = {Performance-aware speculation control using wrong path usefulness prediction},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658626},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658626},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658626.pdf?arnumber=4658626},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Computer architecture, Computer science, Costs, Energy consumption, Energy efficiency, Engines, Hardware, Performance loss, Pipelines, Prefetching, branch-count, correct-path operations, fetch gating mechanisms, multiprocessing systems, performance-aware speculation control, pipeline processing, prefetching benefits, processor pipeline, storage management, wasted energy consumption reduction, wrong path usefulness prediction, wrong-path usefulness predictor, },
 abstract = {Fetch gating mechanisms have been proposed to gate the processor pipeline to reduce the wasted energy consumption due to wrong-path (i.e. mis-speculated) instructions. These schemes assume that all wrong-path instructions are useless for processor performance and try to eliminate the execution of all wrong-path instructions. However, wrong-path memory references can be useful for performance by providing prefetching benefits for later correct-path operations. Therefore, eliminating wrong-path instructions without considering the usefulness of wrong-path execution can significantly reduce performance as well as increase overall energy consumption. This paper proposes a comprehensive, low-cost speculation control mechanism that takes into account the usefulness of wrong-path execution, while effectively reducing the energy consumption due to useless wrong-path instructions. One component of the mechanism is a simple, novel wrong-path usefulness predictor (WPUP) that can accurately predict whether or not wrong-path execution will be beneficial for performance. The other component is a novel branch-count based fetch gating scheme that requires very little hardware cost to detect if the processor is on the wrong path. The key idea of our speculation control mechanism is to gate the processor pipeline only if (1) the number of outstanding branches is above a dynamically-determined threshold and (2) the WPUP predicts that wrong-path execution will not be beneficial for performance. Our results show that our proposal eliminates most of the performance loss incurred by fetch gating mechanisms that assume wrong-path execution is useless, thereby both improving performance and reducing energy consumption while requiring very little (51- byte) hardware cost. },
}

@inproceedings{4658627,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Malik, K. and Agarwal, M. and Dhar, V. and Frank, M.I.},
 year = {2008},
 pages = {50--61},
 publisher = {IEEE},
 title = {PaCo: Probability-based path confidence prediction},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658627},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658627},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658627.pdf?arnumber=4658627},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Bandwidth, Counting circuits, Hardware, PaCo, Performance loss, Pipelines, Pollution, Proposals, RMS error, SMT fetch prioritization, Shift registers, State estimation, Surface-mount technology, estimation theory, fetching, multi-threading, path confidence estimate, path instruction, pipeline gating, pipeline processing, probability, probability-based path confidence prediction, processor goodpath likelihood, storage management, },
 abstract = {A path confidence estimate indicates the likelihood that the processor is currently fetching correct path instructions. Accurate path confidence prediction is critical for applications like pipeline gating and confidence-based SMT fetch prioritization. Previous work in this domain uses a threshold-and-count predictor, where the number of unresolved, low-confidence branches serves as an estimate of path confidence. This approach is inaccurate since it implicitly assumes that all low-confidence branches have the same mispredict rate, and that high-confidence branches never mispredict. We propose an alternative path confidence predictor designed from first principles, called PaCo, that directly estimates the probability that the processor is on the goodpath, and considers contributions from all branches, both high and low confidence. Even though it uses only modest hardware, PaCo can estimate the processorpsilas goodpath likelihood with very high accuracy, with an RMS error of 3.8\%. We show that PaCo significantly outperforms threshold-and-count predictors in pipeline gating and SMT fetch prioritization. In pipeline gating, while the best conventional predictor can reduce badpath instructions executed by 7\% with a small loss in performance, PaCo can reduce bad-path instructions by 32\% without any performance loss. In SMT fetch prioritization, using PaCo instead of conventional path confidence predictors improves performance by up to 23\%, and 5.5\% on average. },
}

@inproceedings{5416649,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Mingsong Bi and Crk, I. and Gniady, C.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {IADVS: On-demand performance for interactive applications},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416649},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416649},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416649.pdf?arnumber=5416649},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Accuracy, CPU performance level, CPU power, CPU usage, Degradation, Delay, Dynamic voltage scaling, Energy consumption, Energy efficiency, Energy management, Frequency, Graphical user interfaces, IADVS, Voltage control, desktop systems, energy consumption, energy efficient CPU, fine-grained interaction capture system, interaction-aware dynamic voltage scaling, interactive applications, interactive systems, interactive workloads, mobile system, on-demand performance, power aware computing, power management, power-hungry processors, prediction accuracy, user-interaction-based CPU energy management, },
 abstract = {Increasingly power-hungry processors have reinforced the need for aggressive power management. Dynamic voltage scaling has become a common design consideration allowing for energy efficient CPUs by matching CPU performance with the computational demand of running processes. In this paper, we propose Interaction-Aware Dynamic Voltage Scaling (IADVS), a novel fine-grained approach to managing CPU power during interactive workloads, which account for the bulk of the processing demand on modern mobile or desktop systems. IADVS is built upon a transparent, fine-grained interaction capture system. Able to track CPU usage for each user interface event, the proposed system sets the CPU performance level to the one that best matches the predicted CPU demand. Compared to the state-of-the-art approach of user-interaction-based CPU energy management, we show that IADVS improves prediction accuracy by 37\%, reduces processing delays by 17\%, and reduces energy consumed of the CPU by as much as 4\%. The proposed design is evaluated with both a detailed trace-based simulation as well as implementation on a real system, verifying the simulation findings. },
}

@inproceedings{501179,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Chunming Qiao and Yousong Mei},
 year = {1996},
 pages = {118--129},
 publisher = {IEEE},
 title = {On the multiplexing degree required to embed permutations in a class of networks with direct interconnects},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501179},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501179},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501179.pdf?arnumber=501179},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Circuits, Delay, Hardware, Hypercubes, Intelligent networks, Optical fiber networks, Optical interconnections, Path Multiplexing, Time division multiplexing, WDM networks, Wavelength division multiplexing, direct interconnects, embedding permutations, hypercubes, meshes, multiplexed optical interconnects, multiplexing, multiplexing, multiprocessor interconnection networks, optical interconnections, rings, tori, },
 abstract = {There are two approaches for establishing a connection in a network whose links are multiplexed with virtual channels. One is called Path Multiplexing (PM), in which the same channel has to be used on each link along a path, and the other is Link Multiplexing (LM), in which different channels may be used. We study the problem of embedding permutations in PM and LM, and in particular, determine the threshold (minimal) multiplexing degree needed for a network to be rearrangeably nonblocking. We found that PM and LM are equally effective in linear arrays, and LM is slightly more effective than PM in rings, meshes, tori and hypercubes. Our results suggest that PM may be more cost-effective in implementing networks with multiplexed optical interconnects },
}

@inproceedings{5416647,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Torrellas, Josep and Gropp, Bill and Sarkar, Vivek and Moreno, Jaime and Olukotun, Kunle},
 year = {2010},
 pages = {1--1},
 publisher = {IEEE},
 title = {Extreme scale computing: Challenges and opportunities},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416647},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416647},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416647.pdf?arnumber=5416647},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Concurrent computing, Energy consumption, Physics computing, },
 abstract = {An extreme scale system is one that is one thousand times more capable than a current comparable system, with the same power and physical footprint. Intuitively, this means that the power consumption and physical footprint of a current departmental server should be enough to deliver petascale performance, and that a single, commodity chip should deliver terascale performance. In this panel, we will discuss the resulting challenges in energy/power efficiency, concurrency and locality, resiliency and programmability, and the research opportunities that may take us to extreme scale systems. },
}

@inproceedings{5416646,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Mingsong Bi and Ran Duan and Gniady, C.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Delay-Hiding energy management mechanisms for DRAM},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416646},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416646},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416646.pdf?arnumber=5416646},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {DRAM, DRAM chips, Degradation, Delay, Energy consumption, Energy efficiency, Energy management, Hardware, I-O handling routines, Memory management, OS kernel, Operating systems, Power system management, Random access memory, buffer cache, cache storage, data-intensive applications, delay-hiding energy management mechanisms, energy consumption, high memory energy savings, memory state transition, operating system kernels, physical memory, power management, },
 abstract = {Current trends in data-intensive applications increase the demand for larger physical memory, resulting in the memory subsystem consuming a significant portion of system's energy. Furthermore, data-intensive applications heavily rely on a large buffer cache that occupies a majority of physical memory. Subsequently, we are focusing on the power management for physical memory dedicated to the buffer cache. Several techniques have been proposed to reduce energy consumption by transitioning DRAM into low-power states. However, transitions between different power states incur delays and may affect whole system performance. We take advantage of the I/O handling routines in the OS kernel to hide the delay incurred by the memory state transition so that performance degradation is minimized while maintaining high memory energy savings. Our evaluation shows that the best of the proposed mechanisms hides almost all transition latencies while only consuming 3\% more energy as compared to the existing on-demand mechanism, which can expose significant delays. },
}

@inproceedings{501174,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Iyengar, V.S. and Trevillyan, L.H. and Bose, P.},
 year = {1996},
 pages = {62--72},
 publisher = {IEEE},
 title = {Representative traces for processor models with infinite cache},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501174},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501174},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501174.pdf?arnumber=501174},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Costs, Hardware, High performance computing, Integrated circuit measurements, Manufacturing, Microarchitecture, Performance analysis, Power system modeling, Process design, R-metric, SMART, Throughput, benchmark programs, cache storage, dynamic instruction traces, graph-based heuristic, infinite cache, iterative design process, microprocessor chips, performance evaluation, performance evaluation, processor models, representative traces, timers, },
 abstract = {Performance evaluation of processor designs using dynamic instruction traces is a critical part of the iterative design process. The widening gap between the billions of instructions in such traces for benchmark programs and the throughput of timers performing the analysis in the tens of thousands of instructions per second has led to the use of reduced traces during design. This opens up the issue of whether these traces are truly representative of the actual workload in these benchmark programs. The first key result in this paper is the introduction of a new metric, called the R-metric, to evaluate the representativeness of these reduced traces when applied to a wide class of processor designs. The second key result, is the development of a novel graph-based heuristic to generate reduced traces based on the notions incorporated in the metric. These ideas have been implemented in a prototype system (SMART) for generating representative and reduced traces. Extensive experimental results are presented on various benchmarks to demonstrate the quality of the synthetic traces and the uses of the R-metric },
}

@inproceedings{501175,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Nayfeh, B.A. and Olukotun, K. and Singh, J.P.},
 year = {1996},
 pages = {74--84},
 publisher = {IEEE},
 title = {The impact of shared-cache clustering in small-scale shared-memory multiprocessors},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501175},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501175},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501175.pdf?arnumber=501175},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Application software, Bandwidth, Degradation, L2 cache, Laboratories, Multichip modules, Organizing, Performance gain, Printed circuits, Topology, Workstations, bus contention, cache storage, high-bandwidth, low-latency interconnections, memory system, multichip module, performance evaluation, processor performance, shared global bus, shared memory systems, shared-cache clustering, small-scale shared-memory multiprocessors, },
 abstract = {As processor performance continues to increase, greater demands are placed on the bus and memory systems of small-scale shared-memory multiprocessors. In this paper, we investigate how to reduce these demands by organizing groups of processors into clusters which are then connected together using a shared global bus. We take advantage of the high-bandwidth, low-latency interconnections available from multichip module (MCM) technology, to build clusters with multiple high-performance processors sharing an L2 cache. The use of MCM technology allows for significantly lower shared-cache access times, and higher shared cache to processor bandwidth, than is possible using printed circuit board (PCB) designs. Our results show that for an eight processor bus-based system, bus contention can be a large portion of the overall execution time, and that clustering can eliminate much or all of it. Clustering also tends to reduce read stall times due to shared working set effects and a reduction in the effect of communication misses. The same is true for two and four processor systems, although to a lesser extent. Overall, we find that clustering can result in significant performance gains for applications which heavily utilize the memory system },
}

@inproceedings{501172,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Farkas, K.I. and Jouppi, N.P. and Chow, P.},
 year = {1996},
 pages = {40--51},
 publisher = {IEEE},
 title = {Register file design considerations in dynamically scheduled processors},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501172},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501172},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501172.pdf?arnumber=501172},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Computer aided instruction, Costs, Dynamic scheduling, Educational institutions, Hardware, High performance computing, Out of order, Parallel processing, Processor scheduling, Registers, SPEC92 benchmarks, dispatch queue performance, dispatch queues, dynamically scheduled processors, file organisation, processor scheduling, register file design considerations, register file requirements, register renaming, },
 abstract = {We have investigated the register file requirements of dynamically scheduled processors using register renaming and dispatch queues running the SPEC92 benchmarks. We looked at processors capable of issuing either four or eight instructions per cycle and found that in most cases implementing precise exceptions requires a relatively small number of additional registers compared to imprecise exceptions. Systems with aggressive non-blacking load support were able to achieve performance similar to processors with perfect memory systems at the cost of some additional registers. Given our machine assumptions, we found that the performance of a four-issue machine with a 32-entry dispatch queue tends to saturate around 80 registers. For an eight-issue machine with a 64-entry dispatch queue performance does not saturate until about 128 registers. Assuming the machine cycle time is proportional to the register file cycle time, the 8-issue machine yields only 20\% higher performance than the 4-issue machine due in part to the cycle time impact of additional hardware },
}

@inproceedings{501173,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Govindarajan, R. and Altman, E.R. and Gao, G.R.},
 year = {1996},
 pages = {52--61},
 publisher = {IEEE},
 title = {Co-scheduling hardware and software pipelines},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501173},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501173},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501173.pdf?arnumber=501173},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Arithmetic, Cathode ray tubes, Computer architecture, Computer science education, Hardware, Hazards, Pipeline processing, Processor scheduling, Software performance, Sparc-20, Supercomputers, classical pipeline theory, collision vectors, coscheduling hardware/software pipelines, forbidden latencies, hardware pipeline reconfiguration, parallel architectures, performance evaluation, pipeline processing, reservation tables, simultaneous design, software pipeline scheduling, state diagrams, },
 abstract = {In this paper we propose co-scheduling, a framework for simultaneous design of hardware pipelines structures and software-pipelined schedules. Two important components of the co-scheduling framework are: (1) An extension to the analysis of hardware pipeline design that meets the needs of periodic (or software pipelined) schedules. Reservation tables, forbidden latencies, collision vectors, and state diagrams from classical pipeline theory are revisited and extended to solve the new problems. (2) An efficient method, based on the above extension of pipeline analysis, to perform (a) software pipeline scheduling and (b) hardware pipeline reconfiguration which are mutually ``compatible". The proposed method has been implemented and preliminary experimental results for 1008 kernel loops are reported. Co-scheduling successfully obtains a schedule for 95\% of these loops. The median time to obtain these schedules is 0.25 seconds on a Sparc-20 },
}

@inproceedings{501170,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Iftode, L. and Dubnicki, C. and Felten, E.W. and Kai Li},
 year = {1996},
 pages = {14--25},
 publisher = {IEEE},
 title = {Improving release-consistent shared virtual memory using automatic update},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501170},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501170},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501170.pdf?arnumber=501170},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Application software, Coherence, Computer networks, Computer science, Costs, Hardware, Network interfaces, Protocols, SHRIMP multicomputer, Software maintenance, Splash-2 applications, Workstations, automatic update, directory-based caches, lazy release consistency based protocol, local writes, memory protocols, network of computers, relaxed consistency models, release-consistent shared virtual memory, remote memory, software technique, virtual storage, },
 abstract = {Shared virtual memory is a software technique to provide shared memory on a network of computers without special hardware support. Although several relaxed consistency models and implementations are quite effective, there is still a considerable performance gap between the ``software-only" approach and the hardware approach that uses directory-based caches. Automatic update is a simple communication mechanism, implemented in the SHRIMP multicomputer, that forwards local writes to remote memory transparently. In this paper we propose a new lazy release consistency based protocol, called Automatic Update Release Consistency (AURC), that uses automatic update to propagate and merge shared memory modifications. We compare the performance of this protocol against a software-only LRC implementation on several Splash-2 applications and show that the AURC approach can substantially improve the performance of LRC. For 16 processors, the average speedup has increased from 5.9 under LRC to 8.3 under AURC },
}

@inproceedings{5416640,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Yi Xu and Bo Zhao and Youtao Zhang and Jun Yang},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Simple virtual channel allocation for high throughput and high frequency on-chip routers},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416640},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416640},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416640.pdf?arnumber=5416640},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Buffer storage, Channel allocation, Delay, Energy efficiency, Multiprocessor interconnection networks, Network-on-a-chip, Power engineering and energy, Radio spectrum management, Throughput, Virtual colonoscopy, buffer storage, high frequency on-chip routers, high throughput on-chip routers, multiprocessor interconnection networks, network routing, network-on-chip, on-chip interconnection networks, packet-switched network-on-chip, technology scaling, virtual channel allocation, virtual-channel buffers, },
 abstract = {Technology scaling has led to the integration of many cores into a single chip. As a result, on-chip interconnection networks start to play a more and more important role in determining the performance and power of the entire chip. Packet-switched network-on-chip (NoC) has provided a scalable solution to the communications for tiled multi-core processors. However the virtual-channel (VC) buffers in the NoC consume significant dynamic and leakage power of the system. To improve the energy efficiency of the router design, it is advantageous to use small buffer sizes while still maintaining throughput of the network. This paper proposes two new virtual channel allocation (VA) mechanisms, termed Fixed VC Assignment with Dynamic VC Allocation (FVADA) and Adjustable VC Assignment with Dynamic VC Allocation (AVADA). The idea is that VCs are assigned based on the designated output port of a packet to reduce the Head-of-Line (HoL) blocking. Also, the number of VCs allocated for each output port can be adjusted dynamically. Unlike previous buffer-pool based designs, we only use a small number of VCs to keep the arbitration latency low. Simulation results show that FVADA and AVADA can improve the network throughput by 41\% on average, compared to a baseline design with the same buffer size. AVADA can still outperform the baseline even when our buffer size is halved. Moreover, we are able to achieve comparable or better throughput than a previous dynamic VC allocator while reducing its critical path delay by 60\%. Our results prove that the proposed VA mechanisms are suitable for low-power, high-throughput, and high-frequency on-chip network designs. },
}

@inproceedings{1598130,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Sumeet Kumar and Aneesh Aggarwal},
 year = {2006},
 pages = { 212-- 221},
 publisher = {IEEE},
 title = {Reducing resource redundancy for concurrent error detection techniques in high performance microprocessors},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598130},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598130},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598130.pdf?arnumber=1598130},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { concurrent error detection,  error detection,  error recovery,  fault tolerant computing,  high performance microprocessor,  instruction sets,  microprocessor chips,  multi-threading,  parallel architectures,  redundant multithreading,  register bit reuse technique,  register value reuse technique,  resource allocation,  resource redundancy reduction, Buffer storage, Clocks, Electromagnetic transients, Hardware, Microprocessors, Redundancy, Registers, Resource management, Runtime, Yarn, },
 abstract = {With reducing feature size, increasing chip capacity, and increasing clock speed, microprocessors are becoming increasingly susceptible to transient (soft) errors. Redundant multi-threading (RMT) is an attractive approach for concurrent error detection and recovery. However, redundant threads significantly increase the pressure on the processor resources, resulting in dramatic performance impact. In this paper, we propose reducing resource redundancy as a means to mitigate the performance impact of redundancy. In this approach, all the instructions are redundantly executed, however, the redundant instructions do not use many of the resources used by an instruction. The approach taken to reduce resource redundancy is to exploit the runtime profile of the leading thread to optimally allocate resources to the trailing thread in a staggered RMT architecture. The key observation used in this approach is that, even with a small slack between the two threads, many instructions in the leading thread have already produced their results before their trailing counterparts are renamed. We investigate two techniques in this approach (i) register bits reuse technique that attempts to use the same register (but different bits) for both the copies of the same instruction, if the result produced by the instruction is of small size, and (ii) register value reuse technique that attempts to use the same register for a main instruction and a distinct redundant instruction, if both the instructions produce the same result. These techniques, along with some others, are used to reduce redundancy in register file, reorder buffer, and load/store buffer. The techniques are evaluated in terms of their performance, power, and vulnerability impact on an RMT processor. Our experiments show that the techniques achieve about 95\% performance improvement and about 17\% energy reduction. The vulnerability of the RMT remains the same with the techniques. },
}

@inproceedings{1598131,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Shi, W. and Fryman, J.B. and Gu, G. and Lee, H.-H.S. and Zhang, Y. and Yang, J.},
 year = {2006},
 pages = { 222-- 231},
 publisher = {IEEE},
 title = {InfoShield: a security architecture for protecting information usage in memory},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598131},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598131},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598131.pdf?arnumber=1598131},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { InfoShield,  Internet security,  cyber theft,  data privacy,  memory encryption,  network client-server application,  security architecture,  security of data,  storage management, Buffer overflow, Computer security, Cryptography, Hard disks, Hardware, IP networks, Information security, Nonvolatile memory, Protection, Web and internet services, },
 abstract = {Cyber theft is a serious threat to Internet security. It is one of the major security concerns by both network service providers and Internet users. Though sensitive information can be encrypted when stored in non-volatile memory such as hard disks, for many e-commerce and network applications, sensitive information is often stored as plaintext in main memory. Documented and reported exploits facilitate an adversary stealing sensitive information from an application's memory. These exploits include illegitimate memory scan, information theft oriented buffer overflow, invalid pointer manipulation, integer overflow, password stealing Trojans and so forth. Today's computing system and its hardware cannot address these exploits effectively in a coherent way. This paper presents a unified and lightweight solution, called InfoShield that can strengthen application protection against theft of sensitive information such as passwords, encryption keys, and other private data with a minimal performance impact. Unlike prior whole memory encryption and information flow based efforts, InfoShield protects the usage of information. InfoShield ensures that sensitive data are used only as defined by application semantics, preventing misuse of information. Comparing with prior art, InfoShield handles a broader range of information theft scenarios in a unified framework with less overhead. Evaluation using popular network client-server applications shows that InfoShield is sound for practical use and incurs little performance loss because InfoShield only protects absolute, critical sensitive information. Based on the profiling results, only 0.3\% of memory accesses and 0.2\% of executed codes are affected by InfoShield. },
}

@inproceedings{1598132,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Prvulovic, M.},
 year = {2006},
 pages = { 232-- 243},
 publisher = {IEEE},
 title = {CORD: cost-effective (and nearly overhead-free) order-recording and data race detection},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598132},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598132},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598132.pdf?arnumber=1598132},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { chip-multiprocessor,  cost-effective order-recording,  data race detection,  general-purpose processing,  microprocessor chips,  parallel programming,  parallel software,  program debugging,  synchronisation,  synchronization bug, Computer bugs, Costs, Degradation, Production, Programming profession, Software debugging, Software maintenance, Software performance, Vehicle dynamics, Writing, },
 abstract = {Chip-multiprocessors are becoming the dominant vehicle for general-purpose processing, and parallel software will be needed to effectively utilize them. This parallel software is notoriously prone to synchronization bugs, which are often difficult to detect and repeat for debugging. While data race detection and order-recording for deterministic replay are useful in debugging such problems, only order-recording schemes are lightweight, whereas data race detection support scales poorly and degrades performance significantly. This paper presents our CORD (cost-effective order-recording and data race detection) mechanism. It is similar in cost to prior order-recording mechanisms, but costs considerably less then prior schemes for data race detection. CORD also has a negligible performance overhead (0.4\% on average) and detects most dynamic manifestations of synchronization problems (77\% on average). Overall, CORD is fast enough to run always (even in performance-sensitive production runs) and provides the support programmers need to deal with the complexities of writing, debugging, and maintaining parallel software for future multi-threaded and multi-core machines. },
}

@inproceedings{1598133,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Huang, R. and Garg, A. and Huang, M.},
 year = {2006},
 pages = { 244-- 253},
 publisher = {IEEE},
 title = {Software-hardware cooperative memory disambiguation},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598133},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598133},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598133.pdf?arnumber=1598133},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { dynamic memory disambiguation logic,  energy efficiency,  floating-point application,  high-end processor,  load-store queue,  main memory,  memory architecture,  microarchitectural resource,  parallel architectures,  software-hardware cooperative memory disambiguation, Application software, Clocks, Computer aided instruction, Delay, Energy consumption, Hardware, High performance computing, Microarchitecture, Out of order, Resource management, },
 abstract = {In high-end processors, increasing the number of in-flight instructions can improve performance by overlapping useful processing with long-latency accesses to the main memory. Buffering these instructions requires a tremendous amount of microarchitectural resources. Unfortunately, large structures negatively impact processor clock speed and energy efficiency. Thus, innovations in effective and efficient utilization of these resources are needed. In this paper, we target the load-store queue, a dynamic memory disambiguation logic that is among the least scalable structures in a modern microprocessor. We propose to use software assistance to identify load instructions that are guaranteed not to overlap with earlier pending stores and prevent them from competing for the resources in the load-store queue. We show that the design is practical, requiring off-line analyses and minimum architectural support. It is also very effective, allowing more than 40\% of loads to bypass the load-store queue for floating-point applications. This reduces resource pressure and can lead to significant performance improvements. },
}

@inproceedings{1598134,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Moore, K.E. and Bobba, J. and Moravan, M.J. and Hill, M.D. and Wood, D.A.},
 year = {2006},
 pages = { 254-- 265},
 publisher = {IEEE},
 title = {LogTM: log-based transactional memory},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598134},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598134},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598134.pdf?arnumber=1598134},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { MOESI directory protocol,  SPLASH-2 benchmark,  cache storage,  cacheable virtual memory,  data version management,  log-based transactional memory,  memory architecture,  parallel architectures,  parallel programming,  parallel programming,  software library,  transaction processing,  virtual storage, Concurrent computing, Hardware, Memory management, Parallel programming, Proposals, Protocols, Software libraries, Software performance, Software systems, Yarn, },
 abstract = {Transactional memory (TM) simplifies parallel programming by guaranteeing that transactions appear to execute atomically and in isolation. Implementing these properties includes providing data version management for the simultaneous storage of both new (visible if the transaction commits) and old (retained if the transaction aborts) values. Most (hardware) TM systems leave old values "in place" (the target memory address) and buffer new values elsewhere until commit. This makes aborts fast, but penalizes (the much more frequent) commits. In this paper, we present a new implementation of transactional memory, log-based transactional memory (LogTM), that makes commits fast by storing old values to a per-thread log in cacheable virtual memory and storing new values in place. LogTM makes two additional contributions. First, LogTM extends a MOESI directory protocol to enable both fast conflict detection on evicted blocks and fast commit (using lazy cleanup). Second, LogTM handles aborts in (library) software with little performance penalty. Evaluations running micro- and SPLASH-2 benchmarks on a 32-way multiprocessor support our decision to optimize for commit by showing that only 1-2\% of transactions abort. },
}

@inproceedings{1598135,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Chung, J.W. and Chafi, H. and Minh, C.C. and McDonald, A. and Carlstrom, B. and Kozyrakis, C. and Olukotun, K.},
 year = {2006},
 pages = { 266-- 277},
 publisher = {IEEE},
 title = {The common case transactional behavior of multithreaded programs},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598135},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598135},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598135.pdf?arnumber=1598135},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { chip-multiprocessor system,  microprocessor chips,  multi-threading,  multiprocessing systems,  multithreaded program,  nonblocking synchronization,  parallel architectures,  parallel programming,  speculative parallelization,  synchronisation,  synchronization management,  transaction processing,  transactional memory, Application software, Computer aided software engineering, Concurrent computing, Frequency synchronization, Hardware, Parallel programming, Programming profession, Proposals, Robustness, System recovery, },
 abstract = {Transactional memory (TM) provides an easy-to-use and high-performance parallel programming model for the upcoming chip-multiprocessor systems. Several researchers have proposed alternative hardware and software TM implementations. However, the lack of transaction-based programs makes it difficult to understand the merits of each proposal and to tune future TM implementations to the common case behavior of real application. This work addresses this problem by analyzing the common case transactional behavior for 35 multithreaded programs from a wide range of application domains. We identify transactions within the source code by mapping existing primitives for parallelism and synchronization management to transaction boundaries. The analysis covers basic characteristics such as transaction length, distribution of read-set and write-set size, and the frequency of nesting and I/O operations. The measured characteristics provide key insights into the design of efficient TM systems for both non-blocking synchronization and speculative parallelization. },
}

@inproceedings{1598136,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Gontmakher, A. and Mendelson, A. and Schuster, A. and Shklover, G.},
 year = {2006},
 pages = { 278-- 287},
 publisher = {IEEE},
 title = {Speculative synchronization and thread management for fine granularity threads},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598136},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598136},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598136.pdf?arnumber=1598136},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { fine granularity thread,  instruction sets,  multi-threading,  multithreaded program,  parallel architectures,  speculative synchronization,  synchronisation,  system recovery mechanism,  thread management, Concurrent computing, Context modeling, Delay, Hardware, Multithreading, Parallel processing, Parallel programming, Technology management, Velocity measurement, Yarn, },
 abstract = {Performance of multithreaded programs is heavily influenced by the latencies of the thread management and synchronization operations. Improving these latencies becomes especially important when the parallelization is performed at fine granularity. In this work we examine the interaction of speculative execution with the thread-related operations. We develop a unified framework which allows all such operations to be executed speculatively and provides efficient recovery mechanisms to handle misspeculation of branches which affect instructions in several threads. The framework was evaluated in the context of Inthreads, a programming model designed for very fine grain parallelization. Our measurements show that the speedup obtained by speculative execution of the threads-related instructions can reach 25\%. },
}

@inproceedings{1598137,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Sharkey, J.J. and Ponomarev, D.V.},
 year = {2006},
 pages = { 288-- 298},
 publisher = {IEEE},
 title = {Efficient instruction schedulers for SMT processors},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598137},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598137},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598137.pdf?arnumber=1598137},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { dynamic scheduler design,  instruction packing,  instruction scheduler,  multi-threading,  parallel architectures,  processor scheduling,  simultaneous multithreading processor, Computer science, Delay, Dynamic scheduling, Process design, Processor scheduling, Scalability, Surface-mount technology, Throughput, Timing, Yarn, },
 abstract = {We propose dynamic scheduler designs to improve the scheduler scalability and reduce its complexity in the SMT processors. Our first design is an adaptation of the recently proposed instruction packing to SMT. Instruction packing opportunistically packs two instructions (possibly from different threads), each with at most one non-ready source operand at the time of dispatch, into the same issue queue entry. Our second design, termed 2OP\&#095;BLOCK, takes these ideas one step further and completely avoids the dispatching of the instructions with two non-ready source operands. This technique has several advantages. First, it reduces the scheduling complexity (and the associated delays) as the logic needed to support the instructions with 2 non-ready source operands is eliminated. More surprisingly, 2OP\&#095;BLOCK simultaneously improves the performance as the same issue queue entry may be reallocated multiple times to the instructions with at most one non-ready source (which usually spend fewer cycles in the queue) as opposed to hogging the entry with an instruction which enters the queue with two non-ready sources. For the schedulers with the capacity to hold 64 instructions, the 2OP\&#095;BLOCK design outperforms the traditional queue by 11\%, on the average, and at the same time results in a 10\% reduction in the overall scheduling delay. },
}

@inproceedings{744379,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Condon, A.E. and Hill, M.D. and Plakal, M. and Sorin, D.J.},
 year = {1999},
 pages = {270--278},
 publisher = {IEEE},
 title = {Using Lamport clocks to reason about relaxed memory models},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744379},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744379},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744379.pdf?arnumber=744379},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Alpha implementation, Alpha specifications, Clocks, Coherence, Databases, Design optimization, File servers, Hardware, Lamport clocks, Multiprocessing systems, Protocols, SGI Origin 2000-like directory protocol, SPARC Total Store Order, Sun, Sun Gigaplane-like split-transaction bus protocol, Systems engineering and theory, cache coherence protocols, first-in-first-out write buffer, formal verification, memory protocols, relaxed memory models, sequential consistency, shared memory systems, shared-memory multiprocessors, },
 abstract = {Cache coherence protocols of current shared-memory multiprocessors are difficult to verify. Our previous work proposed an extension of Lamport's logical clocks for showing that multiprocessors can implement sequential consistency (SC) with an SGI Origin 2000-like directory protocol and a Sun Gigaplane-like split-transaction bus protocol. Many commercial multiprocessors, however, implement more relaxed models, such as SPARC Total Store Order (TSO), a variant of processor consistency, and Compaq (DEC) Alpha, a variant of weak consistency. This paper applies Lamport clocks to both a TSO and an Alpha implementation. Both implementations are based on the same Sun Gigaplane-like split-transaction bus protocol we previously used, but the TSO implementation places a first-in-first-out write buffer between a processor and its cache, while the Alpha implementation uses a coalescing write buffer. Both write buffers satisfy read requests for pending writes (i.e., do bypassing) without requiring the write to be immediately written to cache. Analysis shows how to apply Lamport clocks to verify TSO and Alpha specifications at the architectural level },
}

@inproceedings{386530,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Sastry, R. and Ranganathan, N.},
 year = {1995},
 pages = {330--339},
 publisher = {IEEE},
 title = {A VLSI architecture for computing the tree-to-tree distance},
 date = {1995},
 doi = {10.1109/HPCA.1995.386530},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386530},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386530.pdf?arnumber=386530},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computer architecture, Computer languages, Concurrent computing, Costs, Dynamic programming, Error correction, Genetics, Hardware, Image analysis, VLSI, VLSI architecture, Very large scale integration, approximate tree matching, cluster analysis, deletions, dynamic programming, dynamic programming algorithm, edit distance determination, editing operations, error recovery, fixed size implementation, genetic sequence comparison, insertions, labeled ordered trees, minimum cost sequence, parallel algorithms, parallel systolic realization, partitioning strategies, pattern matching, predecessor-descendant relation, programming languages, scene analysis, substitutions, systolic arrays, tree-to-tree distance computation, trees (mathematics), },
 abstract = {The distance between two labeled ordered trees, \&alpha; and \&beta; is the minimum cost sequence of editing operations (insertions, deletions and substitutions, needed to transform or into \&beta; such that the predecessor-descendant relation between nodes and the ordering of nodes is not changed). Approximate tree matching has applications in genetic sequence comparison, scene analysis, error recovery and correction in programming languages, and cluster analysis. Edit distance determination is a computationally intensive task, and the design of special purpose hardware could result in a significant speed up. This paper describes in detail a VLSI architecture for computing the edit distance between arbitrary ordered trees, based on a parallel, systolic realization of the dynamic programming algorithm proposed by S.Y. Lu (1979). This architecture represents a significant improvement over that described by Sastry and Ranganathan (1994), which restricted the type of trees that could be processed by it. Two partitioning strategies to process trees of arbitrary sizes and structures on a fixed size implementation in multiple passes are proposed and analyzed },
}

@inproceedings{386533,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Govindarajan, R. and Nemawarkar, S.S. and LeNir, P.},
 year = {1995},
 pages = {298--307},
 publisher = {IEEE},
 title = {Design and performance evaluation of a multithreaded architecture },
 date = {1995},
 doi = {10.1109/HPCA.1995.386533},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386533},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386533.pdf?arnumber=386533},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computer architecture, Computer science, Data structures, Delay, Parallel processing, Runtime, Scheduling, Telecommunication traffic, Throughput, Yarn, cache storage, coarse-grain parallelism, communication complexity, data structure cache, discrete event simulation, discrete-event simulation, distributed data structure cache organization, fine-grain instruction level parallelism, memory latencies, multiple resident activations, multithreaded architecture, network latency, parallel architectures, performance evaluation, performance evaluation, processor throughput, synchronisation, unpredictable synchronization delays, virtual machines, },
 abstract = {Multithreaded architectures have the ability to tolerate long memory latencies and unpredictable synchronization delays. We propose a multithreaded architecture that is capable of exploiting both coarse-grain parallelism, and fine-grain instruction level parallelism in a program. Instruction-level parallelism is exploited by grouping instructions from a number of active threads at runtime. The architecture supports multiple resident activations to improve the extent of locality exploited. Further, a distributed data structure cache organization is proposed to reduce both the network: traffic and the latency in accessing remote locations. Initial performance evaluation using discrete-event simulation indicates that the architecture is capable of achieving very high processor throughput. The introduction of the data structure cache reduces the network latency significantly. The impact of various cache organizations on the performance of the architecture is also discussed in this paper },
}

@inproceedings{386532,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Kawano, T. and Kusakabe, S. and Taniguchi, R.-I. and Amamiya, M.},
 year = {1995},
 pages = {308--317},
 publisher = {IEEE},
 title = {Fine-grain multi-thread processor architecture for massively parallel processing},
 date = {1995},
 doi = {10.1109/HPCA.1995.386532},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386532},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386532.pdf?arnumber=386532},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computer architecture, Concurrent computing, Datarol-II, Datarol-II processor architecture, Delay, Information systems, Parallel machines, Parallel processing, Pipelines, Reduced instruction set computing, Registers, Yarn, communication complexity, concurrency control, execution pipeline, fast context switching, fine-grain concurrent processes, fine-grain multi-thread processor architecture, implicit register load/store mechanism, load control mechanism, local memory access latency, massively parallel processing, memory access overhead, parallel architectures, pipeline processing, remote memory access, remote procedure call, two-level hierarchical memory system, },
 abstract = {Latency, caused by remote memory access and remote procedure call, is one of the most serious problems in massively parallel computers. In order to eliminate the processors' idle time caused by these latencies, processors must perform fast context switching among fine-grain concurrent processes. In this paper, we propose a processor architecture, called Datarol-II, that promotes efficient fine-grain multi-thread execution by performing fast context switching among fine-grain concurrent processes. In the Datarol-II processor, an implicit register load/store mechanism is embedded in the execution pipeline in order to reduce memory access overhead caused by context switching. In order to reduce local memory access latency, a two-level hierarchical memory system and a load control mechanism are also introduced. We describe the Datarol-II processor architecture, and show its evaluation results },
}

@inproceedings{386535,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Saulsbury, A. and Wilkinson, T. and Carter, J. and Landin, A.},
 year = {1995},
 pages = {276--285},
 publisher = {IEEE},
 title = {An argument for simple COMA},
 date = {1995},
 doi = {10.1109/HPCA.1995.386535},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386535},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386535.pdf?arnumber=386535},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Cities and towns, Computer architecture, Computer science, Costs, Hardware, Memory architecture, Memory management, Protocols, Random access memory, Software maintenance, automatic data migration, automatic data replication, cache line granularity, cache space allocation, cache storage, cache-only memory architecture machines, distributed virtual shared memory system, hardware complexity, memory architecture, page-granularity, parallel architectures, scalable shared memory multiprocessor architecture, shared memory coherence, shared memory systems, software layer, software multiprocessor architecture, },
 abstract = {We present design details and some initial performance results of a novel scalable shared memory multiprocessor architecture. This architecture features the automatic data migration and replication capabilities of cache-only memory architecture (COMA) machines, without the accompanying hardware complexity. A software layer manages cache space allocation at a page-granularity-similarly to distributed virtual shared memory (DVSM) systems, leaving simpler hardware to maintain shared memory coherence at a cache line granularity. By reducing the hardware complexity, the machine cost and development time are reduced. We call the resulting hybrid hardware and software multiprocessor architecture Simple COMA. Preliminary results indicate that the performance of Simple COMA is comparable to that of more complex contemporary all hardware designs },
}

@inproceedings{386534,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Kontothanassis, L.I. and Scott, M.L.},
 year = {1995},
 pages = {286--295},
 publisher = {IEEE},
 title = {Software cache coherence for large scale multiprocessors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386534},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386534},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386534.pdf?arnumber=386534},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Access protocols, Coherence, Computer science, Hardware, Large-scale systems, Microprocessors, Power engineering and energy, Power engineering computing, Software performance, Software testing, cache storage, cache-write policies, concurrency control, global physical address space, hardware coherence mechanisms, large scale multiprocessors, memory-mapped network interfaces, message passing, message-passing machines, shared memory systems, software cache coherence, software coherence protocol, },
 abstract = {Shared memory is an appealing abstraction for parallel programming. It must be implemented with caches in order to perform well, however and caches require a coherence mechanism to ensure that processors reference current data. Hardware coherence mechanisms for large-scale machines are complex and costly, but existing software mechanisms for message-passing machines have not provided a performance-competitive solution. We claim that an intermediate hardware option-memory-mapped network interfaces that support a global physical address space-can provide most of the performance benefits of hardware cache coherence. We present a software coherence protocol that runs on this class of machines and greatly narrows the performance gap between hardware and software coherence. We compare the performance of the protocol to that of existing software and hardware alternatives and evaluate the tradeoffs among various cache-write policies. We also observe that simple program changes can greatly improve performance. For the programs in our test suite and with the changes in place, software coherence is often faster and never more than 55\% slower than hardware coherence },
}

@inproceedings{386537,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Mckee, S.A. and Wulf, W.A.},
 year = {1995},
 pages = {253--262},
 publisher = {IEEE},
 title = {Access ordering and memory-conscious cache utilization},
 date = {1995},
 doi = {10.1109/HPCA.1995.386537},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386537},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386537.pdf?arnumber=386537},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Application software, Bandwidth, Computer applications, Computer science, Intel i860XR, Performance analysis, Prefetching, Random access memory, Software performance, Timing, Upper bound, access ordering, analytic models, benchmark timings, cache storage, memory bandwidth, memory performance, memory-conscious cache utilization, performance evaluation, storage management, },
 abstract = {As processor speeds increase relative to memory speeds, memory bandwidth is rapidly becoming the limiting performance, factor for many applications. Several approaches to bridging this performance gap have been suggested. This paper examines one approach, access ordering, and pushes its limits to determine bounds on memory performance. We present several access-ordering schemes, and compare their performance, developing analytic models and partially validating these with benchmark timings on the Intel i860XR },
}

@inproceedings{386536,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Anderson, C. and Baer, J.-L.},
 year = {1995},
 pages = {264--275},
 publisher = {IEEE},
 title = {Two techniques for improving performance on bus-based multiprocessors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386536},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386536},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386536.pdf?arnumber=386536},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Broadcasting, Coherence, Computer science, Delay, Illinois protocol, Parallel processing, Protocols, bus-based multiprocessors, cache storage, coherence block, communication complexity, false sharing, invalid blocks, memory latency, memory protocols, read broadcasting, read snarfing, reference patterns, sector caches, shared memory systems, snoopy cache coherence protocol, spatial locality, subblock, system buses, transfer block, },
 abstract = {We explore two techniques for reducing memory latency in bus-based multiprocessors. The first one, designed for sector caches, is a snoopy cache coherence protocol that uses a large transfer block to take advantage of spatial locality, while using a small coherence block (called a subblock to avoid false sharing). The second technique is read snarfing (or read broadcasting), in which all caches can acquire data transmitted in response to a read request to update invalid blocks in their own cache. We evaluated the two techniques by simulating 6 applications that exhibit a variety of reference patterns. We compared the performance of the new protocol against that of the Illinois protocol with both small and large block sizes and found that it was effective in reducing memory latency and providing more consistent, good results than the Illinois protocol with a given line size. Read snarfing also improved performance mostly for protocols that use large line sizes },
}

@inproceedings{386539,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Westerholz, K. and Honal, S. and Plankl, J. and Hafer, C.},
 year = {1995},
 pages = {234--242},
 publisher = {IEEE},
 title = {Improving performance by cache driven memory management},
 date = {1995},
 doi = {10.1109/HPCA.1995.386539},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386539},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386539.pdf?arnumber=386539},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computer architecture, Control systems, Costs, Hardware, Memory management, Operating systems, R4000 based UNIX workstation, Research and development, Runtime, SPECint92 benchmark suite, Tin, Workstations, cache driven memory management, cache storage, competitive memory hierarchy, direct mapped caches, multiprogramming, multiprogramming environment, performance degradation, performance improvement, set associative caches, shortest access time, virtual memory management, virtual storage, },
 abstract = {The efficient utilization of caches is crucial for a competitive memory hierarchy. Access times required by modern processors are continuously decreasing. Direct mapped caches provide the shortest access time. Using them yields reduced hardware costs and fast memory access but implies additional misses in the cache, resulting in performance degradation. Another source of conflicts is the addressing scheme if caches are physically addressed. For such caches, memory management affects cache utilization. Enhancements in virtual memory management as presented in this paper reduce cache misses by as much as 80\% for real-indexed caches. We developed three algorithms that use runtime information. All of them are suitable for direct-mapped and set associative caches. Applied to SPECint92 benchmark suite, we measured a performance improvement of 6.9\% in a multiprogramming environment for a R4000 based UNIX workstation. This figure also includes the overhead caused by the more complex memory management },
}

@inproceedings{386538,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Jesung Kim and Sang Lyul Min and Sanghoon Jeon and Byoungchu Ahn and Deog-Kyoon Jeong and Chong Sang Kim},
 year = {1995},
 pages = {243--252},
 publisher = {IEEE},
 title = {U-cache: a cost-effective solution to synonym problem},
 date = {1995},
 doi = {10.1109/HPCA.1995.386538},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386538},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386538.pdf?arnumber=386538},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {ATUM traces, Buffer storage, Degradation, Delay, Hardware, Performance evaluation, Physics computing, Software performance, U-cache, cache storage, cost-effective solution, minimal hardware addition, optimisation, page alignment, performance, performance evaluation, physically-indexed cache, reverse translation information, software optimization, virtual pages, },
 abstract = {This paper proposes a cost-effective solution to the synonym problem. In this proposed solution, a minimal hardware addition guarantees the correctness whereas the software counterpart helps improve the performance. The key to this proposed solution is an addition of a small physically-indexed cache called U-cache. The U-cache maintains the reverse translation information of the cache blocks that belong to un-aligned virtual pages only, where aligned means that the lower bits of the virtual page number match those of the corresponding physical page number. A U-cache, even with only one entry, ensures correct handling of synonyms. A simple software optimization in the form of page alignment, helps improve the performance. Performance evaluation based on ATUM traces shows that a U-cache, with only a few entries, performs almost as well as (in some cases outperforms) a fully-configured hardware-based solution when more than 95\% of the pages are aligned },
}

@inproceedings{744375,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Vaidya, A.S. and Sivasubramaniam, A. and Das, C.R.},
 year = {1999},
 pages = {236--243},
 publisher = {IEEE},
 title = {LAPSES: a recipe for high performance adaptive router design},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744375},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744375},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744375.pdf?arnumber=744375},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Adaptive algorithm, Costs, Degradation, Economic Storage implementation, Hardware, LAPSES, Look-Ahead routing, Mesh networks, Pipelines, Routing, Table lookup, Telecommunication traffic, Traffic control, computational complexity, fully adaptive algorithm, high performance adaptive router design, intelligent Path Selection, mesh network, meta-table routing, multiprocessor interconnection networks, network performance, path selection heuristics, performance degradation, performance evaluation, table lookup, traffic-sensitive path selection heuristics, },
 abstract = {Earlier research has shown that adaptive routing can help in improving network performance. However, it has not received adequate attention in commercial routers mainly due to the additional hardware complexity, and the perceived cost and performance degradation that may result from this complexity. These concerns can be mitigated if one can design a cost-effective router that can support adaptive routing. This paper proposes a three step recipe-Look-Ahead routing, intelligent Path Selection, and an Economic Storage implementation, called the LAPSES approach-for cost-effective high performance pipelined adaptive router design. The first step, look-ahead routing, reduces a pipeline stage in the router by making table lookup and arbitration concurrent. Next, three new traffic-sensitive path selection heuristics (LRU, LFU and MAX-CREDIT) are proposed to select one of the available alternate paths. Finally, two techniques for reducing routing table size of the adaptive router are presented. These are called meta-table routing and economical storage. The proposed economical storage needs a routing table with only 9 and 27 entries for two and three dimensional meshes, respectively. All these design ideas are evaluated on a (16\&times;16) mesh network via simulation. A fully adaptive algorithm and various traffic patterns are used to examine the performance benefits. Performance results show that the look-ahead design as well as the path selection heuristics boost network performance, while the economical storage approach turns out to be an ideal choice in comparison to full-table and meta-table options. We believe the router resulting from these three design enhancements can make adaptive routing a viable choice for interconnects },
}

@inproceedings{744377,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Dwarkadas, S. and Gharachorloo, K. and Kontothanassis, L. and Scales, D.J. and Scott, M.L. and Stets, R.},
 year = {1999},
 pages = {260--269},
 publisher = {IEEE},
 title = {Comparative evaluation of fine- and coarse-grain approaches for software distributed shared memory},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744377},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744377},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744377.pdf?arnumber=744377},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {AlphaServer SMPs, Application software, Cashmere, Costs, Delay, Hardware, Instruments, Protocols, Runtime, SMP-aware protocols, Shasta, coarse-grain approaches, distributed shared memory systems, fine-grain approaches, protocols, protocols, software distributed shared memory, software performance evaluation, symmetric multiprocessors, virtual memory hardware, virtual storage, },
 abstract = {Symmetric multiprocessors (SMPs) connected with low-latency networks provide attractive building blocks for software distributed shared memory systems. Two distinct approaches have been used: the fine-grain approach that instruments application loads and stores to support a small coherence granularity, and the coarse-grain approach based on virtual memory hardware that provides coherence at a page granularity. Fine-grain systems offer a simple migration path for applications developed on hardware multiprocessors by supporting coherence protocols similar to those implemented in hardware. On the other hand, coarse-grain systems can potentially provide higher performance through more optimized protocols and larger transfer granularities, while avoiding instrumentation overheads. Numerous studies have examined each approach individually, but major differences in experimental platforms and applications make comparison of the approaches difficult. This paper presents a detailed comparison of two mature systems, Shasta and Cashmere, representing the fine- and coarse-grain approaches, respectively. Both systems are tuned to run on the same commercially available, state-of-the-art cluster of AlphaServer SMPs connected via a Memory Channel network. As expected, our results show that Shasta provides robust performance for applications tuned for hardware multiprocessors, and can better tolerate fine-grain synchronization. In contrast, Cashmere is highly sensitive to fine-grain synchronization, but provides a performance edge for applications with coarse-grain behavior. Interestingly, we found that the performance gap between the systems can often be bridged by program modifications that address coherence and synchronization granularity. In addition, our study reveals some unexpected results related to the interaction of current compiler technology with application instrumentation, and the ability of SMP-aware protocols to avoid certain performance disadvantages of coarse-grain approaches },
}

@inproceedings{744376,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Plaat, A. and Bal, H.E. and Hofman, R.F.H.},
 year = {1999},
 pages = {244--253},
 publisher = {IEEE},
 title = {Sensitivity of parallel applications to large differences in bandwidth and latency in two-layer interconnects},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744376},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744376},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744376.pdf?arnumber=744376},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {AC generators, Application software, Bandwidth, Computer science, Delay, Electronic switching systems, Grid computing, Large-scale systems, NUMAs, Testing, Workstations, application performance, bandwidth, communication patterns, latency, multiprocessor interconnection networks, parallel programming, parallel programs, performance evaluation, remote memory access, two-layer interconnects, },
 abstract = {This paper studies application performance on systems with strongly non-uniform remote memory access. In current generation NUMAs the speed difference between the slowest and fastest link in an interconnect-the ``NUMA gap"-is typically less than an order of magnitude, and many conventional parallel programs achieve good performance. We study how different NUMA gaps influence application performance, up to and including typical wide-area latencies and bandwidths. We find that for gaps larger than those of current generation NUMAs, performance suffers considerably (for applications that were designed for a uniform access interconnect). For many applications, however, performance can be greatly improved with comparatively simple changes: traffic over slow links can be reduced by making communication patterns hierarchical-like the interconnect. We find that in four out of our six applications the size of the gap can be increased by an order of magnitude or more without severely impacting speedup. We analyze why the improvements are needed, why they work so well, and how much non-uniformity they can mask },
}

@inproceedings{5749731,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Lee, Hyunjin and Cho, Sangyeun and Childers, Bruce R.},
 year = {2011},
 pages = {219--230},
 publisher = {IEEE},
 title = {CloudCache: Expanding and shrinking private caches},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749731},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749731},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749731.pdf?arnumber=5749731},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The number of cores in a single chip multiprocessor is expected to grow in coming years. Likewise, aggregate on-chip cache capacity is increasing fast and its effective utilization is becoming ever more important. Furthermore, available cores are expected to be underutilized due to the power wall and highly heterogeneous future workloads. This trend makes existing L2 cache management techniques less effective for two problems: increased capacity interference between working cores and longer L2 access latency. We propose a novel scalable cache management framework called CloudCache that creates dynamically expanding and shrinking L2 caches for working threads with fine-grained hardware monitoring and control. The key architectural components of CloudCache are L2 cache chaining, inter- and intra-bank cache partitioning, and a performance-optimized coherence protocol. Our extensive experimental evaluation demonstrates that CloudCache significantly improves performance of a wide range of workloads when all or a subset of cores are occupied. },
}

@inproceedings{569649,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Falsafi, B. and Wood, D.A.},
 year = {1997},
 pages = {128--138},
 publisher = {IEEE},
 title = {Scheduling communication on an SMP node parallel machine},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569649},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569649},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569649.pdf?arnumber=569649},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Computer networks, Concurrent computing, Delay, Distributed computing, High-speed networks, Parallel machines, Processor scheduling, Protocols, SMP node parallel machine, Software performance, Workstations, distributed memory systems, distributed-memory parallel computers, high-speed networks, networks of workstations, parallel systems, performance, performance bottleneck, performance evaluation, processor scheduling, protocols, scheduling communication, software communication protocols, symmetric multiprocessor, synthetic microbenchmarks, workstations, },
 abstract = {Distributed-memory parallel computers and networks of workstations (NOWs) both rely on efficient communication over increasingly high-speed networks. Software communication protocols are often the performance bottleneck. Several current and proposed parallel systems address this problem by dedicating one general-purpose processor in a symmetric multiprocessor (SMP) node specifically for protocol processing. This scheduling convention reduces communication latency and increases effective bandwidth but also reduces the peak performance since the dedicated processor no longer performs computation. In this paper, we study a parallel machine with SMP nodes and compare two protocol processing policies: Fixed, which uses a dedicated protocol processor; and Floating, where all processors perform both computation and protocol processing. The results from synthetic microbenchmarks and five macrobenchmarks show that: (i) a dedicated protocol processor benefits light-weight protocols much more than heavy-weight protocols; (ii) fixed improves performance over Floating when communication becomes the bottleneck, which is more likely when the application is very communication-intensive, overheads are very high, or there are multiple (i.e., more than two) processors per node; (iii) a system with optimal cost-effectiveness is likely to include a dedicated protocol processor, at least for light-weight protocols },
}

@inproceedings{569646,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Kai Hwang and Choming Wang and Cho-Li Wang},
 year = {1997},
 pages = {106--115},
 publisher = {IEEE},
 title = {Evaluating MPI collective communication on the SP2, T3D, and Paragon multicomputers},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569646},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569646},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569646.pdf?arnumber=569646},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Bandwidth, Closed-form solution, Concurrent computing, Cray T3D, Delay effects, Hardware, IBM SP2, Intel Paragon, MPI collective communication evaluation, Message passing, Paragon multicomputers, STAP benchmark experiments, Scattering, Size measurement, Timing, Workstations, architectural support, closed-form expressions, message passing, multiprocessing systems, performance evaluation, startup latency, synchronisation, timing, timing performance, },
 abstract = {We evaluate the architectural support of collective communication operations on the IBM SP2, Cray T3D, and Intel Paragon. The MPI performance data are obtained from the STAP benchmark experiments jointly performed at the USC and HKU. The T3D demonstrated clearly the best timing performance in almost all collective operations. This is attributed to the special hardware built in the T3D for fast messaging and block data transfer. With hardwired barriers, the T3D performs the barrier synchronization in 3 \&mu;s at least 30 times faster than the SP2 or Paragon. The startup latency of collective operations increases either linearly or logarithmically in three multicomputers. For short messages, the SP2 outperforms the Paragon in the barrier, total exchange, scatter, and gather operations. Various collective operations with 64 KBytes per message over 64 nodes of the three machines can be completed in the time range (5.12 ms, 675 ms). The Paragon outperforms the SP2 in almost all collective operations with long messages. We have derived closed-form expressions to quantify the collective messaging times and aggregated bandwidth on all three machines. For total exchange with 64 nodes, the T3D, Paragon, and SP2 achieved an aggregated bandwidth of 1.745, 0.879, and 0.818 GBytes/s, respectively. These findings are useful to those who wish to predict the MPP performance or to optimize parallel applications by trade-offs between divided computation and collective communication },
}

@inproceedings{569647,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Lim, B.-H. and Heidelberger, P. and Pattnaik, P. and Snir, M.},
 year = {1997},
 pages = {116--127},
 publisher = {IEEE},
 title = {Message proxies for efficient, protected communication on SMP clusters},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569647},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569647},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569647.pdf?arnumber=569647},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Concurrent computing, Costs, Delay, Hardware, IBM Model G30 SMPs, Network interfaces, Packaging, Predictive models, Processor scheduling, Protection, Workstations, cache-miss latency, cache-update mechanism, custom hardware, digital simulation, interrupts, message proxies, multiprocessing systems, performance evaluation, performance model, protected communication, symmetric multiprocessor clusters, },
 abstract = {This research addresses the problem of providing efficient, protected communication in an SMP cluster without incurring the overhead of system calls or the cost of custom hardware. It analyzes an approach that uses an idle SMP processor to run a message proxy, a communication process that provides protected access to the network. We implement message proxy based communication between a pair of IBM Model G30 SMPs and analyze the resulting overheads. We derive a performance model that shows that cache-miss latency within an SMP influences message proxy performance significantly. Simulations of a suite of ten parallel applications demonstrate that message proxies match the performance of custom hardware for three of the ten applications, and are between 10-30\% slower for the other seven applications. A direct cache-update mechanism to reduce cache misses improves the performance of message proxies on communication-intensive programs by 7-25\%. We conclude that message proxies provide a viable alternative to custom hardware for protected communication },
}

@inproceedings{569645,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Wallace, S. and Bagherzadeh, N.},
 year = {1997},
 pages = {94--103},
 publisher = {IEEE},
 title = {Multiple branch and block prediction},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569645},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569645},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569645.pdf?arnumber=569645},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Accuracy, Bandwidth, Costs, Decoding, History, Microprocessors, Parallel processing, SPEC95 benchmark suite, Uncertainty, computer architecture, instruction fetch prediction, instruction fetching, instruction fetching mechanism, instruction sets, microprocessor, microprocessor chips, multiple branch and block prediction, },
 abstract = {Accurate branch prediction and instruction fetch prediction of a microprocessor are critical to achieve high performance. For a processor which fetches and executes multiple instructions per cycle, an accurate and high bandwidth instruction fetching mechanism becomes increasingly important to performance. Unfortunately, the relatively small basic block size exhibited in many general-purpose applications severely limits instruction fetching. In order to achieve a high fetching rate for wide-issue superscalars, a scalable method to predict multiple branches per block of sequential instructions is presented. Its accuracy is equivalent to a scalar two-level adaptive prediction. Also, to overcome the limitation imposed by control transfers, a scalable method to predict multiple blocks is presented. As a result, a two black, multiple branch prediction mechanism for a block width of 8 instructions achieves an effective fetching rate of 8 instructions per cycle on the SPEC95 benchmark suite },
}

@inproceedings{4147661,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Jeonghwan Choi and Youngjae Kim and Sivasubramaniam, A. and Srebric, J. and Qian Wang and Joonwon Lee},
 year = {2007},
 pages = {205--215},
 publisher = {IEEE},
 title = {Modeling and Managing Thermal Profiles of Rack-mounted Servers with ThermoStat},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346198},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147661},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147661.pdf?arnumber=4147661},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Cooling, Power system management, Power system modeling, Software design, System software, Temperature, Thermal conductivity, Thermal loading, Thermal management, ThermoStat, Thermostats, computational fluid dynamics, computational fluid dynamics, dynamic thermal management, file servers, processor designs, rack-mounted servers, temperature ramifications, temperature-aware computing, thermal management (packaging), thermal modeling tool, thermal profiles, },
 abstract = {High power densities and the implications of high operating temperatures on the failure rates of components are key driving factors of temperature-aware computing. Computer architects and system software designers need to understand the thermal consequences of their proposals, and develop techniques to lower operating temperatures to reduce both transient and permanent component failures. Tools for understanding temperature ramifications of designs have been mainly restricted to industry for studying packaging and cooling mechanisms, with little access to such toolsets for academic researchers. Developing such tools is an arduous task since it usually requires cross-cutting areas of expertise spanning architecture, systems software, thermodynamics, and cooling systems. Recognizing the need for such tools, there has been work on modeling temperatures of processors at the micro-architectural level which can be easily understood and employed by computer architects for processor designs. However, there is a dearth of such tools in the academic/research community for undertaking architectural/systems studies beyond a processor - a server box, rack or even a machine room. This paper presents a detailed 3-dimensional computational fluid dynamics based thermal modeling tool, called ThermoStat, for rack-mounted server systems. Using this tool, we model a 20 (each with dual Xeon processors) node rack-mounted server system, and validate it with over 30 temperature sensor measurements at different points in the servers/rack. We conduct several experiments with this tool to show how different load conditions affect the thermal profile, and also illustrate how this tool can help design dynamic thermal management techniques },
}

@inproceedings{4147660,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Puttaswamy, K. and Loh, G.H.},
 year = {2007},
 pages = {193--204},
 publisher = {IEEE},
 title = {Thermal Herding: Microarchitecture Techniques for Controlling Hotspots in High-Performance 3D-Integrated Processors},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346197},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147660},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147660.pdf?arnumber=4147660},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {3D power density, 3D-aware instruction scheduler allocation, 3D-integrated processors, 3D/thermal-aware microarchitecture, Communication system control, Delay, Encoding, Heat sinks, L1 data cache, Microarchitecture, Processor scheduling, Stacking, Temperature, Thermal loading, Wires, address memorization, branch target buffer, cache storage, microprocessor chips, on-chip communication, parallel architectures, processor scheduling, significance-partitioned datapath, system-on-chip, thermal herding, },
 abstract = {3D integration technology greatly increases transistor density while providing faster on-chip communication. 3D implementations of processors can simultaneously provide both latency and power benefits due to reductions in critical wires. However, 3D stacking of active devices can potentially exacerbate existing thermal problems. In this work, we propose a family of thermal herding techniques that (1) reduces 3D power density and (2) locates a majority of the power on the top die closest to the heat sink. Our 3D/thermal-aware microarchitecture contributions include a significance-partitioned datapath that places the frequently switching 16-bits on the top die, a 3D-aware instruction scheduler allocation scheme, an address memorization approach for the load and store queues, a partial value encoding for the L1 data cache, and a branch target buffer that exploits a form of frequent partial value locality in target addresses. Compared to a conventional planar processor, our 3D processor achieves a 47.9\% frequency increase which results in a 47.0\% performance improvement (min 7\%, max 77\% on individual benchmarks), while simultaneously reducing total power by 20\% (min 15\%, max 30\%). Without our thermal herding techniques, the worst-case 3D temperature increases by 17 degrees. With our thermal herding techniques, the temperature increase is only 12 degrees (29\% reduction in the 3D worst-case temperature increase) },
}

@inproceedings{4147663,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Alameldeen, A.R. and Wood, D.A.},
 year = {2007},
 pages = {228--239},
 publisher = {IEEE},
 title = {Interactions Between Compression and Prefetching in Chip Multiprocessors},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346200},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147663},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147663.pdf?arnumber=4147663},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Bandwidth, Degradation, Delay, Hardware, Microarchitecture, Pollution, Prefetching, Random access memory, cache compression, cache storage, chip multiprocessors, data compression, link compression, microprocessor chips, off-chip pin bandwidth, on-chip caches, stride-based hardware prefetching, system-on-chip, },
 abstract = {In chip multiprocessors (CMPs), multiple cores compete for shared resources such as on-chip caches and off-chip pin bandwidth. Stride-based hardware prefetching increases demand for these resources, causing contention that can degrade performance (up to 35\% for one of our benchmarks). In this paper, we first show that cache and link (off-chip interconnect) compression can increase the effective cache capacity (thereby reducing off-chip misses) and increase the effective off-chip bandwidth (reducing contention). On an 8-processor CMP with no prefetching, compression improves performance by up to 18\% for commercial workloads. Second, we propose a simple adaptive prefetching mechanism that uses cache compressions extra tags to detect useless and harmful prefetches. Furthermore, in the central result of this paper, we show that compression and prefetching interact in a strong positive way, resulting in combined performance improvement of 10-51\% for seven of our eight workloads },
}

@inproceedings{4147662,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Clark, N. and Hormati, A. and Yehia, S. and Mahlke, S. and Flautner, K.},
 year = {2007},
 pages = {216--227},
 publisher = {IEEE},
 title = {Liquid SIMD: Abstracting SIMD Hardware using Lightweight Dynamic Mapping},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346199},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147662},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147662.pdf?arnumber=4147662},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Acceleration, Application software, Computer aided instruction, Computer architecture, Costs, Delay, Hardware, Instruction sets, Liquid SIMD, Microprocessors, Performance gain, SIMD accelerator, baseline scalar instruction set, dynamic mapping, dynamic optimization, instruction set architecture, instruction sets, microprocessor design, parallel processing, },
 abstract = {Microprocessor designers commonly utilize SIMD accelerators and their associated instruction set extensions to provide substantial performance gains at a relatively low cost for media applications. One of the most difficult problems with using SIMD accelerators is forward migration to newer generations. With larger hardware budgets and more demands for performance, SIMD accelerators evolve with both larger data widths and increased functionality with each new generation. However, this causes difficult problems in terms of binary compatibility, software migration costs, and expensive redesign of the instruction set architecture. In this work, we propose Liquid SIMD to decouple the instruction set architecture from the SIMD accelerator. SIMD instructions are expressed using a processor's baseline scalar instruction set, and light-weight dynamic translation maps the representation onto a broad family of SIMD accelerators. Liquid SIMD effectively bypasses the problems inherent to instruction set modification and binary compatibility across accelerator generations. We provide a detailed description of changes to a compilation framework and processor pipeline needed to support this abstraction. Additionally, we show that the hardware overhead of dynamic optimization is modest, hardware changes do not affect cycle time of the processor, and the performance impact of abstracting the SIMD accelerator is negligible. We conclude that using dynamic techniques to map instructions onto SIMD accelerators is an effective way to improve computation efficiency, without the overhead associated with modifying the instruction set },
}

@inproceedings{4147665,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Qureshi, M.K. and Suleman, M.A. and Patt, Y.N.},
 year = {2007},
 pages = {250--259},
 publisher = {IEEE},
 title = {Line Distillation: Increasing Cache Capacity by Filtering Unused Words in Cache Lines},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346202},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147665},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147665.pdf?arnumber=4147665},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Circuits, Filtering, Lab-on-a-chip, Optimization, Organizing, Out of order, Proposals, Time factors, cache lines, cache organization, cache storage, line distillation, spatial filters, word filtering, },
 abstract = {Caches are organized at a line-size granularity to exploit spatial locality. However, when spatial locality is low, many words in the cache line are not used. Unused words occupy cache space but do not contribute to cache hits. Filtering these words can allow the cache to store more cache lines. We show that unused words in a cache line are unlikely to be accessed in the less recent part of the LRU stack. We propose line distillation (LDIS), a technique that retains only the used words and evicts the unused words in a cache line. We also propose distill cache, a cache organization to utilize the capacity created by LDIS. Our experiments with 16 memory-intensive benchmarks show that LDIS reduces the average misses for a 1MB 8-way L2 cache by 30\% and improves the average IPC by 12\% },
}

@inproceedings{4147664,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Eyerman, S. and Ecckhout, L.},
 year = {2007},
 pages = {240--249},
 publisher = {IEEE},
 title = {A Memory-Level Parallelism Aware Fetch Policy for SMT Processors},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346201},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147664},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147664.pdf?arnumber=4147664},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Artificial intelligence, Delay, Etching, Gas detectors, Ink, Resource management, Surface-mount technology, Tellurium, Tin, Yarn, fetch policy, memory-level parallelism, multi-threading, multiprocessing systems, parallel processing, simultaneous multithreading processor, storage management, },
 abstract = {A thread executing on a simultaneous multithreading (SMT) processor that experiences a long-latency load will eventually stall while holding execution resources. Existing long-latency load aware SMT fetch policies limit the amount of resources allocated by a staffed thread by identifying long-latency loads and preventing the given thread from fetching more instructions - and in some implementations, instructions beyond the long-latency load may even be flushed which frees allocated resources. This paper proposes an SMT fetch policy that hikes into account the available memory-level parallelism (MLP) in a thread. The key idea proposed in this paper is that in case of an isolated long-latency had. i.e. there is no MLP the thread should be prevented from allocating additional resources. However, in case multiple independent long-latency loads overlap, i.e., there is MLP the thread should allocate as many resources as needed in order to fully expose the available MLP. The proposed MLP-aware fetch policy achieves better performance for MLP-intensive threads on an SMT processor and achieves a better overall balance between performance and fairness than previously proposed fetch policies },
}

@inproceedings{4147667,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Yen, L. and Bobba, J. and Marty, M.R. and Moore, K.E. and Volos, H. and Hill, M.D. and Swift, M.M. and Wood, D.A.},
 year = {2007},
 pages = {261--272},
 publisher = {IEEE},
 title = {LogTM-SE: Decoupling Hardware Transactional Memory from Caches},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346204},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147667},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147667.pdf?arnumber=4147667},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Acceleration, Broadcasting, Decoding, Hardware, LogTM-SE, Memory management, Operating systems, Programming profession, Protocols, Yarn, cache arrays, cache storage, cache victimization, eager version management, hardware transactional memory system, operating system, per-thread memory log, },
 abstract = {This paper proposes a hardware transactional memory (HTM) system called LogTM Signature Edition (LogTM-SE). LogTM-SE uses signatures to summarize a transactions read-and write-sets and detects conflicts on coherence requests (eager conflict detection). Transactions update memory "in place" after saving the old value in a per-thread memory log (eager version management). Finally, a transaction commits locally by clearing its signature, resetting the log pointer, etc., while aborts must undo the log. LogTM-SE achieves two key benefits. First, signatures and logs can be implemented without changes to highly-optimized cache arrays because LogTM-SE never moves cached data, changes a blocks cache state, or flash clears bits in the cache. Second, transactions are more easily virtualized because signatures and logs are software accessible, allowing the operating system and runtime to save and restore this state. In particular, LogTM-SE allows cache victimization, unbounded nesting (both open and closed), thread context switching and migration, and paging },
}

@inproceedings{4147666,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {260--260},
 publisher = {IEEE},
 title = {Researching Novel Systems: To Instantiate, Emulate, Simulate, or Analyticate?},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346203},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147666},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147666.pdf?arnumber=4147666},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Analytical models, Buildings, Computational modeling, Computer architecture, Computer simulation, Emulation, Field programmable gate arrays, Hardware, Operating systems, Virtual prototyping, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04147666.png" border="0"> },
}

@inproceedings{4147669,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Jun Shao and Davis, B.T.},
 year = {2007},
 pages = {285--294},
 publisher = {IEEE},
 title = {A Burst Scheduling Access Reordering Mechanism},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346206},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147669},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147669.pdf?arnumber=4147669},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Aerospace industry, Bandwidth, Computational modeling, DRAM chips, Delay, Job shop scheduling, M5 simulator, Out of order, Processor scheduling, Read-write memory, SDRAM, SDRAM devices, SPEC CPU2000 benchmarks, Space exploration, access latency, burst scheduling access reordering mechanism, bus utilization, main memory access streams, order memory scheduling, scheduling, storage management, system buses, },
 abstract = {Utilizing the nonuniform latencies of SDRAM devices, access reordering mechanisms alter the sequence of main memory access streams to reduce the observed access latency. Using a revised M5 simulator with an accurate SDRAM module, the burst scheduling access reordering mechanism is proposed and compared to conventional in order memory scheduling as well as existing academic and industrial access reordering mechanisms. With burst scheduling, memory accesses to the same rows of the same banks are clustered into bursts to maximize bus utilization of the SDRAM device. Subject to a static threshold, memory reads are allowed to preempt ongoing writes for reduced read latency, while qualified writes are piggybacked at the end of bursts to exploit row locality in writes and prevent write queue saturation. Performance improvements contributed by read preemption and write piggybacking are identified. Simulation results show that burst scheduling reduces the average execution time of selected SPEC CPU2000 benchmarks by 21\% over conventional bank in order memory scheduling. Burst scheduling also outperforms Intel's patented out of order memory scheduling and the row hit access reordering mechanism by 11\% and 6\% respectively },
}

@inproceedings{4147668,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Venkataramani, G. and Roemer, B. and Solihin, Y. and Prvulovic, M.},
 year = {2007},
 pages = {273--284},
 publisher = {IEEE},
 title = {MemTracker: Efficient and Programmable Support for Memory Access Monitoring and Debugging},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346205},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147668},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147668.pdf?arnumber=4147668},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Application software, Buffer overflow, Computer bugs, Hardware, MemTracker, Memory management, Monitoring, Security, Software debugging, Software performance, Software tools, checkpointing, debugging checker, memory access debugging, memory access monitoring, monitoring checker, program debugging, programmable state transition table, software complexity, software metrics, storage management, system monitoring, },
 abstract = {Memory bugs are a broad class of bugs that is becoming increasingly common with increasing software complexity, and many of these bugs are also security vulnerabilities. Unfortunately, existing software and even hardware approaches for finding and identifying memory bugs have considerable performance overheads, target only a narrow class of bugs, are costly to implement, or use computational resources inefficiently. This paper describes MemTracker, a new hardware support mechanism that can be configured to perform different kinds of memory access monitoring tasks. MemTracker associates each word of data in memory with a few bits of state, and uses a programmable state transition table to react to different events that can affect this state. The number of state bits per word, the events to which MemTracker reacts, and the transition table are all fully programmable. MemTracker's rich set of states, events, and transitions can be used to implement different monitoring and debugging checkers with minimal performance overheads, even when frequent state updates are needed. To evaluate MemTracker, we map three different checkers onto it, as well as a checker that combines all three. For the most demanding (combined) checker, we observe performance overheads of only 2.7\% on average and 4.8\% worst-case on SPEC 2000 applications. Such low overheads allow continuous (always-on) use of MemTracker-enabled checkers even in production runs },
}

@inproceedings{824349,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Jimenez, M. and Llaberia, J.M. and Fernandez, A.},
 year = {2000},
 pages = {183--194},
 publisher = {IEEE},
 title = {On the performance of hand vs. automatically optimized numerical codes},
 date = {2000},
 doi = {10.1109/HPCA.2000.824349},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824349},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824349.pdf?arnumber=824349},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {BLAS3, Feeds, Microarchitecture, Microprocessors, Multidimensional systems, Pipeline processing, RISC-BLAS library, Registers, Shape, Tiles, automatic-optimized codes, compiler techniques, compiler technology, hand-optimized codes, multilevel tiling, non-rectangular loop nests, numerical analysis, optimising compilers, optimized codes, software performance evaluation, },
 abstract = {In this paper, we compare automatic-optimized codes against hand-optimized codes. The automatic-optimized codes have been generated using our own developed tool that implements compiler techniques proposed in our previous work. Our compiler techniques focus on applying multilevel tiling to non-rectangular loop nests. This type of loop nests are commonly found in linear algebra algorithms, typically used in numerical codes. As hand-optimized codes, we use two different numerical libraries: the BLAS3 library provided by the manufacturers and the RISC-BLAS library proposed in Dayde and Duff (1998). Results will show how compiler technology can make it possible for non-rectangular loop nests to achieve as high performance as hand-optimized codes on modern microprocessors },
}

@inproceedings{824348,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Rajwar, R. and Kagi, A. and Goodman, J.R.},
 year = {2000},
 pages = {168--179},
 publisher = {IEEE},
 title = {Improving the throughput of synchronization by insertion of delays },
 date = {2000},
 doi = {10.1109/HPCA.2000.824348},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824348},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824348.pdf?arnumber=824348},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Application software, Costs, Delay, Hardware, IQOLB, Implicit QOLB, Instruction sets, Programming profession, Protocols, Scalability, Software performance, Throughput, concurrency control, delays, parallel performance, shared memory systems, shared-memory applications, speculation, synchronization, },
 abstract = {Efficiency of synchronization mechanisms can limit the parallel performance of many shared-memory applications. In addition, the ever increasing performance gap between processor and interprocessor communication may further compromise the scalability of these primitives. Ideally, synchronization primitives should provide high performance under both high and low contention without requiring substantial programmer effort and software support. QOLR has been shown to offer substantial speedups and to outperform other synchronization primitives consistently, but at the cost of software support and protocol complexity. This paper proposes the use of speculation and delays to implement a purely hardware-based queueing mechanism called Implicit QOLB. Making use of the pervasiveness of the Load-Linked/Store-Conditional primitives, we present a series of hardware mechanisms to optimize performance for sharing patterns exhibited by locks and associated data. The mechanisms do not require any change to existing software or instruction sets. IQOLB sits alongside the cache-coherence protocol and guides the decisions the protocol makes with respect to lock (and associated data) transfers. Preliminary evaluations indicate that IQOLB may perform as well as, if not better than, QOLB without the additional software and protocol complexity },
}

@inproceedings{824347,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Kaxiras, S. and Young, C.},
 year = {2000},
 pages = {156--167},
 publisher = {IEEE},
 title = {Coherence communication prediction in shared-memory multiprocessors },
 date = {2000},
 doi = {10.1109/HPCA.2000.824347},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824347},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824347.pdf?arnumber=824347},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Access protocols, Delay, Distributed computing, Impedance, Logic testing, Parallel machines, Performance gain, Taxonomy, Time sharing computer systems, address-based, coherence communication prediction, instruction-based, latency-tolerating, parallel architectures, performance, prediction schemes, remote memory access, shared memory systems, shared-memory multiprocessors, },
 abstract = {Sharing patterns in shared-memory multiprocessors are the key to performance: uniprocessor latency-tolerating techniques such as out-of-order execution and non-blocking caches have proved unable to completely hide the latency of remote memory access. Recently proposed prediction mechanisms accelerate coherence protocols by guessing where data will be used next and forwarding them to potential users before they are requested. Prior work in such shared-memory prediction schemes resulted in address-based and instruction-based predictors. Our work innovates in three areas. First, we present a taxonomy of prediction schemes that includes all previously-proposed prediction schemes in a uniform space. Second, we show how statistical techniques from epidemiological screening and polygraph testing can be applied to better measure the effectiveness of sharing prediction schemes; earlier work had reported only the ratio of incorrect predictions to correct predictions but neglected the ratio of correct predictions to actual sharing. Third, we provide simulation results of the accuracy of a practical subset of the space of schemes in our taxonomy, then analyze which components of each scheme contribute the most to prediction accuracy. Through this process, we discovered prediction schemes more accurate than those previously proposed },
}

@inproceedings{824346,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Nanda, A.K. and Nguyen, A.-T. and Michael, M.M. and Joseph, D.J.},
 year = {2000},
 pages = {145--155},
 publisher = {IEEE},
 title = {High-throughput coherence controllers},
 date = {2000},
 doi = {10.1109/HPCA.2000.824346},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824346},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824346.pdf?arnumber=824346},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Access protocols, Bandwidth, Communication system control, Computer science, Costs, Electrical capacitance tomography, Engines, National electric code, Pipelines, Wire, cache storage, coherence controllers, distributed cache coherent, hardwired coherence controllers, multiple protocol engines, performance evaluation, pipelined protocol engines, pipelining, shared memory multiprocessors, shared memory systems, split request-response streams, },
 abstract = {Recent research shows that the occupancy of the coherence controllers is a major performance bottleneck for distributed cache coherent shared memory multiprocessors. In this paper we study three approaches to alleviating this problem in hardwired coherence controllers, namely, multiple protocol engines, pipelined protocol engines, and split request-response streams. Split request-response streams is an innovative contribution of this paper. The performance of pipelining in the context of coherence controllers has not been presented in the literature. Multiple protocol engines has not been studied in the context of hardwired controllers except for a study of ours and only to a limited extent. Using both commercial and scientific benchmarks on detailed simulation models, we present experimental results that show that each mechanism is highly effective at reducing controller occupancy by as much as 66\% and improving execution time by as much as 51\%, for applications with high communication bandwidth requirement. A combination of mechanisms further reduces controller occupancy and execution time by as much as 78\% and 61\%, respectively. Our results show that applying any of the parallel mechanisms in the coherence controllers allows integrating four times as many processors per coherence controller, thus reducing system cost, while maintaining or even exceeding the performance of systems with larger number of coherence controllers },
}

@inproceedings{824345,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Canal, R. and Parcerisa, J.M. and Gonzalez, A.},
 year = {2000},
 pages = {133--142},
 publisher = {IEEE},
 title = {Dynamic cluster assignment mechanisms},
 date = {2000},
 doi = {10.1109/HPCA.2000.824345},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824345},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824345.pdf?arnumber=824345},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Computer aided instruction, Decoding, FP cluster, Hardware, Instruction sets, Irrigation, Logic, Microarchitecture, Proposals, Read-write memory, Registers, cluster assignment mechanisms, inter-cluster communication, microarchitectures, naive code partitioning, parallel architectures, resource allocation, run-time mechanisms, workload balance, },
 abstract = {Clustered microarchitectures are an effective approach to reducing the penalties caused by wire delays inside a chip. Current superscalar processors have in fact a two-cluster microarchitecture with a naive code partitioning approach: integer instructions are allocated to one cluster and floating-point instructions to the other. This partitioning scheme is simple and results in no communications between the two clusters (just through memory) but it is in general far from optimal because she workload is not evenly distributed most of the time. In fact, when the processor is running integer programs, the workload is extremely unbalanced since the FP cluster is not used at all. In this work we investigate run-time mechanisms that dynamically distribute the instructions of a program among these two clusters. By optimizing the trade-off between inter-cluster communication penalty and workload balance, the proposed schemes can achieve an average speed-up of 36\% for the SpecInt95 benchmark suite },
}

@inproceedings{824344,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Mowry, T.C. and Ramkissoon, S.R.},
 year = {2000},
 pages = {121--132},
 publisher = {IEEE},
 title = {Software-controlled multithreading using informing memory operations},
 date = {2000},
 doi = {10.1109/HPCA.2000.824344},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824344},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824344.pdf?arnumber=824344},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Concurrent computing, Delay, Electronic switching systems, Hardware, Microprocessors, Multithreading, Pipelines, Random access memory, Switches, Yarn, informing memory operations, multi-threading, shared memory systems, shared-memory multiprocessor, software-controlled, switch-on-miss multithreading, },
 abstract = {To help tolerate the latency of accessing remote data in a shared-memory multiprocessor, we explore a novel approach to switch-on-miss multithreading that is software-controlled rather than hardware-controlled. Our technique uses informing memory operations to trigger the thread switches with sufficiently low over-head that we observe speedups of 10\% or more for four out of seven applications, with one application speeding up by 14\%. By selectively applying register partitioning to reduce thread switching overhead, we can achieve further gains: e.g. an overall speedup of 23\% for FFT. Although this software-controlled approach does not match the performance of hardware-controlled schemes on multithreaded workloads, it requires substantially less hardware support than preview schemes and is nor likely to degrade single-thread performance. As remote memory accesses continue to become more expensive relative to software overheads, we expect software-controlled multithreading to become increasingly attractive in the future },
}

@inproceedings{824343,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Burns, J. and Gaudiot, J.-L.},
 year = {2000},
 pages = {109--120},
 publisher = {IEEE},
 title = {Quantifying the SMT layout overhead-does SMT pull its weight?},
 date = {2000},
 doi = {10.1109/HPCA.2000.824343},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824343},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824343.pdf?arnumber=824343},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Hardware, Instruction sets, Microarchitecture, Performance analysis, Performance evaluation, Performance gain, SMT layout overhead quantification, Silicon, Surface-mount technology, Throughput, Yarn, area overhead, chip area, hardware technique, instruction set architecture, instruction sets, interconnect level analysis, microarchitecture, microarchitecture issues, multi-threading, performance, performance evaluation, processor throughput, silicon overhead, simultaneous multithreading, },
 abstract = {Simultaneous Multi-Threading (SMT) is a hardware technique that increases processor throughput by issuing instructions simultaneously from multiple threads. However, while SMT can be added to an existing microarchitecture with relatively low overhead, this additional chip area could be used for other resources such as more functional units, larger caches or better branch predictors. How large is the SMT overhead, and at what point does SMT no longer pay off compared to adding other architecture features? This paper evaluates the silicon overhead of SMT by performing a transistor/interconnect level analysis of the layout. We discuss micro-architecture issues that impact SMT implementations, and show how the Instruction Set Architecture (ISA) and microarchitecture can have a large effect on the SMT overhead and performance. Results show that SMT yields large performance gains with small to moderate area overhead },
}

@inproceedings{824342,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Ki Hwan Yum and Vaidya, A. and Das, C.R. and Sivasubramaniam, A.},
 year = {2000},
 pages = {97--106},
 publisher = {IEEE},
 title = {Investigating QoS support for traffic mixes with the MediaWorm router},
 date = {2000},
 doi = {10.1109/HPCA.2000.824342},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824342},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824342.pdf?arnumber=824342},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Bandwidth, Channel allocation, Clocks, Integrated circuit interconnections, MediaWorm router, Personal communication networks, QoS support, Quality of service, Streaming media, Switching circuits, Telecommunication traffic, Traffic control, Virtual Clock, bandwidth allocation, high performance interconnects, high performance networks, jitter-free delivery, multimedia video streams, multiprocessor interconnection networks, network resources, performance evaluation, pipelined circuit switched router, quality of service, rate-based bandwidth allocation, traffic mixes, worm-hole switched routers, },
 abstract = {With the increasing use of clusters in real-time applications, it has become essential to design high performance networks with quality of service (QoS) guarantees. In this paper, we explore the feasibility of providing QoS in worm-hole switched routers, which are otherwise well known for designing high performance interconnects. In particular, we are interested in supporting multimedia video streams, in addition to the conventional best-effort traffic. The proposed MediaWorm router uses a rate-based bandwidth allocation mechanism, called Virtual Clock, to schedule network resources for different traffic classes. Our simulation results on an 8-port router indicate that it is possible to provide jitter-free delivery to VBR/CBR traffic up to an input load of 70-80\% of link bandwidth, and the presence of best effort traffic has no adverse effect on the real-time traffic. Although the MediaWorm router shows a slightly lower performance than a pipelined circuit switched (PCS) router, commercial success of worm-hole switching coupled with the simpler and cheaper design makes it an attractive alternative. Simulation of a (2\&times;2) fat-mesh using this router suggests that clusters designed with appropriate bandwidth balance between links can provide good performance for different types of traffic },
}

@inproceedings{824341,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Casado, R. and Bermudez, A. and Quiles, F.J. and Sanchez, J.L. and Duato, J.},
 year = {2000},
 pages = {85--96},
 publisher = {IEEE},
 title = {Performance evaluation of dynamic reconfiguration in high-speed local area networks},
 date = {2000},
 doi = {10.1109/HPCA.2000.824341},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824341},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824341.pdf?arnumber=824341},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Algorithm design and analysis, Degradation, Distributed computing, Local area networks, Network interfaces, Network topology, Routing, Switches, System recovery, Telecommunication traffic, component failures, concurrency control, deadlock, digital simulation, distributed reconfiguration algorithm, dynamic reconfiguration, high-speed local area networks, link remapping, local area networks, network interface card, performance evaluation, performance evaluation, point-to-point links, quality of service, simulation results, spanning-tree formation, system recovery, },
 abstract = {High-speed local area networks (LANs) consist of a set of switches connected by point-to-point links, and hosts linked to switches through a network interface card. High-speed LANs may change their topology due to switches and hosts being turned on/off, link remapping, and component failures. In these cases, a distributed reconfiguration algorithm analyzes the topology, computes the new routing tables, and downloads them to the corresponding switches. Unfortunately, in most cases, user traffic is stopped during the reconfiguration process to avoid deadlock. Although network reconfigurations are not frequent, static reconfiguration such as this may take hundreds of milliseconds to execute, thus degrading system availability significantly. In this paper, we propose a new deadlock-free distributed reconfiguration algorithm that is able to asynchronously update routing tables without stopping user traffic. This algorithm is valid for any topology, including regular as well as irregular topologies. Simulation results show that the behavior of our algorithm is significantly better than for other algorithms based on a spanning-tree formation },
}

@inproceedings{824340,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Li-Shiuan Peh and Dally, W.J.},
 year = {2000},
 pages = {73--84},
 publisher = {IEEE},
 title = {Flit-reservation flow control},
 date = {2000},
 doi = {10.1109/HPCA.2000.824340},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824340},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824340.pdf?arnumber=824340},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Decision support systems, buffers, channel bandwidth, computer networks, control flits, data latency, data networks, delays, digital simulation, flit-reservation flow control, onchip control wires, performance evaluation, propagation delay, simulations, virtual-channel flow control, },
 abstract = {This paper presents flit-reservation flow control, in which control flits traverse the network in advance of data flits, reserving buffers and channel bandwidth. Flit-reservation flow control requires control flits to precede data flits, which can be realized through fast on-chip control wires or the pipelining of control flits one or more cycles ahead of data flits. Scheduling ahead of data at rival enables buffers to be held only during actual buffer usage, unlike existing flow control methods. It also eliminates data latency due to routing and arbitration decisions. Simulations with fast control wires show that flit-reservation flow control extends the 63\% throughput attained by virtual-channel flow control with 8 flit buffers per input to 77\%, an improvement of 20\% with equal storage and bandwidth overheads. Its throughput with 6 buffers (77\%) approaches that of virtual-channel flow control using 16 buffers (80\%), reflecting the significant buffer savings as a result of efficient buffer utilization. Data latency is also reduced by 15.6\% as compared to virtual-channel flow control. The improvement in throughput is similarly realized by the pipelining of each control flit a cycle ahead of their data flits, using control and data networks with the same propagation delay of 1 cycle },
}

@inproceedings{995698,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Marcuello, P. and Gonzalez, A.},
 year = {2002},
 pages = { 55-- 64},
 publisher = {IEEE},
 title = {Thread-spawning schemes for speculative multithreading},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995698},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995698},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995698.pdf?arnumber=995698},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { 4-thread-unit processor,  8-cycle thread initialization penalty,  clustered speculative multithreaded processor,  heuristics,  loops,  multi-threading,  parallel architectures,  performance difference,  performance evaluation,  performance evaluation,  speculative multithreading,  subroutines,  thread-level parallelism,  thread-spawning schemes, Algorithms, Computer architecture, Hardware, Microarchitecture, Multithreading, Parallel processing, Performance analysis, Program processors, Proposals, Yarn, },
 abstract = {Speculative multithreading has been recently proposed to boost performance by means of exploiting thread-level parallelism in applications difficult to parallelize. The performance of these processors heavily depends on the partitioning policy used to split the program into threads. Previous work uses heuristics to spawn speculative threads based on easily-detectable program constructs such as loops or subroutines. In this work we propose a profile-based mechanism to divide programs into threads by searching for those parts of the code that have certain features that could benefit from potential thread-level parallelism. Our profile-based spawning scheme is evaluated on a Clustered Speculative Multithreaded Processor and results show large performance benefits. When the proposed spawning scheme is compared with traditional heuristics, we outperform them by almost 20\%. When a realistic value predictor and a 8-cycle thread initialization penalty is considered, the performance difference between them is maintained. The speed-up over a single thread execution is higher than 5x for a 16-thread-unit processor and close to 2x for a 4-thread-unit processor. },
}

@inproceedings{995699,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Steffan, J.G. and Colohan, C.B. and Zhai, A. and Mowry, T.C.},
 year = {2002},
 pages = { 65-- 75},
 publisher = {IEEE},
 title = {Improving value communication for thread-level speculation},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995699},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995699},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995699.pdf?arnumber=995699},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { automatic parallelization,  automatically transformed SPECint benchmarks,  compiler,  dynamic synchronization,  general-purpose programs,  hardware instruction prioritization,  multi-threading,  parallel thread execution,  performance,  silent stores,  software performance evaluation,  speculative threads,  synchronisation,  thread-level speculation,  throttled value prediction,  value communication, Application software, Computer science, Costs, Hardware, Microprocessors, Runtime, Sun, Throughput, Writing, Yarn, },
 abstract = {Thread-level speculation (TLS) allows us to automatically parallelize general-purpose programs by supporting parallel execution of threads that might not actually be independent. In this paper, we show that the key to good performance ties in the three different ways to communicate a value between speculative threads: speculation, synchronization and prediction. The difficult part is deciding how and when to apply each method. This paper shows how we can apply value prediction, dynamic synchronization and hardware instruction prioritization to improve value communication and hence performance in several SPECint benchmarks that have been automatically transformed by our compiler to exploit TLS. We find that value prediction can be effective when properly throttled to avoid the high costs of mis-prediction, while most of the gains of value prediction can be more easily achieved by exploiting silent stores. We also show that dynamic synchronization is quite effective for most benchmarks, while hardware instruction prioritization is not. Overall, we find that these techniques have great potential for improving the performance of TLS. },
}

@inproceedings{1183538,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Christodoulopoulou, R. and Azimi, R. and Bilas, A.},
 year = {2003},
 pages = { 203-- 214},
 publisher = {IEEE},
 title = {Dynamic data replication: an approach to providing fault-tolerant shared memory clusters},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183538},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183538},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183538.pdf?arnumber=1183538},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { SMP nodes,  SVM clusters,  SVM protocol,  computer network reliability,  dynamic data replication,  fault tolerant computing,  fault-tolerant shared memory clusters,  high-bandwidth interconnects,  local area networks,  low-latency interconnects,  node failures,  performance evaluation,  performance evaluation,  programming abstraction layer,  protocols,  reliability,  server systems,  shared memory systems,  shared virtual memory clusters, Availability, Buildings, Concurrent computing, Costs, Fault tolerance, Focusing, Monitoring, Operating systems, Protocols, Support vector machines, },
 abstract = {A challenging issue in today's server systems is to transparently deal with failures and application-imposed requirements for continuous operation. In this paper we address this problem in shared virtual memory (SVM) clusters at the programming abstraction layer. We design extensions to an existing SVM protocol that has been tuned for low-latency, high-bandwidth interconnects and SMP nodes and we achieve reliability through dynamic replication of application shared data and protocol information. Our extensions allow us to tolerate single (or multiple, but not simultaneous) node failures. We implement our extensions on a state-of-the-art cluster and we evaluate the common, failure-free case. We find that, although the complexity of our protocol is substantially higher than its failure-free counterpart, by taking advantage of architectural features of modern systems our approach imposes low overhead and can be employed for transparently dealing with system failures. },
}

@inproceedings{1183534,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Khailany, B. and Dally, W.J. and Rixner, S. and Kapasi, U.J. and Owens, J.D. and Towles, B.},
 year = {2003},
 pages = { 153-- 164},
 publisher = {IEEE},
 title = {Exploring the VLSI scalability of stream processors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183534},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183534},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183534.pdf?arnumber=1183534},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { ALU,  VLSI,  VLSI scalability,  application speedup,  energy dissipation,  floating point arithmetic,  floating-point units,  high-performance programmable processors,  intercluster scaling,  intracluster scaling,  kernel speedup,  microprocessor chips,  parallel architectures,  performance evaluation,  power consumption,  stream architectures,  stream processors, Application software, Arithmetic, Bandwidth, Computer architecture, Degradation, Kernel, Laboratories, Scalability, Streaming media, Very large scale integration, },
 abstract = {Stream processors are high-performance programmable processors optimized to run media applications. Recent work has shown these processors to be more area- and energy-efficient than conventional programmable architectures. This paper explores the scalability of stream architectures to future VLSI technologies where over a thousand floating-point units on a single chip will be feasible. Two techniques for increasing the number of ALU in a stream processor are presented: intracluster and intercluster scaling. These scaling techniques are shown to be cost-efficient to tens of ALU per cluster and to hundreds of arithmetic clusters. A 640-ALU stream processor with 128 clusters and 5 ALU per cluster is shown to be feasible in 45 nanometer technology, sustaining over 300 GOPS on kernels and providing 15.3\&times; of kernel speedup and 8.0\&times; of application speedup over a 40-ALU stream processor with a 2\% degradation in area per ALU and a 7\% degradation in energy dissipated per ALU operation. },
}

@inproceedings{1183535,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Slechta, B. and Crowe, D. and Fahs, N. and Fertig, M. and Muthler, G. and Quek, J. and Spadini, F. and Patel, S.J. and Lumetta, S.S.},
 year = {2003},
 pages = { 165-- 176},
 publisher = {IEEE},
 title = {Dynamic optimization of micro-operations},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183535},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183535},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183535.pdf?arnumber=1183535},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { SPECint 2000,  complex instruction set architectures,  dynamic optimization,  instruction sets,  instruction-level parallelism,  micro-operations,  microarchitectural substrate,  optimising compilers,  parallel architectures,  performance evaluation,  rePLay Framework,  redundancies,  redundancy,  trace-driven simulation, &times, 86 applications, Application software, Computer architecture, Decoding, Hardware, Instruction sets, Microarchitecture, Parallel processing, Performance evaluation, Redundancy, Registers, },
 abstract = {Inherent within complex instruction set architectures such as \&times;86 are inefficiencies that do not exist in a simpler ISA. Modern \&times;86 implementations decode instructions into one or more micro-operations in order to deal with the complexity of the ISA. Since these micro-operations are not visible to the compiler the stream of micro-operations can contain redundancies even in statically optimized \&times;86 code. Within a processor implementation, however barriers at the ISA level do not apply, and these redundancies can be removed by optimizing the micro-operation stream. In this paper we explore the opportunities to optimize code at the micro-operation granularity. We execute these micro-operation optimizations using the rePLay Framework as a microarchitectural substrate. Using a simple set of seven optimizations, including two that aggressively and speculatively attempt to remove redundant load instructions, we examine the effects of dynamic optimization of micro-operations using a trace-driven simulation environment. Simulation reveals that across a sampling of SPECint 2000 and real \&times;86 applications, rePLay is able to reduce micro-operation count by 21\% and, in particular load micro-operation count by 22\%. These reductions correspond to a boost in observed instruction-level parallelism on an 8-wide optimizing rePLay processor by 17\% over a non-optimizing configuration. },
}

@inproceedings{1183536,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Ibrahim, K.Z. and Byrd, G.T. and Rotenberg, E.},
 year = {2003},
 pages = { 179-- 190},
 publisher = {IEEE},
 title = {Slipstream execution mode for CMP-based multiprocessors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183536},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183536},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183536.pdf?arnumber=1183536},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { CMP-based multiprocessors,  DSM multiprocessors,  accurate reference stream,  application scalability,  cache storage,  coherence event acceleration,  distributed shared memory systems,  distributed shared-memory,  dual-processor chip multiprocessor nodes,  optimizations,  parallel tasks,  perceived overhead reduction,  performance,  performance evaluation,  prefetching,  self-invalidation,  shared L2 cache,  skipped operations,  slipstream execution mode, Acceleration, Character generation, Coherence, Concurrent computing, Degradation, Multiprocessing systems, Parallel processing, Performance gain, Prefetching, Scalability, },
 abstract = {Scalability of applications on distributed shared-memory (DSM) multiprocessors is limited by communication overheads. At some point, using more processors to increase parallelism yields diminishing returns or even degrades performance. When increasing concurrency is futile, we propose an additional mode of execution, called slipstream mode, that instead enlists extra processors to assist parallel tasks by reducing perceived overheads. We consider DSM multiprocessors built from dual-processor chip multiprocessor (CMP) nodes with shared L2 cache. A task is allocated on one processor of each CMP node. The other processor of each node executes a reduced version of the same task. The reduced version skips shared-memory stores and synchronization, running ahead of the true task. Even with the skipped operations, the reduced task makes accurate forward progress and generates an accurate reference stream, because branches and addresses depend primarily on private data. Slipstream execution mode yields two benefits. First, the reduced task prefetches data on behalf of the true task. Second, reduced tasks provide a detailed picture of future reference behavior, enabling a number of optimizations aimed at accelerating coherence events, e.g., self-invalidation. For multiprocessor systems with up to 16 CMP nodes, slipstream mode outperforms running one or two conventional tasks per CMP in 7 out of 9 parallel scientific benchmarks. Slipstream mode is 12-19\% faster with prefetching only and up to 29\% faster with self-invalidation enabled. },
}

@inproceedings{1183537,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Garzaran, M.J. and Prvulovic, M. and Llaberia, J.M. and Vinals, V. and Rauchwerger, L. and Torrellas, J.},
 year = {2003},
 pages = { 191-- 202},
 publisher = {IEEE},
 title = {Tradeoffs in buffering memory state for thread-level speculation in multiprocessors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183537},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183537},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183537.pdf?arnumber=1183537},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { architectural support,  buffering memory state,  cache storage,  complexity-benefit tradeoff analysis,  distributed caches,  hard-to-analyze code,  multi-threading,  multi-version speculative memory state,  multiple variable versions,  multiprocessing systems,  multiprocessors,  parallel architectures,  parallel code,  performance evaluation,  performance evaluation,  taxonomy,  thread-level speculation, Memory management, Merging, Pollution, Proposals, Taxonomy, },
 abstract = {Thread-level speculation provides architectural support to aggressively run hard-to-analyze code in parallel. As speculative tasks run concurrently, they generate unsafe or speculative memory state that needs to be separately buffered and managed in the presence of distributed caches and buffers. Such state may contain multiple versions of the same variable. In this paper, we introduce a novel taxonomy of approaches to buffering and managing multi-version speculative memory state in multiprocessors. We also present a detailed complexity-benefit tradeoff analysis of the different approaches. Finally, we use numerical applications to evaluate the performance of the approaches under a single architectural framework. Our key insights are that support for buffering the state of multiple speculative tasks and versions per processor is more complexity-effective than support for merging the state of tasks with main memory lazily. Moreover, both supports can be gainfully combined and, in large machines, their effect is nearly fully additive. Finally, the more complex support for future state in main memory can boost performance when buffers are under pressure, but hurts performance when squashes are frequent. },
}

@inproceedings{995696,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Semeraro, G. and Magklis, G. and Balasubramonian, R. and Albonesi, D.H. and Dwarkadas, S. and Scott, M.L.},
 year = {2002},
 pages = { 29-- 40},
 publisher = {IEEE},
 title = {Energy-efficient processor design using multiple clock domains with dynamic voltage and frequency scaling},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995696},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995696},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995696.pdf?arnumber=995696},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Intel XScale,  MediaBench,  Olden,  SPEC2000,  SimpleScalar,  Transmeta LongRun,  Wattch,  average energy-delay product improvement,  clock distribution,  clock frequency,  delays,  dynamic frequency scaling,  dynamic voltage scaling,  energy-efficient processor design,  feature size,  floating point units,  front end,  integer units,  integrated circuit design,  inter-domain synchronization costs,  load-store units,  logic simulation,  low-power electronics,  microprocessor chips,  multiple clock domains,  singly-clocked globally synchronous systems,  synchronisation,  wire delays, Clocks, Costs, Delay, Dynamic voltage scaling, Energy efficiency, Frequency conversion, Frequency synchronization, Potential energy, Process design, Wire, },
 abstract = {As clock frequency increases and feature size decreases, clock distribution and wire delays present a growing challenge to the designers of singly-clocked, globally synchronous systems. We describe an alternative approach, which we call a multiple clock domain (MCD) processor, in which the chip is divided into several clock domains, within which independent voltage and frequency scaling can be performed. Boundaries between domains are chosen to exploit existing queues, thereby minimizing inter-domain synchronization costs. We propose four clock domains, corresponding to the front end , integer units, floating point units, and load-store units. We evaluate this design using a simulation infrastructure based on SimpleScalar and Wattch. In an attempt to quantify potential energy savings independent of any particular on-line control strategy, we use off-line analysis of traces from a single-speed run of each of our benchmark applications to identify profitable reconfiguration points for a subsequent dynamic scaling run. Using applications from the MediaBench, Olden, and SPEC2000 benchmark suites, we obtain an average energy-delay product improvement of 20\% with MCD compared to a modest 3\% savings from voltage scaling a single clock and voltage system. },
}

@inproceedings{995697,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Cintra, M. and Torrellas, J.},
 year = {2002},
 pages = { 43-- 54},
 publisher = {IEEE},
 title = {Eliminating squashes through learning cross-thread violations in speculative parallelization for multiprocessors},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995697},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995697},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995697.pdf?arnumber=995697},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { 64-byte memory lines,  CC-NUMA,  coarse granularity,  cross-thread violations,  hardware mechanisms,  multiprocessing systems,  multiprocessors,  protocols,  speculative parallelization,  speculative thread-level parallelization,  synchronisation,  value prediction, Access protocols, Automatic control, Computer science, Data mining, Delay, Hardware, Informatics, Parallel processing, Resumes, Yarn, },
 abstract = {With speculative thread-level parallelization, codes that cannot be fully compiler-analyzed are aggressively executed in parallel. If the hardware detects a cross-thread dependence violation, it squashes offending threads and resumes execution. Unfortunately, frequent squashing cripples performance. This paper proposes a new framework of hardware mechanisms to eliminate most squashes due to data dependences in multiprocessors. The framework works by learning and predicting violations, and applying delayed-disambiguation, value prediction, and stall and release. The framework is suited for directory-based multiprocessors that track memory accesses at the system level with the coarse granularity of memory lines. Simulations of a 16-processor machine show that the framework is very effective. By adding our framework to a speculative CC-NUMA with 64-byte memory lines, we speed-up applications by an average of 4.3 times. Moreover, the resulting system is even 23\% faster than a machine that tracks memory accesses at the fine granularity of words-a sophisticated system that is not compatible with mainstream cache coherence protocols. },
}

@inproceedings{1183532,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Mutlu, O. and Stark, J. and Wilkerson, C. and Patt, Y.N.},
 year = {2003},
 pages = { 129-- 140},
 publisher = {IEEE},
 title = {Runahead execution: an alternative to very large instruction windows for out-of-order processors},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183532},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183532},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183532.pdf?arnumber=1183532},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { cache storage,  caches,  delays,  high performance processors,  instructions per cycle,  memory latency tolerance,  out-of-order processors,  parallel architectures,  performance evaluation,  runahead execution, Delay, Energy consumption, Engines, Hardware, Microprocessors, Out of order, Prefetching, Software, Trademarks, Windows, },
 abstract = {Today's high performance processors tolerate long latency operations by means of out-of-order execution. However, as latencies increase, the size of the instruction window must increase even faster if we are to continue to tolerate these latencies. We have already reached the point where the size of an instruction window that can handle these latencies is prohibitively large in terms of both design complexity and power consumption. And, the problem is getting worse. This paper proposes runahead execution as an effective way to increase memory latency tolerance in an out-of-order processor without requiring an unreasonably large instruction window. Runahead execution unblocks the instruction window blocked by long latency operations allowing the processor to execute far ahead in the program path. This results in data being prefetched into caches long before it is needed. On a machine model based on the Intel\&reg; Pentium\&reg; processor, having a 128-entry instruction window, adding runahead execution improves the IPC (instructions per cycle) by 22\% across a wide range of memory intensive applications. Also, for the same machine model, runahead execution combined with a 128-entry window performs within 1\% of a machine with no runahead execution and a 384-entry instruction window. },
}

@inproceedings{1183533,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Sakamoto, M. and Katsuno, A. and Inoue, A. and Asakawa, T. and Ueno, H. and Morita, K. and Kimura, Y.},
 year = {2003},
 pages = { 141-- 152},
 publisher = {IEEE},
 title = {Microarchitecture and performance analysis of a SPARC-V9 microprocessor for enterprise server systems},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183533},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183533},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183533.pdf?arnumber=1183533},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { SPARC-V9 microprocessor,  SPARC64 V,  enterprise server systems,  high-performance computing,  memory model,  microarchitecture,  microprocessor chips,  multiuser interactive workloads,  network servers,  performance analysis,  performance evaluation,  performance model,  pipeline processing, Availability, CMOS technology, Hardware, High performance computing, Internet, Microarchitecture, Microprocessors, Performance analysis, Process design, Web server, },
 abstract = {We developed a 1.3-GHz SPARC-V9 processor: the SPARC64 V. This processor is designed to address requirements for enterprise servers and high-performance computing. Processing speed under multiuser interactive workloads is very sensitive to system balance because of the large number of memory requests included. From many years of experience with such workloads in mainframe system developments, we give importance to design a well-balanced communication structure. To accomplish this task, a system-level performance study must begin at an early please. Therefore we developed a performance model, which consists of a detailed processor model and detailed memory model, before hardware design was started. We updated it continuously. Once a logic simulator became available, we used it to verify the performance model for improving its accuracy. The model quite effectively enabled us to achieve performance goals and finish development quickly. This paper describes the SPARC64 V microarchitecture and performance analyses for hardware design. },
}

@inproceedings{5749758,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Ansari, Amin and Feng, Shuguang and Gupta, Shantanu and Mahlke, Scott},
 year = {2011},
 pages = {539--550},
 publisher = {IEEE},
 title = {Archipelago: A polymorphic cache design for enabling robust near-threshold operation},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749758},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749758},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749758.pdf?arnumber=5749758},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Extreme technology integration in the sub-micron regime comes with a rapid rise in heat dissipation and power density for modern processors. Dynamic voltage scaling is a widely used technique to tackle this problem when high performance is not the main concern. However, the minimum achievable supply voltage for the processor is often bounded by the large on-chip caches since SRAM cells fail at a significantly faster rate than logic cells when reducing supply voltage. This is mainly due to the higher susceptibility of the SRAM structures to process-induced parameter variations. In this work, we propose a highly flexible fault-tolerant cache design, Archipelago, that by reconfiguring its internal organization can efficiently tolerate the large number of SRAM failures that arise when operating in the near-threshold region. Archipelago partitions the cache to multiple autonomous islands with various sizes which can operate correctly without borrowing redundancy from each other. Our configuration algorithm \&#x2014; an adapted version of minimum clique covering \&#x2014; exploits the high degree of flexibility in the Archipelago architecture to reduce the granularity of redundancy replacement and minimize the amount of space lost in the cache when operating in near-threshold region. Using our approach, the operational voltage of a processor can be reduced to 375mV, which translates to 79\% dynamic and 51\% leakage power savings (in 90nm) for a microprocessor similar to the Alpha 21364. These power savings come with a 4.6\% performance drop-off when operating in low power mode and 2\% area overhead for the microprocessor. },
}

@inproceedings{5749759,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {},
 year = {2011},
 pages = {551--553},
 publisher = {IEEE},
 title = {Author index},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749759},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749759},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749759.pdf?arnumber=5749759},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {},
}

@inproceedings{501185,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Park, H. and Agrawal, D.P.},
 year = {1996},
 pages = {191--200},
 publisher = {IEEE},
 title = {A topology-independent generic methodology for deadlock-free wormhole routing},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501185},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501185},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501185.pdf?arnumber=501185},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Adaptive algorithm, Algorithm design and analysis, Design methodology, Glass, Hypercubes, Mesh networks, Multiprocessor interconnection networks, Partitioning algorithms, Routing, System recovery, acyclic routing, arbitrary network, concurrency control, deadlock-free routing, deadlock-free wormhole routing, generic methodology, k-ary n-cube torus, multiprocessor interconnection networks, n-dimensional hypercube, n-dimensional mesh, network routing, subdigraph characteristics, subdigraphs, },
 abstract = {This paper introduces a generic methodology for developing deadlock-free routing in an arbitrary network by partitioning a graph into subdigraphs without cyclic dependencies and by strategically assigning virtual channels. We illustrate our scheme by identifying subdigraph characteristics that guarantee acyclic routing for n-dimensional hypercube, n-dimensional mesh and k-ary n-cube torus. Further generalization allows partial cyclic dependencies and forms a larger class of deadlock-free routing algorithms. We apply our technique to k-ary n-cube torus network and develop several novel deadlock-free, adaptive algorithms. Because our technique decomposes networks into several subdigraphs, it simplifies and generalizes the development of both static and adaptive deadlock-free routing algorithms for arbitrary networks },
}

@inproceedings{5749750,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Seznec, Andre},
 year = {2011},
 pages = {443--454},
 publisher = {IEEE},
 title = {Storage free confidence estimation for the TAGE branch predictor},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749750},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749750},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749750.pdf?arnumber=5749750},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {For the past 15 years, it has been shown that confidence estimation of branch prediction can be used for various usages such as fetch gating or throttling for power saving or for controlling resource allocation policies in a SMT processor. In many proposals, using extra hardware and particularly storage tables for branch confidence estimators has been considered as a worthwhile silicon investment. The TAGE predictor presented in 2006 is so far considered as the state-of-the-art conditional branch predictor. In this paper, we show that very accurate confidence estimations can be done for the branch predictions performed by the TAGE predictor by simply observing the outputs of the predictor tables. Many confidence estimators proposed in the literature only discriminate between high confidence predictions and low confidence predictions. It has been recently pointed out that a more selective confidence discrimination could useful. We show that the observation of the outputs of the predictor tables is sufficient to grade the confidence in the branch predictions with a very good granularity. Moreover a slight modification of the predictor automaton allows to discriminate the prediction in three classes, low-confidence (with a misprediction rate in the 30 \% range), medium confidence (with a misprediction rate in 8\&#x2013;12\% range) and high confidence (with a misprediction rate lower than 1 \%). },
}

@inproceedings{5749751,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Jiang, Xiaowei and Solihin, Yan},
 year = {2011},
 pages = {456--465},
 publisher = {IEEE},
 title = {Architectural framework for supporting operating system survivability},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749751},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749751},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749751.pdf?arnumber=5749751},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The ever increasing size and complexity of Operating System (OS) kernel code bring an inevitable increase in the number of security vulnerabilities that can be exploited by attackers. A successful security attack on the kernel has a profound impact that may affect all processes running on it. In this paper we propose an architectural framework that provides survivability to the OS kernel, i.e. able to keep normal system operation despite security faults. It consists of three components that work together: (1) security attack detection, (2) security fault isolation, and (3) a recovery mechanism that resumes normal system operation. Through simple but carefully-designed architecture support, we provide OS kernel survivability with low performance overheads (\&#x003C; 5\% for kernel intensive benchmarks). When tested with real world security attacks, our survivability mechanism automatically prevents the security faults from corrupting the kernel state or affecting other processes, recovers the kernel state and resumes execution. },
}

@inproceedings{5749752,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Yoon, Doe Hyun and Muralimanohar, Naveen and Chang, Jichuan and Ranganathan, Parthasarathy and Jouppi, Norman P. and Erez, Mattan},
 year = {2011},
 pages = {466--477},
 publisher = {IEEE},
 title = {FREE-p: Protecting non-volatile memory against both hard and soft errors},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749752},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749752},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749752.pdf?arnumber=5749752},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Emerging non-volatile memories such as phase-change RAM (PCRAM) offer significant advantages but suffer from write endurance problems. However, prior solutions are oblivious to soft errors (recently raised as a potential issue even for PCRAM) and are incompatible with high-level fault tolerance techniques such as chipkill. To additionally address such failures requires unnecessarily high costs for techniques that focus singularly on wear-out tolerance. In this paper, we propose fine-grained remapping with ECC and embedded pointers (FREE-p). FREE-p remaps fine-grained worn-out NVRAM blocks without requiring large dedicated storage. We discuss how FREE-p protects against both hard and soft errors and can be extended to chipkill. Further, FREE-p can be implemented purely in the memory controller, avoiding custom NVRAM devices. In addition to these benefits, FREE-p increases NVRAM lifetime by up to 26\% over the state-of-the-art even with severe process variation while performance degradation is less than 2\% for the initial 7 years. },
}

@inproceedings{5749753,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Qureshi, Moinuddin K. and Seznec, Andre and Lastras, Luis A. and Franceschini, Michele M.},
 year = {2011},
 pages = {478--489},
 publisher = {IEEE},
 title = {Practical and secure PCM systems by online detection of malicious write streams},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749753},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749753},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749753.pdf?arnumber=5749753},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Phase Change Memory (PCM) may become a viable alternative for the design of main memory systems in the next few years. However PCM suffers from limited write endurance. Therefore future adoption of PCM as a technology for main memory will depend on the availability of practical solutions for wear leveling that avoids uneven usage especially in the presence of potentially malicious users. First generation wear leveling algorithms were designed for typical workloads and have significantly reduced lifetime under malicious access patterns that try to write to the same line continuously. Secure wear leveling algorithms were recently proposed. They can handle such malicious attacks, but require that wear leveling is done at a rate that is orders of magnitude higher than what is sufficient for typical applications, thereby incurring significantly high write overhead, potentially impairing overall performance system. This paper proposes a practical wear-leveling framework that can provide years of lifetime under attacks while still incurring negligible (\&#x003C;1\%) write overhead for typical applications. It uses a simple and novel Online Attack Detector circuit to adapt the rate of wear leveling depending on the properties of the memory reference stream, thereby obtaining the best of both worlds \&#x2014; low overhead for typical applications and years of lifetime under attacks. The proposed attack detector requires a storage overhead of 68 bytes, is effective at estimating the severity of attacks, is applicable to a wide variety of wear leveling algorithms, and reduces the write overhead of several recently proposed wear leveling algorithms by 16x\&#x2013;128x. The paradigm of online attack detection enables other preventive actions as well. },
}

@inproceedings{5749754,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Sampson, Jack and Venkatesh, Ganesh and Goulding-Hotta, Nathan and Garcia, Saturnino and Swanson, Steven and Taylor, Michael Bedford},
 year = {2011},
 pages = {491--502},
 publisher = {IEEE},
 title = {Efficient complex operators for irregular codes},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749754},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749754},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749754.pdf?arnumber=5749754},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Complex \&#x201C;fat operators\&#x201D; are important contributors to the efficiency of specialized hardware. This paper introduces two new techniques for constructing efficient fat operators featuring up to dozens of operations with arbitrary and irregular data and memory dependencies. These techniques focus on minimizing critical path length and load-use delay, which are key concerns for irregular computations. Selective Depipelining(SDP) is a pipelining technique that allows fat operators containing several, possibly dependent, memory operations. SDP allows memory requests to operate at a faster clock rate than the datapath, saving power in the datapath and improving memory performance. Cachelets are small, customized, distributed L0 caches embedded in the datapath to reduce load-use latency. We apply these techniques to Conservation Cores(c-cores) to produce coprocessors that accelerate irregular code regions while still providing superior energy efficiency. On average, these enhanced c-cores reduce EDP by 2\&#x00D7; and area by 35\% relative to c-cores. They are up to 2.5\&#x00D7; faster than a general-purpose processor and reduce energy consumption by up to 8\&#x00D7; for a variety of irregular applications including several SPECINT benchmarks. },
}

@inproceedings{5749755,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Govindaraju, Venkatraman and Ho, Chen-Han and Sankaralingam, Karthikeyan},
 year = {2011},
 pages = {503--514},
 publisher = {IEEE},
 title = {Dynamically Specialized Datapaths for energy efficient computing},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749755},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749755},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749755.pdf?arnumber=5749755},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Due to limits in technology scaling, energy efficiency of logic devices is decreasing in successive generations. To provide continued performance improvements without increasing power, regardless of the sequential or parallel nature of the application, microarchitectural energy efficiency must improve. We propose Dynamically Specialized Datapaths to improve the energy efficiency of general purpose programmable processors. The key insights of this work are the following. First, applications execute in phases and these phases can be determined by creating a path-tree of basic-blocks rooted at the inner-most loop. Second, specialized datapaths corresponding to these path-trees, which we refer to as DySER blocks, can be constructed by interconnecting a set of heterogeneous computation units with a circuit-switched network. These blocks can be easily integrated with a processor pipeline. A synthesized RTL implementation using an industry 55nm technology library shows a 64-functional-unit DySER block occupies approximately the same area as a 64 KB single-ported SRAM and can execute at 2 GHz. We extend the GCC compiler to identify path-trees and code-mapping to DySER and evaluate the PAR-SEC, SPEC and Parboil benchmarks suites. Our results show that in most cases two DySER blocks can achieve the same performance (within 5\%) as having a specialized hardware module for each path-tree. A 64-FU DySER block can cover 12\% to 100\% of the dynamically executed instruction stream. When integrated with a dual-issue out-of-order processor, two DySER blocks provide geometric mean speedup of 2.1X (1.15X to 10X), and geometric mean energy reduction of 40\% (up to 70\%), and 60\% energy reduction if no performance improvement is required. },
}

@inproceedings{5749756,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Liu, Song and Leung, Brian and Neckar, Alexander and Memik, Seda Ogrenci and Memik, Gokhan and Hardavellas, Nikos},
 year = {2011},
 pages = {515--525},
 publisher = {IEEE},
 title = {Hardware/software techniques for DRAM thermal management},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749756},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749756},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749756.pdf?arnumber=5749756},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The performance of the main memory is an important factor on overall system performance. To improve DRAM performance, designers have been increasing chip densities and the number of memory modules. However, these approaches increase power consumption and operating temperatures: temperatures in existing DRAM modules can rise to over 95\&#x00B0;C. Another important property of DRAM temperature is the large variation in DRAM chip temperatures. In this paper, we present our analysis collected from measurements on a real system indicating that temperatures across DRAM chips can vary by over 10\&#x00B0;C. This work aims to minimize this variation as well as the peak DRAM temperature. We first develop a thermal model to estimate the temperature of DRAM chips and validate this model against real temperature measurements. We then propose three hardware and software schemes to reduce peak temperatures. The first technique introduces a new cache line replacement policy that reduces the number of accesses to the overheating DRAM chips. The second technique utilizes a Memory Write Buffer to improve the access efficiency of the overheated chips. The third scheme intelligently allocates pages to relatively cooler ranks of the DIMM. Our experiments show that in a high performance memory system, our schemes reduce the peak DRAM chip temperature by as much as 8.39\&#x00B0;C over 10 workloads (5.36\&#x00B0;C on average). Our schemes also improve performance mainly due to reduction in thermal emergencies: for a baseline system with memory bandwidth throttling scheme, the IPC is improved by as much as 15.8\% (4.1\% on average). },
}

@inproceedings{5749757,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Jiang, Xiaowei and Mishra, Asit and Zhao, Li and Iyer, Ravishankar and Fang, Zhen and Srinivasan, Sadagopan and Makineni, Srihari and Brett, Paul and Das, Chita R.},
 year = {2011},
 pages = {527--538},
 publisher = {IEEE},
 title = {ACCESS: Smart scheduling for asymmetric cache CMPs},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749757},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749757},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749757.pdf?arnumber=5749757},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {In current Chip-multiprocessors (CMPs), a significant portion of the die is consumed by the last-level cache. Until recently, the balance of cache and core space has been primarily guided by the needs of single applications. However, as multiple applications or virtual machines (VMs) are consolidated on such a platform, researchers have observed that not all VMs or applications require significant amount of cache space. In order to take advantage of this phenomenon, we explore the use of asymmetric last-level caches in a CMP platform. While asymmetric cache CMPs provide the benefit of reduced power and area, it is important to build in hardware/software support to appropriately schedule applications on to cores with suitable cache capacity. In this paper, we address this problem with our ACCESS architecture comprising of: (a) asymmetric caches across a group of cores, (b) hardware support that enables prediction of cache performance on the different sized caches and (c) OS scheduler support to make use of the prediction capability and appropriately schedule applications on to core with suitable cache capacity. Measurements on a working prototype using SPEC2006 benchmarks show that our ACCESS architecture can effectively schedule jobs in an asymmetric cache CMP and provide 23\% performance improvement compared to a naive scheduler, and is 97\% close to an oracle scheduler in making schedules. },
}

@inproceedings{4798274,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Matsutani, H. and Koibuchi, M. and Amano, H. and Yoshinaga, T.},
 year = {2009},
 pages = {367--378},
 publisher = {IEEE},
 title = {Prediction router: Yet another low latency on-chip router architecture},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798274},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798274},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798274.pdf?arnumber=4798274},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Communication switching, Computer architecture, Delay, Hardware, Network topology, Network-on-a-chip, Packet switching, Prediction algorithms, Routing, Switches, bypass datapath, fat tree, low latency network-on-chip router architecture, network routing, network-on-chip, output channel, packet transfer, prediction hit rate algorithm, size 65 nm, tori, trees (mathematics), },
 abstract = {Network-on-Chips (NoCs) are quite latency sensitive, since their communication latency strongly affects the application performance on recent many-core architectures. To reduce the communication latency, we propose a low-latency router architecture that predicts an output channel being used by the next packet transfer and speculatively completes the switch arbitration. In the prediction routers, incoming packets are transferred without waiting the routing computation and switch arbitration if the prediction hits. Thus, the primary concern for reducing the communication latency is the hit rates of prediction algorithms, which vary from the network environments, such as the network topology, routing algorithm, and traffic pattern. Although typical low-latency routers that speculatively skip one or more pipeline stages use a bypass datapath for specific packet transfers (e.g., packets moving on the same dimension), our prediction router predictively forwards packets based on a prediction algorithm selected from several candidates in response to the network environments. In this paper, we analyze the prediction hit rates of six prediction algorithms on meshes, tori, and fat trees. Then we provide three case studies, each of which assumes different many-core architecture. We have implemented a prediction router for each case study by using a 65 nm CMOS process, and evaluated them in terms of the prediction hit rate, zero load latency, hardware amount, and energy consumption. The results show that although the area and energy are increased by 6.4-15.9\% and 8.0-9.5\% respectively, up to 89.8\% of the prediction hit rate is achieved in real applications, which provide favorable trade-offs between the modest hardware/energy overheads and the latency saving. },
}

@inproceedings{4798275,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {379--380},
 publisher = {IEEE},
 title = {Session 6A Security, verification, and validation},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798275},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798275},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798275.pdf?arnumber=4798275},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Security, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798275.png" border="0"> },
}

@inproceedings{4798276,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Yunji Chen and Yi Lv and Weiwu Hu and Tianshi Chen and Haihua Shen and Pengyu Wang and Hong Pan},
 year = {2009},
 pages = {381--392},
 publisher = {IEEE},
 title = {Fast complete memory consistency verification},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798276},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798276},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798276.pdf?arnumber=4798276},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Algorithm design and analysis, Computer bugs, Costs, Godson-3 microprocessor, LCHECK, Microarchitecture, Microprocessors, Multiprocessing systems, NP-hard, Observability, Polynomials, Silicon, Testing, cache coherence, chip multi processor, computational complexity, fast complete memory consistency verification, memory architecture, memory consistency, microprocessor chips, multiprocessor systems, natural partial order, optimisation, pending period, silicon, silicon verification platforms, store atomicity, time complexity, time order, time order, time order restriction, verification, },
 abstract = {The verification of an execution against memory consistency is known to be NP-hard. This paper proposes a novel fast memory consistency verification method by identifying a new natural partial order: time order. In multiprocessor systems with store atomicity, a time order restriction exists between two operations whose pending periods are disjoint: the former operation in time order must be observed by the latter operation. Based on the time order restriction, memory consistency verification is localized: for any operation, both inferring related orders and checking related cycles need to take into account only a bounded number of operations. Our method has been implemented in a memory consistency verification tool for CMP (chip multi processor), named LCHECK. The time complexity of the algorithm in LCHECK is O(C<sup>p</sup>p<sup>2</sup>n<sup>2</sup>) (where C is a constant, p is the number of processors and n is the number of operations) for soundly and completely checking, and O(p<sup>3</sup>n) for soundly but incompletely checking. LCHECK has been integrated into both pre and post silicon verification platforms of the Godson-3 microprocessor, and many bugs of memory consistency and cache coherence were found with the help of LCHECK. },
}

@inproceedings{4798277,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Kong, J. and Aciicmez, O. and Seifert, J.-P. and Huiyang Zhou},
 year = {2009},
 pages = {393--404},
 publisher = {IEEE},
 title = {Hardware-software integrated approaches to defend against software cache-based side channel attacks},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798277},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798277},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798277.pdf?arnumber=4798277},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Application software, Cryptography, Electronic countermeasures, Hardware, Laboratories, PLcache, Proposals, Protection, RPcache, Security, Software performance, Yarn, cache attack, cache storage, computer networks, computer system, cryptographic operation, cryptography, hardware complexity, hardware-software integrated approach, memory performance, partition-locked cache, performance overhead, random-permutation cache, security protection, side channel attack, software cache, },
 abstract = {Software cache-based side channel attacks present serious threats to modern computer systems. Using caches as a side channel, these attacks are able to derive secret keys used in cryptographic operations through legitimate activities. Among existing countermeasures, software solutions are typically application specific and incur substantial performance overhead. Recent hardware proposals including the partition-locked cache (PLcache) and random-permutation cache (RPcache) (Wang and Lee, 2007), although very effective in reducing performance overhead while enhancing the security level, may still be vulnerable to advanced cache attacks. In this paper, we propose three hardware-software approaches to defend against software cache-based attacks - they present different tradeoffs between hardware complexity and performance overhead. First, we propose to use preloading to secure the PLcache. Second, we leverage informing loads, which is a lightweight architectural support originally proposed to improve memory performance, to protect the RPcache. Third, we propose novel software permutation to replace the random permutation hardware in the RPcache. This way, regular caches can be protected with hardware support for informing loads. In our experiments, we analyze various processor models for their vulnerability to cache attacks and demonstrate that even to the processor model that is most vulnerable to cache attacks, our proposed software-hardware integrated schemes provide strong security protection. },
}

@inproceedings{4798270,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Chen, X.E. and Aamodt, T.M.},
 year = {2009},
 pages = {329--340},
 publisher = {IEEE},
 title = {A first-order fine-grained multithreaded throughput model},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798270},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798270},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798270.pdf?arnumber=4798270},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Analytical models, Delay, Fires, Markov chain model, Markov processes, Multicore processing, Performance analysis, Predictive models, Probability, Sun, Throughput, Yarn, first-order fine-grained multithreaded throughput model, microprocessor chips, multi-threading, multithreaded architectures, probabilistic model, },
 abstract = {Analytical modeling is an alternative to detailed performance simulation with the potential to shorten the development cycle and provide additional insights. This paper proposes analytical models for predicting the cache contention and throughput of heavily multithreaded architectures such as Sun Microsystems' Niagara. First, it proposes a novel probabilistic model to accurately predict the number of extra cache misses due to cache contention for significantly larger numbers of threads than possible with prior analytical cache contention models. Then it presents a Markov chain model for analytically estimating the throughput of multicore, fine-grained multithreaded architectures. The Markov model uses the number of stalled threads as the states and calculates transition probabilities based upon the rates and latencies of events stalling a thread. By modeling the overlapping of the stalls among threads and taking account of cache contention our models accurately predict system throughput obtained from a cycle-accurate performance simulator with an average error of 7.9\%. We also demonstrate the application of our model to a design problem-optimizing the design of fine-grained multithreaded chip multiprocessors for application-specific workloads-yielding the same result as detailed simulations 65 times faster. Moreover, this paper shows that our models accurately predict cache contention and throughput trends across varying workloads on real hardware-a Sun Fire T1000 server. },
}

@inproceedings{4798271,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Kumar, A. and Huggahalli, R. and Makineni, S.},
 year = {2009},
 pages = {341--352},
 publisher = {IEEE},
 title = {Characterization of Direct Cache Access on multi-core systems and 10GbE},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798271},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798271},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798271.pdf?arnumber=4798271},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Access protocols, CPU cache, Control systems, I/O routing, Kernel, Linux, Linux network, Microarchitecture, Microprocessors, Network servers, Processor scheduling, Technological innovation, Telecommunication traffic, Virtual prototyping, cache storage, direct cache access, multicore system, network performance, network traffic, server platform, shared memory systems, storage capacity 10 Gbit, telecommunication network routing, telecommunication traffic, },
 abstract = {10 GbE connectivity is expected to be a standard feature of server platforms in the near future. Among the numerous methods and features proposed to improve network performance of such platforms is direct cache access (DCA) to route incoming I/O to CPU caches directly. While this feature has been shown to be promising, there can be significant challenges when dealing with high rates of traffic in a multiprocessor and multi-core environment. In this paper, we focus on two practical considerations with DCA. In the first case, we show that the performance benefit from DCA can be limited when network traffic processing rate cannot match the I/O rate. In the second case, we show that affinitizing both stack and application contexts to cores that share a cache is critical. With proper distribution and affinity, we show that a standard Linux network stack runs 32\% faster for 2 KB to 64 KB I/O sizes. },
}

@inproceedings{4798272,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {353--354},
 publisher = {IEEE},
 title = {Session 5B On-chip networks-II},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798272},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798272},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798272.pdf?arnumber=4798272},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Network-on-a-chip, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798272.png" border="0"> },
}

@inproceedings{4798273,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Abad, P. and Puente, V. and Gregorio, J.-A.},
 year = {2009},
 pages = {355--366},
 publisher = {IEEE},
 title = {MRR: Enabling fully adaptive multicast routing for CMP interconnection networks},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798273},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798273},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798273.pdf?arnumber=4798273},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Bandwidth, CMP interconnection networks, Communication system traffic control, Costs, Hardware, Multiprocessor interconnection networks, Network-on-a-chip, Proposals, Routing, Telecommunication traffic, Unicast, adaptive tree, bandwidth utilization, chip multiprocessors, fully adaptive multicast routing, multicast communication, multicast rotary router, multicast traffic, multidestination packets, multidestination traffic, multiprocessor interconnection networks, multiprocessor machines, network utilization, network-on-chip, on-chip networks, on-network congestion control, on-network hardware support, on-network multicast support, },
 abstract = {On-network hardware support for multi-destination traffic is a desirable feature in most multiprocessor machines. Multicast hardware capabilities enable much more effective bandwidth utilization as multi-destination packets do not need to repeatedly use the same resources, as occurs when multicast traffic must be decomposed in unicast packets. Although Chip Multiprocessors are not an exception in this interest, up to date, few fitting proposals exist. The combination of the scarcity of available resources and the common idea that multicast support requires a substantial amount of extra resources is responsible for this situation. In this work, we propose a new approach suitable for on-chip networks capable of managing multi-destination traffic via hardware in an efficient way with negligible complexity. We introduce the Multicast Rotary Router (MRR), a router able to: (1) perform on-network multicast support with almost zero cost over the Rotary Router, (2) use a fully adaptive tree to distribute multicast traffic, (3) perform on-network congestion control extending network utilization range. The performance results, using a state-of-the-art full system simulation framework, show that it improves average full system performance of a CMP using a unicast Rotary Router in its interconnection network by 25\%, and an input buffered router with multicast support by 20\%. },
}

@inproceedings{4798278,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {DeOrio, A. and Wagner, I. and Bertacco, V.},
 year = {2009},
 pages = {405--416},
 publisher = {IEEE},
 title = {Dacota: Post-silicon validation of the memory subsystem in multi-core designs},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798278},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798278},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798278.pdf?arnumber=4798278},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Cache storage, Computer bugs, Dacota, Hardware, Observability, Process design, Processor scheduling, Production, Prototypes, Silicon, Testing, cache storage, cache storage, chip multiprocessors, circuit CAD, customer shipment, data coloring scheme, design verification, distributed algorithm, formal verification, functional errors, functional validation, integrated circuit design, internal node observability, log memory access, memory operation ordering validation, microprocessor chips, modern processor designs, multicore designs, nondeterministic memory subsystems, post-silicon validation, production schedules, },
 abstract = {The number of functional errors escaping design verification and being released into final silicon is growing, due to the increasing complexity and shrinking production schedules of modern processor designs. Recent trends towards chip multiprocessors (CMPs) are exacerbating the problem because of their complex and sometimes non-deterministic memory subsystems, prone to subtle but devastating bugs. This deteriorating situation calls for high-efficiency, high-coverage results in functional validation, results that are be achieved by leveraging the performance of post-silicon validation, that is, those verification tasks that are executed directly on prototype hardware. The orders-of-magnitude faster testing in post-silicon enables designers to achieve much higher coverage before customer release, but only if the limitations of this technology in diagnosis and internal node observability could be overcome. In this work, we unlock the full performance of post-silicon validation through Dacota, a new high-coverage solution for validating memory operation ordering in CMPs. When activated, Dacota reconfigures a portion of the cache storage to log memory accesses using a compact data-coloring scheme. Logs are periodically aggregated and checked by a distributed algorithm running in-situ on the CMP to verify correct memory operation ordering. When the design is ready for customer shipment, Dacota can be deactivated, releasing all cache storage, and only leaving a small silicon area footprint, less than 0.01\% (three orders of magnitude smaller than previous solutions). We found experimentally that Dacota is effective in exposing memory subsystem bugs, and it delivers its high coverage capabilities at a 26\% performance slowdown (only during validation) for real-world applications. },
}

@inproceedings{4798279,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {417--418},
 publisher = {IEEE},
 title = {Session 6B processor microarchitecture-II},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798279},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798279},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798279.pdf?arnumber=4798279},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Microarchitecture, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798279.png" border="0"> },
}

@inproceedings{386548,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Seznec, A.},
 year = {1995},
 pages = {134--143},
 publisher = {IEEE},
 title = {DASC cache},
 date = {1995},
 doi = {10.1109/HPCA.1995.386548},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386548},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386548.pdf?arnumber=386548},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Clocks, DASC cache, Delay, Direct-mapped Access Set-associative Check cache, Frequency, Hardware, Indexing, Manufacturing, Memory management, Microprocessors, Operating systems, Throughput, associativity degree, cache array, cache hit time, cache storage, clock cycle, data consistency, data integrity, direct-mapped, tag array, virtual storage, },
 abstract = {For many microprocessors, cache hit time determines the clock cycle. On the other hand, cache miss penalty(measured in instruction issue delays) becomes higher and higher. Conciliating low cache miss ratio with low cache hit time is an important issue. When caches are virtually indexed, the operating system (or some specific hardware) has to manage data consistency of caches and memory. Unfortunately, conciliating physical indexing of the cache and low cache hit time is very difficult. In this paper, we propose the Direct-mapped Access Set-associative Check cache (DASC) for addressing both difficulties. On a DASC cache, the cache array is direct-mapped, so the cache hit time is low. However the tag array is set-associative and the external miss ratio on a DASC cache is the same as the miss ratio on a set-associative cache. When the size of an associativity degree of the tag array is tied to the minimum page size, a virtually indexed but physically tagged DASC cache correctly handles all difficulties associated with cache consistency. Trace driven simulations show that, for cache sizes in the range of 16 to 64 Kbytes and for page sizes in the range 4 to 8 Kbytes, a DASC cache is a valuable trade-off allowing fast cache hit time and low cache miss ratio while cache consistency management is performed by hardware },
}

@inproceedings{4798238,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Agarwal, N. and Li-Shiuan Peh and Jha, N.K.},
 year = {2009},
 pages = {67--78},
 publisher = {IEEE},
 title = {In-Network Snoop Ordering (INSO): Snoopy coherence on unordered interconnects},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798238},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798238},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798238.pdf?arnumber=4798238},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Bandwidth, Broadcasting, Costs, Delay, Fabrics, Multicast protocols, Network interfaces, Network-on-a-chip, Scalability, Spread spectrum communication, broadcast based token coherence protocol, cache coherence protocol, cache storage, coherence transactions, destination network interfaces, directory protocol, full-system evaluations, global ordering, in-network snoop ordering, integrated circuit interconnections, intermediate routers, logical ordering scheme, many-core chip multiprocessors, microprocessor chips, multihop on-chip networks, scalable cache coherence, scalable on-chip communication, snoop-based protocol, unordered interconnects, unordered on-chip networks, },
 abstract = {Realizing scalable cache coherence in the many-core era comes with a whole new set of constraints and opportunities. It is widely believed that multi-hop, unordered on-chip networks would be needed in many-core chip multiprocessors (CMPs) to provide scalable on-chip communication. However, providing ordering among coherence transactions on unordered interconnects is a challenge. Traditional approaches for tackling coherence either have to use ordered interconnects (snoop-based protocols) which lead to scalability problems, or rely on an ordering point (directory-based protocols) which adds indirection latency. In this paper, we propose In-Network Snoop Ordering (INSO), in which coherence requests from a snoop-based protocol are inserted into the interconnect fabric and the network orders the requests in a distributed manner, creating a global ordering among requests. Essentially, when coherence requests enter the network, they grab snoop-orders at the injection router before being broadcasted. A snoop-order specifies the global ordering of the particular request with respect to other requests. Before requests reach their destinations, they get ordered along the way, at intermediate routers and destination network interfaces. Our logical ordering scheme can be mapped onto any unordered interconnect. This enables a cache coherence protocol which exploits the low-latency nature of unordered interconnects without adding indirection to coherence transactions. Our full-system evaluations compare INSO against a directory protocol and a broadcast based Token Coherence protocol. INSO outperforms these protocols by up to 30\% and 8.5\%, respectively, on a wide range of scientific and emerging applications. },
}

@inproceedings{903269,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Heath, T. and Kaur, S. and Martin, R.P. and Nguyen, T.D.},
 year = {2001},
 pages = {267--277},
 publisher = {IEEE},
 title = {Quantifying the impact of architectural scaling on communication },
 date = {2001},
 doi = {10.1109/HPCA.2001.903269},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903269},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903269.pdf?arnumber=903269},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Communication system control, Computer science, Costs, Hardware, LogP performance, Performance loss, Predictive models, Protocols, Robustness, Switches, TCP/IP, TCPIP, UDP/IP, architectural scaling, messaging performance, parallel architectures, performance evaluation, processor speed, },
 abstract = {This work quantifies how persistent increases in processor speed compared to I/O speed reduce the performance gap between specialized, high performance messaging layers and general purpose protocols such as TCP/IP and UDP/IP. The comparison is important because specialized layers sacrifice considerable system connectivity and robustness to obtain increased performance. We first quantify the scaling effects on small messages by measuring the LogP performance of two Active Message II layers, one running over a specialized VIA layer and the other over stock UDP as we scale the CPU and I/O components. We then predict future LogP performance by mapping the LogP model's network parameters, particularly overhead into architectural components. Our projections show that the performance benefit afforded by specialized messaging for small messages will erode to a factor of 2 in the next 5 years. Our models further show that the performance differential between the two approaches will continue to erode without a radical restructuring of the I/O system. For long messages, we quantify the variable per-page instruction budget that a zero-copy messaging approach has for page table manipulations if it is to outperform a single-copy approach. Finally we conclude with an examination of future I/O advances that would result in substantial improvements to messaging performance },
}

@inproceedings{386540,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Michael, M.M. and Scott, M.L.},
 year = {1995},
 pages = {222--231},
 publisher = {IEEE},
 title = {Implementation of atomic primitives on distributed shared memory multiprocessors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386540},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386540},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386540.pdf?arnumber=386540},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computer science, Concurrent computing, Etching, Hardware, Information science, Large-scale systems, Pattern analysis, Performance analysis, Protocols, Testing, atomic primitives, bets-based machines, cache line, data sharing patterns, distributed memory systems, distributed shared memory multiprocessors, hardware implementations, performance, performance evaluation, shared memory systems, synchronisation, },
 abstract = {In this paper we consider several hardware implementations of the general-purpose atomic primitives fetch and \&Phi;, compare and swap, load linked, and store conditional on large-scale shared-memory multiprocessors. These primitives have proven popular on small-scale bets-based machines, but have yet to become widely available on large-scale, distributed shared memory machines. We propose several alternative hardware implementations of these primitives, and then analyze the performance of these implementations for various data sharing patterns. Our results indicate that good overall performance can be obtained by implementing compare and swap in the cache controllers, and by providing an additional instruction to load an exclusive copy of a cache line },
}

@inproceedings{386541,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Fiske, S. and Dally, W.J.},
 year = {1995},
 pages = {210--221},
 publisher = {IEEE},
 title = {Thread prioritization: a thread scheduling mechanism for multiple-context parallel processors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386541},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386541},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386541.pdf?arnumber=386541},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Communication switching, Computational modeling, Context, Degradation, Delay, Dynamic scheduling, Processor scheduling, Registers, Resource management, Yarn, multiple-context parallel processors, parallel processing, performance evaluation, rapid context switching, register resources, scheduling, simulation performance, synchronisation, synchronization latencies, thread prioritization, thread scheduling mechanism, },
 abstract = {Multiple-context processors provide register resources that allow rapid context switching between several threads as a means of tolerating long communication and synchronization latencies. When scheduling threads on such a processor, we must first decide which threads should have their state loaded into the multiple contexts, and second, which loaded thread is to execute instructions at any given time. In this paper we show that both decisions are important, and that incorrect choices can lead to serious performance degradation. We propose thread prioritization as a means of guiding both levels of scheduling. Each thread has a priority that can change dynamically, and that the scheduler uses to allocate as many computation resources as possible to critical threads. We briefly describe its implementation, and we show simulation performance results for a number of simple benchmarks in which synchronization performance is critical },
}

@inproceedings{386542,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Panda, D.K.},
 year = {1995},
 pages = {200--209},
 publisher = {IEEE},
 title = {Fast barrier synchronization in wormhole k-ary n-cube networks with multidestination worms},
 date = {1995},
 doi = {10.1109/HPCA.1995.386542},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386542},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386542.pdf?arnumber=386542},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Broadcasting, Communication switching, Computer worms, Costs, Hardware, Information science, Magnetic heads, Message passing, Registers, Routing, broadcasting, e-cube routing, fast barrier synchronization, gather, hypercube networks, message passing, multidestination messages, multidestination worms, synchronisation, unicast-based message passing, wake-up phases, wormhole k-ary n-cube networks, },
 abstract = {This paper presents a new approach to implement fast barrier synchronization in wormhole k-ary n-cubes. The novelty lies in using multidestination messages instead of the traditional single destination messages. Two different multidestination worm types, gather and broadcasting, are introduced to implement the report and wake-up phases of barrier synchronization, respectively. Algorithms for complete and arbitrary set barrier synchronization are presented using these new worms. It is shown that complete barrier synchronization in a k-ary n-cube system with e-cube routing can be implemented with 2n communication start-ups as compared to 2n log<sub>2</sub> k start-ups needed with unicast-based message passing. For arbitrary set barrier, an interesting trend is observed where the synchronization cost keeps on reducing beyond a certain number of participating nodes },
}

@inproceedings{386543,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Treiber, R. and Menon, J.},
 year = {1995},
 pages = {186--197},
 publisher = {IEEE},
 title = {Simulation study of cached RAID5 designs},
 date = {1995},
 doi = {10.1109/HPCA.1995.386543},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386543},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386543.pdf?arnumber=386543},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Analytical models, Databases, File systems, Logic, Performance analysis, Prefetching, Production, cache allocation, cache replacement, cache storage, cached RAID5 designs, database I/O traces, destage policies, digital simulation, disk buffering, priority queueing, simulation study, striping, },
 abstract = {This paper considers the performance of cached RAID5 using simulations that are driven by database I/O traces collected at customer sites. This is in contrast to previous performance studies using analytical modelling or random-number simulations. We studied issues of cache size, disk buffering, cache replacement policies, cache allocation policies, destage policies and striping. Our results indicate that: read caching has considerable value; a small amount of cache should be used for writes fast write logic can reduce disk utilization for writes by an order of magnitude; priority queueing should be supported at the disks; disk buffering prefetch should be used; for large caches, it pays to cache sequentially accessed blocks; RAID5 with cylinder striping is superior to parity striping },
}

@inproceedings{386544,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Sterling, T.L. and Savarese, D.F. and Merkey, P.R. and Gardner, J.P.},
 year = {1995},
 pages = {176--185},
 publisher = {IEEE},
 title = {An initial evaluation of the Convex SPP-1000 for earth and space science applications},
 date = {1995},
 doi = {10.1109/HPCA.1995.386544},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386544},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386544.pdf?arnumber=386544},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Beta-test environment, Concurrent computing, Convex SPP-1000, Earth, Extraterrestrial measurements, Geoscience, Hardware, High performance computing, Microprocessors, N-body simulation, NASA, Parallel processing, Protocols, SCI protocol, aerospace computing, cache coherence mechanisms, earth and space science applications, fork-join, geophysics computing, job-stream level parallelism, message passing, message passing synchronization, multiple program workload, performance evaluation, shared memory capability, shared memory systems, synchronisation, tree-code version, },
 abstract = {The Convex SPP-1000, the most recent SPC, is distinguished by a true global shared memory capability based on the first commercial version of directory based cache coherence mechanisms and SCI protocol. The system was evaluated at NASA/GSFC in the Beta-test environment using three classes of operational experiments targeting earth and space science applications. A multiple program workload tested job-stream level parallelism. Synthetic programs measured overhead costs of barrier, fork-join, and message passing synchronization primitives. An efficient tree-code version of an N-body simulation revealed scaling properties and measured the overall efficiency. This paper presents the results of this study and provides the earliest published evaluation of this new scalable architecture },
}

@inproceedings{386545,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Boura, Y.M. and Das, C.R.},
 year = {1995},
 pages = {166--175},
 publisher = {IEEE},
 title = {Modeling virtual channel flow control in hypercubes},
 date = {1995},
 doi = {10.1109/HPCA.1995.386545},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386545},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386545.pdf?arnumber=386545},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Analytical models, Communication switching, Computer science, Delay effects, Hypercubes, M/M/m queueing system, Markov model, Markov processes, Mathematical model, Multiprocessor interconnection networks, Pipelines, Protocols, Routing, analytical model, average message latency, blocking delay, delays, e-cube routing algorithm, hypercube networks, hypercubes, message transfer time, multiplexing, multiplexing delay, queueing theory, virtual channel flow control modelling, },
 abstract = {An analytical model for virtual channel flow control in n-dimensional hypercubes using the e-cube routing algorithm is developed. The model is based on determining the values of the different components that make up the average message latency. These components include the message transfer time, the blocking delay at each dimension, the multiplexing delay at each dimension, and the waiting delay at the source node. The first two components are determined using a probabilistic analysis. The average degree of multiplexing is determined using a Markov model, and the waiting delay at the source node is determined using an M/M/m queueing system. The model is fairly accurate in predicting the average message latency for different message sizes and a varying number of virtual channels per physical channel. It is demonstrated that wormhole switching along with virtual channel flow control make the average message latency insensitive to the network size when the network is relatively lightly loaded (message arrival rate is equal to 40\% of channel capacity), and that the average message latency increases linearly with the average message size. The simplicity and accuracy of the analytical model make it an attractive and effective tool for predicting the behavior of n-dimensional hypercubes },
}

@inproceedings{386546,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Temam, O. and Drach, N.},
 year = {1995},
 pages = {154--163},
 publisher = {IEEE},
 title = {Software assistance for data caches},
 date = {1995},
 doi = {10.1109/HPCA.1995.386546},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386546},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386546.pdf?arnumber=386546},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Algorithm design and analysis, Application software, Costs, Design optimization, Hardware, Laboratories, Memory management, Pollution, Software algorithms, Software performance, bypass mechanism, cache optimizations, cache storage, data caches, optimisation, performance, software assistance, spatial locality, temporal locality, virtual cache lines, },
 abstract = {Hardware and software cache optimizations are active fields of research, that have yielded powerful but occasionally complex designs and algorithms. The purpose of this paper is to investigate the performance of combined though simple software and hardware optimizations. Because current caches provide little flexibility for exploiting temporal and spatial locality, two hardware modifications are proposed to support these two kinds of locality. Spatial locality is exploited by using large virtual cache lines which do not exhibit the performance flaws of large physical cache lines. Temporal locality is exploited by minimizing cache pollution with a bypass mechanism that still allows to exploit spatial locality. Subsequently, it is shown that simple software informations on the spatial/temporal locality of array references, as provided by current data locality optimizing algorithms, can be used to significantly increase cache performance. The performance and design trade-offs of the proposed mechanisms are discussed. Software assisted caches are further shown to provide a convenient support for hardware and software optimizations },
}

@inproceedings{386547,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Theobald, K.B. and Hum, H.H.J. and Gao, G.R.},
 year = {1995},
 pages = {144--153},
 publisher = {IEEE},
 title = {A design framework for hybrid-access caches},
 date = {1995},
 doi = {10.1109/HPCA.1995.386547},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386547},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386547.pdf?arnumber=386547},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computer science, Frequency, Microprocessors, associative caches, cache storage, design framework, digital simulation, direct-mapped caches, high-speed microprocessors, hybrid-access caches, on-chip caches, simulations, },
 abstract = {High-speed microprocessors need fast on-chip caches in order to keep busy. Direct-mapped caches have better access times than set-associative caches, but poorer miss rates. This has led to several hybrid on-chip caches combining the speed of direct-mapped caches with the hit rates of associative caches. In this paper, we unify these hybrids within a single framework which we call the hybrid access cache (HAC) model. Existing hybrid caches lie near the edges of the HAC design space, leaving the middle untouched. We study a group of caches in this middle region, a group we call half-and-half caches, which are half direct-mapped and half set-associative. Simulations confirm the predictive valve of the HAC model, and demonstrate that, for medium to large caches, this middle region yields more efficient cache designs },
}

@inproceedings{650557,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Gonzalez, A. and Gonzalez, J. and Valero, M.},
 year = {1998},
 pages = {175--184},
 publisher = {IEEE},
 title = {Virtual-physical registers},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650557},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650557},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650557.pdf?arnumber=650557},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Costs, Decoding, Delay, Dynamic scheduling, Hardware, Microprocessors, Parallel processing, Pipelines, Processor scheduling, Registers, dynamic register renaming approach, dynamically scheduled processor, experimental evaluation, instruction sets, instruction-level parallelism, issue strategy, microprogramming, naming services, performance, physical register allocation, processor scheduling, simulation, software performance evaluation, storage allocation, storage location, virtual physical registers, virtual storage, write-back strategy, },
 abstract = {A novel dynamic register renaming approach is proposed in this work. The key idea of the novel scheme is to delay the allocation of physical registers until a late stage in the pipeline, instead of doing it in the decode stage as conventional schemes do. In this way, the register pressure is reduced and the processor can exploit more instruction-level parallelism. Delaying the allocation of physical registers require some additional artifact to keep track of dependences. This is achieved by introducing the concept of virtual-physical registers, which do not require any storage location and are used to identify dependences among instructions that have not yet allocated a register to its destination operand. Two alternative allocation strategies have been investigated that differ in the stage where physical registers are allocated: issue or write-back. The experimental evaluation has confirmed the higher performance of the latter alternative. We have performed all evaluation of the novel scheme through a detailed simulation of a dynamically scheduled processor. The results show a significant improvement (e.g., 19\% increase in IPC for a machine with 64 physical registers in each file) when compared with the traditional register renaming approach },
}

@inproceedings{650556,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Ye Zhang and Rauchwerger, L. and Torrellas, J.},
 year = {1998},
 pages = {162--173},
 publisher = {IEEE},
 title = {Hardware for speculative run-time parallelization in distributed shared-memory multiprocessors},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650556},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650556},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650556.pdf?arnumber=650556},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Application software, Circuit simulation, Computer science, Contracts, Hardware, Parallel processing, Processor scheduling, Protocols, Runtime, Testing, cache coherence protocol, cache storage, computationally expensive, data dependence information, dependence violations, distributed memory systems, distributed shared-memory multiprocessors, hardware design, memory hierarchy, memory protocols, parallel architectures, parallel programming, parallelising compilers, parallelizing compilers, performance, performance evaluation, program control structures, program loops, shared memory systems, speculative run-time parallelization hardware, },
 abstract = {Run-time parallelization is often the only way to execute the code in parallel when data dependence information is incomplete at compile time. This situation is common in many important applications. Unfortunately, known techniques for run-time parallelization are often computationally expensive or not general enough. To address this problem, we propose new hardware support for efficient run-time parallelization in distributed shared-memory (DSM) multiprocessors. The idea is to execute the code in parallel speculatively and use extensions to the cache coherence protocol hardware to detect any dependence violations. As soon as a dependence is detected, execution stops, the state is restored, and the code is re-executed serially. This scheme, which we apply to loops, allows iterations to execute and complete in potentially any order. This scheme requires hardware extensions to the cache coherence protocol and memory hierarchy of a DSM. It has low overhead. We present the algorithms and a hardware design of the scheme. Overall, the scheme delivers average loop speedups of 7.3 for 16 processors and is 50\% faster than a related software-only method },
}

@inproceedings{650555,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Basu, S. and Torrellas, J.},
 year = {1998},
 pages = {152--161},
 publisher = {IEEE},
 title = {Enhancing memory use in Simple Coma: Multiplexed Simple Coma},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650555},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650555},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650555.pdf?arnumber=650555},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Access protocols, Computer science, Contracts, Degradation, Delay, Distribution strategy, Flat Coma, Frequency, Hardware, High performance computing, Memory architecture, Multiplexed Simple Coma, Simple Coma, cache storage, cache-only memory architectures, cost performance design, data distribution strategies, data migration, data replication, execution time, memory architecture, memory fragmentation, memory use, multiple virtual pages, off-the-shelf processors, page mapping, page migration schemes, page transfer overhead, page unmapping, paged storage, parallel architectures, performance evaluation, reconfigurable architectures, scalable shared memory multiprocessors, shared memory systems, },
 abstract = {Scalable shared-memory multiprocessors that are designed as cache-only memory architectures (Coma) allow automatic replication and migration of data in the main memory. This enhances programmability by hopefully eliminating the need for data distribution strategies and page migration schemes. A variant of Coma called Simple Coma has been proposed as a lower-cost alternative to hardware-intensive systems like Flat Coma. However, we find that Simple Coma is quite slower than Flat Coma. The main reason is the high page mapping, unmapping, and transfer overhead caused by memory fragmentation in Simple Coma. We propose a solution to the memory fragmentation problem that we call Multiplexed Simple Coma. The idea is to allow multiple virtual pages to map into the same physical page at the same time, therefore compressing the page working set of the application. Multiplexed Simple Coma requires very little support over Simple Coma and reduces its execution time by about 40\%. We find that Multiplexed Simple Coma can be very easily implemented with off-the-shelf processors. In addition, there is no need to be selective when choosing what virtual pages are to share the same physical page. Overall, although Multiplexed Simple Coma is still slower than Flat Coma, since it is cheaper to implement, it represents a good cost-performance design point },
}

@inproceedings{650554,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Ekanadham, K. and Beng-Hong Lim and Pattnaik, P. and Snir, M.},
 year = {1998},
 pages = {140--151},
 publisher = {IEEE},
 title = {PRISM: an integrated architecture for scalable shared memory},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650554},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650554},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650554.pdf?arnumber=650554},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {CC-NUMA, Computer architecture, Control systems, Electronic switching systems, Memory architecture, Memory management, Operating systems, PRISM, Read only memory, Resource management, Runtime, SPLASH applications, Scalability, Simple-COMA, adaptive run-time policies, cache coherence protocol, cache storage, distributed memory systems, distributed shared memory architecture, fault containment boundaries, hardware, memory architecture, memory management algorithms, memory protocols, multiple independent kernels, operating system design, operating system kernels, page faults, page replication, paged storage, parallel architectures, performance evaluation, reconfigurable architectures, reliable performance, scalable shared memory, shared memory page management, shared memory systems, },
 abstract = {This paper describes PRISM, a distributed shared memory architecture that relies on a tightly integrated hardware and operating system design for scalable and reliable performance. PRISM's hardware provides mechanisms for flexible management and dynamic configuration of shared memory pages with different behaviors. As an example, PRISM can configure individual shared memory pages in both CC-NUMA and Simple-COMA styles, maintaining the advantages of both without incorporating any of their disadvantages. PRISM's operating system is structured as multiple independent kernels, where each kernel manages the resources on its local node. PRISM's system structure minimizes the amount of global coordination when managing shared memory. Page faults do not involve global TLB invalidates, and pages can be replicated and migrated without requiring global coordination. The structure also provides natural fault containment boundaries around each node because physical addresses do not address remote memory directly. We simulate PRISM's hardware, cache coherence protocol and memory management algorithms. Results from SPLASH applications on the simulated machine demonstrate a tradeoff between CC-NUMA and Simple-COMA styles of memory management. Adaptive, run-time policies that take advantage of PRISM's ability to dynamically configure shared memory pages with different behaviors significantly outperform pure CC-NUMA or Simple-COMA configurations and are usually within 10\% of optimal performance },
}

@inproceedings{650553,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Panda, D.K.},
 year = {1998},
 pages = {138--138},
 publisher = {IEEE},
 title = {The Emergence Of Workstation Clusters: Should We Continue To Build MPPs?},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650553},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650553},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650553.pdf?arnumber=650553},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Concurrent computing, Ethernet networks, Parallel processing, Workstations, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00650553.png" border="0"> },
}

@inproceedings{650552,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Scales, D.J. and Gharachorloo, K. and Aggarwal, A.},
 year = {1998},
 pages = {125--136},
 publisher = {IEEE},
 title = {Fine-grain software distributed shared memory on SMP clusters},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650552},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650552},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650552.pdf?arnumber=650552},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Alpha multiprocessors, Application software, Computer science, Hardware, Laboratories, Protocols, Read only memory, SMP clusters, SPLASH-2 applications, Shasta system, Software systems, Support vector machines, Time of arrival estimation, Workstations, distributed memory systems, fine granularity, fine-grain software distributed shared memory, performance evaluation, performance results, protocol, protocols, shared memory systems, synchronisation, synchronization, },
 abstract = {Commercial SMP nodes are an attractive building block for software distributed shared memory systems. The advantages of using SMP nodes include fast communication among processors within the same node and potential gains from clustering where remote data fetched by one processor is used by other processors on the same node. This paper describes a major extension to the Shasta distributed shared memory system to run efficiently on a cluster of SMP nodes. The Shasta system keeps shared data coherent across nodes at a fine granularity by inserting inline code that checks the cache state of shared data before each load or store in an application. However allowing processors to share memory within the same SMP is complicated by race conditions that arise because the inline state check is non-atomic with respect to the actual load or store of shared data. We present a novel protocol that avoids such race conditions without the use of costly synchronization in the inline checking code. To characterize the benefits of using SMP nodes in the context of Shasta, we present detailed performance results for nine SPLASH-2 applications running on a cluster of Alpha multiprocessors },
}

@inproceedings{650551,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Samanta, R. and Bilas, A. and Iftode, L. and Singh, J.P.},
 year = {1998},
 pages = {113--124},
 publisher = {IEEE},
 title = {Home-based SVM protocols for SMP clusters: Design and performance },
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650551},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650551},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650551.pdf?arnumber=650551},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Coherence, Computer science, Hardware, Home computing, Identity-based encryption, Intel PentiumPro SMPs, Joining processes, Myrinet, Protocols, SMP clusters, Support vector machines, data structures, data structures, design, hardware cache coherence, home-based SVM protocols, network interface, network interfaces, network interfaces, parallel performance, performance, protocols, shared memory multiprocessors, shared memory programming, shared virtual memory, synchronisation, synchronization mechanisms, uniprocessor nodes, virtual storage, },
 abstract = {As small-scale shared memory multiprocessors proliferate in the market, it is very attractive to construct large-scale systems by connecting smaller multiprocessors together in software using efficient commodity, network interfaces and networks. Using a shared virtual memory (SVM) layer for this purpose preserves the attractive shared memory programming abstraction across nodes. In this paper: We describe home-based SVM protocols that support symmetric multiprocessor (SMP) nodes, taking advantage of the intra-node hardware cache coherence and synchronization mechanisms. Our protocols take no special advantage of the network interface and network except as a fast communication link, and as such are very portable. We present the key design tradeoffs, discuss our choices, and describe key data structures that enable us to implement these choices quite simply. We present an implementation on a network of 4-way Intel PentiumPro SMPs interconnected with Myrinet, and provide performance results. We explore the advantages of SMP nodes over uniprocessor nodes with this protocol, as well as other performance tradeoffs, through both real implementation and simulation as appropriate, since both have important roles to play. We find one approach to deliver good parallel performance on many real applications (at least at the scale we examine) and to improve performance over SVM across uniprocessor nodes },
}

@inproceedings{650550,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Moga, A. and Dubois, M.},
 year = {1998},
 pages = {103--112},
 publisher = {IEEE},
 title = {The effectiveness of SRAM network caches in clustered DSMs},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650550},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650550},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650550.pdf?arnumber=650550},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Costs, Delay, Distributed decision making, Intelligent networks, Operating systems, Program processors, Programming profession, Prototypes, Random access memory, SRAM network caches, Software prototyping, cache storage, capacity misses, clustered distributed shared memories, conflict misses, data caching, distributed memory systems, performance, performance evaluation, remote data caches, shared memory systems, },
 abstract = {The frequency of accesses to remote data is a key factor affecting the performance of all Distributed Shared Memory (DSM) systems. Remote data caching is one of the most effective and general techniques to fight processor stalls due to remote capacity misses in the processor caches. The design space of remote data caches (RDC) has many dimensions and one essential performance trade-off hit ratio versus speed. Some recent commercial systems have opted for large and slow (S)DRAM network caches (NC), but others completely avoid them because of their damaging effects on the remote/local latency ratio. In this paper we will explore small and fast SRAM network caches as a means to reduce the remote stalls and capacity traffic of multiprocessor clusters. The major appeal of SRAM NCs is that they add less penalty on the latency of NC hits and remote accesses. Their small capacity can handle conflict misses and a limited amount of capacity misses. However, they can be coupled with main memory page caches which satisfy the bulk of capacity misses. To maximize performance for a large spectrum of applications, we propose to organize the NC as a victim cache for remote data. We also propose a novel and scalable method to control the page cache by integrating page relocation mechanisms into the network victim cache },
}

@inproceedings{650559,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Gopal, S. and Vijaykumar, T.N. and Smith, J.E. and Sohi, G.S.},
 year = {1998},
 pages = {195--205},
 publisher = {IEEE},
 title = {Speculative versioning cache},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650559},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650559},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650559.pdf?arnumber=650559},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Bandwidth, Computer aided instruction, Data mining, Delay, Electronic switching systems, Microprocessors, Proposals, Registers, Static VAr compensators, Strontium, address resolution buffer, addresses, bandwidth problems, buffer storage, cache coherence, cache storage, centralized buffer, distributed cache, distributed memory systems, hit latency, instruction level parallelism, instruction sets, latency problems, memory dependence speculation, memory location, microprogramming, multiple speculative stores, multiscalar architecture, parallel architectures, performance, performance evaluation, program order, sequential program execution, sequential semantics, snooping bus-based coherent cache, speculative versioning cache, },
 abstract = {Dependences among loads and stores whose addresses are unknown hinder the extraction of instruction level parallelism during the execution of a sequential program. Such ambiguous memory dependences can be overcome by memory dependence speculation which enables a load or store to be speculatively executed before the addresses of all preceding loads and stores are known. Furthermore, multiple speculative stores to a memory location create multiple speculative versions of the location. Program order among the speculative versions must be tracked to maintain sequential semantics. A previously proposed approach, the address resolution buffer (ARB) uses a centralized buffer to support speculative versions. Our proposal, called the speculative versioning cache (SVC), uses distributed caches to eliminate the latency and bandwidth problems of the ARB. The SVC conceptually unifies cache coherence and speculative versioning by using an organization similar to snooping bus-based coherent caches. A preliminary evaluation for the multiscalar architecture shows that hit latency is an important factor affecting performance, and private cache solutions trade-off hit rate for hit latency },
}

@inproceedings{650558,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Tien-Fu Chen},
 year = {1998},
 pages = {185--194},
 publisher = {IEEE},
 title = {Supporting highly speculative execution via adaptive branch trees },
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650558},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650558},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650558.pdf?arnumber=650558},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {ABT scheme, Accuracy, Clocks, Computer science, Councils, Parallel processing, Pipelines, SPEC95 benchmarks, Shape, adaptive branch tree table, adaptive branch trees, branch penalty reduction, branch prediction mechanisms, experiment, instruction sets, microprogramming, parallel architectures, parallel programming, parallelism, prediction mechanisms, program control structures, shift operation, software performance evaluation, speculative execution, token assignment strategy, trees (mathematics), wide-issue processors, },
 abstract = {Most of the prediction mechanisms predict a single path to continue the execution on a branch. Alternatively, we may exploit parallelism from either possible paths of a branch, discarding wrong paths once the branch is resolved. This paper proposes a concept of adaptive branch trees (ABT) to support highly speculative execution for processors with deeper pipelines and wide issue widths. The basic idea of the adaptive branch tree is to dynamically keep track of alternative branch paths and to speculatively execute the code on the most likely path. Hence, unlike branch prediction mechanisms, the ABT scheme would not miss out misprediction paths since the scheme can eventually go back to other alternative paths when the machine has explored more pending branches. The branch tree is realized by an adaptive branch tree table. A token is associated with each basic block and operations in the entire basic blocks are tagged with the token. With a novel token assignment strategy, we can reconfigure the ABT by a shift operation once one branch is resolved. Our experiment results on the SPEC95 benchmarks show that the proposed approach can achieve significant branch penalty reduction for wide-issue processors },
}

@inproceedings{4658651,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Kuskin, J.S. and Young, C. and Grossman, J.P. and Batson, B. and Deneroff, M.M. and Dror, R.O. and Shaw, D.E.},
 year = {2008},
 pages = {343--354},
 publisher = {IEEE},
 title = {Incorporating flexibility in Anton, a specialized machine for molecular dynamics simulation},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658651},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658651},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658651.pdf?arnumber=4658651},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Acceleration, Anton, Application specific integrated circuits, Biological system modeling, Biology computing, Computational modeling, Hardware, Packaging, Poles and towers, Pressure control, Supercomputers, biology computing, data transfer, electronic data interchange, high-throughput interaction subsystem, molecular biophysics, molecular dynamics method, molecular dynamics simulation, parallel machine, parallel machines, particle interactions, special-purpose supercomputer, synchronisation, synchronization, },
 abstract = {An effective special-purpose supercomputer for molecular dynamics (MD) requires much more than high-performance acceleration of computational kernels: such accelerators must be balanced with general-purpose computation and communication resources. Achieving this balance was a significant challenge in the design of Anton, a parallel machine that will accelerate MD simulations by several orders of magnitude. Anton executes its most computationally demanding calculations on a highly specialized, enormously parallel, but largely non-programmable high-throughput interaction subsystem (HTIS). Other elements of the simulation have a less uniform algorithmic structure, and may also change in response to future advances in physical models and simulation techniques. Such calculations are executed on Antonpsilas flexible subsystem, which combines programmability with the computational power required to avoid ldquoAmdahlpsilas Lawrdquo bottlenecks arising from the extremely high throughput of the HTIS. Antonpsilas flexible subsystem is a heterogeneous multiprocessor with 12 cores, each organized around a 128-bit data path. This subsystem includes hardware support for synchronization, data transfer and certain types of particle interactions, along with specialized instructions for geometric operations. All aspects of the flexible subsystem were designed specifically to accelerate MD simulations, and although it relies primarily on what may be regarded as ldquogeneral-purposerdquo processors, even this subsystem contains more application-specific features than many recently proposed ldquospecializedrdquo architectures. },
}

@inproceedings{4798230,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Banerjee, P.},
 year = {2009},
 pages = {3--4},
 publisher = {IEEE},
 title = {An intelligent IT infrastructure for the future},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798230},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798230},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798230.pdf?arnumber=4798230},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Automation, Cloud computing, Computer networks, Concurrent computing, DP industry, Data security, Hewlett Packard Labs, Intelligent networks, Intelligent structures, Nanotechnology, Optical computing, Software performance, automation, automation, cloud-scale system, computer centres, computer networks, datacenters, exascale computing, information technology, intelligent IT infrastructure, intelligent networks, intelligent networks, intelligent storage system, microprocessor chips, multiple CPU cores, multiprocessing systems, nanotechnology, nanotechnology, parallel processes, processor chip, programmable wired network platform, programmable wireless network platform, security of data, smart data centers, transition computing, user adaptability, user security, utility computing, virtualization, wireless LAN, },
 abstract = {An intelligent IT Infrastructure will deliver extremely high performance, adaptability and security to users in the future. Building on the advancements in utility computing, smart data centers, automation, virtualization and intelligent networks, Hewlett Packard Labs is positioning itself to redefine datacenters, networks, software and devices. This talk will provide an overview of research being performed at HP Labs on four areas that will enable an Intelligent Infrastructure: Computing, storage, networking and nanotechnology. 1) We're helping transition computing to exascale computing, in which every processor chip has multiple CPU cores, and a new generation of software puts these parallel processes to good use; 2) We're building a cloud-scale, intelligent storage system that is self-managed and enterprise-grade; 3) A programmable wired and wireless network platform will make the introduction of new features quick, easy and cost-effective; 4) And breakthroughs in nanotechnology are going to revolutionize the way data is collected, stored and transmitted, using technologies such as the memristor and photonic interconnects. },
}

@inproceedings{4658653,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Jiang Lin and Qingda Lu and Xiaoning Ding and Zhao Zhang and Xiaodong Zhang and Sadayappan, P.},
 year = {2008},
 pages = {367--378},
 publisher = {IEEE},
 title = {Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658653},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658653},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658653.pdf?arnumber=4658653},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Computational modeling, Computer simulation, Hardware, Kernel, Multicore processing, OS kernel, Operating systems, QoS, Quality of service, Runtime, SPEC CPU2006 benchmark, Software performance, Yarn, cache sharing, cache storage, dynamic cache partitioning, memory address mapping, multi-threading, multicore processor utilization, multiprocessing systems, operating system, operating system kernels, optimization, quality of service, quality of service, static cache partitioning, },
 abstract = {Cache partitioning and sharing is critical to the effective utilization of multicore processors. However, almost all existing studies have been evaluated by simulation that often has several limitations, such as excessive simulation time, absence of OS activities and proneness to simulation inaccuracy. To address these issues, we have taken an efficient software approach to supporting both static and dynamic cache partitioning in OS through memory address mapping. We have comprehensively evaluated several representative cache partitioning schemes with different optimization objectives, including performance, fairness, and quality of service (QoS). Our software approach makes it possible to run the SPEC CPU2006 benchmark suite to completion. Besides confirming important conclusions from previous work, we are able to gain several insights from whole-program executions, which are infeasible from simulation. For example, giving up some cache space in one program to help another one may improve the performance of both programs for certain workloads due to reduced contention for memory bandwidth. Our evaluation of previously proposed fairness metrics is also significantly different from a simulation-based study. The contributions of this study are threefold. (1) To the best of our knowledge, this is a highly comprehensive execution- and measurement-based study on multicore cache partitioning. This paper not only confirms important conclusions from simulation-based studies, but also provides new insights into dynamic behaviors and interaction effects. (2) Our approach provides a unique and efficient option for evaluating multicore cache partitioning. The implemented software layer can be used as a tool in multicore performance evaluation and hardware design. (3) The proposed schemes can be further refined for OS kernels to improve performance. },
}

@inproceedings{4658652,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Fensch, C. and Cintra, M.},
 year = {2008},
 pages = {355--366},
 publisher = {IEEE},
 title = {An OS-based alternative to full hardware coherence on tiled CMPs},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658652},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658652},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658652.pdf?arnumber=4658652},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Aggregates, Application software, Computer architecture, Degradation, Hardware, Informatics, Out of order, Protocols, Scalability, Tiles, cache coherence, cache storage, chip-multiprocessor system, cost-effective mechanism, data replication, full hardware coherence, integrated circuit interconnections, interconnect mechanism, microprocessor chips, operating system, operating systems (computers), parallel processing, shared memory systems, shared-memory parallel application, tiled CMP, },
 abstract = {The interconnect mechanisms (shared bus or crossbar) used in current chip-multiprocessors (CMPs) are expected to become a bottleneck that prevents these architectures from scaling to a larger number of cores. Tiled CMPs offer better scalability by integrating relatively simple cores with a lightweight point-to-point interconnect. However, such interconnects make snooping impractical and, thus, require alternative solutions to cache coherence. This paper proposes a novel, cost-effective mechanism to support shared-memory parallel applications that forgoes hardware maintained cache coherence. The proposed mechanism is based on the key ideas that mapping of lines to physical caches is done at the page level with OS support and that hardware supports remote cache accesses. It allows only some controlled migration and replication of data and provides a sufficient degree of flexibility in the mapping through an extra level of indirection between virtual pages and physical tiles. We evaluate the proposed tiled CMP architecture on the Splash-2 scientific benchmarks and ALPBench multimedia benchmarks against one with private caches and a distributed directory cache coherence mechanism. Experimental results show that the performance degradation is as little as 0\%, and 16\% on average, compared to the cache coherent architecture across all benchmarks for 16 and 32 processors. },
}

@inproceedings{4658655,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Rashid, M.W. and Huang, M.C.},
 year = {2008},
 pages = {393--404},
 publisher = {IEEE},
 title = {Supporting highly-decoupled thread-level redundancy for parallel programs},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658655},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658655},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658655.pdf?arnumber=4658655},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Circuit noise, Delay, Error correction, Logic, Memory management, Microarchitecture, Noise level, Noise reduction, Redundancy, Voltage, coarse-grain thread-level redundancy, coherence logic, computation logic, consistency logic, microprocessor chips, multi-threading, natural noise tolerance level, parallel programs, power aware computing, redundancy, thread-level redundancy, transistors, transistors, },
 abstract = {The continued scaling of device dimensions and the operating voltage reduces the critical charge and thus natural noise tolerance level of transistors. As a result, circuits can produce transient upsets that corrupt program execution and data. Redundant execution can detect and correct circuit errors on the fly. The increasing prevalence of multi-core architectures makes coarse-grain thread-level redundancy (TLR) very attractive. While TLR has been extensively studied in the context of single-threaded applications, much less attention is paid to the design issues and tradeoffs of supporting parallel codes. In this paper, we propose a microarchitecture to efficiently support TLR for parallel codes. One of the main design goals is to support a large number of unverified instructions, so that long latencies in verification can be easily tolerated. Another important objective is to have a comprehensive coverage that includes not only the computation logic but also the coherence and consistency logic in the memory subsystem. Hence, the redundant copy of the program needs to independently access the memory and the system needs to efficiently manage the non-determinism in parallel execution. The proposed architectural support to achieve these goals is entirely off the processor critical path and can be easily disabled when redundancy is not requested. The design, with a few effective optimizations, is also efficient in that during error-free execution, it causes less than 3\% additional performance degradation on top of throughput loss due to redundancy. },
}

@inproceedings{4658654,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Gupta, M.S. and Rangan, K.K. and Smith, M.D. and Gu-Yeon Wei and Brooks, D.},
 year = {2008},
 pages = {381--392},
 publisher = {IEEE},
 title = {DeCoR: A Delayed Commit and Rollback mechanism for handling inductive noise in processors},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658654},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658654},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658654.pdf?arnumber=4658654},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {DeCoR, Delay, Feedback loop, Microprocessors, Noise robustness, Packaging, Protection, Sensor phenomena and characterization, Stress, Timing, Voltage fluctuations, architected register state, cache write paths, checkpoint-recovery schemes, delayed commit, inductive noise, microcomputers, microprocessor, noise, noise-margin violations, processors, rollback mechanism, voltage fluctuations, voltage sensors, },
 abstract = {Increases in peak current draw and reductions in the operating voltage of processors stress the importance of dealing with voltage fluctuations in processors. Noise-margin violations lead to undesired effects, like timing violations, which may result in incorrect execution of applications. Several recent architectural solutions for inductive noise have been proposed that, unfortunately, have a strong correlation to the underlying power-delivery package model and require a feedback loop that is largely constrained by the voltage/current sensor characteristics. The resulting solutions are not robust across a wide range of microprocessor designs and packaging technologies. This paper proposes a Delayed-commit and rollback scheme (DeCoR) that guarantees correctness, insensitive to the package model or the responsiveness of the voltage sensors. In particular, our approach recovers from, rather than attempting to avoid, voltage emergencies. This approach incurs a small performance penalty when compared to an ideal machine that does not have voltage emergencies. We show that explicit checkpoint-recovery schemes, intended to handle infrequent events, e.g., radiation-induced soft errors, suffer from large performance overheads for frequently-occurring voltage emergencies. DeCoR requires very few modifications to modern processor designs, as it leverages the existing store queue and reorder buffers. Unlike conventional designs that conservatively protect all components of the processor from inductive noise with overly-large timing margins, our approach only requires conservative protection of the architected register state and cache write paths. },
}

@inproceedings{4658657,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Kaiyu Chen and Malik, S. and Patra, P.},
 year = {2008},
 pages = {415--426},
 publisher = {IEEE},
 title = {Runtime validation of memory ordering using constraint graph checking},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658657},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658657},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658657.pdf?arnumber=4658657},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Computer bugs, Formal verification, Hardware, Logic, Memory architecture, Memory management, Microarchitecture, Protection, Runtime, Silicon, constraint graph checking, deep-state logic bug, directed graphs, end-to-end correctness validation, formal specification, inter-processor communication, memory ordering specification, microarchitecture level, multicore shared memory system, optimization technique, pre-silicon verification, program debugging, program verification, runtime memory ordering validation, shared memory systems, storage management, system monitoring, },
 abstract = {An important correctness issue for emerging multi/many-core shared memory systems is to ensure that the inter-processor communication through shared memory conforms to the memory ordering rules, as specified by the architecturepsilas memory consistency model. This presents a significant validation challenge. Growing system complexity makes it increasingly hard to identify all deep-state logic bugs in pre-silicon verification. Further, aggressive technology scaling makes hardware more vulnerable to dynamic errors that can only be detected at runtime. In this paper, we propose an approach for runtime validation of memory ordering. This allows us to survive bugs that escape pre-silicon verification, as well as deal with emerging dynamic errors. Our solution consists of two parts: 1) at the microarchitecture level, we add efficient hardware support to capture the observed ordering among shared-memory operations; 2) we perform online verification of the observed memory ordering by checking for cycles in the constraint graph. We combine these to achieve end-to-end correctness validation of the system execution with respect to the memory ordering specification. There are several challenges that need to be addressed to make this approach practical. We describe these, as well as optimization techniques for reducing the hardware overhead. Estimates obtained from preliminary chip multiprocessor simulation experiments show that the proposed techniques are very effective in achieving acceptable hardware overhead and minimal performance impact. },
}

@inproceedings{4798231,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {5--6},
 publisher = {IEEE},
 title = {Session 1 Best paper nominees},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798231},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798231},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798231.pdf?arnumber=4798231},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798231.png" border="0"> },
}

@inproceedings{4658659,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {},
 year = {2008},
 pages = {xiii--xxi},
 publisher = {IEEE},
 title = {Author Index},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658659},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658659},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658659.pdf?arnumber=4658659},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 abstract = {},
}

@inproceedings{4658658,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Allen, F.},
 year = {2008},
 pages = {429--429},
 publisher = {IEEE},
 title = {Compilers and parallel computing systems},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658658},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658658},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658658.pdf?arnumber=4658658},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Art, Computer languages, Concurrent computing, High performance computing, Laboratories, Natural languages, Optimizing compilers, Parallel processing, Program processors, Societies, high performance system, parallel computing, parallel programming, program compiler, program compilers, programming language, programming languages, },
 abstract = {Increasing the delivered performance of computers by running programs in parallel is an old idea with a new urgency. Multi cores (multi processors) on chips have emerged as a way to increase performance wherever chips are used. The talk will focus on the role programming languages and compilers must play in delivering parallel performance to users and applications. The speakerpsilas personal experiences with languages and compilers for high performance systems will provide the basis for her observations. The talk is intended to encourage the exploration of new approaches. },
}

@inproceedings{4798232,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Ebrahimi, E. and Mutlu, O. and Patt, Y.N.},
 year = {2009},
 pages = {7--17},
 publisher = {IEEE},
 title = {Techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798232},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798232},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798232.pdf?arnumber=4798232},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Bandwidth, Costs, Data structures, Degradation, Feedback, Filtering, Hardware, Large-scale systems, Prefetching, Runtime, bandwidth-efficient prefetching, compiler-guided prefetch filtering mechanism, data structures, hardware-software cooperative technique, hybrid prefetching systems, linked data structures, memory system, storage management, stream-based prefetchers, },
 abstract = {Linked data structure (LDS) accesses are critical to the performance of many large scale applications. Techniques have been proposed to prefetch such accesses. Unfortunately, many LDS prefetching techniques 1) generate a large number of useless prefetches, thereby degrading performance and bandwidth efficiency, 2) require significant hardware or storage cost, or 3) when employed together with stream-based prefetchers, cause significant resource contention in the memory system. As a result, existing processors do not employ LDS prefetchers even though they commonly employ stream-based prefetchers. This paper proposes a low-cost hardware/software cooperative technique that enables bandwidth-efficient prefetching of linked data structures. Our solution has two new components: 1) a compiler-guided prefetch filtering mechanism that informs the hardware about which pointer addresses to prefetch, 2) a coordinated prefetcher throttling mechanism that uses run-time feedback to manage the interference between multiple prefetchers (LDS and stream-based) in a hybrid prefetching system. Evaluations show that the proposed solution improves average performance by 22.5\% while decreasing memory bandwidth consumption by 25\% over a baseline system that employs an effective stream prefetcher on a set of memory- and pointer-intensive applications. We compare our proposal to three different LDS/correlation prefetching techniques and find that it provides significantly better performance on both single-core and multi-core systems, while requiring less hardware cost. },
}

@inproceedings{4798233,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Reddi, V.J. and Gupta, M.S. and Holloway, G. and Gu-Yeon Wei and Smith, M.D. and Brooks, D.},
 year = {2009},
 pages = {18--29},
 publisher = {IEEE},
 title = {Voltage emergency prediction: Using signatures to reduce operating margins},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798233},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798233},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798233.pdf?arnumber=4798233},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Degradation, Delay effects, Microarchitecture, Microprocessors, Noise reduction, Surge protection, Threshold voltage, Timing, Voltage control, Voltage fluctuations, fault diagnosis, inductive noise forces microprocessor, microarchitectural events, microprocessor chips, operating margin reduction, oracle-based predictor, sensor delays, sensor-based reactive schemes, voltage emergency prediction, voltage noise, worst-case droops, },
 abstract = {Inductive noise forces microprocessor designers to sacrifice performance in order to ensure correct and reliable operation of their designs. The possibility of wide fluctuations in supply voltage means that timing margins throughout the processor must be set pessimistically to protect against worst-case droops and surges. While sensor-based reactive schemes have been proposed to deal with voltage noise, inherent sensor delays limit their effectiveness. Instead, this paper describes a voltage emergency predictor that learns the signatures of voltage emergencies (the combinations of control flow and microarchitectural events leading up to them) and uses these signatures to prevent recurrence of the corresponding emergencies. In simulations of a representative superscalar microprocessor in which fluctuations beyond 4\% of nominal voltage are treated as emergencies (an aggressive configuration), these signatures can pinpoint the likelihood of an emergency some 16 cycles ahead of time with 90\% accuracy. This lead time allows machines to operate with much tighter voltage margins (4\% instead of 13\%) and up to 13.5\% higher performance, which closely approaches the 14.2\% performance improvement possible with an ideal oracle-based predictor. },
}

@inproceedings{569689,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Bhandarkar, D. and Ding, J.},
 year = {1997},
 pages = {288--297},
 publisher = {IEEE},
 title = {Performance characterization of the Pentium Pro processor},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569689},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569689},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569689.pdf?arnumber=569689},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Clocks, Computer architecture, Counting circuits, Decoding, Out of order, Pentium Pro processor, Performance analysis, Performance evaluation, Registers, Sprites (computer), Statistics, architectural data, benchmarks, computer architecture, memory system, microprocessor chips, nonblocking cache, performance characterization, performance evaluation, performance monitoring counter tool, },
 abstract = {In this paper, we characterize the performance of several business and technical benchmarks on a Pentium Pro processor based system. Various architectural data are collected using a performance monitoring counter tool. Results show that the Pentium Pro processor achieves significantly lower cycles per instruction than the Pentium processor due to its out of order and speculative execution, and non-blocking cache and memory system. Its higher clock frequency also contributes to even higher performance },
}

@inproceedings{569686,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Zhang, Z. and Torrellas, J.},
 year = {1997},
 pages = {272--281},
 publisher = {IEEE},
 title = {Reducing remote conflict misses: NUMA with remote cache versus COMA },
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569686},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569686},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569686.pdf?arnumber=569686},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {COMA, Contracts, Costs, Memory architecture, NASA, NUMA, NUMA-RC, Random access memory, Research and development, Splash2 applications, cache storage, cache-only memory architecture, data sharing, memory architecture, performance evaluation, read-mostly migration, read-writs migration, remote cache, remote conflict misses reduction, replication, scalable shared-memory multiprocessors, shared memory systems, },
 abstract = {Many future applications for scalable shared-memory multiprocessors are likely to have large working sets that overflow secondary or tertiary caches. Two possible solutions to this problem are to add a very large cache called remote cache that caches remote data (NUMA-RC), or organize the machine as a cache-only memory architecture (COMA). This paper tries to determine which solution is best. To compare the performance of the two organizations for the same amount of total memory, we introduce a model of data sharing. The model uses three data sharing patterns: replication, read-mostly migration, and read-writs migration. Replication data is accessed in read-mostly mode by several processors, while migration data is accessed largely by one processor at a time. For large working sets, the weight of the migration data largely determines whether COMA outperforms NUMA-RC. Ideally, COMA only needs to fit the replication data in its extra memory; the migration data will simply be swapped between attraction memories. The remote cache of NUMA-RC, instead, needs to house both the replication and the migration data. However, simulations of seven Splash2 applications show that COMA does not outperform NUMA-RC. This is due to two reasons. First, the extra memory added has more associativity in NUMA-RC than in COMA and, therefore, can be utilized better by the working set in NUMA-RC. Second, COMA memory accesses are more expensive. Of course, our results are affected by the applications used, which have been optimized for a cache-coherent NUMA machine. Overall, since NUMA-RC is cheaper, NUMA-RC is more cost-effective for these applications },
}

@inproceedings{903264,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Goeman, B. and Vandierendonck, H. and de Bosschere, K.},
 year = {2001},
 pages = {207--216},
 publisher = {IEEE},
 title = {Differential FCM: increasing value prediction accuracy by improving table usage efficiency},
 date = {2001},
 doi = {10.1109/HPCA.2001.903264},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903264},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903264.pdf?arnumber=903264},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Accuracy, Counting circuits, Information systems, Interference, Microprocessors, Upper bound, computer architecture, differential finite context method, finite context method, hybrid predictors, metrics, microprocessor chips, microprocessors, perfect meta-predictor, table usage efficiency, value prediction accuracy, value predictor, },
 abstract = {Value prediction is a relatively new technique to increase the Instruction Level Parallelism (ILP) in future microprocessors. An important problem when designing a value predictor is efficiency, an accurate predictor requires huge prediction tables. This is especially the case for the finite context method (FCM) predictor the most accurate one. In this paper, we show that the prediction accuracy of the FCM can be greatly improved by making the FCM predict studies instead of values. This new predictor is called the differential finite context method (DFCM) predictor. The DFCM predictor outperforms a similar FCM predictor by as much as 33\%, depending on the prediction table size. If we take the additional storage into account, the difference is still 15\% for realistic predictor sizes. We use several metrics to show that the key to this success is reduced aliasing in the level-2 table. We also show that the DFCM is superior to hybrid predictors based on FCM and stride predictors, since its prediction accuracy is higher than that of a hybrid one using a perfect meta-predictor },
}

@inproceedings{569680,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Trancoso, P. and Larriba-Pey, J.-L. and Zhang, Z. and Torrellas, J.},
 year = {1997},
 pages = {250--260},
 publisher = {IEEE},
 title = {The memory performance of DSS commercial workloads in shared-memory multiprocessors},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569680},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569680},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569680.pdf?arnumber=569680},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Computer architecture, Contracts, DSS commercial workloads, Data analysis, Data structures, Database systems, Decision support systems, Index queries, Pattern analysis, Prefetching, Research and development, Sequential queries, Spatial databases, data prefetching, database management systems, decision support system databases, decision support systems, lock-related metadata structures, memory access patterns, memory performance, memory subsystem, memory use, performance, performance evaluation, shared memory systems, shared-data misses, shared-memory multiprocessors, spatial locality, },
 abstract = {Although cache-coherent shared-memory multiprocessors are often used to run commercial workloads, little work has been done to characterize how well these machines support such workloads. In particular, we do not have much insight into the demands of commercial workloads on the memory subsystem of these machines. In this paper, we analyze in detail the memory access patterns of several queries that are representative of Decision Support System (DSS) databases. Our analysis shows that the memory use of queries differs largely depending on how the queries access the database data, namely via indices or by sequentially scanning the records. The former queries, which we call Index queries, suffer most of their shared-data misses on indices and on lock-related metadata structures. The latter queries, which we call Sequential queries, suffer most of their shared-data misses on the database records as they are scanned. An analysis of the data locality in the queries shows that both Index and Sequential queries exhibit spatial locality and, therefore, can benefit from relatively long cache lines. Interestingly, shared data is reused very little inside queries. However, there is data reuse across Sequential queries. Finally, we show that the performance of Sequential queries can be improved moderately with data prefetching },
}

@inproceedings{569681,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Amza, C. and Cox, A.L. and Dwarkadas, S. and Zwaenepoel, W.},
 year = {1997},
 pages = {261--271},
 publisher = {IEEE},
 title = {Software DSM protocols that adapt between single writer and multiple writer},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569681},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569681},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569681.pdf?arnumber=569681},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Access protocols, Automatic logic units, Computer science, Costs, Hardware, Merging, SPARC cluster, Tires, distributed memory systems, multiple writer, performance, performance evaluation, performance factor, protocols, shared memory systems, single writer, software DSM protocols, write granularity, write-write false sharing, },
 abstract = {We present two software DSM protocols that dynamically adapt between a single writer (SW) and a multiple writer (MW) protocol based on the application's sharing patterns. The first protocol (WFS) adapts based on write-write false sharing; the second (WFS+WG) based on a combination of write-write false sharing and write granularity. The adaptation is automatic. No user or compiler information is needed. The choice between SW and MW is made on a per-page basis. We measured the performance of our adaptive protocols on an 8-node SPARC cluster connected by a 155 Mbps ATM network. We used eight applications, covering a broad spectrum in terms of write-write false sharing and write granularity. We compare our adaptive protocols against the MW-only and the SW-only approach. Adaptation to write-write false sharing proves to be the critical performance factor, while adaptation to write granularity plays only a secondary role in our environment and for the applications considered. Each of the two adaptive protocols matches or exceeds the performance of the best of MW and SW in seven out of the eight applications },
}

@inproceedings{903266,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Cain, H.W. and Rajwar, R. and Marden, M. and Lipasti, M.H.},
 year = {2001},
 pages = {229--240},
 publisher = {IEEE},
 title = {An architectural evaluation of Java TPC-W},
 date = {2001},
 doi = {10.1109/HPCA.2001.903266},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903266},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903266.pdf?arnumber=903266},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, Computer languages, Hardware, Java, Java, Java programming language, Java servlets, Logic programming, Middleware, TPC-W, TPC-W web benchmark, Throughput, Transaction databases, Web page design, Web server, coarse-grained multithreaded processor, performance evaluation, transaction processing, transaction processing, web server, },
 abstract = {The use of the Java programming language for implementing server-side application logic is increasingly in popularity yet there is very little known about the architectural requirements of this emerging commercial workload. We present a detailed characterization of the Transaction Processing Council's TPC-W web benchmark, implemented in Java. The TPC-W benchmark is designed to exercise the web server and transaction processing system of a typical e-commerce web site. We have implemented TPC-W as a collection of Java servlets, and present an architectural study detailing the memory system and branch predictor behavior of the workload. We also evaluate the effectiveness of a coarse-grained multithreaded processor at increasing system throughput using TPC-W and other commercial workloads. We measure system throughput improvements from 8\% to 41\% for a two context processor, and 12\% to 60\% for a four context uniprocessor over a single-threaded uniprocessor despite decreased branch prediction accuracy and cache hit rates },
}

@inproceedings{903267,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Zilles, C.B. and Sohi, G.S.},
 year = {2001},
 pages = {241--252},
 publisher = {IEEE},
 title = {A programmable co-processor for profiling},
 date = {2001},
 doi = {10.1109/HPCA.2001.903267},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903267},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903267.pdf?arnumber=903267},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Computer architecture, Coprocessors, Costs, Data mining, Hardware, Microprocessors, Resource management, Testing, Usability, computer architecture, coprocessors, processor, profile samples, profiling, program optimization, programmable co-processor, },
 abstract = {Aggressive program optimization requires accurate profile information, but such accuracy requires many samples to be collected. We explore a novel profiling architecture that reduces the overhead of collecting each sample by including a programmable co-processor that analyzes a stream of profile samples generated by a microprocessor. From this stream of samples, the co-processor can detect correlations between instructions (e.g., memory dependence profiling) as well as those between different dynamic instances of the same instruction (e.g., value profiling). The profiler's programmable nature allows a broad range of data to be extracted, post-processed, and formatted, as well as provides the flexibility to tailor the profiling application to the program under test. Because the co-processor is specialized for profiling, it can execute profiling applications more efficiently than a general-purpose processor. The co-processor should not significantly impact the cost or performance of the main processor because it can be implemented using a small number of transistors at the chip's periphery We demonstrate the proposed design through a detailed evaluation of load value profiling. Our implementation quickly and accurately estimates the value invariance of loads, with time overhead roughly proportional to the size of the instruction working set of the program. This algorithm demonstrates a number of general techniques for profiling, including: estimating the completeness of a profile, a means to focus profiling on particular instructions, management of profiling resources },
}

@inproceedings{744306,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {},
 year = {1999},
 publisher = {IEEE},
 title = {Proceedings Fifth International Symposium on High-Performance Computer Architecture},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744306},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744306},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744306.pdf?arnumber=744306},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {I/O systems, SMP clusters, cache coherence, communication issues, computer architecture, computer architecture, instruction scheduling, memory systems, performance enhancement, performance evaluation, shared memory systems, simultaneous multithreading, },
 abstract = {The following topics were dealt with: performance enhancement; simultaneous multithreading; memory systems; instruction scheduling and speculation; cache coherence; SMP clusters; cache and I/O systems; shared memory systems; and communication issues },
}

@inproceedings{744307,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {},
 year = {1999},
 pages = {v--viii},
 publisher = {IEEE},
 title = {Table of Contents},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744307},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744307},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744307.pdf?arnumber=744307},
 isbn = {0-7695-0004-8},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00744307.png" border="0"> },
}

@inproceedings{1598118,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Riley, N. and Zilles, C.},
 year = {2006},
 pages = { 110-- 120},
 publisher = {IEEE},
 title = {Probabilistic counter updates for predictor hysteresis and stratification},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598118},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598118},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598118.pdf?arnumber=1598118},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { 4-bit counter,  Likelihood of Criticality,  branch confidence,  counter decrement,  counter increment,  criticality predictor,  frequency stratifier,  high-performance processor,  parallel architectures,  performance evaluation,  predictor hysteresis,  predictor stratification,  probabilistic counter update,  pseudo-random number generator, Application software, Computer science, Counting circuits, Encoding, Frequency, Hardware, Hysteresis, Linear feedback shift registers, Negative feedback, },
 abstract = {Hardware counters are a fundamental building block of modern high-performance processors. This paper explores two applications of probabilistic counter updates, in which the output of a pseudo-random number generator decides whether to perform a counter increment or decrement. First, we discuss a probabilistic implementation of counter hysteresis, whereby previously proposed branch confidence and criticality predictors can be reduced in size by factors of 2 and 3, respectively, with negligible impact on performance. Second, we build a frequency stratifier by making increment and decrement probabilities functions of the current counter value. The stratifier enables a 4-bit counter to classify an instruction's Likelihood of Criticality with sufficient accuracy to closely approximate the performance of an unbounded precision classifier. Because probabilistic updates are both simple and effective, we believe these ideas hold great promise for immediate use by industry, perhaps enabling the use of structures such as branch confidence predictors which may have previously been viewed as too expensive given their functionality. },
}

@inproceedings{995717,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Carrera, E.V. and Rao, S. and Iftode, L. and Bianchini, R.},
 year = {2002},
 pages = { 275-- 286},
 publisher = {IEEE},
 title = {User-level communication in cluster-based servers},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995717},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995717},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995717.pdf?arnumber=995717},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Internet,  Internet services,  client-server systems,  cluster nodes,  cluster-based WWW server,  cluster-based servers,  commodity computers clusters,  intra-cluster communication architecture,  network bandwidth,  network servers,  processor overhead,  remote memory writes,  scalability,  user-level communication,  workload characteristics,  workstation clusters,  zero-copy data transfers, Analytical models, Computer architecture, Computer science, Network servers, Read-write memory, Scalability, Stress, Web and internet services, Web server, World Wide Web, },
 abstract = {Clusters of commodity computers are currently being used to provide the scalability required by several popular Internet services. In this paper we evaluate an efficient cluster-based WWW server, as a function of the characteristics of the intra-cluster communication architecture. More specifically, we evaluate the impact of processor overhead, network bandwidth, remote memory writes, and zero-copy data transfers on the performance of our server. Our experimental results with an 8-node cluster and four real WWW traces show that network bandwidth affects the performance of our server by only 6\%. In contrast, user-level communication can improve performance by as much as 29\%. Low processor overhead, remote memory writes, and zero-copy all make small contributions towards this overall gain. To be able to extrapolate from our experimental results, we use an analytical model to assess the performance of our server under different workload characteristics, different numbers of cluster nodes, and higher performance systems. Our modeling results show that higher gains (of up to 55\%) can be accrued for workloads with large working sets and next-generation servers running on large clusters. },
}

@inproceedings{995716,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Jamieson, P. and Bilas, A.},
 year = {2002},
 pages = { 263-- 274},
 publisher = {IEEE},
 title = {CableS : thread control and memory management extensions for shared virtual memory clusters},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995716},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995716},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995716.pdf?arnumber=995716},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { M4 macros,  OdinMP,  OpenMP programs,  SPLASH-2 applications,  Windows NT limitations,  data placement,  high-end workstations,  large-scale computations,  legacy pthreads applications,  low-latency high-bandwidth system,  memory management,  multi-threading,  public domain OpenMP compiler,  scalable servers,  shared memory abstraction,  shared memory systems,  single-cluster image,  thread management,  tightly-coupled systems,  virtual memory mappings granularity,  virtual storage,  workstation clusters, Cables, Cost function, High performance computing, Large-scale systems, Memory management, Personal communication networks, Program processors, Programming profession, Workstations, Yarn, },
 abstract = {Clusters of high-end workstations and PCs are currently used in many application domains to perform large-scale computations or as scalable servers for I/O bound tasks. Although clusters have many advantages, their applicability in emerging areas of applications has been limited. One of the main reasons for this is the fact that clusters do not provide a single system image and thus are hard to program. In this work we address this problem by providing a single-cluster image with respect to thread and memory management. We implement our system, CableS (Cluster enabled threads), on a 32-processor cluster interconnected with a low-latency, high-bandwidth system area network and conduct an early exploration of the costs involved in providing the extra functionality. We demonstrate the versatility :of Cables with a wide range of applications and show that clusters can be used to support applications that have been written for more expensive tightly-coupled systems, With very little effort on the programmer side: (a) We run legacy pthreads applications without any major modifications. (b) We use a public domain OpenMP compiler (OdinMP) to translate OpenMP programs to pthreads and execute them on our system, with no or few modifications to the translated pthreads source code. (c) We provide an implementation of the M4 macros for our pthreads system and run the SPLASH-2 applications. We also show that the overhead introduced by the extra functionality of CableS affects the parallel section of applications that have been tuned for the shared memory abstraction only in cases where the data placement is affected by operating system (WindowsNT) limitations in virtual memory mappings granularity. },
}

@inproceedings{569617,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {August, D.I. and Connors, D.A. and Gyllenhaal, J.C. and Hwu, W.-M.W.},
 year = {1997},
 pages = {84--93},
 publisher = {IEEE},
 title = {Architectural support for compiler-synthesized dynamic branch prediction strategies: Rationale and initial results},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569617},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569617},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569617.pdf?arnumber=569617},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Accuracy, Costs, Counting circuits, Dynamic compiler, Hardware, History, IMPACT compiler, Microarchitecture, Runtime, Statistics, architectural approach, code sequence, code sequences, compiler-synthesized dynamic branch prediction strategies, execution statistics, parallel architectures, predicate enhanced prediction, predicate only prediction, program compilers, },
 abstract = {This paper introduces a new architectural approach that supports compiler-synthesized dynamic branch predication. In compiler-synthesized dynamic branch prediction, the compiler generates code sequences that, when executed, digest relevant state information and execution statistics into a condition bit, or predicate. The hardware then utilizes this information to make predictions. Two categories of such architectures are proposed and evaluated. In Predicate Only Prediction (POP), the hardware simply uses the condition generated by the code sequence as a prediction. In Predicate Enhanced Prediction (PEP), the hardware uses the generated condition to enhance the accuracy of conventional branch prediction hardware. The IMPACT compiler currently provides a minimal level of compiler support for the proposed approach. Experiments based on current predicated code show that the proposed predictors achieve better performance than conventional branch predictors. Furthermore, they enable future compiler techniques which have the potential to achieve extremely high branch prediction accuracies. Several such compiler techniques are proposed in this paper },
}

@inproceedings{995714,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Patterson, D.A.},
 year = {2002},
 pages = { 223-- 223},
 publisher = {IEEE},
 title = {Recovery oriented computing: a new research agenda for a new century},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995714},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995714},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995714.pdf?arnumber=995714},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = {Advertising, Building services, Computer applications, Computer science, Costs, Fault tolerant systems, Hardware, Humans, Operating systems, Web and internet services, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00995714.png" border="0"> },
}

@inproceedings{995713,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Parikh, D. and Skadron, K. and Yan Zhang and Barcella, M. and Stan, M.R.},
 year = {2002},
 pages = { 233-- 244},
 publisher = {IEEE},
 title = {Power issues related to branch prediction},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995713},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995713},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995713.pdf?arnumber=995713},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { banking,  branch predictor organization,  branch target buffer accesses,  microprocessor chips,  on-chip structure,  overall energy consumption,  parallel architectures,  power dissipation,  prediction probe detector,  processor design, Accuracy, Banking, Computer science, Detectors, Embedded computing, Energy consumption, Energy dissipation, Power dissipation, Probes, Process design, },
 abstract = {This paper explores the role of branch predictor organization in power/energy/performance tradeoffs for processor design. We find that as a general rule, to reduce overall energy consumption in the processor it is worthwhile to spend more power in the branch predictor if this results in more accurate predictions that improve running time. Two techniques, however, provide substantial reductions in power dissipation without harming accuracy. Banking reduces the portion of the branch predictor that is active at any one time. And a new on-chip structure, the prediction probe detector (PPD), can use pre-decode bits to entirely eliminate unnecessary predictor and branch target buffer (BTB) accesses. Despite the extra power that must be spent accessing the PPD, it reduces local predictor power and energy dissipation by about 45\% and overall processor power and energy dissipation by 5-6\%. },
}

@inproceedings{995712,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Kampe, M. and Stenstrom, P. and Dubois, M.},
 year = {2002},
 pages = { 223-- 232},
 publisher = {IEEE},
 title = {The FAB predictor: using Fourier analysis to predict the outcome of conditional branches},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995712},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995712},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995712.pdf?arnumber=995712},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Fourier Analysis Branch predictor,  Fourier analysis,  SPEC95 suite,  branch outcome history,  frequency domain,  hybrid branch predictor,  integer applications,  long periodic branch history patterns,  misprediction,  parallel programming,  static scheme, Accuracy, Data compression, Fourier transforms, Frequency domain analysis, History, Parallel processing, Pattern analysis, Pipelines, Runtime, Time domain analysis, },
 abstract = {This paper proposes to transform the branch outcome history from the time domain to the frequency domain. With our proposed Fourier Analysis Branch (FAB) predictor, we can represent long periodic branch history patterns - as long as 2<sup>13</sup> bits - with a realistic number of bits (52 bits). We evaluate the potential gains of the FAB predictor by considering a hybrid branch predictor in which each branch is predicted using a static scheme, the 2-bit dynamic scheme, the PAp and GAp schemes, and our FAB predictor. By including our FAB predictor in the hybrid predictor, it is possible to cut the misprediction rate of integer applications in the SPEC95 suite by between 5 and 50\% with an average of 20\%. Besides evaluating its performance, this paper shows some key properties of our FAB predictor and presents some possible implementation approaches. },
}

@inproceedings{995711,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Sazeides, Y.},
 year = {2002},
 pages = { 211-- 222},
 publisher = {IEEE},
 title = {Modeling value speculation},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995711},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995711},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995711.pdf?arnumber=995711},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { dynamically-scheduled microarchitectures,  fast verification latency,  microarchitectural operations latency,  parallel programming,  performance potential,  speculative execution,  value-speculation,  value-speculation related events,  value-speculative microarchitectures, Application software, Computer science, Counting circuits, Delay, Discrete event simulation, Hardware, Microarchitecture, Parallel processing, Registers, Software performance, },
 abstract = {Several studies of speculative execution based on values have reported promising performance potential. However, virtually all microarchitectures in these studies were described in an ambiguous manner, mainly due to the lack of formalization that defines the effects of value-speculation on a microarchitecture. In particular, the manifestations of value-speculation on the latency of microarchitectural operations, such as releasing resources and reissuing, was at best partially addressed. This may be problematic since results obtained in these studies can be difficult to reproduce and/or appreciate their contribution. This paper introduces a model for a methodical description of dynamically-scheduled microarchitectures that use value-speculation. The model isolates the parts of a microarchitecture that may be influenced by value-speculation in terms of various variables and latency events. This provides systematic means for describing, evaluating and comparing the,performance of value-speculative microarchitectures. The model parameters are integrated in a simulator to investigate the performance of several value-speculation related events. Among other, the results show value-speculation performance to have non-uniform sensitivity to changes in the latency of these events. For example, fast verification latency is found to be essential, but when mis-speculation is infrequent slow invalidation may be acceptable. },
}

@inproceedings{995710,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Sair, S. and Sherwood, T. and Calder, B.},
 year = {2002},
 pages = { 197-- 208},
 publisher = {IEEE},
 title = {Quantifying load stream behavior},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995710},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995710},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995710.pdf?arnumber=995710},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { Olden benchmarks,  SPEC 2000,  access pattern,  access pattern identification,  architectural features,  cache storage,  caches,  feedback-directed optimizations,  hardware profiling architecture,  load stream behavior,  memory latency,  performance gap,  pointer based applications,  prefetching,  program compilers,  software metrics, Application software, Computer architecture, Computer science, Delay, Hardware, Impedance, Prefetching, Resource management, Software algorithms, Software performance, },
 abstract = {The increasing performance gap between processors and memory will force future architectures to devote significant resources towards removing and hiding memory latency. The two major architectural features used to address this growing gap are caches and prefetching. In this paper we perform a detailed quantification of the cache miss patterns for the Olden benchmarks, SPEC 2000 benchmarks, and a collection of pointer based applications. We classify misses into one of four categories corresponding to the type of access pattern. These are next-line, stride, same-object (additional misses that occur to a recently accessed object), or pointer-based transitions. We then propose and evaluate a hardware profiling architecture to correctly identify which type of access pattern is being seen. This access pattern identification could be used to help guide and allocate prefetching resources, and provide information to feedback-directed optimizations. A second goal of this paper is to identify a suite of challenging pointer-based benchmarks that can be used to focus the development of new software and hardware prefetching algorithms, and identify the challenges in performing prefetching for these applications using new metrics. },
}

@inproceedings{995719,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Borch, E. and Tune, E. and Manne, S. and Emer, J.},
 year = {2002},
 pages = { 299-- 310},
 publisher = {IEEE},
 title = {Loose loops sink chips},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995719},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995719},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995719.pdf?arnumber=995719},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { architectural simulator,  distributed register algorithm,  issue-to-execute latency,  load mis-speculations,  load resolution loop,  loose loops,  micro-architectural loops,  microprocessor chips,  multi-threading,  pipeline processing,  processor pipelines, Computational modeling, Computer science, Delay, Feedback loop, Frequency, Hazards, Performance loss, Pipelines, Process design, Registers, },
 abstract = {This paper explores the concept of micro-architectural loops and discusses their impact on processor pipelines. In particular, we establish the relationship between loose loops and pipeline length and configuration, and show their impact on performance. We then evaluate the load resolution loop in detail and propose the distributed register algorithm (DRA) as a way of reducing this loop. It decreases the performance loss due to load mis-speculations by reducing the issue-to-execute latency in the pipeline. A new loose loop is introduced into the pipeline by the DRA, but the frequency of mis-speculations is very low. The reduction in latency from issue to execute, along with a low mis-speculation rate in the DRA result in up to a 4\% to 15\% improvement in performance using a detailed architectural simulator. },
}

@inproceedings{995718,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Brown, M.D. and Patt, Y.N.},
 year = {2002},
 pages = { 289-- 298},
 publisher = {IEEE},
 title = {Using internal redundant representations and limited bypass to support pipelined adders and register files},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995718},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995718},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995718.pdf?arnumber=995718},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { SPECint95 benchmarks,  adders,  execution bandwidth,  internal redundant representations,  microprocessor chips,  multi-level bypass networks,  out-of-order execution cores,  pipeline arithmetic,  pipelined adders,  pipelined functional units,  redundant binary adders,  register files, Bandwidth, Clocks, Delay, Feeds, Frequency, Logic, Microprocessors, Out of order, Processor scheduling, Registers, },
 abstract = {This paper evaluates the use of redundant binary and pipelined 2's complement adders in out-of-order execution cores. Redundant binary adders reduce the ADD latency to less than half that of traditional 2's complement adders, allowing higher core clock frequencies and greater execution bandwidth (in instructions per second). Pipelined 2's complement adders allow a higher clock frequency, but do not reduce the ADD latency. Machines with redundant binary adders are compared to machines with 2's complement adders and the same execution bandwidth and bypass network complexity. Results show that on the SPECint95 benchmarks, the average IPC of an 8-wide machine with 1-cycle redundant binary adders is 9\% higher than a machine using 2-cycle pipelined adders. Pipelined functional units and multi-cycle register files may require multi-level bypass networks to guarantee that an instruction's result is available any cycle after it is produced. Multi-level bypass networks require large fan-in input mixes that increase cycle time. This paper shows that one level of bypass paths in a multi-level bypass network can be removed while still achieving within 3\% to 1\% of the IPC of a machine with a full bypass network. },
}

@inproceedings{386529,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Hur, Y. and Szygenda, S.A. and Scott Fehr, E. and Ott, G.E. and Sungho Kang},
 year = {1995},
 pages = {340--347},
 publisher = {IEEE},
 title = {Massively parallel array processor for logic, fault, and design error simulation},
 date = {1995},
 doi = {10.1109/HPCA.1995.386529},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386529},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386529.pdf?arnumber=386529},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Analytical models, Circuit analysis computing, Circuit faults, Circuit simulation, Computational modeling, Hardware, Logic arrays, Logic circuits, Logic design, PE array, VLSI, VLSI circuits, Very large scale integration, circuit analysis computing, design error simulation, fault simulation, hardware accelerator, high performance, logic CAD, logic simulation, massively parallel array processor, parallel processing, prototyping, software simulation, },
 abstract = {Digital logic, fault, and error simulation of large VLSI circuits is one of the most compute-intensive tasks in digital systems analysis. This paper describes a massively parallel special purpose array processor, or hardware accelerator, for digital logic, fault, and error simulation. Hardware simulation is a viable approach for simulation of large systems, since simulation time increases rapidly as a function of the size and complexity of the systems to be simulated. In order to reduce the cost and to achieve high performance, a massively parallel array processor and new algorithms have been introduced. By executing an efficient and direct model of the design on the PE array, the architecture can provide high performance, similar to prototyping. Simulation results show that the hardware accelerator is orders of magnitude faster than software simulation },
}

@inproceedings{1598112,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Pericas, M. and Cristal, A. and Gonzalez, R. and Jimenez, D.A. and Valero, M.},
 year = {2006},
 pages = { 53-- 64},
 publisher = {IEEE},
 title = {A decoupled KILO-instruction processor},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598112},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598112},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598112.pdf?arnumber=1598112},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { cache processor,  cache storage,  computer architecture,  decoupled KILO-instruction processor,  decoupled microarchitecture,  high latency instructions,  instruction sets,  instruction window processor,  low latency instructions,  memory processor,  multiprocessing systems, Buildings, Computer science, Costs, Delay, Microarchitecture, Microprocessors, Pipelines, Proposals, Timing, Windows, },
 abstract = {Building processors with large instruction windows has been proposed as a mechanism for overcoming the memory wall, but finding a feasible and implementable design has been an elusive goal. Traditional processors are composed of structures that do not scale to large instruction windows because of timing and power constraints. However, the behavior of programs executed with large instruction windows gives rise to a natural and simple alternative to scaling. We characterize this phenomenon of execution locality and propose a microarchitecture to exploit it to achieve the benefit of a large instruction window processor with low implementation cost. Execution locality is the tendency of instructions to exhibit high or low latency based on their dependence on memory operations. In this paper we propose a decoupled microarchitecture that executes low latency instructions on a cache processor and high latency instructions on a memory processor. We demonstrate that such a design, using small structures and many in-order components, can achieve the same performance as much more aggressive proposals while minimizing design complexity. },
}

@inproceedings{1598113,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Samantika Subramaniam and Loh, G.H.},
 year = {2006},
 pages = { 65-- 76},
 publisher = {IEEE},
 title = {Store vectors for scalable memory dependence prediction and scheduling},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598113},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598113},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598113.pdf?arnumber=1598113},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { dependency vectors,  instruction sets,  load-store pair identification,  matrix schedulers,  multiprocessing systems,  nonmemory instructions,  scalable memory dependence prediction,  scheduling,  scheduling,  storage allocation,  store vector prediction algorithm,  superscalar processors, Computer aided instruction, Educational institutions, Job shop scheduling, Microarchitecture, Out of order, Parallel processing, Pipelines, Prediction algorithms, Processor scheduling, Read-write memory, },
 abstract = {Allowing loads to issue out-of-order with respect to earlier unresolved store addresses is very important for extracting parallelism in large-window superscalar processors. Blindly allowing all loads to issue as soon as their addresses are ready can lead to a net performance loss due to a large number of load-store ordering violations. Previous research has proposed memory dependence prediction algorithms to prevent only loads with true memory dependencies from issuing in the presence of unresolved stores. Techniques such as load-store pair identification and store sets have been very successful in achieving performance levels close to that attained by an oracle dependence predictor. These techniques tend to employ relatively complex CAM-based designs, which we believe have been obstacles to the industrial adoption of these algorithms. In this paper, we use the idea of dependency vectors from matrix schedulers for non-memory instructions, and adapt them to implement a new dependence prediction algorithm. For applications that experience frequent memory ordering violations, our "store vector" prediction algorithm delivers an 8.4\% speedup over blind speculation (compared to 8.5\% for perfect dependence prediction), achieves better performance than store sets (8.1\%), and the store vector algorithm's matrix implementation is considerably simpler. },
}

@inproceedings{1598110,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Penry, D.A. and Fay, D. and Hodgdon, D. and Wells, R. and Schelle, G. and August, D.I. and Connors, D.},
 year = {2006},
 pages = { 29-- 40},
 publisher = {IEEE},
 title = {Exploiting parallelism and structure to accelerate the simulation of chip multi-processors},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598110},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598110},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598110.pdf?arnumber=1598110},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { CMP simulators,  CMP structural models,  automated parallelization,  chip multiprocessors,  circuit simulation,  hardware PowerPC,  microarchitectures,  microprocessor chips,  multiple processors,  multiprocessing systems,  parallel processing,  shared-memory multiprocessor,  simulated processors, Acceleration, Computational modeling, Computer science, Computer simulation, Hardware, Libraries, Microarchitecture, Parallel processing, },
 abstract = {Simulation is an important means of evaluating new microarchitectures. Current trends toward chip multiprocessors (CMPs) try the ability of designers to develop efficient simulators. CMP simulation speed can be improved by exploiting parallelism in the CMP simulation model. This may be done by either running the simulation on multiple processors or by integrating multiple processors into the simulation to replace simulated processors. Doing so usually requires tedious manual parallelization or re-design to encapsulate processors. This paper presents techniques to perform automated simulator parallelization and hardware integration for CMP structural models. We show that automated parallelization can achieve an 7.60 speedup for a 16-processor CMP model on a conventional 4-processor shared-memory multiprocessor. We demonstrate the power of hardware integration by integrating eight hardware PowerPC cores into a CMP model, achieving a speedup of up to 5.82. },
}

@inproceedings{1410076,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Michaud, P.},
 year = {2004},
 pages = { 186-- 195},
 publisher = {IEEE},
 title = {Exploiting the cache capacity of a single-chip multi-core processor with execution migration},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10026},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410076},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410076.pdf?arnumber=1410076},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { affinity algorithm,  cache capacity,  cache line distribution,  cache storage,  execution migration,  microprocessor chips,  on-chip cache,  sequential program,  single-chip multicore processor,  supervisory programs, Degradation, Delay, Energy consumption, Microarchitecture, Multicore processing, Operating systems, Pipelines, Retirement, },
 abstract = {We propose to modify a conventional single-chip multicore so that a sequential program can migrate from one core to another automatically during execution. The goal of execution migration is to take advantage of the overall on-chip cache capacity. We introduce the affinity algorithm, a method for distributing cache lines automatically on several caches. We show that on working-sets exhibiting a property called "splittability", it is possible to trade cache misses for migrations. Our experimental results indicate that the proposed method has a potential for improving the performance of certain sequential programs, without degrading significantly the performance of others. },
}

@inproceedings{1410077,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Kim, I. and Lipasti, M.H.},
 year = {2004},
 pages = { 198-- 209},
 publisher = {IEEE},
 title = {Understanding scheduling replay schemes},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10011},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410077},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410077.pdf?arnumber=1410077},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { IPC degradation,  clock cycle,  data-speculation technique,  dependence information propagation loop,  instruction sets,  instruction window,  microprocessor,  parallel architectures,  parallel programming,  pipeline processing,  processor scheduling,  scheduling delay,  scheduling logic,  scheduling replay scheme,  speculative scheduling,  token-based selective replay, Broadcasting, Clocks, Computer architecture, Dynamic scheduling, Hazards, Joining processes, Processor scheduling, Propagation delay, Radio frequency, Registers, },
 abstract = {Modern microprocessors adopt speculative scheduling techniques where instructions are scheduled several clock cycles before they actually execute. Due to this scheduling delay, scheduling misses should be recovered across the multiple levels of dependence chains in order to prevent further unnecessary execution. We explore the design space of various scheduling replay schemes that prevent the propagation of scheduling misses, and find that current and proposed replay schemes do not scale well and require instructions to execute in correct data dependence order, since they track dependences among instructions within the instruction window as a part of the scheduling or execution process. We propose token-based selective replay that moves the dependence information propagation loop out of the scheduler, enabling lower complexity in the scheduling logic and support for data-speculation techniques at the expense of marginal IPC degradation compared to an ideal selective replay scheme. },
}

@inproceedings{1410074,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Peng, L. and Peir, J.-K. and Lai, K.},
 year = {2004},
 pages = { 164-- 175},
 publisher = {IEEE},
 title = {Signature buffer: bridging performance gap between registers and caches},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10020},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410074},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410074.pdf?arnumber=1410074},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { Alpha 21264-like pipeline,  IPC,  SPEC2000 integer benchmark,  base register,  buffer storage,  cache memory,  consumer instruction,  data communication,  data communication,  displacement value,  instruction sets,  instruction-per-cycle,  memory address,  memory hierarchy,  memory instructions,  memory location,  memory signature,  performance evaluation,  performance evaluation,  pipeline processing,  processor pipeline,  producer instruction,  signature buffer,  storage addressing mechanism,  storage allocation,  storage media, Buffer storage, Clocks, Data communication, Degradation, Delay, Information science, Microprocessors, Pipelines, Program processors, Registers, },
 abstract = {Data communications between producer instructions and consumer instructions through memory incur extra delays that degrade processor performance. We introduce a new storage media with a novel addressing mechanism to avoid address calculations. Instead of a memory address, each load and store is assigned a signature for accessing the new storage. A signature consists of the color of the base register along with its displacement value. A unique color is assigned to a register whenever the register is updated. When two memory instructions have the same signature, they address to the same memory location. This memory signature can be formed early in the processor pipeline. A small signature buffer, addressed by the memory signature, can be established to permit stores and loads bypassing normal memory hierarchy for fast data communication. Performance evaluations based on an Alpha 21264-like pipeline using SPEC2000 integer benchmarks show that an IPC (instruction-per-cycle) improvement of 13-18\% is possible using a small 8-entry signature buffer. },
}

@inproceedings{1410075,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Liu, C. and Anand Sivasubramaniam and Kandemir, M.},
 year = {2004},
 pages = { 176-- 185},
 publisher = {IEEE},
 title = {Organizing the last line of defense before hitting the memory wall for CMPs},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10017},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410075},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410075.pdf?arnumber=1410075},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { CMP,  CPU,  IPC,  SPEC OMP,  Shared Processor-Based Split,  Specjbb,  address-interleaved shared design,  cache hierarchy,  cache storage,  chip multiprocessor,  memory architecture,  memory wall,  microprocessor chips,  off-chip memory,  shared interleaved organization,  system simulator, Buildings, Computer science, Costs, Interference, Organizing, Parallel processing, Program processors, Runtime, System-on-a-chip, Yarn, },
 abstract = {The last line of defense in the cache hierarchy before going to off-chip memory is very critical in chip multiprocessors (CMPs) from both the performance and power perspectives. We investigate different organizations for this last line of defense (assumed to be L2 in this article) towards reducing off-chip memory accesses. We evaluate the trade-offs between private L2 and address-interleaved shared L2 designs, noting their individual benefits and drawbacks. The possible imbalance between the L2 demands across the CPUs favors a shared L2 organization, while the interference between these demands can favor a private L2 organization. We propose a new architecture, called Shared Processor-Based Split L2, that captures the benefits of these two organizations, while avoiding many of their drawbacks. Using several applications from the SPEC OMP suite and a commercial benchmark, Specjbb, on a complete system simulator, we demonstrate the benefits of this shared processor-based L2 organization. Our results show as much as 42.50\% improvement in IPC over the private organization (with 11.52\% on the average), and as much as 42.22\% improvement over the shared interleaved organization (with 9.76\% on the average). },
}

@inproceedings{1410072,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Jianyong Zhang and Sivasubramaniam, A. and Franke, H. and Gautam, N. and Yanyong Zhang and Nagar, S.},
 year = {2004},
 pages = { 142-- 142},
 publisher = {IEEE},
 title = {Synthesizing Representative I/O Workloads for TPC-H},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10019},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410072},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410072.pdf?arnumber=1410072},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Character generation, Costs, Debugging, Delay, Design optimization, Filtering, Privacy, Production systems, Security, Stress, },
 abstract = {Synthesizing I/O requests that can accurately capture workload behavior is extremely valuable for the design, implementation and optimization of disk subsystems. This paper presents a synthetic workload generator for TPC-H, an important decision-support commercial workload, by completely characterizing the arrival and access patterns of its queries. We present a novel approach for parameterizing the behavior of inter-mingling streams of sequential requests, and exploit correlations between multiple attributes of these requests, to generate disk block-level traces that are shown to accurately mimic the behavior of a real trace in terms of response time characteristics for each TPC-H query. },
}

@inproceedings{1410073,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Srihari Makineni and Ravi Iyer},
 year = {2004},
 pages = { 152-- 161},
 publisher = {IEEE},
 title = {Architectural characterization of TCP/IP packet processing on the Pentium&reg; M microprocessor},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10024},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410073},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410073.pdf?arnumber=1410073},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { Internet,  Microsoft Windows* Server 2003 operating system,  TCP/IP packet processing,  architectural characterization,  communication protocol,  computer architecture,  microprocessor chips,  network operating systems,  server network throughput,  server workload,  transport protocols, Bandwidth, Intel Pentium&reg,  microprocessor, Microprocessors, Network servers, Operating systems, Performance analysis, Protocols, TCPIP, Termination of employment, Throughput, Web services, },
 abstract = {A majority of the current and next generation server applications (Web services, e-commerce, storage, etc.) employ TCP/IP as the communication protocol of choice. As a result, the performance of these applications is heavily dependent on the efficient TCP/IP packet processing within the termination nodes. This dependency becomes even greater as the bandwidth needs of these applications grow from 100 Mbps to 1 Gbps to 10 Gbps in the near future. Motivated by this, we focus on the following: (a) to understand the performance behavior of the various modes of TCP/IP processing, (b) to analyze the underlying architectural characteristics of TCP/IP packet processing and (c) to quantify the computational requirements of the TCP/IP packet processing component within realistic workloads. We achieve these goals by performing an in-depth analysis of packet processing performance on Intel's state-of-the-art low power Pentium\&reg; M microprocessor running the Microsoft Windows* Server 2003 operating system. Some of our key observations are - (i) that the mode of TCP/IP operation can significantly affect the performance requirements, (ii) that transmit-side processing is largely compute-intensive as compared to receive-side processing which is more memory-bound and (iii) that the computational requirements for sending/receiving packets can form a substantial component (28\% to 40\%) of commercial server workloads. From our analysis, we also discuss architectural as well as stack-related improvements that can help achieve higher server network throughput and result in improved application performance. },
}

@inproceedings{1410070,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Qingbo Zhu and David, F.M. and Devaraj, C.F. and Zhenmin Li and Yuanyuan Zhou and Pei Cao},
 year = {2004},
 pages = { 118-- 118},
 publisher = {IEEE},
 title = {Reducing Energy Consumption of Disk Storage Using Power-Aware Cache Management},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10022},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410070},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410070.pdf?arnumber=1410070},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Cache storage, Computer science, Costs, Delay, Energy consumption, Energy efficiency, Energy management, Energy storage, Greedy algorithms, Power system management, },
 abstract = {Reducing energy consumption is an important issue for data centers. Among the various components of a data center, storage is one of the biggest consumers of energy. Previous studies have shown that the average idle period for a server disk in a data center is very small compared to the time taken to spin down and spin up. This significantly limits the effectiveness of disk power management schemes. This paper proposes several power-aware storage cache management algorithms that provide more opportunities for the underlying disk power management schemes to save energy. More specifically, we present an off-line power-aware greedy algorithm that is more energy-efficient than Belady\&#146;s off-line algorithm (which minimizes cache misses only). We also propose an online power-aware cache replacement algorithm. Our trace-driven simulations show that, compared to LRU, our algorithm saves 16\% more disk energy and provides 50\% better average response time for OLTP I/O workloads. We have also investigated the effects of four storage cache write policies on disk energy consumption. },
}

@inproceedings{1410071,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Carrera, E.V. and Bianchini, R.},
 year = {2004},
 pages = { 130-- 130},
 publisher = {IEEE},
 title = {Improving Disk Throughput in Data-Intensive Servers},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10023},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410071},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410071.pdf?arnumber=1410071},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Computer science, Continuous improvement, Control systems, Delay, Disk drives, File servers, File systems, Impedance, Multimedia databases, Throughput, },
 abstract = {Low disk throughput is one of the main impediments to improving the performance of data-intensive servers. In this paper, we propose two management techniques for the disk controller cache that can significantly increase disk throughput. The first technique, called File-Oriented Read-ahead (FOR), adjusts the number of read-ahead blocks brought into the disk controller cache according to file system information. The second technique, called Host-guided Device Caching (HDC), gives the host control over part of the disk controller cache. As an example use of this mechanism, we keep the blocks that cause the most misses in the host buffer cache permanently cached in the disk controller. Our detailed simulations of real server workloads show that FOR and HDC can increase disk throughput by up to 34\% and 24\%, respectively, in comparison to conventional disk controller cache management techniques. When combined, the techniques can increase throughput by up to 47\%. },
}

@inproceedings{1598116,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Joseph, P.J. and Kapil Vaswani and Thazhuthaveetil, M.J.},
 year = {2006},
 pages = { 99-- 108},
 publisher = {IEEE},
 title = {Construction and use of linear regression models for processor performance analysis},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598116},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598116},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598116.pdf?arnumber=1598116},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { Akaike information criteria,  D-optimal experimental designs,  computer architecture,  cycle-by-cycle superscalar processor simulator,  digital simulation,  iterative process,  linear regression models,  microarchitectural parameters,  multiprocessing systems,  performance evaluation,  processor performance analysis,  regression analysis,  simulation based experiments, Automation, Buildings, Computer science, Data mining, Design for experiments, Design optimization, Linear regression, Performance analysis, Pipelines, Process design, },
 abstract = {Processor architects have a challenging task of evaluating a large design space consisting of several interacting parameters and optimizations. In order to assist architects in making crucial design decisions, we build linear regression models that relate processor performance to micro-architectural parameters, using simulation based experiments. We obtain good approximate models using an iterative process in which Akaike's information criteria is used to extract a good linear model from a small set of simulations, and limited further simulation is guided by the model using D-optimal experimental designs. The iterative process is repeated until desired error bounds are achieved. We used this procedure to establish the relationship of the CPI performance response to 26 key micro-architectural parameters using a detailed cycle-by-cycle superscalar processor simulator. The resulting models provide a significance ordering on all micro-architectural parameters and their interactions, and explain the performance variations of micro-architectural techniques. },
}

@inproceedings{1410078,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Satish Narayanasamy and Hu, Y. and Sair, S. and Calder, B.},
 year = {2004},
 pages = { 210-- 221},
 publisher = {IEEE},
 title = {Creating converged trace schedules using string matching},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10012},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410078},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410078.pdf?arnumber=1410078},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { candidate loop,  converged trace schedule,  in-order machine,  instruction sets,  loop iteration,  optimisation,  pipeline processing,  processor scheduling,  program control structures,  scheduled instruction,  software optimization system,  software pipelined schedule,  string matching,  string matching,  trace block identifier, Computer architecture, Dynamic scheduling, Hardware, Kernel, Optimizing compilers, Pipeline processing, Processor scheduling, Scheduling algorithm, Software algorithms, Yarn, },
 abstract = {We focus on generating efficient software pipelined schedules for in-order machines, which we call converged trace schedules. For a candidate loop, we form a string of trace block identifiers by hashing together addresses of aggressively scheduled instructions from multiple iterations of a loop. In this process, the loop is unrolled and scheduled until we identify a repeating pattern in the string. Instructions corresponding to this repeating pattern form the kernel for our software pipelined schedule. We evaluate this approach to create aggressive schedules by using it in dynamic hardware and software optimization systems for an in-order architecture. },
}

@inproceedings{1410079,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Ehrhart, T.E. and Patel, S.J.},
 year = {2004},
 pages = { 222-- 222},
 publisher = {IEEE},
 title = {Reducing the Scheduling Critical Cycle Using Wakeup Prediction},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10016},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410079},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410079.pdf?arnumber=1410079},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Clocks, Costs, Dynamic scheduling, Feedback, Frequency, Logic, Microprocessors, Pipelines, Processor scheduling, Propagation delay, },
 abstract = {For highest performance, a modern microprocessor must be able to determine if an instruction is ready in the same cycle in which it is to be selected for execution. This creates a cycle of logic involving wakeup and select. However, the time a static instruction spends waiting for wakeup shows little dynamic variance. This idea is used to build a machine where wakeup times are predicted, and instructions executed too early are replayed. This form of self-scheduling reduces the critical cycle by eliminating the wakeup logic at the expense of additional replays. However, replays and other pipeline effects affect the cost of misprediction. To solve this, an allowance is added to the predicted wakeup time to decrease the probability of a replay. This allowance may be associated with individual instructions or the global state, and is dynamically adjusted by a gradient-descent minimum-searching technique. When processor load is low, prediction may be more aggressive \&#8212; increasing the chance of replays, but increasing performance, so the aggressiveness of the predictor is dynamically adjusted using processor load as a feedback parameter. },
}

@inproceedings{824370,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {},
 year = {2000},
 pages = {419--420},
 publisher = {IEEE},
 title = {Author Index},
 date = {2000},
 doi = {10.1109/HPCA.2000.824370},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824370},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824370.pdf?arnumber=824370},
 isbn = {0-7695-0550-3},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00824370.png" border="0"> },
}

@inproceedings{1598115,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Jaleel, A. and Mattina, M. and Jacob, B.},
 year = {2006},
 pages = { 88-- 98},
 publisher = {IEEE},
 title = {Last level cache (LLC) performance of data mining workloads on a CMP - a case study of parallel bioinformatics workloads},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598115},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598115},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598115.pdf?arnumber=1598115},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { CMP,  biology computing,  cache storage,  chip-multiprocessor cache,  data mining,  data-sharing analysis,  genetic data,  last level cache performance,  last-level cache silicon area,  microprocessor chips,  multi-threading,  multiple private caches,  multiprocessing systems,  multithreaded data-mining bioinformatics workloads,  parallel bioinformatics workloads,  scientific information systems,  single cache,  three-level cache hierarchy, Bioinformatics, Biology computing, Computer aided software engineering, Concurrent computing, Data engineering, Data mining, Databases, Educational institutions, Jacobian matrices, Yarn, },
 abstract = {With the continuing growth in the amount of genetic data, members of the bioinformatics community are developing a variety of data-mining applications to understand the data and discover meaningful information. These applications are important in defining the design and performance decisions of future high performance microprocessors. This paper presents a detailed data-sharing analysis and chip-multiprocessor (CMP) cache study of several multithreaded data-mining bioinformatics workloads. For a CMP with a three-level cache hierarchy, we model the last-level of the cache hierarchy as either multiple private caches or a single cache shared amongst different cores of the CMP. Our experiments show that the bioinformatics workloads exhibit significant data-sharing - 50-95\% of the data cache is shared by the different threads of the workload. Furthermore, regardless of the amount of data cache shared, for some workloads, as many as 98\% of the accesses to the last-level cache are to shared data cache lines. Additionally, the amount of data-sharing exhibited by the workloads is a function of the total cache size available - the larger the data cache the better the sharing behavior. Thus, partitioning the available last-level cache silicon area into multiple private caches can cause applications to lose their inherent data-sharing behavior. For the workloads in this study, a shared 32 MB last-level cache is able to capture a tremendous amount of data-sharing and outperform a 32 MB private cache configuration by several orders of magnitude. Specifically, with shared last-level caches, the bandwidth demands beyond the last-level cache can be reduced by factors of 3-625 when compared to private last-level caches. },
}

@inproceedings{903268,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Peh, L.-S. and Dally, W.J.},
 year = {2001},
 pages = {255--266},
 publisher = {IEEE},
 title = {A delay model and speculative architecture for pipelined routers },
 date = {2001},
 doi = {10.1109/HPCA.2001.903268},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903268},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903268.pdf?arnumber=903268},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Bandwidth, Computer architecture, Delay effects, Delay estimation, Laboratories, Multiprocessor interconnection networks, Pipelines, Switches, Throughput, Virtual colonoscopy, interconnection networks, microarchitecture, multiprocessor interconnection networks, performance, pipeline models, pipeline processing, pipelined routers, router delay model, speculative virtual-channel router, },
 abstract = {This paper introduces a router delay model that accurately models key aspects of modern routers. The model accounts for the pipelined nature of contemporary routers, the specific flow control method employed the delay of the flow control credit path, and the sharing of crossbar ports across virtual channels. Motivated by this model, we introduce a microarchitecture for a speculative virtual-channel router that significantly reduces its router latency to that of a brown hole router. Simulations using our pipelined model give results that differ considerably from the commonly assumed `unit-latency' model which is unreasonably optimistic. Using realistic pipeline models, we compare wormhole and virtual-channel flow control. Our results show that a speculative virtual-channel router has the same per-hop router latency as a wormhole router while improving throughput by up to 40\% },
}

@inproceedings{4798239,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Wenisch, T.F. and Ferdman, M. and Ailamaki, A. and Falsafi, B. and Moshovos, A.},
 year = {2009},
 pages = {79--90},
 publisher = {IEEE},
 title = {Practical off-chip meta-data for temporal memory streaming},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798239},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798239},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798239.pdf?arnumber=4798239},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Bandwidth, Costs, Delay, Hardware, Organizing, Prefetching, Sampling methods, Table lookup, Throughput, Web server, bandwidth-efficient meta-data update, hardware-managed main memory hash table, memory architecture, minimal off-chip lookup latency, off-chip lookup cost, off-chip meta-data storage, prefetching, probabilistic sampling, sampled temporal memory streaming, storage management, table lookup, },
 abstract = {Prior research demonstrates that temporal memory streaming and related address-correlating prefetchers improve performance of commercial server workloads though increased memory level parallelism. Unfortunately, these prefetchers require large on-chip meta-data storage, making previously-proposed designs impractical. Hence, to improve practicality, researchers have sought ways to enable timely prefetch while locating meta-data entirely off-chip. Unfortunately, current solutions for off-chip meta-data increase memory traffic by over a factor of three. We observe three requirements to store meta-data off chip: minimal off-chip lookup latency, bandwidth-efficient meta-data updates, and off-chip lookup amortized over many prefetches. In this work, we show: (1) minimal off-chip meta-data lookup latency can be achieved through a hardware-managed main memory hash table, (2) bandwidth-efficient updates can be performed through probabilistic sampling of meta-data updates, and (3) off-chip lookup costs can be amortized by organizing meta-data to allow a single lookup to yield long prefetch sequences. Using these techniques, we develop sampled temporal memory streaming (STMS), a practical address-correlating prefetcher that keeps predictor meta-data in main memory while achieving 90\% of the performance potential of idealized on-chip meta-data storage. },
}

@inproceedings{903260,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Delaluz, V. and Kandemir, M. and Vijaykrishnan, N. and Sivasubramaniam, A. and Irwin, M.J.},
 year = {2001},
 pages = {159--169},
 publisher = {IEEE},
 title = {DRAM energy management using software and hardware directed power mode control},
 date = {2001},
 doi = {10.1109/HPCA.2001.903260},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903260},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903260.pdf?arnumber=903260},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, CPUs, DRAM chips, DRAM energy management, Energy conservation, Energy management, Hardware, Home computing, Memory architecture, Memory management, Proposals, Random access memory, Software performance, energy conservation, energy conservation, energy optimization, hardware directed power mode control, memory architecture, optimisation, peripherals, selective operating mode control, software directed power mode control, },
 abstract = {While there have been several studies and proposals for energy conservation for CPUs and peripherals, energy optimization techniques for selective operating mode control of DRAMs have not been fully explored. It has been shown that as much as 90\% of overall system energy (excluding I/O) is consumed by the DRAM modules, serving as a good candidate for energy optimizations. Further; DRAM technology has also matured to provide several low energy operating modes (power modes), making it an opportunistic moment to conduct studies exploring the potential benefits of mode control techniques. This paper conducts an in-depth investigation of software and hardware techniques to avail of the DRAM mode control capabilities at a module granularity for energy savings },
}

@inproceedings{903261,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Brooks, D. and Martonosi, M.},
 year = {2001},
 pages = {171--182},
 publisher = {IEEE},
 title = {Dynamic thermal management for high-performance microprocessors},
 date = {2001},
 doi = {10.1109/HPCA.2001.903261},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903261},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903261.pdf?arnumber=903261},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {CPU power dissipation, Clocks, Cooling, Costs, Energy management, Hardware, Microprocessors, Power dissipation, Power engineering computing, Power system management, Thermal management, clock gating, clock rate, computational complexity, dynamic thermal management, heat sinks, high-performance microprocessors, microprocessor chips, performance evaluation, power dissipation, system design complexity, thermal heat sinks, thermal management (packaging), thermal trauma, transistor count, },
 abstract = {With the increasing clock rate and transistor count of today's microprocessors, power dissipation is becoming a critical component of system design complexity. Thermal and power-delivery issues are becoming especially critical for high-performance computing systems. In this work, we investigate dynamic thermal management as a technique to control CPU power dissipation. With the increasing usage of clock gating techniques, the average power dissipation typically seen by common applications is becoming much less than the chip's rated maximum power dissipation. However system designers still must design thermal heat sinks to withstand the worse-case scenario. We define and investigate the major components of any dynamic thermal management scheme. Specifically we explore the tradeoffs between several mechanisms for responding to periods of thermal trauma and we consider the effects of hardware and software implementations. With approximate dynamic thermal management, the CPU can be designed for a much lower maximum power rating, with minimal performance impact for typical applications },
}

@inproceedings{903262,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Tune, E. and Dongning Liang and Tullsen, D.M. and Calder, B.},
 year = {2001},
 pages = {185--195},
 publisher = {IEEE},
 title = {Dynamic prediction of critical path instructions},
 date = {2001},
 doi = {10.1109/HPCA.2001.903262},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903262},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903262.pdf?arnumber=903262},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Capacitive sensors, Computer science, Constraint optimization, Cost function, Hardware, Modems, Out of order, Processor scheduling, Registers, Throughput, critical path, critical path instructions, critical path prediction, dynamic prediction, execution speed, microprocessor chips, optimisation, performance optimisation, processor optimizations, processors, role dependences, },
 abstract = {Modern processors come close to executing as fast as role dependences allow. The particular dependences that constrain execution speed constitute the critical path of execution. To optimize the performance of the processor we either have to reduce the critical path or execute it more efficiently. In both cases, it can be done more effectively if we know the actual instructions that constitute that path. This paper describes critical path prediction for dynamically identifying instructions likely to be on the critical path, allowing various processor optimizations to take advantage of this information. We show several possible critical path prediction techniques and apply critical path prediction to value prediction and clustered architecture scheduling. We show that critical path prediction has the potential to increase the effectiveness of these hardware optimizations by as much as 70\%, without adding greatly to their cost },
}

@inproceedings{903263,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Jimenez, D.A. and Lin, C.},
 year = {2001},
 pages = {197--206},
 publisher = {IEEE},
 title = {Dynamic branch prediction with perceptrons},
 date = {2001},
 doi = {10.1109/HPCA.2001.903263},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903263},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903263.pdf?arnumber=903263},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {4K byte hardware budget, Accuracy, Computer architecture, Counting circuits, Hardware, History, Modems, Neural networks, Parallel processing, Prefetching, Space exploration, branch prediction, dynamic branch prediction, hardware resources, neural nets, neural networks, parallel architectures, perceptrons, perceptrons, program compilers, purely dynamic schemes, },
 abstract = {This paper presents a new method for branch prediction. The key idea is to use one of the simplest possible neural networks, the perceptron, as an alternative to the commonly used two-bit counters. Our predictor achieves increased accuracy by making use of long branch histories, which are possible becasue the hardware resources for our method scale linearly with the history length. By contrast, other purely dynamic schemes require exponential resources. We describe our design and evaluate it with respect to two well known predictors. We show that for a 4K byte hardware budget our method improves misprediction rates for the SPEC 2000 benchmarks by 10.1\% over the gshare predictor. Our experiments also provide a better understanding of the situations in which traditional predictors do and do not perform well. Finally, we describe techniques that allow our complex predictor to operate in one cycle },
}

@inproceedings{4798234,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Yi Xu and Yu Du and Bo Zhao and Xiuyi Zhou and Youtao Zhang and Jun Yang},
 year = {2009},
 pages = {30--42},
 publisher = {IEEE},
 title = {A low-radix and low-diameter 3D interconnection network design},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798234},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798234},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798234.pdf?arnumber=4798234},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {3D network-on-chip topology design, 3D stacking technology, Delay, Fabrics, Mesh generation, Mesh networks, Multiprocessor interconnection networks, Network topology, Network-on-a-chip, Stacking, Throughput, Wires, chip multiprocessor, digital arithmetic, logic design, low radix router, low-diameter 3D interconnection network design, multiprocessor interconnection networks, network routing, network topology, network-on-chip, one-hop vertical communication design, small-to-medium sized clique network, sub-micron technology, },
 abstract = {Interconnection plays an important role in performance and power of CMP designs using deep sub-micron technology. The network-on-chip (NoCs) has been proposed as a scalable and high-bandwidth fabric for interconnect design. The advent of the 3D technology has provided further opportunity to reduce on-chip communication delay. However, the design of the 3D NoC topologies has important distinctions from 2D NoCs or off-chip interconnection networks. First, current 3D stacking technology allows only vertical inter-layer links. Hence, there cannot be direct connections between arbitrary nodes in different layers - the vertical connection topology are essentially fixed. Second, the 3D NoC is highly constrained by the complexity and power of routers and links. Hence, low-radix routers are preferred over high-radix routers for lower power and better heat dissipation. This implies long network latency due to high hop counts in network paths. In this paper, we design a low-diameter 3D network using low-radix routers. Our topology leverages long wires to connect remote intra-layer nodes. We take advantage of the start-of-the-art one-hop vertical communication design and utilize lateral long wires to shorten network paths. Effectively, we implement a small-to-medium sized clique network in different layers of a 3D chip. The resulting topology generates a diameter of 3-hop only network, using routers of the same radix as 3D mesh routers. The proposed network shows up to 29\% of network latency reduction, up to 10\% throughput improvement, and up to 24\% energy reduction, when compared to a 3D mesh network. },
}

@inproceedings{903265,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Corbal, J. and Espasa, R. and Valero, M.},
 year = {2001},
 pages = {219--228},
 publisher = {IEEE},
 title = {DLP+TLP processors for the next generation of media workloads},
 date = {2001},
 doi = {10.1109/HPCA.2001.903265},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903265},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903265.pdf?arnumber=903265},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Delay, High performance computing, Instruction sets, Kernel, MPEG 7 Standard, Multithreading, Streaming media, Surface-mount technology, Throughput, Yarn, general purpose processors, media workloads, microprocessor chips, multi-threading, multithreading, performance, performance evaluation, real-time constraints, simultaneous multithreading execution paradigm, smart decoupled cache hierarchies, uni-threaded performance, vector execution, },
 abstract = {Future media workloads will require about two levels of magnitude the performance achieved by current general purpose processors. High uni-threaded performance will be needed to accomplish real-time constraints together with huge computational throughput, as next generation of media workloads will be eminently multithreaded (MPEG-4/MPEG-7). In order to fulfil the challenge of providing both good uni-threaded performance and throughput, we propose to join the simultaneous multithreading execution paradigm (SMT) together with the ability to execute media-oriented streaming \&mu;-SIMD instructions. This paper evaluates the performance of two different aggressive SMT processors: one with conventional \&mu;-SIMD extensions (such as MMX) and one with longer streaming vector \&mu;-SIMD extensions. We will show that future media workloads are, in fact, dominated by the scalar performance. The combination of SMT plus streaming vector \&mu;-SIMD helps alleviate the performance bottleneck of the integer unit. SMT allows ``hiding" vector execution underneath integer execution by overlapping the two types of computation, while the streaming vector \&mu;-SIMD reduces the pressure on issue width and fetch bandwidth, and provides a powerful mechanism to tolerate latency that allows to implement smart decoupled cache hierarchies },
}

@inproceedings{4798236,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Qureshi, M.K.},
 year = {2009},
 pages = {45--54},
 publisher = {IEEE},
 title = {Adaptive Spill-Receive for robust high-performance caching in CMPs},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798236},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798236},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798236.pdf?arnumber=4798236},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Bandwidth, CMP, Cache storage, Cooperative caching, DSR architecture, Delay, Design optimization, Fabrics, Quality of service, Robustness, Throughput, Wire, adaptive spill-receive, cache capacity, cache storage, cache structure, capacity sharing, chip multiprocessor, dynamic spill-receive, harmonic-mean fairness metric, microprocessor chips, multiprogrammed workloads, private caches, quad-core system, quality of service, quality of service, receiver cache, robust high-performance caching, spiller cache, storage overhead, },
 abstract = {In a chip multi-processor (CMP) with private caches, the last level cache is statically partitioned between all the cores. This prevents such CMPs from sharing cache capacity in response to the requirement of individual cores. Capacity sharing can be provided in private caches by spilling a line evicted from one cache to another cache. However, naively allowing all caches to spill evicted lines to other caches have limited performance benefit as such spilling does not take into account which cores benefit from extra capacity and which cores can provide extra capacity. This paper proposes dynamic spill-receive (DSR) for efficient capacity sharing. In a DSR architecture, each cache uses set dueling to learn whether it should act as a ldquospiller cacherdquo or ldquoreceiver cacherdquo for best overall performance. We evaluate DSR for a quad-core system with 1MB private caches using 495 multi-programmed workloads. DSR improves average throughput by 18\% (weighted-speedup by 13\% and harmonic-mean fairness metric by 36\%) compared to no spilling. DSR requires a total storage overhead of less than two bytes per core, does not require any changes to the existing cache structure, and is scalable to a large number of cores (16 in our evaluation). Furthermore, we propose a simple extension of DSR that provides quality of service (QoS) by guaranteeing that the worst-case performance of each application remains similar to that with no spilling, while still providing an average throughput improvement of 17.5\%. },
}

@inproceedings{4798237,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Sangmin Seo and Jaejin Lee and Sura, Z.},
 year = {2009},
 pages = {55--66},
 publisher = {IEEE},
 title = {Design and implementation of software-managed caches for multicores with local memory},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798237},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798237},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798237.pdf?arnumber=4798237},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Cell BE processors, Computer architecture, Computer science, Delay, GPGPU, Hardware, Memory management, Multicore processing, OpenMP, Processor scheduling, Programming profession, Runtime, Sliding mode control, cache storage, extended set-index cache, heterogeneous multicore architectures, instruction scheduling, memory architecture, microprocessor chips, software-managed caches, },
 abstract = {Heterogeneous multicores, such as Cell BE processors and GPGPUs, typically do not have caches for their accelerator cores because coherence traffic, cache misses, and latencies from different types of memory accesses add overhead and adversely affect instruction scheduling. Instead, the accelerator cores have internal local memory to place their code and data. Programmers of such heterogeneous multicore architectures must explicitly manage data transfers between the local memory of a core and the globally shared main memory. This is a tedious and errorprone programming task. A software-managed cache (SMC), implemented in local memory, can be programmed to automatically handle data transfers at runtime, thus simplifying the task of the programmer. In this paper, we propose a new software-managed cache design, called extended set-index cache (ESC). It has the benefits of both set-associative and fully associative caches. Its tag search speed is comparable to the set-associative cache and its miss rate is comparable to the fully associative cache. We examine various line replacement policies for SMCs, and discuss their trade-offs. In addition, we propose adaptive execution strategies that select the optimal cache line size and replacement policy for each program region at runtime. To evaluate the effectiveness of our approach, we implement the ESC and other SMC designs on the Cell BE architecture, and measure their performance with 8 OpenMP applications. The evaluation results show that the ESC outperforms other SMC designs. The results also show that our adaptive execution strategies work well with the ESC. In fact, our approach is applicable to all cores with access to both local and global memory in a multicore architecture. },
}

@inproceedings{5749721,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Gu, Junli and Lumetta, Steven S. and Kumar, Rakesh and Sun, Yihe},
 year = {2011},
 pages = {111--120},
 publisher = {IEEE},
 title = {MOPED: Orchestrating interprocess message data on CMPs},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749721},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749721},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749721.pdf?arnumber=5749721},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Future CMPs will combine many simple cores with deep cache hierarchies. With more cores, cache resources per core are fewer, and must be shared carefully to avoid poor utilization due to conflicts and pollution. Explicit motion of data in these architectures, such as message passing, can provide hints about program behavior that can be used to hide latency and improve cache behavior. However, to make these models attractive, synchronization overhead and data copying must also be offloaded from the processors. In this paper, we describe a Message Orchestration and Performance Enhancement Device (MOPED) that provides hardware mechanisms to support state-of-the-art message passing protocols such as MPI. MOPED extends the per-processor cache controllers and coherence protocol to support message synchronization and management in hardware, to transfer message data efficiently without intermediate buffer copies, and to place useful data in caches in a timely manner. MOPED thus allows full overlap between communication and computation on the cores. We extended a 16-core full-system simulator based on Simics and FeS2. MOPED interacts with the directory controllers to orchestrate message data. We evaluated benefits to performance and coherence traffic by integrating MOPED into the MPICH runtime. Relative to unmodified MPI execution, MOPED reduces execution time of real applications (NAS Parallel Benchmarks) by 17\&#x2013;45\% and of communication microbenchmarks (Intel's IMB) by 76\&#x2013;94\%. Off-chip memory misses are reduced by 43\&#x2013;88\% for applications and by 75\&#x2013;100\% for microbenchmarks. },
}

@inproceedings{5749720,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Lee, Sanghoon and Tiwari, Devesh and Solihin, Yan and Tuck, James},
 year = {2011},
 pages = {99--110},
 publisher = {IEEE},
 title = {HAQu: Hardware-accelerated queueing for fine-grained threading on a chip multiprocessor},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749720},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749720},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749720.pdf?arnumber=5749720},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Queues are commonly used in multithreaded programs for synchronization and communication. However, because software queues tend to be too expensive to support finegrained parallelism, hardware queues have been proposed to reduce overhead of communication between cores. Hardware queues require modifications to the processor core and need a custom interconnect. They also pose difficulties for the operating system because their state must be preserved across context switches. To solve these problems, we propose a hardware-accelerated queue, or HAQu. HAQu adds hardware to a CMP that accelerates operations on software queues. Our design implements fast queueing through an application's address space with operations that are compatible with a fully software queue. Our design provides accelerated and OS-transparent performance in three general ways: (1) it provides a single instruction for enqueueing and dequeueing which significantly reduces the overhead when used in fine-grained threading; (2) operations on the queue are designed to leverage low-level details of the coherence protocol; and (3) hardware ensures that the full state of the queue is stored in the application's address space, thereby ensuring virtualization. We have evaluated our design in the context of application domains: offloading fine-grained checks for improved software reliability, and automatic, fine-grained parallelization using decoupled software pipelining. },
}

@inproceedings{5749723,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Vantrease, Dana and Lipasti, Mikko H. and Binkert, Nathan},
 year = {2011},
 pages = {132--143},
 publisher = {IEEE},
 title = {Atomic Coherence: Leveraging nanophotonics to build race-free cache coherence protocols},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749723},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749723},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749723.pdf?arnumber=5749723},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {This paper advocates Atomic Coherence, a framework that simplifies cache coherence protocol specification, design, and verification by decoupling races from the protocol's operation. Atomic Coherence requires conflicting coherence requests to the same addresses be serialized with a mutex before they are issued. Once issued, requests follow a predictable race-free path. Because requests are guaranteed not to race, coherence protocols are simpler and protocol extensions are straightforward. Our implementation of Atomic Coherence uses optical mutexes because optics provides very low latency. We begin with a state-of-the-art non-atomic MOEFSI protocol and demonstrate that an atomic implementation is much simpler while imposing less than a 2\% performance penalty. We then show how, in the absence of races, it is easy to add support for speculative coherence and improve performance by up to 70\%. Similar performance gains may be possible in a non-atomic protocol, but not without considerable effort in race management. },
}

@inproceedings{5749722,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Nitta, Christopher and Farrens, Matthew and Akella, Venkatesh},
 year = {2011},
 pages = {122--131},
 publisher = {IEEE},
 title = {Addressing system-level trimming issues in on-chip nanophotonic networks},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749722},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749722},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749722.pdf?arnumber=5749722},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The basic building block of on-chip nanophotonic interconnects is the microring resonator [14], and these resonators change their resonant wavelengths due to variations in temperature \&#x2014; a problem that can be addressed using a technique called \&#x201D;trimming\&#x201D;, which involves correcting the drift via heating and/or current injection. Thus far system researchers have modeled trimming as a per ring fixed cost. In this work we show that at the system level using a fixed cost model is inappropriate \&#x2014; our simulations demonstrate that the cost of heating has a non-linear relationship with the number of rings, and also that current injection can lead to thermal runaway. We show that a very narrow Temperature Control Window (TCW) must be maintained in order for the network to work as desired. However, by exploiting the group drift property of co-located rings, it is possible to create a sliding window scheme which can increase the TCW. We also show that partially athermal rings can alleviate but not eliminate the problem. },
}

@inproceedings{5749725,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Li, Jian and Huang, Wei and Lefurgy, Charles and Zhang, Lixin and Denzel, Wolfgang E. and Treumann, Richard R. and Wang, Kun},
 year = {2011},
 pages = {156--167},
 publisher = {IEEE},
 title = {Power shifting in Thrifty Interconnection Network},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749725},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749725},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749725.pdf?arnumber=5749725},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {This paper presents two complementary techniques to manage the power consumption of large-scale systems with a packet-switched interconnection network. First, we propose Thrifty Interconnection Network (TIN), where the network links are activated and de-activated dynamically with little or no overhead by using inherent system events to timely trigger link activation or de-activation. Second, we propose Network Power Shifting (NPS) that dynamically shifts the power budget between the compute nodes and their corresponding network components. TIN activates and trains the links in the interconnection network, just-in-time before the network communication is about to happen, and thriftily puts them into a low-power mode when communication is finished, hence reducing unnecessary network power consumption. Furthermore, the compute nodes can absorb the extra power budget shifted from its attached network components and increase their processor frequency for higher performance with NPS. Our simulation results on a set of real-world workload traces show that TIN can achieve on average 60\% network power reduction, with the support of only one low-power mode. When NPS is enabled, the two together can achieve 12\% application performance improvement and 13\% overall system energy reduction. Further performance improvement is possible if the compute nodes can speed up more and fully utilize the extra power budget reinvested from the thrifty network with more aggressive cooling support. },
}

@inproceedings{5749724,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Fallin, Chris and Craik, Chris and Mutlu, Onur},
 year = {2011},
 pages = {144--155},
 publisher = {IEEE},
 title = {CHIPPER: A low-complexity bufferless deflection router},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749724},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749724},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749724.pdf?arnumber=5749724},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {As Chip Multiprocessors (CMPs) scale to tens or hundreds of nodes, the interconnect becomes a significant factor in cost, energy consumption and performance. Recent work has explored many design tradeoffs for networks-on-chip (NoCs) with novel router architectures to reduce hardware cost. In particular, recent work proposes bufferless deflection routing to eliminate router buffers. The high cost of buffers makes this choice potentially appealing, especially for low-to-medium network loads. However, current bufferless designs usually add complexity to control logic. Deflection routing introduces a sequential dependence in port allocation, yielding a slow critical path. Explicit mechanisms are required for livelock freedom due to the non-minimal nature of deflection. Finally, deflection routing can fragment packets, and the reassembly buffers require large worst-case sizing to avoid deadlock, due to the lack of network backpressure. The complexity that arises out of these three problems has discouraged practical adoption of bufferless routing. To counter this, we propose CHIPPER (Cheap-Interconnect Partially Permuting Router), a simplified router microarchitecture that eliminates in-router buffers and the crossbar. We introduce three key insights: first, that deflection routing port allocation maps naturally to a permutation network within the router; second, that livelock freedom requires only an implicit token-passing scheme, eliminating expensive age-based priorities; and finally, that flow control can provide correctness in the absence of network backpressure, avoiding deadlock and allowing cache miss buffers (MSHRs) to be used as reassembly buffers. Using multiprogrammed SPEC CPU2006, server, and desktop application workloads and SPLASH-2 multithreaded workloads, we achieve an average 54.9\% network power reduction for 13.6\% average performance degradation (multipro-grammed) and 73.4\% power reduction for 1.9\% slowdown (multithreaded), with minimal degradation and - - large power savings at low-to-medium load. Finally, we show 36.2\% router area reduction relative to buffered routing, with comparable timing. },
}

@inproceedings{5749727,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Tseng, Hung-Wei and Tullsen, Dean M.},
 year = {2011},
 pages = {181--192},
 publisher = {IEEE},
 title = {Data-triggered threads: Eliminating redundant computation},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749727},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749727},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749727.pdf?arnumber=5749727},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {This paper introduces the concept of data-triggered threads. Unlike threads in parallel programs in conventional programming models, these threads are initiated on a change to a memory location. This enables increased parallelism and the elimination of redundant, unnecessary computation. This paper focuses primarily on the latter. It is shown that 78\% of all loads fetch redundant data, leading to a high incidence of redundant computation. By expressing computation through data-triggered threads, that computation is executed once when the data changes, and is skipped whenever the data does not change. The set of C SPEC benchmarks show performance speedup of up to 5.9X, and averaging 46\%. },
}

@inproceedings{5749726,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Ferdman, Michael and Lotfi-Kamran, Pejman and Balet, Ken and Falsafi, Babak},
 year = {2011},
 pages = {169--180},
 publisher = {IEEE},
 title = {Cuckoo directory: A scalable directory for many-core systems},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749726},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749726},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749726.pdf?arnumber=5749726},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Growing core counts have highlighted the need for scalable on-chip coherence mechanisms. The increase in the number of on-chip cores exposes the energy and area costs of scaling the directories. Duplicate-tag-based directories require highly associative structures that grow with core count, precluding scalability due to prohibitive power consumption. Sparse directories overcome the power barrier by reducing directory associativity, but require storage area over-provisioning to avoid high invalidation rates. We propose the Cuckoo directory, a power- and area-efficient scalable distributed directory. The cuckoo directory scales to high core counts without the energy costs of wide associative lookup and without gross capacity over-provisioning. Simulation of a 16-core CMP with commercial server and scientific workloads shows that the Cuckoo directory eliminates invalidations while being up to four times more power-efficient than the Duplicate-tag directory and 24\% more power-efficient and up to seven times more area-efficient than the Sparse directory organization. Analytical projections indicate that the Cuckoo directory retains its energy and area benefits with increasing core count, efficiently scaling to at least 1024 cores. },
}

@inproceedings{5749729,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Li, Chao and Zhang, Wangyuan and Cho, Chang-Burm and Li, Tao},
 year = {2011},
 pages = {205--216},
 publisher = {IEEE},
 title = {SolarCore: Solar energy driven multi-core architecture power management},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749729},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749729},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749729.pdf?arnumber=5749729},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The global energy crisis and environmental concerns (e.g. global warming) have driven the IT community into the green computing era. Of clean, renewable energy sources, solar power is the most promising. While efforts have been made to improve the performance-per-watt, conventional architecture power management schemes incur significant solar energy loss since they are largely workload-driven and unaware of the supply-side attributes. Existing solar power harvesting techniques improve the energy utilization but increase the environmental burden and capital investment due to the inclusion of large-scale batteries. Moreover, solar power harvesting itself cannot guarantee high performance without appropriate load adaptation. To this end, we propose SolarCore, a solar energy driven, multi-core architecture power management scheme that combines maximal power provisioning control and workload run-time optimization. Using real-world meteorological data across different geographic sites and seasons, we show that SolarCore is capable of achieving the optimal operation condition (e.g. maximal power point) of solar panels autonomously under various environmental conditions with a high green energy utilization of 82\% on average. We propose efficient heuristics for allocating the time varying solar power across multiple cores and our algorithm can further improve the workload performance by 10.8\% compared with that of round-robin adaptation, and at least 43\% compared with that of conventional fixed-power budget control. This paper makes the first step on maximally reducing the carbon footprint of computing systems through the usage of renewable energy sources. We expect that the novel joint optimization techniques proposed in this paper will contribute to building a truly sustainable, high-performance computing environment. },
}

@inproceedings{5749728,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Brown, Jeffery A. and Porter, Leo and Tullsen, Dean M.},
 year = {2011},
 pages = {193--204},
 publisher = {IEEE},
 title = {Fast thread migration via cache working set prediction},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749728},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749728},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749728.pdf?arnumber=5749728},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The most significant source of lost performance when a thread migrates between cores is the loss of cache state. A significant boost in post-migration performance is possible if the cache working set can be moved, proactively, with the thread. This work accelerates thread startup performance after migration by predicting and prefetching the working set of the application into the new cache. It shows that simply moving cache state performs poorly, and that moving the instruction working set can be even more critical than data. This paper demonstrates a technique that captures the access behavior of a thread, summarizes that behavior into a compact form for transfer between cores, and then prefetches appropriate data into the new caches based on the summary. It presents a detailed study of single-thread migration effects, and then demonstrates its utility on a speculative multithreading architecture. Working set prediction as much as doubles the performance of short-lived threads, and in a full speculative multithreading implementation, the technique is also shown to nearly double the effectiveness of the spawned threads. },
}

@inproceedings{995692,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {},
 year = {2002},
 publisher = {IEEE},
 title = {Proceedings Eighth International Symposium on High Performance Computer Architecture},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995692},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995692},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995692.pdf?arnumber=995692},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { computer architecture,  high-performance computer architecture,  latency tolerance,  memory-aware scheduling,  microarchitecture,  multiprocessor systems,  pipelining,  speculative multithreading,  thermal management, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00995692.png" border="0"> },
}

@inproceedings{995693,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Chou, T.},
 year = {2002},
 pages = { 1-- 1},
 publisher = {IEEE},
 title = {The software industry: ten lessons for long life},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995693},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995693},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995693.pdf?arnumber=995693},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = {Application specific processors, Availability, Computer architecture, Computer industry, Computer networks, Hardware, Microprocessors, Software quality, Web services, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00995693.png" border="0"> },
}

@inproceedings{1410089,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {},
 year = {2004},
 pages = { v-- v},
 publisher = {IEEE},
 title = {10th International Symposium on High Performance Computer Architecture - Table of Contents},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.1},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410089},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410089.pdf?arnumber=1410089},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 abstract = {},
}

@inproceedings{1410088,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {},
 year = {2004},
 pages = { i-- i},
 publisher = {IEEE},
 title = {10th International Symposium on High Performance Computer Architecture - Title Page},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.2},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410088},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410088.pdf?arnumber=1410088},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 abstract = {},
}

@inproceedings{1385949,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 publisher = {IEEE},
 title = {The future of computer architecture research: an industrial perspective},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.36},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385949},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385949.pdf?arnumber=1385949},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { computer architecture,  computer architecture research,  computing industry,  industrial perspective,  industrial vision, },
 abstract = {Summary form only given. This panel focuses on the industrial vision for the future of computer architecture research. The field of computer architecture is in critical need for focus, perhaps now more than ever. The underlying technology is presenting significant roadblocks for next generation designs. The applications that the world wants seem to be out of synch with mainstream research. The potential impact of our work seems to be uncertain. Despite this, there are some excellent opportunities out there. We have invited several distinguished technical leaders from key hardware and software companies in the computing industry to articulate their vision for the future of computer architecture research. },
}

@inproceedings{1385948,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Hofstee, H.P.},
 year = {2005},
 pages = { 258-- 262},
 publisher = {IEEE},
 title = {Power efficient processor architecture and the cell processor},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.26},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385948},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385948.pdf?arnumber=1385948},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { architecture decision,  cell processor,  cellular radio,  design decision,  media applications,  microarchitectural enhancement,  microprocessor chips,  microprocessor design,  nonhomogeneous SMP,  power supply circuits, CMOS technology, Computer architecture, Delay, Design optimization, Frequency measurement, Microarchitecture, Microprocessors, Process design, Software performance, Time measurement, },
 abstract = {This paper provides a background and rationale for some of the architecture and design decisions in the cell processor, a processor optimized for compute-intensive and broadband rich media applications, jointly developed by Sony Group, Toshiba, and IBM. The paper discusses some of the challenges microprocessor designers face and provides motivation for performance per transistor as a reasonable first-order metric for design efficiency. Common microarchitectural enhancements relative to this metric are provided. Also alternate architectural choices and some of its limitations are discussed and non-homogeneous SMP as a means to overcome these limitations is proposed. },
}

@inproceedings{1410083,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Akkary, H. and Srinivasan, S.T. and Koltur, R. and Patil, Y. and Refaai, W.},
 year = {2004},
 pages = { 265-- 265},
 publisher = {IEEE},
 title = {Perceptron-Based Branch Confidence Estimation},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10002},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410083},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410083.pdf?arnumber=1410083},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Neurons, Performance loss, Pipelines, Predictive models, Proposals, Resource management, Yarn, },
 abstract = {Pipeline gating has been proposed for reducing wasted speculative execution due to branch mispredictions. As processors become deeper or wider, pipeline gating becomes more important because the amount of wasted speculative execution increases. The quality of pipeline gating relies heavily on the branch confidence estimator used. Not much work has been done on branch confidence estimators since the initial work [6]. We show the accuracy and coverage characteristics of the initial proposals do not sufficiently reduce mis-speculative execution on future deep pipeline processors. In this paper, we present a new, perceptron-based, branch confidence estimator, which is twice as accurate as the current best-known method and achieves reasonable mispredicted branch coverage. Further, the output of our predictor is multi-valued, which enables us to classify branches further as "strongly low confident" and "weakly low confident". We reverse the predictions of "strongly low confident" branches and apply pipeline gating to the "weakly low confident" branches. This combination of pipeline gating and branch reversal provides a spectrum of interesting design options ranging from significantly reducing total execution for only a small performance loss, to lower but still significant reductions in total execution, without any performance loss. },
}

@inproceedings{1385944,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Jacobson, H. and Bose, P. and Zhigang Hu and Buyuktosunoglu, A. and Zyuban, V. and Eickemeyer, R. and Eisen, L. and Griswell, J. and Logan, D. and Balaram Sinharoy and Tendler, J.},
 year = {2005},
 pages = { 238-- 242},
 publisher = {IEEE},
 title = {Stretching the limits of clock-gating efficiency in server-class processors},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.33},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385944},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385944.pdf?arnumber=1385944},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { active power reduction,  computer power supplies,  dynamic power management,  elastic pipeline clock-gating,  flip-flops,  high-end commercial microprocessor,  high-performance processors,  leakage power savings,  microprocessor chips,  pipeline processing,  server-class processors,  temperature drop,  transparent pipeline clock-gating, CMOS technology, Clocks, Energy management, Jacobian matrices, Microprocessors, Pipelines, Power generation, Power system management, Technology management, Temperature, },
 abstract = {Clock-gating has been introduced as the primary means of dynamic power management in recent high-end commercial microprocessors. The temperature drop resulting from active power reduction can result in additional leakage power savings in future processors. In this paper we first examine the realistic benefits and limits of clock-gating in current generation high-performance processors (e.g. of the POWER4\&trade; or POWER5\&trade; class). We then look beyond classical clock-gating: we examine additional opportunities to avoid unnecessary clocking in real workload executions. In particular, we examine the power reduction benefits of a couple of newly invented schemes called transparent pipeline clock-gating and elastic pipeline clock-gating. Based on our experiences with current designs, we try to bound the practical limits of clock gating efficiency in future microprocessors. },
}

@inproceedings{1410081,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Falcon, A. and Ramirez, A. and Valero, V.},
 year = {2004},
 pages = { 244-- 253},
 publisher = {IEEE},
 title = {A low-complexity, high-performance fetch unit for simultaneous multithreading processors},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10003},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410081},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410081.pdf?arnumber=1410081},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { ILP,  fetch architecture,  low-complexity high-performance fetch unit,  memory bounded benchmark,  multi-threading,  parallel architectures,  parallel execution,  pipeline processing,  processor scheduling,  simultaneous multithreading processor,  stream fetch,  thread fetch priority, Computer architecture, Counting circuits, Engines, Feeds, Multithreading, Pipelines, Proposals, Surface-mount technology, Throughput, Yarn, },
 abstract = {Simultaneous multithreading (SMT) is an architectural technique that allows for the parallel execution of several threads simultaneously. Fetch performance has been identified as the most important bottleneck for SMT processors. The commonly adopted solution has been fetching from more than one thread each cycle. Recent studies have proposed a plethora of fetch policies to deal with fetch priority among threads, trying to increase fetch performance. We demonstrate that the simultaneous sharing of the fetch unit, apart from increasing the complexity of the fetch unit, can be counterproductive in terms of performance. We evaluate the use of high-performance fetch units in the context of SMT. Our new fetch architecture proposal allows us to feed an 8-way processor fetching from a single thread each cycle, reducing complexity, and increasing the usefulness of proposed fetch policies. Our results show that using new high-performance fetch units, like the FTB or the stream fetch, provides higher performance than fetching from two threads using common SMT fetch architectures. Furthermore, our results show that our design obtains better average performance for any kind of workloads (both ILP and memory bounded benchmarks), in contrast to previously proposed solutions. },
}

@inproceedings{1410080,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Hu, J.S. and Vijaykrishnan, N. and Irwin, M.J.},
 year = {2004},
 pages = { 232-- 232},
 publisher = {IEEE},
 title = {Exploring Wakeup-Free Instruction Scheduling},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10014},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410080},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410080.pdf?arnumber=1410080},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = {Broadcasting, Clocks, Computer architecture, Cyclones, Degradation, Delay, Dynamic scheduling, Impedance, Logic design, Processor scheduling, },
 abstract = {Design of wakeup-free issue queues is becoming desirable due to the increasing complexity associated with broadcast-based instruction wakeup. The effectiveness of most wakeup-free issue queue designs is critically based on their success in predicting the issue latency of an instruction accurately. Consequently, the goal of this paper is to explore the predictability of instruction issue latency under different design constraints and to identify the impediments to performance in such wakeup-free architectures. Our results indicate that structural problems in promoting instructions to the head of the instruction queue from where they are issued in wakeup-free architectures, the limited number of candidate instructions that can be considered for instruction issue, and the resource conflicts due to non-availability of issue ports all have a significant impact in degrading the performance of broadcast free architectures. Based on these observation, we explore an architecture that attempts to overcome the structural limitations by employing traditional selection logic and by using pre-check logic to reduce the impact of resource conflicts while still employing a wakeup-free strategy based on predicted instruction issue latencies. Finally, we improve this technique by limiting the selection logic to a small segment of the issue queue. },
}

@inproceedings{1410087,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {},
 year = {2004},
 pages = { 311-- 311},
 publisher = {IEEE},
 title = {Author Index},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10000},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410087},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410087.pdf?arnumber=1410087},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 abstract = {},
}

@inproceedings{1410086,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Fernandez, M. and Espasa, R.},
 year = {2004},
 pages = { 300-- 309},
 publisher = {IEEE},
 title = {Link-time path-sensitive memory redundancy elimination},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10009},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410086},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410086.pdf?arnumber=1410086},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { MRE,  alias analysis algorithm,  link-time optimizer,  link-time path-sensitive memory redundancy elimination,  memory instruction,  memory location,  optimisation,  optimization,  path-insensitive information,  redundancy,  storage management, Algorithm design and analysis, Computer architecture, Electronic mail, Force feedback, Instruments, Performance analysis, Production, Program processors, Registers, Upper bound, },
 abstract = {Optimizations performed at link-time or directly applied to final program executables have received increased attention in recent years. We discuss the discovery and elimination of redundant memory operations in the context of a link-time optimizer, an optimization that we call memory redundancy elimination (MRE). Previous research showed that existing MRE techniques are mainly based on path-insensitive information, which causes many MRE opportunities to be lost. We present a new technique for eliminating redundant loads in a path-sensitive fashion, by using a novel alias analysis algorithm that is able to expose path-sensitive memory redundancies. We also extend our previous work by removing both redundant and dead stores. Our experiments show that around 75\% of load and 10\% of store references in a program can be considered redundant, because they are accessing memory locations that have been referenced less than 256 memory instructions away. By combining our previous optimizations for eliminating load redundancies with the new techniques developed, we show that around 18\% of the loads and 8\% of the stores can be detected and eliminated, which translates into a 10\% reduction in execution time. },
}

@inproceedings{1385943,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Spracklen, L. and Yuan Chou and Abraham, S.G.},
 year = {2005},
 pages = { 225-- 236},
 publisher = {IEEE},
 title = {Effective instruction prefetching in chip multiprocessors for modern commercial applications},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.13},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385943},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385943.pdf?arnumber=1385943},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { L1 cache,  L2 cache,  SPECjAppServer2002,  SPECweb99,  TPC-W,  cache storage,  chip multiprocessors,  database workload,  discontinuity prefetching,  instruction cache miss behavior,  instruction cache prefetching,  instruction sets,  microprocessor chips,  multiprocessing systems,  queueing theory,  sequential prefetching, Bandwidth, Databases, Hardware, Performance analysis, Performance loss, Pipelines, Pollution, Prefetching, Sun, Web server, },
 abstract = {In this paper, we study the instruction cache miss behavior of four modern commercial applications (a database workload, TPC-W, SPECjAppServer2002 and SPECweb99). These applications exhibit high instruction cache miss rates for both the L1 and L2 caches, and a sizable performance improvement can be achieved by eliminating these misses. We show that it is important, not only to address sequential misses, but also misses due to branches and function calls. As a result, we propose an efficient discontinuity prefetching scheme that can be effectively combined with traditional sequential prefetching to address all forms of instruction cache misses. Additionally, with the emergence of chip multiprocessors (CMPs), instruction prefetching schemes must take into account their effect on the shared L2 cache. Specifically aggressive instruction cache prefetching can result in an increase in the number of L2 cache data misses. As a solution, we propose a scheme that does not install prefetches into the L2 cache unless they are proven to be useful. Overall, we demonstrate that the combination of our proposed schemes is successful in reducing the instruction miss rate to only 10\%-16\% of the original miss rate and results in a 1.08X-1.37X performance improvement for the applications studied. },
}

@inproceedings{1183531,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Kronstadt, E.},
 year = {2003},
 pages = { 125-- 125},
 publisher = {IEEE},
 title = {Beyond performance: some (other) challenges for systems design},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183531},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183531},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183531.pdf?arnumber=1183531},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = {Computer architecture, Costs, Power system interconnection, Software performance, Very large scale integration, Vocabulary, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01183531.png" border="0"> },
}

@inproceedings{995694,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Grochowski, E. and Ayers, D. and Tiwari, V.},
 year = {2002},
 pages = { 7-- 16},
 publisher = {IEEE},
 title = {Microarchitectural simulation and control of di/dt-induced power supply voltage variation},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995694},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995694},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995694.pdf?arnumber=995694},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { di/dt-induced power supply voltage variation,  feedback loop,  high performance microprocessors,  microarchitectural simulation,  microarchitecture simulators,  microprocessor chips,  power consumption,  power consumption,  power supply circuits,  real-time computation,  transient response,  voltage simulation capability, Clocks, Computational modeling, Control systems, Energy consumption, Feedback loop, Logic, Microarchitecture, Microprocessors, Power supplies, Voltage control, },
 abstract = {As the power consumption of modern high-performance microprocessors increases beyond 100 W, power becomes an increasingly important design consideration. This paper presents a novel technique to simulate power supply voltage variation as a result of varying activity levels within the microprocessor when executing typical software. The voltage simulation capability may be added to existing microarchitecture simulators that determine the activities of each functional block on a clock-by-clock basis. We then discuss how the same technique can be implemented in logic on the microprocessor die to enable real-time computation of current consumption and power supply voltage. When used in a feedback loop, this logic makes it possible to control the microprocessor's activities to reduce demands on the power delivery system. With on-die voltage computation and di/dt control, we show that a significant reduction in power supply voltage variation may be achieved with little performance loss or average power increase. },
}

@inproceedings{995695,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Skadron, K. and Abdelzaher, T. and Stan, M.R.},
 year = {2002},
 pages = { 17-- 28},
 publisher = {IEEE},
 title = {Control-theoretic techniques and thermal-RC modeling for accurate and localized dynamic thermal management},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995695},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995695},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995695.pdf?arnumber=995695},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { PID controller,  SPEC2000 benchmarks,  Wattch,  adaptive control,  control-theoretic techniques,  controllers,  localized dynamic thermal management,  lumped thermal resistances,  power supply circuits,  processor architecture,  thermal capacitances,  thermal management (packaging),  thermal trigger threshold,  thermal-RC modeling,  three-term control, Adaptive control, Feedback control, Heating, Programmable control, Temperature, Testing, Thermal management, Thermal resistance, Vehicle dynamics, Vehicles, },
 abstract = {This paper proposes the use of formal feedback control theory as a way to implement adaptive techniques in the processor architecture. Dynamic thermal management (DTM) is used as a test vehicle, and variations of a PID controller (Proportional-Integral-Differential) are developed and tested for adaptive control of fetch "toggling." To accurately test the DTM mechanism being proposed, this paper also develops a thermal model based on lumped thermal resistances and thermal capacitances. This model is computationally efficient and tracks temperature at the granularity of individual functional blocks within the processor. Because localized heating occurs much faster than chip-wide heating, some parts of the processor are more likely, to be "hot spots" than others. Experiments using Wattch and the SPEC2000 benchmarks show that the thermal trigger threshold can be set within 0.2\&deg; of the maximum temperature and yet never enter thermal emergency. This cuts the performance loss of DTM by 65\% compared to the previously described fetch toggling technique that uses a response of fixed magnitude. },
}

@inproceedings{501177,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Landin, A. and Dahlgren, F.},
 year = {1996},
 pages = {95--105},
 publisher = {IEEE},
 title = {Bus-based COMA-reducing traffic in shared-bus multiprocessors},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501177},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501177},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501177.pdf?arnumber=501177},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Aggregates, Bandwidth, Clocks, Computer architecture, Computer graphics, Computer science, Memory architecture, SPLASH, Silicon, Testing, Traffic control, bus-based COMA, cache only memory architecture, cache storage, memory architecture, program-driven simulation, shared memory systems, shared-bus multiprocessors, shared-memory multiprocessors, standard UMA architecture, },
 abstract = {A problem with bus-based shared-memory multiprocessors is that the shared bus rapidly becomes a bottleneck in the machine, effectively limiting the machine size to somewhere between ten and twenty processors. We propose a new architecture, the bus-based COMA (BB-COMA) that addresses this problem. Compared to the standard UMA architecture, the BE-COMA has lower requirements on bus bandwidth. We have used program-driven simulation to study the two architectures running applications from the SPLASH suite. We observed a traffic reduction of up to 70\% for BB-COMA, with an average of 46\%, for the programs studied. The results indicate that the BB-COMA is an interesting candidate architecture for future implementations of shared-bus multiprocessors },
}

@inproceedings{5416658,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Yoongu Kim and Dongsu Han and Mutlu, O. and Harchol-Balter, M.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {ATLAS: A scalable and high-performance scheduling algorithm for multiple memory controllers},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416658},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416658},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416658.pdf?arnumber=5416658},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {ATLAS memory scheduling, Adaptive control, Algorithm design and analysis, Bandwidth, Control systems, Feeds, Pareto distribution, Pareto workload distribution, Programmable control, Queueing analysis, Scheduling algorithm, Throughput, Yarn, adaptive per-thread least-attained-service, chip multiprocessor, digital storage, microprocessor chips, multiple memory controllers, multiprocessing systems, multiprogrammed SPEC 2006 workloads, scheduling, scheduling algorithm, single-server queue, },
 abstract = {Modern chip multiprocessor (CMP) systems employ multiple memory controllers to control access to main memory. The scheduling algorithm employed by these memory controllers has a significant effect on system throughput, so choosing an efficient scheduling algorithm is important. The scheduling algorithm also needs to be scalable - as the number of cores increases, the number of memory controllers shared by the cores should also increase to provide sufficient bandwidth to feed the cores. Unfortunately, previous memory scheduling algorithms are inefficient with respect to system throughput and/or are designed for a single memory controller and do not scale well to multiple memory controllers, requiring significant finegrained coordination among controllers. This paper proposes ATLAS (Adaptive per-Thread Least-Attained-Service memory scheduling), a fundamentally new memory scheduling technique that improves system throughput without requiring significant coordination among memory controllers. The key idea is to periodically order threads based on the service they have attained from the memory controllers so far, and prioritize those threads that have attained the least service over others in each period. The idea of favoring threads with least-attained-service is borrowed from the queueing theory literature, where, in the context of a single-server queue it is known that least-attained-service optimally schedules jobs, assuming a Pareto (or any decreasing hazard rate) workload distribution. After verifying that our workloads have this characteristic, we show that our implementation of least-attained-service thread prioritization reduces the time the cores spend stalling and significantly improves system throughput. Furthermore, since the periods over which we accumulate the attained service are long, the controllers coordinate very infrequently to form the ordering of threads, thereby making ATLAS scalable to many controllers. We evaluate ATLAS on a wide variety of mult- - iprogrammed SPEC 2006 workloads and systems with 4-32 cores and 1-16 memory controllers, and compare its performance to five previously proposed scheduling algorithms. Averaged over 32 workloads on a 24-core system with 4 controllers, ATLAS improves instruction throughput by 10.8\%, and system throughput by 8.4\%, compared to PAR-BS, the best previous CMP memory scheduling algorithm. ATLAS's performance benefit increases as the number of cores increases. },
}

@inproceedings{5416659,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Farooq, M.U. and Lei Chen and John, L.K.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Value Based BTB Indexing for indirect jump prediction},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416659},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416659},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416659.pdf?arnumber=5416659},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Accuracy, Computer aided instruction, Engines, Functional programming, Hardware, History, Indexing, Indirect branches, Java, Object oriented modeling, Object oriented programming, branch target prediction, compiler guided branch prediction, conditional branches, correlation-based branch prediction, history-based branch direction predictors, indexing, indirect branches, indirect jump instruction, indirect jump target prediction, jump misprediction, prediction theory, tagged target cache, value based BTB indexing, },
 abstract = {History-based branch direction predictors for conditional branches are shown to be highly accurate. Indirect branches however, are hard to predict as they may have multiple targets corresponding to a single indirect branch instruction. We propose the Value Based BTB Indexing (VBBI), a correlation-based target address prediction scheme for indirect jump instructions. For each static hard-to-predict indirect jump instruction, the compiler identifies a `hint instruction', whose output value strongly correlates with the target address of the indirect jump instruction. At run time, multiple target addresses of the indirect jump instruction are stored and subsequently accessed from the BTB at different indices computed using the jump instruction PC and the hint instruction output values. In case the hint instruction has not finished its execution when the jump instruction is fetched, a second and more accurate target address prediction is made when the hint instruction output is available, thus reducing the jump misprediction penalty. We compare our design to the regular BTB design and the best previously proposed indirect jump predictor, the tagged target cache (TTC). Our evaluation shows that the VBBI scheme improves the indirect jump target prediction accuracy by 48\% and 18\%, compared with the baseline BTB and TTC designs, respectively. This results in average performance improvement of 16.4\% over the baseline BTB scheme, and 13\% improvement over the TTC predictor. Out of this performance improvement 2\% is contributed by target prediction overriding which is accurate 96\% of the time. },
}

@inproceedings{5416654,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Kaseridis, D. and Stuecheli, J. and Jian Chen and John, L.K.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {A bandwidth-aware memory-subsystem resource management using non-invasive resource profilers for large CMP systems},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416654},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416654},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416654.pdf?arnumber=5416654},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Bandwidth, Energy consumption, Heuristic algorithms, Interference, Mattson's stack distance algorithm, Memory management, Partitioning algorithms, Quality of service, Resource management, Throughput, Yarn, bandwidth-aware algorithm, bandwidth-aware memory-subsystem resource management, cache capacity, cache partitioning algorithms, cache storage, chip multiprocessors, cycle-accurate full system simulator, destructive interference, dynamic memory-subsystem resource management scheme, last level cache, memory bandwidth contention, memory bandwidth requirements, memory subsystem, multichip CMP systems, multiple cores, multiprocessing systems, non-invasive resource profilers, resource requirements, resource sharing, resource-overcommitted chips, single chip, storage management chips, system-on-chip, throughput optimizations, },
 abstract = {By integrating multiple cores in a single chip, Chip Multiprocessors (CMP) provide an attractive approach to improve both system throughput and efficiency. This integration allows the sharing of on-chip resources which may lead to destructive interference between the executing workloads. Memorysubsystem is an important shared resource that contributes significantly to the overall throughput and power consumption. In order to prevent destructive interference, the cache capacity and memory bandwidth requirements of the last level cache have to be controlled. While previously proposed schemes focus on resource sharing within a chip, we explore additional possibilities both inside and outside a single chip. We propose a dynamic memory-subsystem resource management scheme that considers both cache capacity and memory bandwidth contention in large multi-chip CMP systems. Our approach uses low overhead, non-invasive resource profilers that are based on Mattson's stack distance algorithm to project each core's resource requirements and guide our cache partitioning algorithms. Our bandwidth-aware algorithm seeks for throughput optimizations among multiple chips by migrating workloads from the most resource-overcommitted chips to the ones with more available resources. Use of bandwidth as a criterion results in an overall 18\% reduction in memory bandwidth along with a 7.9\% reduction in miss rate, compared to existing resource management schemes. Using a cycle-accurate full system simulator, our approach achieved an average improvement of 8.5\% on throughput. },
}

@inproceedings{5416655,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Fang Liu and Xiaowei Jiang and Yan Solihin},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Understanding how off-chip memory bandwidth partitioning in Chip Multiprocessors affects system performance},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416655},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416655},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416655.pdf?arnumber=5416655},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Analytical models, Artificial neural networks, Bandwidth, CMP system, Chip Multi-Processor, Computer architecture, Process design, System performance, Throughput, Yarn, cache partitioning, cache partitioning, cache storage, chip multiprocessor architecture, level cache, microprocessor chips, multiprocessing systems, off-chip memory bandwidth, off-chip memory bandwidth partitioning, off-chip memory bandwidth partitioning scheme, off-chip pin bandwidth, optimum bandwidth partition, shared cache, },
 abstract = {Chip Multi-Processor (CMP) architectures have recently become a mainstream computing platform. Recent CMPs allow cores to share expensive resources, such as the last level cache and off-chip pin bandwidth. To improve system performance and reduce the performance volatility of individual threads, last level cache and off-chip bandwidth partitioning schemes have been proposed. While how cache partitioning affects system performance is well understood, little is understood regarding how bandwidth partitioning affects system performance, and how bandwidth and cache partitioning interact with one another. In this paper, we propose a simple yet powerful analytical model that gives us an ability to answer several important questions: (1) How does off-chip bandwidth partitioning improve system performance? (2) In what situations the performance improvement is high or low, and what factors determine that? (3) In what way cache and bandwidth partitioning interact, and is the interaction negative or positive? (4) Can a theoretically optimum bandwidth partition be derived, and if so, what factors affect it? We believe understanding the answers to these questions is very valuable to CMP system designers in coming up with strategies to deal with the scarcity of off-chip bandwidth in future CMPs with many cores on a chip. },
}

@inproceedings{5416656,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Greskamp, B. and Karpuzcu, U.R. and Torrellas, J.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {LeadOut: Composing low-overhead frequency-enhancing techniques for single-thread performance in configurable multicores},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416656},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416656},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416656.pdf?arnumber=5416656},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Computer science, Dynamic voltage scaling, Error analysis, Frequency domain analysis, Hardware, Microarchitecture, Multicore processing, Temperature, Timing, Yarn, boost voltage, configurable multicores, higher than nominal frequencies, leader checker configuration, low-overhead frequency-enhancing techniques, microprocessor chips, multiprocessing systems, optimisation, optimization, performance critical core, performance critical thread, performance evaluation, safe frequency margins, single thread performance, supply voltage, },
 abstract = {Despite the ubiquity of multicores, it is as important as ever to deliver high single-thread performance. An appealing way to accomplish this is by shutting down the idle cores in the chip and running the busy, performance-critical core(s) at higher-than-nominal frequencies. To enable such frequencies, two low-overhead approaches either boost voltage beyond nominal values, or pair cores in leader-checker configurations and let them run beyond safe frequency margins. We observe that, in a large multicore with varying numbers of busy cores, individual application of either of these two techniques is suboptimal. Each alone is often unable to bring the multicore all the way to its power or temperature envelopes due to limitations in supply voltage or error rate. Moreover, we show that the two techniques are complementary, and can be synergistically combined to unlock much higher levels of single-thread performance. Finally, we demonstrate a dynamic controller that optimizes the two techniques. Our data shows that, given a 16-core multi-core where half of the cores are already busy, an additional, performance-critical thread now attains 34\% higher performance than before, while consuming 220\% more power. },
}

@inproceedings{5416657,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Champagne, D. and Lee, R.B.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Scalable architectural support for trusted software},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416657},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416657},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416657.pdf?arnumber=5416657},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Application software, Bastion, Computer architecture, Hardware, Information security, Microprocessors, OpenSPARC platform, Operating systems, Protection, Scalability, Virtual machine monitors, Virtual machining, enhanced hypervisor software, enhanced microprocessor hardware, fine grained memory compartment, hardware-software architecture, microprocessor chips, safety-critical software, scalable architectural support, secure persistent storage, secure storage, security critical software modules, software architecture, trusted software, },
 abstract = {We present Bastion, a new hardware-software architecture for protecting security-critical software modules in an untrusted software stack. Our architecture is composed of enhanced microprocessor hardware and enhanced hypervisor software. Each trusted software module is provided with a secure, fine-grained memory compartment and its own secure persistent storage area. Bastion is the first architecture to provide direct hardware protection of the hypervisor from both software and physical attacks, before employing the hypervisor to provide the same protection to security-critical OS and application modules. Our implementation demonstrates the feasibility of bypassing an untrusted commodity OS to provide application security and shows better security with higher performance when compared to the Trusted Platform Module (TPM), the current industry state-of-the-art security chip. We provide a proof-of-concept implementation on the OpenSPARC platform. },
}

@inproceedings{5416650,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Guangyu Sun and Yongsoo Joo and Yibo Chen and Dimin Niu and Yuan Xie and Yiran Chen and Hai Li},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {A Hybrid solid-state storage architecture for the performance, energy consumption, and lifetime improvement},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416650},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416650},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416650.pdf?arnumber=5416650},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Computer buffers, Energy consumption, Energy storage, File systems, Flash memory, Hard disks, NAND circuits, NAND flash memory, PRAM log region, Phase change random access memory, Random access memory, Read-write memory, Solid state circuits, energy consumption, erase operations, erase-before-write requirement, flash memories, hybrid solid-state storage architecture, log-based structures, memory architecture, phase change random access memory, },
 abstract = {In recent years, many systems have employed NAND flash memory as storage devices because of its advantages of higher performance (compared to the traditional hard disk drive), high-density, random-access, increasing capacity, and falling cost. On the other hand, the performance of NAND flash memory is limited by its Â¿erase-before-writeÂ¿ requirement. Log-based structures have been used to alleviate this problem by writing updated data to the clean space. Prior log-based methods, however, cannot avoid excessive erase operations when there are frequent updates, which quickly consume free pages, especially when some data are updated repeatedly. In this paper, we propose a hybrid architecture for the NAND flash memory storage, of which the log region is implemented using phase change random access memory (PRAM). Compared to traditional log-based architectures, it has the following advantages: (1) the PRAM log region allows in-place updating so that it significantly improves the usage efficiency of log pages by eliminating out-of-date log records; (2) it greatly reduces the traffic of reading from the NAND flash memory storage since the size of logs loaded for the read operation is decreased; (3) the energy consumption of the storage system is reduced as the overhead of writing and reading log data is decreased with the PRAM log region; (4) the lifetime of NAND flash memory is increased because the number of erase operations are reduced. To facilitate the PRAM log region, we propose several management policies. The simulation results show that our proposed methods can substantially improve the performance, energy consumption, and lifetime of the NAND flash memory storage<sup>1</sup>. },
}

@inproceedings{5416651,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Doudalis, I. and Prvulovic, M.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {HARE: Hardware assisted reverse execution},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416651},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416651},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416651.pdf?arnumber=5416651},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {HARE, Hardware, bidirectional execution, checkpointing, checkpointing, debugging technique, hardware assisted reverse execution, program debugging, },
 abstract = {Bidirectional execution is a powerful debugging technique that allows program execution to proceed both forward and in reverse. Many software-only techniques and tools have emerged that use checkpointing and replay to provide the effect of reverse execution, although with considerable performance overheads in both forward and reverse execution. Recent hardware proposals for checkpointing and execution replay minimize these performance overheads, but in a way that prevents checkpoint consolidation, a key technique for reducing memory use while retaining the ability to reverse long periods of execution. This paper presents HARE, a hardware technique that efficiently supports both checkpointing and consolidation. Our experiments show that on average HARE incurs \&lt;3\% performace overheads even when creating tens of checkpoints per second, provides reverse execution times similar to forward execution times, and reduces the total space used by checkpoints by a factor of 36 on average (this factor gets better for longer runs) relative to prior consolidation-less hardware checkpointing schemes. },
}

@inproceedings{5416652,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Kahng, A.B. and Seokhyeong Kang and Kumar, R. and Sartori, J.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Designing a processor from the ground up to allow voltage/reliability tradeoffs},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416652},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416652},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416652.pdf?arnumber=5416652},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Circuits, Error analysis, Error correction, Flip-flops, Frequency, Latches, Process design, Timing, Virtual colonoscopy, Voltage, energy conservation, error analysis, error rate, error-tolerance mechanism, microprocessor chips, operational errors, optimisation, optimization, power consumption, power savings, power-aware slack redistribution, processor design, reliability tradeoff, soft architectures, timing errors, voltage scaling, voltage tradeoff, },
 abstract = {Current processor designs have a critical operating point that sets a hard limit on voltage scaling. Any scaling beyond the critical voltage results in exceeding the maximum allowable error rate, i.e., there are more timing errors than can be effectively and gainfully detected or corrected by an error-tolerance mechanism. This limits the effectiveness of voltage scaling as a knob for reliability/power tradeoffs. In this paper, we present power-aware slack redistribution, a novel design-level approach to allow voltage/reliability tradeoffs in processors. Techniques based on power-aware slack redistribution reapportion timing slack of the frequently-occurring, near-critical timing paths of a processor in a power- and area-efficient manner, such that we increase the range of voltages over which the incidence of operational (timing) errors is acceptable. This results in soft architectures - designs that fail gracefully, allowing us to perform reliability/power tradeoffs by reducing voltage up to the point that produces maximum allowable errors for our application. The goal of our optimization is to minimize the voltage at which a soft architecture encounters the maximum allowable error rate, thus maximizing the range over which voltage scaling is possible and minimizing power consumption for a given error rate. Our experiments demonstrate 23\% power savings over the baseline design at an error rate of 1\%. Observed power reductions are 29\%, 29\%, 19\%, and 20\% for error rates of 2\%, 4\%, 8\%, and 16\% respectively. Benefits are higher in the face of error recovery using Razor. Area overhead of our techniques is up to 2.7\%. },
}

@inproceedings{5416653,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Jafri, S.A.R. and Thottethodi, M. and Vijaykumar, T.N.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {LiteTM: Reducing transactional state overhead},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416653},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416653},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416653.pdf?arnumber=5416653},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Delay, Error correction codes, Hardware, Impedance, LiteTM, Ll-block granularity, Parallel programming, Power dissipation, Protection, STAMP benchmarks, Switches, System recovery, Yarn, chip multiprocessors, concurrency control, microprocessor chips, shared memory systems, thread identifier, thread migration, transactional memory, transactional read-sharer count, transactional state overhead, },
 abstract = {Transactional memory (TM) has been proposed to address some of the programmability issues of chip multiprocessors. Hardware implementations of transactional memory (HTMs) have made significant progress in providing support for features such as long transactions that spill out of the cache, and context switches, page and thread migration in the middle of transactions. While essential for the adoption of HTMs in real products, supporting these features has resulted in significant state overhead. For instance, TokenTM adds at least 16 bits per block in the caches which is significant in absolute terms, and steals 16 of 64 (25\%) memory ECC bits per block, weakening error protection. Also, the state bits nearly double the tag array size. These significant and practical concerns may impede the adoption of HTMs, squandering the progress achieved by HTMs. The overhead comes from tracking the thread identifier and the transactional read-sharer count at the Ll-block granularity. The thread identifier is used to identify the transaction, if only one, to which an Ll-evicted block belongs. The read-sharer count is used to identify conflicts involving multiple readers (i.e., write to a block with non-zero count). To reduce this overhead, we observe that the thread identifiers and read-sharer counts are not needed in a majority of cases. (1) Repeated misses to the same blocks are rare within a transaction (i.e., locality holds). (2) Transactional read-shared blocks that both are evicted from multiple sharers' Lis and are involved in conflicts are rare. Exploiting these observations, we propose a novel HTM, called LiteTM, which completely eliminates the count and identifier and uses software to infer the lost information. Using simulations of the STAMP benchmarks running on 8 cores, we show that LiteTM reduces TokenTM's state overhead by about 87\% while performing within 4\%, on average, and 10\%, in the worst case, of TokenTM. },
}

@inproceedings{744346,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Rotenberg, E. and Jacobson, Q. and Smith, J.},
 year = {1999},
 pages = {115--124},
 publisher = {IEEE},
 title = {A study of control independence in superscalar processors},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744346},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744346},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744346.pdf?arnumber=744346},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Computer aided instruction, Concurrent computing, Dynamic scheduling, Flow graphs, Hardware, Jacobian matrices, Parallel processing, Process control, Processor scheduling, control independence, data dependences, design alternatives, future generation processors, hardware constraints, idealized machine models, implementation features, implementation issues, incorrect control dependent instructions, instruction level parallelism, instruction sets, parallel architectures, parallel programming, perfect branch prediction, performance potential, superscalar processors, wasted resources, },
 abstract = {Control independence has been put forward as a significant new source of instruction level parallelism for future generation processors. However, its performance potential under practical hardware constraints is not known, and even less is understood about the factors that contribute to or limit the performance of control independence. Important aspects of control independence are identified and singled out for study, and a series of idealized machine models are used to isolate and evaluate these aspects. It is shown that much of the performance potential of control independence is lost due to data dependences and wasted resources consumed by incorrect control dependent instructions. Even so, control independence can close the performance gap between real and perfect branch prediction by as much as half. Next, important implementation issues are discussed and some design alternatives are given. This is followed by a more detailed set of simulations, where the key implementation features are realistically modeled. These simulations show typical performance improvements of 10-30\% },
}

@inproceedings{744347,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Jacobson, Q. and Smith, J.E.},
 year = {1999},
 pages = {125--129},
 publisher = {IEEE},
 title = {Instruction pre-processing in trace processors},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744347},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744347},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744347.pdf?arnumber=744347},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Clocks, Computer aided instruction, Delay, Distributed processing, Encapsulation, Engines, Hardware, Microarchitecture, Optimizing compilers, Processor scheduling, constant propagation, dynamic instruction sequence, hardware optimizations, instruction collapsing, instruction fetch rates, instruction pre-processing, instruction scheduling, instruction sets, parallel architectures, parallel programming, performance benefit, program processors, scheduling, sequential program partitioning, trace processors, wide-issue execution engines, },
 abstract = {In trace processors, a sequential program is partitioned at run time into ``traces". A trace is an encapsulation of a dynamic sequence of instructions. A processor that uses traces as the unit of sequencing and execution achieves high instruction fetch rates and can support very wide-issue execution engines. We propose a new class of hardware optimizations that transform the instructions within traces to increase the performance of trace processors. Traces are ``pre-processed" to optimize the instructions for execution together. We propose three specific optimizations: instruction scheduling, constant propagation, and instruction collapsing. Together, these optimizations offer substantial performance benefit, increasing performance by up to 24\% },
}

@inproceedings{1598109,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Yingmin Li and Lee, B. and Brooks, D. and Zhigang Hu and Skadron, K.},
 year = {2006},
 pages = { 17-- 28},
 publisher = {IEEE},
 title = {CMP design space exploration subject to physical constraints},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598109},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598109},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598109.pdf?arnumber=1598109},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { CMP design space exploration,  CPU-bound workloads,  aggressive cooling solutions,  chip multiprocessors,  computer architecture,  joint optimization,  logic design,  memory-bound workloads,  microprocessor chips,  multidimensional design space,  multiprocessing systems,  physical constraints,  pin-bandwidth,  power delivery,  power density,  thermal constraints, Computer science, Constraint optimization, Cooling, Design engineering, Design optimization, Frequency, Pipelines, Space exploration, Throughput, Voltage, },
 abstract = {This paper explores the multi-dimensional design space for chip multiprocessors, exploring the inter-related variables of core count, pipeline depth, superscalar width, L2 cache size, and operating voltage and frequency, under various area and thermal constraints. The results show the importance of joint optimization. Thermal constraints dominate other physical constraints such as pin-bandwidth and power delivery, demonstrating the importance of considering thermal constraints while optimizing these other parameters. For aggressive cooling solutions, reducing power density is at least as important as reducing total power, while for low-cost cooling solutions, reducing total power is more important. Finally, the paper shows the challenges of accommodating both CPU-bound and memory-bound workloads on the same design. Their respective preferences for more cores and larger caches lead to increasingly irreconcilable configurations as area and other constraints are relaxed; rather than accommodating a happy medium, the extra resources simply encourage more extreme optimization points. },
}

@inproceedings{1598108,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Constantinides, K. and Plaza, S. and Blome, J. and Zhang, B. and Bertacco, V. and Mahlke, S. and Austin, T. and Orshansky, M.},
 year = {2006},
 pages = { 5-- 16},
 publisher = {IEEE},
 title = {BulletProof: a defect-tolerant CMP switch architecture},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598108},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598108},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598108.pdf?arnumber=1598108},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { BulletProof,  CMP router switch,  automatic circuit decomposition,  chip multiprocessor system,  circuit-level timing,  complex computing systems design,  computer architecture,  defect-tolerant CMP switch architecture,  end-to-end error detection,  fault tolerance,  iterative diagnosis,  iterative reconfiguration,  logic design,  microprocessor chips,  multiprocessing systems,  network routing,  online recovery,  online repair,  permanent faults,  reliability,  resource sparing,  silicon technologies,  time-tested bathtub curve,  transient faults, Circuit faults, Computer architecture, Computer errors, Nanoscale devices, Protection, Redundancy, Silicon, Switches, Switching circuits, Timing, },
 abstract = {As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this paper, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. To start, we develop a unified model of faults, based on the time-tested bathtub curve. Using this convenient abstraction, we analyze the reliability versus area tradeoff across a wide spectrum of CMP switch designs, ranging from unprotected designs to fully protected designs with online repair and recovery capabilities. Protection is considered at multiple levels from the entire system down through arbitrary partitions of the design. To better understand the impact of these faults, we evaluate our CMP switch designs using circuit-level timing on detailed physical layouts. Our experimental results are quite illuminating. We find that designs are attainable that can tolerate a larger number of defects with less overhead than naive triple-modular redundancy, using domain-specific techniques such as end-to-end error detection, resource sparing, automatic circuit decomposition, and iterative diagnosis and reconfiguration. },
}

@inproceedings{744342,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Jian Huang and Lilja, D.J.},
 year = {1999},
 pages = {106--114},
 publisher = {IEEE},
 title = {Exploiting basic block value locality with block reuse},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744342},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744342},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744342.pdf?arnumber=744342},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {-O2 optimization level, 4-way issue superscalar processor, Accuracy, Computer aided instruction, Computer science, Ear, GCC compiler, History, Optimizing compilers, Program processors, Registers, SPEC benchmark programs, SPEC programs, Testing, aggressive speculation, basic block reuse, basic block value locality, block history buffer, block reuse, buffer storage, coarser granularity, compiler support, execution time, instruction level, instruction sets, live register outputs, memory inputs, memory outputs, parallel architectures, parallel programming, performance benefits, program compilers, register inputs, software reusability, storage allocation, value prediction, },
 abstract = {Value prediction at the instruction level has been introduced to allow more aggressive speculation and reuse than previous techniques. We investigate the input and output values of basic blocks and find that these values can be quite regular and predictable, suggesting that using compiler support to extend value prediction and reuse to a coarser granularity may have substantial performance benefits. For the SPEC benchmark programs evaluated, 90\% of the basic blocks have fewer than 4 register inputs, 5 live register outputs, 4 memory inputs and 2 memory outputs. About 16\% to 41\% of all the basic blocks are simply repeating earlier calculations when the programs are compiled with the -O2 optimization level in the GCC compiler. We evaluate the potential benefit of basic block reuse using a novel mechanism called a block history buffer. This mechanism records input and live output values of basic blocks to provide value prediction and reuse at the basic block level. Simulation results show that using a reasonably sized block history buffer to provide basic block reuse in a 4-way issue superscalar processor can improve execution time for the tested SPEC programs by 1\% to 14\% with an overall average of 9\% },
}

@inproceedings{650569,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Mowry, T.C. and Chan, C.Q.C. and Lo, A.K.W.},
 year = {1998},
 pages = {300--311},
 publisher = {IEEE},
 title = {Comparative evaluation of latency tolerance techniques for software distributed shared memory},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650569},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650569},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650569.pdf?arnumber=650569},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Application software, Communication system software, Computer science, Delay, Electric breakdown, Hardware, Multithreading, Prefetching, Software performance, Workstations, comparative evaluation, distributed memory systems, distributed shared memory, latency tolerance, memory latency, multithreading, performance evaluation, prefetching, shared memory systems, software distributed shared memory, },
 abstract = {A key challenge in achieving high performance on software DSMs is overcoming their relatively large communication latencies. In this paper, we consider two techniques which address this problem: prefetching and multithreading. While previous studies have examined each of these techniques in isolation, this paper is the first to evaluate both techniques using a consistent hardware platform and set of applications, thereby allowing direct comparisons. In addition, this is the first study to consider combining prefetching and multithreading in a software DSM . We performed our experiments on real hardware using a full implementation of both techniques. Our experimental results demonstrate that both prefetching and multithreading result in significant performance improvements when applied individually. In addition, we observe that prefetching and multithreading can potentially complement each other by using prefetching to hide memory latency and multithreading to hide synchronization latency },
}

@inproceedings{650566,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Havanki, W.A. and Banerjia, S. and Conte, T.M.},
 year = {1998},
 pages = {266--276},
 publisher = {IEEE},
 title = {Treegion scheduling for wide issue processors},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650566},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650566},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650566.pdf?arnumber=650566},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Hardware, Linearity, Microprocessors, Parallel processing, Processor scheduling, Program processors, Tail, Topology, Tree data structures, Tree graphs, compilation, high-performance processors, instruction scheduling, linear regions, multiple execution paths, parallel architectures, processor scheduling, superblocks, traces, treegion, wide issue processors, },
 abstract = {Instruction scheduling is one of the most important phases of compilation for high-performance processors. A compiler typically divides a program into multiple regions of code and then schedules each region. Many past efforts have focused on linear regions such as traces and superblocks. The linearity of these regions can limit speculation, leading to under-utilization of processor resources, especially on wide-issue machines. A type of non-linear region called a treegion is presented in this paper. The formation and scheduling of treegions takes into account multiple execution paths, and the larger scope of treegions allows more speculation, leading to higher utilization and better performance. Multiple scheduling heuristics for treegions are compared against scheduling for several types of linear regions. Empirical results illustrate that instruction scheduling using treegions treegion scheduling-holds promise. Treegion scheduling using the global weight heuristic outperforms the next highest performing region-superblocks by up to 20\% },
}

@inproceedings{650567,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Ghosh, K. and Christie, A.J.},
 year = {1998},
 pages = {277--287},
 publisher = {IEEE},
 title = {Communication across fault-containment firewalls on the SGI origin },
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650567},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650567},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650567.pdf?arnumber=650567},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Buildings, Delay, Graphics, Hardware, IPC mechanisms, Kernel, Operating systems, Protection, Registers, SGI Origin-2000, Scalability, Silicon, fault tolerant computing, fault-containment firewalls, high-performance computing, kernel software, parallel architectures, performance numbers, reliability, reliability, shared memory systems, shared-memory, },
 abstract = {Scalability and reliability are inseparable in high-performance computing. Fault-isolation through hardware is a popular means of providing reliability. Unfortunately, such isolation also increases communication latencies: typically, one has to drop into and out of the kernel to communicate between failure domains. On the other hand, relaxing fault isolation domains allows efficient communication, but at the risk of failure propagation, and thus reduced reliability. We are concerned with finding a middle ground between these extremes. We first review a few salient aspects of the SGI Origin-2000 architecture, mentioning the hardware features germane to efficient communication, and building protection-firewalls. Then, we describe a mechanism for risk-free, point-to-point communication between processes on distinct failure domains. Quoting performance numbers, we show that the overheads of crossing domains render this mechanism unattractive for small messages. To address this issue, we describe a mechanism for controlled opening of the firewalls, thereby achieving explicit inter-partition shared-memory for communication. We describe the kernel software that addresses the resulting reliability issues, and discuss how familiar IPC mechanisms such as MPI and SysV shared-memory can use the explicit shared-memory to advantage. Finally, based on the lessons learnt, we discuss some future directions, and draw concluding remarks },
}

@inproceedings{1598107,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Shaw, D.E.},
 year = {2006},
 publisher = {IEEE},
 title = {New architectures for a new biology},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598107},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598107},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598107.pdf?arnumber=1598107},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { biology,  biology computing,  biomolecular simulation,  digital simulation,  high-performance computing technologies,  molecular dynamics simulations, Acceleration, Biological system modeling, Biomedical imaging, Chemical technology, Chemistry, Computational modeling, Computer architecture, Medical simulation, Molecular biophysics, Physics, },
 abstract = {Summary form only given. Some of the most important outstanding questions in the fields of biology, chemistry, and medicine remain unsolved as a result of our limited understanding of the structure, behavior and interaction of biologically significant molecules. The laws of physics that determine the form and function of these biomolecules are well understood. Current technology, however, does not allow us to simulate the effect of these laws with sufficient accuracy, and for a sufficient period of time, to answer many of the questions that biologists, biochemists, and biomedical researchers are most anxious to answer. This talk describes the current state of the art in biomolecular simulation and explores the potential role of high-performance computing technologies in extending current capabilities. Efforts within our own lab to develop novel architectures and algorithms to accelerate molecular dynamics simulations by several orders of magnitude would be described, along with work by other researchers pursuing alternative approaches. If such efforts ultimately prove successful, one might imagine the emergence of an entirely new paradigm in which computational experiments take their place alongside those conducted in "wet" laboratories as central tools in the quest to understand living organisms at a molecular level, and to develop safe, effective, precisely targeted medicines capable of relieving suffering and saving human lives. },
}

@inproceedings{650562,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Mackenzie, K. and Kubiatowicz, J. and Frank, M. and Lee, W. and Lee, V. and Agarwal, A. and Kaashoek, M.F.},
 year = {1998},
 pages = {231--242},
 publisher = {IEEE},
 title = {Exploiting two-case delivery for fast protected messaging},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650562},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650562},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650562.pdf?arnumber=650562},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Buffer storage, Computer science, Hardware, Laboratories, Message passing, Network interfaces, Parallel processing, Parallel programming, Protection, Read only memory, buffer capacity, buffer storage, direct application access, emulated hardware, fast protected messaging, memory consumption, memory-based network interface, message passing, multicomputer, multiprocessing systems, multiprogrammed environment, multiprogramming, network hardware, network interfaces, scheduling, scheduling, simulator, software buffering, tightly-coupled network interface, two-case delivery, virtual buffering, virtual memory, virtual storage, },
 abstract = {We propose and evaluate two complementary techniques to protect and virtualize a tightly-coupled network interface in a multicomputer. The techniques allow efficient, direct application access to network hardware in a multiprogrammed environment while gaining most of the benefits of a memory-based network interface. First, two-case delivery allows an application to receive a message directly from the network hardware in ordinary circumstances, but provides buffering transparently when required for protection. Second, virtual buffering stores messages in virtual memory on demand, providing the convenience of effectively unlimited buffer capacity while keeping actual physical memory consumption low. The evaluation is based on workloads of real and synthetic applications running on a simulator and partly on emulated hardware. The results show that the direct path is also the common path, justifying the use of software buffering. Further results show that physical buffering requirements remain low in our applications despite the use of unacknowledged messages and despite adverse scheduling conditions },
}

@inproceedings{650563,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Kalamationos, J. and Kaeli, D.R.},
 year = {1998},
 pages = {244--253},
 publisher = {IEEE},
 title = {Temporal-based procedure reordering for improved instruction cache performance},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650563},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650563},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650563.pdf?arnumber=650563},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {C++ applications, Cache memory, Frequency, Gnu applications, Hardware, Information analysis, Microprocessors, Performance analysis, SPEC 95, cache storage, calling frequencies, color-based mapping, conflict miss graph, conflict misses, edge weights, first generation cache conflict, graph theory, instruction cache performance, instruction sets, link-time code reordering algorithm, memory architecture, memory performance, performance evaluation, procedure-level temporal interaction, processor performance, storage allocation, temporal-based procedure reordering, },
 abstract = {As the gap between memory and processor performance continues to grow, it becomes increasingly important to exploit cache memory effectively. Both hardware and software techniques can be used to better utilize the cache. Hardware solutions focus on organization, while most software solutions investigate how to best layout a program on the available memory space. We present a new link-time code reordering algorithm targeted at reducing the frequency of misses in the cache. In past work we focused on eliminating first generation cache conflicts (i.e., conflicts between a procedure, and any of its immediate callers or callees) based on calling frequencies. In this work we exploit procedure-level temporal interaction, using a structure called a conflict miss graph (CMG). In the CMG every edge weight is an approximation of the worst-case number of misses two competing procedures can inflict upon one another. We use the ordering implied by the edge weights to apply color-based mapping and eliminate conflict misses between procedures lying either in the same or in different call chains. Using programs taken from SPEC 95, Gnu applications, and C++ applications, we have been able to improve upon previous algorithms, reducing the number of instruction cache conflicts by 20\% on average compared to the best procedure reordering algorithm },
}

@inproceedings{650560,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Mukherjee, S.S. and Hill, M.D.},
 year = {1998},
 pages = {207--218},
 publisher = {IEEE},
 title = {The impact of data transfer and buffering alternatives on network interface design},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650560},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650560},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650560.pdf?arnumber=650560},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Clocks, Communication switching, Computer interfaces, Computer networks, Delay, Explosives, Microprocessors, Network interfaces, Registers, Switches, access bottleneck, block loads, block stores, buffer storage, buffering, coherent network interfaces, data transfer, fine-grain communication latency, internal memory structures, memory architecture, memory bus, microprocessor performance, network interface design, network interfaces, network performance, performance evaluation, processor access, user-level DMA, },
 abstract = {The explosive growth in the performance of microprocessors and networks has created a new opportunity to reduce the latency of fine-grain communication. Microprocessor clock speeds are now approaching the gigahertz range. Network switch latencies have dropped to tens of nanoseconds. Unfortunately, this explosive growth also exposes processor accesses to the network interface (NI) as a critical bottleneck for fine-grain communication. Researchers have proposed several techniques, such as using block loads and stores, user-level DMA, and coherent network interfaces, to alleviate this NI access bottleneck. We systematically identify, examine and evaluate the key parameters that underlie these design alternatives. We classify these parameters into two categories: data transfer and buffering parameters. The data transfer parameters capture how messages are transferred between internal memory structures (e.g. processor caches, main memory) of a computer and a memory bus NI. The buffering parameters capture how and where an NI buffers incoming network messages. We evaluate seven memory bus NIs that we believe capture the essential components of the design space exposed by these data transfer and buffering parameters },
}

@inproceedings{650561,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Schoinas, I. and Hill, M.D.},
 year = {1998},
 pages = {219--230},
 publisher = {IEEE},
 title = {Address translation mechanisms in network interfaces},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650561},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650561},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650561.pdf?arnumber=650561},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Aerospace electronics, Birth disorders, CPU, Delay, Electronic switching systems, Government, Intelligent networks, Lab-on-a-chip, Myrinet, Network interfaces, Operating systems, Throughput, address translation mechanisms, application messaging performance, copying, lookup, message passing, microprocessor, minimal messaging, miss handling, network hardware performance, network interfaces, network interfaces, network operating systems, operating system, performance evaluation, protocols, simulation, single-copy protocol, translation locality, user-level messaging, },
 abstract = {Good network hardware performance is often squandered by overheads for accessing the network interface (NI) within a host. NIs that support user-level messaging avoid frequent operating system (OS) action yet unnecessary copying can still result in low performance. We explore improving application messaging performance by eliminating all unnecessary copies (minimal messaging). For minimal messaging, NIs must support address translation and must do so more richly than has been done in the past. NI address translation should flexibly support higher-level abstractions, map all user space, exploit translation locality, and degrade gracefully, when locality is poor. We classify NI address translation implementations based on where the lookup and the miss handling are performed (CPU or NI). We present alternative designs and we consider how they interact with the OS. We provide simulation results that evaluate the alternative design points and we demonstrate feasibility with a real implementation using Myrinet. We find: NIs need not have hardware lookup structures, as software schemes are fast enough; it is difficult for an NI to handle its own translation misses unless commercial operating systems are substantially modified to view an NI as CPU peer; in the conventional situation where the operating system views the NI as a device, minimal messaging should be used only when the translation is present, while a single-copy protocol is used when it is not; and alternatively one can currently get acceptable performance when the CPU handle misses if the kernel provides very fast trap interfaces but microprocessor and operating system trends may make this alternative less viable in the long run },
}

@inproceedings{386531,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Yamin Li and Wanming Chu},
 year = {1995},
 pages = {318--325},
 publisher = {IEEE},
 title = {The effects of STEF in finely parallel multithreaded processors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386531},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386531},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386531.pdf?arnumber=386531},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Delay, Dispatching, Multithreading, Parallel processing, Pipeline processing, Process design, Processor scheduling, Registers, Switches, Yarn, concurrency control, finely parallel multithreaded processors, interleaved dispatching, interleaved storage, interlock delay cycles, multiple-pipelined processor, parallel architectures, performance evaluation, performance prediction, pipeline dependencies, pipeline processing, round robin scheduling, structure conflicts, },
 abstract = {The throughput of a multiple-pipelined processor suffers due to lack of sufficient instructions to make multiple pipelines busy and due to delays associated with pipeline dependencies. Finely Parallel Multithreaded Processor (FPMP) architectures try to solve these problems by dispatching multiple instructions from multiple instruction threads in parallel. This paper proposes an analytic model which is used to quantify the advantage of FPMP architectures. The effects of four important parameters in FPMP, S,T,E, and F (STEF) are evaluated. Unlike previous analytic models of multithreaded architecture, the model presented concerns the performance of multiple pipelines. It deals not only with pipelines dependencies but also with structure conflicts. The model accepts the configuration parameters of a FPMP, the distribution of instruction types, and the distribution of interlock delay cycles. The model provides a quick performance prediction and a quick utilization prediction which are helpful in the processor design },
}

@inproceedings{569659,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Qin, X. and Baer, J.-L.},
 year = {1997},
 pages = {182--193},
 publisher = {IEEE},
 title = {On the use and performance of explicit communication primitives in cache-coherent multiprocessor systems},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569659},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569659},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569659.pdf?arnumber=569659},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Application software, Coherence, Communication system software, Coprocessors, Hardware, Message passing, Multiprocessing systems, PRAM model, Phase change random access memory, Protocols, Software performance, application performance, cache coherence protocol, cache storage, cache-coherent multiprocessor systems, communication mechanisms, explicit communication primitives, hardware cache coherence scheme, hardware implementation, memory protocols, message passing, message passing, performance, performance evaluation, protocol optimization, shared memory systems, shared-memory multiprocessor systems, },
 abstract = {Recent developments in shared-memory multiprocessor systems advocate using off-the-shelf hardware to provide basic communication mechanisms and using software to implement cache coherence policies. The exposure of communication mechanisms to software opens many opportunities for enhancing application performance. In this paper we propose a set of communication primitives implemented on a communication co-processor that introduce a flavor of message passing and permit protocol optimization. To assess the overhead of the software implementation of the primitives and protocols, we compare a PRAM model, a hardware cache coherence scheme, a software scheme implementing only the basic cache coherence protocol, and an optimized software solution supporting the additional communication primitives and running with applications annotated with those primitives. With the parameters we chose for the communication processor, the overall memory system overhead of the basic software scheme is at least 50\% higher than that of the hardware implementation. With the adequate insertion of the communication primitives, the optimized software solution has a performance comparable to that of the hardware scheme },
}

@inproceedings{569658,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Stricker, T. and Cross, T.},
 year = {1997},
 pages = {168--179},
 publisher = {IEEE},
 title = {Global address space, non-uniform bandwidth: a memory system performance characterization of parallel systems},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569658},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569658},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569658.pdf?arnumber=569658},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Bandwidth, Clocks, Computer science, Computerized monitoring, Concurrent computing, Contracts, Cray T3D, Cray T3E, DEC 8400, DEC Alpha based parallel systems, DEC-Alpha processor architecture, Modems, Optimizing compilers, Read-write memory, System performance, access pattern, cache storage, cache storage, clock speed, coherency, compiler, cost benefit model, empirical evaluation, global address space, local memory, local memory accesses, memory architecture, memory system performance characterization, nonuniform bandwidth, parallel processing, parallel systems, performance evaluation, program compilers, remote write, scalability, spatial locality, },
 abstract = {Many parallel systems offer a simple view of memory: all storage cells are addressed uniformly. Despite a uniform view of the memory, the machines differ significantly in their memory system performance (and may offer slightly different consistency models). Cached and local memory accesses are much faster than remote read accesses to data generated by another processor or remote write to data intentionally pushed to memories close to another processor. The bandwidth from/to cache and local memory can be an order of magnitude (or more) higher than the bandwidth to/from remote memory. The situation is further complicated by the heavy influence of the access pattern (i.e. the spatial locality of reference) on both the local and the remote memory system bandwidth. In these modern machines, a compiler for a parallel system is faced with a number of options to accomplish a data transfer most efficiently. The decision for the best option requires a cost benefit model, obtained in an empirical evaluation of the memory system performance. We evaluate three DEC Alpha based parallel systems, to demonstrate the practicality of this approach. The common DEC-Alpha processor architecture facilitates a direct comparison of memory system performance. These systems are the DEC 8400, the Cray T3D, and the Cray T3E. The three systems differ in their clock speed, their scalability and in the amount of coherency they provide },
}

@inproceedings{5416638,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Dan Tang and Yungang Bao and Weiwu Hu and Mingyu Chen},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {DMA cache: Using on-chip storage to architecturally separate I/O data from CPU data for improving I/O performance},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416638},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416638},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416638.pdf?arnumber=5416638},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Bandwidth, Buffer storage, CPU data, CPU memory reference behavior, Cache storage, Computer architecture, DMA cache designs, DMA cache technique, Delay, Emulation, FPGA-based emulation platform, Hard disks, I/O buses, I/O data, I/O performance, Laboratories, PBDC, System-on-a-chip, Throughput, cache storage, decoupled DMA cache, field programmable gate arrays, last level cache, memory access latency, on-chip storage, partition-based DMA cache, snooping-cache scheme, system buses, system-on-chip, },
 abstract = {As technology advances both in increasing bandwidth and in reducing latency for I/O buses and devices, moving I/O data in/out memory has become critical. In this paper, we have observed the different characteristics of I/O and CPU memory reference behavior, and found the potential benefits of separating I/O data from CPU data. We propose a DMA cache technique to store I/O data in dedicated on-chip storage and present two DMA cache designs. The first design, Decoupled DMA Cache (DDC), adopts additional on-chip storage as the DMA cache to buffer I/O data. The second design, Partition-Based DMA Cache (PBDC), does not require additional on-chip storage, but can dynamically use some ways of the processor's last level cache (LLC) as the DMA cache. We have implemented and evaluated the two DMA cache designs by using an FPGA-based emulation platform and the memory reference traces of real-world applications. Experimental results show that, compared with the existing snooping-cache scheme, DDC can reduce memory access latency (in bus cycles) by 34.8\% on average (up to 58.4\%), while PBDC can achieve about 80\% of DDC's performance improvements despite no additional on-chip storage. },
}

@inproceedings{569650,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Skadron, K. and Clark, D.W.},
 year = {1997},
 pages = {144--155},
 publisher = {IEEE},
 title = {Design issues and tradeoffs for write buffers},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569650},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569650},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569650.pdf?arnumber=569650},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Aggregates, Bridges, Buffer storage, Computer science, Delay, Prefetching, Process design, Read-write memory, Retirement, SPEC92 benchmarks, Writing, cache storage, design issues, digital simulation, instruction level simulation, load-hazard policies, memory architecture, memory hierarchy, memory protocols, retirement policies, write buffers, write latency, write-through caches, },
 abstract = {Processors with write-through caches typically require a write buffer to hide the write latency to the next level of memory hierarchy and to reduce write traffic. A write buffer can cause processor stalls when it is full, when it contends with a cache miss for access to the next level of the hierarchy and when it contains the freshest copy of data needed by a load. This paper uses instruction level simulation of SPEC92 benchmarks to investigate how different write buffer depths, retirement policies, and load-hazard policies affect these three types of write-buffer stalls. Deeper buffers with adequate headroom, lazier retirement policies, and the ability to read data directly from the write buffer combine to substantially reduce write-buffer-induced stalls },
}

@inproceedings{569652,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Jacob, B. and Mudge, T.},
 year = {1997},
 pages = {156--167},
 publisher = {IEEE},
 title = {Software-managed address translation},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569652},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569652},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569652.pdf?arnumber=569652},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Clocks, Computer architecture, Energy management, Hardware, Jacobian matrices, Mach, Magnetic heads, Memory management, OSF/1, Operating systems, Power system management, Protection, high clock-rate PowerPC implementation, memory architecture, memory management design, program interpreters, shared memory, software-managed address translation, sparse address spaces, storage management, sub-page protection, superpages, },
 abstract = {In this paper we explore software-managed address translation. The purpose of the study is to specify the memory management design for a high clock-rate PowerPC implementation in which a simple design is a prerequisite for a fast clock and a short design cycle. We show that software-managed address translation is just as efficient as hardware-managed address translation and it is much more flexible. Operating systems such as OSF/1 and Mach charge between 0.10 and 0.28 cycles per instruction (CPI) for address translation using dedicated memory-management hardware. Software-managed translation requires 0.05 CPI. Mechanisms to support such features as shared memory, superpages, sub-page protection, and sparse address spaces can be defined completely in software, allowing much more flexibility than in hardware-defined mechanisms },
}

@inproceedings{1385945,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Mukherjee, S.S. and Emer, J. and Reinhardt, S.K.},
 year = {2005},
 pages = { 243-- 247},
 publisher = {IEEE},
 title = {The soft error problem: an architectural perspective},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.37},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385945},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385945.pdf?arnumber=1385945},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { computer architecture,  computer architecture,  computer system design,  double-bit errors,  microprocessor chips,  radiation-induced soft errors,  soft error problem,  soft error rate,  system recovery, Additives, Alpha particles, Computer architecture, Computer errors, Error analysis, Error correction, Particle measurements, Pollution measurement, Semiconductor device measurement, Testing, },
 abstract = {Radiation-induced soft errors have emerged as a key challenge in computer system design. If the industry is to continue to provide customers with the level of reliability they expect, microprocessor architects must address this challenge directly. This effort has two parts. First, architects must understand the impact of soft errors on their designs. Second, they must select judiciously from among available techniques to reduce this impact in order to meet their reliability targets with minimum overhead. To provide a foundation for these efforts, this paper gives a broad overview of the soft error problem from an architectural perspective. We start with basic definitions, followed by a description of techniques to compute the soft error rate. Then, we summarize techniques used to reduce the soft error rate. This paper also describes problems with double-bit errors. Finally, this paper outlines future directions for architecture research in soft errors. },
}

@inproceedings{1410082,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Amit Gandhi and Akkary, H. and Srinivasan, S.T.},
 year = {2004},
 pages = { 254-- 264},
 publisher = {IEEE},
 title = {Reducing branch misprediction penalty via selective branch recovery},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10004},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410082},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410082.pdf?arnumber=1410082},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { SBR,  baseline processor,  branch misprediction penalty,  convergent instruction,  exact convergence,  instruction sets,  instruction window,  misspeculative execution,  parallel architectures,  pipeline processing,  selective branch recovery, Computer aided instruction, Convergence, Flow graphs, Microarchitecture, Pipelines, },
 abstract = {Branch misprediction penalty consists of two components: the time wasted on misspeculative execution until the mispredicted branch is resolved and the time to restart the pipeline with useful instructions once the branch is resolved. Current processor trends, large instruction windows and deep pipelines, amplify both components of the branch misprediction penalty. We propose a novel method, called selective branch recovery (SBR), to reduce both components of branch misprediction penalty. SBR exploits a frequently occurring type of control independence - exact convergence - where the mispredicted path converges exactly at the beginning of the correct path. In such cases, SBR selectively reuses the results computed during misspeculative execution and obviates the need to fetch or rename convergent instructions again. Thus, SBR addresses both components of branch misprediction penalty. To increase the likelihood of branch mispredictions that can be handled with SBR, we also present an effective means for inducing exact convergence on misspeculative paths. With SBR, we significantly improve performance (between 3\%-22\%, average 8\%) on a wide range of benchmarks over our baseline processor that does not exploit SBR. },
}

@inproceedings{4147674,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Lee, B.C. and Brooks, D.M.},
 year = {2007},
 pages = {340--351},
 publisher = {IEEE},
 title = {Illustrative Design Space Studies with Microarchitectural Regression Models},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346211},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147674},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147674.pdf?arnumber=4147674},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Computational modeling, Costs, Design optimization, Microarchitecture, Pareto analysis, Pareto frontier analysis, Pareto optima, Pareto optimisation, Performance analysis, Pipelines, Predictive models, Sampling methods, Space exploration, benchmark optimal architectures, design space evaluation, design space optimization, design space sampling, logic design, microarchitectural regression models, microprocessor chips, multiprocessor heterogeneity analysis, pipeline depth analysis, regression analysis, statistical inference, },
 abstract = {We apply a scalable approach for practical, comprehensive design space evaluation and optimization. This approach combines design space sampling and statistical inference to identify trends from a sparse simulation of the space. The computational efficiency of sampling and inference enables new capabilities in design space exploration. We illustrate these capabilities using performance and power models for three studies of a 260,000 point design space: (1) Pareto frontier analysis, (2) pipeline depth analysis, and (3) multiprocessor heterogeneity analysis. For each study, we provide an assessment of predictive error and sensitivity of observed trends to such error. We construct Pareto frontiers and find predictions for Pareto optima are no less accurate than those for the broader design space. We reproduce and enhance prior pipeline depth studies, demonstrating constrained sensitivity studies may not generalize when many other design parameters are held at constant values. Lastly, we identify efficient heterogeneous core designs by clustering per benchmark optimal architectures. Collectively, these studies motivate the application of techniques in statistical inference for more effective use of modern simulator infrastructure },
}

@inproceedings{4147675,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {353--354},
 publisher = {IEEE},
 title = {Author Index},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346212},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147675},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147675.pdf?arnumber=4147675},
 isbn = {1-4244-0805-9},
 language = {English},
 abstract = {},
}

@inproceedings{4147672,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Yuho Jin and Eim Jung Kim and Ki Hwan Yum},
 year = {2007},
 pages = {318--327},
 publisher = {IEEE},
 title = {A Domain-Specific On-Chip Network Design for Large Scale Cache Systems},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346209},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147672},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147672.pdf?arnumber=4147672},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Delay effects, Integrated circuit interconnections, Large-scale systems, Mesh networks, Network topology, Network-on-a-chip, Resource management, Scalability, System-on-a-chip, Wires, cache replacement, cache storage, circuit integration technology, deadlock-free XYX routing algorithm, domain-specific on-chip network design, fast-LRU replacement, halo network topology, large scale cache systems, logic design, multiprocessor interconnection networks, network routing, network topology, nonuniform cache architectures, on-chip L2 caches, single-cycle router architecture, system-on-chip, wormhole-routed 2D mesh networks, },
 abstract = {As circuit integration technology advances, the design of efficient interconnects has become critical. On-chip networks have been adopted to overcome scalability and the poor resource sharing problems of shared buses or dedicated wires. However, using a general on-chip network for a specific domain may cause underutilization of the network resources and huge network delays because the interconnects are not optimized for the domain. Addressing these two issues is challenging because in-depth knowledges of interconnects and the specific domain are required. Non-uniform cache architectures (NUCAs) use wormhole-routed 2D mesh networks to improve the performance of on-chip L2 caches. We observe that network resources in NUCAs are underutilized and occupy considerable chip area (52\% of cache area). Also the network delay is significantly large (63\% of cache access time). Motivated by our observations, we investigate how to optimize cache operations and and design the network in large scale cache systems. We propose a single-cycle router architecture that can efficiently support multicasting in on-chip caches. Next, we present fast-LRU replacement, where cache replacement overlaps with data request delivery. Finally we propose a deadlock-free XYX routing algorithm and a new halo network topology to minimize the number of links in the network. Simulation results show that our networked cache system improves the average IPC by 38\% over the mesh network design with multicast promotion replacement while using only 23\% of the interconnection area. Specifically, multicast fast-LRU replacement improves the average IPC by 20\% compared with multicast promotion replacement. A halo topology design additionally improves the average IPC by 18\% over a mesh topology },
}

@inproceedings{1385947,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Ranganathan, P. and Jouppi, N.},
 year = {2005},
 pages = { 253-- 256},
 publisher = {IEEE},
 title = {Enterprise IT trends and implications for architecture research},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.14},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385947},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385947.pdf?arnumber=1385947},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { IT infrastructure trends,  SLA-driven performance tuning,  architecture research,  business data processing,  computer architecture,  enterprise IT systems,  information technology,  service-centric computing, Application software, Companies, Computer architecture, Cost function, Design optimization, Energy management, Hardware, Internet, Portals, Power system management, },
 abstract = {The last decade has seen several changes in the structure and emphasis of enterprise IT systems. Specific infrastructure trends have included the emergence of large consolidated data centers, the adoption of virtualization and modularization, and an increased commoditization of hardware. At the application level, both the workload mix and usage patterns have evolved to an increased emphasis on service-centric computing and SLA-driven performance tuning. These, often dramatic, changes in the enterprise IT landscape motivate equivalent changes in the emphasis of architecture research. In this paper, we summarize some recent trends in enterprise IT systems and discuss the implications for architecture research, suggesting some high-level challenges and open questions for the community to address. },
}

@inproceedings{4147670,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Agarwal, M. and Malik, K. and Woley, K.M. and Stone, S.S. and Frank, M.I.},
 year = {2007},
 pages = {295--305},
 publisher = {IEEE},
 title = {Exploiting Postdominance for Speculative Parallelization},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346207},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147670},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147670.pdf?arnumber=4147670},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Centralized control, Computer languages, Data mining, Flow graphs, Performance analysis, Program processors, Tree graphs, Yarn, compiler, dynamic reconvergence prediction, if-else statements, immediate postdominators, loops, parallel processing, parallel tasks, postdominance-based task selection, procedure calls, program compilers, program control structures, programming language constructs, sequential thread, speculative parallelization, task analysis, },
 abstract = {Task-selection policies are critical to the performance of any architecture that uses speculation to extract parallel tasks from a sequential thread. This paper demonstrates that the immediate postdominators of conditional branches provide a larger set of parallel tasks than existing task-selection heuristics, which are limited to programming language constructs (such as loops or procedure calls). Our evaluation shows that postdominance-based task selection achieves, on average, more than double the speedup of the best individual heuristic, and 33\% more speedup than the best combination of heuristics. The specific contributions of this paper include, first, a description of task selection based on immediate post-dominance for a system that speculatively creates tasks. Second, our experimental evaluation demonstrates that existing task-selection heuristics based on loops, procedure calls, and if-else statements are all subsumed by compiler-generated immediate postdominators. Finally, by demonstrating that dynamic reconvergence prediction closely approximates immediate postdominator analysis, we show that the notion of immediate postdominators may also be useful in constructing dynamic task selection mechanisms },
}

@inproceedings{4147671,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Willmann, P. and Shafer, J. and Carr, D. and Menon, A. and Rixner, S. and Cox, A.L. and Zwaenepoel, W.},
 year = {2007},
 pages = {306--317},
 publisher = {IEEE},
 title = {Concurrent Direct Network Access for Virtual Machine Monitors},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346208},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147671},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147671.pdf?arnumber=4147671},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Computer architecture, Hardware, Linux, Network interfaces, Operating systems, Protection, Software performance, Telecommunication traffic, Virtual machine monitors, Virtual machining, concurrent direct network access, interrupt delivery, memory protection, network interfaces, operating systems, operating systems (computers), physical network interface, software multiplexing, software-virtualized network interface, traffic multiplexing, virtual machine monitor, virtual machines, },
 abstract = {This paper presents hardware and software mechanisms to enable concurrent direct network access (CDNA) by operating systems running within a virtual machine monitor. In a conventional virtual machine monitor, each operating system running within a virtual machine must access the network through a software-virtualized network interface. These virtual network interfaces are multiplexed in software onto a physical network interface, incurring significant performance overheads. The CDNA architecture improves networking efficiency and performance by dividing the tasks of traffic multiplexing, interrupt delivery, and memory protection between hardware and software in a novel way. The virtual machine monitor delivers interrupts and provides protection between virtual machines, while the network interface performs multiplexing of the network data. In effect, the CDNA architecture provides the abstraction that each virtual machine is connected directly to its own network interface. Through the use of CDNA, many of the bottlenecks imposed by software multiplexing can be eliminated without sacrificing protection, producing substantial efficiency improvements },
}

@inproceedings{1385946,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Spracklen, L. and Abraham, S.G.},
 year = {2005},
 pages = { 248-- 252},
 publisher = {IEEE},
 title = {Chip multithreading: opportunities and challenges},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.10},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385946},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385946.pdf?arnumber=1385946},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { CMP design,  CMT chips,  CMT design,  CMT processors,  SMT design,  chip multi-threaded processors,  chip multiprocessing,  chip multithreading,  microprocessor chips,  multi-threading,  multiprocessing systems,  parallel architectures,  simultaneous multithreading,  thread-level parallelism, Bandwidth, Delay, Hardware, Multithreading, Out of order, Parallel processing, Pipelines, Process design, Surface-mount technology, Yarn, },
 abstract = {Chip multi-threaded (CMT) processors provide support for many simultaneous hardware threads of execution in various ways, including simultaneous multithreading (SMT) and chip multiprocessing (CMP). CMT processors are especially suited to server workloads, which generally have high levels of thread-level parallelism (TLP). In this paper, we describe the evolution of CMT chips in industry and highlight the pervasiveness of CMT designs in upcoming general-purpose processors. The CMT design space accommodates a range of designs between the extremes represented by the SMT and CMP designs and a variety of attractive design options are currently unexplored. Though there has been extensive research on utilizing multiple hardware threads to speed up single-threaded applications via speculative parallelization, there are many challenges in designing CMT processors, even when sufficient TLP is present. This paper describes some of these challenges including, hot sets, hot banks, speculative prefetching strategies, request prioritization and off-chip bandwidth reduction. },
}

@inproceedings{4658619,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {},
 year = {2008},
 pages = {i--iii},
 publisher = {IEEE},
 title = {Sessions},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658619},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658619},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658619.pdf?arnumber=4658619},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {DRAM, DRAM chips, SMT, branch prediction, cache storage, code analysis, codes, microarchitecture analysis, microarchitecture modeling, network-on-chip, network-on-chip, optimisation, optimisation, parallel architectures, parallel architectures, path prediction, power management, reliability, security, security, thermal management, validation, },
 abstract = {The following topics were dealt with: path and branch prediction; power and thermal management; SMT; security; network-on-chip; microarchitecture modeling and analysis; code analysis and optimisation; DRAM; parallel architectures; reliability and validation. },
}

@inproceedings{1385940,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Jaleel, J. and Jacob, B.},
 year = {2005},
 pages = { 191-- 200},
 publisher = {IEEE},
 title = {Using virtual load/store queues (VLSQs) to reduce the negative effects of reordered memory instructions},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.42},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385940},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385940.pdf?arnumber=1385940},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { aggressive prefetching,  cache behavior,  cache storage,  computer power supplies,  data cache access,  instruction sets,  instruction windows,  issue logic,  memory architecture,  memory consistency model,  memory ordering model,  out-of-order aggressiveness,  processor performance,  queueing theory,  reorder buffer sizes,  reordered memory instructions,  select logic,  virtual load/store queues,  virtual storage, Buffer storage, Computer aided instruction, Computer architecture, Educational institutions, Frequency, Jacobian matrices, Logic, Out of order, Pipelines, Waste handling, },
 abstract = {The use of large instruction windows coupled with aggressive out-of-order and prefetching capabilities has provided significant improvements in processor performance. In this paper, we quantify the effects of increased out-of-order aggressiveness on a processor's memory ordering/consistency model as well as an application's cache behavior. We observe that increasing reorder buffer sizes cause less than one third of issued memory instructions to be executed in actual program order. We show that increasing the reorder buffer size from 80 to 512 entries results in an increase in the frequency of memory traps by a factor of six and an increase in total execution overhead by 10-40\%. Additionally, we observe that the reordering of memory instructions increases the L1 data cache accesses by 10-60\% and the L1 data cache misses by 10-20\%. These findings reveal that increased out-of-order capability can waste energy in two ways. First, re-fetching and re-executing instructions flushed due to traps require the fetch, map, and execution units to dissipate energy on work that has already been done before. Second, an increase in the number of cache accesses and cache misses needlessly dissipates energy. Both these side effects can be related to the reordering of memory instructions. Thus, to avoid wasting both energy and performance, we propose a virtual load/store queue (VLSQ) within the existing physical load/store queue. The VLSQ reduces the reordering of memory instructions by limiting the number of memory instructions visible to the select and issue logic. We show that VLSQs can reduce trap overhead, cache accesses, and cache misses by as much as 45\%, 50\%, and 15\% respectively when compared to traditional load/store queues. We observe that these reductions yield net power savings of 10-50\% with degradation in performance by 1-5\%. },
}

@inproceedings{824338,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Wong, W.A. and Baer, J.-L.},
 year = {2000},
 pages = {49--60},
 publisher = {IEEE},
 title = {Modified LRU policies for improving second-level cache behavior},
 date = {2000},
 doi = {10.1109/HPCA.2000.824338},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824338},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824338.pdf?arnumber=824338},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Delay, Hardware, Optimized production technology, Radio access networks, associativity, cache storage, content-addressable storage, effective replacement algorithms, modified LRU policies, performance evaluation, profile-based scheme, replacement algorithm, second-level cache behavior, second-level caches, temporal locality, },
 abstract = {Main memory accesses continue to be a significant bottleneck for applications whose working sets do not fit in second-level caches. With the trend of greater associativity in second-level caches, implementing effective replacement algorithms might become more important than reducing conflict misses. After showing that an opportunity exists to close part of the gap between the OPT and the LRU algorithms, we present a replacement algorithm based on the detection of temporal locality in lines residing in the L2 cache. Rather than always replacing the LRU line, the victim is chosen by considering both its priority in the LRU stack and whether it exhibits temporal locality or not. We consider two strategies which use this replacement algorithm: a profile-based scheme where temporal locality is detected by processing a trace from a training set of the application and an on-line scheme, where temporal locality is detected with the assistance of a small locality table. Both schemes improve on the second-level cache miss rate over a pure LRU algorithm, by as much as 12\% in the profiling case and 20\% in the dynamic case },
}

@inproceedings{824339,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Jourdan, S. and Rappoport, L. and Almog, Y. and Erez, M. and Yoaz, A. and Ronen, R.},
 year = {2000},
 pages = {61--70},
 publisher = {IEEE},
 title = {eXtended block cache},
 date = {2000},
 doi = {10.1109/HPCA.2000.824339},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824339},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824339.pdf?arnumber=824339},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Bandwidth, Decoding, Degradation, Delay, Identity-based encryption, Information retrieval, Iron, Out of order, Radio access networks, Steady-state, XBC, cache storage, eXtended block cache, instruction sets, instruction-supply mechanism, multiple-entry single-exit instruction block, nearly redundant free structure, redundancy, redundancy, trace cache hit rate, },
 abstract = {This paper describes a new instruction-supply mechanism, called the eXtended Block Cache (XBC). The goal of the XBC is to improve on the Trace Cache (TC) hit rate, while providing the same bandwidth. The improved hit rate is achieved by having the XBC a nearly redundant free structure. The basic unit recorded in the XBC is the extended block (XB), which is a multiple-entry single-exit instruction block. A XB is a sequence of instructions ending on a conditional or an indirect branch. Unconditional direct jumps do not end a XB. In order to enable multiple entry points per XB, the XB index is derived from the IP of its ending instruction. Instructions within the XB are recorded in reverse order, enabling easy extension of XBs. The multiple entry-points remove most of the redundancy. Since there is at most one conditional branch per XB, we can fetch up to n XBs per cycle by predicting n branches. The multiple fetch enables the XBC to march the TC bandwidth },
}

@inproceedings{1385941,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Hallnor, E.G. and Reinhardt, S.K.},
 year = {2005},
 pages = { 201-- 212},
 publisher = {IEEE},
 title = {A unified compressed memory hierarchy},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.4},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385941},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385941.pdf?arnumber=1385941},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { 1 GByte,  128 byte,  512 bytes,  SPEC CPU2000 benchmarks,  cache storage,  compressed memory hierarchy,  data compression,  last-level on-chip cache,  memory architecture,  off-chip bandwidth,  off-chip main memory,  off-chip memory channel,  system performance,  unified compression scheme, Bandwidth, Computer architecture, Costs, Degradation, Delay, Economics, Frequency, Laboratories, Random access memory, System performance, },
 abstract = {The memory system's large and growing contribution to system performance motivates more aggressive approaches to improving its efficiency. We propose and analyze a memory hierarchy that uses a unified compression scheme encompassing the last-level on-chip cache, the off-chip memory channel, and off-chip main memory. This scheme simultaneously increases the effective on-chip cache capacity, off-chip bandwidth, and main memory size, while avoiding compression and decompression overheads between levels. Simulations of the SPEC CPU2000 benchmarks using a 1MB cache and 128-byte blocks show an average speedup of 19\%, while degrading performance by no more than 5\%. The combined scheme achieves a peak improvement of 292\%, compared to 165\% and 83\% for cache or bus compression alone. The compressed system generally provides even better performance as the block size is increased to 512 bytes. },
}

@inproceedings{824333,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {},
 year = {2000},
 publisher = {IEEE},
 title = {Proceedings Sixth International Symposium on High-Performance Computer Architecture. HPCA-6 (Cat. No.PR00550)},
 date = {12-12 Jan. 2000},
 doi = {10.1109/HPCA.2000.824333},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824333},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824333.pdf?arnumber=824333},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {cache, computer architecture, high-performance computer architecture, memory, microarchitecture, multithreading, networks, novel architecture issues, parallel systems, performance, prediction, shared memory, software techniques, system architecture tradeoffs, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00824333.png" border="0"> },
}

@inproceedings{1410084,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Chen, C.F. and Yang, S.-H. and Falsafi, B. and Moshovos, A.},
 year = {2004},
 pages = { 276-- 287},
 publisher = {IEEE},
 title = {Accurate and complexity-effective spatial pattern prediction},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10010},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410084},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410084.pdf?arnumber=1410084},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { cache line,  cache storage,  cache's spatial usage,  data cache,  hardware mechanism,  instruction address,  pattern recognition,  predictor memory,  reference pattern,  set-associative Ll data cache,  spatial data structures,  spatial group,  spatial pattern predictor, Bandwidth, CMOS technology, Computer architecture, Degradation, Delay, Design optimization, Energy dissipation, Laboratories, Power dissipation, Prefetching, },
 abstract = {Recent research suggests that there are large variations in a cache's spatial usage, both within and across programs. Unfortunately, conventional caches typically employ fixed cache line sizes to balance the exploitation of spatial and temporal locality, and to avoid prohibitive cache fill bandwidth demands. The resulting inability of conventional caches to exploit spatial variations leads to suboptimal performance and unnecessary cache power dissipation. We describe the spatial pattern predictor (SPP), a cost-effective hardware mechanism that accurately predicts reference patterns within a spatial group (i.e., a contiguous region of data in memory) at runtime. The key observation enabling an accurate, yet low-cost, SPP design is that spatial patterns correlate well with instruction addresses and data reference offsets within a cache line. We require only a small amount of predictor memory to store the predicted patterns. Simulation results for a 64-Kbyte 2-way set-associative Ll data cache with 64-byte lines show that: (1) a 256-entry tag-less direct-mapped SPP can achieve, on average, a prediction coverage of 95\%, over-predicting the patterns by only 8\%, (2) assuming a 70 nm process technology, the SPP helps reduce leakage energy in the base cache by 41\% on average, incurring less than 1\% performance degradation, and (3) prefetching spatial groups of up to 512 bytes using SPP improves execution time by 33\% on average and up to a factor of two. },
}

@inproceedings{824336,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Figueiredo, R.J.O. and Fortes, J.A.B.},
 year = {2000},
 pages = {26--35},
 publisher = {IEEE},
 title = {Impact of heterogeneity on DSM performance},
 date = {2000},
 doi = {10.1109/HPCA.2000.824336},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824336},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824336.pdf?arnumber=824336},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Analytical models, Concurrent computing, Costs, Delay, Distributed computing, Parallel processing, Performance analysis, SPLASH-2 benchmark suite, Sensitivity analysis, Silicon, Yarn, area/parallelism tradeoffs, cache sizes, distributed shared memory performance, distributed shared memory systems, heterogeneity, heterogeneous organization, homogeneous multiprocessor programs, memory latency, multi-thread parallelism, network latency, performance evaluation, quantitative performance, sensitivity analysis, sensitivity analysis, sequential codes, sequential codes, simulation-based analysis, single-chip computing nodes, static thread assignment, },
 abstract = {This paper explores area/parallelism tradeoffs in the design of distributed shared-memory (DSM) multiprocessors built out of large single-chip computing nodes. In this context, area-efficiency arguments motivate a heterogeneous organization consisting of few nodes with large caches designed for single-thread parallelism, and a larger number of nodes with smaller caches designed for multi-thread parallelism. Quantitative performance of such organization is reported for a set of homogeneous multiprocessor programs from the SPLASH-2 benchmark suite. These programs are mapped onto the heterogeneous processors without source code modifications via static thread assignment policies. Simulation-based analysis is used to compare the performance of heterogeneous and homogeneous DSMs that occupy the same silicon area. The analysis shows that a 4-node heterogeneous DSM with 21 processors outperforms its homogeneous counterpart with 4 processors by an average age of 36\% for the studied multiprocessor workload, while having the same performance for sequential codes. A sensitivity analysis based on a factorial design experiment is used to study the implications of processor, memory, and network heterogeneity on overall cost and performance of a heterogeneous DSM. The studied benchmarks are affected, on average, primarily by heterogeneity in processor performance (59.3\%), followed by cache sizes (18.2\%), memory latency (14.6\%), and network latency (5.6\%) },
}

@inproceedings{824337,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Mathew, B.K. and McKee, S.A. and Carter, J.B. and Davis, A.},
 year = {2000},
 pages = {39--48},
 publisher = {IEEE},
 title = {Design of a parallel vector access unit for SDRAM memory systems },
 date = {2000},
 doi = {10.1109/HPCA.2000.824337},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824337},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824337.pdf?arnumber=824337},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Cities and towns, Computer science, DRAM chips, Electrical capacitance tomography, Identity-based encryption, Microprocessors, Military computing, Personal communication networks, SDRAM, SDRAM memory systems, Supercomputers, US Government, computational complexity, data structures, data structures, digital simulation, formal analysis, functional simulation, gate-level simulation, hardware complexity, memory bandwidth, memory bottleneck, memory controller, parallel vector access unit, performance evaluation, vector memory subsystem, },
 abstract = {We are attacking the memory bottleneck by building a ``smart" memory controller that improves effective memory bandwidth, bus utilization, and cache efficiency by letting applications dictate how their data is accessed and cached. This paper describes a parallel vector access unit (PVA), the vector memory subsystem that efficiently ``gathers" sparse, strided data structures in parallel on a multi-bank SDRAM memory. We have validated our PVA design via gate-level simulation, and have evaluated its performance via functional simulation and formal analysis. On unit-stride vectors, PVA performance equals or exceeds that of an SDRAM system optimized for cache line fills. On vectors with larger strides, the PVA is up to 32.8 times faster. Our design is up to 3.3 times faster than a pipelined, serial SDRAM memory system that gathers sparse vector data, and the gathering mechanism is two to five times faster than in other PVAs with similar goals. Our PVA only slightly increases hardware complexity with respect to these other systems, and the scalable design is appropriate for a range of computing platforms, from vector supercomputers to commodity PCs },
}

@inproceedings{824334,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Barroso, L.A. and Gharachorloo, K. and Nowatzyk, A. and Verghese, B.},
 year = {2000},
 pages = {3--14},
 publisher = {IEEE},
 title = {Impact of chip-level integration on performance of OLTP workloads },
 date = {2000},
 doi = {10.1109/HPCA.2000.824334},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824334},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824334.pdf?arnumber=824334},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Bandwidth, Databases, Delay, Hardware, Microprocessors, Network servers, Network-on-a-chip, Next generation networking, OLTP workloads, Oracle commercial database engine, Out of order, Process design, architectural choices, chip-level integration, coherence hardware, data mining, high-performance servers, memory controller, microprocessor designs, multiprocessing systems, multiprocessor configurations, network router, online transaction processing, performance evaluation, system functionality, system simulations, system-level modules, uniprocessor, },
 abstract = {With increasing chip densities, future microprocessor designs have the opportunity to integrate many of the traditional system-level modules onto the same chip as the processor. Some current designs already integrate extremely large on-chip caches, and there are aggressive next-generation designs that attempt to also integrate the memory controller, coherence hardware, and network router all onto a single chip. The tight coupling of these modules will enable efficient memory systems with substantially better latency and bandwidth characteristics relative to current designs. Among the important application areas for high-performance servers, online transaction processing (OLTP) workloads are likely to benefit most from these trends due to their large instruction and data footprints and high communication miss rates. This paper examines the design trade-offs that arise as more system functionality is integrated onto the processor chip, and identifies a number of important architectural choices that are influenced by chip-level integration. In addition, the paper presents a detailed study of the performance impact of chip-level integration in the context of OLTP workloads. Our results are based on full system simulations of the Oracle commercial database engine running on both in-order and out-of-order issue processors used in uniprocessor and multiprocessor configurations. The results show that chip-level integration can improve the performance of both configurations by about 1.4 to 1.5 times, though for different reasons. For uniprocessors, integration of the L2 cache and the resulting lower hit latency is the primary factor in performance improvement. For multiprocessors, the improvement comes from both the integration of the L2 cache (lower L2 hit latency) and the integration of the other memory system components (better dirty remote latency). Furthermore, we find that the higher associativity afforded by integrating the L2 cache plays a critical role in counteracting the loss of capacity relative to larger off-chip caches. Finally, we find that the relative gains from chip-level integration are virtually identical for in-order and out-of-order processors },
}

@inproceedings{824335,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Torrellas, J. and Liuxi Yang and Nguyen, A.-T.},
 year = {2000},
 pages = {15--25},
 publisher = {IEEE},
 title = {Toward a cost-effective DSM organization that exploits processor-memory integration},
 date = {2000},
 doi = {10.1109/HPCA.2000.824335},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824335},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824335.pdf?arnumber=824335},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {CC-NUMA organizations, COMA, Computer architecture, Hardware, Microprocessors, Organizing, Protocols, Tagging, VLSI chip, Very large scale integration, cache coherence protocol, cache-coherent DSM machine, commodity microprocessors, distributed shared memory organization, distributed shared memory systems, machine hardware, multiprocessors, off-the-shelf hardware, performance evaluation, processor-memory integration, protocols, reconfigurability, reconfigurable architectures, },
 abstract = {Dramatic increases in the number of transistors that can be integrated on a VLSI chip will soon allow commodity microprocessors to include both processor and a sizable fraction of main memory on chip. Distributed Shared-Memory (DSM) multiprocessors typically use the latest off-the-shelf microprocessors and thus will be affected by the upcoming processor-memory integration. In this paper, we explore how a cache-coherent DSM machine built around Processor-In-Memory (PIM) chips might be cost-effectively organized. To take advantage of the close coupling between processor and memory, we propose tagging the memory and organizing it as a cache. Furthermore, commercial considerations dictate the use of off-the-shelf hardware largely designed for uniprocessors. Consequently, we keep the directory control off-chip. To keep the multiprocessor cheap and simple, and to allow for reconfigurability, directory control is performed by chips that are identical to the ones used as compute nodes. As a result, the machine hardware can be easily reconfigured for computing or coherence-handling depending on the needs of the application. We also propose a cache coherence protocol that is tailored to our architecture: it uses the memory very efficiently while exploiting the large caching space available. Overall, the resulting machine is simple and inexpensive, and delivers performance that is comparable to, and higher than, the more expensive traditional COMA and CC-NUMA organizations, respectively },
}

@inproceedings{4147655,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Ceze, L. and Montesinos, P. and von Praun, C. and Torrellas, J.},
 year = {2007},
 pages = {133--144},
 publisher = {IEEE},
 title = {Colorama: Architectural Support for Data-Centric Synchronization},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346192},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147655},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147655.pdf?arnumber=4147655},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Colorama, Data structures, Distributed control, Hardware, Joining processes, Parallel programming, Programming profession, Proposals, Technological innovation, Writing, Yarn, architectural support, constraint handling, data-centric synchronization, local reasoning, parallel architectures, parallel programming, parallel programming, programming complexity, synchronization constraints, ubiquitous computing, ubiquitous multicore architectures, },
 abstract = {With the advent of ubiquitous multi-core architectures, a major challenge is to simplify parallel programming. One way to tame one of the main sources of programming complexity, namely synchronization, is transactional memory (TM). However, we argue that TM does not go far enough, since the programmer still needs nonlocal reasoning to decide where to place transactions in the code. A significant improvement to the art is data-centric synchronization (DCS), where the programmer uses local reasoning to assign synchronization constraints to data. Based on these, the system automatically infers critical sections and inserts synchronization operations. This paper proposes novel architectural support to make DCS feasible, and describes its programming model and interface. The proposal, called Colorama, needs only modest hardware extensions, supports general-purpose, pointer-based languages such as C/C++ and, in our opinion, can substantially simplify the task of writing new parallel programs },
}

@inproceedings{995720,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {Cascaval, C. and Castanos, J.G. and Ceze, L. and Denneau, M. and Gupta, M. and Lieber, D. and Moreira, J.E. and Strauss, K. and Warren, H.S., Jr.},
 year = {2002},
 pages = { 311-- 321},
 publisher = {IEEE},
 title = {Evaluation of a multithreaded architecture for cellular computing},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995720},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995720},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995720.pdf?arnumber=995720},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 keywords = { 40 GB/s,  80 GB/s,  Cyclops,  SGI Origin 3800,  STREAM benchmark,  Splash-2 FFT kernel,  cellular computing,  embedded memory,  fast Fourier transforms,  fast barrier hardware,  flexible cache organization,  functional unit latency tolerance,  high-performance parallel computers,  high-performance systems,  in-cache bandwidth,  integrated communications hardware,  intra-chip parallelism,  memory hierarchy,  memory latency tolerance,  multi-threading,  multiple execution threads,  multiple memory banks,  multithreaded architecture,  performance evaluation,  performance evaluation,  processing elements,  single-chip symmetric multiprocessor system,  sustainable memory bandwidth,  systolic arrays, Bandwidth, Computer architecture, Concurrent computing, Delay, Hardware, High performance computing, Logic, Parallel processing, Silicon, Yarn, },
 abstract = {Cyclops is a new architecture for high-performance parallel computers that is being developed at the IBM T. J. Watson Research Center. The basic cell of this architecture is a single-chip SMP (symmetric multiprocessor) system with multiple threads of execution, embedded memory and integrated communications hardware. Massive intra-chip parallelism is used to tolerate memory and functional unit latencies. Large systems with thousands of chips can be built by replicating this basic cell in a regular pattern. In this paper, we describe the Cyclops architecture and evaluate two of its new hardware features: a memory hierarchy with a flexible cache organization and fast barrier hardware. Our experiments with the STREAM benchmark show that a particular design can achieve a sustainable memory bandwidth of 40 GB/s, equal to the peak hardware bandwidth and similar to the performance of a 128-processor SGI Origin 3800. For small vectors, we have observed in-cache bandwidth above 80 GB/s. We also show that the fast barrier hardware can improve the performance of the Splash-2 FFT kernel by up to 10\%. Our results demonstrate that the Cyclops approach of integrating a large number of simple processing elements and multiple memory banks in the same chip is an effective alternative for designing high-performance systems. },
}

@inproceedings{995721,
 booktitle = {High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on},
 author = {},
 year = {2002},
 pages = { 293-- 293},
 publisher = {IEEE},
 title = {Author Index},
 date = {2-6 Feb. 2002},
 doi = {10.1109/HPCA.2002.995721},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=995721},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7816/21483/00995721.pdf?arnumber=995721},
 issn = {1530-0897  },
 isbn = {0-7695-1525-8},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00995721.png" border="0"> },
}

@inproceedings{4798235,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {43--44},
 publisher = {IEEE},
 title = {Session 2A Multicore cache architectures},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798235},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798235},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798235.pdf?arnumber=4798235},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Multicore processing, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798235.png" border="0"> },
}

@inproceedings{501171,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Adve, S.V. and Cox, A.L. and Dwarkadas, S. and Rajamony, R. and Zwaenepoel, W.},
 year = {1996},
 pages = {26--37},
 publisher = {IEEE},
 title = {A comparison of entry consistency and lazy release consistency implementations},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501171},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501171},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501171.pdf?arnumber=501171},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {3D-FFT, Application software, Barnes-Hut, Computer science, Distributed computing, IS, Instruments, Memory management, Programming profession, Propagation delay, Quicksort, Random access memory, SOR, Water, distributed memory systems, entry consistency, lazy release consistency, lock rebinding, object granularity, performance, relaxed memory models, shared memory systems, software distributed shared memory systems, synchronisation, synchronization, synchronization object, virtual storage, },
 abstract = {This paper compares several implementations of entry consistency (EC) and lazy release consistency (LRC), two relaxed memory models in use with software distributed shared memory (DSM) systems. We use six applications in our study: SOR, Quicksort, Water, Barnes-Hut, IS, and 3D-FFT. For these applications, EC's requirement that all shared data be associated with a synchronization object leads to a fair amount of additional programming effort. We identify, in particular, extra synchronization, lock rebinding, and object granularity as sources of extra complexity. In terms of performance, for the set of applications and for the computing environment utilized neither model is consistently better than the other. For SOR and IS, execution times are about the same, but LRC is faster for Water (33\%) and Barnes-Hut (41\%) and EC is faster for Quicksort (14\%) and 3D-FFT (10\%) },
}

@inproceedings{4147653,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Ganesh, B. and Jaleel, A. and Wang, D. and Jacob, B.},
 year = {2007},
 pages = {109--120},
 publisher = {IEEE},
 title = {Fully-Buffered DIMM Memory Architectures: Understanding Mechanisms, Overheads and Scaling},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346190},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147653},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147653.pdf?arnumber=4147653},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Bandwidth, Costs, DDRx based memory controller, DDRx system, Degradation, Delay, Memory architecture, Memory management, Performance gain, Proposals, Random access memory, System performance, bandwidth allocation, bandwidth requirement, buffer management, buffer storage, fully-buffered DIMM memory architecture, memory architecture, memory bus, performance evaluation, scheduling, split-bus architecture, system buses, system performance, },
 abstract = {Performance gains in memory have traditionally been obtained by increasing memory bus widths and speeds. The diminishing returns of such techniques have led to the proposal of an alternate architecture, the fully-buffered DIMM. This new standard replaces the conventional memory bus with a narrow, high-speed interface between the memory controller and the DIMMs. This paper examines how traditional DDRx based memory controller policies for scheduling and row buffer management perform on a fully-buffered DIMM memory architecture. The split-bus architecture used by FBDIMM systems results in an average improvement of 7\% in latency and 10\% in bandwidth at higher utilizations. On the other hand, at lower utilizations, the increased cost of serialization resulted in a degradation in latency and bandwidth of 25\% and 10\% respectively. The split-bus architecture also makes the system performance sensitive to the ratio of read and write traffic in the workload. In larger configurations, we found that the FBDIMM system performance was more sensitive to usage of the FBDIMM links than to DRAM bank availability. In general, FBDIMM performance is similar to that of DDRx systems, and provides better performance characteristics at higher utilization, making it a relatively inexpensive mechanism for scaling capacity at higher bandwidth requirements. The mechanism is also largely insensitive to scheduling policies, provided certain ground rules are obeyed },
}

@inproceedings{4798241,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Xin Fu and Tao Li and Fortes, J.A.B.},
 year = {2009},
 pages = {93--104},
 publisher = {IEEE},
 title = {Soft error vulnerability aware process variation mitigation},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798241},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798241},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798241.pdf?arnumber=4798241},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {CMOS technology, Circuits, Delay, Fluctuations, Frequency estimation, Microarchitecture, Microprocessors, Registers, Robustness, Voltage, coarse granularity, high performance microprocessors, integrated circuit design, integrated circuit reliability, logic design, microarchitecture soft error reliability, microarchitecture soft error robustness, microprocessor chips, soft error vulnerability aware process variation mitigation, subthreshold voltage, transistor gate length, },
 abstract = {As transistor process technology approaches the nanometer scale, process variation significantly affects the design and optimization of high performance microprocessors. Prior studies have shown that chip operating frequency and leakage power can have large variations due to fluctuations in transistor gate length and sub-threshold voltage. In this work, we study the impact of process variation on microarchitecture soft error robustness, an increasing reliability design challenge in the billion-transistor chip era. We explore two techniques that can effectively mitigate the effect of design parameter variation while significantly enhancing microarchitecture soft error reliability. Our first technique is entry-based. It tolerates the deleterious impact of variable latency techniques on soft error reliability by reducing the quantity and residency cycle of vulnerable bits in the microarchitecture structure at a fine granularity. Our second technique is structure-based. It applies body biasing schemes to dynamically adapt transistor sub-threshold voltage (and hence device-level soft error robustness) to the program reliability characteristics at a coarse granularity. We also combine the two techniques which further produces improved results. Compared to existing process variation tolerant schemes, our proposed techniques achieve optimal trade-offs between reliability, performance, and power. To our knowledge, this paper presents the first study on characterizing and optimizing processor microarchitecture resilience to soft errors in light of process variation. },
}

@inproceedings{4798240,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {91--92},
 publisher = {IEEE},
 title = {Session 2B Reliability},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798240},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798240},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798240.pdf?arnumber=4798240},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798240.png" border="0"> },
}

@inproceedings{4798243,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Sridharan, V. and Kaeli, D.R.},
 year = {2009},
 pages = {117--128},
 publisher = {IEEE},
 title = {Eliminating microarchitectural dependency from Architectural Vulnerability},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798243},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798243},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798243.pdf?arnumber=4798243},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Failure analysis, Hardware, Microarchitecture, Microprocessors, Optimizing compilers, Personal communication networks, Program processors, Software design, Software measurement, Timing, architectural vulnerability factor, architecture-level masking, compiler optimizations, fault probability, hardware structure, microarchitectural dependency elimination, microarchitectural-architectural fault masking effects, program vulnerability factor metrics, software architecture, software designers, software metrics, software reliability, software-dependent methods, system recovery, },
 abstract = {The architectural vulnerability factor (AVF) of a hardware structure is the probability that a fault in the structure will affect the output of a program. AVF captures both microarchitectural and architectural fault masking effects; therefore, AVF measurements cannot generate insight into the vulnerability of software independent of hardware. To evaluate the behavior of software in the presence of hardware faults, we must isolate the software-dependent (architecture-level masking) portion of AVF from the hardware-dependent (microarchitecture-level masking) portion, providing a quantitative basis to make reliability decisions about software independent of hardware. In this work, we demonstrate that the new program vulnerability factor (PVF) metric provides such a basis: PVF captures the architecture-level fault masking inherent in a program, allowing software designers to make quantitative statements about a program's tolerance to soft errors. PVF can also explain the AVF behavior of a program when executed on hardware; PVF captures the workload-driven changes in AVF for all structures. Finally, we demonstrate two practical uses for PVF: choosing algorithms and compiler optimizations to reduce a program's failure rate. },
}

@inproceedings{4798242,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Man-Lap Li and Ramachandran, P. and Karpuzcu, U.R. and Hari, S. and Adve, S.V.},
 year = {2009},
 pages = {105--116},
 publisher = {IEEE},
 title = {Accurate microarchitecture-level fault modeling for studying hardware faults},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798242},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798242},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798242.pdf?arnumber=4798242},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Analytical models, Circuit faults, Electrical fault detection, Fault detection, Fault tolerant systems, Hardware, Latches, Logic arrays, Microarchitecture, Redundancy, SWAT-Sim, fault injection infrastructure, fault simulation, fault tolerant computing, gate-level fault model, hardware faults, hardware reliability, hierarchical simulation, microarchitectural simulations, microarchitecture-level fault modeling, performance overhead, permanent gate-level faults, probabilistic microarchitecture-level fault model, system-level manifestation, transient gate-level faults, },
 abstract = {Decreasing hardware reliability is expected to impede the exploitation of increasing integration projected by Moore's Law. There is much ongoing research on efficient fault tolerance mechanisms across all levels of the system stack, from the device level to the system level. High-level fault tolerance solutions, such as at the microarchitecture and system levels, are commonly evaluated using statistical fault injections with microarchitecture-level fault models. Since hardware faults actually manifest at a much lower level, it is unclear if such high level fault models are acceptably accurate. On the other hand, lower level models, such as at the gate level, may be more accurate, but their increased simulation times make it hard to track the system-level propagation of faults. Thus, an evaluation of high-level reliability solutions entails the classical tradeoff between speed and accuracy. This paper seeks to quantify and alleviate this tradeoff. We make the following contributions: (1) We introduce SWAT-Sim, a novel fault injection infrastructure that uses hierarchical simulation to study the system-level manifestations of permanent (and transient) gate-level faults. For our experiments, SWAT-Sim incurs a small average performance overhead of under 3x, for the components we simulate, when compared to pure microarchitectural simulations. (2) We study system-level manifestations of faults injected under different microarchitecture-level and gate-level fault models and identify the reasons for the inability of microarchitecture-level faults to model gate-level faults in general. (3) Based on our analysis, we derive two probabilistic microarchitecture-level fault models to mimic gate-level stuck-at and delay faults. Our results show that these models are, in general, inaccurate as they do not capture the complex manifestation of gate-level faults. The inaccuracies in existing models and the lack of more accurate microarchitecture-level models motivate using infrastruc- - tures similar to SWAT-Sim to faithfully model the microarchitecture-level effects of gate-level faults. },
}

@inproceedings{4798245,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {141--142},
 publisher = {IEEE},
 title = {Panel (joint with PPoPP)},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798245},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798245},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798245.pdf?arnumber=4798245},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798245.png" border="0"> },
}

@inproceedings{4798244,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Lide Duan and Bin Li and Lu Peng},
 year = {2009},
 pages = {129--140},
 publisher = {IEEE},
 title = {Versatile prediction and fast estimation of Architectural Vulnerability Factor from processor performance metrics},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798244},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798244},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798244.pdf?arnumber=4798244},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Clocks, Frequency estimation, Hardware, Logic devices, Measurement, Predictive models, Regression tree analysis, State estimation, System performance, Testing, architectural vulnerability factor, boosted regression tree, microprocessor chips, nonparametric tree-based predictive modeling, patient rule induction method, processor performance metrics, regression analysis, transient fault, trees (mathematics), },
 abstract = {The shrinking processor feature size, lower threshold voltage and increasing clock frequency make modern processors highly vulnerable to transient faults. Architectural vulnerability factor (AVF) reflects the possibility that a transient fault eventually causes a visible error in the program output, and it indicates a system's susceptibility to transient faults. Therefore, the awareness of the AVF especially at early design stage is greatly helpful to achieve a trade-off between system performance and reliability. However, tracking the AVF during program execution is extremely costly, which makes accurate AVF prediction extraordinarily attractive to computer architects. In this paper, we propose to use boosted regression trees, a nonparametric tree-based predictive modeling scheme, to identify the correlation across workloads, execution phases and processor configurations between a key processor structure's AVF and various performance metrics. The proposed method not only makes an accurate prediction but quantitatively illustrates individual performance variable's importance to the AVF. Moreover, to reduce the prediction complexity, we also utilize a technique named patient rule induction method to extract some simple selecting rules on important metrics. Applying these rules during run time can fast identify execution intervals with a relatively high AVF. },
}

@inproceedings{4798247,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {145--146},
 publisher = {IEEE},
 title = {Keynote II (joint with PPoPP)},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798247},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798247},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798247.pdf?arnumber=4798247},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798247.png" border="0"> },
}

@inproceedings{4798246,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Hill, M.D.},
 year = {2009},
 pages = {143--144},
 publisher = {IEEE},
 title = {Opportunities beyond single-core microprocessors},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798246},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798246},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798246.pdf?arnumber=4798246},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Computer architecture, Environmental economics, Government, Hardware, Microprocessors, Parallel processing, Wires, microcomputers, performance evaluation, performance scaling, sequential cores, single-core microprocessors, },
 abstract = {Designers of future hardware and software face many challenges. Performance scaling of sequential cores is not what it once was. The best architectures for many cores, or equivalents, are not clear. Software, especially on clients, does not make abundant parallelism clear. Computer use and disposal is not so green. Smarter phones must not burn your pocket. Transistors and wires are getting less reliable. CMOS may run out of steam for technical or economic reasons. And the list goes on. But in challenges, optimists find opportunities. This panel will be about opportunities. The author have invited experts with experience in academia, industry, and government to discuss the great possibilities ahead. He asked them to identify two or three topics, where, at most, one is part of their current work. Then they have a discussion. },
}

@inproceedings{4798249,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {},
 year = {2009},
 pages = {149--150},
 publisher = {IEEE},
 title = {Session 3A on-chip networks-I},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798249},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798249},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798249.pdf?arnumber=4798249},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Network-on-a-chip, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04798249.png" border="0"> },
}

@inproceedings{4798248,
 booktitle = {High Performance Computer Architecture, 2009. HPCA 2009. IEEE 15th International Symposium on},
 author = {Patt, Y.},
 year = {2009},
 pages = {147--148},
 publisher = {IEEE},
 title = {Multi-core demands multi-interfaces},
 date = {14-18 Feb. 2009},
 doi = {10.1109/HPCA.2009.4798248},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4798248},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4795428/4798227/04798248.pdf?arnumber=4798248},
 issn = {1530-0897},
 isbn = {978-1-4244-2932-5},
 language = {English},
 keywords = {Computer architecture, Computer industry, Costs, Education, Microarchitecture, Microprocessors, Moore's Law, Programming profession, Springs, microarchitecture, microprocessor chips, multicore architecture, onchip caches, software interface, uniprocessor, },
 abstract = {The challenge for the microarchitect has always been (with very few notable domain-specific exceptions) how to translate the continually increasing processing power provided by Moore's Law into increased performance, or more recently into similar performance at lower cost in energy. The mechanisms in the past (almost entirely) kept the interface intact and used the increase in transistor count to improve the performance of the microarchitecture of the uniprocessor. When that became too hard, we went to larger and larger on-chip caches. Both are consistent with the notion that ldquoabstractions are good.rdquo At some point, we got overwhelmed with too many transistors; predictably, multi-core was born. As the transistor count continues to skyrocket, we are faced with two questions: what should be on the chip, and how should the software interface to it. If we expect to continue to take advantage of what process technology is providing, I think we need to do several things, starting with rethinking the notion of abstraction and providing multiple interfaces for the programmer. },
}

@inproceedings{1385942,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Zhichun Zhu and Zhao Zhang},
 year = {2005},
 pages = { 213-- 224},
 publisher = {IEEE},
 title = {A performance comparison of DRAM memory system optimizations for SMT processors},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.2},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385942},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385942.pdf?arnumber=1385942},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { DRAM chips,  DRAM latency reduction,  DRAM memory system optimizations,  Rambus DRAM systems,  SMT processors,  SMT systems,  buffer storage,  independent channels,  memory channel organization,  memory concurrency,  multi-threading,  multichannel DDR SDRAM,  optimisation,  performance evaluation,  row buffer hit rates,  scheduling,  simultaneous multithreading,  single-threaded systems,  thread-aware DRAM access scheduling,  thread-aware DRAM optimization, Concurrent computing, DRAM chips, Delay, Design optimization, Employment, Multithreading, Parallel processing, Random access memory, Surface-mount technology, Yarn, },
 abstract = {Memory system optimizations have been well studied on single-threaded systems; however, the wide use of simultaneous multithreading (SMT) techniques raises questions over their effectiveness in the new context. In this study, we thoroughly evaluate contemporary multi-channel DDR SDRAM and Rambus DRAM systems in SMT systems, and search for new thread-aware DRAM optimization techniques. Our major findings are: (1) in general, increasing the number of threads tends to increase the memory concurrency and thus the pressure on DRAM systems, but some exceptions do exist; (2) the application performance is sensitive to memory channel organizations, e.g. independent channels may outperform ganged organizations by up to 90\%; (3) the DRAM latency reduction through improving row buffer hit rates becomes less effective due to the increased bank contentions; and (4) thread-aware DRAM access scheduling schemes may improve performance by up to 30\% on workload mixes of memory-intensive applications. In short, the use of SMT techniques has somewhat changed the context of DRAM optimizations but does not make them obsolete. },
}

@inproceedings{569593,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Wolfe, A. and Fritts, J. and Dutta, S. and Fernandes, E.S.T.},
 year = {1997},
 pages = {24--35},
 publisher = {IEEE},
 title = {Datapath design for a VLIW video signal processor},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569593},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569593},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569593.pdf?arnumber=569593},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Clocks, High level languages, LAN interconnection, Program processors, Registers, Signal design, Signal processing, VLIW, VLIW architectures, VLIW video signal processor, VLSI, VLSI, Very large scale integration, Video signal processing, circuit layout CAD, compiler design, datapath design, high parallelism, high-bandwidth interconnect, high-connectivity register files, high-level language programmability, instruction sets, multiprocessing systems, parameterizable versions, very long instruction word, video signal processing, },
 abstract = {This paper represents a design study of the datapath for a very long instruction word (VLIW) video signal processor (VSP). VLIW architectures provide high parallelism and excellent high-level language programmability, but require careful attention to VLSI and compiler design. Flexible, high-bandwidth interconnect, high-connectivity register files, and fast cycle times are required to achieve real-time video signal processing. Parameterizable versions of key modules have been designed in a 0.25 \&mu;m process, allowing us to explore tradeoffs in the VLIW VSP design space. The designs target 33 operations per cycle at clock rates exceeding 600 MHz. Various VLIW code scheduling techniques have been applied to 6 VSP kernels and evaluated on 7 different candidate datapath designs. The results of these simulations are used to indicate which architectural tradeoffs enhance overall performance in this application domain },
}

@inproceedings{1385916,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 publisher = {IEEE},
 title = {Proceedings. 11th International Symposium on High-Performance Computer Architecture},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.11},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385916},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385916.pdf?arnumber=1385916},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { communication architecture,  computer architecture,  computer energy,  computer power,  computer temperature,  energy reduction,  evaluation methodologies,  industrial perspective,  memory architecture,  memory system,  microprocessor chips,  multi-threading,  multiprocessing systems,  multiprocessors,  multithreading,  power reduction,  processor architecture,  program debugging,  software debugging support, },
 abstract = {},
}

@inproceedings{1385917,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { i-- iv},
 publisher = {IEEE},
 title = {Title Page},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.38},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385917},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385917.pdf?arnumber=1385917},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 abstract = {},
}

@inproceedings{744380,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Cox, A.L. and de Lara, E. and Hu, C. and Zwaenepoel, W.},
 year = {1999},
 pages = {279--283},
 publisher = {IEEE},
 title = {A performance comparison of homeless and home-based lazy release consistency protocols in software shared memory},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744380},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744380},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744380.pdf?arnumber=744380},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Automatic logic units, Bandwidth, Communication system control, Computer science, Electronic switching systems, Encoding, Lapping, Protocols, Software performance, TreadMarks, distributed shared memory systems, lazy release consistency protocols, optimisation, paged storage, performance comparison, performance evaluation, protocols, software shared memory, },
 abstract = {In this paper, we compare the performance of two multiple-writer protocols based on lazy release consistency. In particular, we compare the performance of Princeton's home-based protocol and TreadMarks' protocol on a 32-processor platform. We found that the performance difference between the two protocols was less than 4\% for four out of seven applications. For the three applications on which performance differed by more than 4\%, the TreadMarks protocol performed better for two because most of their data were migratory, while the home-based protocol performed better for one. For this one application, the explicit control over the location of data provided by the home-based protocol resulted in a better distribution of communication load across the processors. These results differ from those of a previous comparison of the two protocols. We attribute this difference to (1) a different ratio of memory to network bandwidth on our platform and (2) lazy diffing and request overlapping, two optimizations used by TreadMarks that were not used in the previous study },
}

@inproceedings{744381,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Chen-Chi Kuo and Carter, J. and Kuramkote, R.},
 year = {1999},
 pages = {284--288},
 publisher = {IEEE},
 title = {MP-LOCKs: replacing H/W synchronization primitives with message passing},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744381},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744381},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744381.pdf?arnumber=744381},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Birth disorders, Cities and towns, Computer science, Contracts, Guidelines, Hardware, MP-LOCKs, Message passing, Radio access networks, Testing, US Government, computational complexity, design complexity, hardware synchronization primitives, interprocessor synchronization operations, message passing, message passing, message passing lock algorithms, runtime occupancy, scalable multiprocessors, shared memory accesses, shared memory programs, shared memory systems, synchronisation, },
 abstract = {Shared memory programs guarantee the correctness of concurrent accesses to shared data using interprocessor synchronization operations. The most common synchronization operators are locks, which are traditionally implemented via a mix of shared memory accesses and hardware synchronization primitives like test-and-set. In this paper, we argue that synchronization operations implemented using fast message passing and kernel-embedded lock managers are an attractive alternative to dedicated synchronization hardware. We propose three message passing lock (MP-LOCK) algorithms (centralized, distributed, and reactive) and provide implementation guidelines. MP-LOCKs reduce the design complexity and runtime occupancy of DSM controllers and can exploit software's inherent flexibility to adapt to differing applications lock access patterns. We compared the performance of MP-LOCKs with two common shared memory lock algorithms: test-and-test-and-set and MCS locks and found that MP-LOCKs scale better. For machines with 16 to 32 nodes, applications using MP-LOCKs ran up to 186\% faster than the same applications with shared memory locks. For small systems (up to 8 nodes), three applications with MP-LOCKs slow down by no more than 18\%, while the other two slowed by no more than 180\% due to higher software overhead. We conclude that locks based on message passing should be considered as a replacement for hardware locks in future scalable multiprocessors that support efficient message passing mechanisms },
}

@inproceedings{744382,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Yuanyuan Yang and Jianchao Wang},
 year = {1999},
 pages = {290--299},
 publisher = {IEEE},
 title = {Efficient all-to-all broadcast in all-port mesh and torus networks },
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744382},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744382},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744382.pdf?arnumber=744382},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Application software, Broadcasting, Communication switching, Computer science, Delay, Hardware, Intelligent networks, Laboratories, Parallel processing, Routing, all-port mesh, all-to-all broadcast, delays, message switching, message switching time, multiprocessor interconnection networks, parallel computing, torus networks, total communication delay, transmission time, },
 abstract = {All-to-all communication is one of the most dense communication patterns and occurs in many important applications in parallel computing. In this paper, we present a new all-to-all broadcast algorithm in all-port mesh and torus networks. Unlike existing all-to-all broadcast algorithms, the new algorithm takes advantage of overlapping of message switching time and transmission time, and achieves optimal transmission time for all-to-all broadcast. In addition, in most cases, the total communication delay is close to the lower bound of all-to-all broadcast within a small constant range. Finally, the algorithm is conceptually simple, and symmetrical for every message and every node so that it can be easily implemented in hardware and achieves the optimum in practice },
}

@inproceedings{744383,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Duato, J. and Yalamanchili, S. and Caminero, M.B. and Love, D. and Quiles, F.J.},
 year = {1999},
 pages = {300--309},
 publisher = {IEEE},
 title = {MMR: a high-performance MultiMedia Router-architecture and design trade-offs},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744383},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744383},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744383.pdf?arnumber=744383},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {ATM, Application software, Bandwidth, Bit rate, Communication system control, Cost accounting, Delay, LANs, Local area networks, MMR, Quality of service, Streaming media, Telecommunication traffic, constant bit rate traffic streams, coordinated scheduling, high-performance multimedia router, local area networks, multimedia systems, multiprocessor interconnection networks, performance evaluation, performance evaluation, virtual channels, },
 abstract = {This paper presents the architecture of a router designed to efficiently support traffic generated by multimedia applications. The router is targeted for use in clusters and LANs rather than in WANs, the latter being served by communication substrates such as ATM. The distinguishing features of the proposed router architecture are the use of small fixed-size buffers, a large number of virtual channels, link-level virtual channel flow control, support for dynamic modification of connection bandwidth and priorities, and coordinated scheduling of connections across all output channels. The paper begins with a discussion of the design choices and architectural trade-offs made in the current MultiMedia Router (MMR) project. The performance evaluation section presents some preliminary results of the coordinated scheduling of constant bit rate (CBR) traffic streams },
}

@inproceedings{744384,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Sohn, A. and Yunheung Paek and Jui-Yuan Ku and Kodama, Y. and Yamaguchi, Y.},
 year = {1999},
 pages = {310--314},
 publisher = {IEEE},
 title = {Communication studies of single-threaded and multithreaded distributed-memory machines},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744384},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744384},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744384.pdf?arnumber=744384},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Computer architecture, Delay, ETL EM-X, Fast Fourier transforms, IBM SP-2, Image segmentation, Lapping, LogGP communication models, LogP, Multithreading, Performance analysis, SGI/Cray T3E, Sorting, Switches, Yarn, bitonic sorting, communication overlapping capabilities, communication studies, distributed memory systems, distributed-memory machines, fast Fourier transform, fast Fourier transforms, },
 abstract = {This report explicates the communication overlapping capabilities of three distributed-memory machines, SGI/Cray T3E, IBM SP-2 with wide nodes, and the ETL EM-X. Bitonic sorting and Fast Fourier Transform are selected for experiments. Various message sizes are used to determine when, where, how much and why the overlapping takes place. Experimental results with up to 64 processors indicated that the communication performance of EM-X is insensitive to various message sizes while SP-2 is the most sensitive. T3E stayed in between. The EM-X gave the highest communication overlapping capability while T3E did the lowest. The experimental results are compared with the analytical results based on LogP and LogGP communication models },
}

@inproceedings{744385,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Martinez, J.M. and Lopez, P. and Duato, J.},
 year = {1999},
 pages = {315--318},
 publisher = {IEEE},
 title = {Impact of buffer size on the efficiency of deadlock detection},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744385},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744385},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744385.pdf?arnumber=744385},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Degradation, Electrical capacitance tomography, Multiprocessor interconnection networks, Performance analysis, Proposals, Routing, System recovery, Throughput, Wires, Workstations, buffer size, concurrency control, deadlock detection, deep buffers, interconnection networks routing, multiprocessor interconnection networks, multiprocessors, recovery strategies, simulation, },
 abstract = {Deadlock detection is one of the most important design issues in recovery strategies for routing in interconnection networks. In a previous paper, we presented an efficient deadlock detection mechanism. This mechanism requires that when a message header blocks it must be quickly notified to all the channels reserved by that message. To achieve this goal, the detection mechanism uses the information provided by flow control. Some recent commercial multiprocessors use deep buffers, since they may increase network throughput and efficiently allow transmission over long wires. However, deep buffers may increase the elapsed time between header blocking at a router and the propagation of flow control signals, thus negatively affecting the behavior of our deadlock detection mechanism. On the other hand, deeper buffers reduce deadlock frequency. As a consequence, buffer size has opposing effects on deadlock detection. In this paper, we analyze by simulation the influence of these effects on the efficiency of our deadlock detection mechanism, showing that overall performance improves with buffer size },
}

@inproceedings{744386,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {},
 year = {1999},
 pages = {323--324},
 publisher = {IEEE},
 title = {Author Index},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744386},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744386},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744386.pdf?arnumber=744386},
 isbn = {0-7695-0004-8},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00744386.png" border="0"> },
}

@inproceedings{1385919,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { ix-- ix},
 publisher = {IEEE},
 title = {Message from the General Chair},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.19},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385919},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385919.pdf?arnumber=1385919},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 abstract = {},
}

@inproceedings{5749761,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {},
 year = {2011},
 pages = {c1--c1},
 publisher = {IEEE},
 title = {Front cover},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749761},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749761},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749761.pdf?arnumber=5749761},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {},
}

@inproceedings{5749760,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {},
 year = {2011},
 pages = {iii--vii},
 publisher = {IEEE},
 title = {Table of contents},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749760},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749760},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749760.pdf?arnumber=5749760},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {},
}

@inproceedings{4658648,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Hur, I. and Lin, C.},
 year = {2008},
 pages = {305--316},
 publisher = {IEEE},
 title = {A comprehensive approach to DRAM power management},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658648},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658648},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658648.pdf?arnumber=4658648},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Adaptive scheduling, DRAM chips, DRAM energy efficiency, DRAM power management, Delay estimation, Energy consumption, Energy efficiency, Energy management, Memory management, Power system management, Random access memory, SDRAM, Technology management, adaptive history-based memory scheduler, memory controller, },
 abstract = {This paper describes a comprehensive approach for using the memory controller to improve DRAM energy efficiency and manage DRAM power. We make three contributions: (1) we describe a simple power-down policy for exploiting low power modes of modern DRAMs; (2) we show how the idea of adaptive history-based memory schedulers can be naturally extended to manage power and energy; and (3) for situations in which additional DRAM power reduction is needed, we present a throttling approach that arbitrarily reduces DRAM activity by delaying the issuance of memory commands. Using detailed microarchitectural simulators of the IBM Power5+ and a DDR2-533 SDRAM, we show that our first two techniques combine to increase DRAM energy efficiency by an average of 18.2\%, 21.7\%, 46.1\%, and 37.1\% for the Stream, NAS, SPEC2006fp, and commercial benchmarks, respectively. We also show that our throttling approach provides performance that is within 4.4\% of an idealized oracular approach. },
}

@inproceedings{4658649,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Aggarwal, N. and Cantin, J.F. and Lipasti, M.H. and Smith, J.E.},
 year = {2008},
 pages = {317--328},
 publisher = {IEEE},
 title = {Power-Efficient DRAM Speculation},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658649},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658649},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658649.pdf?arnumber=4658649},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Broadcasting, Content management, DRAM chips, DRAM latency, DRAM power consumption, DRAM power management, Delay, Energy consumption, Energy management, Hardware, Multiprocessing systems, Power system management, Random access memory, Resource management, broadcast snoop, broadcast-based shared-memory multiprocessor systems, low-power electronics, power aware computing, power optimization, power-efficient DRAM speculation, region coherence arrays, shared memory systems, },
 abstract = {Power-efficient DRAM Speculation (PEDS) is a power optimization targeted at broadcast-based shared-memory multiprocessor systems that speculatively access DRAM in parallel with the broadcast snoop. Although speculatively accessing DRAM has the potential performance advantage of overlapping DRAM latency with the snoop, it wastes power for memory requests that obtain data from other processorspsila caches. PEDS takes advantage of information provided by a Region Coherence Array to identify requests that have a high likelihood of obtaining data from another processorpsilas cache, and does not access DRAM speculatively for those requests. By doing so, PEDS eliminates DRAM reads, reduces DRAM power consumption, reduces contention for DRAM resources, and increases the opportunity for DRAM power management. PEDS requires almost no additional hardware in systems that incorporate Region Coherence Arrays. Detailed simulation results show PEDS reduces average DRAM read traffic 28-32\%, reduces average DRAM power dissipation 17-22\%, and reduces average DRAM energy consumption 16-21\%. },
}

@inproceedings{4658646,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {JaeWoong Chung and Dalton, M. and Kannan, H. and Kozyrakis, C.},
 year = {2008},
 pages = {279--289},
 publisher = {IEEE},
 title = {Thread-safe dynamic binary translation using transactional memory},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658646},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658646},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658646.pdf?arnumber=4658646},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Application software, Buffer overflow, Data security, Hardware, Information security, Instruments, Laboratories, Runtime, Software tools, Yarn, bug detection tools, dynamic information flow tracking, multi-threading, multithreaded programs, runtime instrumentation technique, software maintenance, software tools, software transactions, thread-safe dynamic binary translation, transactional memory, },
 abstract = {Dynamic binary translation (DBT) is a runtime instrumentation technique commonly used to support profiling, optimization, secure execution, and bug detection tools for application binaries. However, DBT frameworks may incorrectly handle multithreaded programs due to races involving updates to the application data and the corresponding metadata maintained by the DBT. Existing DBT frameworks handle this issue by serializing threads, disallowing multithreaded programs, or requiring explicit use of locks. This paper presents a practical solution for correct execution of multithreaded programs within DBT frameworks. To eliminate races involving metadata, we propose the use of transactional memory (TM). The DBT uses memory transactions to encapsulate the data and metadata accesses in a trace, within one atomic block. This approach guarantees correct execution of concurrent threads of the translated program, as TM mechanisms detect and correct races. To demonstrate this approach, we implemented a DBT-based tool for secure execution of x86 binaries using dynamic information flow tracking. This is the first such framework that correctly handles multithreaded binaries without serialization. We show that the use of software transactions in the DBT leads to a runtime overhead of 40\%. We also show that software optimizations in the DBT and hardware support for transactions can reduce the runtime overhead to 6\%. },
}

@inproceedings{4658647,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Hongtao Zhong and Mehrara, M. and Lieberman, S. and Mahlke, S.},
 year = {2008},
 pages = {290--301},
 publisher = {IEEE},
 title = {Uncovering hidden loop level parallelism in sequential applications},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658647},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658647},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658647.pdf?arnumber=4658647},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Application software, Computer architecture, Failure analysis, Hardware, History, Multicore processing, Parallel processing, Parallel programming, Performance gain, Programming profession, automatic parallelization, general-purpose software, hidden loop level parallelism, legacy single-threaded software, multicore systems, parallel programming, parallel programming, software maintenance, thread-level parallelism, },
 abstract = {As multicore systems become the dominant mainstream computing technology, one of the most difficult challenges the industry faces is the software. Applications with large amounts of explicit thread-level parallelism naturally scale performance with the number of cores, but single-threaded applications realize little to no gains with additional cores. One solution to this problem is automatic parallelization that frees the programmer from the difficult task of parallel programming and offers hope for handling the vast amount of legacy single-threaded software. There is a long history of automatic parallelization for scientific applications, but the techniques have generally failed in the context of general-purpose software. Thread-level speculation overcomes the problem of memory dependence analysis by speculating unlikely dependences that serialize execution. However, this approach has lead to only modest performance gains. In this paper, we take another look at exploiting loop-level parallelism in single-threaded applications. We show that substantial amounts of loop-level parallelism is available in general-purpose applications, but it lurks beneath the surface and is often obfuscated by a small number of data and control dependences. We adapt and extend several code transformations from the instruction-level and scientific parallelization communities to uncover the hidden parallelism. Our results show that 61\% of the dynamic execution of studied benchmarks can be parallelized with our techniques compared to 27\% using traditional thread-level speculation techniques, resulting in a speedup of 1.84 on a four core system compared to 1.41 without transformations. },
}

@inproceedings{4658644,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Salverda, P. and Zilles, C.},
 year = {2008},
 pages = {252--263},
 publisher = {IEEE},
 title = {Fundamental performance constraints in horizontal fusion of in-order cores},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658644},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658644},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658644.pdf?arnumber=4658644},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Clocks, Computer science, Graphics, Hardware, Out of order, Parallel processing, Physics computing, Resists, Signal processing, Signal synthesis, active dataflow chains, dynamically-scheduled designs, fused machine, fusion-related overheads, horizontal fusion, in-order cores, multiprocessing systems, out-of-order cores, processor scheduling, },
 abstract = {A conceptually appealing approach to supporting a broad range of workloads is a system comprising many small cores that can be fused, on demand, into larger cores. We demonstrate that using in-order cores for this purpose, even under idealized assumptions about fusion-related overheads, would introduce fundamental obstacles to achieving good performance - obstacles that are not present when out-of-order cores are used. Matching the performance of modern dynamically-scheduled designs demands that a fused machine be able to simultaneously manage a large number of active dataflow chains, many more than the amount of ILP typically extracted from the code. When it is in-order cores that are fused, this requirement, in turn, demands either that the active dataflow chains be carefully interleaved among the available issue queues, or that enough cores be provided for them to reside at distinct queues. Using an abstract model for reasoning about the performance of these machines, we show that the former option is fundamentally hard, in the sense that it necessitates instruction steering hardware that would be too complex to build. The latter option would demand so many cores that the machine would be overwhelmed by fusion-related overheads. In short, if the goal is to match the performance of modern dynamically-scheduled machines, fusion of in-order cores is not a very compelling approach; either a fundamentally new method for fusing cores is needed, or some form of out-of-order capability must be provided at the constituent cores. },
}

@inproceedings{4658645,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Wells, P.M. and Sohi, G.S.},
 year = {2008},
 pages = {264--275},
 publisher = {IEEE},
 title = {Serializing instructions in system-intensive workloads: Amdahl&#x2019;s Law strikes again},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658645},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658645},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658645.pdf?arnumber=4658645},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Amdahl Law, Computer aided instruction, Control systems, Instruction sets, Logic, Operating systems, Out of order, Parallel processing, Pipelines, Registers, Yarn, operating system, operating systems (computers), out-of-order, serializing instructions, system-intensive workloads, },
 abstract = {Serializing instructions (SIs), such as writes to control registers, have many complex dependencies, and are difficult to execute out-of-order (OoO). To avoid unnecessary complexity, processors often serialize the pipeline to maintain sequential semantics for these instructions. },
}

@inproceedings{4658642,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Joshi, A.M. and Eeckhout, L. and John, L.K. and Isen, C.},
 year = {2008},
 pages = {229--239},
 publisher = {IEEE},
 title = {Automated microprocessor stressmark generation},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658642},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658642},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658642.pdf?arnumber=4658642},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Automatic generation control, Character generation, Cooling, Energy management, Microprocessors, Packaging, Power system management, Temperature, Thermal management, Thermal stresses, automated microprocessor stressmark generation, control flow behavior, instruction mix, instruction-level parallelism, memory access patterns, microprocessor chips, performance evaluation, performance evaluation, },
 abstract = {Estimating the maximum power and thermal characteristics of a processor is essential for designing its power delivery system, packaging, cooling, and power/thermal management schemes. Typical benchmark suites used in performance evaluation do not stress the processor to its limit though, and current practice in industry is to develop artificial benchmarks that are specifically written to generate maximum processor (component) activity. However, manually developing and tuning so called stressmarks is extremely tedious and time-consuming while requiring an intimate understanding of the processor. A synthetic program that can be tuned to produce a variety of benchmark characteristics would significantly help in addressing this problem by enabling the automatic exploration of the large temperature and power design space. This paper demonstrates that with a suitable choice of only 40 hardware-independent program characteristics related to the instruction mix, instruction-level parallelism, control flow behavior, and memory access patterns, it is possible to generate a synthetic benchmark whose performance relates to that of general-purpose and commercial applications. Leveraging this abstract workload modeling approach, we propose StressMaker, a framework that uses machine learning for the automated generation of stressmarks. A comparison with an exhaustive exploration of a large power design space demonstrates that StressMaker is very effective in automatically generating stressmarks in a limited amount of time. },
}

@inproceedings{4658643,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Lee, B.C. and Brooks, D.},
 year = {2008},
 pages = {240--251},
 publisher = {IEEE},
 title = {Roughness of microarchitectural design topologies and its implications for optimization},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658643},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658643},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658643.pdf?arnumber=4658643},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Computational modeling, Data visualization, Design engineering, Design optimization, Machine learning, Microarchitecture, Optimization methods, Predictive models, Robustness, Topology, contour map, correlation method, correlation methods, error statistics, gradient ascent method, high-order derivative, integral equations, learning (artificial intelligence), logic design, machine learning, microarchitectural design topology, microprocessor chips, multidimensional integral, optimisation, optimization, regression analysis, regression model error, roughness metrics, statistical inference, },
 abstract = {Recent advances in statistical inference and machine learning close the divide between simulation and classical optimization, thereby enabling more rigorous and robust microarchitectural studies. To most effectively utilize these now computationally tractable techniques, we characterize design topology roughness and leverage this characterization to guide our usage of analysis and optimization methods. In particular, we compute roughness metrics that require high-order derivatives and multi-dimensional integrals of design metrics, such as performance and power. These roughness metrics exhibit noteworthy correlations (1) against regression model error, (2) against non-linearities and non-monotonicities of contour maps, and (3) against the effectiveness of optimization heuristics such as gradient ascent. Thus, this work quantifies the implications of design topology roughness for commonly used methods and practices in microarchitectural analysis. },
}

@inproceedings{4658640,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Gratz, P. and Grot, B. and Keckler, S.W.},
 year = {2008},
 pages = {203--214},
 publisher = {IEEE},
 title = {Regional congestion awareness for load balance in networks-on-chip},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658640},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658640},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658640.pdf?arnumber=4658640},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Delay, Fault tolerance, Logic, Multiprocessor interconnection networks, Network-on-a-chip, NoC, Routing, System-on-a-chip, Telecommunication traffic, Throughput, Wiring, adapting routing, global network balance, interconnection networks-on-chip, load balance, multiprocessor interconnection networks, network-on-chip, regional congestion awareness, },
 abstract = {Interconnection networks-on-chip (NOCs) are rapidly replacing other forms of interconnect in chip multiprocessors and system-on-chip designs. Existing interconnection networks use either oblivious or adaptive routing algorithms to determine the route taken by a packet to its destination. Despite somewhat higher implementation complexity, adaptive routing enjoys better fault tolerance characteristics, increases network throughput, and decreases latency compared to oblivious policies when faced with non-uniform or bursty traffic. However, adaptive routing can hurt performance by disturbing any inherent global load balance through greedy local decisions. To improve load balance in adapting routing, we propose Regional Congestion Awareness (RCA), a lightweight technique to improve global network balance. Instead of relying solely on local congestion information, RCA informs the routing policy of congestion in parts of the network beyond adjacent routers. Our experiments show that RCA matches or exceeds the performance of conventional adaptive routing across all workloads examined, with a 16\% average and 71\% maximum latency reduction on SPLASH-2 benchmarks running on a 49-core CMP. Compared to a baseline adaptive router, RCA incurs a negligible logic and modest wiring overhead. },
}

@inproceedings{4658641,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Das, R. and Mishra, A.K. and Nicopoulos, C. and Dongkook Park and Narayanan, V. and Iyer, R. and Yousif, M.S. and Das, C.R.},
 year = {2008},
 pages = {215--225},
 publisher = {IEEE},
 title = {Performance and power optimization through data compression in Network-on-Chip architectures},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658641},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658641},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658641.pdf?arnumber=4658641},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Cache storage, Context, Data compression, Delay, Energy consumption, Fabrics, Frequency, Microarchitecture, Multicore processing, Network-on-a-chip, NoC communication latency, NoC performance, cache compression, cache storage, communication compression, data compression, data compression, energy behavior, network routing, network-on-chip, network-on-chip architecture, nonuniform cache-based multicore architectures, on-chip cache, packet-based network-on-chip, power consumption, power optimization, router architecture, storage compression, },
 abstract = {The trend towards integrating multiple cores on the same die has accentuated the need for larger on-chip caches. Such large caches are constructed as a multitude of smaller cache banks interconnected through a packet-based network-on-chip (NoC) communication fabric. Thus, the NoC plays a critical role in optimizing the performance and power consumption of such non-uniform cache-based multicore architectures. While almost all prior NoC studies have focused on the design of router microarchitectures for achieving this goal, in this paper, we explore the role of data compression on NoC performance and energy behavior. In this context, we examine two different configurations that explore combinations of storage and communication compression: (1) Cache compression (CC) and (2) Compression in the NIC (NC). We also address techniques to hide the decompression latency by overlapping with NoC communication latency. Our simulation results with a diverse set of scientific and commercial benchmark traces reveal that CC can provide up to 33\% reduction in network latency and up to 23\% power savings. Even in the case of NC - where the data is compressed only when passing through the NoC fabric of the NUCA architecture and stored uncompressed - performance and power savings of up to 32\% and 21\%, respectively, can be obtained. These performance benefits in the interconnect translate up to 17\% reduction in CPI. These benefits are orthogonal to any router architecture and make a strong case for utilizing compression for optimizing the performance and power envelope of NoC architectures. In addition, the study demonstrates the criticality of designing faster routers in shaping the performance behavior. },
}

@inproceedings{569699,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Vien Dao, B. and Yalamanchili, S. and Duato, J.},
 year = {1997},
 pages = {343--352},
 publisher = {IEEE},
 title = {Architectural support for reducing communication overhead in multiprocessor interconnection networks},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569699},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569699},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569699.pdf?arnumber=569699},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Circuits, Communication switching, Delay, Electronic mail, Intelligent networks, Message passing, Modems, Multiprocessor interconnection networks, Network interfaces, Program processors, application program kernels, architectural support, cluster interconnects, communication locality, communication overhead reduction, communication traces, inter-processor communication traffic, message in-flight time, message passing, multiprocessor interconnection networks, multiprocessor interconnection networks, network interfaces, network interfaces, routers, },
 abstract = {Modern multicomputer interconnection networks offer the delivery of messages with very low latency. However the message in-flight time is only a small portion of the total time that is required to send a message from source to destination. For fine to medium grained message sizes, the majority of time is spent in overheads for setting up and managing message transmission. It is often possible for compilers/programmers to separate inter-processor communication traffic into messages that exhibit communication locality and messages that do not. This paper proposes architectural modifications to network interfaces and routers to enable compilers/programmers to exploit known locality properties of programs in reducing the fixed overhead of transmission. These techniques work well on traffic exhibiting communication locality without unduly penalizing ``ordinary" message traffic. The proposed techniques are evaluated using communication traces from 5 application program kernels. Significant reductions in average message latency are possible, and we argue that the approach can be used in the next generation of cluster interconnects },
}

@inproceedings{569697,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Welsh, M. and Basu, A. and von Eicken, T.},
 year = {1997},
 pages = {332--342},
 publisher = {IEEE},
 title = {ATM and fast Ethernet network interfaces for user-level communication},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569697},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569697},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569697.pdf?arnumber=569697},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {ATM, Computer architecture, Coprocessors, Delay, Distributed computing, Ethernet networks, Fabrics, Network interfaces, Operating systems, Protection, Split-C parallel benchmarks, U-Net communication architecture, Workstations, application-level performance, asynchronous transfer mode, distributed computing, fast Ethernet network interfaces, high-bandwidth communication, high-performance computing, local area networks, low-latency communication, network interfaces, parallel computing, programmable coprocessors, user-level access, user-level communication, workstation clusters, },
 abstract = {Fast Ethernet and ATM are two attractive network technologies for interconnecting workstation clusters for parallel and distributed computing. This paper compares network interfaces with and without programmable co-processors for the two types of networks using the U-Net communication architecture to provide low-latency and high-bandwidth communication. U-Net provides protected, user-level access to the network interface and offers application-level round-trip latencies as low as 60 \&mu;sec over Fast Ethernet and 90 \&mu;sec over ATM. The design of the network interface and the underlying network fabric have a large bearing on the U-Net design and performance. Network interfaces with programmable co-processors can transfer data directly to and from user space while others require aid from the operating system kernel. The paper provides detailed performance analysis of U-Net for Fast Ethernet and ATM, including application-level performance on a set of Split-C parallel benchmarks. These results show that high-performance computing is possible on a network of PCs connected via Fast Ethernet },
}

@inproceedings{569696,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Markatos, E.P. and Katevenis, M.G.H.},
 year = {1997},
 pages = {322--331},
 publisher = {IEEE},
 title = {User-level DMA without operating system kernel modification},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569696},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569696},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569696.pdf?arnumber=569696},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Application software, Assembly, Computer architecture, Computer networks, Electrostatic precipitators, Engines, Kernel, Multiprocessor interconnection networks, Operating systems, Protection, architecture trends, assembly instructions, computer architecture, file organisation, host computer, interconnection network, user-level direct memory access, },
 abstract = {Direct Memory Access (DMA) is frequently used to transfer data between the main memory of a host computer and the interconnection network, in order to free the host processor from the burden of the transfer. DMA operations are traditionally initiated by the operating system kernel, mainly to prevent one application from tampering with another applications' data. Recent architecture trends suggest that interconnection networks get faster, while operating systems get slower (compared to processor speeds). These trends imply that the initiation of a DMA operation becomes slower (due to operating system involvement), while the DMA data transfer itself becomes faster with time. Soon, the operating system overhead associated with starting a DMA will be larger than the data transfer itself, esp. for small data transfers. This paper proposes several algorithms that allow user-level applications to start DMA operating without the involvement of the operating system. Our algorithms allow user applications to have direct (but controlled) access to the DMA engine registers. Overhead user-level DMA is achieved without compromising protection, and without requiring changes to the underlying operating system kernel. Using our proposed algorithms, a DMA operation can be initiated in 2 to 5 assembly instructions. By comparison, operating system-based initiation of DMA requires thousands of assembly instructions },
}

@inproceedings{569691,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Noonburg, D.B. and Shen, J.P.},
 year = {1997},
 pages = {298--309},
 publisher = {IEEE},
 title = {A framework for statistical modeling of superscalar processor performance},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569691},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569691},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569691.pdf?arnumber=569691},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Computational modeling, Costs, Markov chain, Markov processes, Microarchitecture, Parallel processing, Performance analysis, Pipelines, Probability, Resource management, State-space methods, Timing, buffers, discrete event simulation, machine model, parallel processing, performance data, performance evaluation, performance evaluation, pipelines, processor microarchitectures, program parallelism parameters, resource utilization, simulation times, statistical modeling, superscalar processor performance, trace-driven simulation, trace-driven techniques, },
 abstract = {Presents a statistical approach to modeling superscalar processor performance. Standard trace-driven techniques are very accurate, but require extremely long simulation times, especially as traces reach lengths in the billions of instructions. A framework for statistical models is described which facilitates fast, accurate performance evaluation. A machine model is built up from components: buffers, pipelines, etc. Each program trace is scanned once, generating a set of program parallelism parameters which can be used across an entire family of machine models. The machine model and program parallelism parameters are combined to form a Markov chain. The Markov chain is partitioned in order to reduce the size of the state space, and the resulting linked models are solved using an iterative technique. The use of this framework is demonstrated with two simple processor microarchitectures. The IPC estimates are very close to the IPCs generated by trace-driven simulation of the same microarchitectures. Resource utilization and other performance data can also be obtained from the statistical model },
}

@inproceedings{569693,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Chodnekar, S. and Srinivasan, V. and Vaidya, A.S. and Sivasubramaniam, A. and Das, C.R.},
 year = {1997},
 pages = {310--319},
 publisher = {IEEE},
 title = {Towards a communication characterization methodology for parallel applications},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569693},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569693},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569693.pdf?arnumber=569693},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {2-D mesh network simulator, Discrete event simulation, Feeds, Frequency, IBM SP2, Mesh networks, Multiprocessor interconnection networks, Packaging machines, Parallel machines, Regression analysis, SPASM, Statistical analysis, Synthetic aperture sonar, communication characterization methodology, communication events, execution-driven simulator, interconnection network, message generation frequency, message length, message passing, message passing applications, multiprocessor interconnection networks, parallel applications, parallel machine, parallel machines, performance evaluation, performance evaluation, regression analysis, shared memory applications, spatial distribution, spatial distribution of messages, statistical analysis package, },
 abstract = {The interconnection network (ICN) is a vital component of a parallel machine and is often the limiting factor in the performance of several parallel applications. While ICN performance evaluation has been a widely researched topic, there have been very few studies that have used real applications to drive this research. In this paper we develop a framework for characterizing the communication properties of parallel applications. Message generation frequency, spatial distribution of messages and message length are the three attributes that quantify any communication. We develop a methodology to quantify these attributes, in particular the first two attributes. We employ two strategies, namely dynamic and static, in our methodology. In the former, the applications are executed on an execution-driven simulator called SPASM, while in the latter they are executed on a parallel machine, IBM SP2. We gather communication events from these executions and feed them to a 2-D mesh network simulator. The log of the network activity is then analyzed using a statistical analysis package (SAS) to find the message inter-arrival time distribution and spatial distribution via regression analysis. Five shared memory applications and two message passing applications are analyzed to quantify their communication workloads. It is shown that it is possible to express the message generation and spatial distribution of an application in terms of commonly used distributions. These distributions can be used in the analysis of ICNs for developing realistic performance models },
}

@inproceedings{501194,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Gulati, M. and Bagherzadeh, N.},
 year = {1996},
 pages = {291--301},
 publisher = {IEEE},
 title = {Performance study of a multithreaded superscalar microprocessor},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501194},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501194},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501194.pdf?arnumber=501194},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Computational modeling, Computer architecture, Hardware, Microprocessors, Multithreading, Out of order, Parallel processing, Registers, Very large scale integration, Yarn, cache performance, fetch policy, functional unit utilization, instruction scheduling, instruction-level parallelism, microprocessor chips, minimal extra hardware, multithreaded superscalar microprocessor, performance, performance evaluation, },
 abstract = {This paper describes a technique for improving the performance of a superscalar processor through multithreading. The technique exploits the instruction-level parallelism available both inside each individual stream, and across streams. The former is exploited through out-of-order execution of instructions within a stream, and the latter through execution of instructions from different streams simultaneously. Aspects of multithreaded superscalar design, such as fetch policy, cache performance, instruction scheduling, and functional unit utilization are studied. We analyze performance based on the simulation of a superscalar architecture and show that it is possible to provide support for multiple streams with minimal extra hardware, yet achieving significant performance gain (20-55\%) across a range of benchmarks },
}

@inproceedings{501195,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Anderson, C. and Karlin, A.R.},
 year = {1996},
 pages = {303--313},
 publisher = {IEEE},
 title = {Two adaptive hybrid cache coherency protocols},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501195},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501195},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501195.pdf?arnumber=501195},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Access protocols, Broadcasting, Computer science, Costs, Counting circuits, Measurement, Pathology, Performance evaluation, Writing, adaptive hybrid cache coherency protocols, cache blocks, memory protocols, performance evaluation, performance measurements, shared memory systems, shared-memory multiprocessors, write-invalidate protocols, write-update protocols, },
 abstract = {We present and evaluate adaptive, hybrid cache coherence protocols for bus-based, shared-memory multiprocessors. Such protocols are motivated by the observation that sharing patterns vary substantially between different programs and even cache blocks within the same program. Performance measurements across a range of parallel applications indicate that the adaptive protocols we present perform well compared to both write-invalidate and write-update protocols },
}

@inproceedings{501196,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Takahashi, M. and Takano, H. and Kaneko, E. and Suzuki, S.},
 year = {1996},
 pages = {314--322},
 publisher = {IEEE},
 title = {A shared-bus control mechanism and a cache coherence protocol for a high-performance on-chip multiprocessor},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501196},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501196},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501196.pdf?arnumber=501196},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Access protocols, CRAC, Clocks, Costs, Delay, Microprocessors, Monitoring, Research and development, System-on-a-chip, Traffic control, Very large scale integration, advanced VLSI technology, bus transaction, cache coherence protocol, cache storage, central coherence unit, high-performance on-chip multiprocessor, memory protocols, multiprocessing systems, performance evaluation, shared-bus control mechanism, simulator, },
 abstract = {A new cache coherence solution is proposed for an over 500 MHz on-chip multiprocessor using advanced VLSI technology. In order to reduce shared-bus transaction time, the central coherence unit (CCU) is introduced. The CCU controls all shared-bus transactions, monitoring all cache rays every clock cycle, and executes a bus transaction in four clock cycles while a conventional bus mechanism requires eight clock cycles. A new cache coherence protocol (CRAC) is also introduced in order to reduce external memory access. The CRAC protocol makes it possible to load a desired data from any cache having a copy, and to transfer write-back responsibility to another cache having a copy. An implementation of CCU and CRAC is presented and evaluated using a cycle-based multiprocessor simulator. Simulation results show that introduction of CCU and CRAC is effective to reduce shared-bus traffic and total execution time. Furthermore, proposed multiprocessor model with CCU and CRAC is proved to be more scalable than a conventional multiprocessor model },
}

@inproceedings{501197,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Raynaud, A. and Zheng Zhang and Torrellas, J.},
 year = {1996},
 pages = {323--334},
 publisher = {IEEE},
 title = {Distance-adaptive update protocols for scalable shared-memory multiprocessors},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501197},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501197},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501197.pdf?arnumber=501197},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Access protocols, Contracts, Costs, Delay effects, Multithreading, NASA, Prefetching, Research and development, Telecommunication traffic, Traffic control, directory, distance-adaptive update protocols, dynamic pointer scheme, performance, performance evaluation, protocols, scalable shared-memory multiprocessors, shared memory systems, },
 abstract = {While update protocols generally induce lower miss rates than invalidate protocols, they tend to generate much traffic. This is one of the reasons why they are considered less cost-effectively scalable than invalidate protocols and, as a result, are avoided in most existing designs of scalable shared-memory multiprocessors. However, given the increasing relative cost of cache misses, update protocols are becoming more worthy of exploration. In this paper, we present a model of sharing that is key to investigating the performance of optimized update protocols: the update distance model. The model gives insight into the update patterns that optimized protocols need to handle. Using this model, we design a new family of protocols that we call distance-adaptive protocols. In these schemes, the directory records the update patterns observed and then uses them to selectively send updates and invalidations to processors. As a result, traffic and miss rates are kept low. We present an implementation of these protocols based on a dynamic pointer scheme. A performance comparison between one of these protocols and efficient invalidate and delayed competitive-update protocols over five applications shows that the new protocol decreases the execution time by an average of 15\% and 10\% respectively },
}

@inproceedings{501190,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Calder, B. and Grunwald, D. and Emer, J.},
 year = {1996},
 pages = {244--253},
 publisher = {IEEE},
 title = {Predictive sequential associative cache},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501190},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501190},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501190.pdf?arnumber=501190},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Added delay, Computer science, Delay effects, Pipelines, Probes, Timing, access latency, access time, content-addressable storage, direct-mapped cache, memory architecture, miss rate, prediction sources, predictive sequential associative cache, storage management, },
 abstract = {In this paper we propose a cache design that provides the same miss rate as a two-way set associative cache, but with an access time closer to a direct-mapped cache. As with other designs, a traditional direct-mapped cache is conceptually partitioned into multiple banks, and the blocks in each set are probed, or examined, sequentially. Other designs either probe the set in a fixed order or add extra delay in the access path for all accesses. We use prediction sources to guide the cache examination, reducing the amount of searching and thus the average access latency. A variety of accurate prediction sources are considered, with some being available in early pipeline stages. We feel that our design offers the same or better performance and is easier to implement than previous designs },
}

@inproceedings{501191,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Alexander, T. and Kedem, G.},
 year = {1996},
 pages = {254--263},
 publisher = {IEEE},
 title = {Distributed prefetch-buffer/cache design for high performance memory systems},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501191},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501191},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501191.pdf?arnumber=501191},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Bandwidth, Clocks, Computer science, DRAM access times, Delay, Gears, Hardware, Microprocessors, Prefetching, Random access memory, SRAM, System performance, cache design, cache storage, distributed cache architecture, distributed prefetch buffer design, high performance memory systems, memory architecture, memory system architecture, memory system speed, prefetching technique, table based prediction scheme, },
 abstract = {Microprocessor execution speeds are improving at a rate of 50\%-80\% per year while DRAM access times are improving at a much lower rate of 5\%-10\% per year. Computer systems are rapidly approaching the point at which overall system performance is determined not by the speed of the CPU but by the memory system speed. We present a high performance memory system architecture that overcomes the growing speed disparity between high performance microprocessors and current generation DRAMs. A novel prediction and prefetching technique is combined with a distributed cache architecture to build a high performance memory system. We use a table based prediction scheme with a prediction cache to prefetch data from the on-chip DRAM array to an on-chip SRAM prefetch buffer. By prefetching data we are able to hide the large latency associated with DRAM access and cycle times. Our experiments show that with a small (32 KB) prediction cache we can get an effective main memory access time that is close to the access time of larger secondary caches },
}

@inproceedings{501192,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Cvetanovic, Z. and Bhandarkar, D.},
 year = {1996},
 pages = {270--280},
 publisher = {IEEE},
 title = {Performance characterization of the Alpha 21164 microprocessor using TP and SPEC workloads},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501192},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501192},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501192.pdf?arnumber=501192},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Alpha 21164 microprocessor, AlphaServer 8200 system, Application software, Bandwidth, CMOS technology, Clocks, DEC 7000 server, Data analysis, Delay, Hardware, Microprocessors, Pipelines, SPEC workloads, TP workloads, Transaction databases, microprocessor chips, performance characteristics, performance characterization, performance evaluation, },
 abstract = {This paper compares the performance characteristics of the Alpha 21164 to the previous-generation 21064 microprocessor. Measurements on the 21164-based AlphaServer 8200 system are compared to the 21064-based DEC 7000 server using several commercial and technical workloads. The data analyzed includes cycles per instruction, multiple-issued instructions, branch predictions, stall components, cache misses, and instruction frequencies. The AlphaServer 8200 provides 2 to 3 times the performance of the DEC 7000 server based on the faster clock, larger on-chip cache, expanded multiple-issuing, and lower cache/memory latencies and higher bandwidth },
}

@inproceedings{501193,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Espasa, R. and Valero, M.},
 year = {1996},
 pages = {281--290},
 publisher = {IEEE},
 title = {Decoupled vector architectures},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501193},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501193},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501193.pdf?arnumber=501193},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Computational modeling, Computer aided instruction, Computer architecture, Costs, Delay, Hardware, Multithreading, Parallel processing, Perfect Club programs, Vector processors, Yarn, bypassing technique, decoupled vector architectures, hardware cost, performance, performance advantages, performance evaluation, realistic memory latencies, total memory traffic, trace driven approach, vector processor, vector processor systems, },
 abstract = {The purpose of this paper is to show that using decoupling techniques in a vector processor, the performance of vector programs can be greatly improved. Using a trace driven approach, we simulate a selection of the Perfect Club programs and compare their execution time on a conventional vector architecture and on a decoupled vector architecture. Decoupling provides a performance advantage of more than a factor of two for realistic memory latencies, and even with an ideal memory system with no latency, there is still a speedup of as much as 50\%. A bypassing technique between the load/store queues is introduced and we show how it can give up to an extra speedup of 22\% while also reducing total memory traffic by an average of 20\%. An important part of this paper is devoted to study the tradeoffs involved in choosing an adequate size for the different queues of the architecture, so that the hardware cost of the queues can be minimized while still retaining most of the performance advantages of decoupling },
}

@inproceedings{501198,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {},
 year = {1996},
 publisher = {IEEE},
 title = {Author Index},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501198},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501198},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501198.pdf?arnumber=501198},
 isbn = {0-8186-7237-4},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00501198.png" border="0"> },
}

@inproceedings{4147637,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {vi--vi},
 publisher = {IEEE},
 title = {Message from the General Chairs and the Program Chair},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346174},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147637},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147637.pdf?arnumber=4147637},
 isbn = {1-4244-0805-9},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04147637.png" border="0"> },
}

@inproceedings{824351,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Karlsson, M. and Dahlgren, F. and Stenstrom, P.},
 year = {2000},
 pages = {206--217},
 publisher = {IEEE},
 title = {A prefetching technique for irregular accesses to linked data structures},
 date = {2000},
 doi = {10.1109/HPCA.2000.824351},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824351},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824351.pdf?arnumber=824351},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Data structures, Decision support systems, Delay, Engines, Expert systems, Hardware, Java, Kernel, Memory management, Prefetching, data structures, linked data structure, linked data structures, prefetch arrays, prefetching, storage allocation, storage management, },
 abstract = {Prefetching offers the potential to improve the performance of linked data structure (LDS) traversals. However, previously proposed prefetching methods only work well when there is enough work processing a node that the prefetch latency can be hidden, or when the LDS is long enough and the traversal path is known a priori. This paper presents a prefetching technique called prefetch arrays which can prefetch both short LDS, as the lists found in hash tables, and trees when the traversal path is nor known a priori. We offer two implementations, one software-only and one which combines software annotations with a prefetch engine in hardware. On a pointer-intensive benchmark suite, we show that our implementations reduce the memory stall lime by 23\% to 51\% for the kernels with linked lists, while the other prefetching methods cause reductions that are substantially less. For binary-trees, our hardware method manages to cut nearly 60\% of the memory stall time even when the traversal path is not known a priori. However, when the branching factor of the tree is too high, our technique does not improve performance. Another contribution of the paper is that we quantify pointer-chasing found in interesting applications such as OLTP, Expert Systems, DSS, and JAVA codes and discuss which prefetching techniques are relevant to use in each case },
}

@inproceedings{4147638,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {vii--vii},
 publisher = {IEEE},
 title = {Organizing Committee},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346175},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147638},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147638.pdf?arnumber=4147638},
 isbn = {1-4244-0805-9},
 language = {English},
 abstract = {},
}

@inproceedings{4147639,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {vii--vii},
 publisher = {IEEE},
 title = {Conference committee},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346176},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147639},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147639.pdf?arnumber=4147639},
 isbn = {1-4244-0805-9},
 language = {English},
 abstract = {},
}

@inproceedings{386557,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Chunming Qiao and Melhem, R.},
 year = {1995},
 pages = {34--43},
 publisher = {IEEE},
 title = {Reducing communication latency with path multiplexing: in optically interconnected multiprocessor systems},
 date = {1995},
 doi = {10.1109/HPCA.1995.386557},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386557},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386557.pdf?arnumber=386557},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Bandwidth, Communication switching, Delay effects, Multiprocessing systems, Multiprocessor interconnection networks, Optical interconnections, Scalability, Time division multiplexing, Wavelength division multiplexing, Wiring, all-optical time division multiplexed communications, communication complexity, communication latency, multiprocessor interconnection networks, optical interconnections, optically interconnected multiprocessor systems, path multiplexing, switching devices, time division multiplexing, time-slot interchanging switching devices, virtual link, virtual links, },
 abstract = {A physical link can be time-multiplexed to create several time slots, each of which corresponding to a virtual link. A conventional approach establishes a connection along a path using a set of independent time slots (or virtual links) and thus requires the use of switching devices capable of interchanging time slots. This paper proposes a different approach to all-optical Time Division Multiplexed (TDM) communications in multiprocessor systems. The idea is to establish a connection along a path using a set of time slots (or virtual links) that are dependent on each other, so that no time-slot interchanging is required. It is found that, despite of the possibility that establishing a connection may take a longer time, the proposed approach will result in lower overall communication latency as it eliminates the delays introduced by the time-slot interchanging switching devices },
}

@inproceedings{386556,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Cappello, F. and Germain, C.},
 year = {1995},
 pages = {44--53},
 publisher = {IEEE},
 title = {Toward high communication performance through compiled communications on a circuit switched interconnection network},
 date = {1995},
 doi = {10.1109/HPCA.1995.386556},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386556},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386556.pdf?arnumber=386556},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Bandwidth, Communication networks, Computer networks, Concurrent computing, Delay, Hardware, Multiprocessor interconnection networks, Parallel architectures, Pattern analysis, Performance analysis, application features, circuit switched interconnection network, communication complexity, communication networks, communication pattern, compiled communications, high communication performance, massively parallel architectures, multiprocessor interconnection networks, network load, numerical computation, parallel architectures, },
 abstract = {This paper discusses a new principle of interconnection network for massively parallel architectures in the field of numerical computation. The principle is motivated by an analysis of the application features and the need to design new kind of communication networks combining very high bandwidth, very low latency, performance independence to communication pattern or network load and a performance improvement proportional to the hardware performance improvement. Our approach is to associate compiled communications and a circuit switched interconnection network. This paper presents the motivations for this principle, the hardware and software issues and the design of a first prototype. The expected performance are a sustained aggregate bandwidth of more than 500 GBytes/s and an overall latency less than 270 ns, for a large implementation (4K inputs) with the current available technology },
}

@inproceedings{386555,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Sivasubramaniam, A. and Singla, M. and Ramachandran, U. and Venkateswaran, H.},
 year = {1995},
 pages = {54--63},
 publisher = {IEEE},
 title = {Abstracting network characteristics and locality properties of parallel systems},
 date = {1995},
 doi = {10.1109/HPCA.1995.386555},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386555},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386555.pdf?arnumber=386555},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Analytical models, Computational modeling, Concurrent computing, Context modeling, Delay, Educational institutions, Hardware, LogP model, Multiprocessor interconnection networks, Network topology, Performance analysis, cache storage, cache-coherent shared memory multiprocessors, coherence maintenance, communication characteristics, communication locality, contention overhead, execution-driven simulators, hardware induced artifacts, interconnection network, latency overhead, locality properties, message passing, multiprocessor interconnection networks, network characteristics, parallel architectures, parallel systems, performance evaluation, shared memory systems, virtual machines, },
 abstract = {Abstracting features of parallel systems is a technique that has been traditionally used in theoretical and analytical models for program development and performance evaluation. We explore the use of abstractions in execution-driven simulators in order to speed up simulation. In particular, we evaluate abstractions for the interconnection network and locality, properties of parallel systems in the context of simulating cache-coherent shared memory (CC-NUMA) multiprocessors. We use the recently proposed LogP model to abstract the network. We abstract locality by modeling a cache at each processing node in the system which is maintained coherent, without modeling the overheads associated with coherence maintenance. Such an abstraction tries to capture the true communication characteristics of the application without modeling any hardware induced artifacts. Using a suite of applications and three network topologies simulated on a novel simulation platform, we show that the latency overhead modeled by LogP is fairly accurate. On the other hand, the contention overhead can become pessimistic when the applications display sufficient communication locality. Our abstraction for data locality closely models the behavior of the target system over the chosen range of applications. The simulation model which incorporated these abstractions was around 250-300\% faster than the simulation of the target machine },
}

@inproceedings{386554,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Dahlgren, F. and Stenstrom, P.},
 year = {1995},
 pages = {68--77},
 publisher = {IEEE},
 title = {Effectiveness of hardware-based stride and sequential prefetching in shared-memory multiprocessors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386554},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386554},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386554.pdf?arnumber=386554},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Application software, Bandwidth, Coherence, Contracts, Counting circuits, Delay, Hardware, Large-scale systems, Prefetching, Read-write memory, cache storage, communication complexity, hardware-based sequential prefetching, hardware-based stride prefetching, memory-system bandwidth, multiprocessor interconnection networks, read misses, read-miss penalties, shared memory systems, shared-memory multiprocessors, storage management, },
 abstract = {We study the relative efficiency of previously proposed stride and sequential prefetching-two promising hardware-based prefetching schemes to reduce read-miss penalties in shared-memory multiprocessors. Although stride accesses dominate in four out of six of the applications we study, we find that sequential prefetching does better than stride prefetching for three applications. This is because (i) most strides are shorter than the block size (we assume 32 byte blocks), which means that sequential prefetching is as effective for stride accesses, and (ii) sequential prefetching also exploits the locality of read misses for non-stride accesses. However we find that since stride prefetching causes fewer useless prefetches, it consumes less memory-system bandwidth },
}

@inproceedings{386553,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Farkas, K.I. and Jouppi, N.P. and Chow, P.},
 year = {1995},
 pages = {78--89},
 publisher = {IEEE},
 title = {How useful are non-blocking loads, stream buffers and speculative execution in multiple issue processors?},
 date = {1995},
 doi = {10.1109/HPCA.1995.386553},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386553},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386553.pdf?arnumber=386553},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computational modeling, Costs, Delay, Educational institutions, Hardware, Microprocessors, Multiflow compiler, Prefetching, Processor scheduling, Registers, Runtime, SPEC92 benchmarks, associative cache, cache storage, fetch latency, multiple issue processors, nonblocking loads, parallel architectures, performance evaluation, reduced instruction set computing, speculative execution, statically scheduled quad-issue processor model, stream buffers, virtual machines, },
 abstract = {We investigate the relative performance impact of non-blocking loads, stream buffers, and speculative execution both used individually and in conjunction with each other. We have simulated the SPEC92 benchmarks on a statically scheduled quad-issue processor model, running code from the Multiflow compiler. Non-blocking loads and stream buffers both provide a significant performance advantage, and their combination performs significantly better than either alone. For example, with a 64-byte, 2-way set associative cache with 32 cycle fetch latency, non-blocking loads reduce the run-time by 21\% while stream-buffers reduce it by 26\%, and the combined use of the two yields a 47\% reduction. The addition of speculative execution further improves the performance of the systems that we have simulated, with or without non-blocking loads and stream buffers, by an additional 20\% to 4O\%. We expect that the use of all three of these techniques will be important in future generations of microprocessors },
}

@inproceedings{386552,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Citron, D. and Rudolph, L.},
 year = {1995},
 pages = {90--99},
 publisher = {IEEE},
 title = {Creating a wider bus using caching techniques},
 date = {1995},
 doi = {10.1109/HPCA.1995.386552},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386552},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386552.pdf?arnumber=386552},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {16 bit, 32 bit, Bandwidth, Compaction, Computer aided instruction, Computer science, Data compression, Degradation, Microprocessors, Pins, System buses, Wires, cache storage, cache-based processor, caching techniques, data compression, data compression techniques, effective bandwidth, external communication ports, memory module, system buses, },
 abstract = {The effective bandwidth of a bus and external communication ports can be increased by using a variant of data compression techniques that compacts words instead of data streams. The compaction is performed by caching the high order bits into a table and sending the index into the table along with the low order bits. A coherent table at the receiving end expands the word into it original form. Compaction/expansion units can be placed between processor and memory, between processor and local bus, and between devices that access the system bus. Simulations have shown that over 90\% of all informative transferred can be sent in a single cycle when using a 32 bit processor connected by a 16 bit wide bus to a 32 bit memory module. This is for all forms of data, address, data, and instructions, and when a cache-based processor is used },
}

@inproceedings{386551,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Libeskind-Hadas, R. and Brandt, E.},
 year = {1995},
 pages = {102--111},
 publisher = {IEEE},
 title = {Origin-based fault-tolerant routing in the mesh},
 date = {1995},
 doi = {10.1109/HPCA.1995.386551},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386551},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386551.pdf?arnumber=386551},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = { concurrency control,  deadlock,  distributed memory systems,  fault tolerant computing,  fault-tolerant routing algorithms,  livelock,  message passing,  multiprocessor interconnection networks,  n-dimensional meshes,  origin-based fault-tolerant routing,  reliability,  virtual channels, Computer science, Concurrent computing, Delay, Distributed computing, Educational institutions, Fault tolerance, Radio access networks, Routing, System recovery, Topology, },
 abstract = {The ability to tolerate faults is critical in multi-computers employing large numbers of processors. This paper describes a class of fault-tolerant routing algorithms for n-dimensional meshes that can tolerate large numbers of faults without using virtual channels. We show that these routing algorithms prevent livelock and deadlock while remaining highly adaptive },
}

@inproceedings{386550,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Upadhyay, J.H. and Varavithya, V. and Mohapatra, P.},
 year = {1995},
 pages = {112--121},
 publisher = {IEEE},
 title = {Efficient and balanced adaptive routing in two-dimensional meshes },
 date = {1995},
 doi = {10.1109/HPCA.1995.386550},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386550},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386550.pdf?arnumber=386550},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Adaptive algorithm, Concurrent computing, Delay, Distributed computing, Intelligent networks, Multiprocessor interconnection networks, Routing, System performance, Telecommunication traffic, Throughput, adaptive routing, multiprocessor interconnection networks, network latency, performance, performance evaluation, routing algorithms, telecommunication network routing, throughput, two-dimensional meshes, uneven workload, wormhole networks, },
 abstract = {In this paper, we present a new concept of region of adaptivity with respect to various routing algorithms in wormhole networks. Using this concept, we demonstrate that the previously proposed routing algorithms, though more adaptive, causes an uneven workload in the network which limits the performance improvement. A is observed that balanced distribution of traffic has greater impact on system performance than the adaptivity or efficiency of the algorithm. Based on these motivating factors, we have presented a new fully adaptive routing algorithm for 2-dimensional meshes using one extra virtual channel. The algorithm is more efficient in terms of the number of paths it offers between the source and the destination and also distributes the network load more evenly and symmetrically. The simulation results are presented and are compared with the results of previously proposed algorithms. It is shown that the proposed algorithm results in much better performance in terms of the average network latency and the throughput },
}

@inproceedings{744317,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Durbhakula, M. and Pai, V.S. and Adve, S.},
 year = {1999},
 pages = {23--32},
 publisher = {IEEE},
 title = {Improving the accuracy vs. speed tradeoff for simulating shared-memory multiprocessors with ILP processors},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744317},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744317},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744317.pdf?arnumber=744317},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Computational modeling, Computer architecture, Computer simulation, Costs, DirectRSIM, High performance computing, ILP processors, accuracy vs. speed tradeoff, direct execution, error, instruction-level parallelism features, performance evaluation, shared memory systems, shared-memory architectures, shared-memory multiprocessor simulation, simple-processor based simulators, simulation accuracy, simulation speed, virtual machines, },
 abstract = {Previous simulators for shared-memory architectures have imposed a large tradeoff between simulation accuracy and speed. Most such simulators model simple processors that do not exploit common instruction-level parallelism (ILP) features, consequently exhibiting large errors when used to model current systems. A few newer simulators model current ILP processors in detail, but we find them to be about ten times slower. We propose a new simulation technique, based on a novel adaptation of direct execution, that alleviates this accuracy vs. speed tradeoff. We compare the speed and accuracy of our new simulator, DirectRSIM, with three other simulators-RSIM (a detailed simulator for multiprocessors with ILP processors) and two representative simple-processor based simulators. Compared to RSIM, on average, DirectRSIM is 3.6 times faster and exhibits a relative error of only 1.3\% in total execution time. Compared to the simple-processor based simulators, DirectRSIM is far superior in accuracy, and yet is only 2.7 times slower },
}

@inproceedings{501169,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Karlsson, M. and Stenstrom, P.},
 year = {1996},
 pages = {4--13},
 publisher = {IEEE},
 title = {Performance evaluation of a cluster-based multiprocessor built from ATM switches and bus-based multiprocessor servers},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501169},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501169},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501169.pdf?arnumber=501169},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {ATM switches, Asynchronous transfer mode, Bandwidth, Delay, Fabrics, Network servers, Paper technology, Processor scheduling, Switches, Web server, Workstations, architectural simulations, asynchronous transfer mode, bus-based multiprocessor servers, cluster-based multiprocessor, distributed virtual shared memory, network of workstations, network servers, performance evaluation, performance evaluation, scheduling policies, shared memory systems, shared-memory model, virtual storage, },
 abstract = {We consider a network of workstations (NOW) organization consisting of a number of bus-based multiprocessor servers interconnected by an ATM switch. A shared-memory model is supported by distributed virtual shared memory (DVSM) and this paper focuses on the access penalties incurred by (1) ATM and (2) the DVSM software. First, through detailed architectural simulations we find that while the bandwidth and the latency of the ATM switch fabrics are found to be acceptable, the latency incurred by commercially available ATM interfaces has a first order effect on the performance. We also study the effects of various scheduling policies for the coherence handlers. Our data suggest that since the probability of finding an idle processor within a cluster is high, a good policy is to schedule it there instead of letting an extra compute processor execute coherence handlers. Overall, by adjusting the adaptation layer of ATM to a DVSM system we find that ATM is a promising technology for these kinds of systems },
}

@inproceedings{744314,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Brooks, D. and Martonosi, M.},
 year = {1999},
 pages = {13--22},
 publisher = {IEEE},
 title = {Dynamically exploiting narrow width operands to improve processor power and performance},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744314},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744314},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744314.pdf?arnumber=744314},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {64 bit, 64 bit word widths, ALU, CPUs, Dynamic range, Electronic switching systems, Energy consumption, Instruction sets, MediaBench benchmark suite, Merging, Microprocessors, Program processors, Programming profession, Proposals, SPECint95 benchmark suite, Sun, addressing needs, aggressive clock gating, dynamic narrow width operand exploitation, general-purpose microprocessors, hardware mechanisms, instruction set, instruction sets, integer problems, multimedia instruction set extensions, parallel processing, performance evaluation, performance improvement, power consumption, power-oriented optimization, processor power improvement, sub-word operations, },
 abstract = {In general-purpose microprocessors, recent trends have pushed towards 64 bit word widths, primarily to accommodate the large addressing needs of some programs. Many integer problems, however, rarely need the full 64 bit dynamic range these CPUs provide. In fact, another recent instruction set trend has been increased support for sub-word operations (that is, manipulating data in quantities less than the full word size). In particular, most major processor families have introduced ``multimedia" instruction set extensions that operate in parallel on several sub-word quantities in the same ALU. This paper notes that across the SPECint95 benchmarks, over half of the integer operation executions require 16 bits or less. With this as motivation, our work proposes hardware mechanisms that dynamically recognize and capitalize on these ``narrow-bitwidth" instances. Both optimizations require little additional hardware, and neither requires compiler support. The first, power-oriented, optimization reduces processor power consumption by using aggressive clock gating to turn off portions of integer arithmetic units that will be unnecessary for narrow bitwidth operations. This optimization results in an over 50\% reduction in the integer unit's power consumption for the SPECint95 and MediaBench benchmark suites. The second optimization improves performance by merging together narrow integer operations and allowing them to share a single functional unit. Conceptually akin to a dynamic form of MMX, this optimization offers speedups of 4.3\%-6.2\% for SPECint95 and 8.0\%-10.4\% for MediaBench },
}

@inproceedings{744311,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Nakra, T. and Gupta, R. and Soffa, M.L.},
 year = {1999},
 pages = {4--12},
 publisher = {IEEE},
 title = {Global context-based value prediction},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744311},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744311},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744311.pdf?arnumber=744311},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Accuracy, Computer aided instruction, Computer architecture, Computer science, Concurrent computing, Data mining, Hardware, History, Parallel processing, Upper bound, data dependencies, dependent instructions, execution speed-up, fetch stage, global context information, global context-based value prediction, instruction computed value prediction, instruction execution path, last value predictors, parallel architectures, parallel programming, programs, stride predictors, table sizes, },
 abstract = {Various methods for value prediction have been proposed to overcome the limits imposed by data dependencies within programs. Using a value prediction scheme, an instruction's computed value is predicted during the fetch stage and forwarded to all dependent instructions to speed up execution. Value prediction schemes have been based on a local context by predicting values using the values generated by the same instruction. This paper presents techniques that predict values of an instruction based on a global context where the behavior of other instructions is used in prediction. The global context includes the path along which an instruction is executed and the values computed by other previously completed instructions. We present techniques that augment conventional last value and stride predictors with global context information. Experiments performed using path-based techniques with realistic table sizes resulted in an increase in prediction of 6.4-8.4\% over the current prediction schemes. Prediction using values computed by other instructions resulted in a further improvement of 7.2\% prediction accuracy over the best path-based predictor },
}

@inproceedings{501168,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {},
 year = {1996},
 publisher = {IEEE},
 title = {Proceedings. Second International Symposium on High-Performance Computer Architecture},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501168},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501168},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501168.pdf?arnumber=501168},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {cache protocols, cache storage, computer architecture, computer networks, high performance computer architecture, high performance processors, instruction scheduling, instruction sets, interconnection networks, multiprocessor interconnection networks, network interfaces, network interfaces, network routing, shared memory multiprocessors, shared memory systems, workstation networks, },
 abstract = {The following topics were dealt with: high performance computer architecture; workstation networks; instruction scheduling; shared memory multiprocessors; interconnection networks; network interfaces; network routing; caches; high performance processors; and cache protocols },
}

@inproceedings{386549,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Cunningham, C.M. and Avresky, D.R.},
 year = {1995},
 pages = {122--131},
 publisher = {IEEE},
 title = {Fault-tolerant adaptive routing for two-dimensional meshes},
 date = {1995},
 doi = {10.1109/HPCA.1995.386549},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386549},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386549.pdf?arnumber=386549},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Computational modeling, Computer science, Concurrent computing, Fault tolerance, Heuristic algorithms, Intel Paragon, Intelligent networks, Routing, System recovery, Terminology, Throughput, XY routing, adaptive routing, deterministic XY wormhole routing, fault tolerance, fault tolerant computing, multiprocessor interconnection networks, network routing, routing adaptability, routing strategies, two-dimensional meshes, },
 abstract = {Many massively parallel computers in use today utilize simple deterministic XY wormhole routing to transmit messages between nodes. Because XY routing does not provide any routing adaptability, it lacks the ability to avoid congested links, as well as faults. Therefore, the focus of this paper will be two-fold: improving the performance of wormhole routing and providing fault tolerance for up to N-1 faults in an N\&times;N two-dimensional mesh. A simulation model based on the Intel Paragon is presented that compares several known routing strategies with the proposed strategy to illustrate how local state information can be used to provide a potential network throughput improvement of up to 20\%, while achieving fault tolerance },
}

@inproceedings{5416637,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Yaozu Dong and Xiaowei Yang and Xiaoyong Li and Jianhui Li and Kun Tian and Haibing Guan},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {High performance network virtualization with SR-IOV},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416637},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416637},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416637.pdf?arnumber=5416637},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {CPU overhead, CPU utilization, Hardware, Protection, SR-IOV, Scalability, Software performance, System performance, Throughput, Virtual Machine, Virtual machine monitors, Virtual machining, Virtual manufacturing, Virtualization, Voice mail, Xen, generic virtualization architecture, high performance network virtualization, paravirtualization, paravirtualized network driver, scale network, virtual machine monitors, virtual machines, },
 abstract = {Virtualization poses new challenges to I/O performance. The single-root I/O virtualization (SR-IOV) standard allows an I/O device to be shared by multiple Virtual Machines (VMs), without losing runtime performance. We propose a generic virtualization architecture for SR-IOV devices, which can be implemented on multiple Virtual Machine Monitors (VMMs). With the support of our architecture, the SR-IOV device driver is highly portable and agnostic of underlying VMM. Based on our first implementation of network device driver, we applied several optimizations to reduce virtualization overhead. Then, we carried out comprehensive experiments to evaluate SR-IOV performance and compare it with paravirtualized network driver. The results show SR-IOV can achieve line rate (9.48 Gbps) and scale network up to 60 VMs at the cost of only 1.76\% additional CPU overhead per VM, without sacrificing throughput. It has better throughout, scalability, and lower CPU utilization than paravirtualization. },
}

@inproceedings{1183549,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Hu, Z. and Martonosi, M. and Kaxiras, S.},
 year = {2003},
 pages = { 317-- 326},
 publisher = {IEEE},
 title = {TCP: tag correlating prefetchers},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183549},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183549},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183549.pdf?arnumber=1183549},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { CPU,  L1 cache,  L2 cache,  aggressive out-of-order processor,  branch prediction mechanisms,  cache storage,  dead-block prediction,  main memory,  memory architecture,  microprocessor chips,  multiple address sequences,  per-set tag sequences,  performance,  performance evaluation,  prefetching,  repetitive patterns,  space savings,  speed gap,  tag correlating prefetchers, Delay, Hardware, History, Investments, Out of order, Prefetching, Proposals, Software systems, Spine, Technical Activities Guide -TAG, },
 abstract = {Although caches for decades have been the backbone of the memory system, the speed gap between CPU and main memory suggests their augmentation with prefetching mechanisms. Recently, sophisticated hardware correlating prefetching mechanisms have been proposed, in some cases coupled with some form of dead-block prediction. In many proposals, however correlating prefetchers demand a significant investment in hardware. In this paper we show that correlating prefetchers that work with tags instead of cache-line addresses are significantly more resource-efficient, providing equal or better performance than previous proposals. We support this claim by showing that per-set tag sequences exhibit highly repetitive patterns both within a set and across different sets. Because a single tag sequence can capture multiple address sequences spread over different cache sets, significant space savings can be achieved. We propose a tag-based prefetcher called a tag correlating prefetcher (TCP). Even with very small history tables, TCP outperforms address-based correlating prefetchers many times larger. In addition, we show that such a prefetcher can yield most of its performance benefits if placed at the L2 level of an aggressive out-of-order processor. Only if one wants prefetching all the way up to L1, is dead-block prediction required. Finally, we draw parallels between the two-level structure of TCP and similar structures for branch prediction mechanisms; these parallels raise interesting opportunities for improving correlating memory prefetchers by harnessing lessons already learned for correlating branch predictors. },
}

@inproceedings{1183548,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Memik, G. and Reinman, G. and Mangione-Smith, W.H.},
 year = {2003},
 pages = { 307-- 316},
 publisher = {IEEE},
 title = {Just say no: benefits of early cache miss determination},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183548},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183548},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183548.pdf?arnumber=1183548},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { SPEC2000 applications,  SimpleScalar simulator,  cache storage,  data access times,  delays,  early cache miss determination,  execution time,  latency hiding,  memory architecture,  memory hierarchy,  memory subsystem,  microprocessor chips,  multi-level caches,  performance evaluation,  performance gap,  power consumption,  power consumption,  processor cores, Computer science, Delay, Energy consumption, Microprocessors, },
 abstract = {As the performance gap between the processor cores and the memory subsystem increases, designers are forced to develop new latency hiding techniques. Arguably, the most common technique is to utilize multi-level caches. Each new generation of processors is equipped with higher levels of memory hierarchy with increasing sizes at each level. In this paper, we propose 5 different techniques that will reduce the data access times and power consumption in processors with multi-level caches. Using the information about the blocks placed into and replaced from the caches, the techniques quickly determine whether an access at any cache level will be a miss. The accesses that are identified to miss are aborted. The structures used to recognize misses are much smaller than the cache structures. Consequently the data access times and power consumption are reduced. Using the SimpleScalar simulator, we study the performance of these techniques for a processor with 5 cache levels. The best technique is able to abort 53.1\% of the misses on average in SPEC2000 applications. Using these techniques, the execution time of the applications is reduced by up to 12.4\% (5.4\% on average), and the power consumption of the caches is reduced by as much as 11.6\% (3.8\% on average). },
}

@inproceedings{1183545,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Narayanasamy, S. and Sherwood, T. and Sair, S. and Calder, B. and Varghese, G.},
 year = {2003},
 pages = { 269-- 280},
 publisher = {IEEE},
 title = {Catching accurate profiles in hardware},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183545},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183545},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183545.pdf?arnumber=1183545},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { benchmarks,  cache storage,  caching,  edge profiling,  interval-based profiling,  memory architecture,  microprocessor chips,  multi-hash architecture,  multiple hash tables,  performance,  performance evaluation,  processors,  run-time optimization, Computer architecture, Computer errors, Computer science, Filtering, Hardware, Prefetching, Runtime, Software systems, Software tools, System software, },
 abstract = {Run-time optimization is one of the most important ways of getting performance out of modern processors. Techniques such as prefetching, trace caching, memory disambiguation etc., are all based upon the principle of observation followed by adaptation, and all make use of some sort of profile information gathered at run-time. Programs are very complex, and the real trick in generating useful run-time profiles is sifting through all the unimportant and infrequently occurring events to find those that are important enough to warrant optimization. In this paper, we present the multi-hash architecture to catch important events even in the presence of extensive noise. Multi-hash uses a small amount of area, between 7 to 16 Kilo-bytes, to accurately capture these important events in hardware, without requiring any software support. This is achieved using multiple hash tables for the filtering, and interval-based profiling to help identify how important an event is in relationship to all the other events. We evaluate our design for value and edge profiling, and show that over a set of benchmarks, we get an average error less than 1\%. },
}

@inproceedings{1183544,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Kogge, P.M.},
 year = {2003},
 pages = { 266-- 266},
 publisher = {IEEE},
 title = {The state of state},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183544},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183544},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183544.pdf?arnumber=1183544},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = {Computer architecture, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01183544.png" border="0"> },
}

@inproceedings{1183547,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Gassend, B. and Suh, G.E. and Clarke, D. and van Dijk, M. and Devadas, S.},
 year = {2003},
 pages = { 295-- 306},
 publisher = {IEEE},
 title = {Caches and hash trees for efficient memory integrity verification},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183547},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183547},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183547.pdf?arnumber=1183547},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { cache storage,  certified program execution,  hardware cost,  hash trees,  high performance processor,  memory architecture,  memory integrity verification,  on-processor L2 cache,  performance evaluation,  tree data structures,  untrusted external memory, Application software, Bandwidth, Computer science, Coprocessors, Costs, Hardware, Laboratories, Machinery, Protection, Security, },
 abstract = {We study the hardware cost of implementing hash-tree based verification of untrusted external memory by a high performance processor. This verification could enable applications such as certified program execution. A number of schemes are presented with different levels of integration between the on-processor L2 cache and the hash-tree machinery. Simulations show that for the best of our methods, the performance overhead is less than 25\%, a significant decrease from the 10\&times; overhead of a naive implementation. },
}

@inproceedings{1183546,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Yi, J.J. and Lilja, D.J. and Hawkins, D.M.},
 year = {2003},
 pages = { 281-- 291},
 publisher = {IEEE},
 title = {A statistically rigorous approach for improving simulation methodology},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183546},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183546},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183546.pdf?arnumber=1183546},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { Plackett Burman design,  benchmark classification,  computer architecture,  computer architecture,  key processor parameters,  microprocessor chips,  performance evaluation,  processor performance enhancements,  simulation methodology,  statistical analysis,  statistically rigorous approach,  virtual machines, Analytical models, Application software, Computational modeling, Computer architecture, Computer simulation, Costs, Performance analysis, Space exploration, Statistical analysis, Time factors, },
 abstract = {Due to cost, time, and flexibility constraints, simulators are often used to explore the design space when developing new processor architectures, as well as when evaluating the performance of new processor enhancements. However, despite this dependence on simulators, statistically rigorous simulation methodologies are not typically used in computer architecture research. A formal methodology can provide a sound basis for drawing conclusions gathered from simulation results by adding statistical rigor, and consequently, can increase confidence in the simulation results. This paper demonstrates the application of a rigorous statistical technique to the setup and analysis phases of the simulation process. Specifically, we apply a Plackett and Burman design to: (1) identify key processor parameters; (2) classify benchmarks based on how they affect the processor; and (3) analyze the effect of processor performance enhancements. Our technique expands on previous work by applying a statistical method to improve the simulation methodology instead of applying a statistical model to estimate the performance of the processor. },
}

@inproceedings{1183541,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Nagaraja, K. and Krishnan, N. and Bianchini, R. and Martin, R.P. and Nguyen, T.D.},
 year = {2003},
 pages = { 229-- 240},
 publisher = {IEEE},
 title = {Evaluating the impact of communication architecture on the performability of cluster-based services},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183541},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183541},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183541.pdf?arnumber=1183541},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { Internet,  TCP,  VIA,  Web server,  analytic modeling,  availability,  channel resource pre-allocation,  cluster-based servers,  cluster-based services,  communication architecture,  communication protocols,  computer network reliability,  error reporting,  fault rates,  fault tolerant computing,  fault-injection experiments,  intra-cluster communication substrate,  message boundaries,  packet errors,  performability,  performance,  performance evaluation,  single-copy transfers,  transport protocols,  workstation clusters, Availability, Computer architecture, Computer science, Network servers, Operating systems, Performance analysis, Performance evaluation, Protocols, Robustness, Web server, },
 abstract = {We consider the impact of different communication architectures on the performability (performance plus availability) of cluster-based servers. In particular, we use a combination of fault-injection experiments and analytic modeling to evaluate the performability of two popular communication protocols, TCP and VIA, as the intra-cluster communication substrate of a sophisticated Web server. Our analysis leads to several interesting conclusions, the most surprising of which is, under the same fault load, VIA-based servers deliver greater availability than TCP-based servers. If we assume higher fault rates for VIA-based servers because the underlying technology is more immature and programming model more complex, we find that packet errors or application faults would have to occur at approximately 4 times the rate in TCP-based servers before their performabilities equalize. We use our results from the study to suggest that high-performance and robust communication layers for highly available cluster-based servers should preserve message boundaries, as opposed to using byte streams, use single-copy transfers, pre-allocate channel resources, and report errors in manner consistent with the network fabric's fault model. },
}

@inproceedings{1183540,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Karlsson, M. and Moore, K.E. and Hagersten, E. and Wood, D.A.},
 year = {2003},
 pages = { 217-- 228},
 publisher = {IEEE},
 title = {Memory system behavior of Java-based middleware},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183540},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183540},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183540.pdf?arnumber=1183540},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { ECperf,  Java,  Java-based middleware,  SPECjbb,  Simics full-system simulation,  cache storage,  chip multiprocessor,  commercial server hardware,  intermediate-size instruction caches,  memory architecture,  memory footprint,  memory system behavior,  middleware,  multiprocessor memory systems,  performance evaluation,  primary working sets,  shared caches,  shared memory systems,  virtual machines, Application software, Data analysis, Databases, Decision support systems, Hardware, Information technology, Java, Middleware, Monitoring, Virtual machining, },
 abstract = {In this paper, we present a detailed characterization of the memory system, behavior of ECperf and SPECjbb using both commercial server hardware and Simics full-system simulation. We find that the memory footprint and primary working sets of these workloads are small compared to other commercial workloads (e.g. on-line transaction processing), and that a large fraction of the working sets are shared between processors. We observed two key differences between ECperf and SPECjbb that highlight the importance of isolating the behavior of the middle tier. First, ECperf has a larger instruction footprint, resulting in much higher miss rates for intermediate-size instruction caches. Second, SPECjbb's data set size increases linearly as the benchmark scales up, while ECperf's remains roughly constant. This difference can lead to opposite conclusions on the design of multiprocessor memory systems, such as the utility of moderate sized (i.e. 1 MB) shared caches in a chip multiprocessor. },
}

@inproceedings{1183543,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Eun Jung Kim and Ki Hwan Yum and Das, C.R. and Yousif, M. and Duato, J.},
 year = {2003},
 pages = { 253-- 262},
 publisher = {IEEE},
 title = {Performance enhancement techniques for InfiniBandTM Architecture},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183543},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183543},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183543.pdf?arnumber=1183543},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { IBA specification,  InfiniBand Architecture,  SAN,  SPF algorithm,  clusters,  concurrency control,  congestion minimization,  customized applications,  deadlock,  design tradeoffs,  deterministic algorithms,  deterministic packet routing,  integrated workload,  local area networks,  multicast communication,  multicasting support,  multipath routing mechanism,  packet switching,  performance enhancement techniques,  performance evaluation,  selective packet dropping,  shortest path first algorithm,  simulation testbed,  system area networks,  telecommunication congestion control,  telecommunication network routing, Clustering algorithms, Communication system traffic control, Computer architecture, Delay, Multicast algorithms, OFDM modulation, Routing, Switches, System recovery, Traffic control, },
 abstract = {The InfiniBand<sup>TM</sup> Architecture (IBA) is envisioned to be the default communication fabric for future system area networks (SAN). However, the released IBA specification outlines only higher level functionalities, leaving it open for exploring various design alternatives. In this paper we investigate four co-related techniques to provide high and predictable performance in IBA. These are: (i) using the shortest path first (SPF) algorithm for deterministic packet routing; (ii) developing a multipath routing mechanism for minimizing congestion; (iii) developing a selective packet dropping scheme to handle deadlock and congestion; and (iv) providing multicasting support for customized applications. These designs are evaluated using an integrated workload on a versatile IBA simulation testbed. Simulation results indicate that the SPF routing, multipath routing, packet dropping, and multicasting schemes are quite effective in delivering high and assured performance in clusters. One of the major contributions of this research is the IBA simulation testbed, which is an essential tool to evaluate various design tradeoffs. },
}

@inproceedings{1183542,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Radovic, Z. and Hagersten, E.},
 year = {2003},
 pages = { 241-- 252},
 publisher = {IEEE},
 title = {Hierarchical backoff locks for nonuniform communication architectures},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183542},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183542},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183542.pdf?arnumber=1183542},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { CC-NUMA,  NUCA,  benchmarks,  cache storage,  chip multiprocessors,  contended locks,  hierarchical backoff locks,  lock contention,  lock handover time,  neighbor cache,  node affinity,  nonuniform communication architectures,  parallel architectures,  performance evaluation,  scalable general-purpose locks,  shared memory systems,  software-based HBO,  starvation,  uncontested locks, Application software, Computer architecture, Information technology, Multithreading, Proposals, Scalability, Surface-mount technology, Testing, Traffic control, Yarn, },
 abstract = {This paper identifies node affinity as an important property for scalable general-purpose locks. Nonuniform communication architectures (NUCA), for example CC-NUMA built from a few large nodes or from chip multiprocessors (CMP), have a lower penalty for reading data from a neighbor's cache than from a remote cache. Lock implementations that encourages handing over locks to neighbors will improve the lock handover time, as well as the access to the critical data guarded by the lock, but will also be vulnerable to starvation. We propose a set of simple software-based hierarchical backoff locks (HBO) that create node affinity in NUCA. A solution for lowering the risk of starvation is also suggested. The HBO locks are compared with other software-based lock implementations using simple benchmarks, and are shown to be very competitive for uncontested locks while being more than twice as fast for contended locks. An application study also demonstrates superior performance for applications with high lock contention and competitive performance for other programs. },
}

@inproceedings{4147647,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Katayama, Y. and Okazaki, A.},
 year = {2007},
 pages = {46--50},
 publisher = {IEEE},
 title = {Optical Interconnect Opportunities for Future Server Memory Systems},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346184},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147647},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147647.pdf?arnumber=4147647},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Acceleration, Bandwidth, Blades, Cache memory, Degradation, Laboratories, Memory architecture, Multicore processing, Optical fiber communication, Optical interconnections, bandwidth allocation, bandwidth-distance product, memory architecture, multicore CPU generation, multiprocessor interconnection networks, optical interconnect technology, optically-attached memory systems, physically-remote main memory, server memory architecture, server memory system, ultra-high-bandwidth parallel optical interconnect, },
 abstract = {This paper deals with alternative server memory architecture options in multicore CPU generations using optically-attached memory systems. Thanks to its large bandwidth-distance product, optical interconnect technology enables CPUs and local memory to be placed meters away from each other without sacrificing bandwidth. This topologically-local but physically-remote main memory attached via an ultra-high-bandwidth parallel optical interconnect can lead to flexible memory architecture options using low-cost commodity memory technologies },
}

@inproceedings{4147646,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Annavaram, M. and Grochowski, E. and Reed, P.},
 year = {2007},
 pages = {37--45},
 publisher = {IEEE},
 title = {Implications of Device Timing Variability on Full Chip Timing},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346183},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147646},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147646.pdf?arnumber=4147646},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Databases, Degradation, Delay, Educational institutions, Etching, Fluctuations, Intel Core Duo microprocessor design database, Microarchitecture, Microprocessors, Probability distribution, Timing, device timing variability, full chip timing, functional block level, logic design, microprocessor chips, parallel architectures, path timing margins, probability distribution, random processes, random timing variation, random variation, statistical distributions, systematic timing variation, timing, },
 abstract = {As process technologies continue to scale, the magnitude of within-die device parameter variations is expected to increase and may lead to significant timing variability. This paper presents a quantitative evaluation of how low level device timing variations impact the timing at the functional block level. We evaluate two types of timing variations: random and systematic variations. The study introduces random and systematic timing variations to several functional blocks in Intelreg Coretrade Duo microprocessor design database and measures the resulting timing margins. The primary conclusion of this research is that as a result of combining two probability distributions (the distribution of the random variation and the distribution of path timing margins) functional block timing margins degrade non-linearly with increasing variability },
}

@inproceedings{4147645,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Hongtao Zhong and Lieberman, S.A. and Mahlke, S.A.},
 year = {2007},
 pages = {25--36},
 publisher = {IEEE},
 title = {Extending Multicore Architectures to Exploit Hybrid Parallelism in Single-thread Applications},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346182},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147645},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147645.pdf?arnumber=4147645},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Computer architecture, Concurrent computing, Data structures, Drives, Multicore processing, Parallel processing, Performance gain, Power dissipation, Throughput, VLIW, Voltron architecture, Yarn, associated compiler support, chip multiprocessors, hybrid parallelism, intercore communication, microprocessor chips, multi-threading, multicore architecture, multithreaded, parallel architectures, program compilers, single-thread application, },
 abstract = {Chip multiprocessors with multiple simpler cores are gaining popularity because they have the potential to drive future performance gains without exacerbating the problems of power dissipation and complexity. Current chip multiprocessors increase throughput by utilizing multiple cores to perform computation in parallel. These designs provide real benefits for server-class applications that are explicitly multi-threaded. However, for desktop and other systems where single-thread applications dominate, multicore systems have yet to offer much benefit. Chip multiprocessors are most efficient at executing coarse-grain threads that have little communication. However, general-purpose applications do not provide many opportunities for identifying such threads, due to frequent use of pointers, recursive data structures, if-then-else branches, small function bodies, and loops with small trip counts. To attack this mismatch, this paper proposes a multicore architecture, referred to as Voltron that extends traditional multicore systems in two ways. First, it provides a dual-mode scalar operand network to enable efficient inter-core communication and lightweight synchronization. Second, Voltron can organize the cores for execution in either coupled or decoupled mode. In coupled mode, the cores execute multiple instruction streams in lock-step to collectively function as a wide-issue VLIW. In decoupled mode, the cores execute a set of fine-grain communicating threads extracted by the compiler. This paper describes the Voltron architecture and associated compiler support for orchestrating bi-modal execution },
}

@inproceedings{4147644,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Ranger, C. and Raghuraman, R. and Penmetsa, A. and Bradski, G. and Kozyrakis, C.},
 year = {2007},
 pages = {13--24},
 publisher = {IEEE},
 title = {Evaluating MapReduce for Multi-core and Multiprocessor Systems},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346181},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147644},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147644.pdf?arnumber=4147644},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Concurrent computing, Dynamic scheduling, Fault tolerance, Laboratories, MapReduce, Multiprocessing systems, Parallel programming, Phoenix, Processor scheduling, Programming profession, Runtime, Yarn, data partitioning, distributed system, dynamic task scheduling, error recovery, fault tolerance, fault tolerance, functional-style code, multi-threading, multicore system, multiprocessing systems, multiprocessor system, performance evaluation, programming API and, scheduling, shared-memory systems, thread creation, },
 abstract = {This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automatically parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix runtime automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful implementation, MapReduce is a promising model for scalable performance on shared-memory systems with simple parallel code },
}

@inproceedings{4147643,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Dybdahl, H. and Stenstrom, P.},
 year = {2007},
 pages = {2--12},
 publisher = {IEEE},
 title = {An Adaptive Shared/Private NUCA Cache Partitioning Scheme for Chip Multiprocessors},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346180},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147643},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147643.pdf?arnumber=4147643},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Bandwidth, Degradation, Delay, Microprocessors, Pollution, Size control, adaptive shared/private NUCA cache partitioning, cache performance, cache storage, chip memory bandwidth, chip multiprocessors, microprocessor chips, nonuniform cache architectures, parallel architectures, resource sharing, shared cache, shared memory systems, },
 abstract = {The significant speed-gap between processor and memory and the limited chip memory bandwidth make last-level cache performance crucial for future chip multiprocessors. To use the capacity of shared last-level caches efficiently and to allow for a short access time, proposed non-uniform cache architectures (NUCAs) are organized into per-core partitions. If a core runs out of cache space, blocks are typically relocated to nearby partitions, thus managing the cache as a shared cache. This uncontrolled sharing of all resources may unfortunately result in pollution that degrades performance. We propose a novel non-uniform cache architecture in which the amount of cache space that can be shared among the cores is controlled dynamically. The adaptive scheme estimates, continuously, the effect of increasing/decreasing the shared partition size on the overall performance. We show that our scheme outperforms a private and shared cache organization as well as a hybrid NUCA organization in which blocks in a local partition can spill over to neighbor core partitions },
}

@inproceedings{4147642,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Dally, W.J.},
 year = {2007},
 pages = {1--1},
 publisher = {IEEE},
 title = {Interconnect-Centric Computing},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346179},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147642},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147642.pdf?arnumber=4147642},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Commercialization, Computer architecture, Computer networks, Computer science, Concurrent computing, Cray BlackWidow, Multiprocessor interconnection networks, Network topology, Network-on-a-chip, Routing, System-on-a-chip, computer system, interconnect-centric computing, interconnection network architecture, multicore chips, multiprocessor interconnection networks, network design, on-chip interconnection networks, parallel architectures, system-on-chip, },
 abstract = {Summary form only given. As we enter the many-core era, the interconnection networks of a computer system, rather than the processor or memory modules, will dominate its performance. Several recent developments in interconnection network architecture including global adaptive routing, high-radix routers, and technology-matched topologies offer large improvements in the performance and efficiency of this critical component. The implementation of a portion of several interconnection networks on multi-core chips also raises new opportunities and challenges for network design. This talk explores the role of interconnection networks in modern computer systems, recent developments in network architecture and design, and the challenges of on-chip interconnection networks. Examples will be drawn from several systems including the Cray BlackWidow },
}

@inproceedings{4147641,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {ix--xi},
 publisher = {IEEE},
 title = {Table of Contents},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346178},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147641},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147641.pdf?arnumber=4147641},
 isbn = {1-4244-0805-9},
 language = {English},
 abstract = {},
}

@inproceedings{4147640,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {},
 year = {2007},
 pages = {viii--viii},
 publisher = {IEEE},
 title = {External Reviewers},
 date = {Feb.  2007},
 doi = {10.1109/HPCA.2007.346177},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147640},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147640.pdf?arnumber=4147640},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {IEEE, },
 abstract = {},
}

@inproceedings{4147649,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Quinones, E. and Parcerisa, J.-M. and Gonzalez, A.},
 year = {2007},
 pages = {75--84},
 publisher = {IEEE},
 title = {Improving Branch Prediction and Predicated Execution in Out-of-Order Processors},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346186},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147649},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147649.pdf?arnumber=4147649},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Accuracy, Computer aided instruction, Costs, Degradation, Hardware, Instruction sets, Out of order, Pipelines, Proposals, Registers, branch prediction, compare-branch model, if-conversion, out-of-order processors, predicate prediction, predicated ISA, predicated execution, program compilers, },
 abstract = {If-conversion is a compiler technique that reduces the misprediction penalties caused by hard-to-predict branches, transforming control dependencies into data dependencies. Although it is globally beneficial, it has a negative side-effect because the removal of branches eliminates useful correlation information necessary for conventional branch predictors. The remaining branches may become harder to predict. However, in predicated ISAs with a compare-branch model, the correlation information not only resides in branches, but also in compare instructions that compute their guarding predicates. When a branch is removed, its correlation information is still available in its compare instruction. We propose a branch prediction scheme based on predicate prediction. It has three advantages: First, since the prediction is not done on a branch basis but on a predicate define basis, branch removal after if-conversion does not lose any correlation information, so accuracy is not degraded. Second, the mechanism we propose permits using the computed value of the branch predicate when available, instead of the predicted value, thus effectively achieving 100\% accuracy on such early-resolved branches. Third, as shown in previous work, the selective predicate prediction is a very effective technique to implement if-conversion on out-of-order processors, since it avoids the problem of multiple register definitions and reduces the unnecessary resource consumption of nullified instructions. Hence, our approach enables a very efficient implementation of if-conversion for an out-of-order processor, with almost no additional hardware cost, because the same hardware is used to predict the predicates of if-converted code and to predict branches without accuracy degradation },
}

@inproceedings{4147648,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Srinath, S. and Mutlu, O. and Hyesoon Kim and Patt, Y.N.},
 year = {2007},
 pages = {63--74},
 publisher = {IEEE},
 title = {Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346185},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147648},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147648.pdf?arnumber=4147648},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Bandwidth, Clocks, Delay, Energy consumption, Hardware, LRU stack, Negative feedback, PC-based stride prefetcher, Pollution, Prefetching, Random access memory, Runtime, bandwidth-efficiency, cache pollution, data prefetching, dynamic feedback, feedback directed prefetching, global-history-buffer based delta correlation prefetcher, hardware prefetcher, memory bandwidth, storage management, stream-based prefetcher, },
 abstract = {High performance processors employ hardware data prefetching to reduce the negative performance impact of large main memory latencies. While prefetching improves performance substantially on many programs, it can significantly reduce performance on others. Also, prefetching can significantly increase memory bandwidth requirements. This paper proposes a mechanism that incorporates dynamic feedback into the design of the prefetcher to increase the performance improvement provided by prefetching as well as to reduce the negative performance and bandwidth impact of prefetching. Our mechanism estimates prefetcher accuracy, prefetcher timeliness, and prefetcher-caused cache pollution to adjust the aggressiveness of the data prefetcher dynamically. We introduce a new method to track cache pollution caused by the prefetcher at run-time. We also introduce a mechanism that dynamically decides where in the LRU stack to insert the prefetched blocks in the cache based on the cache pollution caused by the prefetcher. Using the proposed dynamic mechanism improves average performance by 6.5\% on 17 memory-intensive benchmarks in the SPEC CPU2000 suite compared to the best-performing conventional stream-based data prefetcher configuration, while it consumes 18.7\% less memory bandwidth. Compared to a conventional stream-based data prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13.6\% higher performance. Our results show that feedback-directed prefetching eliminates the large negative performance impact incurred on some benchmarks due to prefetching, and it is applicable to stream-based prefetchers, global-history-buffer based delta correlation prefetchers, and PC-based stride prefetchers },
}

@inproceedings{824365,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Bosch, R. and Stolte, C. and Stoll, G. and Rosenblum, M. and Hanrahan, P.},
 year = {2000},
 pages = {360--371},
 publisher = {IEEE},
 title = {Performance analysis and visualization of parallel systems using SimOS and Rivet: a case study},
 date = {2000},
 doi = {10.1109/HPCA.2000.824365},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824365},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824365.pdf?arnumber=824365},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Application software, Argus parallel rendering library, Computational modeling, Computer aided software engineering, Computer science, Hardware, Instruments, Operating systems, Performance analysis, Rivet visualization environment, SimOS machine simulator, Timing, Visualization, case study, data aggregation schemes, evolving system, exploratory interaction techniques, flexibility, linear speedup, memory system, operating system, operating systems (computers), parallel applications, parallel programming, parallel systems visualization, performance analysis, process scheduling activity, program visualisation, rendering (computer graphics), scalability, shared memory multiprocessors, shared memory systems, software libraries, software performance evaluation, virtual machines, },
 abstract = {Presents an evolving system for the analysis and visualization of parallel application performance on shared memory multiprocessors. Our system couples SimOS, a complete machine simulator, with Rivet, a powerful visualization environment. This system demonstrates how visualization is necessary to realize the full power of simulation for performance analysis. We identify several features required of the visualization system, including flexibility, exploratory interaction techniques and data aggregation schemes. We demonstrate the effectiveness of this parallel analysis and visualization system with a case study. We developed two visualizations within Rivet to study the Argus parallel rendering library, focusing on the memory system and process scheduling activity of Argus, respectively. Using these visualizations, we uncovered several unexpected interactions between Argus and the underlying operating system. The results of the analysis led to changes that greatly improved its performance and scalability. Argus had previously been unable to scale beyond 26 processors; after analysis and modification, it achieved linear speedup up to 45 processors },
}

@inproceedings{824364,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Cappello, F. and Richard, O. and Etiemble, D.},
 year = {2000},
 pages = {349--359},
 publisher = {IEEE},
 title = {Investigating the performance of two programming models for clusters of SMP PCs},
 date = {2000},
 doi = {10.1109/HPCA.2000.824364},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824364},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824364.pdf?arnumber=824364},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Costs, Counting circuits, Hardware, Hidden Markov models, High-speed networks, Message passing, Myrinet network, NAS benchmarks, Parallel machines, Particle measurements, Personal communication networks, Read only memory, clusters of SMP PCs, dual processor node configurations speedup, hardware counters, high performance networks, hybrid memory model, message passing, message passing, multiprocessing systems, multiprocessors, performance, performance evaluation, programming models, workstation clusters, },
 abstract = {Multiprocessors and high performance networks allow building CLUsters of MultiProcessors (CLUMPs). One distinctive feature over traditional parallel computers is their hybrid memory model (message passing between the nodes and shared memory inside the nodes). We evaluate the performance of a cluster of dual processor PCs connected by a Myrinet network for NAS benchmarks using two programming models: a Single Memory Model based on the MPICH-PM/CLUMP library of the RWCP and a Hybrid Memory Model using MPICH-PM and OpenMP. MPI programs are used as the reference in all experiments involving programming models. We compare dual processor node configurations speedup versus uniprocessor node configurations for each model. We demonstrate that the superiority of one model over the other depends on the features of the applications. In particular, we detail the speedup results from breakdowns of the benchmark execution times and from measurements of hardware counters },
}

@inproceedings{824367,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Radhakrishnan, R. and Vijaykrishnan, N. and John, L.K. and Sivasubramaniam, A.},
 year = {2000},
 pages = {387--398},
 publisher = {IEEE},
 title = {Architectural issues in Java runtime systems},
 date = {2000},
 doi = {10.1109/HPCA.2000.824367},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824367},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824367.pdf?arnumber=824367},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Application software, CPU architectural support, Central Processing Unit, Computer architecture, Computer science, Costs, Hardware, Java, Java, Java Virtual Machine, Java runtime systems, Portable computers, Runtime, SpecJVM98 benchmarks, Virtual machining, cache architectural support, cache storage, computer architecture, computer architecture, costs, efficiency, embedded systems, enabling technologies, hand-held systems, hardware realization, interpretation, just-in-time compilation, performance enhancement, portable Java bytecodes, program compilers, program interpreters, resource-constrained systems, resource-rich servers, smart JIT compiler strategy, synchronisation, synchronization support, virtual machines, },
 abstract = {The Java Virtual Machine (JVM) is the cornerstone of Java technology, and its efficiency in executing portable Java bytecodes is crucial for the success of this technology. Interpretation, just-in-time (JIT) compilation and hardware realization are well-known solutions for JVM, and previous research has proposed optimizations for each of these techniques. However, each technique has its pros and cons and may not be uniformly attractive for all hardware platforms. Instead, an understanding of the architectural implications of JVM implementations with real applications can be crucial to the development of enabling technologies for efficient Java runtime system development on a wide range of platforms (from resource-rich servers to resource-constrained hand-held/embedded systems). Towards this goal, this paper examines architectural issues, from both the hardware and JVM implementation perspectives. It specifically explores the potential of a smart JIT compiler strategy that can dynamically interpret or compile based on associated costs, investigates the CPU and cache architectural support that would benefit JVM implementations, and examines the synchronization support for enhancing performance, using applications from the SpecJVM98 benchmarks },
}

@inproceedings{824366,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Rixner, S. and Dally, W.J. and Khailany, B. and Mattson, P. and Kapasi, U.J. and Owens, J.D.},
 year = {2000},
 pages = {375--386},
 publisher = {IEEE},
 title = {Register organization for media processing},
 date = {2000},
 doi = {10.1109/HPCA.2000.824366},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824366},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824366.pdf?arnumber=824366},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Decision support systems, Virtual reality, arithmetic unit communication, cost reduction, data parallelism, data streams, delay, delays, digital arithmetic, file organisation, hierarchical register organization optimization, image coding, image processing, image synthesis, image understanding, instruction-level parallelism, media processing, memory hierarchy, parallel memories, partitioning, performance, performance degradation, performance evaluation, power dissipation, processor architectures, register architecture taxonomy, register file area, register file partitioning, register storage, },
 abstract = {Processor architectures with tens to hundreds of arithmetic units are emerging to handle media processing applications. These applications, such as image coding, image synthesis and image understanding, require arithmetic rates of up to 10<sup>11</sup> operations per second. As the number of arithmetic units in a processor increases to meet these demands, register storage and communication between the arithmetic units dominate the area, delay and power of the arithmetic units. In this paper, we show that partitioning the register file along three axes reduces the cost of register storage and communication without significantly impacting performance. We develop a taxonomy of register architectures by partitioning across the data-parallel, instruction-level-parallel and memory-hierarchy axes, and by optimizing the hierarchical register organization for operation on streams of data. Compared to a centralized global register file, the most compact of these organizations reduces the register file area, delay and power dissipation of a media processor by factors of 195, 230 and 430 respectively. This reduction in cost is achieved with a performance degradation of only 8\% on a representative set of media processing benchmarks },
}

@inproceedings{824361,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Ramirez, A. and Larriba-Pey, J.Ll. and Valero, M.},
 year = {2000},
 pages = {325--333},
 publisher = {IEEE},
 title = {Trace cache redundancy: red and blue traces},
 date = {2000},
 doi = {10.1109/HPCA.2000.824361},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824361},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824361.pdf?arnumber=824361},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Cache storage, Collaboration, Contracts, Costs, Degradation, Engines, Hardware, Read only memory, Runtime, cache storage, code reordering, hardware mechanisms, hardware resources, implementation cost, parallel processing, performance evaluation, redundancy, software trace cache, trace cache, trace cache mechanism, trace cache redundancy, },
 abstract = {The objective of this paper is to improve the use of the hardware resources of the trace cache mechanism, reducing the implementation cost with no performance degradation. We achieve that by eliminating the replication of traces between the instruction cache and the trace cache. As we show, the trace cache mechanism is generating a high degree of redundancy between the traces stored in the trace cache and those built by the compiler, already present in the instruction cache. Furthermore, code reordering techniques like the software trace cache arrange the basic blocks in a program so that the fall-through path is the most common, effectively increasing this trace redundancy. We propose selective trace storage to avoid trace redundancy between the trace cache and the instruction cache. A simple modification of the fill unit allows the trace cache to store only those traces containing taken branches, which can nor be obtained in a single cycle from the instruction cache. Our results show that selective trace storage and the software trace cache used on a 32 entry trace cache (2 KB) perform as well as a 2048 entry trace cache (128 KB) without the enhancements. This shows that the cooperation between hardware and software is crucial to improve the performance and reduce the requirements of hardware mechanisms in the fetch engine },
}

@inproceedings{824360,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Neefs, H. and Vandierendonck, H. and De Bosschere, K.},
 year = {2000},
 pages = {313--324},
 publisher = {IEEE},
 title = {A technique for high bandwidth and deterministic low latency load/store accesses to multiple cache banks},
 date = {2000},
 doi = {10.1109/HPCA.2000.824360},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824360},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824360.pdf?arnumber=824360},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Bandwidth, Clocks, Data mining, Delay, Information systems, Multiprocessor interconnection networks, Out of order, Prediction algorithms, Read only memory, buffer management, buffer storage, computer architecture, deterministic latency, deterministic low latency load/store accesses, high bandwidth, instruction sets, load/store execution, multiple cache banks, out-of-order processors, performance evaluation, resource conflicts, },
 abstract = {One of the problems in future processors will be the resource conflicts caused by several load/store units competing to access the same cache bank. The traditional approach for handling this case is by introducing buffers combined with a cross-bar. This approach suffers from (i) the non-deterministic latency of a load/store and (ii) the extra latency caused by the cross-bar and the buffer management. A deterministic latency is of the utmost importance for the forwarding mechanism of out-of-order processors because it enables back-to-back operation of instructions. We propose a technique by which we eliminate the buffers and cross-bars from the critical path of the load/store execution. This results in both, a low and a deterministic latency. Our solution consists of predicting which bank is to be accessed. Only in the case of a wrong prediction a penalty results },
}

@inproceedings{824363,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Uysal, M. and Acharya, A. and Saltz, J.},
 year = {2000},
 pages = {337--348},
 publisher = {IEEE},
 title = {Evaluation of active disks for decision support databases},
 date = {2000},
 doi = {10.1109/HPCA.2000.824363},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824363},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824363.pdf?arnumber=824363},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Arm, Bandwidth, Computer science, Databases, Disk drives, Ice, Itemsets, Milling machines, Postal services, SMP-based conventional disk farms, Shape, active databases, active disks, commodity clusters, decision support databases, decision support systems, direct disk-to-disk communication, disk-to-disk communication architecture, performance, performance evaluation, processing power, },
 abstract = {Growth and usage trends for large decision support databases indicate that there is a need for architectures that scale the processing power as the dataset grows. To meet this need, several researchers have recently proposed active disk architectures which integrate substantial processing power and memory into disk units. In this paper, we evaluate Active Disks for decision support databases. First, we compare the performance of Active Disks with that of existing scalable server architectures: SMP-based conventional disk farms and commodity clusters of PCs. Second, we evaluate the impact of several design choices on the performance of Active Disks. We focus on the performance impact of interconnect bandwidth, amount of disk memory and disk-to-disk communication architecture on decision support workloads. Our results show that for identical disks, number of processors and I/O interconnect, Active Disks provide better price/performance than both SMP-based conventional disk farms and commodity cluster. Experiments evaluating the impact of design alternatives in Active Disk architectures indicate that: (1) for configurations up to 64 disks, a dual fibre channel arbitrated loop interconnect is sufficient even for the most communication-intensive decision support tasks: (2) most decision support task do not require a large amount of memory: and (3) direct disk-to-disk communication is necessary for achieving good performance on tasks that repartition all (or a large fraction of) their dataset },
}

@inproceedings{903270,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Annavaram, M. and Patel, J.M. and Davidson, E.S.},
 year = {2001},
 pages = {281--290},
 publisher = {IEEE},
 title = {Call graph prefetching for database applications},
 date = {2001},
 doi = {10.1109/HPCA.2001.903270},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903270},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903270.pdf?arnumber=903270},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Application software, Call Graph Prefetching, Database systems, Delay, Hardware, History, Prefetching, Software design, Software systems, cache storage, database management systems, database servers, file servers, graph prefetching, performance, performance bottleneck, processor caches, software performance evaluation, },
 abstract = {With the continuing technological trend of ever cheaper and larger memory, most data sets in database servers will soon be able to reside in main memory. In this configuration, the performance bottleneck is likely to be the gap between the processing speed of the CPU and the memory access latency. Previous work has shown that database applications have large instruction and data footprints and hence do not use processor caches effectively. In this paper we propose Call Graph Prefetching (CGP), a hardware technique that analyzes the call graph of a database system and prefetches instructions from the function that is deemed likely to be called next. CGP capitalizes on the highly predictable function call sequences that are typical of database systems. We evaluate the performance of CGP on sets of Wisconsin and TPC-H queries, as well as on CPU-2000 benchmarks. For most CPU-2000 applications the number of l-cache misses were very few even without any prefetching, obviating the need for CGP. Our database experiments show that CGP reduces the I-cache misses by 83\% and can improve the performance of a database system by 30\% over a baseline system that uses the OM tool to layout the code so as to improve I-cache performance. CGP also achieved 7\% higher performance than OM with next-N-line prefetching on database applications },
}

@inproceedings{824369,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Tzi-Cker Chiueh and Pradhan, P.},
 year = {2000},
 pages = {409--418},
 publisher = {IEEE},
 title = {Cache memory design for network processors},
 date = {2000},
 doi = {10.1109/HPCA.2000.824369},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824369},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824369.pdf?arnumber=824369},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {Cache memory, Computer architecture, Computer networks, Explosions, Hardware, IP networks, Internet, Internet traffic, Microprocessors, Process design, Routing, Telecommunication traffic, cache memory design, cache storage, caching algorithms, computer architecture, computer network management, data movement, data path streamlining, design tradeoffs, hardware caches, microprocessors, network packet processing, network packet streams, network processors, packet forwarding performance, packet routing, packet switching, performance, routing table lookup time, table lookup, telecommunication computing, telecommunication network routing, telecommunication traffic, temporal locality, trace-drive simulation, virtual machines, },
 abstract = {The exponential growth in Internet traffic has motivated the development of a new breed of microprocessors called network processors, which are designed to address the performance problems resulting from the explosion in Internet traffic. The development efforts of these network processors concentrate almost exclusively on streamlining their data paths to speed up network packet processing, which mainly consists of routing and data movement. Rather than blindly pushing the performance of packet processing hardware, an alternative approach is to avoid repeated computation by applying the time-tested architecture idea of caching to network packet processing. Because the data streams presented to network processors and general-purpose CPUs exhibit different characteristics, detailed cache design tradeoffs for the two also differ considerably. This research focuses on cache memory design specifically for network processors. Using a trace-drive simulation methodology, we evaluate a series of three progressively more aggressive routing-table cache designs. Our simulation results demonstrate that the incorporation of hardware caches into network processors, when combined with efficient caching algorithms, can significantly improve the overall packet forwarding performance due to a sufficiently high degree of temporal locality in the network packet streams. Moreover, different cache designs can result in up to a factor of 5 difference in the average routing table lookup time, and thus in the packet forwarding rate },
}

@inproceedings{824368,
 booktitle = {High-Performance Computer Architecture, 2000. HPCA-6. Proceedings. Sixth International Symposium on},
 author = {Vartanian, A. and Bechennec, J.-L. and Drach-Temam, N.},
 year = {2000},
 pages = {399--408},
 publisher = {IEEE},
 title = {The best distribution for a parallel OpenGL 3D engine with texture caches},
 date = {2000},
 doi = {10.1109/HPCA.2000.824368},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=824368},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6665/17815/00824368.pdf?arnumber=824368},
 isbn = {0-7695-0550-3},
 language = {English},
 keywords = {3D accelerators, Concurrent computing, Costs, Electronic switching systems, Engines, High performance computing, Hip, Parallel processing, Rendering (computer graphics), Tail, Virtual reality, block shape, block size, cache performance, cache storage, chip design, computer graphic equipment, distribution schemes, external bus, image parallelism, image rendering, image texture, load balancing, maximum performance, memory system simulation, microcomputer applications, open systems, parallel OpenGL 3D engine, parallel machine, parallel machines, performance evaluation, processor number, real-time high-end virtual reality system, real-time systems, rendering (computer graphics), resource allocation, scan-line interleaving, spatial locality, special purpose computers, speedup, square block interleaving, texture caches, textured triangles, triangle buffer, virtual reality, virtual reality benchmarks, },
 abstract = {The quality of a real-time high-end virtual reality system depends on its ability to draw millions of textured triangles in 1/60 s. The idea of using commodity PC 3D accelerators to build a parallel machine instead of custom ASICs seems more and more attractive as such chips are getting faster. If image parallelism is used, designers have the choice between two distributions: line interleaving and square block interleaving. Having a fixed block shape and size makes chip design easier. A PC 3D accelerator has a cost-effective external bus and an on-chip texture cache. The performance of such a cache depends on spatial locality. If the image is rendered on multiple engines, this locality is reduced. Locality and load balancing depend on the distribution scheme of the machine. This paper investigates the impact of the distribution scheme on the performance of such a machine. We use detailed cache and memory system simulations with virtual reality benchmarks running on different configurations. We show that: (i) both distributions have the same maximum performance with less than 16 processors, but the square block has a better speedup with 64 processors; (ii) the best SLI (scan-line interleaving) block size depends on the number of processors of the machine and is not suitable for a scalable chip with a fixed block size; and (iii) using a large triangle buffer in the texture mapping engine has a very important impact on the performance },
}

@inproceedings{1385918,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { v-- viii},
 publisher = {IEEE},
 title = {Table of Contents},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.34},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385918},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385918.pdf?arnumber=1385918},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 abstract = {},
}

@inproceedings{501187,
 booktitle = {High-Performance Computer Architecture, 1996. Proceedings. Second International Symposium on},
 author = {Muller, H.L. and Stallard, P.W.A. and Warren, D.H.D.},
 year = {1996},
 pages = {212--221},
 publisher = {IEEE},
 title = {Multitasking and multithreading on a multiprocessor with virtual shared memory},
 date = {3-7 Feb 1996},
 doi = {10.1109/HPCA.1996.501187},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=501187},
 pdf_url = {http://ieeexplore.ieee.org/iel3/3595/10710/00501187.pdf?arnumber=501187},
 isbn = {0-8186-7237-4},
 language = {English},
 keywords = {Delay, Distributed decision making, Hardware, Multitasking, Multithreading, Operating systems, Processor scheduling, Space technology, Throughput, Yarn, multitasking, multithreading, parallel architectures, related threads, shared memory parallel machine, shared memory systems, unrelated threads, virtual shared memory, virtual storage, },
 abstract = {In this paper we investigate the combination of multitasking and multithreading in a (virtual) shared memory parallel machine running a number of parallel applications. In particular, we investigate whether it is better to run related threads, or unrelated threads on each node to achieve the best system throughput and to complete a mix of applications as quickly as possible. The experiments provide results for a range of mixes of applications. One of our benchmarks has a clear preference to place its threads across the whole machine, while the others have a slight preference to run their threads on smaller partitions of the machine. The differences are mostly slight, suggesting that the system scheduler has considerable flexibility in thread placement without jeopardising performance },
}

@inproceedings{650568,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Monnerat, L.R. and Bianchini, R.},
 year = {1998},
 pages = {289--299},
 publisher = {IEEE},
 title = {Efficiently adapting to sharing patterns in software DSMs},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650568},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650568},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650568.pdf?arnumber=650568},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {ADSM, Application software, Delay, Electrical capacitance tomography, Lazy Release Consistency, Merging, Operating systems, Programming profession, Protocols, Software performance, Systems engineering and theory, TreadMarks, Workstations, distributed memory systems, dynamic categorization, page-based, parallel applications, parallel computing, performance, shared data, shared memory systems, software DSMs, },
 abstract = {In this paper we introduce a page-based Lazy Release Consistency protocol called ADSM that constantly and efficiently adapts to the applications' sharing patterns. Adaptation in ADSM is based on our dynamic categorization of the type of sharing experienced by each page. Pages can be categorized as falsely-shared, migratory, or producer/consumer(s). Migratory and producer/consumer(s) pages are managed in single-writer mode, while falsely-shared data are managed in multiple-writer mode. Coherence is kept with invalidations for most types of the shared data, but updates are used for lock-protected data in migratory state and barrier-protected delta in producer/consumer(s) state. We performed experiments with 6 parallel applications on an 8-node SP2 system, comparing our protocol against standard TreadMarks and a version of TreadMarks that also adapts to sharing patterns. Our results show that ADSM consistently outperforms its competitors; our protocol can improve the TreadMarks speedups by as much as 155\%, while surpassing the performance of the adaptive TreadMarks implementation by as much as 67\%. Our main conclusions are that our categorization and adaptation strategies are useful techniques for improving the performance of page-based software DSMs, while ADSM is a highly-efficient option for low-cost parallel computing },
}

@inproceedings{4147673,
 booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
 author = {Liqun Cheng and Carter, J.B. and Donglai Dai},
 year = {2007},
 pages = {328--339},
 publisher = {IEEE},
 title = {An Adaptive Cache Coherence Protocol Optimized for Producer-Consumer Sharing},
 date = {10-14 Feb. 2007},
 doi = {10.1109/HPCA.2007.346210},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147673},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4147635/4147636/04147673.pdf?arnumber=4147673},
 isbn = {1-4244-0805-9},
 language = {English},
 keywords = {Access protocols, Control systems, Delay, Graphics, Hardware, History, Murphi, SGI multiprocessor, Scientific computing, Silicon, System buses, Yarn, adaptive cache coherence protocol, benchmark programs, cache line, cache storage, cycle-accurate execution-driven simulator, directory delegation mechanism, enterprise computing facilities, producer-consumer sharing, protocols, scientific computing facilities, shared memory multiprocessors, shared memory systems, speculative update mechanisms, },
 abstract = {Shared memory multiprocessors play an increasingly important role in enterprise and scientific computing facilities. Remote misses limit the performance of shared memory applications, and their significance is growing as network latency increases relative to processor speeds. This paper proposes two mechanisms that improve shared memory performance by eliminating remote misses and/or reducing the amount of communication required to maintain coherence. We focus on improving the performance of applications that exhibit producer-consumer sharing. We first present a simple hardware mechanism for detecting producer-consumer sharing. We then describe a directory delegation mechanism whereby the "home node" of a cache line can be delegated to a producer node, thereby converting 3-hop coherence operations into 2-hop operations. We then extend the delegation mechanism to support speculative updates for data accessed in a producer-consumer pattern, which can convert 2-hop misses into local misses, thereby eliminating the remote memory latency. Both mechanisms can be implemented without changes to the processor. We evaluate our directory delegation and speculative update mechanisms on seven benchmark programs that exhibit producer-consumer sharing using a cycle-accurate execution-driven simulator of a future 16-node SGI multiprocessor. We find that the mechanisms proposed in this paper reduce the average remote miss rate by 40\%, reduce network traffic by 15\%, and improve performance by 21\%. Finally, we use Murphi to verify that each mechanism is error-free and does not violate sequential consistency },
}

@inproceedings{386559,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Weiss, S.},
 year = {1995},
 pages = {14--21},
 publisher = {IEEE},
 title = {Implementing register interlocks in parallel-pipeline, multiple instruction queue, superscalar processors},
 date = {1995},
 doi = {10.1109/HPCA.1995.386559},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386559},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386559.pdf?arnumber=386559},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Arithmetic, Bandwidth, Clocks, Control systems, Counting circuits, Hardware, Logic design, Pipelines, Reduced instruction set computing, Registers, high-bandwidth implementation, interlock logic, parallel architectures, parallel-pipeline multiple instruction queue superscalar processors, pipeline control hardware, pipeline organization, pipeline processing, queueing theory, reduced instruction set computing, register interlocks, storage management, table-based register renaming, },
 abstract = {A dependence for data, control, or resources might cause one instruction to become stalled in a pipeline stage waiting for a preceding instruction to produce a result or release a resource. The pipeline control hardware checks for dependences, and prevents the instruction from going to the next pipeline stage if a dependence occurs. We refer to this hardware as interlock logic. The amount and complexity of the interlock logic required to support a ten+ instruction issue bandwidth is a major concern in the design of the pipeline control hardware. We look specifically at register interlocks in the context of a parallel pipeline with separate dispatch and issue phases-a generalization of the pipeline organization implemented by a number of prominent recent superscalar processors. We describe four implementations of the register interlock logic and a comparison based on the number of logic levels. We also present a high-bandwidth implementation of table-based register renaming },
}

@inproceedings{386558,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Llosa, J. and Valero, M. and Ayguade, E.},
 year = {1995},
 pages = {22--31},
 publisher = {IEEE},
 title = {Non-consistent dual register files to reduce register pressure},
 date = {1995},
 doi = {10.1109/HPCA.1995.386558},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386558},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386558.pdf?arnumber=386558},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Bandwidth, Computer aided instruction, Costs, Microprocessors, Parallel processing, Pipeline processing, Processor scheduling, Proposals, Registers, VLIW, access time, bandwidth demands, implementation cost, initiation interval, instruction level parallelism, loop performance, memory traffic, nonconsistent dual register files, parallel architectures, pipeline processing, processor scheduling, register file organization, register pressure, software pipelined loops, spill code, storage management, unified register file, },
 abstract = {The continuous grow on instruction level parallelism offered by microprocessors requires a large register file and a large number of ports to access it. This paper presents the non-consistent dual register file, an alternative implementation and management of the register file. Non-consistent dual register files support the bandwidth demands and the high register requirements, penalizing neither access time nor implementation cost. The proposal is evaluated for software pipelined loops and compared against a unified register file. Empirical results show improvements on performance and a noticeable reduction of the density of memory traffic due to a reduction of the spill code. The spill code can in general increase the minimum initiation interval and decrease loop performance. Additional improvements can be obtained when the operations are scheduled having in mind the register file organization proposed },
}

@inproceedings{650564,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Jimenez, M. and Llaberia, J.M. and Fernandez, A.},
 year = {1998},
 pages = {254--265},
 publisher = {IEEE},
 title = {Performance evaluation of tiling for the register level},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650564},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650564},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650564.pdf?arnumber=650564},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {ALPHA 21164 processor, Computer architecture, Linear algebra, Memory management, Pipeline processing, Proposals, Registers, Scheduling, Software performance, Tiles, cache level, data reuse, loop transformation, performance evaluation, performance evaluation, pipeline processing, register level, tiling, },
 abstract = {Tiling is a well-known loop transformation, which is basically used to expose coarse-grain parallelism and to exploit data reuse at the cache level. However, it can also be used to exploit data reuse at the register level and to improve programs's ILP. Previous work on tiling and also commercial compilers are able to perform tiling for the register level in more than one dimension when the iteration space is rectangular. Non-rectangular iteration spaces are commonly found in linear algebra algorithms or can arise as a result of applying previous transformations such as loop skewing. In this paper we evaluate the technique presented in Jimenez et al. (1996) which is able to perform tiling for the register level in more than one dimension in both rectangular and non-rectangular iteration spaces. We use typical linear algebra algorithms having non-rectangular iteration spaces as benchmarks and compare our proposal against commercial preprocessors able to perform optimizing code transformations such as inner unrolling, outer unrolling and software pipelining. We will also present quantitative data showing the benefits of tiling only for the register level, tiling only for the cache level and tiling for both levels simultaneously. Results measured on a ALPHA 21164 processor show that tiling for both cache and register levels improves upon commercial compilers and preprocessors by factors in the range of 1.3 to 6.3 },
}

@inproceedings{5749738,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Ouyang, Xiangyong and Nellans, David and Wipfel, Robert and Flynn, David and Panda, Dhabaleswar K.},
 year = {2011},
 pages = {301--311},
 publisher = {IEEE},
 title = {Beyond block I/O: Rethinking traditional storage primitives},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749738},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749738},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749738.pdf?arnumber=5749738},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Over the last twenty years the interfaces for accessing persistent storage within a computer system have remained essentially unchanged. Simply put, seek, read and write have defined the fundamental operations that can be performed against storage devices. These three interfaces have endured because the devices within storage subsystems have not fundamentally changed since the invention of magnetic disks. Non-volatile (flash) memory (NVM) has recently become a viable enterprise grade storage medium. Initial implementations of NVM storage devices have chosen to export these same disk-based seek/read/write interfaces because they provide compatibility for legacy applications. We propose there is a new class of higher order storage primitives beyond simple block I/O that high performance solid state storage should support. One such primitive, atomic-write, batches multiple I/O operations into a single logical group that will be persisted as a whole or rolled back upon failure. By moving write-atomicity down the stack into the storage device, it is possible to significantly reduce the amount of work required at the application, filesystem, or operating system layers to guarantee the consistency and integrity of data. In this work we provide a proof of concept implementation of atomic-write on a modern solid state device that leverages the underlying log-based flash translation layer (FTL). We present an example of how database management systems can benefit from atomic-write by modifying the MySQL InnoDB transactional storage engine. Using this new atomic-write primitive we are able to increase system throughput by 33\%, improve the 90th percentile transaction response time by 20\%, and reduce the volume of data written from MySQL to the storage subsystem by as much as 43\% on industry standard benchmarks, while maintaining ACID transaction semantics. },
}

@inproceedings{5749739,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Hou, Rui and Zhang, Lixin and Huang, Michael C. and Wang, Kun and Franke, Hubertus and Ge, Yi and Chang, Xiaotao},
 year = {2011},
 pages = {312--320},
 publisher = {IEEE},
 title = {Efficient data streaming with on-chip accelerators: Opportunities and challenges},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749739},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749739},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749739.pdf?arnumber=5749739},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The transistor density of microprocessors continues to increase as technology scales. Microprocessors designers have taken advantage of the increased transistors by integrating a significant number of cores onto a single die. However, a large number of cores are met with diminishing returns due to software and hardware scalability issues and hence designers have started integrating on-chip special-purpose logic units (i.e., accelerators) that were previously available as PCI-attached units. It is anticipated that more accelerators will be integrated on-chip due to the increasing abundance of transistors and the fact that not all logic can be powered at all times due to power budget limits. Thus, on-chip accelerator architectures deserve more attention from the research community. There is a wide spectrum of research opportunities for design and optimization of accelerators. This paper attempts to bring out some insights by studying the data access streams of on-chip accelerators that hopefully foster some future research in this area. Specifically, this paper uses a few simple case studies to show some of the common characteristics of the data streams introduced by on-chip accelerators, discusses challenges and opportunities in exploiting these characteristics to optimize the power and performance of accelerators, and then analyzes the effectiveness of some simple optimizing extensions proposed. },
}

@inproceedings{5749736,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Yang, Qing and Ren, Jin},
 year = {2011},
 pages = {278--289},
 publisher = {IEEE},
 title = {I-CASH: Intelligently Coupled Array of SSD and HDD},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749736},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749736},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749736.pdf?arnumber=5749736},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {This paper presents a new disk I/O architecture composed of an array of a flash memory SSD (solid state disk) and a hard disk drive (HDD) that are intelligently coupled by a special algorithm. We call this architecture I-CASH: Intelligently Coupled Array of SSD and HDD. The SSD stores seldom-changed and mostly read reference data blocks whereas the HDD stores a log of deltas between currently accessed I/O blocks and their corresponding reference blocks in the SSD so that random writes are not performed in SSD during online I/O operations. High speed delta compression and similarity detection algorithms are developed to control the pair of SSD and HDD. The idea is to exploit the fast read performance of SSDs and the high speed computation of modern multi-core CPUs to replace and substitute, to a great extent, the mechanical operations of HDDs. At the same time, we avoid runtime SSD writes that are slow and wearing. An experimental prototype I-CASH has been implemented and is used to evaluate I-CASH performance as compared to existing SSD/HDD I/O architectures. Numerical results on standard benchmarks show that I-CASH reduces the average I/O response time by an order of magnitude compared to existing disk I/O architectures such as RAID and SSD/HDD storage hierarchy, and provides up to 2.8 speedup over state-of-the-art pure SSD storage. Furthermore, I-CASH reduces random writes to SSD implying reduced wearing and prolonged life time of the SSD. },
}

@inproceedings{5749737,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Madan, Niti and Buyuktosunoglu, Alper and Bose, Pradip and Annavaram, Murali},
 year = {2011},
 pages = {291--300},
 publisher = {IEEE},
 title = {A case for guarded power gating for multi-core processors},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749737},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749737},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749737.pdf?arnumber=5749737},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Dynamic power management has become an essential part of multi-core processors and associated systems. Dedicated controllers with embedded power management firmware are now an integral part of design in such multi-core server systems. Devising a robust power management policy that meets system-intended functionality across a diverse range of workloads remains a key challenge. One of the primary issues of concern in architecting a power management policy is that of performance degradation beyond a specified limit. A secondary issue is that of negative power savings. Guarding against such \&#x201C;holes\&#x201D; in the management policy is crucial in order to ensure successful deployment and use in real customer environments. It is also important to focus on developing new models and addressing the limitations of current modeling infrastructure, in analyzing alternate management policies during the design of modern multi-core systems. In this concept paper, we highlight the above specific challenges that are faced today by the server chip and system design industry in the area of power management. },
}

@inproceedings{5749734,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Liao, Guangdeng and Znu, Xia and Bnuyan, Laxmi},
 year = {2011},
 pages = {255--265},
 publisher = {IEEE},
 title = {A new server I/O architecture for high speed networks},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749734},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749734},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749734.pdf?arnumber=5749734},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Traditional architectural designs are normally focused on CPUs and have been often decoupled from I/O considerations. They are inefficient for high-speed network processing with a bandwidth of 10Gbps and beyond. Long latency I/O interconnects on mainstream servers also substantially complicate the NIC designs. In this paper, we start with fine-grained driver and OS instrumentation to fully understand the network processing overhead over 10GbE on mainstream servers. We obtain several new findings: 1) besides data copy identified by previous works, the driver and buffer release are two unexpected major overheads (up to 54\%); 2) the major source of the overheads is memory stalls and data relating to socket buffer (SKB) and page data structures are mainly responsible for the stalls; 3) prevailing platform optimizations like Direct Cache Access (DCA) are insufficient for addressing the network processing bottlenecks. Motivated by the studies, we propose a new server I/O architecture where DMA descriptor management is shifted from NICs to an on-chip network engine (NEngine), and descriptors are extended with information about data incurring memory stalls. NEngine relies on data lookups and preloads data to eliminate the stalls during network processing. Moreover, NEngine implements efficient packet movement inside caches to address the remaining issues in data copy. The new architecture allows DMA engine to have very fast access to descriptors and keeps packets in CPU caches instead of NIC buffers, significantly simplifying NICs. Experimental results demonstrate that the new server I/O architecture improves the network processing efficiency by 47\% and web server throughput by 14\%, while substantially reducing the NIC hardware complexity. },
}

@inproceedings{5749735,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Chen, Feng and Lee, Rubao and Zhang, Xiaodong},
 year = {2011},
 pages = {266--277},
 publisher = {IEEE},
 title = {Essential roles of exploiting internal parallelism of flash memory based solid state drives in high-speed data processing},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749735},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749735},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749735.pdf?arnumber=5749735},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Flash memory based solid state drives (SSDs) have shown a great potential to change storage infrastructure fundamentally through their high performance and low power. Most recent studies have mainly focused on addressing the technical limitations caused by special requirements for writes in flash memory. However, a unique merit of an SSD is its rich internal parallelism, which allows us to offset for the most part of the performance loss related to technical limitations by significantly increasing data processing throughput. In this work we present a comprehensive study of essential roles of internal parallelism of SSDs in high-speed data processing. Besides substantially improving I/O bandwidth (e.g. 7.2\&#x00D7;), we show that by exploiting internal parallelism, SSD performance is no longer highly sensitive to access patterns, but rather to other factors, such as data access interferences and physical data layout. Specifically, through extensive experiments and thorough analysis, we obtain the following new findings in the context of concurrent data processing in SSDs. (1) Write performance is largely independent of access patterns (regardless of being sequential or random), and can even outperform reads, which is opposite to the long-existing common understanding about slow writes on SSDs. (2) One performance concern comes from interference between concurrent reads and writes, which causes substantial performance degradation. (3) Parallel I/O performance is sensitive to physical data-layout mapping, which is largely not observed without parallelism. (4) Existing application designs optimized for magnetic disks can be suboptimal for running on SSDs with parallelism. Our study is further supported by a group of case studies in database systems as typical data-intensive applications. With these critical findings, we give a set of recommendations to application designers and system architects for exploiting internal parallelism and maximizing the performance potentia- - l of SSDs. },
}

@inproceedings{5749732,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Srikantaiah, Shekhar and Kultursay, Emre and Zhang, Tao and Kandemir, Mahmut and Irwin, Mary Jane and Xie, Yuan},
 year = {2011},
 pages = {231--242},
 publisher = {IEEE},
 title = {MorphCache: A Reconfigurable Adaptive Multi-level Cache hierarchy},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749732},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749732},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749732.pdf?arnumber=5749732},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Given the diverse range of application characteristics that chip multiprocessors (CMPs) need to cater to, a \&#x201C;one-cache-topology-fits-all\&#x201D; design philosophy will clearly be inadequate. In this paper, we propose MorphCache, a Reconfigurable Adaptive Multi-level Cache hierarchy. Mor-phCache dynamically tunes a multi-level cache topology in a CMP to allow significantly different cache topologies to exist on the same architecture. Starting from per-core L2 and L3 cache slices as the basic design point, MorphCache alters the cache topology dynamically by merging or splitting cache slices and modifying the accessibility of different cache slice groups to different cores in a CMP. We evaluated MorphCache on a 16 core CMP on a full system simulator and found that it significantly improves both average throughput and harmonic mean of speedups of diverse multithreaded and multiprogrammed workloads. Specifically, our results show that MorphCache improves throughput of the multiprogrammed mixes by 29.9\% over a topology with all-shared L2 and L3 caches and 27.9\% over a topology with per core private L2 cache and shared L3 cache. In addition, we also compared MorphCache to partitioning a single shared cache at each level using promotion/insertion pseudo-partitioning (PIPP) [28] and managing per-core private cache at each level using dynamic spill receive caches (DSR) [18]. We found that MorphCache improves average throughput by 6.6\% over PIPP and by 5.7\% over DSR when applied to both L2 and L3 caches. },
}

@inproceedings{5749733,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Manikantan, R and Rajan, Kaushik and Govindarajan, R},
 year = {2011},
 pages = {243--253},
 publisher = {IEEE},
 title = {NUcache: An efficient multicore cache organization based on Next-Use distance},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749733},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749733},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749733.pdf?arnumber=5749733},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {The effectiveness of the last-level shared cache is crucial to the performance of a multi-core system. In this paper, we observe and make use of the DelinquentPC \&#x2014; Next-Use characteristic to improve shared cache performance. We propose a new PC-centric cache organization, NUcache, for the shared last level cache of multi-cores. NUcache logically partitions the associative ways of a cache set into MainWays and DeliWays. While all lines have access to the MainWays, only lines brought in by a subset of delinquent PCs, selected by a PC selection mechanism, are allowed to enter the DeliWays. The PC selection mechanism is an intelligent cost-benefit analysis based algorithm that utilizes Next-Use information to select the set of PCs that can maximize the hits experienced in DeliWays. Performance evaluation reveals that NUcache improves the performance over a baseline design by 9.6\%, 30\% and 33\% respectively for dual, quad and eight core workloads comprised of SPEC benchmarks. We also show that NUcache is more effective than other well-known cache-partitioning algorithms. },
}

@inproceedings{5749730,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {McKinley, Kathryn S.},
 year = {2011},
 pages = {217--217},
 publisher = {IEEE},
 title = {Keynote address II: How's the parallel computing revolution going?},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749730},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749730},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749730.pdf?arnumber=5749730},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Two trends changed the computing landscape over the past decade: (1) hardware vendors started delivering chip multiprocessors (CMPs) instead of uniprocessors, and (2) software developers increasingly chose managed languages instead of native languages. Unfortunately, the former change is disrupting the virtuous-cycle between performance improvements and software innovation. Establishing a new parallel performance virtuous cycle for managed languages will require scalable applications executing on scalable Virtual Machine (VM) services, since the VM schedules, monitors, compiles, optimizes, garbage collects, and executes together with the application. This talk describes current progress, opportunities, and challenges for scalable VM services. The parallel computing revolution urgently needs more innovations. },
}

@inproceedings{744349,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Fernandes, M.M. and Llosa, J. and Topham, N.},
 year = {1999},
 pages = {130--134},
 publisher = {IEEE},
 title = {Distributed modulo scheduling},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744349},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744349},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744349.pdf?arnumber=744349},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Clocks, Computer architecture, Computer science, DMS, Data analysis, Microprocessors, Pipeline processing, Processor scheduling, Radio frequency, Registers, VLIW, VLIW approach, VLIW architectures, code partitioning, compiler, distributed algorithms, distributed modulo scheduling, functional units, hardware complexities, instruction sets, parallel architectures, parallel programming, partitioning strategies, private register files, processor scheduling, program compilers, register file, superscalar processors, vectorizable loops, wide-issue ILP machines, },
 abstract = {Wide-issue ILP machines can be built using the VLIW approach as many of the hardware complexities found in superscalar processors can be transferred to the compiler. However, the scalability of VLIW architectures is still constrained by the size and number of ports of the register file required by a large number of functional units. Organizations composed of clusters of a few functional units and small private register files have been proposed to deal with this problem; an approach highly dependent on scheduling and partitioning strategies. The paper presents DMS, an algorithm that integrates modulo scheduling and code partitioning in a single procedure. Experimental results have shown that the algorithm is effective for configurations up to 8 clusters, or even more when targeting vectorizable loops },
}

@inproceedings{5416639,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Udipi, A.N. and Muralimanohar, N. and Balasubramonian, R.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Towards scalable, energy-efficient, bus-based on-chip networks},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416639},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416639},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416639.pdf?arnumber=5416639},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Bandwidth, Broadcasting, Energy consumption, Energy efficiency, Filters, Network-on-a-chip, OS page coloring, Performance loss, Protocols, Wires, Wiring, area, broadcast bus, bus-based on-chip networks, central bus, complex routers, delay, energy, many-core processors, microprocessor chips, multiple address-interleaved buses, multiple bloom filters, network routing, network-protocol design, packet- switched networks, power-hungry routers, protocols, scalable directory-based coherence protocol, snooping protocols, verification effort, },
 abstract = {It is expected that future on-chip networks for many-core processors will impose huge overheads in terms of energy, delay, complexity, verification effort, and area. There is a common belief that the bandwidth necessary for future applications can only be provided by employing packet-switched networks with complex routers and a scalable directory-based coherence protocol. We posit that such a scheme might likely be overkill in a well designed system in addition to being expensive in terms of power because of a large number of power-hungry routers. We show that bus-based networks with snooping protocols can significantly lower energy consumption and simplify network/protocol design and verification, with no loss in performance. We achieve these characteristics by dividing the chip into multiple segments, each having its own broadcast bus, with these buses further connected by a central bus. This helps eliminate expensive routers, but suffers from the energy overhead of long wires. We propose the use of multiple Bloom filters to effectively track data presence in the cache and restrict bus broadcasts to a subset of segments, significantly reducing energy consumption. We further show that the use of OS page coloring helps maximize locality and improves the effectiveness of the Bloom filters. We also employ low-swing wiring to further reduce the energy overheads of the links. Performance can also be improved at relatively low costs by utilizing more of the abundant metal budgets on-chip and employing multiple address-interleaved buses rather than multiple routers. Thus, with the combination of all the above innovations, we extend the scalability of buses and believe that buses can be a viable and attractive option for future on-chip networks. We show energy reductions of up to 31X on average compared to many state-of-the-art packet switched networks. },
}

@inproceedings{1385958,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { 363-- 363},
 publisher = {IEEE},
 title = {Author Index},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.7},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385958},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385958.pdf?arnumber=1385958},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 abstract = {},
}

@inproceedings{1385952,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Feng Qin and Shan Lu and Yuanyuan Zhou},
 year = {2005},
 pages = { 291-- 302},
 publisher = {IEEE},
 title = {SafeMem: exploiting ECC-memory for detecting memory leaks and memory corruption during production runs},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.29},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385952},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385952.pdf?arnumber=1385952},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { ECC memory technology,  SafeMem,  dynamic monitoring tools,  intelligent dynamic memory usage behavior analysis,  memory corruption detection,  memory leak detection,  memory monitoring,  production-runs,  program compilers,  program debugging,  program diagnostics,  storage management,  supervisory programs, Computer bugs, Data security, Databases, Hardware, Leak detection, Monitoring, Production, Programming profession, Runtime, Software performance, },
 abstract = {Memory leaks and memory corruption are two major forms of software bugs that severely threaten system availability and security. According to the US-CERT vulnerability notes database, 68\% of all reported vulnerabilities in 2003 were caused by memory leaks or memory corruption. Dynamic monitoring tools, such as the state-of-the-art Purify, are commonly used to detect memory leaks and memory corruption. However, most of these tools suffer from high overhead, with up to a 20 times slowdown, making them infeasible to be used for production-runs. This paper proposes a tool called SafeMem to detect memory leaks and memory corruption on-the-fly during production-runs. This tool does not rely on any new hardware support. Instead, it makes a novel use of existing ECC memory technology and exploits intelligent dynamic memory usage behavior analysis to detect memory leaks and corruption. We have evaluated SafeMem with seven real-world applications that contain memory leak or memory corruption bugs. SafeMem detects all tested bugs with low overhead (only 1.6\%-14.4\%), 2-3 orders of magnitudes smaller than Purify. Our results also show that ECC-protection is effective in pruning false positives for memory leak detection, and in reducing the amount of memory waste (by a factor of 64-74) used for memory monitoring in memory corruption detection compared to page-protection. },
}

@inproceedings{1385953,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Corliss, M.L. and Lewis, E.C. and Roth, A.},
 year = {2005},
 pages = { 303-- 314},
 publisher = {IEEE},
 title = {Low-overhead interactive debugging via dynamic instrumentation with DISE},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.18},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385953},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385953.pdf?arnumber=1385953},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { DISE embedding,  application-debugger context switches,  breakpoint-watchpoint interface,  debugger logic,  debugging primitives,  dynamic instruction stream editing,  dynamic instrumentation,  embedded systems,  instruction sets,  low-overhead interactive debugging,  program debugging,  programmable hardware facility, Context modeling, Costs, Debugging, Decoding, Degradation, Hardware, Instruments, Logic, Open source software, Switches, },
 abstract = {Breakpoints, watchpoints, and conditional variants of both are essential debugging primitives, but their natural implementations often degrade performance significantly. Slowdown arises because the debugger - the tool implementing the breakpoint/watchpoint interface - is implemented in a process separate from the debugged application. Since the debugger evaluates the watchpoint expressions and conditional predicates to determine whether to invoke the user, a debugging session typically requires many expensive application-debugger context switches, resulting in slowdowns of 40,000 times or more in current commercial and open-source debuggers! In this paper, we present an effective and efficient implementation of (conditional) breakpoints and watchpoints that uses DISE to dynamically embed debugger logic into the running application. DISE (dynamic instruction stream editing) is a previously proposed, programmable hardware facility for dynamically customizing applications by transforming the instruction stream as it is decoded. DISE embedding preserves the logical separation of application and debugger nstructions are added dynamically and transparently, existing application code and data are not statically modified - and has little startup cost. Cycle-level simulation on the SPEC 2000 integer benchmarks shows that the DISE approach eliminates all unnecessary context switching, typically limits debugging overhead to 25\% or less for a wide range of watch-points, and outperforms alternative implementations. },
}

@inproceedings{1385950,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Yi, J.J. and Kodakara, S.V. and Sendag, R. and Lilja, D.J. and Hawkins, D.M.},
 year = {2005},
 pages = { 266-- 277},
 publisher = {IEEE},
 title = {Characterizing and comparing prevailing simulation techniques},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.8},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385950},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385950.pdf?arnumber=1385950},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { SMARTS,  SimPoint,  characterization methods,  computer architecture,  decision tree,  decision trees,  digital simulation,  processor enhancement,  reduced input set,  reference input set,  sampling simulation techniques,  truncated execution, Computational modeling, Computer architecture, Computer networks, Computer simulation, Decision trees, Microarchitecture, Performance analysis, Sampling methods, Statistics, Testing, },
 abstract = {Due to the simulation time of the reference input set, architects often use alternative simulation techniques. Although these alternatives reduce the simulation time, what has not been evaluated is their accuracy relative to the reference input set, and with respect to each other. To rectify this deficiency, this paper uses three methods to characterize the reduced input set, truncated execution, and sampling simulation techniques while also examining their speed versus accuracy trade-off and configuration dependence. Finally, to illustrate the effect that a technique could have on the apparent speedup results, we quantify the speedups obtained with two processor enhancements. The results show that: 1) the accuracy of the truncated execution techniques was poor for all three characterization methods and for both enhancements, 2) the characteristics of the reduced input sets are not reference-like, and 3) SimPoint and SMARTS, the two sampling techniques, are extremely accurate and have the best speed versus accuracy trade-offs. Finally, this paper presents a decision tree which can help architects choose the most appropriate technique for their simulations. },
}

@inproceedings{1385951,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Lau, J. and Schoenmackers, S. and Calder, B.},
 year = {2005},
 pages = { 278-- 289},
 publisher = {IEEE},
 title = {Transition phase classification and prediction},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.39},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385951},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385951.pdf?arnumber=1385951},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { adaptive system,  computer architecture,  dynamic phase classification,  dynamic phase prediction,  hardware metrics,  processor scheduling,  program compilers,  program diagnostics,  transition phase classification,  transition phase prediction, Accuracy, Adaptive systems, Computer architecture, Computer science, Hardware, Heuristic algorithms, Optimizing compilers, Random access memory, Statistics, Yarn, },
 abstract = {Most programs are repetitive, where similar behavior can be seen at different execution times. Proposed on-line systems automatically group these similar intervals of execution into phases, where the intervals in a phase have homogeneous behavior and similar resource requirements. These systems are driven by algorithms that dynamically classify intervals of execution into phases and predict phase changes. In this paper, we examine several improvements to dynamic phase classification and prediction. The first improvement is to appropriately deal with phase transitions. This modification identifies phase transitions for what they are, instead of classifying them into a new phase, which increases phase prediction accuracy. We also describe an adaptive system that dynamically adjusts classification thresholds and splits phases with poor homogeneity. This modification increases the homogeneity of the hardware metrics across the intervals in each phase. We improve phase prediction accuracy by applying confidence to phase prediction, and we develop architectures that can accurately predict the outcome of the next phase change, and the length of the next phase. },
}

@inproceedings{1385956,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Dhruba Chandra and Fei Guo and Seongbeom Kim and Yan Solihin},
 year = {2005},
 pages = { 340-- 351},
 publisher = {IEEE},
 title = {Predicting inter-thread cache contention on a chip multi-processor architecture},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.27},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385956},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385956.pdf?arnumber=1385956},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { L2 cache misses,  L2 cache sharing,  cache storage,  chip multiprocessor architecture,  circular sequence profile,  circular sequence thread profile,  computational complexity,  computational complexity,  computer architecture,  coscheduled thread,  dual-core CMP architecture,  inductive probability model,  interthread cache,  isolated L2 cache stack distance,  microprocessor chips,  multi-threading,  multiprocessing systems,  nonuniform threading,  temporal reuse behavior, Accuracy, Analytical models, Art, Bars, Career development, Computer architecture, Hardware, Predictive models, Throughput, Yarn, },
 abstract = {This paper studies the impact of L2 cache sharing on threads that simultaneously share the cache, on a chip multi-processor (CMP) architecture. Cache sharing impacts threads nonuniformly, where some threads may be slowed down significantly, while others are not. This may cause severe performance problems such as sub-optimal throughput, cache thrashing, and thread starvation for threads that fail to occupy sufficient cache space to make good progress. Unfortunately, there is no existing model that allows extensive investigation of the impact of cache sharing. To allow such a study, we propose three performance models that predict the impact of cache sharing on co-scheduled threads. The input to our models is the isolated L2 cache stack distance or circular sequence profile of each thread, which can be easily obtained on-line or off-line. The output of the models is the number of extra L2 cache misses for each thread due to cache sharing. The models differ by their complexity and prediction accuracy. We validate the models against a cycle-accurate simulation that implements a dual-core CMP architecture, on fourteen pairs of mostly SPEC benchmarks. The most accurate model, the inductive probability model, achieves an average error of only 3.9\%. Finally, to demonstrate the usefulness and practicality of the model, a case study that details the relationship between an application's temporal reuse behavior and its cache sharing impact is presented. },
}

@inproceedings{1385957,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Youtao Zhang and Lan Gao and Jun Yang and Xiangyu Zhang and Rajiv Gupta},
 year = {2005},
 pages = { 352-- 362},
 publisher = {IEEE},
 title = {SENSS: security enhancement to symmetric shared memory multiprocessors},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.31},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385957},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385957.pdf?arnumber=1385957},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { SENSS model,  SMP system,  advanced encryption standard,  block codes,  bus transaction authentication,  bus transaction encryption,  cache coherence protocol,  cipher block chaining mode,  cryptographic computation,  cryptography,  high performance multiprocessor enterprise server,  integrity checking code,  message authentication,  multiprocessor environment,  security enhancement,  shared bus communication,  shared bus decryption,  shared bus encryption,  shared memory systems,  symmetric shared memory multiprocessor,  system buses,  text communication,  transaction processing, Authentication, Bandwidth, Computational modeling, Cryptography, Data security, Delay, Performance evaluation, Protection, Standards development, Throughput, },
 abstract = {With the increasing concern of the security on high performance multiprocessor enterprise servers, more and more effort is being invested into defending against various kinds of attacks. This paper proposes a security enhancement model called SENSS, that allows programs to run securely on a symmetric shared memory multiprocessor (SMP) environment. In SENSS, a program, including both code and data, is stored in the shared memory in encrypted form but is decrypted once it is fetched into any of the processors. In contrast to the traditional uniprocessor XOM model (Lie et al., 2000), the main challenge in developing SENSS lies in the necessity for guarding the clear text communication between processors in a multiprocessor environment. In this paper we propose an inexpensive solution that can effectively protect the shared bus communication. The proposed schemes include both encryption and authentication for bus transactions. We develop a scheme that utilizes the cipher block chaining mode of the advanced encryption standard (CBC-AES) to achieve ultra low latency for the shared bus encryption and decryption. In addition, CBC-AES can generate integrity checking code for the bus communication over time, achieving bus authentication. Further, we develop techniques to ensure the cryptographic computation throughput meets the high bandwidth of gigabyte buses. We performed full system simulation using Simics to measure the overhead of the security features on a SMP system with a snooping write invalidate cache coherence protocol. Overall, only a slight performance degradation of 2.03\% on average was observed when the security is provided at the highest level. },
}

@inproceedings{1385954,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Ananian, C.S. and Asanovic, K. and Kuszmaul, B.C. and Leiserson, C.E. and Lie, S.},
 year = {2005},
 pages = { 316-- 327},
 publisher = {IEEE},
 title = {Unbounded transactional memory},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.41},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385954},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385954.pdf?arnumber=1385954},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { Java programs,  LTM,  Linux 2.4.19 kernel,  SPECjvm98 Java benchmarks,  UTM,  cycle-accurate simulation,  digital simulation,  hardware transactional memory,  storage management,  transaction processing,  unbounded transactional memory,  virtual memory, Artificial intelligence, Computer science, Hardware, Java, Kernel, Linux, Protocols, Transaction databases, Turing machines, Yarn, },
 abstract = {Hardware transactional memory should support unbounded transactions: transactions of arbitrary size and duration. We describe a hardware implementation of unbounded transactional memory, called UTM, which exploits the common case for performance without sacrificing correctness on transactions whose footprint can be nearly as large as virtual memory. We performed a cycle-accurate simulation of a simplified architecture, called LTM. LTM is based on UTM but is easier to implement, because it does not change the memory subsystem outside of the processor. LTM allows nearly unbounded transactions, whose footprint is limited only by physical memory size and whose duration by the length of a timeslice. We assess UTM and LTM through microbenchmarking and by automatically converting the SPECjvm98 Java benchmarks and the Linux 2.4.19 kernel to use transactions instead of locks. We use both cycle-accurate simulation and instrumentation to understand benchmark behavior. Our studies show that the common case is small transactions that commit, even when contention is high, but that some applications contain very large transactions. For example, although 99.9\% of transactions in the Linux study touch 54 cache lines or fewer, some transactions touch over 8000 cache lines. Our studies also indicate that hardware support is required, because some applications spend over half their time in critical regions. Finally, they suggest that hardware support for transactions can make Java programs run faster than when run using locks and can increase the concurrency of the Linux kernel by as much as a factor of 4 with no additional programming work. },
}

@inproceedings{1385955,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Marty, M.R. and Bingham, J.D. and Hill, M.D. and Hu, A.J. and Martin, M.M.K. and Wood, D.A.},
 year = {2005},
 pages = { 328-- 339},
 publisher = {IEEE},
 title = {Improving multiple-CMP systems using token coherence},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.17},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385955},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385955.pdf?arnumber=1385955},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { chip multiprocessors,  formal verification,  hierarchical protocol,  inter-CMP protocol,  intra-CMP coherence protocol,  microprocessor chips,  model checking,  multiple-CMP systems,  multiprocessing systems,  protocols,  semiconductor technology,  shared memory,  token coherence protocol, Coherence, Computer architecture, Computer science, Delay, Information science, Proposals, Protocols, Read-write memory, Robustness, Safety, },
 abstract = {Improvements in semiconductor technology now enable chip multiprocessors (CMPs). As many future computer systems will use one or more CMPs and support shared memory, such systems will have caches that must be kept coherent. Coherence is a particular challenge for multiple-CMP (M-CMP) systems. One approach is to use a hierarchical protocol that explicitly separates the intra-CMP coherence protocol from the inter-CMP protocol, but couples them hierarchically to maintain coherence. However, hierarchical protocols are complex, leading to subtle, difficult-to-verify race conditions. Furthermore, most previous hierarchical protocols use directories at one or both levels, incurring indirections - and thus extra latency - for sharing misses, which are common in commercial workloads. In contrast, this paper exploits the separation of correctness substrate and performance policy in the recently-proposed token coherence protocol to develop the first M-CMP coherence protocol that is flat for correctness, but hierarchical for performance. Via model checking studies, we show that flat correctness eases verification. Via simulation with micro-benchmarks, we make new protocol variants more robust under contention. Finally, via simulation with commercial workloads on a commercial operating system, we show that new protocol variants can be 10-50\% faster than a hierarchical directory protocol. },
}

@inproceedings{5749718,
 booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
 author = {Blake, Geoffrey and Dreslinski, Ronald G. and Mudge, Trevor},
 year = {2011},
 pages = {75--86},
 publisher = {IEEE},
 title = {Bloom Filter Guided Transaction Scheduling},
 date = {12-16 Feb. 2011},
 doi = {10.1109/HPCA.2011.5749718},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5749718},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5743111/5749710/05749718.pdf?arnumber=5749718},
 issn = {1530-0897},
 isbn = {978-1-4244-9432-3},
 language = {English},
 abstract = {Contention management is an important design component to a transactional memory system. Without effective contention management to ensure forward progress, a transactional memory system can experience live-lock, which is difficult to debug in parallel programs. Early work in contention management focused on heuristic managers that reacted to conflicts between transactions by picking the most appropriate transaction to abort. Reactive methods allow conflicts to happen repeatedly as they do not try to prevent future conflicts from happening. These shortcomings of reactive contention managers have led to proposals that approach contention management as a scheduling problem \&#x2014; proactive managers. Proactive techniques range from throttling execution in predicted periods of high contention to preventing groups of transactions running concurrently that are predicted likely to conflict. We propose a novel transaction scheduling scheme called \&#x201C;Bloom Filter Guided Transaction Scheduling\&#x201D; (BFGTS), that uses a combination of simple hardware and Bloom filter heuristics to guide scheduling decisions and provide enhanced performance in high contention situations. We compare to two state-of-the-art transaction schedulers, \&#x201C;Adaptive Transaction Scheduling\&#x201D; and \&#x201C;Proactive Transaction Scheduling\&#x201D; and show that BFGTS attains up to a 4.6\&#x00D7; and 1.7\&#x00D7; improvement on high contention benchmarks respectively. Across all benchmarks it shows a 35\% and 25\% average performance improvement respectively. },
}

@inproceedings{386560,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {Nuth, P.R. and Dally, W.J.},
 year = {1995},
 pages = {4--13},
 publisher = {IEEE},
 title = {The Named-State Register File: implementation and performance},
 date = {1995},
 doi = {10.1109/HPCA.1995.386560},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386560},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386560.pdf?arnumber=386560},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {Artificial intelligence, Costs, Hardware, Laboratories, Milling machines, Program processors, Registers, Switches, Traffic control, Yarn, access time, cache storage, concurrent contexts, file organisation, fine-grain associative register file, hardware techniques, named-state register file, parallel architectures, parallel procedure activations, program compilers, register file organization, reload traffic, sequential procedure activations, software performance evaluation, software techniques, storage allocation, storage management, },
 abstract = {Context switches are slow in conventional processors because the entire processor state must be saved and restored, even if much of the state is not used before the next context switch. This paper introduces the Named-State Register File, a fine-grain associative register file. The NSF uses hardware and software techniques to efficiently manage registers among sequential or parallel procedure activations. The NSF holds more live data per register than conventional register files, and requires much less spill and reload traffic to switch between concurrent contexts. The NSF speeds execution of some sequential and parallel programs by 9\% to 17\% over alternative register file organizations. The NSF has access time comparable to a conventional register file and only adds 5\% to the area of a typical processor chip },
}

@inproceedings{386561,
 booktitle = {High-Performance Computer Architecture, 1995. Proceedings., First IEEE Symposium on},
 author = {},
 year = {1995},
 publisher = {IEEE},
 title = {Proceedings of 1995 1st IEEE Symposium on High Performance Computer Architecture},
 date = {1995},
 doi = {10.1109/HPCA.1995.386561},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=386561},
 pdf_url = {http://ieeexplore.ieee.org/iel2/3040/8763/00386561.pdf?arnumber=386561},
 isbn = {0-8186-6445-2},
 language = {English},
 keywords = {cache memory, cache storage, code optimisation, computer architecture, interconnection networks, latency reduction, memory management, modelling, multiprocessor interconnection networks, multithreaded architecture, performance evaluation, performance evaluation, register management, routing in mesh, scheduling, special purpose architechtures, synchronisation, },
 abstract = {The following topics were dealt with: register management; interconnection networks; latency reduction; routing in mesh; cache memory; modelling and performance evaluation; synchronisation and scheduling; memory management; cache coherence; multithreaded architecture; special purpose architechtures; and code optimisation },
}

@inproceedings{903271,
 booktitle = {High-Performance Computer Architecture, 2001. HPCA. The Seventh International Symposium on},
 author = {Srinivasan, V. and Davidson, E.S. and Tyson, G.S. and Charney, M.J. and Puzak, T.R.},
 year = {2001},
 pages = {291--300},
 publisher = {IEEE},
 title = {Branch history guided instruction prefetching},
 date = {2001},
 doi = {10.1109/HPCA.2001.903271},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=903271},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7240/19534/00903271.pdf?arnumber=903271},
 isbn = {0-7695-1019-1},
 language = {English},
 keywords = {Accuracy, Branch History Guided Prefetching, Current measurement, Delay, History, Loss measurement, Pipelines, Prefetching, Timing, cache storage, instruction prefetches, instruction prefetching, instruction prefetching mechanism, memory architecture, },
 abstract = {Instruction cache misses stall the fetch stage of the processor pipeline and hence affect instruction supply to the processor. Instruction prefetching has been proposed as a mechanism to reduce instruction cache (I-cache) misses. However, a prefetch is effective only if accurate and initiated sufficiently early to cover the miss penalty. This paper presents a new hardware-based instruction prefetching mechanism, Branch History Guided Prefetching (BHGP), to improve the timeliness of instruction prefetches. BHGP correlates the execution of a branch instruction with I-cache misses and uses branch instructions to trigger prefetches of instructions that occur (N-1) branches later in the program execution, for a given N\&gt;1. Evaluations on commercial applications, windows-NT applications, and some CPU2000 applications show an average reduction of 66\% in miss rate over all applications. BHGP improved the IPC bp 12 to 14\% for the CPU2000 applications studied; on average 80\% of the BHGP prefetches arrived in cache before their next use, even on a 4-wide issue machine with a 15 cycle L2 access penalty },
}

@inproceedings{5416661,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {},
 year = {2010},
 pages = {1--4},
 publisher = {IEEE},
 title = {Table of contents},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416661},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416661},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416661.pdf?arnumber=5416661},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {cache architectures, cache storage, computer architecture, emerging technologies, energy efficiency, industrial perspectives, memory systems, microprocessor chips, microprocessor chips, multicore architectures, multiprocessing systems, on-chip networks, performance evaluation, processor microarchitecture, reliability, system architecture, },
 abstract = {The paper deals with the following topics: multicore architectures; microprocessor chips; reliability and energy efficiency; memory systems; cache architectures; on-chip networks; system architecture and performance evaluation; processor microarchitecture; industrial perspectives; and architectures for emerging technologies. },
}

@inproceedings{5416660,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Tong Li and Brett, P. and Knauerhase, R. and Koufaty, D. and Reddy, D. and Hahn, S.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Operating system support for overlapping-ISA heterogeneous multi-core architectures},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416660},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416660},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416660.pdf?arnumber=5416660},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Costs, Frequency, Hardware, Instruction sets, Linux, Linux, Linux 2.6.24 kernel, Manufacturing processes, OS support, Operating systems, Parallel processing, Round robin, Scheduling, heterogeneous architectures, heterogeneous processor, multi-threading, multiprocessing systems, multithread throughput, operating system kernels, operating system support, overlapping-ISA heterogeneous multicore architectures, processor manufacturers, single-thread performance, },
 abstract = {A heterogeneous processor consists of cores that are asymmetric in performance and functionality. Such a design provides a cost-effective solution for processor manufacturers to continuously improve both single-thread performance and multi-thread throughput. This design, however, faces significant challenges in the operating system, which traditionally assumes only homogeneous hardware. This paper presents a comprehensive study of OS support for heterogeneous architectures in which cores have asymmetric performance and overlapping, but non-identical instruction sets. Our algorithms allow applications to transparently execute and fairly share different types of cores. We have implemented these algorithms in the Linux 2.6.24 kernel and evaluated them on an actual heterogeneous platform. Evaluation results demonstrate that our designs efficiently manage heterogeneous hardware and enable significant performance improvements for a range of applications. },
}

@inproceedings{5416663,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {},
 year = {2010},
 pages = {1--2},
 publisher = {IEEE},
 title = {Message from the general chairs},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416663},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416663},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416663.pdf?arnumber=5416663},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 abstract = {},
}

@inproceedings{5416662,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {Agerwala, T.},
 year = {2010},
 pages = {1--1},
 publisher = {IEEE},
 title = {Exascale computing: The challenges and opportunities in the next decade},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416662},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416662},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416662.pdf?arnumber=5416662},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 keywords = {Application software, Computer architecture, Computer industry, Computer networks, Costs, Power engineering and energy, Power engineering computing, Power system reliability, Technological innovation, Usability, computer system, cutting-edge engineering, exascale computing, parallel machines, petaflop machine, scientific discovery, supercomputing systems, },
 abstract = {Summary form only given. Supercomputing systems have made great strides in recent years as the extensive computing needs of cutting-edge engineering work and scientific discovery have driven the development of more powerful systems. In 2008, the first petaflop machine was released, and historic trends indicate that in ten years, we should be at the exascale level. Indeed, various agencies are targeting a computer system capable of 1 Exaop (10**18 ops) of computation within the next decade. We believe that applications in many industries will be materially transformed by exascale computers. Meeting the exascale challenge will require significant innovation in technology, architecture and programmability. Power is a fundamental problem at all levels; traditional memory cost and performance are not keeping pace with compute potential; the storage hierarchy will have to be re-architected; networks will be a much bigger part of the system cost; reliability at exascale levels will require a holistic approach to architecture design, and programmability and ease-of-use will be an essential component to extract the promised performance at the exascale level. In this talk, I will discuss the importance of exascale computing and address the major challenges, touching on the areas of technology, architecture, reliability and usability. },
}

@inproceedings{5416664,
 booktitle = {High Performance Computer Architecture (HPCA), 2010 IEEE 16th International Symposium on},
 author = {},
 year = {2010},
 pages = {1--5},
 publisher = {IEEE},
 title = {HPCA-16 organizing committee},
 date = {9-14 Jan. 2010},
 doi = {10.1109/HPCA.2010.5416664},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5416664},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5410726/5416625/05416664.pdf?arnumber=5416664},
 issn = {1530-0897},
 isbn = {978-1-4244-5658-1},
 language = {English},
 abstract = {},
}

@inproceedings{744351,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Ye Zhang and Rauchwerger, L. and Torrellas, J.},
 year = {1999},
 pages = {135--139},
 publisher = {IEEE},
 title = {Hardware for speculative parallelization of partially-parallel loops in DSM multiprocessors},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744351},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744351},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744351.pdf?arnumber=744351},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {DSM machine, DSM multiprocessors, Hardware, L1 cache, cache coherence protocol, chip-multiprocessor schemes, code re-execution, cross-iteration dependences, dependence violation, dependence violations, directory based cache coherence, distributed shared memory systems, large scale DSM parallelism, load imbalance, multiprocessor chip, parallel execution, parallel programming, partially-parallel loops, program control structures, recovery costs, safe state, software based run time parallelization scheme, speculative parallelization, speculative state, strict in-order task commit policy, },
 abstract = {Recently, we introduced a novel framework for speculative parallelization in hardware (Y. Zhang et al., 1998). The scheme is based on a software based run time parallelization scheme that we proposed earlier (L. Rauchwerger and D. Padue, 1995). The idea is to execute the code (loops) speculatively in parallel. As parallel execution proceeds, extra hardware added to the directory based cache coherence of the DSM machine detects if there is a dependence violation. If such a violation occurs, execution is interrupted, the state is rolled back in software to the most recent safe state, and the code is re-executed serially from that point. The safe state is typically established at the beginning of the loop. Such a scheme is somewhat related to speculative parallelization inside a multiprocessor chip, which also relies on extending the cache coherence protocol to detect dependence violations. Our scheme, however, is targeted to large scale DSM parallelism. In addition, it does not have some of the limitations of the proposed chip-multiprocessor schemes. Such limitations include the need to bound the size of the speculative state to fit in a buffer or L1 cache, and a strict in-order task commit policy that may result in load imbalance among processors. Unfortunately, our scheme has higher recovery costs if a dependence violation is detected, because execution has to backtrack to a safe state that is usually the beginning of the loop. Therefore, the aim of the paper is to extend our previous hardware scheme to effectively handle codes (loops) with a modest number of cross-iteration dependences },
}

@inproceedings{1598119,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Isci, C. and Martonosi, M.},
 year = {2006},
 pages = { 121-- 132},
 publisher = {IEEE},
 title = {Phase characterization for power: evaluating control-flow-based and event-counter-based techniques},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598119},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598119},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598119.pdf?arnumber=1598119},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { SPEC2000 benchmark,  basic block vector,  benchmark testing,  code-oriented technique,  computer architecture,  control-flow sampling,  event counter,  mainstream desktop application,  performance counter,  performance evaluation,  power consumption,  power consumption,  power phase characterization,  real-system power measurement, Counting circuits, Energy consumption, Hardware, Performance analysis, Phase measurement, Power measurement, Power system management, Runtime, Sampling methods, Software systems, },
 abstract = {Computer systems increasingly rely on dynamic, phase-based system management techniques, in which system hardware and software parameters may be altered or tuned at runtime for different program phases. Prior research has considered a range of possible phase analysis techniques, but has focused almost exclusively on performance-oriented phases; the notion of power-oriented phases has not been explored. Moreover, the bulk of phase-analysis studies have focused on simulation evaluation. There is need for real-system experiments that provide direct comparison of different practical techniques (such as control flow sampling, event counters, and power measurements) for gauging phase behavior. In this paper, we propose and evaluate a live, real-system measurement framework for collecting and analyzing power phases in running applications. Our experimental frameworks simultaneously collects control flow, performance counter and live power measurement information. Using this framework, we directly compare between code-oriented techniques (such as "basic block vectors") and performance counter techniques for characterizing power phases. Across a collection of both SPEC2000 benchmarks as well as mainstream desktop applications, our results indicate that both techniques are promising, but that performance counters consistently provide better representation of power behavior. For many of the experimented cases, basic block vectors demonstrate a strong relationship between the execution path and power consumption. However, there are instances where power behavior cannot be captured from control flow, for example due to differences in memory hierarchy performance. We demonstrate these with examples from real applications. Overall, counter-based techniques offer average classification errors of 1.9\% for SPEC and 7.1\% for other benchmarks, while basic block vectors achieve 2.9\% average errors for SPEC and 11.7\% for other benchmarks respectively. },
}

@inproceedings{744357,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Iyer, R. and Bhuyan, L.N.},
 year = {1999},
 pages = {152--160},
 publisher = {IEEE},
 title = {Switch cache: a framework for improving the remote memory access latency of CC-NUMA multiprocessors},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744357},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744357},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744357.pdf?arnumber=744357},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Access protocols, CAESAR, CAche Embedded Switch ARchitecture, CC-NUMA multiprocessors, Computer science, Computer worms, Costs, Delay, Postal services, Random access memory, Read only memory, Switches, application execution time, cache coherent non-uniform memory access multiprocessors, cache storage, caching protocol, cost-effective solution, crossbar switches, data transfer latencies, distant remote memories, execution-driven simulations, hardware caching technique, interconnection network, memory module, memory technology, multiprocessor interconnection networks, network routing, remote memory access latencies, remote memory access latency, remote memory accesses, requesting processor, shared data, small fast caches, storage management, switch cache, virtual channels, wormhole routing, },
 abstract = {Cache coherent non-uniform memory access (CC-NUMA) multiprocessors continue to suffer from remote memory access latencies due to comparatively slow memory technology and data transfer latencies in the interconnection network. We propose a novel hardware caching technique, called switch cache. The main idea is to implement small fast caches in crossbar switches of the interconnect medium to capture and store shared data as they flow from the memory module to the requesting processor. This stored data acts as a cache for subsequent requests, thus reducing the latency of remote memory accesses tremendously. The implementation of a cache in a crossbar switch needs to be efficient and robust, yet flexible for changes in the caching protocol. The design and implementation details of a CAche Embedded Switch ARchitecture, CAESAR, using wormhole routing with virtual channels is presented. Using detailed execution-driven simulations, we find that the CAESAR switch cache is capable of improving the performance of CC-NUMA multiprocessors by reducing the number of reads served at distant remote memories by up to 45\% and improving the application execution time by as high as 20\%. We conclude that the switch caches provide a cost-effective solution for designing high performance CC-NUMA multiprocessors },
}

@inproceedings{744354,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Michael, M.M. and Nanda, A.K.},
 year = {1999},
 pages = {142--151},
 publisher = {IEEE},
 title = {Design and performance of directory caches for scalable shared memory multiprocessors},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744354},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744354},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744354.pdf?arnumber=744354},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Access protocols, DRAM memory, Delay, Ear, Electronic switching systems, Hardware, National electric code, Random access memory, Read only memory, Roads, SPLASH-2 suite, Time measurement, associativity, cache line size, cache size, cache storage, coherence controller designs, coherence controller occupancy, coherence controllers, coherence protocol handlers, communication intensive applications, data cache, data set size, directory access time, directory cache design parameters, directory cache miss ratio, directory caches, directory entries, distributed cache coherent shared memory multiprocessors, execution time, hardwired coherence controller designs, line size, linear relation, multi-entry directory cache lines, parallel applications, parallel programming, performance bottleneck, protocol handler data, protocol processors, protocols, scalable shared memory multiprocessors, shared memory systems, spatial locality, storage management, },
 abstract = {Recent research shows that the occupancy of the coherence controllers is a major performance bottleneck for distributed cache coherent shared memory multiprocessors. A significant part of the occupancy is due to the latency of accessing the directory which is usually kept in DRAM memory. Most coherence controller designs that use protocol processors for executing the coherence protocol handlers use the data cache of the protocol processor for caching directory entries along with protocol handler data. Analogously, a fast Directory Cache (DC) can also be used by the hardwired coherence controller designs to minimize directory access time. The paper studies the performance of directory caches using parallel applications from the SPLASH-2 suite. We demonstrate that using a directory cache can result in 40\% or more improvement in the execution time of communication intensive applications. We also investigate the various directory cache design parameters: cache size, cache line size, and associativity. Experimental results show that the directory cache size requirements grow sub-linearly with the increase in the application's data set size. The results also show the performance advantage of multi-entry directory cache lines, as a result of spatial locality and the absence of sharing of directories. The impact of the associativity of the directory caches on performance is less than that of the size and the line size. We also find a linear relation between the directory cache miss ratio and the coherence controller occupancy, and between both measures and the execution time of the applications },
}

@inproceedings{650575,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {},
 year = {1998},
 pages = {352--352},
 publisher = {IEEE},
 title = {Author Index},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650575},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650575},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650575.pdf?arnumber=650575},
 isbn = {0-8186-8323-6},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00650575.png" border="0"> },
}

@inproceedings{650574,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Vengroff, D.E. and Gao, G.R.},
 year = {1998},
 pages = {342--351},
 publisher = {IEEE},
 title = {Partial sampling with reverse state reconstruction: A new technique for branch predictor performance estimation},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650574},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650574},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650574.pdf?arnumber=650574},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Automata, Computational modeling, Counting circuits, Data structures, Microarchitecture, Performance analysis, Performance evaluation, Predictive models, SPEC95, Sampling methods, Uncertainty, branch predictors, computer architecture, deterministic automata, finite automata, full-trace modeling, partial sampling, performance estimation, performance evaluation, reverse state reconstruction, },
 abstract = {Exploring the design space of branch predictors can consume tremendous computational resources. In order to mitigate this problem we present a new non-clustered sampling technique for rapidly evaluating the performance of a large number of branch predictors in a single rapid pass through a trace. The predictors studied in this single pass need not closely resemble one another. Each may use a radically different method of indexing into one or more arrays of two bit counters. In experiments with SPEC95 benchmarks we have found that while sampling on the order of one branch per every ten thousand we can typically produce correct results for all but a few hundredths of a percent of the branches in the sample. The only instances we have found where this is not the case are degenerate cases in which we show that full-trace modeling also fails to give accurate results. Our technique is based on a general approach we call partial sampling. Partial sampling maintains a generic data structure as it scans a trace. At selected sample points in the trace, this structure is queried to determine the behavior of particular operations. The sampled operations need not be clustered },
}

@inproceedings{744359,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Kaxiras, S. and Goodman, J.R.},
 year = {1999},
 pages = {161--170},
 publisher = {IEEE},
 title = {Improving CC-NUMA performance using Instruction-based Prediction },
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744359},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744359},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744359.pdf?arnumber=744359},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Access protocols, CC-NUMA performance, Computer architecture, Design optimization, Hardware, History, Instruction-based Prediction, Prefetching, adaptive cache coherence protocols, address based prediction, address based schemes, cache storage, coherent events, data block access history, directory based cache coherent NUMA shared memory, execution driven simulation, hardware resources, instruction sets, migratory sharing optimization, mis speculation rates, parallel architectures, parallel programming, producer consumer optimization, shared memory systems, small prediction structures, speculative execution, transparent shared memory, wide sharing optimization, },
 abstract = {We propose Instruction-based Prediction as a means to optimize directory based cache coherent NUMA shared memory. Instruction-based prediction is based on observing the behavior of load and store instructions in relation to coherent events and predicting their future behavior. Although this technique is well established in the uniprocessor world, it has not been widely applied for optimizing transparent shared memory. Typically, in this environment, prediction is based on data block access history (address based prediction) in the form of adaptive cache coherence protocols. The advantage of instruction-based prediction is that it requires few hardware resources in the form of small prediction structures per node to match (or exceed) the performance of address based prediction. To show the potential of instruction-based prediction we propose and evaluate three different optimizations: i) a migratory sharing optimization, ii) a wide sharing optimization, and iii) a producer consumer optimization based on speculative execution. With execution driven simulation and a set of nine benchmarks we show that i) for the first two optimizations, instruction-based prediction, using few predictor entries per node, outpaces address based schemes, and (ii) for the producer consumer optimization which uses speculative execution, low mis speculation rates show promise for performance improvements },
}

@inproceedings{1598111,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Hu, S. and Kim, I. and Lipasti, M.H. and Smith, J.E.},
 year = {2006},
 pages = { 41-- 52},
 publisher = {IEEE},
 title = {An approach for implementing efficient superscalar CISC processors},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598111},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598111},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598111.pdf?arnumber=1598111},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { CISC superblocks,  RISC-style micro-ops,  SPEC2000 benchmarks,  cache storage,  code cache,  code sequences,  collapsed 3-1 ALU,  dynamic translation software,  four-wide superscalar processor,  hardware-software codesign,  hardware-software codesign,  instruction level communication,  issue buffer,  macro-ops,  microprocessor chips,  multiprocessing systems,  pipeline processing,  pipelined instruction scheduling logic,  reduced instruction set computing,  register file ports,  superscalar CISC processors,  two-wide superscalar processor,  x86 ISA, Hardware, Instruction sets, Logic, Microarchitecture, Pipelines, Process design, Scheduling, Software performance, Software quality, Virtual manufacturing, },
 abstract = {An integrated, hardware/software co-designed CISC processor is proposed and analyzed. The objectives are high performance and reduced complexity. Although the x86 ISA is targeted, the overall approach is applicable to other CISC ISAs. To provide high performance on frequently executed code sequences, fully transparent dynamic translation software decomposes CISC superblocks into RISC-style micro-ops. Then, pairs of dependent micro-ops are reordered and fused into macro-ops held in a large, concealed code cache. The macro-ops are fetched from the code cache and processed throughout the pipeline as single units. Consequently, instruction level communication and management are reduced, and processor resources such as the issue buffer and register file ports are better utilized. Moreover, fused instructions lead naturally to pipelined instruction scheduling (issue) logic, and collapsed 3-1 ALUs can be used, resulting in much simplified result forwarding logic. Steady state performance is evaluated for the SPEC2000 benchmarks, and a proposed x86 implementation with complexity similar to a two-wide superscalar processor is shown to provide performance (instructions per cycle) that is equivalent to a conventional four-wide superscalar processor. },
}

@inproceedings{650571,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Abramson, D. and Logothetis, P. and Postula, A. and Randall, M.},
 year = {1998},
 pages = {324--333},
 publisher = {IEEE},
 title = {FPGA based custom computing machines for irregular problems},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650571},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650571},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650571.pdf?arnumber=650571},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Application software, Arithmetic, Bandwidth, Concurrent computing, Field programmable gate arrays, Hardware, High performance computing, Parallel processing, RISC processors, Reduced instruction set computing, Table lookup, concurrency, custom computing machines, field programmable gate arrays, field programmable gate arrays, irregular problems, parallel architectures, performance evaluation, programmable custom machines, speedup, },
 abstract = {Over the past few years there has been increased interest in building custom computing machines (CCMs) as a way of achieving very high performance on specific problems. The advent of high density field programmable gate arrays (FPGAs), in combination with new synthesis tools, have made it relatively easy to produce programmable custom machines without building specific hardware. In many cases, the performance achieved by a FPGA based custom computer is attributed to the exploitation of massive concurrency in the underlying application. In this paper we explore the sources of speedup for irregular problems in which is difficult to exploit such parallelism. We highlight 5 main sources of speedup that we have observed, namely the provision of high memory bandwidth, the use of flexible address generation hardware, the use of gather-scatter array operations, the use of lookup tables and the use of multiple tailored arithmetic units. By considering some representative examples of such irregular problems, the paper illustrates that good performance is possible given the current generation of FPGA devices and RISC processors. The paper then explores whether this performance gain will be possible given the next generation of RISC processors and FPGAs. It concludes that the only way to maintain the speedup is to alter the architecture of CCMs in combination with architectural changes to the FPGAs themselves },
}

@inproceedings{650570,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Speight, E. and Bennett, J.K.},
 year = {1998},
 pages = {312--322},
 publisher = {IEEE},
 title = {Using multicast and multithreading to reduce communication in software DSM systems},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650570},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650570},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650570.pdf?arnumber=650570},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {Application software, Brazos, Brazos software, Communication system software, Computer architecture, Costs, Electrical capacitance tomography, Multicast communication, Multithreading, Operating systems, Software performance, Software systems, distributed memory systems, distributed shared memory, local multiprocessing, multicast, multithreading, network operating systems, performance benefits, performance evaluation, shared memory systems, software DSM, },
 abstract = {This paper examines the performance benefits of employing multicast communication and application-level multithreading in the Brazos software distributed shared memory (DSM) system. Application-level multithreading in Brazos allows programs to transparently take advantage of available local multiprocessing. Brazos uses multicast communication to reduce the number of consistency-related messages, and employs two adaptive mechanisms that reduce the detrimental side effects of using multicast communication. We compare three software DSM systems running on identical hardware: (1) a single-threaded point-to-point system, (2) a multithreaded point-to-point system, and (3) Brazos, which incorporates both multithreading and multicast communication. For the six applications studied, multicast and multithreading improve speedup on eight processors by an average of 38\% },
}

@inproceedings{1598114,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Li, J. and Martinez, J.F.},
 year = {2006},
 pages = { 77-- 87},
 publisher = {IEEE},
 title = {Dynamic power-performance adaptation of parallel computation on chip multiprocessors},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598114},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598114},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598114.pdf?arnumber=1598114},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { CMP,  chip multiprocessors,  dynamic power-performance adaptation,  frequency scaling,  microprocessor chips,  multiprocessing systems,  optimisation,  parallel computation,  parallel processing,  power consumption,  power consumption,  power-performance characteristics,  voltage scaling, Circuits, Concurrent computing, Energy consumption, Frequency, Laboratories, Parallel processing, Runtime environment, System-on-a-chip, Voltage, Yarn, },
 abstract = {Previous proposals for power-aware thread-level parallelism on chip multiprocessors (CMPs) mostly focus on multiprogrammed workloads. Nonetheless, parallel computation of a single application is critical in light of the expanding performance demands of important future workloads. This work addresses the problem of dynamically optimizing power consumption of a parallel application that executes on a many-core CMP under a given performance constraint. The optimization space is two-dimensional, allowing changes in the number of active processors and applying dynamic voltage/frequency scaling. We demonstrate that the particular optimum operating point depends nontrivially on the power-performance characteristics of the CMP, the application's behavior, and the particular performance target. We present simple, low-overhead heuristics for dynamic optimization that significantly cut down on the search effort along both dimensions of the optimization space. In our evaluation of several parallel applications with different performance targets, these heuristics quickly lock on a configuration that yields optimal power savings in virtually all cases. },
}

@inproceedings{650572,
 booktitle = {High-Performance Computer Architecture, 1998. Proceedings., 1998 Fourth International Symposium on},
 author = {Miller, M.F. and Janik, K.J. and Shih-Lien Lu},
 year = {1998},
 pages = {334--341},
 publisher = {IEEE},
 title = {Non-stalling counterflow architecture},
 date = {1-4 Feb 1998},
 doi = {10.1109/HPCA.1998.650572},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650572},
 pdf_url = {http://ieeexplore.ieee.org/iel3/5242/14198/00650572.pdf?arnumber=650572},
 isbn = {0-8186-8323-6},
 language = {English},
 keywords = {CFPP, Computer aided instruction, Computer architecture, Costs, Data engineering, Decoding, Global communication, Java simulator, Pipelines, Radio frequency, Radiofrequency identification, System recovery, asynchronous circuits, counterflow architecture, counterflow pipeline, data movement, dataflow, design tradeoffs, distributed decision making, localized clocking, multithreading, parallel architectures, performance, performance evaluation, pipeline, pipeline processing, scalable architecture, virtual register, },
 abstract = {The counterflow pipeline concept was originated by Sproull et al.(1994) to demonstrate the concept of asynchronous circuits. This architecture relies on distributed decision making and localized clocking and data movement. We have taken these ideas and reformulated them into a substantially faster more scalable architecture that has the same distributed decision making and locality for clocking and data, but adds very aggressive speculation, no stalls, and other desirable characteristics. A high level Java simulator has been built to explore the design tradeoffs and evaluate performance },
}

@inproceedings{1385929,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Chaparro, P. and Magklis, G. and Gonzalez, J. and Gonzalez, A.},
 year = {2005},
 pages = { 61-- 70},
 publisher = {IEEE},
 title = {Distributing the frontend for temperature reduction},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.12},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385929},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385929.pdf?arnumber=1385929},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { bank hopping mechanism,  biased mapping function,  clustered microarchitectures,  computer power supplies,  cooling,  distributed bank accesses,  distributed frontend,  microprocessor chips,  power density reduction,  processor design,  subbanked trace cache,  temperature control,  temperature reduction,  thermal emergencies, Cooling, Costs, Dynamic voltage scaling, Leakage current, Microarchitecture, Power dissipation, Power generation, Process design, Temperature control, Threshold voltage, },
 abstract = {Due to increasing power densities, both on-chip average and peak temperatures are fast becoming a serious bottleneck in processor design. This is due to the cost of removing the heat generated, and the performance impact of dealing with thermal emergencies. So far microarchitectural techniques to control temperature have mainly focused on the processor backend (in particular the execution units), whereas the frontend has not received much attention. However, as the temperature of the backend remains controlled and the processor throughput increases, the heat dissipated by the frontend becomes more significant, and one of the major contributors to the total average temperature. This paper proposes and evaluates a distributed frontend for clustered microarchitectures that is able to reduce power density and temperature. First, a distributed mechanism for renaming and committing instructions is proposed. Second, a sub-banked trace cache with a bank hopping mechanism is presented. Finally, a method to improve the sub-banking is proposed based on a biased mapping function to distribute bank accesses to balance temperature. },
}

@inproceedings{1385928,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Sundaresan, K. and Mahapatra, N.R.},
 year = {2005},
 pages = { 51-- 60},
 publisher = {IEEE},
 title = {Accurate energy dissipation and thermal modeling for nanometer-scale buses},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.5},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385928},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385928.pdf?arnumber=1385928},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { ITRS road-map,  SPEC CPU2000 benchmarks,  bus line energy dissipation,  capacitive coupling,  computer power supplies,  data address buses,  dynamic simulation,  energy dissipation modeling,  global bus wires,  instruction address buses,  lateral heat transfer,  localized heating,  low-power bus design schemes,  nanometer-scale buses,  nanotechnology,  repeater insertion,  semiglobal bus wires,  switching energy,  system buses,  technology parameters,  technology scaling,  thermal analysis,  thermal modeling, Capacitance, Clocks, Delay effects, Dielectrics, Energy dissipation, Frequency, Repeaters, Temperature, Thermal conductivity, Wires, },
 abstract = {With technology scaling, power dissipation and localized heating in global and semi-global bus wires are becoming increasingly important, and this necessitates the development of accurate models to explore these effects during design stage, with simulators and using realistic workloads. In this work, we present a unified nanometer-scale bus energy dissipation and thermal model that helps designers monitor energy dissipation and temperature rise in individual wires during dynamic simulation. In addition to self capacitance, our model incorporates the effects of capacitive coupling between adjacent as well as non-adjacent pairs of wires on switching energy, effect of repeater insertion, and the effect of lateral heat transfer between adjacent wires, all of which have been ignored in earlier models. Next, using our integrated model in a first-of-its-kind study, we study energy dissipation and thermal characteristics of instruction and data address buses with traces obtained from standard SPEC CPU2000 benchmarks and using technology parameters for various nanometer technology nodes from the ITRS road-map. We also evaluate the effect of some well-known low-power bus design schemes on bus line energy dissipation. },
}

@inproceedings{1385927,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Kondo, M. and Nakamura, H.},
 year = {2005},
 pages = { 40-- 49},
 publisher = {IEEE},
 title = {A small, fast and low-power register file by bit-partitioning},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.3},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385927},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385927.pdf?arnumber=1385927},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { access delays,  bit partitioning,  computer power supplies,  instruction level parallelism,  logic partitioning,  low-power register file,  microprocessor chips,  multiported register file,  parallel architectures,  performance evaluation,  power consumption,  register entries,  register operands,  shift registers,  superscalar processors, Delay, Dynamic scheduling, Energy consumption, Microarchitecture, Microcomputers, Microprocessors, Parallel processing, Processor scheduling, Registers, Workstations, },
 abstract = {A large multi-ported register file is indispensable for exploiting instruction level parallelism (ILP) in today's dynamically scheduled superscalar processors. The number of ports and the size of the register file must be enlarged as the issue width and instruction window size increase. However, a larger register file causes longer access delays and more power consumption. To tackle these problems, we propose bit-partitioned register file which reduces the area, access time, and energy consumption of the register file. The proposed method relies on the fact that many operands do not need the full-bit width (typically a 32-bit or 64-bit width) of a register entry. Because the effective bit-width of most register operands is narrower than the full-bit width of a register entry, the upper bits of the register entries assigned to such narrow-width operands are useless. Thus, we propose to use of these useless upper bits for other operands by partitioning the register entries. In this paper, we show the mechanism of the proposed register file and evaluate its performance and power consumption. The evaluation results reveal that the proposed register file achieves higher instruction per cycle (IPC) in a smaller physical area, and consequently with shorter access time and less power consumption. },
}

@inproceedings{1385926,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Balasubramonian, R. and Muralimanohar, N. and Ramani, K. and Venkatachalapathy, V.},
 year = {2005},
 pages = { 28-- 39},
 publisher = {IEEE},
 title = {Microarchitectural wire management for performance and power in partitioned architectures},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.21},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385926},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385926.pdf?arnumber=1385926},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { VLSI,  VLSI techniques,  baseline processor,  cache pipeline design,  computer architecture,  computer power supplies,  heterogeneous interconnect,  high clock speeds,  high-performance billion-transistor processors,  integrated circuit interconnections,  inter-partition communication,  interconnect load imbalance,  logic partitioning,  low design complexity,  low power architectures,  microarchitectural wire management,  microprocessor chips,  narrow bit-width operands,  noncritical data,  partitioned architectures,  pipeline processing,  power consumption,  processor performance, Bandwidth, Clocks, Delay, Energy consumption, Energy management, Microarchitecture, Pipelines, Processor scheduling, Very large scale integration, Wire, },
 abstract = {Future high-performance billion-transistor processors are likely to employ partitioned architectures to achieve high clock speeds, high parallelism, low design complexity, and low power. In such architectures, inter-partition communication over global wires has a significant impact on overall processor performance and power consumption. VLSI techniques allow a variety of wire implementations, but these wire properties have previously never been exposed to the microarchitecture. This paper advocates global wire management at the microarchitecture level and proposes a heterogeneous interconnect that is comprised of wires with varying latency, bandwidth, and energy characteristics. We propose and evaluate microarchitectural techniques that can exploit such a heterogeneous interconnect to improve performance and reduce energy consumption. These techniques include a novel cache pipeline design, the identification of narrow bit-width operands, the classification of non-critical data, and the detection of interconnect load imbalance. For a dynamically scheduled partitioned architecture, our results demonstrate that the proposed innovations result in up to 11\% reductions in overall processor ED<sup>2</sup>, compared to a baseline processor that employs a homogeneous interconnect. },
}

@inproceedings{1385925,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Kirman, N. and Kirman, M. and Chaudhuri, M. and Martinez, J.F.},
 year = {2005},
 pages = { 16-- 27},
 publisher = {IEEE},
 title = {Checkpointed early load retirement},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.9},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385925},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385925.pdf?arnumber=1385925},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { aggressive hardware prefetcher,  architectural registers,  cache storage,  checkpointed early load retirement,  checkpointing,  dependent instructions,  floating point arithmetic,  floating-point applications,  load-value prediction,  long-latency loads,  memory architecture,  out-of-order processor,  register checkpointing,  reorder buffer,  resource allocation,  uncommitted instructions, Application software, Checkpointing, Computer aided instruction, Hardware, Laboratories, Out of order, Pipelines, Prefetching, Registers, Retirement, },
 abstract = {Long-latency loads are critical in today's processors due to the ever-increasing speed gap with memory. Not only do these loads block the execution of dependent instructions, they also prevent other instructions from moving through the in-order reorder buffer (ROB) and retire. As a result, the processor quickly fills up with uncommitted instructions, and computation ultimately stalls. To attack this problem, we propose checkpointed early load retirement, a mechanism that combines register checkpointing and back-end .e., at retirement - load-value prediction. When a long-latency load hits the ROB head unresolved, the processor enters clear mode by (1) taking a checkpoint of the architectural registers, (2) supplying a load-value prediction to consumers, and (3) early-retiring the long-latency load. This unclogs the ROB, thereby "clearing the way" for subsequent instructions to retire, and also allowing instructions dependent on the long-latency load to execute sooner. When the actual value returns from memory, it is compared against the prediction. A misprediction causes the processor to roll back to the checkpoint, discarding all subsequent computation. The benefits of executing in clear mode come from providing early forward progress on correct predictions, and from warming up caches and other structures on wrong predictions. Our evaluation shows that a clear implementation with support for four checkpoints yields an average speedup of 1.12 for both eleven integer and eight floating-point applications (1.27 and 1.19 for five integer and five floating point memory-bound applications, respectively), relative to a contemporary out-of-order processor with an aggressive hardware prefetcher. },
}

@inproceedings{1385924,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Tuck, N. and Tullsen, D.M.},
 year = {2005},
 pages = { 5-- 15},
 publisher = {IEEE},
 title = {Multithreaded value prediction},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.22},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385924},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385924.pdf?arnumber=1385924},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = { SPEC benchmarks,  long latency loads,  multi-threading,  multiprocessing systems,  multithreaded value prediction,  realistic hardware parameters,  simultaneous multithreading processor,  value-speculative execution, Application software, Circuit synthesis, Computer science, Delay, Hardware, Microarchitecture, Microprocessors, Multithreading, Performance gain, Yarn, },
 abstract = {This paper introduces a technique which leverages value prediction and multithreading on a simultaneous multithreading processor to achieve higher performance in a single threaded application. By allowing the value-speculative execution to proceed in a separate thread, this technique overcomes barriers that make traditional value prediction relatively ineffective for tolerating long latency loads. It shows that this technique can be as much as 2-5 times more effective than traditional value prediction, achieving more than 40\% average performance gain on the SPEC benchmarks with realistic hardware parameters. These gains come from two effects: allowing greater separation between the stalled load and the speculative execution, and the ability to speculate on multiple values for a single load. },
}

@inproceedings{1385923,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {Weber, F.},
 year = {2005},
 pages = { 3-- 3},
 publisher = {IEEE},
 title = {Trends in High-Performance Processors},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.40},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385923},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385923.pdf?arnumber=1385923},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = {Computer architecture, },
 abstract = {},
}

@inproceedings{1385922,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { xiv-- xiv},
 publisher = {IEEE},
 title = {Organizing Committee},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.24},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385922},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385922.pdf?arnumber=1385922},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 abstract = {},
}

@inproceedings{1385921,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { xi-- xi},
 publisher = {IEEE},
 title = {Reviewers},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.28},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385921},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385921.pdf?arnumber=1385921},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 keywords = {IEEE, },
 abstract = {},
}

@inproceedings{1385920,
 booktitle = {High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on},
 author = {},
 year = {2005},
 pages = { x-- x},
 publisher = {IEEE},
 title = {Message from the Program Chair},
 date = {12-16 Feb. 2005},
 doi = {10.1109/HPCA.2005.20},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385920},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9519/30167/01385920.pdf?arnumber=1385920},
 issn = {1530-0897  },
 isbn = {0-7695-2275-0},
 language = {English},
 abstract = {},
}

@inproceedings{569660,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Sivasubramaniam, A.},
 year = {1997},
 pages = {194--203},
 publisher = {IEEE},
 title = {Reducing the communication overhead of dynamic applications on shared memory multiprocessors},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569660},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569660},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569660.pdf?arnumber=569660},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Access protocols, Application software, Databases, Delay, Geographic Information Systems, Hardware, Information systems, Program processors, Programming profession, Scalability, communication overhead, competitive update mechanism, data transfer, dynamic applications, dynamic communication behavior, geographical information systems, intelligent sender-initiated data transfer mechanisms, invalidation-based protocols, memory architecture, performance benefits, performance evaluation, protocols, receiver-initiated communication, redundant updates, scalability, shared address space, shared memory architectures, shared memory multiprocessors, shared memory systems, spatial locality, temporal locality, write overheads, },
 abstract = {Shared memory machines offer the convenience of a shared address space. This makes them particularly appealing for applications with dynamic communication behavior since the mechanisms for data transfer between processors is hidden from the programmer. But the scalability of these machines is often limited by the latencies incurred in accessing locations in remote memories. Caches alleviate this problem by exploiting the temporal and spatial locality in an application. However, the induced traffic for maintaining coherence can have a large impact on limiting performance. Invalidation-based protocols for coherence maintenance are conservative and always resort to receiver-initiated communication. Thus the receiver may have to experience the entire latency of the data transfer even though the data item may have been available much earlier. Update-based schemes, though sender-initiated, can incur high write overheads by sending redundant updates to processors that may not need them. The goal of this research is to reduce the read and write latencies of applications with dynamic communication behavior by employing intelligent sender-initiated data transfer mechanisms. In the process, we would like to keep our demands from the programmer, the compiler, and the hardware as low as possible. Towards this goal, we present a set of write primitives that lower the communication overhead for shared memory accesses governed by locks. We demonstrate the performance benefits of these primitives using a database application drawn from the Geographical Information Systems (GIS) domain. We explore the competitive update mechanism for the remaining shared memory accesses. Using a set of applications, we examine the amount of history that we need to maintain for an effective competitive update scheme. We also show how this effective scheme can be implemented in software on emerging shared memory architectures with little hardware support },
}

@inproceedings{569661,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {Abdel-Shafi, H. and Hall, J. and Adve, S.V. and Adve, V.S.},
 year = {1997},
 pages = {204--215},
 publisher = {IEEE},
 title = {An evaluation of fine-grain producer-initiated communication in cache-coherent multiprocessors},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569661},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569661},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569661.pdf?arnumber=569661},
 isbn = {0-8186-7764-3},
 language = {English},
 keywords = {Application software, Computer science, Contracts, Delay, Prefetching, Read-write memory, Software performance, Software systems, cache-coherent multiprocessors, communication latency, fine-grain producer-initiated communication, irregular communication, memory system overhead, performance benefits, performance evaluation, pipelined loops, prefetching, shared memory systems, shared-memory multiprocessors, synchronisation, synchronization, },
 abstract = {Prefetching is a widely used consumer-initiated mechanism to hide communication latency in shared-memory multiprocessors. However, prefetching is inapplicable or insufficient for some communication patterns such as irregular communication, pipelined loops, and synchronization. For these cases, a combination of two fine-grain, producer-initiated primitives (referred to as remote-writes) is better able to reduce the latency of communication. This paper demonstrates experimentally that remote writes provide significant performance benefits in cache-coherent shared-memory multiprocessors with and without prefetching. Further, the combination of remote writes and prefetching is able to eliminate most of the memory system overhead in the applications, except misses due to cache conflicts },
}

@inproceedings{1598117,
 booktitle = {High-Performance Computer Architecture, 2006. The Twelfth International Symposium on},
 author = {Stenstrom, P.},
 year = {2006},
 publisher = {IEEE},
 title = {Chip-multiprocessing and beyond},
 date = {11-15 Feb. 2006},
 doi = {10.1109/HPCA.2006.1598117},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598117},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10647/33614/01598117.pdf?arnumber=1598117},
 issn = {1530-0897},
 isbn = {0-7803-9368-6},
 language = {English},
 keywords = { chip-multiprocessing,  high application performance,  higher level clock frequency,  instruction-level parallelism,  microprocessor chips,  multiple processor core,  multiprocessing systems,  on-chip cache hierarchies,  on-chip memory subsystem,  parallel architectures, Bandwidth, Bridges, Clocks, Frequency, },
 abstract = {Summary form only given. At a point in time when it is harder to harvest more instruction-level parallelism and to push the clock frequency to higher levels, industry has opted for integrating multiple processor cores on a chip. It is an attractive way of reducing the verification time by simply replicating moderately complex cores on a chip, but it introduces several challenges. The first challenge is to transform the processing power of multiple cores to a high application performance. The second challenge is to bridge the increasing speedgap between processor and memory by more elaborate on-chip memory hierarchies. Related to the second challenge is how to make more effective use of the limited bandwidth out of and into the chip. A third cross-cutting challenge is how we can move forward and yet manage the complexity of billion transistor chips. In this paper the author elaborates on the opportunities that chip-multiprocessing offer along with the research issues that it introduces. Even if multiprocessing has been studied for more than two decades, the tight integration of cores and their on-chip memory subsystems opens up new unexplored terrains. The author discusses and reflects upon approaches to explore new forms of parallelism as well as approaches to manage the on-chip cache hierarchies in the pursuit of making multi-core chips deliver higher performance. },
}

@inproceedings{1183551,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Bedford Taylor, M. and Lee, W. and Amarasinghe, S. and Agarwal, A.},
 year = {2003},
 pages = { 341-- 353},
 publisher = {IEEE},
 title = {Scalar operand networks: on-chip interconnect for ILP in partitioned architectures},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183551},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183551},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183551.pdf?arnumber=1183551},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { ILP,  Raw microprocessor,  bypass paths,  delays,  distributed resources,  microprocessor chips,  multiple ALU,  multiported register files,  on-chip interconnect,  parallel architectures,  partitioned architectures,  performance,  performance evaluation,  pipeline processing,  pipeline stages,  point-to-point interconnects,  scalability,  scalar operand networks,  ultra-fast operation-operand matching,  ultra-low latencies, Computer industry, Counting circuits, Delay, Distributed computing, Microprocessors, Network-on-a-chip, Pipelines, Registers, Scalability, System recovery, },
 abstract = {The bypass paths and multiported register files in microprocessors serve as an implicit interconnect to communicate operand values among pipeline stages and multiple ALU. Previous superscalar designs implemented this interconnect using centralized structures that do not scale with increasing ILP demands. In search of scalability, recent microprocessor designs in industry and academia exhibit a trend towards distributed resources such as partitioned register files, banked caches, multiple independent compute pipelines, and even multiple program counters. Some of these partitioned microprocessor designs have begun to implement bypassing and operand transport using point-to-point interconnects rather than centralized networks. We call interconnects optimized for scalar data transport, whether centralized or distributed, scalar operand networks. Although these networks share many of the challenges of multiprocessor networks such as scalability and deadlock avoidance, they have many unique requirements, including ultra-low latencies (a few cycles versus tens of cycles) and ultra-fast operation-operand matching. This paper discusses the unique properties of scalar operand networks, examines alternative ways of implementing them, and describes in detail the implementation of one such network in the Raw microprocessor. The paper analyzes the performance of these networks for ILP workloads and the sensitivity of overall ILP performance to network properties. },
}

@inproceedings{569701,
 booktitle = {High-Performance Computer Architecture, 1997., Third International Symposium on},
 author = {},
 year = {1997},
 pages = {353--353},
 publisher = {IEEE},
 title = {Author Index},
 date = {1-5 Feb 1997},
 doi = {10.1109/HPCA.1997.569701},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=569701},
 pdf_url = {http://ieeexplore.ieee.org/iel3/4300/12370/00569701.pdf?arnumber=569701},
 isbn = {0-8186-7764-3},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00569701.png" border="0"> },
}

@inproceedings{4658650,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Larson, R.H. and Salmon, J.K. and Dror, R.O. and Deneroff, M.M. and Young, C. and Grossman, J.P. and Yibing Shan and Klepeis, J.L. and Shaw, D.E.},
 year = {2008},
 pages = {331--342},
 publisher = {IEEE},
 title = {High-throughput pairwise point interactions in Anton, a specialized machine for molecular dynamics simulation},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658650},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658650},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658650.pdf?arnumber=4658650},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Acceleration, Anton, Arithmetic, Atomic measurements, Biological system modeling, Biology computing, Computational modeling, Feeds, Grid computing, Pipelines, Supercomputers, biochemistry, biology computing, high-throughput interaction subsystem, high-throughput pairwise point interactions, molecular biophysics, molecular dynamics method, molecular dynamics simulation, parallel machines, parallel special-purpose supercomputer, },
 abstract = {Anton is a massively parallel special-purpose supercomputer designed to accelerate molecular dynamics (MD) simulations by several orders of magnitude, making possible for the first time the atomic-level simulation of many biologically important phenomena that take place over microsecond to millisecond time scales. The majority of the computation required for MD simulations involves the calculation of pairwise interactions between particles and/or gridpoints separated by no more than some specified cutoff radius. In Anton, such range-limited interactions are handled by a high-throughput interaction subsystem (HTIS). The HTIS on each of Antonpsilas 512 ASICs includes 32 computational pipelines running at 800 MHz, each producing a result on every cycle that would require approximately 50 arithmetic operations to compute on a general-purpose processor. In order to feed these pipelines and collect their results at a speed sufficient to take advantage of this computational power, Anton uses two novel techniques to limit inter- and intra-chip communication. The first is a recently developed parallelization algorithm for the range-limited N-body problem that offers major advantages in both asymptotic and absolute terms by comparison with traditional methods. The second is an architectural feature that processes pairs of points chosen from two point sets in time proportional to the product of the sizes of those sets, but with input and output volume proportional only to their sum. Together, these features allow Anton to perform pairwise interactions with very high throughput and unusually low latency, enabling MD simulations on time scales inaccessible to other general- and special-purpose parallel systems. },
}

@inproceedings{744326,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Tullsen, D.M. and Lo, J.L. and Eggers, S.J. and Levy, H.M.},
 year = {1999},
 pages = {54--58},
 publisher = {IEEE},
 title = {Supporting fine-grained synchronization on a simultaneous multithreading processor},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744326},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744326},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744326.pdf?arnumber=744326},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Computer science, Costs, Hardware, Lifting equipment, Multithreading, Parallel processing, Program processors, Programming profession, Surface-mount technology, Yarn, blocked threads, code parallelization, fine-grained synchronization, lock release prediction, multi-threading, multiprocessing systems, parallel architectures, scalable mechanism, simultaneous multithreading processor, synchronisation, synchronization cost, },
 abstract = {This paper proposes and evaluates new synchronization schemes for a simultaneous multithreaded processor. We present a scalable mechanism that permits threads to cheaply synchronize within the processor, with blocked threads consuming no processor resources. We also introduce the concept of lock release prediction, which gains an additional improvement of 40\%. Overall, we show that these improvements in synchronization cost enable parallelization of code that could not be effectively parallelized using traditional techniques },
}

@inproceedings{744323,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Wallace, S. and Tullsen, D.M. and Calder, B.},
 year = {1999},
 pages = {44--53},
 publisher = {IEEE},
 title = {Instruction recycling on a multiple-path processor},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744323},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744323},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744323.pdf?arnumber=744323},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Bandwidth, Computer science, Decoding, Delay, Electronic switching systems, Hardware, Pipelines, Read only memory, Recycling, Yarn, decoded instructions, execution pipeline, fetch bandwidth problem, fetch latency, hard-to-predict branches, instruction recycling, instruction reuse, instruction traces, multi-threading, multiple-path processor, multiple-program workloads, operands, parallel architectures, performance, rename stage, simultaneous multithreading architecture, single-program workloads, speculative path, },
 abstract = {Processors that can simultaneously execute multiple paths of execution will only exacerbate the fetch bandwidth problem already plaguing conventional processors. On a multiple-path processor which speculatively executes less likely paths of hard-to-predict branches, the work done along a speculative path is normally discarded if that path is found to be incorrect. Instead, it can be beneficial to keep these instruction traces stored in the processor for possible future use. This paper introduces instruction recycling, where previously decoded instructions from recently executed paths are injected back into the rename stage. This increases the supply of instructions to the execution pipeline and decreases fetch latency. In addition, if the operands have not changed for a recycled instruction, the instruction can bypass the issue and execution stages, benefiting from instruction reuse. Instruction recycling and reuse are examined for a simultaneous multithreading architecture with multiple path execution. It is shown to increase performance by 7\% for single-program workloads and by 12\% on multiple-program workloads },
}

@inproceedings{744320,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Gatlin, K.S. and Carter, L.},
 year = {1999},
 pages = {33--42},
 publisher = {IEEE},
 title = {Memory hierarchy considerations for fast transpose and bit-reversals},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744320},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744320},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744320.pdf?arnumber=744320},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Algorithm design and analysis, Application software, Arithmetic, Cache Optimal Bit Reverse Algorith, Computer science, Digital Alpha 21164, Drives, IBM Power2, Linear algebra, Optimizing compilers, Ores, Read only memory, Sun, Sun Ultrasparc 2, algorithm design, bit-reversal reordering, cache storage, computer memory hierarchy, execution time, extra data movement, fast transpose, matrix algebra, matrix transpose, near optimal bit-reversal algorithm, performance degradation, scientific subroutines, subroutines, },
 abstract = {This paper explores the interplay between algorithm design and a computer's memory hierarchy. Matrix transpose and the bit-reversal reordering are important scientific subroutines which often exhibit severe performance degradation due to cache and TLB associativity problems. We give lower bounds that show for typical memory hierarchy designs, extra data movement is unavoidable. We also prescribe characteristics of various levels of the memory hierarchy needed to perform efficient bit-reversals. Insight gained from our analysis leads to the design of a near optimal bit-reversal algorithm. This Cache Optimal Bit Reverse Algorithm (COBRA) is implemented on the Digital Alpha 21164, Sun Ultrasparc 2, and IBM Power2. We show that COBRA is near optimal with respect to execution time on these machines and performs much better than previous best known algorithms },
}

@inproceedings{744329,
 booktitle = {High-Performance Computer Architecture, 1999. Proceedings. Fifth International Symposium On},
 author = {Parcerisa, J.-M. and Gonzalez, A.},
 year = {1999},
 pages = {59--63},
 publisher = {IEEE},
 title = {The synergy of multithreading and access/execute decoupling},
 date = {9-13 Jan 1999},
 doi = {10.1109/HPCA.1999.744329},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=744329},
 pdf_url = {http://ieeexplore.ieee.org/iel4/6021/16082/00744329.pdf?arnumber=744329},
 isbn = {0-7695-0004-8},
 language = {English},
 keywords = {Clocks, Delay, Dynamic scheduling, Hardware, Microarchitecture, Multithreading, Out of order, Registers, Throughput, Yarn, access/execute decoupling, clock speed, critical path delays, delays, functional unit latency hiding, hardware complexity reduction, in-order issue policy, in-order issue stage, issue width, maximum throughput, memory latency hiding efficiency, memory system, miss latency tolerance, multi-threading, multithreaded architecture, parallel architectures, partitioned layout, processor microarchitecture, processor scheduling, simulations, simultaneous multithreading, virtual machines, },
 abstract = {This work presents and evaluates a novel processor microarchitecture which combines two paradigms: access/execute decoupling and simultaneous multithreading. We investigate how both techniques complement each other: while decoupling features an excellent memory latency hiding efficiency, multithreading supplies the in-order issue stage with enough ILP to hide the functional unit latencies. Its partitioned layout, together with its in-order issue policy makes it potentially less complex, in terms of critical path delays, than a centralized out-of-order design, to support future growths in issue-width and clock speed. The simulations show that by adding decoupling to a multithreaded architecture, its miss latency tolerance is sharply increased and in addition, it needs fewer threads to achieve maximum throughput, especially for a large miss latency. Fewer threads result in a hardware complexity reduction and lower demands on the memory system, which becomes a critical resource for large miss latencies, since bandwidth may become a bottleneck },
}

@inproceedings{4658656,
 booktitle = {High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International Symposium on},
 author = {Kumar, S. and Aggarwal, A.},
 year = {2008},
 pages = {405--414},
 publisher = {IEEE},
 title = {Speculative instruction validation for performance-reliability trade-off},
 date = {16-20 Feb. 2008},
 doi = {10.1109/HPCA.2008.4658656},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4658656},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4653641/4658618/04658656.pdf?arnumber=4658656},
 issn = {1530-0897},
 isbn = {978-1-4244-2070-4},
 language = {English},
 keywords = {Clocks, Computer aided instruction, Computer errors, Concurrent Error Detection, Error analysis, Frequency, Instruction Validation, Microprocessors, Performance-Reliability Trade-off, Proposals, Reducing Instruction Redundancy, Redundancy, Redundant Multi-threading, Voltage, Yarn, concurrent error detection, formal verification, microcomputers, microprocessors, multi-threading, performance-reliability trade-off, redundant multithreading, speculative instruction validation, },
 abstract = {With reducing feature size, increasing chip capacity, and increasing clock speed, microprocessors are becoming increasingly susceptible to transient (soft) errors. Redundant multi-threading (RMT) is an attractive approach for concurrent error detection. RMT provides complete error coverage, while incurring a significant performance impact because of the redundant thread. Achieving perfect reliability at the expense of a high performance drop is not a good design option for systems where slight vulnerability may still achieve the desired error rates. In this paper, we explore speculative mechanisms to trade-off reliability for performance in RMT. Our basic approach validates the execution of an instruction by comparing its result against the expected result. Only those instructions are redundantly executed for which the validations fail. This mechanism is expected to have a minimal vulnerability impact because it is highly unlikely that an erroneous result matches the expected value. We also propose several extensions to the basic approach that further explore the performance-reliability trade-off design space. A combination of these techniques incur about 10\% performance impact and about 0.09\% undetected base error rate, compared to about 25\% performance impact for RMT with no undetected errors. },
}

@inproceedings{1410085,
 booktitle = {High Performance Computer Architecture, 2004. HPCA-10. Proceedings. 10th International Symposium on},
 author = {Kharbutli, M. and Irwin, K. and Solihin, Y. and Lee, J.},
 year = {2004},
 pages = { 288-- 299},
 publisher = {IEEE},
 title = {Using prime numbers for cache indexing to eliminate conflict misses},
 date = {14-18 Feb. 2004},
 doi = {10.1109/HPCA.2004.10015},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410085},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9679/30563/01410085.pdf?arnumber=1410085},
 issn = {1530-0897  },
 isbn = {0-7695-2053-7},
 language = {English},
 keywords = { cache indexing,  cache storage,  hashing function,  memory intensive application,  prime displacement,  prime modulo,  prime number,  skewed associative cache, Computer science, Concurrent computing, Costs, Distributed computing, Educational programs, Educational technology, Hardware, Indexing, Pathology, Performance analysis, },
 abstract = {Using alternative cache indexing/hashing functions is a popular technique to reduce conflict misses by achieving a more uniform cache access distribution across the sets in the cache. Although various alternative hashing functions have been demonstrated to eliminate the worst case conflict behavior, no study has really analyzed the pathological behavior of such hashing functions that often result in performance slowdown. We present an in-depth analysis of the pathological behavior of cache hashing functions. Based on the analysis, we propose two new hashing functions: prime modulo and prime displacement that are resistant to pathological behavior and yet are able to eliminate the worst case conflict behavior in the L2 cache. We show that these two schemes can be implemented in fast hardware using a set of narrow add operations, with negligible fragmentation in the L2 cache. We evaluate the schemes on 23 memory intensive applications. For applications that have nonuniform cache accesses, both prime modulo and prime displacement hashing achieve an average speedup of 1.27 compared to traditional hashing, without slowing down any of the 23 benchmarks. We also evaluate using multiple prime displacement hashing functions in conjunction with a skewed associative L2 cache. The skewed associative cache achieves a better average speedup at the cost of some pathological behavior that slows down four applications by up to 7\%. },
}

@inproceedings{1183518,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {},
 year = {2003},
 publisher = {IEEE},
 title = {Proceedings the Ninth International Symposium on High-Performance Computer Architecture. HPCA-9 2003},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183518},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183518},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183518.pdf?arnumber=1183518},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = { branch prediction,  cache storage,  caching,  communication performance,  high-performance computer architecture,  local area networks,  memory architecture,  memory performance,  multi-threading,  multi-threading,  multiprocessing systems,  multiprocessor systems,  networks,  parallel architectures,  performance evaluation,  power consumption,  power efficient designs,  prefetching,  profiling,  simulation,  simulation support,  superscalars, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01183518.png" border="0"> },
}

@inproceedings{1183519,
 booktitle = {High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on},
 author = {Bhandarkar, D.},
 year = {2003},
 pages = { 3-- 3},
 publisher = {IEEE},
 title = {Billion transistor chips in mainstream enterprise platforms of the future},
 date = {8-12 Feb. 2003},
 doi = {10.1109/HPCA.2003.1183519},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1183519},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8433/26557/01183519.pdf?arnumber=1183519},
 issn = {1530-0897  },
 isbn = {0-7695-1871-0},
 language = {English},
 keywords = {Computer aided manufacturing, Lead compounds, Manufacturing processes, Microprocessors, Moore's Law, Multiprocessing systems, Nanotechnology, Parallel processing, Semiconductor device manufacture, Transistors, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01183519.png" border="0"> },
}

@inproceedings{,
 booktitle = {},
 author = {},
 year = {},
 abstract = {},
}

