%T Listening Typewriter Simulation Studies
%A A. F. Newell
%A J. L. Arnott
%A K. Carter
%A G. Cruickshank
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 1
%P 1-19
%* (c) Copyright 1990 Academic Press
%X In order to investigate the acceptability of automatic speech recognition
systems in a creative writing task, experiments have been performed using a
simulation in which a human operator is used to convert the speech signal into
orthography.  Pilot experiments investigated the appropriateness of natural
dialogue in such a task.  The major experiment was a partial replication of
Gould, Conti and Hovanyecz (1983) "Listening Typewriter" simulation experiment.
In contrast to Gould et al., however, a machine shorthand transcription system
was used rather than a QWERTY keyboard.  This ensured that the simulation of
the speech recognizer was not restricted by the speed at which the operator
could enter text.  Also an important variable in the experiment was whether or
not the subjects had been made aware that they were using a simulation and not
a fully automatic speech recognition machine.

%T Supervised Learning in N-Tuple Neural Networks
%A J. R. Doyle
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 1
%P 21-40
%* (c) Copyright 1990 Academic Press
%X An N-tuple Neural Network (NNN) is described in which each node fires
selectively to its own table of binary trigger patterns.  Each node receives
input from k input terminals.  Supervised learning is used with specially
constructed problems: the system is taught to map specific instances of an
input set onto specific instances of an output set.  Learning is achieved by:
(1) calculating a global error term (how far the set of actual outputs differs
from the desired set of outputs); (2) either changing the connections between
input terminals and N-tuple nodes, or by changing the trigger patterns that the
node fires to; (3) re-calculating the global error term, and retaining the
changes to the network if the error is less than in (1).  Steepest descent
optimisation described in (3), is compared with simulated annealing
optimisation.  Simulated annealing gives better solutions.  Other results are
that as connectivity k increases the number of possible solutions increases,
but the number of possible non-solutions increases even faster.  Simulated
annealing is particularly helpful when the relative difficulty (ratio of search
to solution) increases.  In randomly chosen network configurations there is
less entropy in the output than there is in the input to the system.  When
output is re-cycled as input, NNN either cycles or reaches an end-point.  When
solving complex I/O maps the system counteracts this trend by systematically
increasing its sensitivity.  Predicativity can be improved by combining the
results of two or more independent NNN models.

%T Font Recognition by a Neural Network
%A Ming-Chih Lee
%A William J. B. Oldham
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 1
%P 41-61
%* (c) Copyright 1990 Academic Press
%X Two neural network models, labelled Model H-H1 and model H-H2 by Hogg and
Huberman have been successfully applied to recognize 26 English capital
letters, each with six font representations.  These two models are very
similar, but Model H-H2 has the capability for modification of the basins of
attraction during the training phase, whereas Model H-H1 does not.  This
appears to be a desirable feature for a neural network.  It is shown in this
work that this is indeed true.  In either model, it is difficult to find a
single set of parameters for one network or memory that can distinguish all of
the characters.  Therefore, a cascade of memories is utilized.  Thus, in the
training phase, a decision tree is built by cascading the memory matrices that
represent the models.  That is successive layers of refinement in selection of
basins of attraction are used to generate output patterns unique to each input
pattern.  In the recognition phase, the subject characters are recognized by
searching in the tree.  Model parameters such as memory array size, S{sub:min}
S{sub:max}, and M{sub:min} M{sub:max} were varied to elucidate the models'
behavior.  It is shown that there exist parameter values for both models to
achieve a 100% recognition rate when all six fonts are used both as the
training and the recognition set.  Model H-H2 significantly outperformed Model
H-H1 in terms of recognition rate, use of memory space, and learning speed when
all six fonts were used as the training set.

%T An Algorithm for Segmenting Handwritten Postal Codes
%A Michel Cesar
%A Rajjan Shinghal
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 1
%P 63-80
%* (c) Copyright 1990 Academic Press
%X Postal codes, although known by different names, are used in many countries
to uniquely identify a location.  In automated mail sorting, these postal codes
are recognized by optical character readers.  One problem faced for the
recognition of handwritten postal codes is that of segmentation; each character
in the postal code should ideally be assigned a different segment. 
Difficulties arise in segmentation because of the writing habits of people:
characters may be broken, may touch one another, or may overlap one another. 
In this paper we present an algorithm to segment handwritten postal codes.  The
heuristic components of the algorithm were developed after analysing the
writing habits of the people.  Experimental results with handwritten Canadian
postal codes show the algorithm to be effective, robust, and general enough
that it can be suitably adapted for the postal codes of many other countries.

%T The Impact of Pascal Education on Debugging Skill
%A Dan N. Stone
%A Eleanor W. Jordan
%A M. Keith Wright
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 1
%P 81-95
%* (c) Copyright 1990 Academic Press
%X Education in the Pascal programming language has been touted as a means of
learning structured programming principles.  This paper reports the results of
two experiments that tested the effect of Pascal education on the debugging
skills of novice programmers under timed test conditions.  Results of both
experiments indicate (1) that Pascal education is a better predictor of
debugging performance than major, previous COBOL education, number of computer
courses taken, or professional programming experience, and (2) that novice
programmers who have studied Pascal demonstrate superior debugging performance
regardless of program structure.  Measures of program comprehension used in
Experiment Two suggest that Pascal education may improve debugging performance
by increasing the comprehension of program goals and plans.  These results
suggest that the value of structured programming techniques may be realized
more in the programmer's way of thinking about a program than in the creation
of a structured program per se.

%T Individual Differences and Effective Learning Procedures:
The Case of Statistical Computing
%A Alison J. K. Green
%A Kenneth J. Gilhooly
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 1
%P 97-119
%* (c) Copyright 1990 Academic Press
%X This paper reports two experiments examining individual differences in
procedures for learning to use MINITAB (1982 edition).  In Experiment 1, ten
novices provided think aloud protocols over five sessions of learning to use
MINITAB.  On the basis of overall performance, novices were divided into two
groups of five faster and five slower learners.  The protocols suggested that
subjects used two major classes of learning strategy: learning by doing and
learning by thinking.  Each class of learning strategy comprised a set of
learning procedures.  Differences in procedure usage were confined primarily to
the learning by doing procedures.  Faster learners used the mapping and
exploratory procedures more frequently than slower learners, paid more
attention to prompts and error messages and acted appropriately on evaluation
feedback.  In contrast, slower learners used the trial and error and repetition
procedures more frequently than faster learners.  In Experiments 2, 26 novices
were allocated to one of three experimental groups.  Group 1 received no
instruction, Group 2 received instruction in the use of ineffective procedures
(those procedures that did not serve to differentiate between fast and slow
learners in Experiment 1) and Group 3 received instruction in the use of
effective procedures (those procedures that did differentiate between fast and
slow learners in Experiment 1).  The overall performance of the effective
procedures groups was significantly better than either the control or the
ineffective procedures groups.  Effective procedure usage ratings correlated
significantly with overall performance.  The negative correlations between
effective procedure usage ratings and both requests for help and mean time to
complete the MINITAB tasks were significant.  Finally, on a free recall task,
the effective procedures group remembered significantly more of the procedures
they had been taught than the ineffective procedure groups.

%T An Introduction to Hypermedia Issues, Systems and Application Areas
%A John A. Begoray
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 2
%P 121-147
%* (c) Copyright 1990 Academic Press
%X This article is intended to be a general introduction to the new information
representation technology: hypermedia.  There is a lack of consensus as to a
specific definition of hypermedia.  Several general characteristics and
specific terms are, however, emerging and are presented here as an introduction
to the area.
   A number of design issues associated with hypermedia systems have been
identified.  This survey presents these issues and discusses a number of the
more pre-eminent hypermedia systems within the context of the issues they
address.
   From one perspective, hypermedia is not an application.  It is, instead, a
technology which can be used to develop and enhance many application areas. 
Hypermedia's potential contribution to some of these areas is discussed and
some general conclusions presented.

%T A Database Interface Based on Montague's Approach to the Interpretation
of Natural Language
%A R. A. Frost
%A W. S. Saba
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 2
%P 149-176
%* (c) Copyright 1990 Academic Press
%X In this paper we describe a database interface that is loosely based upon
some of the concepts proposed by Richard Montague in his approach to the
interpretation of natural language.  The system is implemented as an executable
attribute grammar specified in a higher order, lazy, pure functional
programming language.  The attribute grammar formalism provides a simple means
of implementing Montague's notion of "semantic rule to syntactic rule
correspondence" and the higher order functional language in which the
attributes grammar is constructed provides an appropriate vehicle for
implementing Montague's higher order semantics.  The purpose of the paper is
two-fold: (i) to demonstrate that many of Montague's ideas can be used to
advantage in creating natural language interfaces to databases, and (ii) to
introduce a method for implementing attribute grammars in functional languages
that is suitable for investigating both grammars and semantic theories of
language.

%T Validation of Intent Inferencing by a Model-Based Operator's Association
%A Patricia M. Jones
%A Christine M. Mitchell
%A Kenneth S. Rubin
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 2
%P 177-202
%* (c) Copyright 1990 Academic Press
%X OFMspert (Operator Function Model expert system) is an architecture for an
intelligent operator's associate.  The function of such an associate is to
provide intelligent assistance for the human operator of a complex dynamic
system.  The basis for intelligent, context-sensitive advice and reminders is
the ability of the associate to infer likely operator intentions in real time.
This paper describes the implementation and validation of OFMspert's intent
inferencing capability.  In particular, a two-stage methodology for validation
is proposed.  This methodology is then used in the experimental validation of
OFMspert's intent inferencing.

%T Utilizing a Process Model During the Acquisition of Redesign Knowledge
%A Evangelos Simoudis
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 2
%P 203-226
%* (c) Copyright 1990 Academic Press
%X The paper presents REKL, a learning component which interacts with TLTS, a
knowledge-based tool that uses a model for redesign in the domain of printed
circuit boards.  The model has been implemented around a blackboard
architecture.  REKL acquires four types of redesign knowledge: (meta-knowledge,
eligibility demons, redesign operator scheduling rules, and clusters of
redesign operators).  The last three types of knowledge are organized by REKL
into redesign knowledge sources.  REKL acquires new redesign knowledge sources
by compiling the knowledge that is contained in redesign plans that the user
creates in cooperation with TLTS.  It acquires meta-knowledge for selecting
among the redesign knowledge sources, by interacting with the user and TLTS.

%T A Comparison of Formal Knowledge Representation Schemes as Communication
Tools: Predicate Logic vs Semantic Network
%A John T. Nosek
%A Itzhak Roth
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 2
%P 227-239
%* (c) Copyright 1990 Academic Press
%X An experiment was conducted to test the effectiveness of two popular
knowledge representation schemes as communication vehicles between the human
expert and the knowledge engineer.  Validation by the human expert of the
knowledge encapsulated depends upon how well the expert understands and
interprets a representation scheme.  A between-group experiment was conducted.
Each group received two treatments of the same representation technique, with
the second treatment slightly more complex that the first.  All the scores for
the Semantic Network representations were higher than that obtained for the
Predicate Logic representations; and the Semantic Networks were clearly better
for comprehension and conceptualization tasks.  The results demonstrate some of
the weaknesses of Predicate Logic and some of the strengths of Semantic
Networks as communication tools during the validation process.

%T Categories of Programming Knowledge and Their Application
%A Ruven Brooks
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 3
%P 241-246
%* (c) Copyright 1990 Academic Press
%X N/A

%T More or Less Following a Plan During Design: Opportunistic Deviations
in Specification
%A Willemien Visser
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 3
%P 247-278
%* (c) Copyright 1990 Academic Press
%X An observational study was conducted on a mechanical engineer throughout his
task of defining the functional specifications for the machining operations of
a factory automation cell.  The engineer described his activity as following a
hierarchically structured plan.  The actual activity is in fact
opportunistically organized.  The engineer follows his plan as long as it is
cognitively cost-effective.  As soon as other actions are more interesting, he
abandons his plan to proceed to these actions.  This paper analyses when and
how these alternative-to-the-plan actions come up.  Quantitative results are
presented with regard to the degree of plan deviation, the design components
and the definitional aspects which are most affected by these deviations, and
the deviation patterns.  Qualitative results concern their nature.  An
explanatory framework for plan deviation is proposed in the context of a
blackboard model.  Plan deviation is supposed to occur if the control,
according to certain selection criteria, selects an
alternative-to-the-planned-action proposal rather than the planned action
proposal.  Implications of these results for assistance tools are discussed
briefly.

%T Knowledge Exploited by Experts during Software System Design
%A Raymonde Guindon
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 3
%P 279-304
%* (c) Copyright 1990 Academic Press
%X High-level software design is characterized by incompletely specified
requirements, no predetermined solution path, and by the integration of
multiple domains of knowledge at various levels of abstraction.  The
application of data-driven knowledge rules characterizes expertise.  A verbal
protocol study describes these domains of knowledge and how experts exploit
their rich knowledge during design.  It documents how designers heavily rely on
problem domain scenario simulations throughout solution development.  These
simulations trigger the inferences of new requirements and complete the
requirement specification.  Designers recognize partial solutions at various
levels of abstraction in the design decomposition through the application of
data-driven rules.  Designers also rely heavily on simulations of their design
solutions, but these are shallow, that is, limited to one level of design
methods, notations, and specialized software design schemas.  Finally, the
study describes how designers exploit powerful heuristics and personalized
evaluation criteria to constrain the design process and select a satisfactory
solution.  Studies, such as this one, help map the road to understanding
expertise in complex tasks.

%T Variability in Program Design: The Interaction of Process with Knowledge
%A Robert S. Rist
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 3
%P 305-322
%* (c) Copyright 1990 Academic Press
%X A model of program design is proposed to explain program variability, and is
experimentally supported.  Variability is shown to be the result of different
decisions made by programmers during three stages in the design process.  In
the first stage, a solution is created based on a particular design approach. 
In the second stage, actions in the solution are organized by features they
share.  The actions may then be merged together to define a more concise
solution in program code, the third stage of design.  Different programs will
be created depending on the approach taken to design the features selected to
group actions in a solution, and the features used to merge actions to form
program code.  Each of the variants observed in the study was traced to the use
of a specific piece of information by a programmer at one of the three stages
of program design.  Many different programs were created as the process of
design interacted with the knowledge of the programmer.

%T An Empirically-Derived Control Structure for the Process of Program
Understanding
%A Francoise Detienne
%A Elliot Soloway
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 3
%P 323-342
%* (c) Copyright 1990 Academic Press
%X Various models of program understanding have been developed from the Schema
Theory.  To date, the authors have sought to identify the knowledge that
programmers have and use in understanding programs, i.e. Programming Plans and
Rules of Discourse.  However, knowledge is only one aspect of program
understanding.  The other aspect is the cognitive mechanisms that use
knowledge.  The contribution of this study is the identification of different
mechanisms involved in program understanding by experts, specifically the
mechanisms which cope with novelty.  An experiment was conducted to identify
and describe the expert's strategies involved in understanding usual
(plan-like) and unusual (unplan-like) programs.  While performing a
fill-in-a-blank task, subjects were asked to talk aloud.  The analysis of
verbal protocols allowed the identification of four different strategies of
understanding.  Under "normal" conditions the strategy of symbolic simulation
is involved.  But when failures occur additional strategies are required.  The
authors identified three types of understanding failures the subject may
experience (no expectation, expectation clashes, insufficient expectations) and
the additional strategies invoked in those cases: (1) reasoning according to
rules of discourse and principles of the task domain; (2) reasoning with plan
constraints; (3) concrete simulation.  The authors develop an operational
description of these strategies and discuss the control structure of program
understanding in the framework of schema theory.

%T Common Cognitive Representations of Program Code Across Tasks and Languages
%A Scott P. Robertson
%A Chiung-Chen Yu
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 3
%P 343-360
%* (c) Copyright 1990 Academic Press
%X Plans are underlying cognitive structures used by programmers to represent
code.  In two studies we examined the content of plan-based representations and
sought to show that common representations are used for programs that
instantiate the same plans, even when they perform different tasks and are
written in different languages (Pascal or FORTRAN).  Our results support
plan-based models and show that the organizing structures for chunks of code
are abstract programming goals.  The same abstract structures are formed for
programs that perform different tasks using the same plans and for programs
written in different languages but using the same plans.  While plans were the
primary organizing structures for code representations, other task-related
information also played a role suggesting that programmers really utilize
multiple representations.  We advocate viewing code comprehension more like a
plan recognition process and less like a text comprehension process.

%T A Methodology for Validating Large Knowledge Bases
%A Rajiv Enand
%A Gary S. Kahn
%A Robert A. Mills
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 361-371
%* (c) Copyright 1990 Academic Press
%X Knowledge acquisition is not complete until a knowledge base is fully
verified and validated.  During verification and validation, knowledge bases
are substantially refined and corrected.  This paper offers a methodology for
verification and validation that focuses knowledge acquisition on a
progressively deeper set of issues related to knowledge base correctness. 
These are knowledge base verification, domain validation, procedural
validation, and procedural optimization.  This methodology has been developed
in the course of using the TESTBENCH diagnostic shell to build a large system.

%T Developing a Tool for Knowledge Integration: Initial Results
%A Kenneth S. Murray
%A Bruce W. Porter
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 373-383
%* (c) Copyright 1990 Academic Press
%X Knowledge integration is the task of incorporating new information into
existing knowledge.  The task is difficult because the consequences of an
addition to an extensive knowledge base can be numerous and subtle.  Current
methods for automated knowledge acquisition largely ignore this task, although
it is increasingly important with the move toward large scale, multifunctional
knowledge bases.  To study knowledge integration, we propose to develop and
evaluate a knowledge-acquisition tool that helps with extending a knowledge
base through interaction with a knowledge engineer.  An initial prototype of
this tool has been implemented and demonstrated on a complex extension to a
large knowledge base.

%T Semi-Automatic Acquisition of Conceptual Structure from Technical Texts
%A Stan Szpakowicz
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 385-397
%* (c) Copyright 1990 Academic Press
%X We present a system which processes technical text semi-automatically and
incrementally builds a conceptual model of the domain.  Starting from an
initial general model, knowledge-based text understanding is turned into
knowledge acquisition.  Incompletely understood text fragments may contain new
information which should be integrated into the model under the control of an
operator.  The text is assumed to describe the domain fully.  Typical problems
in this domain are assumed to be solvable by indicating activities which
manipulate objects.  Activities, objects and their properties enter
relationships that form a conceptual network.  To test our representation, we
have created a large hierarchy of concepts for PowerHouse Quiz.  The system
relies in its operation on the text and the growing network; it includes a
parser with broad syntactic coverage, and a matcher retrieving subnetworks
relevant to the current text fragment.  The frequency of the operator's
necessary interventions depends on the initial network's size which will be
determined experimentally.  We discuss the status of the system and outline
further work.

%T Semantic Strings: A New Technique for Detecting and Correcting User Errors
%A James H. Bradford
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 399-407
%* (c) Copyright 1990 Academic Press
%X Modern software is used to control a spectrum of industrial applications
ranging from banking networks to nuclear reactors.  The design of an effective
error-handling strategy for user interfaces is becoming a vital issue.  Such a
strategy must include techniques for handling lexical, grammatical and semantic
errors.  The handling of semantic errors is the most difficult of these
problems.  This paper introduces a new technique for detecting and correcting
semantic errors in user dialogue.

%T Individualized Tutoring Using an Intelligent Fuzzy Temporal Relational
Database
%A L. W. Hawkes
%A S. J. Derry
%A E. A. Rundensteiner
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 409-429
%* (c) Copyright 1990 Academic Press
%X The student record (SR) is a major source of input for any decision making
done by an Intelligent Tutoring System (ITS) and is a basis of the
individualization in such systems.  However, most ITSs still have "generalized"
student models which represent a type of student rather than a particular one.
Until the SR becomes truly representative of each individual student, the goal
of providing individualized tutoring cannot be attained.  In this paper we
describe an Intelligent Fuzzy Temporal Relational Database (IFTReD), and
intelligent system-independent SR which allows for almost any degree of
individualization the designer wishes to incorporate.  It is anticipated that
this IFTReD will provide a significant improvement over standard AI storage
techniques for the SR.  These improvements will be realized in terms of: (1)
intelligence; (2) greater storage efficiency; (3) greater speed in retrieval
and query; (4) ability to handle linguistic codes, ranges, fuzzy possibilities,
and incomplete data in student models; (5) friendliness of query language; (6)
availability of temporal knowledge to give a history of past performance; and
(7) a more holistic view of the student, permitting greater individualization
of the tutor.

%T EMCUD: A Knowledge Acquisition Method which Captures Embedded Meanings Under
Uncertainty
%A G. J. Hwang
%A S. S. Tseng
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 431-451
%* (c) Copyright 1990 Academic Press
%X In this paper, we propose a knowledge acquisition method EMCUD which can
elicit embedded meanings of the initial knowledge provided by domain experts. 
EMCUD also helps experts to decide uncertainty of the embedded meanings
according to the relationships of the embedded meanings and the initial
knowledge.  The strategy of EMCUD could easily be added to the repertory
grid-oriented methods or systems to enhance the knowledge in the prototype.  We
present a realistic example to show how EMCUD enriches the knowledge base
constructed by a repertory grid-oriented method and hence ease the refinement
processes.

%T Direct Manipulation as a Source of Cognitive Feedback:
A Human-Computer Experiment with a Judgment Task
%A Dov Te'eni
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 453-466
%* (c) Copyright 1990 Academic Press
%X Correctly designed feedback can play a pivotal role in improving
performance.  Human-computer interaction can generate various forms of
feedback, and it is important to examine the effectiveness of the different
options.  This work compares: (1) feedback that is presented as information
which is distinct from the user's action, with (2) feedback that is generated
by direct manipulation and is embedded in the same information which
facilitates the user's action.  The former is the traditional form of feedback
in which the user acts and receives feedback information from a distinct
source.  The latter is information generated during the user's action, and it
becomes effective feedback only when the individual uses this information as
feedback.
   This paper explains why certain elements of direct manipulation can invoke
the second form of feedback.  An experiment demonstrates that feedback
resulting from direct manipulation is more effective and time efficient than
the distinct form of feedback in conditions of high task complexity.  However,
direct manipulation has its limits and must be complemented with traditional
forms of feedback for complex cognitive tasks.

%T Two Perspectives of the Dempster-Shafer Theory of Belief Functions
%A Pawan Lingras
%A S. K. M. Wong
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 4
%P 467-487
%* (c) Copyright 1990 Academic Press
%X This study presents two different perspectives of the Dempster-Shafer theory
of belief functions.  The first view of the theory, called the compatibility
view, constructs the belief function for a frame of discernment by using the
compatibility relationship of the frame with another frame called evidence
frame for which the probability function is known.  The second view referred to
as the probability allocation view provides a generalization of the Bayesian
theory by allocating the probability mass to the propositions which may not
necessarily be singleton sets of possible answers, based on some vague body of
evidence.  Both these views are useful in the design and implementation of
expert systems.  The belief functions constructed using the evidence frames are
useful in situations where limited information regarding the relationship
between two frames of discernment is available.  The belief functions based on
the allocation of probability mass are useful when the evidence cannot be
explicitly expressed in terms of propositions, but probability allocation based
on evidence is possible.  A successful implementation of these two views
requires a rule for combining evidence.  However, the Dempster rule of
combination cannot incorporate dependencies among different bodies of evidence.
This study illustrates a possible method for incorporating dependencies based
on limited information.

%T Learning Plans for an Intelligent Assistant by Observing User Behavior
%A Keith R. Levi
%A Valerie L. Shalin
%A David L. Perschbacher
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 489-503
%* (c) Copyright 1990 Academic Press
%X A critical requirement of intelligent automated assistants is a
representation of actions and goals that is common to both the user and the
automated assistant.  Updating the intelligent system's knowledge base by
observing user behavior is a convenient method for acquiring this common
representation.  We are developing an explanation based learning system to
automate the acquisition of new plans for a large pilot-aiding expert system. 
We have developed a planning/learning shell that is based on the TWEAK planning
system and DeJong's explanation based learning system.  We are applying this
shell to the pilot-aiding problem in a joint industry/university research
effort involving Honeywell, Lockheed, ISX, Search Technology, and the
Universities of Illinois and Michigan.

%T The Meaning Triangle as a Tool for the Acquisition of Abstract,
Conceptual Knowledge
%A Stephen Regoczei
%A Graeme Hirst
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 505-520
%* (c) Copyright 1990 Academic Press
%X The meaning triangle is presented as a useful diagramming tool for
organizing knowledge in the informant-analyst interaction-based, natural
language-mediated knowledge acquisition process.  In concepts-oriented
knowledge acquisition, he knowledge explication phase dominates.  During the
conceptual analysis process, it is helpful to separate verbal, conceptual, and
referent entities.  Diagramming these entities on an agent-centered meaning
triangle clarifies for both informant and analyst the ontological structure
that underlies the discourse and the creation of domains of discourses.

%T Using Personal Construct Techniques for Collaborative Evaluation
%A Douglas Schuler
%A Peter Russo
%A John Boose
%A Jeffrey Bradshaw
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 521-536
%* (c) Copyright 1990 Academic Press
%X Efforts are under way to characterize better group processes at the Boeing
Company as part of a project to design software for computer-supported
collaboration.  This paper describes work in progress to support multi-user,
collaborative situations using Aquinas, a knowledge acquisition workbench.  An
experiment is described in which Aquinas is used to facilitate the
collaborative evaluation of an in-house Boeing Advanced Technology Center
course in knowledge engineering.

%T Functional Design of a Menu-Tree Interface within Structured System
Development
%A Peretz Shoval
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 537-556
%* (c) Copyright 1990 Academic Press
%X A systematic method for designing a menu-tree interface is presented.  The
method is part of ADISSA, a comprehensive systems analysis and design
methodology, which is based on the use of modified hierarchical data flow
diagrams (DFD).  Thus, the functional structure of the designed menu-tree is
consistent with the functional structure and the user-model of the system.  The
method consists of several steps, beginning with an initial menu-tree derived
automatically from DFDs, which is then improved and modified, taking into
consideration factors other than functionality.  Menu Tree Designer, one of the
software tools of ADISSA methodology, supports the designer in all stages of
the interface design.

%T An Intelligent Language Tutoring System
%A Camilla B. Schwind
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 557-579
%* (c) Copyright 1990 Academic Press
%X In this paper, we present the theoretical background and describe the design
and implementation of an intelligent language tutoring system (ILTS).  The most
important properties of our system are: (1) The system is based on a very
complete and "objective" grammar knowledge base; (2) Students can at any moment
during an exercise ask the system questions about the grammar, and they are
immediately answered without losing the exercise context.  Thus the normal
behaviour of a tutor is better simulated, which contributes to a user-friendly
interface; and (3) It allows for individual correction of errors and reaction
to errors.  This is due to the fact that the system is firmly based on a
linguistically well-founded analysis.  The sentences formulated by the students
are parsed and analysed.  They are not simply matched against predefined
answers as is still the case with many other more classically oriented systems.

%T An Evaluation of the Effectiveness and Efficiency of an Automobile
Moving-Map Navigational Display
%A Jonathan F. Antin
%A Thomas A. Dingus
%A Melissa C. Hulse
%A Walter W. Wierwille
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 581-594
%* (c) Copyright 1990 Academic Press
%X This experiment was performed to evaluate the effectiveness and efficiency
of navigating with an automobile moving-map display relative to navigating with
a conventional paper map and along a memorized route, which served as a
baseline for comparison.  Results indicated that there were no differences in
the quality of routes selected when using either the paper map or the moving
map to navigate.  However, the moving map significantly drew the driver's gaze
away from the driving task relative to the norm established in the memorized
route condition, as well as in comparison to the paper map.  These findings are
discussed in the context of the different navigation strategies evoked by use
of the paper and moving-map methods of navigation.

%T Different Notions of Uncertainty in Quasi-Probabilistic Models
%A L. C. van der Gaag
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 5
%P 595-606
%* (c) Copyright 1990 Academic Press
%X In the early years of the research into plausible reasoning several
quasi-probabilistic models for handling uncertainty in rule-based expert
systems have been proposed.  These models were computationally feasible but
could not be justified mathematically.  Although current research in this
sub-area of artificial intelligence concentrates on the development of
mathematically sound models, the early quasi-probabilistic models are still
employed frequently in present-day rule-based expert systems.  In this paper we
show that two of these models, the certainty factor model developed by E. H.
Shortliffe and B. G. Buchanan, and the subjective Bayesian method developed by
R. O. Duda, P. E. Hart and N. J. Nilsson, model different notions of
uncertainty.  We support this statement by pointing out the difference in the
interpretation and application of production rules in the respective models.

%T PRED: A Frame-Based Primitive Editor
%A Shun-En Xie
%A David F. Dumaresq
%A Philip H. Winne
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 6
%P 607-621
%* (c) Copyright 1990 Academic Press
%X PRED allows domain experts to easily create frame-based windowed knowledge
acquisition interfaces for knowledge systems development.  These interfaces
provide visibility for complex domain knowledge and speed up the entry of
knowledge by allowing the domain expert to supply lists of default values. 
Domain experts first use PRED to schematize a knowledge unit, building a
knowledge description.  The knowledge description becomes a map to create a
panel or the data structure to contain each instance of entered knowledge.  A
knowledge description can be customized by locking certain slots giving special
views of the unit.  This is useful for reducing choices in a generalized model
and improves the time for entering data.

%T Towards a Classification of Text Types: A Repertory Grid Approach
%A Andrew Dillon
%A Cliff McKnight
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 6
%P 623-636
%* (c) Copyright 1990 Academic Press
%X The advent of hypertext brings with it associated problems of how best to
present non-linear texts.  As yet, knowledge of readers' models of texts and
their uses is limited.  Repertory grid analysis offers an insightful method of
examining these issues and gaining an understanding of the type of texts that
exist in the readers' worlds.  The present study investigates six researchers'
perceptions of texts in terms of their use, content and structure.  Results
indicate that individuals construe texts in terms of three broad attributes:
why read them, what type of information they contain, and how they are read. 
When applied to a variety of texts these attributes facilitate a classificatory
system incorporating both individual and task differences and provide guidance
on how their electronic versions could be designed.

%T Display-Based Competence: Towards User Models for Menu-Driven Interfaces
%A Andrew Howes
%A Stephen J. Payne
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 6
%P 637-655
%* (c) Copyright 1990 Academic Press
%X This paper discusses the critical role played by aspects of the display in
the use of many computer systems, especially those driven by menus.  We outline
a formal model of "display-based competence" by extending the Task-Action
Grammar notation (Payne & Green, 1986).  The model, D-TAG (for display-oriented
task-action grammar) is illustrated with examples for the well-known Macintosh
desk-top interface, and from a more deeply-nested menu interface to a device
used for the remote testing of telephone lines (RATES).
   D-TAG exploits two extensions of TAG to address important aspects of
interface consistency.  The most important extension uses a featural
description of the display to capture the role of the display in structuring
task-action mappings; the second describes the "side-effects" of a task, i.e.
those effects not described by the semantic attributes of a task.  By embedding
these extensions within the organizing framework of TAG's feature-grammar, we
are able to develop descriptions of interfaces which highlight aspects of
(display) design that are outside the scope of other formal user models.

%T Integration of Linguistic Probabilities
%A David V. Budescu
%A Rami Zwick
%A Thomas S. Wallsten
%A Ido Erev
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 6
%P 657-676
%* (c) Copyright 1990 Academic Press
%X In a previous study, Zwick, Budescu and Wallsten (1988) found that the
membership functions representing the subjective combinations of two
independent linguistic probabilistic judgements could not be predicted by
applying any dual t- and co-t-norm to the functions of the underlying terms. 
Their results showed further that judgements involving the "and" connective
were best modelled as the fuzzy mean of the two separate components.  The
present experiment extended those results by manipulating the instructions
regarding the "and" connective and also including an additional task in which
subjects selected a third phrase to represent the integration of the two
independent judgements.  Again, no t-norm rule predicted subjects' responses,
which were now best modelled by the point-wise arithmetic or geometric means of
the functions.  In addition, most subjects selected phrases and provided
membership functions in response to two identical forecasts that were more
extreme and more precise than the individual forecast, a result inconsistent
with any t-norm or averaging model.  A minority of subjects responded with the
same phrase contained in the forecasts.  The entire pattern of results in the
Zwick et al.(1988) and the present study is used to argue against the
indiscriminate application of mathematically prescribed, but empirically
unsupported operations in computerized expert systems intended to represent and
combine linguistic information.

%T Sorting-Based Menu Categories
%A Douglas Hayhoe
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 6
%P 677-705
%* (c) Copyright 1990 Academic Press
%X Several researchers have conducted sorting experiments or pairwise
comparisons with a database of menu items in order to form coherent menu
categories.  However, these experiments have all contained one or more of the
following potential weaknesses: (1) they used only one particular database; (2)
they used too few sorting subjects; (3) they uncritically used only one scaling
technique to form the clusters; or (4) they did not conduct an experimental
comparison of the categories formed.  In the present research, sorting
experiments were conducted with 48 subjects and four 48-item databases:
clothes, furniture, occupations, and sports.  Latent partition analysis and
hierarchical clustering (Ward's method and group average linkage) were used to
form menu categories.  These were placed into a "pull-down" menu system in two
conditions: (1) titles chosen by each individual subject; and (2) titles chosen
by the investigator.  Two other conditions were added: (3) categories and
titles formed by software design experts; and (4) categories and titles formed
by each subject for his or her own work.  Two within-subjects menu experiments
were performed.  The sorting-based categories with investigator titles were
superior to the expert categories in selection times, selection errors,
"goodness of fit" ratings, and memory recall errors.  A detailed analysis
showed that the expert categories contained more "miscategorization" errors and
vague category titles than the sorting-based categories, while both conditions
contained overlapping categories.

%T Novices' Debugging when Programming in Pascal
%A Carl Martin Allwood
%A Carl-Gustav Bjorhag
%J International Journal of Man-Machine Studies
%D 1990
%V 33
%N 6
%P 707-724
%* (c) Copyright 1990 Academic Press
%X In this study an analysis was made of novices debugging their own Pascal
programs.  Eight novices verbalized their thoughts aloud while attempting to
solve a programming task.  Novices' debugging is seen as taking place in
negative evaluation episodes (henceforth: evaluation episodes).  During the
three hour programming session, the novices spent 51% of the time in evaluation
episodes.  This percentage would presumably have been higher if the subjects
had been given more time for the session.  Evaluation episodes were found to be
triggered in four different contexts: Reaction to an error message (67% of the
total time spent in any evaluation episode), Reaction to the resulting value of
a test run (23%), Hint from the experimenter (4%) and Other (6%).  When related
to results presented by Gray and Anderson (1987), our results indicate that
novices perform the substantial part of their debugging after they have
compiled the program, or part of it, for the first time.  Despite the
information given in the computer's error messages, the percentage of errors
eliminated in episodes triggered by such message was not higher than could be
expected from the time spent in these episodes.  Our results indicate that the
importance of activity oriented towards understanding one's program during
debugging depends on: (1) whether the error elicits an error message from the
computer or not; and (2) the general programming strategy used by the subject.

