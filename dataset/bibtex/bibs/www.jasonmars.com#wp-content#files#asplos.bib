@article{Brewer:2010:TDR:1735971.1736021,
 author = {Brewer, Eric A.},
 title = {Technology for developing regions: Moore's law is not enough},
 abstract = {The historic focus of development has rightfully been on macroeconomics and good governance, but technology has an increasingly large role to play. In this talk, I review several novel technologies that we have deployed in India and Africa, and discuss the challenges and opportunities of this new subfield of EECS research. Working with the Aravind Eye Hospital, we are currently supporting doctor / patient videoconferencing in 30+ rural villages; more than 25,000 people have had their blindness cured due to these exams. Although Moore's Law has led to great cost reductions and thus enabled new technologies, we have reached essentially the low point for cost: the computing is essentially free compared to the rest of the system. The premium is thus on a combination of 1) deeper integration (fewer compo-nents), 2) shared usage models (even phones are shared), and 3) lower operating costs in terms of power and connectivity.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1735971.1736021},
 doi = {http://doi.acm.org/10.1145/1735971.1736021},
 acmid = {1736021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {developing regions, ictd, it for development.},
} 

@inproceedings{Brewer:2010:TDR:1736020.1736021,
 author = {Brewer, Eric A.},
 title = {Technology for developing regions: Moore's law is not enough},
 abstract = {The historic focus of development has rightfully been on macroeconomics and good governance, but technology has an increasingly large role to play. In this talk, I review several novel technologies that we have deployed in India and Africa, and discuss the challenges and opportunities of this new subfield of EECS research. Working with the Aravind Eye Hospital, we are currently supporting doctor / patient videoconferencing in 30+ rural villages; more than 25,000 people have had their blindness cured due to these exams. Although Moore's Law has led to great cost reductions and thus enabled new technologies, we have reached essentially the low point for cost: the computing is essentially free compared to the rest of the system. The premium is thus on a combination of 1) deeper integration (fewer compo-nents), 2) shared usage models (even phones are shared), and 3) lower operating costs in terms of power and connectivity.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1736020.1736021},
 doi = {http://doi.acm.org/10.1145/1736020.1736021},
 acmid = {1736021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {developing regions, ictd, it for development.},
} 

@article{Brewer:2010:TDR:1735970.1736021,
 author = {Brewer, Eric A.},
 title = {Technology for developing regions: Moore's law is not enough},
 abstract = {The historic focus of development has rightfully been on macroeconomics and good governance, but technology has an increasingly large role to play. In this talk, I review several novel technologies that we have deployed in India and Africa, and discuss the challenges and opportunities of this new subfield of EECS research. Working with the Aravind Eye Hospital, we are currently supporting doctor / patient videoconferencing in 30+ rural villages; more than 25,000 people have had their blindness cured due to these exams. Although Moore's Law has led to great cost reductions and thus enabled new technologies, we have reached essentially the low point for cost: the computing is essentially free compared to the rest of the system. The premium is thus on a combination of 1) deeper integration (fewer compo-nents), 2) shared usage models (even phones are shared), and 3) lower operating costs in terms of power and connectivity.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1735970.1736021},
 doi = {http://doi.acm.org/10.1145/1735970.1736021},
 acmid = {1736021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {developing regions, ictd, it for development.},
} 

@article{Ipek:2010:DRM:1735970.1736023,
 author = {Ipek, Engin and Condit, Jeremy and Nightingale, Edmund B. and Burger, Doug and Moscibroda, Thomas},
 title = {Dynamically replicated memory: building reliable systems from nanoscale resistive memories},
 abstract = {DRAM is facing severe scalability challenges in sub-45nm tech- nology nodes due to precise charge placement and sensing hur- dles in deep-submicron geometries. Resistive memories, such as phase-change memory (PCM), already scale well beyond DRAM and are a promising DRAM replacement. Unfortunately, PCM is write-limited, and current approaches to managing writes must de- commission pages of PCM when the first bit fails. This paper presents dynamically replicated memory</i> (DRM), the first hardware and operating system interface designed for PCM that allows continued operation through graceful degradation</i> when hard faults occur. DRM reuses memory pages that con- tain hard faults by dynamically forming pairs of complementary pages that act as a single page of storage. No changes are required to the processor cores, the cache hierarchy, or the operating sys- tem's page tables. By changing the memory controller, the TLBs, and the operating system to be DRM-aware, we can improve the lifetime of PCM by up to 40x over conventional error-detection techniques.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736023},
 doi = {http://doi.acm.org/10.1145/1735970.1736023},
 acmid = {1736023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {phase-change memory, write endurance},
} 

@article{Ipek:2010:DRM:1735971.1736023,
 author = {Ipek, Engin and Condit, Jeremy and Nightingale, Edmund B. and Burger, Doug and Moscibroda, Thomas},
 title = {Dynamically replicated memory: building reliable systems from nanoscale resistive memories},
 abstract = {DRAM is facing severe scalability challenges in sub-45nm tech- nology nodes due to precise charge placement and sensing hur- dles in deep-submicron geometries. Resistive memories, such as phase-change memory (PCM), already scale well beyond DRAM and are a promising DRAM replacement. Unfortunately, PCM is write-limited, and current approaches to managing writes must de- commission pages of PCM when the first bit fails. This paper presents dynamically replicated memory</i> (DRM), the first hardware and operating system interface designed for PCM that allows continued operation through graceful degradation</i> when hard faults occur. DRM reuses memory pages that con- tain hard faults by dynamically forming pairs of complementary pages that act as a single page of storage. No changes are required to the processor cores, the cache hierarchy, or the operating sys- tem's page tables. By changing the memory controller, the TLBs, and the operating system to be DRM-aware, we can improve the lifetime of PCM by up to 40x over conventional error-detection techniques.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736023},
 doi = {http://doi.acm.org/10.1145/1735971.1736023},
 acmid = {1736023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {phase-change memory, write endurance},
} 

@inproceedings{Ipek:2010:DRM:1736020.1736023,
 author = {Ipek, Engin and Condit, Jeremy and Nightingale, Edmund B. and Burger, Doug and Moscibroda, Thomas},
 title = {Dynamically replicated memory: building reliable systems from nanoscale resistive memories},
 abstract = {DRAM is facing severe scalability challenges in sub-45nm tech- nology nodes due to precise charge placement and sensing hur- dles in deep-submicron geometries. Resistive memories, such as phase-change memory (PCM), already scale well beyond DRAM and are a promising DRAM replacement. Unfortunately, PCM is write-limited, and current approaches to managing writes must de- commission pages of PCM when the first bit fails. This paper presents dynamically replicated memory</i> (DRM), the first hardware and operating system interface designed for PCM that allows continued operation through graceful degradation</i> when hard faults occur. DRM reuses memory pages that con- tain hard faults by dynamically forming pairs of complementary pages that act as a single page of storage. No changes are required to the processor cores, the cache hierarchy, or the operating sys- tem's page tables. By changing the memory controller, the TLBs, and the operating system to be DRM-aware, we can improve the lifetime of PCM by up to 40x over conventional error-detection techniques.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736023},
 doi = {http://doi.acm.org/10.1145/1736020.1736023},
 acmid = {1736023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {phase-change memory, write endurance},
} 

@article{Kirman:2010:PAO:1735970.1736024,
 author = {Kirman, Nevin and Mart\'{\i}nez, Jos\'{e} F.},
 title = {A power-efficient all-optical on-chip interconnect using wavelength-based oblivious routing},
 abstract = {We present an all-optical approach to constructing data networks on chip that combines the following key features: (1) Wavelength-based routing, where the route followed by a packet depends solely on the wavelength of its carrier signal, and not on information either contained in the packet or traveling along with it. (2) Oblivious routing, by which the wavelength (and thus the route) employed to connect a source-destination pair is invariant for that pair, and does not depend on ongoing transmissions by other nodes, thereby simplifying design and operation. And (3) passive optical wavelength routers, whose routing pattern is set at design time, which allows for area and power optimizations not generally available to solutions that use dynamic routing. Compared to prior proposals, our evaluation shows that our solution is significantly more power efficient at a similar level of performance.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {15--28},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736024},
 doi = {http://doi.acm.org/10.1145/1735970.1736024},
 acmid = {1736024},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {nanophotonics, on-chip network, optical network, wavelength-based oblivious routing},
} 

@inproceedings{Kirman:2010:PAO:1736020.1736024,
 author = {Kirman, Nevin and Mart\'{\i}nez, Jos\'{e} F.},
 title = {A power-efficient all-optical on-chip interconnect using wavelength-based oblivious routing},
 abstract = {We present an all-optical approach to constructing data networks on chip that combines the following key features: (1) Wavelength-based routing, where the route followed by a packet depends solely on the wavelength of its carrier signal, and not on information either contained in the packet or traveling along with it. (2) Oblivious routing, by which the wavelength (and thus the route) employed to connect a source-destination pair is invariant for that pair, and does not depend on ongoing transmissions by other nodes, thereby simplifying design and operation. And (3) passive optical wavelength routers, whose routing pattern is set at design time, which allows for area and power optimizations not generally available to solutions that use dynamic routing. Compared to prior proposals, our evaluation shows that our solution is significantly more power efficient at a similar level of performance.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {15--28},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736024},
 doi = {http://doi.acm.org/10.1145/1736020.1736024},
 acmid = {1736024},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {nanophotonics, on-chip network, optical network, wavelength-based oblivious routing},
} 

@article{Kirman:2010:PAO:1735971.1736024,
 author = {Kirman, Nevin and Mart\'{\i}nez, Jos\'{e} F.},
 title = {A power-efficient all-optical on-chip interconnect using wavelength-based oblivious routing},
 abstract = {We present an all-optical approach to constructing data networks on chip that combines the following key features: (1) Wavelength-based routing, where the route followed by a packet depends solely on the wavelength of its carrier signal, and not on information either contained in the packet or traveling along with it. (2) Oblivious routing, by which the wavelength (and thus the route) employed to connect a source-destination pair is invariant for that pair, and does not depend on ongoing transmissions by other nodes, thereby simplifying design and operation. And (3) passive optical wavelength routers, whose routing pattern is set at design time, which allows for area and power optimizations not generally available to solutions that use dynamic routing. Compared to prior proposals, our evaluation shows that our solution is significantly more power efficient at a similar level of performance.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {15--28},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736024},
 doi = {http://doi.acm.org/10.1145/1735971.1736024},
 acmid = {1736024},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {nanophotonics, on-chip network, optical network, wavelength-based oblivious routing},
} 

@article{Neelakantam:2010:RSE:1735970.1736026,
 author = {Neelakantam, Naveen and Ditzel, David R. and Zilles, Craig},
 title = {A real system evaluation of hardware atomicity for software speculation},
 abstract = {In this paper we evaluate the atomic region compiler abstraction by incorporating it into a commercial system. We find that atomic regions are simple and intuitive to integrate into an x86 binary-translation system. Furthermore, doing so trivially enables additional optimization opportunities beyond that achievable by a high-performance dynamic optimizer, which already implements superblocks. We show that atomic regions can suffer from severe performance penalties if misspeculations are left uncontrolled, but that a simple software control mechanism is sufficient to reign in all detrimental side-effects. We evaluate using full reference runs of the SPEC CPU2000 integer benchmarks and find that atomic regions enable up to a 9\% (3\% on average) improvement beyond the performance of a tuned product. These performance improvements are achieved without any negative side effects. Performance side effects such as code bloat are absent with atomic regions; in fact, static code size is reduced. The hardware necessary is synergistic with other needs and was already available on the commercial product used in our evaluation. Finally, the software complexity is minimal as a single developer was able to incorporate atomic regions into a sophisticated 300,000 line code base in three months, despite never having seen the translator source code beforehand.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {29--38},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1735970.1736026},
 doi = {http://doi.acm.org/10.1145/1735970.1736026},
 acmid = {1736026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, checkpoint, dynamic translation, optimization, speculation},
} 

@article{Neelakantam:2010:RSE:1735971.1736026,
 author = {Neelakantam, Naveen and Ditzel, David R. and Zilles, Craig},
 title = {A real system evaluation of hardware atomicity for software speculation},
 abstract = {In this paper we evaluate the atomic region compiler abstraction by incorporating it into a commercial system. We find that atomic regions are simple and intuitive to integrate into an x86 binary-translation system. Furthermore, doing so trivially enables additional optimization opportunities beyond that achievable by a high-performance dynamic optimizer, which already implements superblocks. We show that atomic regions can suffer from severe performance penalties if misspeculations are left uncontrolled, but that a simple software control mechanism is sufficient to reign in all detrimental side-effects. We evaluate using full reference runs of the SPEC CPU2000 integer benchmarks and find that atomic regions enable up to a 9\% (3\% on average) improvement beyond the performance of a tuned product. These performance improvements are achieved without any negative side effects. Performance side effects such as code bloat are absent with atomic regions; in fact, static code size is reduced. The hardware necessary is synergistic with other needs and was already available on the commercial product used in our evaluation. Finally, the software complexity is minimal as a single developer was able to incorporate atomic regions into a sophisticated 300,000 line code base in three months, despite never having seen the translator source code beforehand.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {29--38},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1735971.1736026},
 doi = {http://doi.acm.org/10.1145/1735971.1736026},
 acmid = {1736026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, checkpoint, dynamic translation, optimization, speculation},
} 

@inproceedings{Neelakantam:2010:RSE:1736020.1736026,
 author = {Neelakantam, Naveen and Ditzel, David R. and Zilles, Craig},
 title = {A real system evaluation of hardware atomicity for software speculation},
 abstract = {In this paper we evaluate the atomic region compiler abstraction by incorporating it into a commercial system. We find that atomic regions are simple and intuitive to integrate into an x86 binary-translation system. Furthermore, doing so trivially enables additional optimization opportunities beyond that achievable by a high-performance dynamic optimizer, which already implements superblocks. We show that atomic regions can suffer from severe performance penalties if misspeculations are left uncontrolled, but that a simple software control mechanism is sufficient to reign in all detrimental side-effects. We evaluate using full reference runs of the SPEC CPU2000 integer benchmarks and find that atomic regions enable up to a 9\% (3\% on average) improvement beyond the performance of a tuned product. These performance improvements are achieved without any negative side effects. Performance side effects such as code bloat are absent with atomic regions; in fact, static code size is reduced. The hardware necessary is synergistic with other needs and was already available on the commercial product used in our evaluation. Finally, the software complexity is minimal as a single developer was able to incorporate atomic regions into a sophisticated 300,000 line code base in three months, despite never having seen the translator source code beforehand.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {29--38},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1736020.1736026},
 doi = {http://doi.acm.org/10.1145/1736020.1736026},
 acmid = {1736026},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, checkpoint, dynamic translation, optimization, speculation},
} 

@article{Harris:2010:DFM:1735970.1736027,
 author = {Harris, Tim and Tomic, Sa\v{s}a and Cristal, Adri\'{a}n and Unsal, Osman},
 title = {Dynamic filtering: multi-purpose architecture support for language runtime systems},
 abstract = {This paper introduces a new abstraction to accelerate the read-barriers and write-barriers used by language runtime systems. We exploit the fact that, dynamically, many barrier executions perform checks but no real work -- e.g., in generational garbage collection (GC), frequent checks are needed to detect the creation of inter-generational references, even though such references occur rarely in many workloads. We introduce a form of dynamic filtering that identifies redundant checks by (i) recording checks that have recently been executed, and (ii) detecting when a barrier is repeating one of these checks. We show how this technique can be applied to a variety of algorithms for GC, transactional memory, and language-based security. By supporting dynamic filtering in the instruction set, we show that the fast-paths of these barriers can be streamlined, reducing the impact on the quality of surrounding code. We show how we accelerate the barriers used for generational GC and transactional memory in the Bartok research compiler. With a 2048-entry filter, dynamic filtering eliminates almost all the overhead of the GC write-barriers. Dynamic filtering eliminates around half the overhead of STM over a non-synchronized baseline -- even when used with an STM that is already designed for low overhead, and which employs static analyses to avoid redundant operations.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {39--52},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736027},
 doi = {http://doi.acm.org/10.1145/1735970.1736027},
 acmid = {1736027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, runtime systems, transactional memory},
} 

@inproceedings{Harris:2010:DFM:1736020.1736027,
 author = {Harris, Tim and Tomic, Sa\v{s}a and Cristal, Adri\'{a}n and Unsal, Osman},
 title = {Dynamic filtering: multi-purpose architecture support for language runtime systems},
 abstract = {This paper introduces a new abstraction to accelerate the read-barriers and write-barriers used by language runtime systems. We exploit the fact that, dynamically, many barrier executions perform checks but no real work -- e.g., in generational garbage collection (GC), frequent checks are needed to detect the creation of inter-generational references, even though such references occur rarely in many workloads. We introduce a form of dynamic filtering that identifies redundant checks by (i) recording checks that have recently been executed, and (ii) detecting when a barrier is repeating one of these checks. We show how this technique can be applied to a variety of algorithms for GC, transactional memory, and language-based security. By supporting dynamic filtering in the instruction set, we show that the fast-paths of these barriers can be streamlined, reducing the impact on the quality of surrounding code. We show how we accelerate the barriers used for generational GC and transactional memory in the Bartok research compiler. With a 2048-entry filter, dynamic filtering eliminates almost all the overhead of the GC write-barriers. Dynamic filtering eliminates around half the overhead of STM over a non-synchronized baseline -- even when used with an STM that is already designed for low overhead, and which employs static analyses to avoid redundant operations.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {39--52},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736027},
 doi = {http://doi.acm.org/10.1145/1736020.1736027},
 acmid = {1736027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, runtime systems, transactional memory},
} 

@article{Harris:2010:DFM:1735971.1736027,
 author = {Harris, Tim and Tomic, Sa\v{s}a and Cristal, Adri\'{a}n and Unsal, Osman},
 title = {Dynamic filtering: multi-purpose architecture support for language runtime systems},
 abstract = {This paper introduces a new abstraction to accelerate the read-barriers and write-barriers used by language runtime systems. We exploit the fact that, dynamically, many barrier executions perform checks but no real work -- e.g., in generational garbage collection (GC), frequent checks are needed to detect the creation of inter-generational references, even though such references occur rarely in many workloads. We introduce a form of dynamic filtering that identifies redundant checks by (i) recording checks that have recently been executed, and (ii) detecting when a barrier is repeating one of these checks. We show how this technique can be applied to a variety of algorithms for GC, transactional memory, and language-based security. By supporting dynamic filtering in the instruction set, we show that the fast-paths of these barriers can be streamlined, reducing the impact on the quality of surrounding code. We show how we accelerate the barriers used for generational GC and transactional memory in the Bartok research compiler. With a 2048-entry filter, dynamic filtering eliminates almost all the overhead of the GC write-barriers. Dynamic filtering eliminates around half the overhead of STM over a non-synchronized baseline -- even when used with an STM that is already designed for low overhead, and which employs static analyses to avoid redundant operations.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {39--52},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736027},
 doi = {http://doi.acm.org/10.1145/1735971.1736027},
 acmid = {1736027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, runtime systems, transactional memory},
} 

@article{Bergan:2010:CCR:1735970.1736029,
 author = {Bergan, Tom and Anderson, Owen and Devietti, Joseph and Ceze, Luis and Grossman, Dan},
 title = {CoreDet: a compiler and runtime system for deterministic multithreaded execution},
 abstract = {The behavior of a multithreaded program does not depend only on its inputs. Scheduling, memory reordering, timing, and low-level hardware effects all introduce nondeterminism in the execution of multithreaded programs. This severely complicates many tasks, including debugging, testing, and automatic replication. In this work, we avoid these complications by eliminating their root cause: we develop a compiler and runtime system that runs arbitrary multithreaded C/C++ POSIX Threads programs deterministically. A trivial non-performant approach to providing determinism is simply deterministically serializing execution. Instead, we present a compiler and runtime infrastructure that ensures determinism but resorts to serialization rarely, for handling interthread communication and synchronization. We develop two basic approaches, both of which are largely dynamic with performance improved by some static compiler optimizations. First, an ownership-based approach detects interthread communication via an evolving table that tracks ownership of memory regions by threads. Second, a buffering approach uses versioned memory and employs a deterministic commit protocol to make changes visible to other threads. While buffering has larger single-threaded overhead than ownership, it tends to scale better (serializing less often). A hybrid system sometimes performs and scales better than either approach individually. Our implementation is based on the LLVM compiler infrastructure. It needs neither programmer annotations nor special hardware. Our empirical evaluation uses the PARSEC and SPLASH2 benchmarks and shows that our approach scales comparably to nondeterministic execution.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {53--64},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736029},
 doi = {http://doi.acm.org/10.1145/1735970.1736029},
 acmid = {1736029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilers, determinism, multicore, multithreading},
} 

@inproceedings{Bergan:2010:CCR:1736020.1736029,
 author = {Bergan, Tom and Anderson, Owen and Devietti, Joseph and Ceze, Luis and Grossman, Dan},
 title = {CoreDet: a compiler and runtime system for deterministic multithreaded execution},
 abstract = {The behavior of a multithreaded program does not depend only on its inputs. Scheduling, memory reordering, timing, and low-level hardware effects all introduce nondeterminism in the execution of multithreaded programs. This severely complicates many tasks, including debugging, testing, and automatic replication. In this work, we avoid these complications by eliminating their root cause: we develop a compiler and runtime system that runs arbitrary multithreaded C/C++ POSIX Threads programs deterministically. A trivial non-performant approach to providing determinism is simply deterministically serializing execution. Instead, we present a compiler and runtime infrastructure that ensures determinism but resorts to serialization rarely, for handling interthread communication and synchronization. We develop two basic approaches, both of which are largely dynamic with performance improved by some static compiler optimizations. First, an ownership-based approach detects interthread communication via an evolving table that tracks ownership of memory regions by threads. Second, a buffering approach uses versioned memory and employs a deterministic commit protocol to make changes visible to other threads. While buffering has larger single-threaded overhead than ownership, it tends to scale better (serializing less often). A hybrid system sometimes performs and scales better than either approach individually. Our implementation is based on the LLVM compiler infrastructure. It needs neither programmer annotations nor special hardware. Our empirical evaluation uses the PARSEC and SPLASH2 benchmarks and shows that our approach scales comparably to nondeterministic execution.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {53--64},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736029},
 doi = {http://doi.acm.org/10.1145/1736020.1736029},
 acmid = {1736029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilers, determinism, multicore, multithreading},
} 

@article{Bergan:2010:CCR:1735971.1736029,
 author = {Bergan, Tom and Anderson, Owen and Devietti, Joseph and Ceze, Luis and Grossman, Dan},
 title = {CoreDet: a compiler and runtime system for deterministic multithreaded execution},
 abstract = {The behavior of a multithreaded program does not depend only on its inputs. Scheduling, memory reordering, timing, and low-level hardware effects all introduce nondeterminism in the execution of multithreaded programs. This severely complicates many tasks, including debugging, testing, and automatic replication. In this work, we avoid these complications by eliminating their root cause: we develop a compiler and runtime system that runs arbitrary multithreaded C/C++ POSIX Threads programs deterministically. A trivial non-performant approach to providing determinism is simply deterministically serializing execution. Instead, we present a compiler and runtime infrastructure that ensures determinism but resorts to serialization rarely, for handling interthread communication and synchronization. We develop two basic approaches, both of which are largely dynamic with performance improved by some static compiler optimizations. First, an ownership-based approach detects interthread communication via an evolving table that tracks ownership of memory regions by threads. Second, a buffering approach uses versioned memory and employs a deterministic commit protocol to make changes visible to other threads. While buffering has larger single-threaded overhead than ownership, it tends to scale better (serializing less often). A hybrid system sometimes performs and scales better than either approach individually. Our implementation is based on the LLVM compiler infrastructure. It needs neither programmer annotations nor special hardware. Our empirical evaluation uses the PARSEC and SPLASH2 benchmarks and shows that our approach scales comparably to nondeterministic execution.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {53--64},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736029},
 doi = {http://doi.acm.org/10.1145/1735971.1736029},
 acmid = {1736029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilers, determinism, multicore, multithreading},
} 

@article{Raman:2010:SPU:1735971.1736030,
 author = {Raman, Arun and Kim, Hanjun and Mason, Thomas R. and Jablin, Thomas B. and August, David I.},
 title = {Speculative parallelization using software multi-threaded transactions},
 abstract = {With the right techniques, multicore architectures may be able to continue the exponential performance trend that elevated the performance of applications of all types for decades. While many scientific programs can be parallelized without speculative techniques, speculative parallelism appears to be the key to continuing this trend for general-purpose applications. Recently-proposed code parallelization techniques, such as those by Bridges et al. and by Thies et al., demonstrate scalable performance on multiple cores by using speculation to divide code into atomic units (transactions) that span multiple threads in order to expose data parallelism. Unfortunately, most software and hardware Thread-Level Speculation (TLS) memory systems and transactional memories are not sufficient because they only support single-threaded atomic units. Multi-threaded Transactions (MTXs) address this problem, but they require expensive hardware support as currently proposed in the literature. This paper proposes a Software MTX (SMTX) system that captures the applicability</i> and performance</i> of hardware MTX, but on existing multicore machines</i>. The SMTX system yields a harmonic mean speedup of 13.36x on native hardware with four 6-core processors (24 cores in total) running speculatively parallelized applications.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {65--76},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736030},
 doi = {http://doi.acm.org/10.1145/1735971.1736030},
 acmid = {1736030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, loop-level parallelism, multi-threaded transactions, pipelined parallelism, software transactional memory, thread-level speculation},
} 

@article{Raman:2010:SPU:1735970.1736030,
 author = {Raman, Arun and Kim, Hanjun and Mason, Thomas R. and Jablin, Thomas B. and August, David I.},
 title = {Speculative parallelization using software multi-threaded transactions},
 abstract = {With the right techniques, multicore architectures may be able to continue the exponential performance trend that elevated the performance of applications of all types for decades. While many scientific programs can be parallelized without speculative techniques, speculative parallelism appears to be the key to continuing this trend for general-purpose applications. Recently-proposed code parallelization techniques, such as those by Bridges et al. and by Thies et al., demonstrate scalable performance on multiple cores by using speculation to divide code into atomic units (transactions) that span multiple threads in order to expose data parallelism. Unfortunately, most software and hardware Thread-Level Speculation (TLS) memory systems and transactional memories are not sufficient because they only support single-threaded atomic units. Multi-threaded Transactions (MTXs) address this problem, but they require expensive hardware support as currently proposed in the literature. This paper proposes a Software MTX (SMTX) system that captures the applicability</i> and performance</i> of hardware MTX, but on existing multicore machines</i>. The SMTX system yields a harmonic mean speedup of 13.36x on native hardware with four 6-core processors (24 cores in total) running speculatively parallelized applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {65--76},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736030},
 doi = {http://doi.acm.org/10.1145/1735970.1736030},
 acmid = {1736030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, loop-level parallelism, multi-threaded transactions, pipelined parallelism, software transactional memory, thread-level speculation},
} 

@inproceedings{Raman:2010:SPU:1736020.1736030,
 author = {Raman, Arun and Kim, Hanjun and Mason, Thomas R. and Jablin, Thomas B. and August, David I.},
 title = {Speculative parallelization using software multi-threaded transactions},
 abstract = {With the right techniques, multicore architectures may be able to continue the exponential performance trend that elevated the performance of applications of all types for decades. While many scientific programs can be parallelized without speculative techniques, speculative parallelism appears to be the key to continuing this trend for general-purpose applications. Recently-proposed code parallelization techniques, such as those by Bridges et al. and by Thies et al., demonstrate scalable performance on multiple cores by using speculation to divide code into atomic units (transactions) that span multiple threads in order to expose data parallelism. Unfortunately, most software and hardware Thread-Level Speculation (TLS) memory systems and transactional memories are not sufficient because they only support single-threaded atomic units. Multi-threaded Transactions (MTXs) address this problem, but they require expensive hardware support as currently proposed in the literature. This paper proposes a Software MTX (SMTX) system that captures the applicability</i> and performance</i> of hardware MTX, but on existing multicore machines</i>. The SMTX system yields a harmonic mean speedup of 13.36x on native hardware with four 6-core processors (24 cores in total) running speculatively parallelized applications.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {65--76},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736030},
 doi = {http://doi.acm.org/10.1145/1736020.1736030},
 acmid = {1736030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, loop-level parallelism, multi-threaded transactions, pipelined parallelism, software transactional memory, thread-level speculation},
} 

@inproceedings{Lee:2010:REO:1736020.1736031,
 author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
 title = {Respec: efficient online multiprocessor replayvia speculation and external determinism},
 abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently. Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races. We modified the Linux kernel to implement our techniques. Our software system adds on average about 18\% overhead to the execution time for recording and replaying programs with two threads and 55\% overhead for programs with four threads.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736031},
 doi = {http://doi.acm.org/10.1145/1736020.1736031},
 acmid = {1736031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {external determinism, replay, speculative execution},
} 

@article{Lee:2010:REO:1735971.1736031,
 author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
 title = {Respec: efficient online multiprocessor replayvia speculation and external determinism},
 abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently. Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races. We modified the Linux kernel to implement our techniques. Our software system adds on average about 18\% overhead to the execution time for recording and replaying programs with two threads and 55\% overhead for programs with four threads.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736031},
 doi = {http://doi.acm.org/10.1145/1735971.1736031},
 acmid = {1736031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {external determinism, replay, speculative execution},
} 

@article{Lee:2010:REO:1735970.1736031,
 author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
 title = {Respec: efficient online multiprocessor replayvia speculation and external determinism},
 abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently. Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races. We modified the Linux kernel to implement our techniques. Our software system adds on average about 18\% overhead to the execution time for recording and replaying programs with two threads and 55\% overhead for programs with four threads.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {77--90},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736031},
 doi = {http://doi.acm.org/10.1145/1735970.1736031},
 acmid = {1736031},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {external determinism, replay, speculative execution},
} 

@article{Eyerman:2010:PJS:1735971.1736033,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {Probabilistic job symbiosis modeling for SMT processor scheduling},
 abstract = {Symbiotic job scheduling boosts simultaneous multithreading (SMT) processor performance by co-scheduling jobs that have `compatible' demands on the processor's shared resources. Existing approaches however require a sampling phase, evaluate a limited number of possible co-schedules, use heuristics to gauge symbiosis, are rigid in their optimization target, and do not preserve system-level priorities/shares. This paper proposes probabilistic job symbiosis modeling, which predicts whether jobs will create positive or negative symbiosis when co-scheduled without requiring the co-schedule to be evaluated. The model, which uses per-thread cycle stacks computed through a previously proposed cycle accounting architecture, is simple enough to be used in system software. Probabilistic job symbiosis modeling provides six key innovations over prior work in symbiotic job scheduling: (i) it does not require a sampling phase, (ii) it readjusts the job co-schedule continuously, (iii) it evaluates a large number of possible co-schedules at very low overhead, (iv) it is not driven by heuristics, (v) it can optimize a performance target of interest (e.g., system throughput or job turnaround time), and (vi) it preserves system-level priorities/shares. These innovations make symbiotic job scheduling both practical and effective. Our experimental evaluation, which assumes a realistic scenario in which jobs come and go, reports an average 16\% (and up to 35\%) reduction in job turnaround time compared to the previously proposed SOS (sample, optimize, symbios) approach for a two-thread SMT processor, and an average 19\% (and up to 45\%) reduction in job turnaround time for a four-thread SMT processor.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736033},
 doi = {http://doi.acm.org/10.1145/1735971.1736033},
 acmid = {1736033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance modeling, simultaneous multi-threading (smt), symbiotic job scheduling},
} 

@article{Eyerman:2010:PJS:1735970.1736033,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {Probabilistic job symbiosis modeling for SMT processor scheduling},
 abstract = {Symbiotic job scheduling boosts simultaneous multithreading (SMT) processor performance by co-scheduling jobs that have `compatible' demands on the processor's shared resources. Existing approaches however require a sampling phase, evaluate a limited number of possible co-schedules, use heuristics to gauge symbiosis, are rigid in their optimization target, and do not preserve system-level priorities/shares. This paper proposes probabilistic job symbiosis modeling, which predicts whether jobs will create positive or negative symbiosis when co-scheduled without requiring the co-schedule to be evaluated. The model, which uses per-thread cycle stacks computed through a previously proposed cycle accounting architecture, is simple enough to be used in system software. Probabilistic job symbiosis modeling provides six key innovations over prior work in symbiotic job scheduling: (i) it does not require a sampling phase, (ii) it readjusts the job co-schedule continuously, (iii) it evaluates a large number of possible co-schedules at very low overhead, (iv) it is not driven by heuristics, (v) it can optimize a performance target of interest (e.g., system throughput or job turnaround time), and (vi) it preserves system-level priorities/shares. These innovations make symbiotic job scheduling both practical and effective. Our experimental evaluation, which assumes a realistic scenario in which jobs come and go, reports an average 16\% (and up to 35\%) reduction in job turnaround time compared to the previously proposed SOS (sample, optimize, symbios) approach for a two-thread SMT processor, and an average 19\% (and up to 45\%) reduction in job turnaround time for a four-thread SMT processor.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736033},
 doi = {http://doi.acm.org/10.1145/1735970.1736033},
 acmid = {1736033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance modeling, simultaneous multi-threading (smt), symbiotic job scheduling},
} 

@inproceedings{Eyerman:2010:PJS:1736020.1736033,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {Probabilistic job symbiosis modeling for SMT processor scheduling},
 abstract = {Symbiotic job scheduling boosts simultaneous multithreading (SMT) processor performance by co-scheduling jobs that have `compatible' demands on the processor's shared resources. Existing approaches however require a sampling phase, evaluate a limited number of possible co-schedules, use heuristics to gauge symbiosis, are rigid in their optimization target, and do not preserve system-level priorities/shares. This paper proposes probabilistic job symbiosis modeling, which predicts whether jobs will create positive or negative symbiosis when co-scheduled without requiring the co-schedule to be evaluated. The model, which uses per-thread cycle stacks computed through a previously proposed cycle accounting architecture, is simple enough to be used in system software. Probabilistic job symbiosis modeling provides six key innovations over prior work in symbiotic job scheduling: (i) it does not require a sampling phase, (ii) it readjusts the job co-schedule continuously, (iii) it evaluates a large number of possible co-schedules at very low overhead, (iv) it is not driven by heuristics, (v) it can optimize a performance target of interest (e.g., system throughput or job turnaround time), and (vi) it preserves system-level priorities/shares. These innovations make symbiotic job scheduling both practical and effective. Our experimental evaluation, which assumes a realistic scenario in which jobs come and go, reports an average 16\% (and up to 35\%) reduction in job turnaround time compared to the previously proposed SOS (sample, optimize, symbios) approach for a two-thread SMT processor, and an average 19\% (and up to 45\%) reduction in job turnaround time for a four-thread SMT processor.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736033},
 doi = {http://doi.acm.org/10.1145/1736020.1736033},
 acmid = {1736033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance modeling, simultaneous multi-threading (smt), symbiotic job scheduling},
} 

@article{Shen:2010:RBV:1735971.1736034,
 author = {Shen, Kai},
 title = {Request behavior variations},
 abstract = {A large number of user requests execute (often concurrently) within a server system. A single request may exhibit fluctuating hardware characteristics (such as instruction completion rate and on-chip resource usage) over the course of its execution, due to inherent variations in application execution semantics as well as dynamic resource competition on resource-sharing processors like multicores. Understanding such behavior variations can assist fine-grained request modeling and adaptive resource management. This paper presents operating system management to track request behavior variations online. In addition to metric sample collection during periodic interrupts, we exploit the frequent system calls in server applications to perform low-cost in-kernel sampling. We utilize identified behavior variations to support or enhance request modeling in request classification, anomaly analysis, and online request signature construction. A foundation of our request modeling is the ability to quantify the difference between two requests' time series behaviors. We evaluate several differencing measures and enhance the classic dynamic time warping technique with additional penalties for asynchronous warp steps. Finally, motivated by fluctuating request resource usage and the resulting contention, we implement contention-easing CPU scheduling on multicore platforms and demonstrate its effectiveness in improving the worst-case request performance. Experiments in this paper are based on five server applications -- Apache web server, TPCC, TPCH, RUBiS online auction benchmark, and a user-content-driven online teaching application called WeBWorK.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {103--116},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736034},
 doi = {http://doi.acm.org/10.1145/1735971.1736034},
 acmid = {1736034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware counter, multicore, operating system adaptation, request modeling, server system},
} 

@inproceedings{Shen:2010:RBV:1736020.1736034,
 author = {Shen, Kai},
 title = {Request behavior variations},
 abstract = {A large number of user requests execute (often concurrently) within a server system. A single request may exhibit fluctuating hardware characteristics (such as instruction completion rate and on-chip resource usage) over the course of its execution, due to inherent variations in application execution semantics as well as dynamic resource competition on resource-sharing processors like multicores. Understanding such behavior variations can assist fine-grained request modeling and adaptive resource management. This paper presents operating system management to track request behavior variations online. In addition to metric sample collection during periodic interrupts, we exploit the frequent system calls in server applications to perform low-cost in-kernel sampling. We utilize identified behavior variations to support or enhance request modeling in request classification, anomaly analysis, and online request signature construction. A foundation of our request modeling is the ability to quantify the difference between two requests' time series behaviors. We evaluate several differencing measures and enhance the classic dynamic time warping technique with additional penalties for asynchronous warp steps. Finally, motivated by fluctuating request resource usage and the resulting contention, we implement contention-easing CPU scheduling on multicore platforms and demonstrate its effectiveness in improving the worst-case request performance. Experiments in this paper are based on five server applications -- Apache web server, TPCC, TPCH, RUBiS online auction benchmark, and a user-content-driven online teaching application called WeBWorK.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {103--116},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736034},
 doi = {http://doi.acm.org/10.1145/1736020.1736034},
 acmid = {1736034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware counter, multicore, operating system adaptation, request modeling, server system},
} 

@article{Shen:2010:RBV:1735970.1736034,
 author = {Shen, Kai},
 title = {Request behavior variations},
 abstract = {A large number of user requests execute (often concurrently) within a server system. A single request may exhibit fluctuating hardware characteristics (such as instruction completion rate and on-chip resource usage) over the course of its execution, due to inherent variations in application execution semantics as well as dynamic resource competition on resource-sharing processors like multicores. Understanding such behavior variations can assist fine-grained request modeling and adaptive resource management. This paper presents operating system management to track request behavior variations online. In addition to metric sample collection during periodic interrupts, we exploit the frequent system calls in server applications to perform low-cost in-kernel sampling. We utilize identified behavior variations to support or enhance request modeling in request classification, anomaly analysis, and online request signature construction. A foundation of our request modeling is the ability to quantify the difference between two requests' time series behaviors. We evaluate several differencing measures and enhance the classic dynamic time warping technique with additional penalties for asynchronous warp steps. Finally, motivated by fluctuating request resource usage and the resulting contention, we implement contention-easing CPU scheduling on multicore platforms and demonstrate its effectiveness in improving the worst-case request performance. Experiments in this paper are based on five server applications -- Apache web server, TPCC, TPCH, RUBiS online auction benchmark, and a user-content-driven online teaching application called WeBWorK.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {103--116},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736034},
 doi = {http://doi.acm.org/10.1145/1735970.1736034},
 acmid = {1736034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware counter, multicore, operating system adaptation, request modeling, server system},
} 

@article{Johnson:2010:DCM:1735970.1736035,
 author = {Johnson, F. Ryan and Stoica, Radu and Ailamaki, Anastasia and Mowry, Todd C.},
 title = {Decoupling contention management from scheduling},
 abstract = {Many parallel applications exhibit unpredictable communication between threads, leading to contention for shared objects. The choice of contention management strategy impacts strongly the performance and scalability of these applications: spinning provides maximum performance but wastes significant processor resources, while blocking-based approaches conserve processor resources but introduce high overheads on the critical path of computation. Under situations of high or changing load, the operating system complicates matters further with arbitrary scheduling decisions which often preempt lock holders, leading to long serialization delays until the preempted thread resumes execution. We observe that contention management is orthogonal to the problems of scheduling and load management and propose to decouple them so each may be solved independently and effectively. To this end, we propose a load control mechanism which manages the number of active threads in the system separately from any contention which may exist. By isolating contention management from damaging interactions with the OS scheduler, we combine the efficiency of spinning with the robustness of blocking. The proposed load control mechanism results in stable, high performance for both lightly and heavily loaded systems, requires no special privileges or modifications at the OS level, and can be implemented as a library which benefits existing code.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736035},
 doi = {http://doi.acm.org/10.1145/1735970.1736035},
 acmid = {1736035},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blocking, concurrency control, contention, load management, multicore, scheduling, spinning, threads},
} 

@inproceedings{Johnson:2010:DCM:1736020.1736035,
 author = {Johnson, F. Ryan and Stoica, Radu and Ailamaki, Anastasia and Mowry, Todd C.},
 title = {Decoupling contention management from scheduling},
 abstract = {Many parallel applications exhibit unpredictable communication between threads, leading to contention for shared objects. The choice of contention management strategy impacts strongly the performance and scalability of these applications: spinning provides maximum performance but wastes significant processor resources, while blocking-based approaches conserve processor resources but introduce high overheads on the critical path of computation. Under situations of high or changing load, the operating system complicates matters further with arbitrary scheduling decisions which often preempt lock holders, leading to long serialization delays until the preempted thread resumes execution. We observe that contention management is orthogonal to the problems of scheduling and load management and propose to decouple them so each may be solved independently and effectively. To this end, we propose a load control mechanism which manages the number of active threads in the system separately from any contention which may exist. By isolating contention management from damaging interactions with the OS scheduler, we combine the efficiency of spinning with the robustness of blocking. The proposed load control mechanism results in stable, high performance for both lightly and heavily loaded systems, requires no special privileges or modifications at the OS level, and can be implemented as a library which benefits existing code.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736035},
 doi = {http://doi.acm.org/10.1145/1736020.1736035},
 acmid = {1736035},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blocking, concurrency control, contention, load management, multicore, scheduling, spinning, threads},
} 

@article{Johnson:2010:DCM:1735971.1736035,
 author = {Johnson, F. Ryan and Stoica, Radu and Ailamaki, Anastasia and Mowry, Todd C.},
 title = {Decoupling contention management from scheduling},
 abstract = {Many parallel applications exhibit unpredictable communication between threads, leading to contention for shared objects. The choice of contention management strategy impacts strongly the performance and scalability of these applications: spinning provides maximum performance but wastes significant processor resources, while blocking-based approaches conserve processor resources but introduce high overheads on the critical path of computation. Under situations of high or changing load, the operating system complicates matters further with arbitrary scheduling decisions which often preempt lock holders, leading to long serialization delays until the preempted thread resumes execution. We observe that contention management is orthogonal to the problems of scheduling and load management and propose to decouple them so each may be solved independently and effectively. To this end, we propose a load control mechanism which manages the number of active threads in the system separately from any contention which may exist. By isolating contention management from damaging interactions with the OS scheduler, we combine the efficiency of spinning with the robustness of blocking. The proposed load control mechanism results in stable, high performance for both lightly and heavily loaded systems, requires no special privileges or modifications at the OS level, and can be implemented as a library which benefits existing code.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736035},
 doi = {http://doi.acm.org/10.1145/1735971.1736035},
 acmid = {1736035},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blocking, concurrency control, contention, load management, multicore, scheduling, spinning, threads},
} 

@inproceedings{Zhuravlev:2010:ASR:1736020.1736036,
 author = {Zhuravlev, Sergey and Blagodurov, Sergey and Fedorova, Alexandra},
 title = {Addressing shared resource contention in multicore processors via scheduling},
 abstract = {Contention for shared resources on multicore processors remains an unsolved problem in existing systems despite significant research efforts dedicated to this problem in the past. Previous solutions focused primarily on hardware techniques and software page coloring to mitigate this problem. Our goal is to investigate how and to what extent contention for shared resource can be mitigated via thread scheduling. Scheduling is an attractive tool, because it does not require extra hardware and is relatively easy to integrate into the system. Our study is the first to provide a comprehensive analysis of contention-mitigating techniques that use only scheduling. The most difficult part of the problem is to find a classification scheme for threads, which would determine how they affect each other when competing for shared resources. We provide a comprehensive analysis of such classification schemes using a newly proposed methodology that enables to evaluate these schemes separately from the scheduling algorithm itself and to compare them to the optimal. As a result of this analysis we discovered a classification scheme that addresses not only contention for cache space, but contention for other shared resources, such as the memory controller, memory bus and prefetching hardware. To show the applicability of our analysis we design a new scheduling algorithm, which we prototype at user level, and demonstrate that it performs within 2\\% of the optimal. We also conclude that the highest impact of contention-aware scheduling techniques is not in improving performance of a workload as a whole but in improving quality of service or performance isolation for individual applications.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {129--142},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736036},
 doi = {http://doi.acm.org/10.1145/1736020.1736036},
 acmid = {1736036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multicore processors, scheduling, shared resource contention},
} 

@article{Zhuravlev:2010:ASR:1735970.1736036,
 author = {Zhuravlev, Sergey and Blagodurov, Sergey and Fedorova, Alexandra},
 title = {Addressing shared resource contention in multicore processors via scheduling},
 abstract = {Contention for shared resources on multicore processors remains an unsolved problem in existing systems despite significant research efforts dedicated to this problem in the past. Previous solutions focused primarily on hardware techniques and software page coloring to mitigate this problem. Our goal is to investigate how and to what extent contention for shared resource can be mitigated via thread scheduling. Scheduling is an attractive tool, because it does not require extra hardware and is relatively easy to integrate into the system. Our study is the first to provide a comprehensive analysis of contention-mitigating techniques that use only scheduling. The most difficult part of the problem is to find a classification scheme for threads, which would determine how they affect each other when competing for shared resources. We provide a comprehensive analysis of such classification schemes using a newly proposed methodology that enables to evaluate these schemes separately from the scheduling algorithm itself and to compare them to the optimal. As a result of this analysis we discovered a classification scheme that addresses not only contention for cache space, but contention for other shared resources, such as the memory controller, memory bus and prefetching hardware. To show the applicability of our analysis we design a new scheduling algorithm, which we prototype at user level, and demonstrate that it performs within 2\\% of the optimal. We also conclude that the highest impact of contention-aware scheduling techniques is not in improving performance of a workload as a whole but in improving quality of service or performance isolation for individual applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {129--142},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736036},
 doi = {http://doi.acm.org/10.1145/1735970.1736036},
 acmid = {1736036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multicore processors, scheduling, shared resource contention},
} 

@article{Zhuravlev:2010:ASR:1735971.1736036,
 author = {Zhuravlev, Sergey and Blagodurov, Sergey and Fedorova, Alexandra},
 title = {Addressing shared resource contention in multicore processors via scheduling},
 abstract = {Contention for shared resources on multicore processors remains an unsolved problem in existing systems despite significant research efforts dedicated to this problem in the past. Previous solutions focused primarily on hardware techniques and software page coloring to mitigate this problem. Our goal is to investigate how and to what extent contention for shared resource can be mitigated via thread scheduling. Scheduling is an attractive tool, because it does not require extra hardware and is relatively easy to integrate into the system. Our study is the first to provide a comprehensive analysis of contention-mitigating techniques that use only scheduling. The most difficult part of the problem is to find a classification scheme for threads, which would determine how they affect each other when competing for shared resources. We provide a comprehensive analysis of such classification schemes using a newly proposed methodology that enables to evaluate these schemes separately from the scheduling algorithm itself and to compare them to the optimal. As a result of this analysis we discovered a classification scheme that addresses not only contention for cache space, but contention for other shared resources, such as the memory controller, memory bus and prefetching hardware. To show the applicability of our analysis we design a new scheduling algorithm, which we prototype at user level, and demonstrate that it performs within 2\\% of the optimal. We also conclude that the highest impact of contention-aware scheduling techniques is not in improving performance of a workload as a whole but in improving quality of service or performance isolation for individual applications.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {129--142},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736036},
 doi = {http://doi.acm.org/10.1145/1735971.1736036},
 acmid = {1736036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multicore processors, scheduling, shared resource contention},
} 

@article{Yuan:2010:SED:1735971.1736038,
 author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
 title = {SherLog: error diagnosis by connecting clues from run-time logs},
 abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors. Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution. We evaluate SherLog with 8 representative real world</i> software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736038},
 doi = {http://doi.acm.org/10.1145/1735971.1736038},
 acmid = {1736038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure diagnostics, log, static analysis},
} 

@inproceedings{Yuan:2010:SED:1736020.1736038,
 author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
 title = {SherLog: error diagnosis by connecting clues from run-time logs},
 abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors. Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution. We evaluate SherLog with 8 representative real world</i> software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736038},
 doi = {http://doi.acm.org/10.1145/1736020.1736038},
 acmid = {1736038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure diagnostics, log, static analysis},
} 

@article{Yuan:2010:SED:1735970.1736038,
 author = {Yuan, Ding and Mai, Haohui and Xiong, Weiwei and Tan, Lin and Zhou, Yuanyuan and Pasupathy, Shankar},
 title = {SherLog: error diagnosis by connecting clues from run-time logs},
 abstract = {Computer systems often fail due to many factors such as software bugs or administrator errors. Diagnosing such production run failures is an important but challenging task since it is difficult to reproduce them in house due to various reasons: (1) unavailability of users' inputs and file content due to privacy concerns; (2) difficulty in building the exact same execution environment; and (3) non-determinism of concurrent executions on multi-processors. Therefore, programmers often have to diagnose a production run failure based on logs collected back from customers and the corresponding source code. Such diagnosis requires expert knowledge and is also too time-consuming, tedious to narrow down root causes. To address this problem, we propose a tool, called SherLog, that analyzes source code by leveraging information provided by run-time logs to infer what must or may have happened during the failed production run. It requires neither re-execution of the program nor knowledge on the log's semantics. It infers both control and data value information regarding to the failed execution. We evaluate SherLog with 8 representative real world</i> software failures (6 software bugs and 2 configuration errors) from 7 applications including 3 servers. Information inferred by SherLog are very useful for programmers to diagnose these evaluated failures. Our results also show that SherLog can analyze large server applications such as Apache with thousands of logging messages within only 40 minutes.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736038},
 doi = {http://doi.acm.org/10.1145/1735970.1736038},
 acmid = {1736038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure diagnostics, log, static analysis},
} 

@article{Weeratunge:2010:AMD:1735970.1736039,
 author = {Weeratunge, Dasarath and Zhang, Xiangyu and Jagannathan, Suresh},
 title = {Analyzing multicore dumps to facilitate concurrency bug reproduction},
 abstract = {Debugging concurrent programs is difficult. This is primarily because the inherent non-determinism that arises because of scheduler interleavings makes it hard to easily reproduce bugs that may manifest only under certain interleavings. The problem is exacerbated in multi-core environments where there are multiple schedulers, one for each core. In this paper, we propose a reproduction technique for concurrent programs that execute on multi-core platforms. Our technique performs a lightweight analysis of a failing execution that occurs in a multi-core environment, and uses the result of the analysis to enable reproduction of the bug in a single-core system, under the control of a deterministic scheduler. More specifically, our approach automatically identifies the execution point in the re-execution that corresponds to the failure point. It does so by analyzing the failure core dump and leveraging a technique called execution indexing</i> that identifies a related point in the re-execution. By generating a core dump at this point, and comparing the differences betwen the two dumps, we are able to guide a search algorithm to efficiently generate a failure inducing schedule. Our experiments show that our technique is highly effective and has reasonable overhead.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736039},
 doi = {http://doi.acm.org/10.1145/1735970.1736039},
 acmid = {1736039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, execution indexing, multi-core, reproduction},
} 

@article{Weeratunge:2010:AMD:1735971.1736039,
 author = {Weeratunge, Dasarath and Zhang, Xiangyu and Jagannathan, Suresh},
 title = {Analyzing multicore dumps to facilitate concurrency bug reproduction},
 abstract = {Debugging concurrent programs is difficult. This is primarily because the inherent non-determinism that arises because of scheduler interleavings makes it hard to easily reproduce bugs that may manifest only under certain interleavings. The problem is exacerbated in multi-core environments where there are multiple schedulers, one for each core. In this paper, we propose a reproduction technique for concurrent programs that execute on multi-core platforms. Our technique performs a lightweight analysis of a failing execution that occurs in a multi-core environment, and uses the result of the analysis to enable reproduction of the bug in a single-core system, under the control of a deterministic scheduler. More specifically, our approach automatically identifies the execution point in the re-execution that corresponds to the failure point. It does so by analyzing the failure core dump and leveraging a technique called execution indexing</i> that identifies a related point in the re-execution. By generating a core dump at this point, and comparing the differences betwen the two dumps, we are able to guide a search algorithm to efficiently generate a failure inducing schedule. Our experiments show that our technique is highly effective and has reasonable overhead.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736039},
 doi = {http://doi.acm.org/10.1145/1735971.1736039},
 acmid = {1736039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, execution indexing, multi-core, reproduction},
} 

@inproceedings{Weeratunge:2010:AMD:1736020.1736039,
 author = {Weeratunge, Dasarath and Zhang, Xiangyu and Jagannathan, Suresh},
 title = {Analyzing multicore dumps to facilitate concurrency bug reproduction},
 abstract = {Debugging concurrent programs is difficult. This is primarily because the inherent non-determinism that arises because of scheduler interleavings makes it hard to easily reproduce bugs that may manifest only under certain interleavings. The problem is exacerbated in multi-core environments where there are multiple schedulers, one for each core. In this paper, we propose a reproduction technique for concurrent programs that execute on multi-core platforms. Our technique performs a lightweight analysis of a failing execution that occurs in a multi-core environment, and uses the result of the analysis to enable reproduction of the bug in a single-core system, under the control of a deterministic scheduler. More specifically, our approach automatically identifies the execution point in the re-execution that corresponds to the failure point. It does so by analyzing the failure core dump and leveraging a technique called execution indexing</i> that identifies a related point in the re-execution. By generating a core dump at this point, and comparing the differences betwen the two dumps, we are able to guide a search algorithm to efficiently generate a failure inducing schedule. Our experiments show that our technique is highly effective and has reasonable overhead.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736039},
 doi = {http://doi.acm.org/10.1145/1736020.1736039},
 acmid = {1736039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, execution indexing, multi-core, reproduction},
} 

@inproceedings{Burckhardt:2010:RSP:1736020.1736040,
 author = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
 title = {A randomized scheduler with probabilistic guarantees of finding bugs},
 abstract = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n</i> threads and k</i> steps, our scheduler detects a concurrency bug of depth d</i> with probability at least 1/nk</i><sup>d-1</sup>. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736040},
 doi = {http://doi.acm.org/10.1145/1736020.1736040},
 acmid = {1736040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, race conditions, randomized algorithms, testing},
} 

@article{Burckhardt:2010:RSP:1735971.1736040,
 author = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
 title = {A randomized scheduler with probabilistic guarantees of finding bugs},
 abstract = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n</i> threads and k</i> steps, our scheduler detects a concurrency bug of depth d</i> with probability at least 1/nk</i><sup>d-1</sup>. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736040},
 doi = {http://doi.acm.org/10.1145/1735971.1736040},
 acmid = {1736040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, race conditions, randomized algorithms, testing},
} 

@article{Burckhardt:2010:RSP:1735970.1736040,
 author = {Burckhardt, Sebastian and Kothari, Pravesh and Musuvathi, Madanlal and Nagarakatte, Santosh},
 title = {A randomized scheduler with probabilistic guarantees of finding bugs},
 abstract = {This paper presents a randomized scheduler for finding concurrency bugs. Like current stress-testing methods, it repeatedly runs a given test program with supplied inputs. However, it improves on stress-testing by finding buggy schedules more effectively and by quantifying the probability of missing concurrency bugs. Key to its design is the characterization of the depth of a concurrency bug as the minimum number of scheduling constraints required to find it. In a single run of a program with n</i> threads and k</i> steps, our scheduler detects a concurrency bug of depth d</i> with probability at least 1/nk</i><sup>d-1</sup>. We hypothesize that in practice, many concurrency bugs (including well-known types such as ordering errors, atomicity violations, and deadlocks) have small bug-depths, and we confirm the efficiency of our schedule randomization by detecting previously unknown and known concurrency bugs in several production-scale concurrent programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736040},
 doi = {http://doi.acm.org/10.1145/1735970.1736040},
 acmid = {1736040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, race conditions, randomized algorithms, testing},
} 

@inproceedings{Zhang:2010:CDS:1736020.1736041,
 author = {Zhang, Wei and Sun, Chong and Lu, Shan},
 title = {ConMem: detecting severe concurrency bugs through an effect-oriented approach},
 abstract = {Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and non-deterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production runs and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable. Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity-violations), this paper targets concurrency bugs that result in one type of severe effects: program crashes. Our study of the error-propagation process of realworld concurrency bugs reveals a common pattern (50\% in our non-deadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereference, dangling-pointer, buffer-overflow, uninitialized-read) on shared memory objects. Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predicatively detect these common and severe concurrency-memory bugs. We also built a validator ConMem-v to automatically prune false positives by enforcing potential bug-triggering interleavings. We evaluated ConMem using 7 open-source programs with 9 real-world severe concurrency bugs. ConMem detects more tested bugs (8 out of 9 bugs) than a lock-set-based race detector and an unserializable-interleaving detector that detect 4 and 5 bugs respectively, with a false positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {179--192},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736041},
 doi = {http://doi.acm.org/10.1145/1736020.1736041},
 acmid = {1736041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, software testing},
} 

@article{Zhang:2010:CDS:1735970.1736041,
 author = {Zhang, Wei and Sun, Chong and Lu, Shan},
 title = {ConMem: detecting severe concurrency bugs through an effect-oriented approach},
 abstract = {Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and non-deterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production runs and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable. Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity-violations), this paper targets concurrency bugs that result in one type of severe effects: program crashes. Our study of the error-propagation process of realworld concurrency bugs reveals a common pattern (50\% in our non-deadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereference, dangling-pointer, buffer-overflow, uninitialized-read) on shared memory objects. Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predicatively detect these common and severe concurrency-memory bugs. We also built a validator ConMem-v to automatically prune false positives by enforcing potential bug-triggering interleavings. We evaluated ConMem using 7 open-source programs with 9 real-world severe concurrency bugs. ConMem detects more tested bugs (8 out of 9 bugs) than a lock-set-based race detector and an unserializable-interleaving detector that detect 4 and 5 bugs respectively, with a false positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {179--192},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736041},
 doi = {http://doi.acm.org/10.1145/1735970.1736041},
 acmid = {1736041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, software testing},
} 

@article{Zhang:2010:CDS:1735971.1736041,
 author = {Zhang, Wei and Sun, Chong and Lu, Shan},
 title = {ConMem: detecting severe concurrency bugs through an effect-oriented approach},
 abstract = {Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and non-deterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production runs and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable. Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity-violations), this paper targets concurrency bugs that result in one type of severe effects: program crashes. Our study of the error-propagation process of realworld concurrency bugs reveals a common pattern (50\% in our non-deadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereference, dangling-pointer, buffer-overflow, uninitialized-read) on shared memory objects. Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predicatively detect these common and severe concurrency-memory bugs. We also built a validator ConMem-v to automatically prune false positives by enforcing potential bug-triggering interleavings. We evaluated ConMem using 7 open-source programs with 9 real-world severe concurrency bugs. ConMem detects more tested bugs (8 out of 9 bugs) than a lock-set-based race detector and an unserializable-interleaving detector that detect 4 and 5 bugs respectively, with a false positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {179--192},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736041},
 doi = {http://doi.acm.org/10.1145/1735971.1736041},
 acmid = {1736041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, software testing},
} 

@article{Mesa-Martinez:2010:CPT:1735970.1736043,
 author = {Mesa-Martinez, Francisco Javier and Ardestani, Ehsan K. and Renau, Jose},
 title = {Characterizing processor thermal behavior},
 abstract = {Temperature is a dominant factor in the performance, reliability, and leakage power consumption of modern processors. As a result, increasing numbers of researchers evaluate thermal characteristics in their proposals. In this paper, we measure a real processor focusing on its thermal characterization executing diverse workloads. Our results show that in real designs, thermal transients operate at larger scales than their performance and power counterparts. Conventional thermal simulation methodologies based on profile-based simulation or statistical sampling, such as Simpoint, tend to explore very limited execution spans. Short simulation times can lead to reduced matchings between performance and thermal phases. To illustrate these issues we characterize and classify from a thermal standpoint SPEC00 and SPEC06 applications, which are traditionally used in the evaluation of architectural proposals. This paper concludes with a list of recommendations regarding thermal modeling considerations based on our experimental insights.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736043},
 doi = {http://doi.acm.org/10.1145/1735970.1736043},
 acmid = {1736043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microarchitecture, temperature, thermal simulation},
} 

@inproceedings{Mesa-Martinez:2010:CPT:1736020.1736043,
 author = {Mesa-Martinez, Francisco Javier and Ardestani, Ehsan K. and Renau, Jose},
 title = {Characterizing processor thermal behavior},
 abstract = {Temperature is a dominant factor in the performance, reliability, and leakage power consumption of modern processors. As a result, increasing numbers of researchers evaluate thermal characteristics in their proposals. In this paper, we measure a real processor focusing on its thermal characterization executing diverse workloads. Our results show that in real designs, thermal transients operate at larger scales than their performance and power counterparts. Conventional thermal simulation methodologies based on profile-based simulation or statistical sampling, such as Simpoint, tend to explore very limited execution spans. Short simulation times can lead to reduced matchings between performance and thermal phases. To illustrate these issues we characterize and classify from a thermal standpoint SPEC00 and SPEC06 applications, which are traditionally used in the evaluation of architectural proposals. This paper concludes with a list of recommendations regarding thermal modeling considerations based on our experimental insights.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736043},
 doi = {http://doi.acm.org/10.1145/1736020.1736043},
 acmid = {1736043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microarchitecture, temperature, thermal simulation},
} 

@article{Mesa-Martinez:2010:CPT:1735971.1736043,
 author = {Mesa-Martinez, Francisco Javier and Ardestani, Ehsan K. and Renau, Jose},
 title = {Characterizing processor thermal behavior},
 abstract = {Temperature is a dominant factor in the performance, reliability, and leakage power consumption of modern processors. As a result, increasing numbers of researchers evaluate thermal characteristics in their proposals. In this paper, we measure a real processor focusing on its thermal characterization executing diverse workloads. Our results show that in real designs, thermal transients operate at larger scales than their performance and power counterparts. Conventional thermal simulation methodologies based on profile-based simulation or statistical sampling, such as Simpoint, tend to explore very limited execution spans. Short simulation times can lead to reduced matchings between performance and thermal phases. To illustrate these issues we characterize and classify from a thermal standpoint SPEC00 and SPEC06 applications, which are traditionally used in the evaluation of architectural proposals. This paper concludes with a list of recommendations regarding thermal modeling considerations based on our experimental insights.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736043},
 doi = {http://doi.acm.org/10.1145/1735971.1736043},
 acmid = {1736043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microarchitecture, temperature, thermal simulation},
} 

@article{Venkatesh:2010:CCR:1735970.1736044,
 author = {Venkatesh, Ganesh and Sampson, Jack and Goulding, Nathan and Garcia, Saturnino and Bryksin, Vladyslav and Lugo-Martinez, Jose and Swanson, Steven and Taylor, Michael Bedford},
 title = {Conservation cores: reducing the energy of mature computations},
 abstract = {Growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall</i> that limits the fraction of a chip that can run at full speed at one time. In this regime, specialized, energy-efficient processors can increase parallelism by reducing the per-computation power requirements and allowing more computations to execute under the same power budget. To pursue this goal, this paper introduces conservation cores</i>. Conservation cores, or c-cores</i>, are specialized processors that focus on reducing energy and energy-delay instead of increasing performance. This focus on energy makes c-cores an excellent match for many applications that would be poor candidates for hardware acceleration (e.g., irregular integer codes). We present a toolchain for automatically synthesizing c-cores from application source code and demonstrate that they can significantly reduce energy and energy-delay for a wide range of applications. The c-cores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. Our results show that conservation cores can reduce energy consumption by up to 16.0x for functions and by up to 2.1x for whole applications, while patching can extend the useful lifetime of individual c-cores to match that of conventional processors.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {205--218},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736044},
 doi = {http://doi.acm.org/10.1145/1735970.1736044},
 acmid = {1736044},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conservation core, heterogeneous many-core, patching, utilization wall},
} 

@inproceedings{Venkatesh:2010:CCR:1736020.1736044,
 author = {Venkatesh, Ganesh and Sampson, Jack and Goulding, Nathan and Garcia, Saturnino and Bryksin, Vladyslav and Lugo-Martinez, Jose and Swanson, Steven and Taylor, Michael Bedford},
 title = {Conservation cores: reducing the energy of mature computations},
 abstract = {Growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall</i> that limits the fraction of a chip that can run at full speed at one time. In this regime, specialized, energy-efficient processors can increase parallelism by reducing the per-computation power requirements and allowing more computations to execute under the same power budget. To pursue this goal, this paper introduces conservation cores</i>. Conservation cores, or c-cores</i>, are specialized processors that focus on reducing energy and energy-delay instead of increasing performance. This focus on energy makes c-cores an excellent match for many applications that would be poor candidates for hardware acceleration (e.g., irregular integer codes). We present a toolchain for automatically synthesizing c-cores from application source code and demonstrate that they can significantly reduce energy and energy-delay for a wide range of applications. The c-cores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. Our results show that conservation cores can reduce energy consumption by up to 16.0x for functions and by up to 2.1x for whole applications, while patching can extend the useful lifetime of individual c-cores to match that of conventional processors.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {205--218},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736044},
 doi = {http://doi.acm.org/10.1145/1736020.1736044},
 acmid = {1736044},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conservation core, heterogeneous many-core, patching, utilization wall},
} 

@article{Venkatesh:2010:CCR:1735971.1736044,
 author = {Venkatesh, Ganesh and Sampson, Jack and Goulding, Nathan and Garcia, Saturnino and Bryksin, Vladyslav and Lugo-Martinez, Jose and Swanson, Steven and Taylor, Michael Bedford},
 title = {Conservation cores: reducing the energy of mature computations},
 abstract = {Growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall</i> that limits the fraction of a chip that can run at full speed at one time. In this regime, specialized, energy-efficient processors can increase parallelism by reducing the per-computation power requirements and allowing more computations to execute under the same power budget. To pursue this goal, this paper introduces conservation cores</i>. Conservation cores, or c-cores</i>, are specialized processors that focus on reducing energy and energy-delay instead of increasing performance. This focus on energy makes c-cores an excellent match for many applications that would be poor candidates for hardware acceleration (e.g., irregular integer codes). We present a toolchain for automatically synthesizing c-cores from application source code and demonstrate that they can significantly reduce energy and energy-delay for a wide range of applications. The c-cores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. Our results show that conservation cores can reduce energy consumption by up to 16.0x for functions and by up to 2.1x for whole applications, while patching can extend the useful lifetime of individual c-cores to match that of conventional processors.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {205--218},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736044},
 doi = {http://doi.acm.org/10.1145/1735971.1736044},
 acmid = {1736044},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conservation core, heterogeneous many-core, patching, utilization wall},
} 

@article{Sudan:2010:MID:1735970.1736045,
 author = {Sudan, Kshitij and Chatterjee, Niladrish and Nellans, David and Awasthi, Manu and Balasubramonian, Rajeev and Davis, Al},
 title = {Micro-pages: increasing DRAM efficiency with locality-aware data placement},
 abstract = {Power consumption and DRAM latencies are serious concerns in modern chip-multiprocessor (CMP or multi-core) based compute systems. The management of the DRAM row buffer can significantly impact both power consumption and latency. Modern DRAM systems read data from cell arrays and populate a row buffer as large as 8 KB on a memory request. But only a small fraction of these bits are ever returned back to the CPU. This ends up wasting energy and time to read (and subsequently write back) bits which are used rarely. Traditionally, an open-page policy has been used for uni-processor systems and it has worked well because of spatial and temporal locality in the access stream. In future multi-core processors, the possibly independent access streams of each core are interleaved, thus destroying the available locality and significantly under-utilizing the contents of the row buffer. In this work, we attempt to improve row-buffer utilization for future multi-core systems. The schemes presented here are motivated by our observations that a large number of accesses within heavily accessed OS pages are to small, contiguous "chunks" of cache blocks. Thus, the co-location of chunks (from different OS pages) in a row-buffer will improve the overall utilization of the row buffer contents, and consequently reduce memory energy consumption and access time. Such co-location can be achieved in many ways, notably involving a reduction in OS page size and software or hardware assisted migration of data within DRAM. We explore these mechanisms and discuss the trade-offs involved along with energy and performance improvements from each scheme. On average, for applications with room for improvement, our best performing scheme increases performance by 9\% (max. 18\%) and reduces memory energy consumption by 15\% (max. 70\%).},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {219--230},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736045},
 doi = {http://doi.acm.org/10.1145/1735970.1736045},
 acmid = {1736045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data placement, dram row-buffer management},
} 

@article{Sudan:2010:MID:1735971.1736045,
 author = {Sudan, Kshitij and Chatterjee, Niladrish and Nellans, David and Awasthi, Manu and Balasubramonian, Rajeev and Davis, Al},
 title = {Micro-pages: increasing DRAM efficiency with locality-aware data placement},
 abstract = {Power consumption and DRAM latencies are serious concerns in modern chip-multiprocessor (CMP or multi-core) based compute systems. The management of the DRAM row buffer can significantly impact both power consumption and latency. Modern DRAM systems read data from cell arrays and populate a row buffer as large as 8 KB on a memory request. But only a small fraction of these bits are ever returned back to the CPU. This ends up wasting energy and time to read (and subsequently write back) bits which are used rarely. Traditionally, an open-page policy has been used for uni-processor systems and it has worked well because of spatial and temporal locality in the access stream. In future multi-core processors, the possibly independent access streams of each core are interleaved, thus destroying the available locality and significantly under-utilizing the contents of the row buffer. In this work, we attempt to improve row-buffer utilization for future multi-core systems. The schemes presented here are motivated by our observations that a large number of accesses within heavily accessed OS pages are to small, contiguous "chunks" of cache blocks. Thus, the co-location of chunks (from different OS pages) in a row-buffer will improve the overall utilization of the row buffer contents, and consequently reduce memory energy consumption and access time. Such co-location can be achieved in many ways, notably involving a reduction in OS page size and software or hardware assisted migration of data within DRAM. We explore these mechanisms and discuss the trade-offs involved along with energy and performance improvements from each scheme. On average, for applications with room for improvement, our best performing scheme increases performance by 9\% (max. 18\%) and reduces memory energy consumption by 15\% (max. 70\%).},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {219--230},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736045},
 doi = {http://doi.acm.org/10.1145/1735971.1736045},
 acmid = {1736045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data placement, dram row-buffer management},
} 

@inproceedings{Sudan:2010:MID:1736020.1736045,
 author = {Sudan, Kshitij and Chatterjee, Niladrish and Nellans, David and Awasthi, Manu and Balasubramonian, Rajeev and Davis, Al},
 title = {Micro-pages: increasing DRAM efficiency with locality-aware data placement},
 abstract = {Power consumption and DRAM latencies are serious concerns in modern chip-multiprocessor (CMP or multi-core) based compute systems. The management of the DRAM row buffer can significantly impact both power consumption and latency. Modern DRAM systems read data from cell arrays and populate a row buffer as large as 8 KB on a memory request. But only a small fraction of these bits are ever returned back to the CPU. This ends up wasting energy and time to read (and subsequently write back) bits which are used rarely. Traditionally, an open-page policy has been used for uni-processor systems and it has worked well because of spatial and temporal locality in the access stream. In future multi-core processors, the possibly independent access streams of each core are interleaved, thus destroying the available locality and significantly under-utilizing the contents of the row buffer. In this work, we attempt to improve row-buffer utilization for future multi-core systems. The schemes presented here are motivated by our observations that a large number of accesses within heavily accessed OS pages are to small, contiguous "chunks" of cache blocks. Thus, the co-location of chunks (from different OS pages) in a row-buffer will improve the overall utilization of the row buffer contents, and consequently reduce memory energy consumption and access time. Such co-location can be achieved in many ways, notably involving a reduction in OS page size and software or hardware assisted migration of data within DRAM. We explore these mechanisms and discuss the trade-offs involved along with energy and performance improvements from each scheme. On average, for applications with room for improvement, our best performing scheme increases performance by 9\% (max. 18\%) and reduces memory energy consumption by 15\% (max. 70\%).},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {219--230},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736045},
 doi = {http://doi.acm.org/10.1145/1736020.1736045},
 acmid = {1736045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data placement, dram row-buffer management},
} 

@inproceedings{Pelley:2010:PRD:1736020.1736047,
 author = {Pelley, Steven and Meisner, David and Zandevakili, Pooya and Wenisch, Thomas F. and Underwood, Jack},
 title = {Power routing: dynamic power provisioning in the data center},
 abstract = {Data center power infrastructure incurs massive capital costs, which typically exceed energy costs over the life of the facility. To squeeze maximum value from the infrastructure, researchers have proposed over-subscribing power circuits, relying on the observation that peak loads are rare. To ensure availability, these proposals employ power capping, which throttles server performance during utilization spikes to enforce safe power budgets. However, because budgets must be enforced locally -- at each power distribution unit (PDU) -- local utilization spikes may force throttling even when power delivery capacity is available elsewhere. Moreover, the need to maintain reserve capacity for fault tolerance on power delivery paths magnifies the impact of utilization spikes. In this paper, we develop mechanisms to better utilize installed power infrastructure, reducing reserve capacity margins and avoiding performance throttling. Unlike conventional high-availability data centers, where collocated servers share identical primary and secondary power feeds, we reorganize power feeds to create shuffled power distribution topologies. Shuffled topologies spread secondary power feeds over numerous PDUs, reducing reserve capacity requirements to tolerate a single PDU failure. Second, we propose Power Routing, which schedules IT load dynamically across redundant power feeds to: (1) shift slack to servers with growing power demands, and (2) balance power draw across AC phases to reduce heating and improve electrical stability. We describe efficient heuristics for scheduling servers to PDUs (an NP-complete problem). Using data collected from nearly 1000 servers in three production facilities, we demonstrate that these mechanisms can reduce the required power infrastructure capacity relative to conventional high-availability data centers by 32\% without performance degradation.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {231--242},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736047},
 doi = {http://doi.acm.org/10.1145/1736020.1736047},
 acmid = {1736047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data centers, power infrastructure},
} 

@article{Pelley:2010:PRD:1735970.1736047,
 author = {Pelley, Steven and Meisner, David and Zandevakili, Pooya and Wenisch, Thomas F. and Underwood, Jack},
 title = {Power routing: dynamic power provisioning in the data center},
 abstract = {Data center power infrastructure incurs massive capital costs, which typically exceed energy costs over the life of the facility. To squeeze maximum value from the infrastructure, researchers have proposed over-subscribing power circuits, relying on the observation that peak loads are rare. To ensure availability, these proposals employ power capping, which throttles server performance during utilization spikes to enforce safe power budgets. However, because budgets must be enforced locally -- at each power distribution unit (PDU) -- local utilization spikes may force throttling even when power delivery capacity is available elsewhere. Moreover, the need to maintain reserve capacity for fault tolerance on power delivery paths magnifies the impact of utilization spikes. In this paper, we develop mechanisms to better utilize installed power infrastructure, reducing reserve capacity margins and avoiding performance throttling. Unlike conventional high-availability data centers, where collocated servers share identical primary and secondary power feeds, we reorganize power feeds to create shuffled power distribution topologies. Shuffled topologies spread secondary power feeds over numerous PDUs, reducing reserve capacity requirements to tolerate a single PDU failure. Second, we propose Power Routing, which schedules IT load dynamically across redundant power feeds to: (1) shift slack to servers with growing power demands, and (2) balance power draw across AC phases to reduce heating and improve electrical stability. We describe efficient heuristics for scheduling servers to PDUs (an NP-complete problem). Using data collected from nearly 1000 servers in three production facilities, we demonstrate that these mechanisms can reduce the required power infrastructure capacity relative to conventional high-availability data centers by 32\% without performance degradation.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {231--242},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736047},
 doi = {http://doi.acm.org/10.1145/1735970.1736047},
 acmid = {1736047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data centers, power infrastructure},
} 

@article{Pelley:2010:PRD:1735971.1736047,
 author = {Pelley, Steven and Meisner, David and Zandevakili, Pooya and Wenisch, Thomas F. and Underwood, Jack},
 title = {Power routing: dynamic power provisioning in the data center},
 abstract = {Data center power infrastructure incurs massive capital costs, which typically exceed energy costs over the life of the facility. To squeeze maximum value from the infrastructure, researchers have proposed over-subscribing power circuits, relying on the observation that peak loads are rare. To ensure availability, these proposals employ power capping, which throttles server performance during utilization spikes to enforce safe power budgets. However, because budgets must be enforced locally -- at each power distribution unit (PDU) -- local utilization spikes may force throttling even when power delivery capacity is available elsewhere. Moreover, the need to maintain reserve capacity for fault tolerance on power delivery paths magnifies the impact of utilization spikes. In this paper, we develop mechanisms to better utilize installed power infrastructure, reducing reserve capacity margins and avoiding performance throttling. Unlike conventional high-availability data centers, where collocated servers share identical primary and secondary power feeds, we reorganize power feeds to create shuffled power distribution topologies. Shuffled topologies spread secondary power feeds over numerous PDUs, reducing reserve capacity requirements to tolerate a single PDU failure. Second, we propose Power Routing, which schedules IT load dynamically across redundant power feeds to: (1) shift slack to servers with growing power demands, and (2) balance power draw across AC phases to reduce heating and improve electrical stability. We describe efficient heuristics for scheduling servers to PDUs (an NP-complete problem). Using data collected from nearly 1000 servers in three production facilities, we demonstrate that these mechanisms can reduce the required power infrastructure capacity relative to conventional high-availability data centers by 32\% without performance degradation.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {231--242},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736047},
 doi = {http://doi.acm.org/10.1145/1735971.1736047},
 acmid = {1736047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data centers, power infrastructure},
} 

@article{Ahmad:2010:JOI:1735971.1736048,
 author = {Ahmad, Faraz and Vijaykumar, T. N.},
 title = {Joint optimization of idle and cooling power in data centers while maintaining response time},
 abstract = {Server power and cooling power amount to a significant fraction of modern data centers' recurring costs. While data centers provision enough servers to guarantee response times under the maximum loading, data centers operate under much less loading most of the times (e.g., 30-70\% of the maximum loading). Previous server-power proposals exploit this under-utilization to reduce the server idle power by keeping active only as many servers as necessary and putting the rest into low-power standby modes. However, these proposals incur higher cooling power due to hot spots created by concentrating the data center loading on fewer active servers, or degrade response times due to standby-to-active transition delays, or both. Other proposals optimize the cooling power but incur considerable idle power. To address the first issue of power, we propose PowerTrade</i>, which trades-off idle power and cooling power for each other, thereby reducing the total power. To address the second issue of response time, we propose SurgeGuard</i> to overprovision the number of active servers beyond that needed by the current loading so as to absorb future increases in the loading. SurgeGuard is a two-tier scheme which uses well-known over-provisioning at coarse time granularities (e.g., one hour) to absorb the common, smooth increases in the loading, and a novel fine-grain replenishment of the over-provisioned reserves at fine time granularities (e.g., five minutes) to handle the uncommon, abrupt loading surges. Using real-world traces, we show that combining PowerTrade and SurgeGuard reduces total power by 30\% compared to previous low-power schemes while maintaining response times within 1.7\%.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {243--256},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736048},
 doi = {http://doi.acm.org/10.1145/1735971.1736048},
 acmid = {1736048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooling power, data center, idle power, power management, response time},
} 

@inproceedings{Ahmad:2010:JOI:1736020.1736048,
 author = {Ahmad, Faraz and Vijaykumar, T. N.},
 title = {Joint optimization of idle and cooling power in data centers while maintaining response time},
 abstract = {Server power and cooling power amount to a significant fraction of modern data centers' recurring costs. While data centers provision enough servers to guarantee response times under the maximum loading, data centers operate under much less loading most of the times (e.g., 30-70\% of the maximum loading). Previous server-power proposals exploit this under-utilization to reduce the server idle power by keeping active only as many servers as necessary and putting the rest into low-power standby modes. However, these proposals incur higher cooling power due to hot spots created by concentrating the data center loading on fewer active servers, or degrade response times due to standby-to-active transition delays, or both. Other proposals optimize the cooling power but incur considerable idle power. To address the first issue of power, we propose PowerTrade</i>, which trades-off idle power and cooling power for each other, thereby reducing the total power. To address the second issue of response time, we propose SurgeGuard</i> to overprovision the number of active servers beyond that needed by the current loading so as to absorb future increases in the loading. SurgeGuard is a two-tier scheme which uses well-known over-provisioning at coarse time granularities (e.g., one hour) to absorb the common, smooth increases in the loading, and a novel fine-grain replenishment of the over-provisioned reserves at fine time granularities (e.g., five minutes) to handle the uncommon, abrupt loading surges. Using real-world traces, we show that combining PowerTrade and SurgeGuard reduces total power by 30\% compared to previous low-power schemes while maintaining response times within 1.7\%.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {243--256},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736048},
 doi = {http://doi.acm.org/10.1145/1736020.1736048},
 acmid = {1736048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooling power, data center, idle power, power management, response time},
} 

@article{Ahmad:2010:JOI:1735970.1736048,
 author = {Ahmad, Faraz and Vijaykumar, T. N.},
 title = {Joint optimization of idle and cooling power in data centers while maintaining response time},
 abstract = {Server power and cooling power amount to a significant fraction of modern data centers' recurring costs. While data centers provision enough servers to guarantee response times under the maximum loading, data centers operate under much less loading most of the times (e.g., 30-70\% of the maximum loading). Previous server-power proposals exploit this under-utilization to reduce the server idle power by keeping active only as many servers as necessary and putting the rest into low-power standby modes. However, these proposals incur higher cooling power due to hot spots created by concentrating the data center loading on fewer active servers, or degrade response times due to standby-to-active transition delays, or both. Other proposals optimize the cooling power but incur considerable idle power. To address the first issue of power, we propose PowerTrade</i>, which trades-off idle power and cooling power for each other, thereby reducing the total power. To address the second issue of response time, we propose SurgeGuard</i> to overprovision the number of active servers beyond that needed by the current loading so as to absorb future increases in the loading. SurgeGuard is a two-tier scheme which uses well-known over-provisioning at coarse time granularities (e.g., one hour) to absorb the common, smooth increases in the loading, and a novel fine-grain replenishment of the over-provisioned reserves at fine time granularities (e.g., five minutes) to handle the uncommon, abrupt loading surges. Using real-world traces, we show that combining PowerTrade and SurgeGuard reduces total power by 30\% compared to previous low-power schemes while maintaining response times within 1.7\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {243--256},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736048},
 doi = {http://doi.acm.org/10.1145/1735970.1736048},
 acmid = {1736048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooling power, data center, idle power, power management, response time},
} 

@article{Goodstein:2010:BAA:1735970.1736050,
 author = {Goodstein, Michelle L. and Vlachos, Evangelos and Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
 title = {Butterfly analysis: adapting dataflow analysis to dynamic parallel monitoring},
 abstract = {Online program monitoring is an effective technique for detecting bugs and security attacks in running applications. Extending these tools to monitor parallel programs is challenging because the tools must account for inter-thread dependences and relaxed memory consistency models. Existing tools assume sequential consistency and often slow down the monitored program by orders of magnitude. In this paper, we present a novel approach that avoids these pitfalls by not relying on strong consistency models or detailed inter-thread dependence tracking. Instead, we only assume that events in the distant past on all threads have become visible; we make no assumptions on (and avoid the overheads of tracking) the relative ordering of more recent events on other threads. To overcome the potential state explosion of considering all the possible orderings among recent events, we adapt two techniques from static dataflow analysis, reaching definitions and reaching expressions, to this new domain of dynamic parallel monitoring. Significant modifications to these techniques are proposed to ensure the correctness and efficiency of our approach. We show how our adapted analysis can be used in two popular memory and security tools. We prove that our approach does not miss errors, and sacrifices precision only due to the lack of a relative ordering among recent events. Moreover, our simulation study on a collection of Splash-2 and Parsec 2.0 benchmarks running a memory-checking tool on a hardware-assisted logging platform demonstrates the potential benefits in trading off a very low false positive rate for (i) reduced overhead and (ii) the ability to run on relaxed consistency models.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {257--270},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736050},
 doi = {http://doi.acm.org/10.1145/1735970.1736050},
 acmid = {1736050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, dynamic program monitoring, parallel programming, static analysis},
} 

@article{Goodstein:2010:BAA:1735971.1736050,
 author = {Goodstein, Michelle L. and Vlachos, Evangelos and Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
 title = {Butterfly analysis: adapting dataflow analysis to dynamic parallel monitoring},
 abstract = {Online program monitoring is an effective technique for detecting bugs and security attacks in running applications. Extending these tools to monitor parallel programs is challenging because the tools must account for inter-thread dependences and relaxed memory consistency models. Existing tools assume sequential consistency and often slow down the monitored program by orders of magnitude. In this paper, we present a novel approach that avoids these pitfalls by not relying on strong consistency models or detailed inter-thread dependence tracking. Instead, we only assume that events in the distant past on all threads have become visible; we make no assumptions on (and avoid the overheads of tracking) the relative ordering of more recent events on other threads. To overcome the potential state explosion of considering all the possible orderings among recent events, we adapt two techniques from static dataflow analysis, reaching definitions and reaching expressions, to this new domain of dynamic parallel monitoring. Significant modifications to these techniques are proposed to ensure the correctness and efficiency of our approach. We show how our adapted analysis can be used in two popular memory and security tools. We prove that our approach does not miss errors, and sacrifices precision only due to the lack of a relative ordering among recent events. Moreover, our simulation study on a collection of Splash-2 and Parsec 2.0 benchmarks running a memory-checking tool on a hardware-assisted logging platform demonstrates the potential benefits in trading off a very low false positive rate for (i) reduced overhead and (ii) the ability to run on relaxed consistency models.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {257--270},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736050},
 doi = {http://doi.acm.org/10.1145/1735971.1736050},
 acmid = {1736050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, dynamic program monitoring, parallel programming, static analysis},
} 

@inproceedings{Goodstein:2010:BAA:1736020.1736050,
 author = {Goodstein, Michelle L. and Vlachos, Evangelos and Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
 title = {Butterfly analysis: adapting dataflow analysis to dynamic parallel monitoring},
 abstract = {Online program monitoring is an effective technique for detecting bugs and security attacks in running applications. Extending these tools to monitor parallel programs is challenging because the tools must account for inter-thread dependences and relaxed memory consistency models. Existing tools assume sequential consistency and often slow down the monitored program by orders of magnitude. In this paper, we present a novel approach that avoids these pitfalls by not relying on strong consistency models or detailed inter-thread dependence tracking. Instead, we only assume that events in the distant past on all threads have become visible; we make no assumptions on (and avoid the overheads of tracking) the relative ordering of more recent events on other threads. To overcome the potential state explosion of considering all the possible orderings among recent events, we adapt two techniques from static dataflow analysis, reaching definitions and reaching expressions, to this new domain of dynamic parallel monitoring. Significant modifications to these techniques are proposed to ensure the correctness and efficiency of our approach. We show how our adapted analysis can be used in two popular memory and security tools. We prove that our approach does not miss errors, and sacrifices precision only due to the lack of a relative ordering among recent events. Moreover, our simulation study on a collection of Splash-2 and Parsec 2.0 benchmarks running a memory-checking tool on a hardware-assisted logging platform demonstrates the potential benefits in trading off a very low false positive rate for (i) reduced overhead and (ii) the ability to run on relaxed consistency models.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {257--270},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736050},
 doi = {http://doi.acm.org/10.1145/1736020.1736050},
 acmid = {1736050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow analysis, dynamic program monitoring, parallel programming, static analysis},
} 

@article{Vlachos:2010:PEA:1735970.1736051,
 author = {Vlachos, Evangelos and Goodstein, Michelle L. and Kozuch, Michael A. and Chen, Shimin and Falsafi, Babak and Gibbons, Phillip B. and Mowry, Todd C.},
 title = {ParaLog: enabling and accelerating online parallel monitoring of multithreaded applications},
 abstract = {Instruction-grain lifeguards</i> monitor the events of a running application at the level of individual instructions in order to identify and help mitigate application bugs and security exploits. Because such lifeguards impose a 10-100X slowdown on existing platforms, previous studies have proposed hardware designs to accelerate lifeguard processing. However, these accelerators are either tailored to a specific class of lifeguards or suitable only for monitoring singlethreaded programs. We present ParaLog, the first design of a system enabling fast online parallel monitoring of multithreaded parallel applications. ParaLog supports a broad class of software-defined lifeguards. We show how three existing accelerators can be enhanced to support online multithreaded monitoring, dramatically reducing lifeguard overheads. We identify and solve several challenges in monitoring parallel applications and/or parallelizing these accelerators, including (i) enforcing inter-thread data dependences, (ii) dealing with inter-thread effects that are not reflected in coherence traffic, (iii) dealing with unmonitored operating system activity, and (iv) ensuring lifeguards can access shared metadata with negligible synchronization overheads. We present our system design for both Sequentially Consistent and Total Store Ordering processors. We implement and evaluate our design on a 16 core simulated CMP, using benchmarks from SPLASH-2 and PARSEC and two lifeguards: a data-flow tracking lifeguard and a memory-access checker lifeguard. Our results show that (i) our parallel accelerators improve performance by 2-9X and 1.13-3.4X for our two lifeguards, respectively, (ii) we are 5-126X faster than the time-slicing approach required by existing techniques, and (iii) our average overheads for applications with eight threads are 51\% and 28\% for the two lifeguards, respectively.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {271--284},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736051},
 doi = {http://doi.acm.org/10.1145/1735970.1736051},
 acmid = {1736051},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware support for debugging, instruction-grain lifeguards, online parallel monitoring},
} 

@article{Vlachos:2010:PEA:1735971.1736051,
 author = {Vlachos, Evangelos and Goodstein, Michelle L. and Kozuch, Michael A. and Chen, Shimin and Falsafi, Babak and Gibbons, Phillip B. and Mowry, Todd C.},
 title = {ParaLog: enabling and accelerating online parallel monitoring of multithreaded applications},
 abstract = {Instruction-grain lifeguards</i> monitor the events of a running application at the level of individual instructions in order to identify and help mitigate application bugs and security exploits. Because such lifeguards impose a 10-100X slowdown on existing platforms, previous studies have proposed hardware designs to accelerate lifeguard processing. However, these accelerators are either tailored to a specific class of lifeguards or suitable only for monitoring singlethreaded programs. We present ParaLog, the first design of a system enabling fast online parallel monitoring of multithreaded parallel applications. ParaLog supports a broad class of software-defined lifeguards. We show how three existing accelerators can be enhanced to support online multithreaded monitoring, dramatically reducing lifeguard overheads. We identify and solve several challenges in monitoring parallel applications and/or parallelizing these accelerators, including (i) enforcing inter-thread data dependences, (ii) dealing with inter-thread effects that are not reflected in coherence traffic, (iii) dealing with unmonitored operating system activity, and (iv) ensuring lifeguards can access shared metadata with negligible synchronization overheads. We present our system design for both Sequentially Consistent and Total Store Ordering processors. We implement and evaluate our design on a 16 core simulated CMP, using benchmarks from SPLASH-2 and PARSEC and two lifeguards: a data-flow tracking lifeguard and a memory-access checker lifeguard. Our results show that (i) our parallel accelerators improve performance by 2-9X and 1.13-3.4X for our two lifeguards, respectively, (ii) we are 5-126X faster than the time-slicing approach required by existing techniques, and (iii) our average overheads for applications with eight threads are 51\% and 28\% for the two lifeguards, respectively.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {271--284},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736051},
 doi = {http://doi.acm.org/10.1145/1735971.1736051},
 acmid = {1736051},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware support for debugging, instruction-grain lifeguards, online parallel monitoring},
} 

@inproceedings{Vlachos:2010:PEA:1736020.1736051,
 author = {Vlachos, Evangelos and Goodstein, Michelle L. and Kozuch, Michael A. and Chen, Shimin and Falsafi, Babak and Gibbons, Phillip B. and Mowry, Todd C.},
 title = {ParaLog: enabling and accelerating online parallel monitoring of multithreaded applications},
 abstract = {Instruction-grain lifeguards</i> monitor the events of a running application at the level of individual instructions in order to identify and help mitigate application bugs and security exploits. Because such lifeguards impose a 10-100X slowdown on existing platforms, previous studies have proposed hardware designs to accelerate lifeguard processing. However, these accelerators are either tailored to a specific class of lifeguards or suitable only for monitoring singlethreaded programs. We present ParaLog, the first design of a system enabling fast online parallel monitoring of multithreaded parallel applications. ParaLog supports a broad class of software-defined lifeguards. We show how three existing accelerators can be enhanced to support online multithreaded monitoring, dramatically reducing lifeguard overheads. We identify and solve several challenges in monitoring parallel applications and/or parallelizing these accelerators, including (i) enforcing inter-thread data dependences, (ii) dealing with inter-thread effects that are not reflected in coherence traffic, (iii) dealing with unmonitored operating system activity, and (iv) ensuring lifeguards can access shared metadata with negligible synchronization overheads. We present our system design for both Sequentially Consistent and Total Store Ordering processors. We implement and evaluate our design on a 16 core simulated CMP, using benchmarks from SPLASH-2 and PARSEC and two lifeguards: a data-flow tracking lifeguard and a memory-access checker lifeguard. Our results show that (i) our parallel accelerators improve performance by 2-9X and 1.13-3.4X for our two lifeguards, respectively, (ii) we are 5-126X faster than the time-slicing approach required by existing techniques, and (iii) our average overheads for applications with eight threads are 51\% and 28\% for the two lifeguards, respectively.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {271--284},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736051},
 doi = {http://doi.acm.org/10.1145/1736020.1736051},
 acmid = {1736051},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware support for debugging, instruction-grain lifeguards, online parallel monitoring},
} 

@article{Hormati:2010:MMS:1735970.1736053,
 author = {Hormati, Amir H. and Choi, Yoonseo and Woh, Mark and Kudlur, Manjunath and Rabbah, Rodric and Mudge, Trevor and Mahlke, Scott},
 title = {MacroSS: macro-SIMDization of streaming applications},
 abstract = {SIMD (Single Instruction, Multiple Data) engines are an essential part of the processors in various computing markets, from servers to the embedded domain. Although SIMD-enabled architectures have the capability of boosting the performance of many application domains by exploiting data-level parallelism, it is very challenging for compilers and also programmers to identify and transform parts of a program that will benefit from a particular SIMD engine. The focus of this paper is on the problem of SIMDization for the growing application domain of streaming. Streaming applications are an ideal solution for targeting multi-core architectures, such as shared/distributed memory systems, tiled architectures, and single-core systems. Since these architectures, in most cases, provide SIMD acceleration units as well, it is highly beneficial to generate SIMD code from streaming programs. Specifically, we introduce MacroSS, which is capable of performing macro-SIMDization on high-level streaming graphs. Macro-SIMDization uses high-level information such as execution rates of actors and communication patterns between them to transform the graph structure, vectorize actors of a streaming program, and generate intermediate code. We also propose low-overhead architectural modifications that accelerate shuffling of data elements between the scalar and vectorized parts of a streaming program. Our experiments show that MacroSS is capable of generating code that, on average, outperforms scalar code compiled with the current state-of-art auto-vectorizing compilers by 54\%. Using the low-overhead data shuffling hardware, performance is improved by an additional 8\% with less than 1\% area overhead.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {285--296},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736053},
 doi = {http://doi.acm.org/10.1145/1735970.1736053},
 acmid = {1736053},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD architecture, compiler, optimization, streaming},
} 

@article{Hormati:2010:MMS:1735971.1736053,
 author = {Hormati, Amir H. and Choi, Yoonseo and Woh, Mark and Kudlur, Manjunath and Rabbah, Rodric and Mudge, Trevor and Mahlke, Scott},
 title = {MacroSS: macro-SIMDization of streaming applications},
 abstract = {SIMD (Single Instruction, Multiple Data) engines are an essential part of the processors in various computing markets, from servers to the embedded domain. Although SIMD-enabled architectures have the capability of boosting the performance of many application domains by exploiting data-level parallelism, it is very challenging for compilers and also programmers to identify and transform parts of a program that will benefit from a particular SIMD engine. The focus of this paper is on the problem of SIMDization for the growing application domain of streaming. Streaming applications are an ideal solution for targeting multi-core architectures, such as shared/distributed memory systems, tiled architectures, and single-core systems. Since these architectures, in most cases, provide SIMD acceleration units as well, it is highly beneficial to generate SIMD code from streaming programs. Specifically, we introduce MacroSS, which is capable of performing macro-SIMDization on high-level streaming graphs. Macro-SIMDization uses high-level information such as execution rates of actors and communication patterns between them to transform the graph structure, vectorize actors of a streaming program, and generate intermediate code. We also propose low-overhead architectural modifications that accelerate shuffling of data elements between the scalar and vectorized parts of a streaming program. Our experiments show that MacroSS is capable of generating code that, on average, outperforms scalar code compiled with the current state-of-art auto-vectorizing compilers by 54\%. Using the low-overhead data shuffling hardware, performance is improved by an additional 8\% with less than 1\% area overhead.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {285--296},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736053},
 doi = {http://doi.acm.org/10.1145/1735971.1736053},
 acmid = {1736053},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD architecture, compiler, optimization, streaming},
} 

@inproceedings{Hormati:2010:MMS:1736020.1736053,
 author = {Hormati, Amir H. and Choi, Yoonseo and Woh, Mark and Kudlur, Manjunath and Rabbah, Rodric and Mudge, Trevor and Mahlke, Scott},
 title = {MacroSS: macro-SIMDization of streaming applications},
 abstract = {SIMD (Single Instruction, Multiple Data) engines are an essential part of the processors in various computing markets, from servers to the embedded domain. Although SIMD-enabled architectures have the capability of boosting the performance of many application domains by exploiting data-level parallelism, it is very challenging for compilers and also programmers to identify and transform parts of a program that will benefit from a particular SIMD engine. The focus of this paper is on the problem of SIMDization for the growing application domain of streaming. Streaming applications are an ideal solution for targeting multi-core architectures, such as shared/distributed memory systems, tiled architectures, and single-core systems. Since these architectures, in most cases, provide SIMD acceleration units as well, it is highly beneficial to generate SIMD code from streaming programs. Specifically, we introduce MacroSS, which is capable of performing macro-SIMDization on high-level streaming graphs. Macro-SIMDization uses high-level information such as execution rates of actors and communication patterns between them to transform the graph structure, vectorize actors of a streaming program, and generate intermediate code. We also propose low-overhead architectural modifications that accelerate shuffling of data elements between the scalar and vectorized parts of a streaming program. Our experiments show that MacroSS is capable of generating code that, on average, outperforms scalar code compiled with the current state-of-art auto-vectorizing compilers by 54\%. Using the low-overhead data shuffling hardware, performance is improved by an additional 8\% with less than 1\% area overhead.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {285--296},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736053},
 doi = {http://doi.acm.org/10.1145/1736020.1736053},
 acmid = {1736053},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD architecture, compiler, optimization, streaming},
} 

@article{Woo:2010:CPD:1735971.1736054,
 author = {Woo, Dong Hyuk and Lee, Hsien-Hsin S.},
 title = {COMPASS: a programmable data prefetcher using idle GPU shaders},
 abstract = {A traditional fixed-function graphics accelerator has evolved into a programmable general-purpose graphics processing unit over the last few years. These powerful computing cores are mainly used for accelerating graphics applications or enabling low-cost scientific computing. To further reduce the cost and form factor, an emerging trend is to integrate GPU along with the memory controllers onto the same die with the processor cores. However, given such a system-on-chip, the GPU, while occupying a substantial part of the silicon, will sit idle and contribute nothing to the overall system performance when running non-graphics workloads or applications lack of data-level parallelism. In this paper, we propose COMPASS, a compute shader-assisted data prefetching scheme, to leverage the GPU resource for improving single-threaded performance on an integrated system. By harnessing the GPU shader cores with very lightweight architectural support, COMPASS can emulate the functionality of a hardware-based prefetcher using the idle GPU and successfully improve the memory performance of single-thread applications. Moreover, thanks to its flexibility and programmability, one can implement the best performing prefetch scheme to improve each specific application as demonstrated in this paper. With COMPASS, we envision that a future application vendor can provide a custom-designed COMPASS shader bundled with its software to be loaded at runtime to optimize the performance. Our simulation results show that COMPASS can improve the single-thread performance of memory-intensive applications by 68\% on average.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {297--310},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736054},
 doi = {http://doi.acm.org/10.1145/1735971.1736054},
 acmid = {1736054},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, compute shader, prefetch},
} 

@article{Woo:2010:CPD:1735970.1736054,
 author = {Woo, Dong Hyuk and Lee, Hsien-Hsin S.},
 title = {COMPASS: a programmable data prefetcher using idle GPU shaders},
 abstract = {A traditional fixed-function graphics accelerator has evolved into a programmable general-purpose graphics processing unit over the last few years. These powerful computing cores are mainly used for accelerating graphics applications or enabling low-cost scientific computing. To further reduce the cost and form factor, an emerging trend is to integrate GPU along with the memory controllers onto the same die with the processor cores. However, given such a system-on-chip, the GPU, while occupying a substantial part of the silicon, will sit idle and contribute nothing to the overall system performance when running non-graphics workloads or applications lack of data-level parallelism. In this paper, we propose COMPASS, a compute shader-assisted data prefetching scheme, to leverage the GPU resource for improving single-threaded performance on an integrated system. By harnessing the GPU shader cores with very lightweight architectural support, COMPASS can emulate the functionality of a hardware-based prefetcher using the idle GPU and successfully improve the memory performance of single-thread applications. Moreover, thanks to its flexibility and programmability, one can implement the best performing prefetch scheme to improve each specific application as demonstrated in this paper. With COMPASS, we envision that a future application vendor can provide a custom-designed COMPASS shader bundled with its software to be loaded at runtime to optimize the performance. Our simulation results show that COMPASS can improve the single-thread performance of memory-intensive applications by 68\% on average.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {297--310},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736054},
 doi = {http://doi.acm.org/10.1145/1735970.1736054},
 acmid = {1736054},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, compute shader, prefetch},
} 

@inproceedings{Woo:2010:CPD:1736020.1736054,
 author = {Woo, Dong Hyuk and Lee, Hsien-Hsin S.},
 title = {COMPASS: a programmable data prefetcher using idle GPU shaders},
 abstract = {A traditional fixed-function graphics accelerator has evolved into a programmable general-purpose graphics processing unit over the last few years. These powerful computing cores are mainly used for accelerating graphics applications or enabling low-cost scientific computing. To further reduce the cost and form factor, an emerging trend is to integrate GPU along with the memory controllers onto the same die with the processor cores. However, given such a system-on-chip, the GPU, while occupying a substantial part of the silicon, will sit idle and contribute nothing to the overall system performance when running non-graphics workloads or applications lack of data-level parallelism. In this paper, we propose COMPASS, a compute shader-assisted data prefetching scheme, to leverage the GPU resource for improving single-threaded performance on an integrated system. By harnessing the GPU shader cores with very lightweight architectural support, COMPASS can emulate the functionality of a hardware-based prefetcher using the idle GPU and successfully improve the memory performance of single-thread applications. Moreover, thanks to its flexibility and programmability, one can implement the best performing prefetch scheme to improve each specific application as demonstrated in this paper. With COMPASS, we envision that a future application vendor can provide a custom-designed COMPASS shader bundled with its software to be loaded at runtime to optimize the performance. Our simulation results show that COMPASS can improve the single-thread performance of memory-intensive applications by 68\% on average.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {297--310},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736054},
 doi = {http://doi.acm.org/10.1145/1736020.1736054},
 acmid = {1736054},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, compute shader, prefetch},
} 

@inproceedings{Sanchez:2010:FAS:1736020.1736055,
 author = {Sanchez, Daniel and Yoo, Richard M. and Kozyrakis, Christos},
 title = {Flexible architectural support for fine-grain scheduling},
 abstract = {To make efficient use of CMPs with tens to hundreds of cores, it is often necessary to exploit fine-grain parallelism. However, managing tasks of a few thousand instructions is particularly challenging, as the runtime must ensure load balance without compromising locality and introducing small overheads. Software-only schedulers can implement various scheduling algorithms that match the characteristics of different applications and programming models, but suffer significant overheads as they synchronize and communicate task information over the deep cache hierarchy of a large-scale CMP. To reduce these costs, hardware-only schedulers like Carbon, which implement task queuing and scheduling in hardware, have been proposed. However, a hardware-only solution fixes the scheduling algorithm and leaves no room for other uses of the custom hardware. This paper presents a combined hardware-software approach to build fine-grain schedulers that retain the flexibility of software schedulers while being as fast and scalable as hardware ones. We propose asynchronous direct messages (ADM), a simple architectural extension that provides direct exchange of asynchronous, short messages between threads in the CMP without going through the memory hierarchy. ADM is sufficient to implement a family of novel, software-mostly schedulers that rely on low-overhead messaging to efficiently coordinate scheduling and transfer task information. These schedulers match and often exceed the performance and scalability of Carbon when using the same scheduling algorithm. When the ADM runtime tailors its scheduling algorithm to application characteristics, it outperforms Carbon by up to 70\%.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736055},
 doi = {http://doi.acm.org/10.1145/1736020.1736055},
 acmid = {1736055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip-multiprocessors, fine-grain scheduling, many-core, messaging, scheduling, work-stealing},
} 

@article{Sanchez:2010:FAS:1735970.1736055,
 author = {Sanchez, Daniel and Yoo, Richard M. and Kozyrakis, Christos},
 title = {Flexible architectural support for fine-grain scheduling},
 abstract = {To make efficient use of CMPs with tens to hundreds of cores, it is often necessary to exploit fine-grain parallelism. However, managing tasks of a few thousand instructions is particularly challenging, as the runtime must ensure load balance without compromising locality and introducing small overheads. Software-only schedulers can implement various scheduling algorithms that match the characteristics of different applications and programming models, but suffer significant overheads as they synchronize and communicate task information over the deep cache hierarchy of a large-scale CMP. To reduce these costs, hardware-only schedulers like Carbon, which implement task queuing and scheduling in hardware, have been proposed. However, a hardware-only solution fixes the scheduling algorithm and leaves no room for other uses of the custom hardware. This paper presents a combined hardware-software approach to build fine-grain schedulers that retain the flexibility of software schedulers while being as fast and scalable as hardware ones. We propose asynchronous direct messages (ADM), a simple architectural extension that provides direct exchange of asynchronous, short messages between threads in the CMP without going through the memory hierarchy. ADM is sufficient to implement a family of novel, software-mostly schedulers that rely on low-overhead messaging to efficiently coordinate scheduling and transfer task information. These schedulers match and often exceed the performance and scalability of Carbon when using the same scheduling algorithm. When the ADM runtime tailors its scheduling algorithm to application characteristics, it outperforms Carbon by up to 70\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736055},
 doi = {http://doi.acm.org/10.1145/1735970.1736055},
 acmid = {1736055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip-multiprocessors, fine-grain scheduling, many-core, messaging, scheduling, work-stealing},
} 

@article{Sanchez:2010:FAS:1735971.1736055,
 author = {Sanchez, Daniel and Yoo, Richard M. and Kozyrakis, Christos},
 title = {Flexible architectural support for fine-grain scheduling},
 abstract = {To make efficient use of CMPs with tens to hundreds of cores, it is often necessary to exploit fine-grain parallelism. However, managing tasks of a few thousand instructions is particularly challenging, as the runtime must ensure load balance without compromising locality and introducing small overheads. Software-only schedulers can implement various scheduling algorithms that match the characteristics of different applications and programming models, but suffer significant overheads as they synchronize and communicate task information over the deep cache hierarchy of a large-scale CMP. To reduce these costs, hardware-only schedulers like Carbon, which implement task queuing and scheduling in hardware, have been proposed. However, a hardware-only solution fixes the scheduling algorithm and leaves no room for other uses of the custom hardware. This paper presents a combined hardware-software approach to build fine-grain schedulers that retain the flexibility of software schedulers while being as fast and scalable as hardware ones. We propose asynchronous direct messages (ADM), a simple architectural extension that provides direct exchange of asynchronous, short messages between threads in the CMP without going through the memory hierarchy. ADM is sufficient to implement a family of novel, software-mostly schedulers that rely on low-overhead messaging to efficiently coordinate scheduling and transfer task information. These schedulers match and often exceed the performance and scalability of Carbon when using the same scheduling algorithm. When the ADM runtime tailors its scheduling algorithm to application characteristics, it outperforms Carbon by up to 70\%.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736055},
 doi = {http://doi.acm.org/10.1145/1735971.1736055},
 acmid = {1736055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip-multiprocessors, fine-grain scheduling, many-core, messaging, scheduling, work-stealing},
} 

@article{Romanescu:2010:SDV:1735971.1736057,
 author = {Romanescu, Bogdan F. and Lebeck, Alvin R. and Sorin, Daniel J.},
 title = {Specifying and dynamically verifying address translation-aware memory consistency},
 abstract = {Computer systems with virtual memory are susceptible to design bugs and runtime faults in their address translation (AT) systems. Detecting bugs and faults requires a clear specification of correct behavior. To address this need, we develop a framework for AT-aware memory consistency models. We expand and divide memory consistency into the physical address memory consistency (PAMC) model that defines the behavior of operations on physical addresses and the virtual address memory consistency (VAMC) model that defines the behavior of operations on virtual addresses. As part of this expansion, we show what AT features are required to bridge the gap between PAMC and VAMC. Based on our AT-aware memory consistency specifications, we design efficient dynamic verification hardware that can detect violations of VAMC and thus detect the effects of design bugs and runtime faults, including most AT related bugs in published errata.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {323--334},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736057},
 doi = {http://doi.acm.org/10.1145/1735971.1736057},
 acmid = {1736057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address translation, dynamic verification, memory consistency, virtual memory},
} 

@inproceedings{Romanescu:2010:SDV:1736020.1736057,
 author = {Romanescu, Bogdan F. and Lebeck, Alvin R. and Sorin, Daniel J.},
 title = {Specifying and dynamically verifying address translation-aware memory consistency},
 abstract = {Computer systems with virtual memory are susceptible to design bugs and runtime faults in their address translation (AT) systems. Detecting bugs and faults requires a clear specification of correct behavior. To address this need, we develop a framework for AT-aware memory consistency models. We expand and divide memory consistency into the physical address memory consistency (PAMC) model that defines the behavior of operations on physical addresses and the virtual address memory consistency (VAMC) model that defines the behavior of operations on virtual addresses. As part of this expansion, we show what AT features are required to bridge the gap between PAMC and VAMC. Based on our AT-aware memory consistency specifications, we design efficient dynamic verification hardware that can detect violations of VAMC and thus detect the effects of design bugs and runtime faults, including most AT related bugs in published errata.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {323--334},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736057},
 doi = {http://doi.acm.org/10.1145/1736020.1736057},
 acmid = {1736057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address translation, dynamic verification, memory consistency, virtual memory},
} 

@article{Romanescu:2010:SDV:1735970.1736057,
 author = {Romanescu, Bogdan F. and Lebeck, Alvin R. and Sorin, Daniel J.},
 title = {Specifying and dynamically verifying address translation-aware memory consistency},
 abstract = {Computer systems with virtual memory are susceptible to design bugs and runtime faults in their address translation (AT) systems. Detecting bugs and faults requires a clear specification of correct behavior. To address this need, we develop a framework for AT-aware memory consistency models. We expand and divide memory consistency into the physical address memory consistency (PAMC) model that defines the behavior of operations on physical addresses and the virtual address memory consistency (VAMC) model that defines the behavior of operations on virtual addresses. As part of this expansion, we show what AT features are required to bridge the gap between PAMC and VAMC. Based on our AT-aware memory consistency specifications, we design efficient dynamic verification hardware that can detect violations of VAMC and thus detect the effects of design bugs and runtime faults, including most AT related bugs in published errata.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {323--334},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736057},
 doi = {http://doi.acm.org/10.1145/1735970.1736057},
 acmid = {1736057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address translation, dynamic verification, memory consistency, virtual memory},
} 

@inproceedings{Ebrahimi:2010:FVS:1736020.1736058,
 author = {Ebrahimi, Eiman and Lee, Chang Joo and Mutlu, Onur and Patt, Yale N.},
 title = {Fairness via source throttling: a configurable and high-performance fairness substrate for multi-core memory systems},
 abstract = {Cores in a chip-multiprocessor (CMP) system share multiple hardware resources in the memory subsystem. If resource sharing is unfair, some applications can be delayed significantly while others are unfairly prioritized. Previous research proposed separate fairness mechanisms in each individual resource. Such resource-based fairness mechanisms implemented independently in each resource can make contradictory decisions, leading to low fairness and loss of performance. Therefore, a coordinated mechanism that provides fairness in the entire shared memory system is desirable. This paper proposes a new approach that provides fairness in the entire shared memory system</i>, thereby eliminating the need for and complexity of developing fairness mechanisms for each individual resource. Our technique, Fairness via Source Throttling (FST), estimates the unfairness in the entire shared memory system. If the estimated unfairness is above a threshold set by system software, FST throttles down cores causing unfairness by limiting the number of requests they can inject into the system and the frequency at which they do. As such, our source-based</i> fairness control ensures fairness decisions are made in tandem in the entire memory system. FST also enforces thread priorities/weights, and enables system software to enforce different fairness objectives and fairness-performance tradeoffs in the memory system. Our evaluations show that FST provides the best system fairness and performance compared to four systems with no fairness control and with state-of-the-art fairness mechanisms implemented in both shared caches and memory controllers.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {335--346},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736058},
 doi = {http://doi.acm.org/10.1145/1736020.1736058},
 acmid = {1736058},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fairness, multi-core systems, shared memory systems, system performance},
} 

@article{Ebrahimi:2010:FVS:1735970.1736058,
 author = {Ebrahimi, Eiman and Lee, Chang Joo and Mutlu, Onur and Patt, Yale N.},
 title = {Fairness via source throttling: a configurable and high-performance fairness substrate for multi-core memory systems},
 abstract = {Cores in a chip-multiprocessor (CMP) system share multiple hardware resources in the memory subsystem. If resource sharing is unfair, some applications can be delayed significantly while others are unfairly prioritized. Previous research proposed separate fairness mechanisms in each individual resource. Such resource-based fairness mechanisms implemented independently in each resource can make contradictory decisions, leading to low fairness and loss of performance. Therefore, a coordinated mechanism that provides fairness in the entire shared memory system is desirable. This paper proposes a new approach that provides fairness in the entire shared memory system</i>, thereby eliminating the need for and complexity of developing fairness mechanisms for each individual resource. Our technique, Fairness via Source Throttling (FST), estimates the unfairness in the entire shared memory system. If the estimated unfairness is above a threshold set by system software, FST throttles down cores causing unfairness by limiting the number of requests they can inject into the system and the frequency at which they do. As such, our source-based</i> fairness control ensures fairness decisions are made in tandem in the entire memory system. FST also enforces thread priorities/weights, and enables system software to enforce different fairness objectives and fairness-performance tradeoffs in the memory system. Our evaluations show that FST provides the best system fairness and performance compared to four systems with no fairness control and with state-of-the-art fairness mechanisms implemented in both shared caches and memory controllers.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {335--346},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736058},
 doi = {http://doi.acm.org/10.1145/1735970.1736058},
 acmid = {1736058},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fairness, multi-core systems, shared memory systems, system performance},
} 

@article{Ebrahimi:2010:FVS:1735971.1736058,
 author = {Ebrahimi, Eiman and Lee, Chang Joo and Mutlu, Onur and Patt, Yale N.},
 title = {Fairness via source throttling: a configurable and high-performance fairness substrate for multi-core memory systems},
 abstract = {Cores in a chip-multiprocessor (CMP) system share multiple hardware resources in the memory subsystem. If resource sharing is unfair, some applications can be delayed significantly while others are unfairly prioritized. Previous research proposed separate fairness mechanisms in each individual resource. Such resource-based fairness mechanisms implemented independently in each resource can make contradictory decisions, leading to low fairness and loss of performance. Therefore, a coordinated mechanism that provides fairness in the entire shared memory system is desirable. This paper proposes a new approach that provides fairness in the entire shared memory system</i>, thereby eliminating the need for and complexity of developing fairness mechanisms for each individual resource. Our technique, Fairness via Source Throttling (FST), estimates the unfairness in the entire shared memory system. If the estimated unfairness is above a threshold set by system software, FST throttles down cores causing unfairness by limiting the number of requests they can inject into the system and the frequency at which they do. As such, our source-based</i> fairness control ensures fairness decisions are made in tandem in the entire memory system. FST also enforces thread priorities/weights, and enables system software to enforce different fairness objectives and fairness-performance tradeoffs in the memory system. Our evaluations show that FST provides the best system fairness and performance compared to four systems with no fairness control and with state-of-the-art fairness mechanisms implemented in both shared caches and memory controllers.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {335--346},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736058},
 doi = {http://doi.acm.org/10.1145/1735971.1736058},
 acmid = {1736058},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fairness, multi-core systems, shared memory systems, system performance},
} 

@inproceedings{Gelado:2010:ADS:1736020.1736059,
 author = {Gelado, Isaac and Stone, John E. and Cabezas, Javier and Patel, Sanjay and Navarro, Nacho and Hwu, Wen-mei W.},
 title = {An asymmetric distributed shared memory model for heterogeneous parallel systems},
 abstract = {Heterogeneous computing combines general purpose CPUs with accelerators to efficiently execute both sequential control-intensive and data-parallel phases of applications. Existing programming models for heterogeneous computing rely on programmers to explicitly manage data transfers between the CPU system memory and accelerator memory. This paper presents a new programming model for heterogeneous computing, called Asymmetric Distributed Shared Memory (ADSM), that maintains a shared logical memory space for CPUs to access objects in the accelerator physical memory but not vice versa. The asymmetry allows light-weight implementations that avoid common pitfalls of symmetrical distributed shared memory systems. ADSM allows programmers to assign data objects to performance critical methods. When a method is selected for accelerator execution, its associated data objects are allocated within the shared logical memory space, which is hosted in the accelerator physical memory and transparently accessible by the methods executed on CPUs. We argue that ADSM reduces programming efforts for heterogeneous computing systems and enhances application portability. We present a software implementation of ADSM, called GMAC, on top of CUDA in a GNU/Linux environment. We show that applications written in ADSM and running on top of GMAC achieve performance comparable to their counterparts using programmer-managed data transfers. This paper presents the GMAC system and evaluates different design choices. We further suggest additional architectural support that will likely allow GMAC to achieve higher application performance than the current CUDA model.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736059},
 doi = {http://doi.acm.org/10.1145/1736020.1736059},
 acmid = {1736059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric distributed shared memory, data-centric programming models, heterogeneous systems},
} 

@article{Gelado:2010:ADS:1735970.1736059,
 author = {Gelado, Isaac and Stone, John E. and Cabezas, Javier and Patel, Sanjay and Navarro, Nacho and Hwu, Wen-mei W.},
 title = {An asymmetric distributed shared memory model for heterogeneous parallel systems},
 abstract = {Heterogeneous computing combines general purpose CPUs with accelerators to efficiently execute both sequential control-intensive and data-parallel phases of applications. Existing programming models for heterogeneous computing rely on programmers to explicitly manage data transfers between the CPU system memory and accelerator memory. This paper presents a new programming model for heterogeneous computing, called Asymmetric Distributed Shared Memory (ADSM), that maintains a shared logical memory space for CPUs to access objects in the accelerator physical memory but not vice versa. The asymmetry allows light-weight implementations that avoid common pitfalls of symmetrical distributed shared memory systems. ADSM allows programmers to assign data objects to performance critical methods. When a method is selected for accelerator execution, its associated data objects are allocated within the shared logical memory space, which is hosted in the accelerator physical memory and transparently accessible by the methods executed on CPUs. We argue that ADSM reduces programming efforts for heterogeneous computing systems and enhances application portability. We present a software implementation of ADSM, called GMAC, on top of CUDA in a GNU/Linux environment. We show that applications written in ADSM and running on top of GMAC achieve performance comparable to their counterparts using programmer-managed data transfers. This paper presents the GMAC system and evaluates different design choices. We further suggest additional architectural support that will likely allow GMAC to achieve higher application performance than the current CUDA model.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736059},
 doi = {http://doi.acm.org/10.1145/1735970.1736059},
 acmid = {1736059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric distributed shared memory, data-centric programming models, heterogeneous systems},
} 

@article{Gelado:2010:ADS:1735971.1736059,
 author = {Gelado, Isaac and Stone, John E. and Cabezas, Javier and Patel, Sanjay and Navarro, Nacho and Hwu, Wen-mei W.},
 title = {An asymmetric distributed shared memory model for heterogeneous parallel systems},
 abstract = {Heterogeneous computing combines general purpose CPUs with accelerators to efficiently execute both sequential control-intensive and data-parallel phases of applications. Existing programming models for heterogeneous computing rely on programmers to explicitly manage data transfers between the CPU system memory and accelerator memory. This paper presents a new programming model for heterogeneous computing, called Asymmetric Distributed Shared Memory (ADSM), that maintains a shared logical memory space for CPUs to access objects in the accelerator physical memory but not vice versa. The asymmetry allows light-weight implementations that avoid common pitfalls of symmetrical distributed shared memory systems. ADSM allows programmers to assign data objects to performance critical methods. When a method is selected for accelerator execution, its associated data objects are allocated within the shared logical memory space, which is hosted in the accelerator physical memory and transparently accessible by the methods executed on CPUs. We argue that ADSM reduces programming efforts for heterogeneous computing systems and enhances application portability. We present a software implementation of ADSM, called GMAC, on top of CUDA in a GNU/Linux environment. We show that applications written in ADSM and running on top of GMAC achieve performance comparable to their counterparts using programmer-managed data transfers. This paper presents the GMAC system and evaluates different design choices. We further suggest additional architectural support that will likely allow GMAC to achieve higher application performance than the current CUDA model.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736059},
 doi = {http://doi.acm.org/10.1145/1735971.1736059},
 acmid = {1736059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric distributed shared memory, data-centric programming models, heterogeneous systems},
} 

@inproceedings{Bhattacharjee:2010:ICT:1736020.1736060,
 author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
 title = {Inter-core cooperative TLB for chip multiprocessors},
 abstract = {Translation Lookaside Buffers (TLBs) are commonly employed in modern processor designs and have considerable impact on overall system performance. A number of past works have studied TLB designs to lower access times and miss rates, specifically for uniprocessors. With the growing dominance of chip multiprocessors (CMPs), it is necessary to examine TLB performance in the context of parallel workloads. This work is the first to present TLB prefetchers that exploit commonality in TLB miss patterns across cores in CMPs. We propose and evaluate two Inter-Core Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness at eliminating TLB misses both individually and together. Our results show these approaches require at most modest hardware and can collectively eliminate 19\% to 90\% of data TLB (D-TLB) misses across the surveyed parallel workloads. We also compare performance improvements across a range of hardware and software implementation possibilities. We find that while a fully-hardware implementation results in average performance improvements of 8-46\% for a range of TLB sizes, a hardware/software approach yields improvements of 4-32\%. Overall, our work shows that TLB prefetchers exploiting inter-core correlations can effectively eliminate TLB misses.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736060},
 doi = {http://doi.acm.org/10.1145/1736020.1736060},
 acmid = {1736060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallelism, prefetching, translation lookaside buffer},
} 

@article{Bhattacharjee:2010:ICT:1735971.1736060,
 author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
 title = {Inter-core cooperative TLB for chip multiprocessors},
 abstract = {Translation Lookaside Buffers (TLBs) are commonly employed in modern processor designs and have considerable impact on overall system performance. A number of past works have studied TLB designs to lower access times and miss rates, specifically for uniprocessors. With the growing dominance of chip multiprocessors (CMPs), it is necessary to examine TLB performance in the context of parallel workloads. This work is the first to present TLB prefetchers that exploit commonality in TLB miss patterns across cores in CMPs. We propose and evaluate two Inter-Core Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness at eliminating TLB misses both individually and together. Our results show these approaches require at most modest hardware and can collectively eliminate 19\% to 90\% of data TLB (D-TLB) misses across the surveyed parallel workloads. We also compare performance improvements across a range of hardware and software implementation possibilities. We find that while a fully-hardware implementation results in average performance improvements of 8-46\% for a range of TLB sizes, a hardware/software approach yields improvements of 4-32\%. Overall, our work shows that TLB prefetchers exploiting inter-core correlations can effectively eliminate TLB misses.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736060},
 doi = {http://doi.acm.org/10.1145/1735971.1736060},
 acmid = {1736060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallelism, prefetching, translation lookaside buffer},
} 

@article{Bhattacharjee:2010:ICT:1735970.1736060,
 author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
 title = {Inter-core cooperative TLB for chip multiprocessors},
 abstract = {Translation Lookaside Buffers (TLBs) are commonly employed in modern processor designs and have considerable impact on overall system performance. A number of past works have studied TLB designs to lower access times and miss rates, specifically for uniprocessors. With the growing dominance of chip multiprocessors (CMPs), it is necessary to examine TLB performance in the context of parallel workloads. This work is the first to present TLB prefetchers that exploit commonality in TLB miss patterns across cores in CMPs. We propose and evaluate two Inter-Core Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness at eliminating TLB misses both individually and together. Our results show these approaches require at most modest hardware and can collectively eliminate 19\% to 90\% of data TLB (D-TLB) misses across the surveyed parallel workloads. We also compare performance improvements across a range of hardware and software implementation possibilities. We find that while a fully-hardware implementation results in average performance improvements of 8-46\% for a range of TLB sizes, a hardware/software approach yields improvements of 4-32\%. Overall, our work shows that TLB prefetchers exploiting inter-core correlations can effectively eliminate TLB misses.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736060},
 doi = {http://doi.acm.org/10.1145/1735970.1736060},
 acmid = {1736060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallelism, prefetching, translation lookaside buffer},
} 

@article{Huang:2010:OES:1735970.1736062,
 author = {Huang, Ruirui and Deng, Daniel Y. and Suh, G. Edward},
 title = {Orthrus: efficient software integrity protection on multi-cores},
 abstract = {This paper proposes an efficient hardware/software system that significantly enhances software security through diversified replication on multi-cores. Recent studies show that a large class of software attacks can be detected by running multiple versions of a program simultaneously and checking the consistency of their behaviors. However, execution of multiple replicas incurs significant overheads on today's computing platforms, especially with fine-grained comparisons necessary for high security. Orthrus exploits similarities in automatically generated replicas to enable simultaneous execution of those replicas with minimal overheads; the architecture reduces memory and bandwidth overheads by compressing multiple memory spaces together, and additional power consumption and silicon area by eliminating redundant computations. Utilizing the hardware architecture, Orthrus implements a fine-grained memory layout diversification with the LLVM compiler and can detect corruptions in both pointers and critical data. Experiments indicate that the Orthrus architecture incurs minimal overheads and provides a protection against a broad range of attacks.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {371--384},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735970.1736062},
 doi = {http://doi.acm.org/10.1145/1735970.1736062},
 acmid = {1736062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory protection, multi-core architecture, replication-aware architecture, software diversity and redundancy, software security},
} 

@inproceedings{Huang:2010:OES:1736020.1736062,
 author = {Huang, Ruirui and Deng, Daniel Y. and Suh, G. Edward},
 title = {Orthrus: efficient software integrity protection on multi-cores},
 abstract = {This paper proposes an efficient hardware/software system that significantly enhances software security through diversified replication on multi-cores. Recent studies show that a large class of software attacks can be detected by running multiple versions of a program simultaneously and checking the consistency of their behaviors. However, execution of multiple replicas incurs significant overheads on today's computing platforms, especially with fine-grained comparisons necessary for high security. Orthrus exploits similarities in automatically generated replicas to enable simultaneous execution of those replicas with minimal overheads; the architecture reduces memory and bandwidth overheads by compressing multiple memory spaces together, and additional power consumption and silicon area by eliminating redundant computations. Utilizing the hardware architecture, Orthrus implements a fine-grained memory layout diversification with the LLVM compiler and can detect corruptions in both pointers and critical data. Experiments indicate that the Orthrus architecture incurs minimal overheads and provides a protection against a broad range of attacks.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {371--384},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1736020.1736062},
 doi = {http://doi.acm.org/10.1145/1736020.1736062},
 acmid = {1736062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory protection, multi-core architecture, replication-aware architecture, software diversity and redundancy, software security},
} 

@article{Huang:2010:OES:1735971.1736062,
 author = {Huang, Ruirui and Deng, Daniel Y. and Suh, G. Edward},
 title = {Orthrus: efficient software integrity protection on multi-cores},
 abstract = {This paper proposes an efficient hardware/software system that significantly enhances software security through diversified replication on multi-cores. Recent studies show that a large class of software attacks can be detected by running multiple versions of a program simultaneously and checking the consistency of their behaviors. However, execution of multiple replicas incurs significant overheads on today's computing platforms, especially with fine-grained comparisons necessary for high security. Orthrus exploits similarities in automatically generated replicas to enable simultaneous execution of those replicas with minimal overheads; the architecture reduces memory and bandwidth overheads by compressing multiple memory spaces together, and additional power consumption and silicon area by eliminating redundant computations. Utilizing the hardware architecture, Orthrus implements a fine-grained memory layout diversification with the LLVM compiler and can detect corruptions in both pointers and critical data. Experiments indicate that the Orthrus architecture incurs minimal overheads and provides a protection against a broad range of attacks.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {371--384},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1735971.1736062},
 doi = {http://doi.acm.org/10.1145/1735971.1736062},
 acmid = {1736062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory protection, multi-core architecture, replication-aware architecture, software diversity and redundancy, software security},
} 

@inproceedings{Feng:2010:SPS:1736020.1736063,
 author = {Feng, Shuguang and Gupta, Shantanu and Ansari, Amin and Mahlke, Scott},
 title = {Shoestring: probabilistic soft error reliability on the cheap},
 abstract = {Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with "shoestring" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9\% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6\%. This reliability improvement comes at a modest performance overhead of 15.8\%.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {385--396},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736063},
 doi = {http://doi.acm.org/10.1145/1736020.1736063},
 acmid = {1736063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler analysis, error detection, fault injection},
} 

@article{Feng:2010:SPS:1735971.1736063,
 author = {Feng, Shuguang and Gupta, Shantanu and Ansari, Amin and Mahlke, Scott},
 title = {Shoestring: probabilistic soft error reliability on the cheap},
 abstract = {Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with "shoestring" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9\% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6\%. This reliability improvement comes at a modest performance overhead of 15.8\%.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {385--396},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736063},
 doi = {http://doi.acm.org/10.1145/1735971.1736063},
 acmid = {1736063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler analysis, error detection, fault injection},
} 

@article{Feng:2010:SPS:1735970.1736063,
 author = {Feng, Shuguang and Gupta, Shantanu and Ansari, Amin and Mahlke, Scott},
 title = {Shoestring: probabilistic soft error reliability on the cheap},
 abstract = {Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with "shoestring" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9\% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6\%. This reliability improvement comes at a modest performance overhead of 15.8\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {385--396},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736063},
 doi = {http://doi.acm.org/10.1145/1735970.1736063},
 acmid = {1736063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler analysis, error detection, fault injection},
} 

@article{Yoon:2010:VFE:1735971.1736064,
 author = {Yoon, Doe Hyun and Erez, Mattan},
 title = {Virtualized and flexible ECC for main memory},
 abstract = {We present a general scheme for virtualizing main memory error-correction mechanisms, which map redundant information needed to correct errors into the memory namespace itself. We rely on this basic idea, which increases flexibility to increase error protection capabilities, improve power efficiency, and reduce system cost; with only small performance overheads. We augment the virtual memory system architecture to detach the physical mapping of data from the physical mapping of its associated ECC information. We then use this mechanism to develop two-tiered error protection techniques that separate the process of detecting errors from the rare need to also correct errors, and thus save energy. We describe how to provide strong chipkill and double-chip kill protection using existing DRAM and packaging technology. We show how to maintain access granularity and redundancy overheads, even when using x8 DRAM chips. We also evaluate error correction for systems that do not use ECC DIMMs. Overall, analysis of demanding SPEC CPU 2006 and PARSEC benchmarks indicates that performance overhead is only 1\% with ECC DIMMs and less than 10\% using standard Non-ECC DIMM configurations, that DRAM power savings can be as high as 27\%, and that the system energy-delay product is improved by 12\% on average.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {3},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {397--408},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735971.1736064},
 doi = {http://doi.acm.org/10.1145/1735971.1736064},
 acmid = {1736064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error correction, fault tolerance, memory systems, reliability},
} 

@article{Yoon:2010:VFE:1735970.1736064,
 author = {Yoon, Doe Hyun and Erez, Mattan},
 title = {Virtualized and flexible ECC for main memory},
 abstract = {We present a general scheme for virtualizing main memory error-correction mechanisms, which map redundant information needed to correct errors into the memory namespace itself. We rely on this basic idea, which increases flexibility to increase error protection capabilities, improve power efficiency, and reduce system cost; with only small performance overheads. We augment the virtual memory system architecture to detach the physical mapping of data from the physical mapping of its associated ECC information. We then use this mechanism to develop two-tiered error protection techniques that separate the process of detecting errors from the rare need to also correct errors, and thus save energy. We describe how to provide strong chipkill and double-chip kill protection using existing DRAM and packaging technology. We show how to maintain access granularity and redundancy overheads, even when using x8 DRAM chips. We also evaluate error correction for systems that do not use ECC DIMMs. Overall, analysis of demanding SPEC CPU 2006 and PARSEC benchmarks indicates that performance overhead is only 1\% with ECC DIMMs and less than 10\% using standard Non-ECC DIMM configurations, that DRAM power savings can be as high as 27\%, and that the system energy-delay product is improved by 12\% on average.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {38},
 issue = {1},
 month = {March},
 year = {2010},
 issn = {0163-5964},
 pages = {397--408},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735970.1736064},
 doi = {http://doi.acm.org/10.1145/1735970.1736064},
 acmid = {1736064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error correction, fault tolerance, memory systems, reliability},
} 

@inproceedings{Yoon:2010:VFE:1736020.1736064,
 author = {Yoon, Doe Hyun and Erez, Mattan},
 title = {Virtualized and flexible ECC for main memory},
 abstract = {We present a general scheme for virtualizing main memory error-correction mechanisms, which map redundant information needed to correct errors into the memory namespace itself. We rely on this basic idea, which increases flexibility to increase error protection capabilities, improve power efficiency, and reduce system cost; with only small performance overheads. We augment the virtual memory system architecture to detach the physical mapping of data from the physical mapping of its associated ECC information. We then use this mechanism to develop two-tiered error protection techniques that separate the process of detecting errors from the rare need to also correct errors, and thus save energy. We describe how to provide strong chipkill and double-chip kill protection using existing DRAM and packaging technology. We show how to maintain access granularity and redundancy overheads, even when using x8 DRAM chips. We also evaluate error correction for systems that do not use ECC DIMMs. Overall, analysis of demanding SPEC CPU 2006 and PARSEC benchmarks indicates that performance overhead is only 1\% with ECC DIMMs and less than 10\% using standard Non-ECC DIMM configurations, that DRAM power savings can be as high as 27\%, and that the system energy-delay product is improved by 12\% on average.},
 booktitle = {Proceedings of the fifteenth edition of ASPLOS on Architectural support for programming languages and operating systems},
 series = {ASPLOS '10},
 year = {2010},
 isbn = {978-1-60558-839-1},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {397--408},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1736020.1736064},
 doi = {http://doi.acm.org/10.1145/1736020.1736064},
 acmid = {1736064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error correction, fault tolerance, memory systems, reliability},
} 

@inproceedings{Larus:2011:CCE:1950365.1950367,
 author = {Larus, James R.},
 title = {The cloud will change everything},
 abstract = {Cloud computing is fast on its way to becoming a meaningless, oversold marketing slogan. In the midst of this hype, it is easy to overlook the fundamental change that is occurring. Computation, which used to be confined to the machine beside your desk, is increasingly centralized in vast shared facilities and at the same time liberated by battery-powered, wireless devices. Performance, security, and reliability are no longer problems that can be considered in isolation -- the wires and software connecting pieces offer more challenges and opportunities than components themselves. The eXtreme Computing Group (XCG) in Microsoft Research is taking a holistic approach to research in this area, by bring together researchers and developers with expertise in data center design, computer architecture, operating systems, computer security, programming language, mobile computation, and user interfaces to tackle the challenges of cloud computing.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1950365.1950367},
 doi = {http://doi.acm.org/10.1145/1950365.1950367},
 acmid = {1950367},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing},
} 

@inproceedings{Yuan:2011:ISD:1950365.1950369,
 author = {Yuan, Ding and Zheng, Jing and Park, Soyeon and Zhou, Yuanyuan and Savage, Stefan},
 title = {Improving software diagnosability via log enhancement},
 abstract = {Diagnosing software failures in the field is notoriously difficult, in part due to the fundamental complexity of trouble-shooting any complex software system, but further exacerbated by the paucity of information that is typically available in the production setting. Indeed, for reasons of both overhead and privacy, it is common that only the run-time log generated by a system (e.g., syslog) can be shared with the developers. Unfortunately, the ad-hoc nature of such reports are frequently insufficient for detailed failure diagnosis. This paper seeks to improve this situation within the rubric of existing practice. We describe a tool, LogEnhancer that automatically "enhances" existing logging code to aid in future post-failure debugging. We evaluate LogEnhancer on eight large, real-world applications and demonstrate that it can dramatically reduce the set of potential root failure causes that must be considered during diagnosis while imposing negligible overheads.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950369},
 doi = {http://doi.acm.org/10.1145/1950365.1950369},
 acmid = {1950369},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {log, software diagnosability, static analysis},
} 

@inproceedings{Veeraraghavan:2011:DPS:1950365.1950370,
 author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
 title = {DoublePlay: parallelizing sequential logging and replay},
 abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order or values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15\% with two worker threads and 28\% with four threads.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950370},
 doi = {http://doi.acm.org/10.1145/1950365.1950370},
 acmid = {1950370},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deterministic replay, uniparallelism},
} 

@inproceedings{Casper:2011:HAT:1950365.1950372,
 author = {Casper, Jared and Oguntebi, Tayo and Hong, Sungpack and Bronson, Nathan G. and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Hardware acceleration of transactional memory on commodity systems},
 abstract = {The adoption of transactional memory is hindered by the high overhead of software transactional memory and the intrusive design changes required by previously proposed TM hardware. We propose that hardware to accelerate software transactional memory (STM) can reside outside an unmodified commodity processor core, thereby substantially reducing implementation costs. This paper introduces Transactional Memory Acceleration using Commodity Cores (TMACC), a hardware-accelerated TM system that does not modify the processor, caches, or coherence protocol. We present a complete hardware implementation of TMACC using a rapid prototyping platform. Using this hardware, we implement two unique conflict detection schemes which are accelerated using Bloom filters on an FPGA. These schemes employ novel techniques for tolerating the latency of fine-grained asynchronous communication with an out-of-core accelerator. We then conduct experiments to explore the feasibility of accelerating TM without modifying existing system hardware. We show that, for all but short transactions, it is not necessary to modify the processor to obtain substantial improvement in TM performance. In these cases, TMACC outperforms an STM by an average of 69\% in applications using moderate-length transactions, showing maximum speedup within 8\% of an upper bound on TM acceleration. Overall, we demonstrate that hardware can substantially accelerate the performance of an STM on unmodified commodity processors.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {27--38},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950372},
 doi = {http://doi.acm.org/10.1145/1950365.1950372},
 acmid = {1950372},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fpga, hardware acceleration, transactional memory},
} 

@inproceedings{Dalessandro:2011:HNC:1950365.1950373,
 author = {Dalessandro, Luke and Carouge, Fran\c{c}ois and White, Sean and Lev, Yossi and Moir, Mark and Scott, Michael L. and Spear, Michael F.},
 title = {Hybrid NOrec: a case study in the effectiveness of best effort hardware transactional memory},
 abstract = {Transactional memory (TM) is a promising synchronization mechanism for the next generation of multicore processors. Best-effort Hardware Transactional Memory (HTM) designs, such as Sun's prototype Rock processor and AMD's proposed Advanced Synchronization Facility (ASF), can efficiently execute many transactions, but abort in some cases due to various limitations. Hybrid TM systems can use a compatible software TM (STM) in such cases. We introduce a family of hybrid TMs built using the recent NOrec STM algorithm that, unlike existing hybrid approaches, provide both low overhead on hardware transactions and concurrent execution of hardware and software transactions. We evaluate implementations for Rock and ASF, exploring how the differing HTM designs affect optimization choices. Our investigation yields valuable input for designers of future best-effort HTMs.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {39--52},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950373},
 doi = {http://doi.acm.org/10.1145/1950365.1950373},
 acmid = {1950373},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {transactional memory},
} 

@inproceedings{Singh:2011:EPS:1950365.1950375,
 author = {Singh, Abhayendra and Marino, Daniel and Narayanasamy, Satish and Millstein, Todd and Musuvathi, Madan},
 title = {Efficient processor support for DRFx, a memory model with exceptions},
 abstract = {A longstanding challenge of shared-memory concurrency is to provide a memory model that allows for efficient implementation while providing strong and simple guarantees to programmers. The C++0x and Java memory models admit a wide variety of compiler and hardwareoptimizations and provide sequentially consistent (SC) semantics for data-race-free programs. However, they either do not provide any semantics (C++0x) or provide a hard-to-understand semantics (Java) for racy programs, compromising the safety and debuggability of such programs. In earlier work we proposed the DRFx memory model, which addresses this problem by dynamically detecting potential violations of SC due to the interaction of compiler or hardware optimizations with data races and halting execution upon detection. In this paper, we present a detailed micro-architecture design for supporting the DRFx memory model, formalize the design and prove its correctness, and evaluate the design using a hardware simulator. We describe a set of DRFx-compliant complexity-effective optimizations which allow us to attain performance close to that of TSO (Total Store Model) and DRF0 while providing strong guarantees for all programs.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {53--66},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950375},
 doi = {http://doi.acm.org/10.1145/1950365.1950375},
 acmid = {1950375},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-races, memory model exception, memory models, soft fences},
} 

@inproceedings{Devietti:2011:RRC:1950365.1950376,
 author = {Devietti, Joseph and Nelson, Jacob and Bergan, Tom and Ceze, Luis and Grossman, Dan},
 title = {RCDC: a relaxed consistency deterministic computer},
 abstract = {Providing deterministic execution significantly simplifies the debugging, testing, replication, and deployment of multithreaded programs. Recent work has developed deterministic multiprocessor architectures as well as compiler and runtime systems that enforce determinism in current hardware. Such work has incidentally imposed strong memory-ordering properties. Historically, memory ordering has been relaxed in favor of higher performance in shared memory multiprocessors and, interestingly, determinism exacerbates the cost of strong memory ordering. Consequently, we argue that relaxed memory ordering is vital to achieving faster deterministic execution. This paper introduces RCDC, a deterministic multiprocessor architecture that takes advantage of relaxed memory orderings to provide high-performance deterministic execution with low hardware complexity. RCDC has two key innovations: a hybrid HW/SW approach to enforcing determinism; and a new deterministic execution strategy that leverages data-race-free-based memory models (e.g., the models for Java and C++) to improve performance and scalability without sacrificing determinism, even in the presence of races. In our hybrid HW/SW approach, the only hardware mechanisms required are software-controlled store buffering and support for precise instruction counting; we do not require speculation. A runtime system uses these mechanisms to enforce determinism for arbitrary programs. We evaluate RCDC using PARSEC benchmarks and show that relaxing memory ordering leads to performance and scalability close to nondeterministic execution without requiring any form of speculation. We also compare our new execution strategy to one based on TSO (total-store-ordering) and show that some applications benefit significantly from the extra relaxation. We also evaluate a software-only implementation of our new deterministic execution strategy.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {67--78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950376},
 doi = {http://doi.acm.org/10.1145/1950365.1950376},
 acmid = {1950376},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {determinism, multicore, parallel programming, relaxed consistency},
} 

@inproceedings{Burnim:2011:SCS:1950365.1950377,
 author = {Burnim, Jacob and Necula, George and Sen, Koushik},
 title = {Specifying and checking semantic atomicity for multithreaded programs},
 abstract = {In practice, it is quite difficult to write correct multithreaded programs due to the potential for unintended and nondeterministic interference between parallel threads. A fundamental correctness property for such programs is atomicity---a block of code in a program is atomic if, for any parallel execution of the program, there is an execution with the same overall program behavior in which the block is executed serially. We propose semantic atomicity, a generalization of atomicity with respect to a programmer-defined notion of equivalent behavior. We propose an assertion framework in which a programmer can use bridge predicates to specify noninterference properties at the level of abstraction of their application. Further, we propose a novel algorithm for systematically testing atomicity specifications on parallel executions with a bounded number of interruptions---i.e. atomic blocks whose execution is interleaved with that of other threads. We further propose a set of sound heuristics and optional user annotations that increase the efficiency of checking atomicity specifications in the common case where the specifications hold. We have implemented our assertion framework for specifying and checking semantic atomicity for parallel Java programs, and we have written semantic atomicity specifications for a number of benchmarks. We found that using bridge predicates allowed us to specify the natural and intended atomic behavior of a wider range of programs than did previous approaches. Further, in checking our specifications, we found several previously unknown bugs, including in the widely-used java.util.concurrent library.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {79--90},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950377},
 doi = {http://doi.acm.org/10.1145/1950365.1950377},
 acmid = {1950377},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity, concurrency, linearizability},
} 

@inproceedings{Volos:2011:MLP:1950365.1950379,
 author = {Volos, Haris and Tack, Andres Jaan and Swift, Michael M.},
 title = {Mnemosyne: lightweight persistent memory},
 abstract = {New storage-class memory (SCM) technologies, such as phase-change memory, STT-RAM, and memristors, promise user-level access to non-volatile storage through regular memory instructions. These memory devices enable fast user-mode access to persistence, allowing regular in-memory data structures to survive system crashes. In this paper, we present Mnemosyne, a simple interface for programming with persistent memory. Mnemosyne addresses two challenges: how to create and manage such memory, and how to ensure consistency in the presence of failures. Without additional mechanisms, a system failure may leave data structures in SCM in an invalid state, crashing the program the next time it starts. In Mnemosyne, programmers declare global persistent data with the keyword "pstatic" or allocate it dynamically. Mnemosyne provides primitives for directly modifying persistent variables and supports consistent updates through a lightweight transaction mechanism. Compared to past work on disk-based persistent memory, Mnemosyne reduces latency to storage by writing data directly to memory at the granularity of an update rather than writing memory pages back to disk through the file system. In tests emulating the performance characteristics of forthcoming SCMs, we show that Mnemosyne can persist data as fast as 3 microseconds. Furthermore, it provides a 35 percent performance increase when applied in the OpenLDAP directory server. In microbenchmark studies we find that Mnemosyne can be up to 1400\% faster than alternative persistence strategies, such as Berkeley DB or Boost serialization, that are designed for disks.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {91--104},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950379},
 doi = {http://doi.acm.org/10.1145/1950365.1950379},
 acmid = {1950379},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory transactions, performance, persistence, persistent memory, storage-class memory},
} 

@inproceedings{Coburn:2011:NMP:1950365.1950380,
 author = {Coburn, Joel and Caulfield, Adrian M. and Akel, Ameen and Grupp, Laura M. and Gupta, Rajesh K. and Jhala, Ranjit and Swanson, Steven},
 title = {NV-Heaps: making persistent objects fast and safe with next-generation, non-volatile memories},
 abstract = {Persistent, user-defined objects present an attractive abstraction for working with non-volatile program state. However, the slow speed of persistent storage (i.e., disk) has restricted their design and limited their performance. Fast, byte-addressable, non-volatile technologies, such as phase change memory, will remove this constraint and allow programmers to build high-performance, persistent data structures in non-volatile storage that is almost as fast as DRAM. Creating these data structures requires a system that is lightweight enough to expose the performance of the underlying memories but also ensures safety in the presence of application and system failures by avoiding familiar bugs such as dangling pointers, multiple free()s, and locking errors. In addition, the system must prevent new types of hard-to-find pointer safety bugs that only arise with persistent objects. These bugs are especially dangerous since any corruption they cause will be permanent. We have implemented a lightweight, high-performance persistent object system called NV-heaps that provides transactional semantics while preventing these errors and providing a model for persistence that is easy to use and reason about. We implement search trees, hash tables, sparse graphs, and arrays using NV-heaps, BerkeleyDB, and Stasis. Our results show that NV-heap performance scales with thread count and that data structures implemented using NV-heaps out-perform BerkeleyDB and Stasis implementations by 32x and 244x, respectively, by avoiding the operating system and minimizing other software overheads. We also quantify the cost of enforcing the safety guarantees that NV-heaps provide and measure the costs of NV-heap primitive operations.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {105--118},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950380},
 doi = {http://doi.acm.org/10.1145/1950365.1950380},
 acmid = {1950380},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {acid transactions, memory mangement, non-volatile heap, persistent objects, phase-change memory, pointer safety, spin-torque transfer memory, transactional memory},
} 

@inproceedings{Schupbach:2011:DLA:1950365.1950382,
 author = {Sch\"{u}pbach, Adrian and Baumann, Andrew and Roscoe, Timothy and Peter, Simon},
 title = {A declarative language approach to device configuration},
 abstract = {C remains the language of choice for hardware programming (device drivers, bus configuration, etc.): it is fast, allows low-level access, and is trusted by OS developers. However, the algorithms required to configure and reconfigure hardware devices and interconnects are becoming more complex and diverse, with the added burden of legacy support, quirks, and hardware bugs to work around. Even programming PCI bridges in a modern PC is a surprisingly complex problem, and is getting worse as new functionality such as hotplug appears. Existing approaches use relatively simple algorithms, hard-coded in C and closely coupled with low-level register access code, generally leading to suboptimal configurations. We investigate the merits and drawbacks of a new approach: separating hardware configuration logic (algorithms to determine configuration parameter values) from mechanism (programming device registers). The latter we keep in C, and the former we encode in a declarative programming language with constraint-satisfaction extensions. As a test case, we have implemented full PCI configuration, resource allocation, and interrupt assignment in the Barrelfish research operating system, using a concise expression of efficient algorithms in constraint logic programming. We show that the approach is tractable, and can successfully configure a wide range of PCs with competitive runtime cost. Moreover, it requires about half the code of the C-based approach in Linux while offering considerably more functionality. Additionally it easily accommodates adaptations such as hotplug, fixed regions, and quirks.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {119--132},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950382},
 doi = {http://doi.acm.org/10.1145/1950365.1950382},
 acmid = {1950382},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {constraint logic programming, eclipse clp, hardware programming, pci configuration},
} 

@inproceedings{Ryzhyk:2011:IDD:1950365.1950383,
 author = {Ryzhyk, Leonid and Keys, John and Mirla, Balachandra and Raghunath, Arun and Vij, Mona and Heiser, Gernot},
 title = {Improved device driver reliability through hardware verification reuse},
 abstract = {Faulty device drivers are a major source of operating system failures. We argue that the underlying cause of many driver faults is the separation of two highly-related tasks: device verification and driver development. These two tasks have a lot in common, and result in software that is conceptually and functionally similar, yet kept totally separate. The result is a particularly bad case of duplication of effort: the verification code is correct, but is discarded after the device has been manufactured; the driver code is inferior, but used in actual device operation. We claim that the two tasks, and the software they produce, can and should be unified, and this will result in drastic improvement of device-driver quality and reduction in the development cost and time to market. In this paper we propose a device driver design and verification workflow that achieves such unification. We apply this workflow to develop and test drivers for four different I/O devices and demonstrate that it improves the driver test coverage and allows detecting driver defects that are extremely hard to find using conventional testing techniques.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950383},
 doi = {http://doi.acm.org/10.1145/1950365.1950383},
 acmid = {1950383},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated testing, co-verification, device drivers, reliability, rtl testbenches},
} 

@inproceedings{Hashmi:2011:CNI:1950365.1950385,
 author = {Hashmi, Atif and Nere, Andrew and Thomas, James Jamal and Lipasti, Mikko},
 title = {A case for neuromorphic ISAs},
 abstract = {The desire to create novel computing systems, paired with recent advances in neuroscientific understanding of the brain, has led researchers to develop neuromorphic architectures that emulate the brain. To date, such models are developed, trained, and deployed on the same substrate. However, excessive co-dependence between the substrate and the algorithm prevents portability, or at the very least requires reconstructing and retraining the model whenever the substrate changes. This paper proposes a well-defined abstraction layer -- the Neuromorphic instruction set architecture, or NISA -- that separates a neural application's algorithmic specification from the underlying execution substrate, and describes the Aivo framework, which demonstrates the concrete advantages of such an abstraction layer. Aivo consists of a NISA implementation for a rate-encoded neuromorphic system based on the cortical column abstraction, a state-of-the-art integrated development and runtime environment (IDE), and various profile-based optimization tools. Aivo's IDE generates code for emulating cortical networks on the host CPU, multiple GPGPUs, or as boolean functions. Its runtime system can deploy and adaptively optimize cortical networks in a manner similar to conventional just-in-time compilers in managed runtime systems (e.g. Java, C#). We demonstrate the abilities of the NISA abstraction by constructing a cortical network model of the mammalian visual cortex, deploying on multiple execution substrates, and utilizing the various optimization tools we have created. For this hierarchical configuration, Aivo's profiling based network optimization tools reduce the memory footprint by 50\% and improve the execution time by a factor of 3x on the host CPU. Deploying the same network on a single GPGPU results in a 30x speedup. We further demonstrate that a speedup of 480x can be achieved by deploying a massively scaled cortical network across three GPGPUs. Finally, converting a trained hierarchical network to C/C++ boolean constructs on the host CPU results in 44x speedup.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {145--158},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950385},
 doi = {http://doi.acm.org/10.1145/1950365.1950385},
 acmid = {1950385},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cortical learning algorithms, gpgpu, neuromorphic archi- tectures},
} 

@inproceedings{Ransford:2011:MSS:1950365.1950386,
 author = {Ransford, Benjamin and Sorber, Jacob and Fu, Kevin},
 title = {Mementos: system support for long-running computation on RFID-scale devices},
 abstract = {Transiently powered computing devices such as RFID tags, kinetic energy harvesters, and smart cards typically rely on programs that complete a task under tight time constraints before energy starvation leads to complete loss of volatile memory. Mementos is a software system that transforms general-purpose programs into interruptible computations that are protected from frequent power losses by automatic, energy-aware state checkpointing. Mementos comprises a collection of optimization passes for the LLVM compiler infrastructure and a linkable library that exercises hardware support for energy measurement while managing state checkpoints stored in nonvolatile memory. We evaluate Mementos against diverse test cases in a trace-driven simulator of transiently powered RFID-scale devices. Although Mementos's energy checks increase run time when energy is plentiful, they allow Mementos to safely suspend execution when energy dwindles, effectively spreading computation across zero or more power failures. This paper's contributions are: a study of the runtime environment for programs on RFID-scale devices; an energy-aware state checkpointing system for these devices that is implemented for the MSP430 family of microcontrollers; and a trace-driven simulator of transiently powered RFID-scale devices.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950386},
 doi = {http://doi.acm.org/10.1145/1950365.1950386},
 acmid = {1950386},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computational rfid, energy-aware checkpointing, mementos, rfid-scale devices},
} 

@inproceedings{Koukoumidis:2011:PC:1950365.1950387,
 author = {Koukoumidis, Emmanouil and Lymberopoulos, Dimitrios and Strauss, Karin and Liu, Jie and Burger, Doug},
 title = {Pocket cloudlets},
 abstract = {Cloud services accessed through mobile devices suffer from high network access latencies and are constrained by energy budgets dictated by the devices' batteries. Radio and battery technologies will improve over time, but are still expected to be the bottlenecks in future systems. Non-volatile memories (NVM), however, may continue experiencing significant and steady improvements in density for at least ten more years. In this paper, we propose to leverage the abundance in memory capacity of mobile devices to mitigate latency and energy issues when accessing cloud services. We first analyze NVM technology scaling trends, and then propose a cloud service cache architecture that resides on the mobile device's NVM (pocket cloudlet). This architecture utilizes both individual user and community access models to maximize its hit rate, and subsequently reduce overall service latency and energy consumption. As a showcase we present the design, implementation and evaluation of PocketSearch, a search and advertisement pocket cloudlet. We perform mobile search characterization to guide the design of PocketSearch and evaluate it with 200 million mobile queries from the search logs of m.bing.com</i>. We show that PocketSearch can serve, on average, 66\% of the web search queries submitted by an individual user without having to use the slow 3G link, leading to 16x service access speedup. Finally, based on experience with PocketSearch we provide additional insight and guidelines on how future pocket cloudlets should be organized, from both an architectural and an operating system perspective.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {171--184},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950387},
 doi = {http://doi.acm.org/10.1145/1950365.1950387},
 acmid = {1950387},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flash storage, mobile cloud, mobile search},
} 

@inproceedings{Sharma:2011:BMS:1950365.1950389,
 author = {Sharma, Navin and Barker, Sean and Irwin, David and Shenoy, Prashant},
 title = {Blink: managing server clusters on intermittent power},
 abstract = {Reducing the energy footprint of data centers continues to receive significant attention due to both its financial and environmental impact. There are numerous methods that limit the impact of both factors, such as expanding the use of renewable energy or participating in automated demand-response programs. To take advantage of these methods, servers and applications must gracefully handle intermittent constraints in their power supply. In this paper, we propose blinking---metered transitions between a high-power active state and a low-power inactive state---as the primary abstraction for conforming to intermittent power constraints. We design Blink, an application-independent hardware-software platform for developing and evaluating blinking applications, and define multiple types of blinking policies. We then use Blink to design BlinkCache, a blinking version of memcached, to demonstrate the effect of blinking on an example application. Our results show that a load-proportional blinking policy combines the advantages of both activation and synchronous blinking for realistic Zipf-like popularity distributions and wind/solar power signals by achieving near optimal hit rates (within 15\% of an activation policy), while also providing fairer access to the cache (within 2\% of a syn- chronous policy) for equally popular objects.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {185--198},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950389},
 doi = {http://doi.acm.org/10.1145/1950365.1950389},
 acmid = {1950389},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blink, intermittent, power, renewable energy},
} 

@inproceedings{Hoffmann:2011:DKR:1950365.1950390,
 author = {Hoffmann, Henry and Sidiroglou, Stelios and Carbin, Michael and Misailovic, Sasa and Agarwal, Anant and Rinard, Martin},
 title = {Dynamic knobs for responsive power-aware computing},
 abstract = {We present PowerDial, a system for dynamically adapting application behavior to execute successfully in the face of load and power fluctuations. PowerDial transforms static configuration parameters into dynamic knobs that the PowerDial control system can manipulate to dynamically trade off the accuracy of the computation in return for reductions in the computational resources that the application requires to produce its results. These reductions translate directly into performance improvements and power savings. Our experimental results show that PowerDial can enable our benchmark applications to execute responsively in the face of power caps that would otherwise significantly impair responsiveness. They also show that PowerDial can significantly reduce the number of machines required to service intermittent load spikes, enabling reductions in power and capital costs.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {199--212},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950390},
 doi = {http://doi.acm.org/10.1145/1950365.1950390},
 acmid = {1950390},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accuracy-aware computing, power-aware computing, self-aware systems},
} 

@inproceedings{Liu:2011:FSD:1950365.1950391,
 author = {Liu, Song and Pattabiraman, Karthik and Moscibroda, Thomas and Zorn, Benjamin G.},
 title = {Flikker: saving DRAM refresh-power through critical data partitioning},
 abstract = {Energy has become a first-class design constraint in computer systems. Memory is a significant contributor to total system power. This paper introduces Flikker, an application-level technique to reduce refresh power in DRAM memories. Flikker enables developers to specify critical and non-critical data in programs and the runtime system allocates this data in separate parts of memory. The portion of memory containing critical data is refreshed at the regular refresh-rate, while the portion containing non-critical data is refreshed at substantially lower rates. This partitioning saves energy at the cost of a modest increase in data corruption in the non-critical data. Flikker thus exposes and leverages an interesting trade-off between energy consumption and hardware correctness. We show that many applications are naturally tolerant to errors in the non-critical data, and in the vast majority of cases, the errors have little or no impact on the application's final outcome. We also find that Flikker can save between 20-25\% of the power consumed by the memory sub-system in a mobile device, with negligible impact on application performance. Flikker is implemented almost entirely in software, and requires only modest changes to the hardware.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {213--224},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950391},
 doi = {http://doi.acm.org/10.1145/1950365.1950391},
 acmid = {1950391},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {allocation, critical data, dram refresh, power-savings, soft errors},
} 

@inproceedings{Deng:2011:MAL:1950365.1950392,
 author = {Deng, Qingyuan and Meisner, David and Ramos, Luiz and Wenisch, Thomas F. and Bianchini, Ricardo},
 title = {MemScale: active low-power modes for main memory},
 abstract = {Main memory is responsible for a large and increasing fraction of the energy consumed by servers. Prior work has focused on exploiting DRAM low-power states to conserve energy. However, these states require entire DRAM ranks to be idled, which is difficult to achieve even in lightly loaded servers. In this paper, we propose to conserve memory energy while improving its energy-proportionality by creating active low-power modes for it. Specifically, we propose MemScale, a scheme wherein we apply dynamic voltage and frequency scaling (DVFS) to the memory controller and dynamic frequency scaling (DFS) to the memory channels and DRAM devices. MemScale is guided by an operating system policy that determines the DVFS/DFS mode of the memory subsystem based on the current need for memory bandwidth, the potential energy savings, and the performance degradation that applications are willing to withstand. Our results demonstrate that MemScale reduces energy consumption significantly compared to modern memory energy management approaches. We conclude that the potential benefits of the MemScale mechanisms and policy more than compensate for their small hardware cost.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {225--238},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950392},
 doi = {http://doi.acm.org/10.1145/1950365.1950392},
 acmid = {1950392},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling, energy conservation, memory subsystem},
} 

@inproceedings{Gao:2011:TMH:1950365.1950394,
 author = {Gao, Qi and Zhang, Wenbin and Chen, Zhezhe and Zheng, Mai and Qin, Feng},
 title = {2ndStrike: toward manifesting hidden concurrency typestate bugs},
 abstract = {Concurrency bugs are becoming increasingly prevalent in the multi-core era. Recently, much research has focused on data races and atomicity violation bugs, which are related to low-level memory accesses. However, a large number of concurrency typestate bugs such as "invalid reads to a closed file from a different thread" are under-studied. These concurrency typestate bugs are important yet challenging to study since they are mostly relevant to high-level program semantics. This paper presents 2ndStrike, a method to manifest hidden concurrency typestate bugs in software testing. Given a state machine describing correct program behavior on certain object typestates, 2ndStrike profiles runtime events related to the typestates and thread synchronization. Based on the profiling results, 2ndStrike then identifies bug candidates, each of which is a pair of runtime events that would cause typestate violation if the event order is reversed. Finally, 2ndStrike re-executes the program with controlled thread interleaving to manifest bug candidates. We have implemented a prototype of 2ndStrike on Linux and have illustrated our idea using three types of concurrency typestate bugs, including invalid file operation, invalid pointer dereference, and invalid lock operation. We have evaluated 2ndStrike with six real world bugs (including one previously unknown bug) from three open-source server and desktop programs (i.e., MySQL, Mozilla, pbzip2). Our experimental results show that 2ndStrike can effectively and efficiently manifest all six software bugs, most of which are difficult or impossible to manifest using stress testing or active testing techniques that are based on data race/atomicity violation. Additionally, 2ndStrike reports no false positives, provides detailed bug reports for each manifested bug, and can consistently reproduce the bug after manifesting it once.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950394},
 doi = {http://doi.acm.org/10.1145/1950365.1950394},
 acmid = {1950394},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, software testing, typestate bugs},
} 

@inproceedings{Zhang:2011:CDC:1950365.1950395,
 author = {Zhang, Wei and Lim, Junghee and Olichandran, Ramya and Scherpelz, Joel and Jin, Guoliang and Lu, Shan and Reps, Thomas},
 title = {ConSeq: detecting concurrency bugs through sequential errors},
 abstract = {Concurrency bugs are caused by non-deterministic interleavings between shared memory accesses. Their effects propagate through data and control dependences until they cause software to crash, hang, produce incorrect output, etc. The lifecycle of a bug thus consists of three phases: (1) triggering, (2) propagation, and (3) failure. Traditional techniques for detecting concurrency bugs mostly focus on phase (1)--i.e., on finding certain structural patterns of interleavings that are common triggers of concurrency bugs, such as data races. This paper explores a consequence-oriented approach to improving the accuracy and coverage of state-space search and bug detection. The proposed approach first statically identifies potential failure sites in a program binary (i.e., it first considers a phase (3) issue). It then uses static slicing to identify critical read instructions that are highly likely to affect potential failure sites through control and data dependences (phase (2)). Finally, it monitors a single (correct) execution of a concurrent program and identifies suspicious interleavings that could cause an incorrect state to arise at a critical read and then lead to a software failure (phase (1)). ConSeq's backwards approach, (3)!(2)!(1), provides advantages in bug-detection coverage and accuracy but is challenging to carry out. ConSeq makes it feasible by exploiting the empirical observationthat phases (2) and (3) usually are short and occur within one thread. Our evaluation on large, real-world C/C++ applications shows that ConSeq detects more bugs than traditional approaches and has a much lower false-positive rate.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {251--264},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950395},
 doi = {http://doi.acm.org/10.1145/1950365.1950395},
 acmid = {1950395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bugs, software testing},
} 

@inproceedings{Chipounov:2011:SPI:1950365.1950396,
 author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
 title = {S2E: a platform for in-vivo multi-path analysis of software systems},
 abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each. S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software. Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {265--278},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950396},
 doi = {http://doi.acm.org/10.1145/1950365.1950396},
 acmid = {1950396},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analysis, binary, consistency models, dbt, framework, in-vivo, performance, symbolic execution, testing, virtualization},
} 

@inproceedings{Hofmann:2011:EOS:1950365.1950398,
 author = {Hofmann, Owen S. and Dunn, Alan M. and Kim, Sangman and Roy, Indrajit and Witchel, Emmett},
 title = {Ensuring operating system kernel integrity with OSck},
 abstract = {Kernel rootkits that modify operating system state to avoid detection are a dangerous threat to system security. This paper presents OSck, a system that discovers kernel rootkits by detecting malicious modifications to operating system data. OSck integrates and extends existing techniques for detecting rootkits, and verifies safety properties for large portions of the kernel heap with minimal overhead. We deduce type information for verification by analyzing unmodified kernel source code and in-memory kernel data structures. High-performance integrity checks that execute concurrently with a running operating system create data races, and we demonstrate a deterministic solution for ensuring kernel memory is in a consistent state. We introduce two new classes of kernel rootkits that are undetectable by current systems, motivating the need for the OSck API that allows kernel developers to conveniently specify arbitrary integrity properties.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950398},
 doi = {http://doi.acm.org/10.1145/1950365.1950398},
 acmid = {1950398},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {rootkit detection},
} 

@inproceedings{Porter:2011:RLO:1950365.1950399,
 author = {Porter, Donald E. and Boyd-Wickizer, Silas and Howell, Jon and Olinsky, Reuben and Hunt, Galen C.},
 title = {Rethinking the library OS from the top down},
 abstract = {This paper revisits an old approach to operating system construc-tion, the library OS, in a new context. The idea of the library OS is that the personality of the OS on which an application depends runs in the address space of the application. A small, fixed set of abstractions connects the library OS to the host OS kernel, offering the promise of better system security and more rapid independent evolution of OS components. We describe a working prototype of a Windows 7 library OS that runs the latest releases of major applications such as Microsoft Excel, PowerPoint, and Internet Explorer. We demonstrate that desktop sharing across independent, securely isolated, library OS instances can be achieved through the pragmatic reuse of net-working protocols. Each instance has significantly lower overhead than a full VM bundled with an application: a typical application adds just 16MB of working set and 64MB of disk footprint. We contribute a new ABI below the library OS that enables application mobility. We also show that our library OS can address many of the current uses of hardware virtual machines at a fraction of the overheads. This paper describes the first working prototype of a full commercial OS redesigned as a library OS capable of running significant applications. Our experience shows that the long-promised benefits of the library OS approach better protection of system integrity and rapid system evolution are readily obtainable.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {291--304},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950399},
 doi = {http://doi.acm.org/10.1145/1950365.1950399},
 acmid = {1950399},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {drawbridge, libos, library os},
} 

@inproceedings{Palix:2011:FLT:1950365.1950401,
 author = {Palix, Nicolas and Thomas, Ga\"{e}l and Saha, Suman and Calv\`{e}s, Christophe and Lawall, Julia and Muller, Gilles},
 title = {Faults in linux: ten years later},
 abstract = {In 2001, Chou et al. published a study of faults found by applying a static analyzer to Linux versions 1.0 through 2.4.1. A major result of their work was that the drivers directory contained up to 7 times more of certain kinds of faults than other directories. This result inspired a number of development and research efforts on improving the reliability of driver code. Today Linux is used in a much wider range of environments, provides a much wider range of services, and has adopted a new development and release model. What has been the impact of these changes on code quality? Are drivers still a major problem? To answer these questions, we have transported the experiments of Chou et al. to Linux versions 2.6.0 to 2.6.33, released between late 2003 and early 2010. We find that Linux has more than doubled in size during this period, but that the number of faults per line of code has been decreasing. And, even though drivers still accounts for a large part of the kernel code and contains the most faults, its fault rate is now below that of other directories, such as arch (HAL) and fs (file systems). These results can guide further development and research efforts. To enable others to continually update these results as Linux evolves, we define our experimental protocol and make our checkers and results available in a public archive.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {305--318},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950401},
 doi = {http://doi.acm.org/10.1145/1950365.1950401},
 acmid = {1950401},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault-finding tools, linux},
} 

@inproceedings{Esmaeilzadeh:2011:LBL:1950365.1950402,
 author = {Esmaeilzadeh, Hadi and Cao, Ting and Xi, Yang and Blackburn, Stephen M. and McKinley, Kathryn S.},
 title = {Looking back on the language and hardware revolutions: measured power, performance, and scaling},
 abstract = {This paper reports and analyzes measured chip power and performance on five process technology generations executing 61 diverse benchmarks with a rigorous methodology. We measure representative Intel IA32 processors with technologies ranging from 130nm to 32nm while they execute sequential and parallel benchmarks written in native and managed languages. During this period, hardware and software changed substantially: (1) hardware vendors delivered chip multiprocessors instead of uniprocessors, and independently (2) software developers increasingly chose managed languages instead of native languages. This quantitative data reveals the extent of some known and previously unobserved hardware and software trends. Two themes emerge. (I) Workload: The power, performance, and energy trends of native workloads do not approximate managed workloads. For example, (a) the SPEC CPU2006 native benchmarks on the i7 (45) and i5 (32) draw significantly less power than managed or scalable native benchmarks; and (b) managed runtimes exploit parallelism even when running single-threaded applications. The results recommend architects always include native and managed workloads when designing and evaluating energy efficient hardware. (II) Architecture: Clock scaling, microarchitecture, simultaneous multithreading, and chip multiprocessors each elicit a huge variety of power, performance, and energy responses. This variety and the difficulty of obtaining power measurements recommends exposing on-chip power meters and when possible structure specific power meters for cores, caches, and other structures. Just as hardware event counters provide a quantitative grounding for performance innovations, power meters are necessary for optimizing energy.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {319--332},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950402},
 doi = {http://doi.acm.org/10.1145/1950365.1950402},
 acmid = {1950402},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy, managed languages, measurement, native languages, performance, power},
} 

@inproceedings{Nguyen:2011:SCS:1950365.1950404,
 author = {Nguyen, Donald and Pingali, Keshav},
 title = {Synthesizing concurrent schedulers for irregular algorithms},
 abstract = {Scheduling is the assignment of tasks or activities to processors for execution, and it is an important concern in parallel programming. Most prior work on scheduling has focused either on static scheduling of applications in which the dependence graph is known at compile-time or on dynamic scheduling of independent loop iterations such as in OpenMP. In irregular algorithms, dependences between activities are complex functions of runtime values so these algorithms are not amenable to compile-time analysis nor do they consist of independent activities. Moreover, the amount of work can vary dramatically with the scheduling policy. To handle these complexities, implementations of irregular algorithms employ carefully handcrafted, algorithm-specific schedulers but these schedulers are themselves parallel programs, complicating the parallel programming problem further. In this paper, we present a flexible and efficient approach for specifying and synthesizing scheduling policies for irregular algorithms. We develop a simple compositional specification language and show how it can concisely encode scheduling policies in the literature. Then, we show how to synthesize efficient parallel schedulers from these specifications. We evaluate our approach for five irregular algorithms on three multicore architectures and show that (1) the performance of some algorithms can improve by orders of magnitude with the right scheduling policy, and (2) for the same policy, the overheads of our synthesized schedulers are comparable to those of fixed-function schedulers.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {333--344},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950404},
 doi = {http://doi.acm.org/10.1145/1950365.1950404},
 acmid = {1950404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {scheduling, synthesis},
} 

@inproceedings{Hoang:2011:ECT:1950365.1950405,
 author = {Hoang, Giang and Findler, Robby Bruce and Joseph, Russ},
 title = {Exploring circuit timing-aware language and compilation},
 abstract = {By adjusting the design of the ISA and enabling circuit timing-sensitive optimizations in a compiler, we can more effectively exploit timing speculation. While there has been growing interest in systems that leverage circuit-level timing speculation to improve the performance and power-efficiency of processors, most of the innovation has been at the microarchitectural level. We make the observation that some code sequences place greater demand on circuit timing deadlines than others. Furthermore, by selectively replacing these codes with instruction sequences which are semantically equivalent but reduce activity on timing critical circuit paths, we can trigger fewer timing errors and hence reduce recovery costs.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {345--356},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950405},
 doi = {http://doi.acm.org/10.1145/1950365.1950405},
 acmid = {1950405},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler, isa design, timing speculation},
} 

@inproceedings{Farhad:2011:OAM:1950365.1950406,
 author = {Farhad, Sardar M. and Ko, Yousun and Burgstaller, Bernd and Scholz, Bernhard},
 title = {Orchestration by approximation: mapping stream programs onto multicore architectures},
 abstract = {We present a novel 2-approximation algorithm for deploying stream graphs on multicore computers and a stream graph transformation that eliminates bottlenecks. The key technical insight is a data rate transfer model that enables the computation of a "closed form", i.e., the data rate transfer function of an actor depending on the arrival rate of the stream program. A combinatorial optimization problem uses the closed form to maximize the throughput of the stream program. Although the problem is inherently NP-hard, we present an efficient and effective 2-approximation algorithm that provides a lower bound on the quality of the solution. We introduce a transformation that uses the closed form to identify and eliminate bottlenecks. We show experimentally that state-of-the art integer linear programming approaches for orchestrating stream graphs are (1) intractable or at least impractical for larger stream graphs and larger number of processors and (2)our 2-approximation algorithm is highly efficient and its results are close to the optimal solution for a standard set of StreamIt benchmark programs.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {357--368},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950406},
 doi = {http://doi.acm.org/10.1145/1950365.1950406},
 acmid = {1950406},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multicore, stream programming, streamit},
} 

@inproceedings{Zhang:2011:OED:1950365.1950408,
 author = {Zhang, Eddy Z. and Jiang, Yunlian and Guo, Ziyu and Tian, Kai and Shen, Xipeng},
 title = {On-the-fly elimination of dynamic irregularities for GPU computing},
 abstract = {The power-efficient massively parallel Graphics Processing Units (GPUs) have become increasingly influential for general-purpose computing over the past few years. However, their efficiency is sensitive to dynamic irregular memory references and control flows in an application. Experiments have shown great performance gains when these irregularities are removed. But it remains an open question how to achieve those gains through software approaches on modern GPUs. This paper presents a systematic exploration to tackle dynamic irregularities in both control flows and memory references. It reveals some properties of dynamic irregularities in both control flows and memory references, their interactions, and their relations with program data and threads. It describes several heuristics-based algorithms and runtime adaptation techniques for effectively removing dynamic irregularities through data reordering and job swapping. It presents a framework, G-Streamline, as a unified software solution to dynamic irregularities in GPU computing. G-Streamline has several distinctive properties. It is a pure software solution and works on the fly, requiring no hardware extensions or offline profiling. It treats both types of irregularities at the same time in a holistic fashion, maximizing the whole-program performance by resolving conflicts among optimizations. Its optimization overhead is largely transparent to GPU kernel executions, jeopardizing no basic efficiency of the GPU application. Finally, it is robust to the presence of various complexities in GPU applications. Experiments show that G-Streamline is effective in reducing dynamic irregularities in GPU computing, producing speedups between 1.07 and 2.5 for a variety of applications.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {369--380},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950408},
 doi = {http://doi.acm.org/10.1145/1950365.1950408},
 acmid = {1950408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cpu-gpu pipelining, data transformation, gpgpu, memory coalescing, thread data remapping, thread divergence},
} 

@inproceedings{Hormati:2011:SPS:1950365.1950409,
 author = {Hormati, Amir H. and Samadi, Mehrzad and Woh, Mark and Mudge, Trevor and Mahlke, Scott},
 title = {Sponge: portable stream programming on graphics engines},
 abstract = {Graphics processing units (GPUs) provide a low cost platform for accelerating high performance computations. The introduction of new programming languages, such as CUDA and OpenCL, makes GPU programming attractive to a wide variety of programmers. However, programming GPUs is still a cumbersome task for two primary reasons: tedious performance optimizations and lack of portability. First, optimizing an algorithm for a specific GPU is a time-consuming task that requires a thorough understanding of both the algorithm and the underlying hardware. Unoptimized CUDA programs typically only achieve a small fraction of the peak GPU performance. Second, GPU code lacks efficient portability as code written for one GPU can be inefficient when executed on another. Moving code from one GPU to another while maintaining the desired performance is a non-trivial task often requiring significant modifications to account for the hardware differences. In this work, we propose Sponge, a compilation framework for GPUs using synchronous data flow streaming languages. Sponge is capable of performing a wide variety of optimizations to generate efficient code for graphics engines. Sponge alleviates the problems associated with current GPU programming methods by providing portability across different generations of GPUs and CPUs, and a better abstraction of the hardware details, such as the memory hierarchy and threading model. Using streaming, we provide a write-once software paradigm and rely on the compiler to automatically create optimized CUDA code for a wide variety of GPU targets. Sponge's compiler optimizations improve the performance of the baseline CUDA implementations by an average of 3.2x.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {381--392},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950409},
 doi = {http://doi.acm.org/10.1145/1950365.1950409},
 acmid = {1950409},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler, gpu, optimization, portability, streaming},
} 

@inproceedings{Kamruzzaman:2011:IPM:1950365.1950411,
 author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.},
 title = {Inter-core prefetching for multicore processors using migrating helper threads},
 abstract = {Multicore processors have become ubiquitous in today's systems, but exploiting the parallelism they offer remains difficult, especially for legacy application and applications with large serial components. The challenge, then, is to develop techniques that allow multiple cores to work in concert to accelerate a single thread. This paper describes inter-core prefetching, a technique to exploit multiple cores to accelerate a single thread. Inter-core prefetching extends existing work on helper threads for SMT machines to multicore machines. Inter-core prefetching uses one compute thread and one or more prefetching threads. The prefetching threads execute on cores that would otherwise be idle, prefetching the data that the compute thread will need. The compute thread then migrates between cores, following the path of the prefetch threads, and finds the data already waiting for it. Inter-core prefetching works with existing hardware and existing instruction set architectures. Using a range of state-of-the-art multiprocessors, this paper characterizes the potential benefits of the technique with microbenchmarks and then measures its impact on a range of memory intensive applications. The results show that inter-core prefetching improves performance by an average of 31 to 63\%, depending on the architecture, and speeds up some applications by as much as 2.8x. It also demonstrates that inter-core prefetching reduces energy consumption by between 11 and 26\% on average.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {393--404},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1950365.1950411},
 doi = {http://doi.acm.org/10.1145/1950365.1950411},
 acmid = {1950411},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, compilers, helper threads, single-thread performance},
} 

@inproceedings{Hayashizaki:2011:IPT:1950365.1950412,
 author = {Hayashizaki, Hiroshige and Wu, Peng and Inoue, Hiroshi and Serrano, Mauricio J. and Nakatani, Toshio},
 title = {Improving the performance of trace-based systems by false loop filtering},
 abstract = {Trace-based compilation is a promising technique for language compilers and binary translators. It offers the potential to expand the compilation scopes that have traditionally been limited by method boundaries. Detecting repeating cyclic execution paths and capturing the detected repetitions into traces is a key requirement for trace selection algorithms to achieve good optimization and performance with small amounts of code. One important class of repetition detection is cyclic-path-based repetition detection, where a cyclic execution path (a path that starts and ends at the same instruction address) is detected as a repeating cyclic execution path. However, we found many cyclic paths that are not repeating cyclic execution paths, which we call false loops. A common class of false loops occurs when a method is invoked from multiple call-sites. A cycle is formed between two invocations of the method from different call-sites, but which does not represent loops or recursion. False loops can result in shorter traces and smaller compilation scopes, and degrade the performance. We propose false loop filtering, an approach to reject false loops in the repetition detection step of trace selection, and a technique called false loop filtering by call-stack-comparison, which rejects a cyclic path as a false loop if the call stacks at the beginning and the end of the cycle are different. We applied false loop filtering to our trace-based Java\&#8482; JIT compiler that is based on IBM's J9 JVM. We found that false loop filtering achieved an average improvement of 16\% and 10\% for the DaCapo benchmark when applied to two baseline trace selection algorithms, respectively, with up to 37\% improvement for individual benchmarks. In the end, with false loop filtering, our trace-based JIT achieves a performance comparable to that of the method-based J9 JVM/JIT using the corresponding optimization level.},
 booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '11},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {405--418},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950412},
 doi = {http://doi.acm.org/10.1145/1950365.1950412},
 acmid = {1950412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {repetition detection, trace selection, trace-based compilation},
} 

@inproceedings{Gebhart:2009:ETC:1508244.1508246,
 author = {Gebhart, Mark and Maher, Bertrand A. and Coons, Katherine E. and Diamond, Jeff and Gratz, Paul and Marino, Mario and Ranganathan, Nitya and Robatmili, Behnam and Smith, Aaron and Burrill, James and Keckler, Stephen W. and Burger, Doug and McKinley, Kathryn S.},
 title = {An evaluation of the TRIPS computer system},
 abstract = {The TRIPS system employs a new instruction set architecture (ISA) called Explicit Data Graph Execution (EDGE) that renegotiates the boundary between hardware and software to expose and exploit concurrency. EDGE ISAs use a block-atomic execution model in which blocks are composed of dataflow instructions. The goal of the TRIPS design is to mine concurrency for high performance while tolerating emerging technology scaling challenges, such as increasing wire delays and power consumption. This paper evaluates how well TRIPS meets this goal through a detailed ISA and performance analysis. We compare performance, using cycles counts, to commercial processors. On SPEC CPU2000, the Intel Core 2 outperforms compiled TRIPS code in most cases, although TRIPS matches a Pentium 4. On simple benchmarks, compiled TRIPS code outperforms the Core 2 by 10\% and hand-optimized TRIPS code outperforms it by factor of 3. Compared to conventional ISAs, the block-atomic model provides a larger instruction window, increases concurrency at a cost of more instructions executed, and replaces register and memory accesses with more efficient direct instruction-to-instruction communication. Our analysis suggests ISA, microarchitecture, and compiler enhancements for addressing weaknesses in TRIPS and indicates that EDGE architectures have the potential to exploit greater concurrency in future technologies.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508246},
 doi = {http://doi.acm.org/10.1145/1508244.1508246},
 acmid = {1508246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {trips},
} 

@article{Gebhart:2009:ETC:1508284.1508246,
 author = {Gebhart, Mark and Maher, Bertrand A. and Coons, Katherine E. and Diamond, Jeff and Gratz, Paul and Marino, Mario and Ranganathan, Nitya and Robatmili, Behnam and Smith, Aaron and Burrill, James and Keckler, Stephen W. and Burger, Doug and McKinley, Kathryn S.},
 title = {An evaluation of the TRIPS computer system},
 abstract = {The TRIPS system employs a new instruction set architecture (ISA) called Explicit Data Graph Execution (EDGE) that renegotiates the boundary between hardware and software to expose and exploit concurrency. EDGE ISAs use a block-atomic execution model in which blocks are composed of dataflow instructions. The goal of the TRIPS design is to mine concurrency for high performance while tolerating emerging technology scaling challenges, such as increasing wire delays and power consumption. This paper evaluates how well TRIPS meets this goal through a detailed ISA and performance analysis. We compare performance, using cycles counts, to commercial processors. On SPEC CPU2000, the Intel Core 2 outperforms compiled TRIPS code in most cases, although TRIPS matches a Pentium 4. On simple benchmarks, compiled TRIPS code outperforms the Core 2 by 10\% and hand-optimized TRIPS code outperforms it by factor of 3. Compared to conventional ISAs, the block-atomic model provides a larger instruction window, increases concurrency at a cost of more instructions executed, and replaces register and memory accesses with more efficient direct instruction-to-instruction communication. Our analysis suggests ISA, microarchitecture, and compiler enhancements for addressing weaknesses in TRIPS and indicates that EDGE architectures have the potential to exploit greater concurrency in future technologies.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508246},
 doi = {http://doi.acm.org/10.1145/1508284.1508246},
 acmid = {1508246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {trips},
} 

@article{Pistol:2009:AIN:1508284.1508247,
 author = {Pistol, Constantin and Chongchitmate, Wutichai and Dwyer, Christopher and Lebeck, Alvin R.},
 title = {Architectural implications of nanoscale integrated sensing and computing},
 abstract = {This paper explores the architectural implications of integrating computation and molecular probes to form nanoscale sensor processors (nSP). We show how nSPs may enable new computing domains and automate tasks that currently require expert scientific training and costly equipment. This new application domain severely constrains nSP size, which significantly impacts the architectural design space. In this context, we explore nSP architectures and present an nSP design that includes a simple accumulator-based ISA, sensors, limited memory and communication transceivers. To reduce the application memory footprint, we introduce the concept of instruction-fused sensing. We use simulation and analytical models to evaluate nSP designs executing a representative set of target applications. Furthermore, we propose a candidate nSP technology based on optical Resonance Energy Transfer (RET) logic that enables the small size required by the application domain; our smallest design is about the size of the largest known virus. We also show laboratory results that demonstrate initial steps towards a prototype.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508247},
 doi = {http://doi.acm.org/10.1145/1508284.1508247},
 acmid = {1508247},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {biological computing, instruction fused computing, nanocomputing, new computing domain},
} 

@inproceedings{Pistol:2009:AIN:1508244.1508247,
 author = {Pistol, Constantin and Chongchitmate, Wutichai and Dwyer, Christopher and Lebeck, Alvin R.},
 title = {Architectural implications of nanoscale integrated sensing and computing},
 abstract = {This paper explores the architectural implications of integrating computation and molecular probes to form nanoscale sensor processors (nSP). We show how nSPs may enable new computing domains and automate tasks that currently require expert scientific training and costly equipment. This new application domain severely constrains nSP size, which significantly impacts the architectural design space. In this context, we explore nSP architectures and present an nSP design that includes a simple accumulator-based ISA, sensors, limited memory and communication transceivers. To reduce the application memory footprint, we introduce the concept of instruction-fused sensing. We use simulation and analytical models to evaluate nSP designs executing a representative set of target applications. Furthermore, we propose a candidate nSP technology based on optical Resonance Energy Transfer (RET) logic that enables the small size required by the application domain; our smallest design is about the size of the largest known virus. We also show laboratory results that demonstrate initial steps towards a prototype.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508247},
 doi = {http://doi.acm.org/10.1145/1508244.1508247},
 acmid = {1508247},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {biological computing, instruction fused computing, nanocomputing, new computing domain},
} 

@article{Park:2009:CEA:1508284.1508249,
 author = {Park, Soyeon and Lu, Shan and Zhou, Yuanyuan},
 title = {CTrigger: exposing atomicity violation bugs from their hiding places},
 abstract = {Multicore hardware is making concurrent programs pervasive. Unfortunately, concurrent programs are prone to bugs. Among different types of concurrency bugs, atomicity violation bugs are common and important. Existing techniques to detect atomicity violation bugs suffer from one limitation: requiring bugs to manifest during monitored runs, which is an open problem in concurrent program testing. This paper makes two contributions. First, it studies the interleaving characteristics of the common practice in concurrent program testing (i.e., running a program over and over) to understand why atomicity violation bugs are hard to expose. Second, it proposes CTrigger to effectively and efficiently expose atomicity violation bugs in large programs. CTrigger focuses on a special type of interleavings (i.e., unserializable interleavings) that are inherently correlated to atomicity violation bugs, and uses trace analysis to systematically identify (likely) feasible unserializable interleavings with low occurrence-probability. CTrigger then uses minimum execution perturbation to exercise low-probability interleavings and expose difficult-to-catch atomicity violation. We evaluate CTrigger with real-world atomicity violation bugs from four sever/desktop applications (Apache, MySQL, Mozilla, and PBZIP2) and three SPLASH2 applications on 8-core machines. CTrigger efficiently exposes the tested bugs within 1--235 seconds, two to four orders of magnitude faster than stress testing. Without CTrigger, some of these bugs do not manifest even after 7 full days of stress testing. In addition, without deterministic replay support, once a bug is exposed, CTrigger can help programmers reliably reproduce it for diagnosis. Our tested bugs are reproduced by CTrigger mostly within 5 seconds, 300 to over 60000 times faster than stress testing.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508249},
 doi = {http://doi.acm.org/10.1145/1508284.1508249},
 acmid = {1508249},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bug, software testing},
} 

@inproceedings{Park:2009:CEA:1508244.1508249,
 author = {Park, Soyeon and Lu, Shan and Zhou, Yuanyuan},
 title = {CTrigger: exposing atomicity violation bugs from their hiding places},
 abstract = {Multicore hardware is making concurrent programs pervasive. Unfortunately, concurrent programs are prone to bugs. Among different types of concurrency bugs, atomicity violation bugs are common and important. Existing techniques to detect atomicity violation bugs suffer from one limitation: requiring bugs to manifest during monitored runs, which is an open problem in concurrent program testing. This paper makes two contributions. First, it studies the interleaving characteristics of the common practice in concurrent program testing (i.e., running a program over and over) to understand why atomicity violation bugs are hard to expose. Second, it proposes CTrigger to effectively and efficiently expose atomicity violation bugs in large programs. CTrigger focuses on a special type of interleavings (i.e., unserializable interleavings) that are inherently correlated to atomicity violation bugs, and uses trace analysis to systematically identify (likely) feasible unserializable interleavings with low occurrence-probability. CTrigger then uses minimum execution perturbation to exercise low-probability interleavings and expose difficult-to-catch atomicity violation. We evaluate CTrigger with real-world atomicity violation bugs from four sever/desktop applications (Apache, MySQL, Mozilla, and PBZIP2) and three SPLASH2 applications on 8-core machines. CTrigger efficiently exposes the tested bugs within 1--235 seconds, two to four orders of magnitude faster than stress testing. Without CTrigger, some of these bugs do not manifest even after 7 full days of stress testing. In addition, without deterministic replay support, once a bug is exposed, CTrigger can help programmers reliably reproduce it for diagnosis. Our tested bugs are reproduced by CTrigger mostly within 5 seconds, 300 to over 60000 times faster than stress testing.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508249},
 doi = {http://doi.acm.org/10.1145/1508244.1508249},
 acmid = {1508249},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency bug, software testing},
} 

@article{Sidiroglou:2009:AAS:1508284.1508250,
 author = {Sidiroglou, Stelios and Laadan, Oren and Perez, Carlos and Viennot, Nicolas and Nieh, Jason and Keromytis, Angelos D.},
 title = {ASSURE: automatic software self-healing using rescue points},
 abstract = {Software failures in server applications are a significant problem for preserving system availability. We present ASSURE, a system that introduces rescue points that recover software from unknown faults while maintaining both system integrity and availability, by mimicking system behavior under known error conditions. Rescue points are locations in existing application code for handling a given set of programmer-anticipated failures, which are automatically repurposed and tested for safely enabling fault recovery from a larger class of (unanticipated) faults. When a fault occurs at an arbitrary location in the program, ASSURE restores execution to an appropriate rescue point and induces the program to recover execution by virtualizing the program's existing error-handling facilities. Rescue points are identified using fuzzing, implemented using a fast coordinated checkpoint-restart mechanism that handles multi-process and multi-threaded applications, and, after testing, are injected into production code using binary patching. We have implemented an ASSURE Linux prototype that operates without application source code and without base operating system kernel changes. Our experimental results on a set of real-world server applications and bugs show that ASSURE enabled recovery for all of the bugs tested with fast recovery times, has modest performance overhead, and provides automatic self-healing orders of magnitude faster than current human-driven patch deployment methods.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508250},
 doi = {http://doi.acm.org/10.1145/1508284.1508250},
 acmid = {1508250},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary patching, chekpoint restart, error recovery, reliable software, software self-healing},
} 

@inproceedings{Sidiroglou:2009:AAS:1508244.1508250,
 author = {Sidiroglou, Stelios and Laadan, Oren and Perez, Carlos and Viennot, Nicolas and Nieh, Jason and Keromytis, Angelos D.},
 title = {ASSURE: automatic software self-healing using rescue points},
 abstract = {Software failures in server applications are a significant problem for preserving system availability. We present ASSURE, a system that introduces rescue points that recover software from unknown faults while maintaining both system integrity and availability, by mimicking system behavior under known error conditions. Rescue points are locations in existing application code for handling a given set of programmer-anticipated failures, which are automatically repurposed and tested for safely enabling fault recovery from a larger class of (unanticipated) faults. When a fault occurs at an arbitrary location in the program, ASSURE restores execution to an appropriate rescue point and induces the program to recover execution by virtualizing the program's existing error-handling facilities. Rescue points are identified using fuzzing, implemented using a fast coordinated checkpoint-restart mechanism that handles multi-process and multi-threaded applications, and, after testing, are injected into production code using binary patching. We have implemented an ASSURE Linux prototype that operates without application source code and without base operating system kernel changes. Our experimental results on a set of real-world server applications and bugs show that ASSURE enabled recovery for all of the bugs tested with fast recovery times, has modest performance overhead, and provides automatic self-healing orders of magnitude faster than current human-driven patch deployment methods.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508250},
 doi = {http://doi.acm.org/10.1145/1508244.1508250},
 acmid = {1508250},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary patching, chekpoint restart, error recovery, reliable software, software self-healing},
} 

@article{Lenharth:2009:RDO:1508284.1508251,
 author = {Lenharth, Andrew and Adve, Vikram S. and King, Samuel T.},
 title = {Recovery domains: an organizing principle for recoverable operating systems},
 abstract = {We describe a strategy for enabling existing commodity operating systems to recover from unexpected run-time errors in nearly any part of the kernel, including core kernel components. Our approach is dynamic and request-oriented; it isolates the effects of a fault to the requests that caused the fault rather than to static kernel components. This approach is based on a notion of "recovery domains," an organizing principle to enable rollback of state affected by a request in a multithreaded system with minimal impact on other requests or threads. We have applied this approach on v2.4.22 and v2.6.27 of the Linux kernel and it required 132 lines of changed or new code: the other changes are all performed by a simple instrumentation pass of a compiler. Our experiments show that the approach is able to recover from otherwise fatal faults with minimal collateral impact during a recovery event.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508251},
 doi = {http://doi.acm.org/10.1145/1508284.1508251},
 acmid = {1508251},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {akeso, automatic fault recovery, recovery domains},
} 

@inproceedings{Lenharth:2009:RDO:1508244.1508251,
 author = {Lenharth, Andrew and Adve, Vikram S. and King, Samuel T.},
 title = {Recovery domains: an organizing principle for recoverable operating systems},
 abstract = {We describe a strategy for enabling existing commodity operating systems to recover from unexpected run-time errors in nearly any part of the kernel, including core kernel components. Our approach is dynamic and request-oriented; it isolates the effects of a fault to the requests that caused the fault rather than to static kernel components. This approach is based on a notion of "recovery domains," an organizing principle to enable rollback of state affected by a request in a multithreaded system with minimal impact on other requests or threads. We have applied this approach on v2.4.22 and v2.6.27 of the Linux kernel and it required 132 lines of changed or new code: the other changes are all performed by a simple instrumentation pass of a compiler. Our experiments show that the approach is able to recover from otherwise fatal faults with minimal collateral impact during a recovery event.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508251},
 doi = {http://doi.acm.org/10.1145/1508244.1508251},
 acmid = {1508251},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {akeso, automatic fault recovery, recovery domains},
} 

@inproceedings{Dimitrov:2009:ABP:1508244.1508252,
 author = {Dimitrov, Martin and Zhou, Huiyang},
 title = {Anomaly-based bug prediction, isolation, and validation: an automated approach for software debugging},
 abstract = {Software defects, commonly known as bugs, present a serious challenge for system reliability and dependability. Once a program failure is observed, the debugging activities to locate the defects are typically nontrivial and time consuming. In this paper, we propose a novel automated approach to pin-point the root-causes of software failures. Our proposed approach consists of three steps. The first step is bug prediction, which leverages the existing work on anomaly-based bug detection as exceptional behavior during program execution has been shown to frequently point to the root cause of a software failure. The second step is bug isolation, which eliminates false-positive bug predictions by checking whether the dynamic forward slices of bug predictions lead to the observed program failure. The last step is bug validation, in which the isolated anomalies are validated by dynamically nullifying their effects and observing if the program still fails. The whole bug prediction, isolation and validation process is fully automated and can be implemented with efficient architectural support. Our experiments with 6 programs and 7 bugs, including a real bug in the gcc 2.95.2 compiler, show that our approach is highly effective at isolating only the relevant anomalies. Compared to state-of-art debugging techniques, our proposed approach pinpoints the defect locations more accurately and presents the user with a much smaller code set to analyze.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508252},
 doi = {http://doi.acm.org/10.1145/1508244.1508252},
 acmid = {1508252},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architectural support, automated debugging},
} 

@article{Dimitrov:2009:ABP:1508284.1508252,
 author = {Dimitrov, Martin and Zhou, Huiyang},
 title = {Anomaly-based bug prediction, isolation, and validation: an automated approach for software debugging},
 abstract = {Software defects, commonly known as bugs, present a serious challenge for system reliability and dependability. Once a program failure is observed, the debugging activities to locate the defects are typically nontrivial and time consuming. In this paper, we propose a novel automated approach to pin-point the root-causes of software failures. Our proposed approach consists of three steps. The first step is bug prediction, which leverages the existing work on anomaly-based bug detection as exceptional behavior during program execution has been shown to frequently point to the root cause of a software failure. The second step is bug isolation, which eliminates false-positive bug predictions by checking whether the dynamic forward slices of bug predictions lead to the observed program failure. The last step is bug validation, in which the isolated anomalies are validated by dynamically nullifying their effects and observing if the program still fails. The whole bug prediction, isolation and validation process is fully automated and can be implemented with efficient architectural support. Our experiments with 6 programs and 7 bugs, including a real bug in the gcc 2.95.2 compiler, show that our approach is highly effective at isolating only the relevant anomalies. Compared to state-of-art debugging techniques, our proposed approach pinpoints the defect locations more accurately and presents the user with a much smaller code set to analyze.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508252},
 doi = {http://doi.acm.org/10.1145/1508284.1508252},
 acmid = {1508252},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architectural support, automated debugging},
} 

@article{Montesinos:2009:CSI:1508284.1508254,
 author = {Montesinos, Pablo and Hicks, Matthew and King, Samuel T. and Torrellas, Josep},
 title = {Capo: a software-hardware interface for practical deterministic multiprocessor replay},
 abstract = {While deterministic replay of parallel programs is a powerful technique, current proposals have shortcomings. Specifically, software-based replay systems have high overheads on multiprocessors, while hardware-based proposals focus only on basic hardware-level mechanisms, ignoring the overall replay system. To be practical, hardware-based replay systems need to support an environment with multiple parallel jobs running concurrently -- some being recorded, others being replayed and even others running without recording or replay. Moreover, they need to manage limited-size log buffers. This paper addresses these shortcomings by introducing, for the first time, a set of abstractions and a software-hardware interface for practical hardware-assisted replay of multiprocessor systems. The approach, called Capo</i>, introduces the novel abstraction of the Replay Sphere</i> to separate the responsibilities of the hardware and software components of the replay system. In this paper, we also design and build CapoOne</i>, a prototype of a deterministic multiprocessor replay system that implements Capo using Linux and simulated DeLorean hardware. Our evaluation of 4-processor executions shows that CapoOne</i> largely records with the efficiency of hardware-based schemes and the flexibility of software-based schemes.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508254},
 doi = {http://doi.acm.org/10.1145/1508284.1508254},
 acmid = {1508254},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capo, capoone, deterministic replay, replay sphere},
} 

@inproceedings{Montesinos:2009:CSI:1508244.1508254,
 author = {Montesinos, Pablo and Hicks, Matthew and King, Samuel T. and Torrellas, Josep},
 title = {Capo: a software-hardware interface for practical deterministic multiprocessor replay},
 abstract = {While deterministic replay of parallel programs is a powerful technique, current proposals have shortcomings. Specifically, software-based replay systems have high overheads on multiprocessors, while hardware-based proposals focus only on basic hardware-level mechanisms, ignoring the overall replay system. To be practical, hardware-based replay systems need to support an environment with multiple parallel jobs running concurrently -- some being recorded, others being replayed and even others running without recording or replay. Moreover, they need to manage limited-size log buffers. This paper addresses these shortcomings by introducing, for the first time, a set of abstractions and a software-hardware interface for practical hardware-assisted replay of multiprocessor systems. The approach, called Capo</i>, introduces the novel abstraction of the Replay Sphere</i> to separate the responsibilities of the hardware and software components of the replay system. In this paper, we also design and build CapoOne</i>, a prototype of a deterministic multiprocessor replay system that implements Capo using Linux and simulated DeLorean hardware. Our evaluation of 4-processor executions shows that CapoOne</i> largely records with the efficiency of hardware-based schemes and the flexibility of software-based schemes.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508254},
 doi = {http://doi.acm.org/10.1145/1508244.1508254},
 acmid = {1508254},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capo, capoone, deterministic replay, replay sphere},
} 

@inproceedings{Devietti:2009:DDS:1508244.1508255,
 author = {Devietti, Joseph and Lucia, Brandon and Ceze, Luis and Oskin, Mark},
 title = {DMP: deterministic shared memory multiprocessing},
 abstract = {Current shared memory multicore and multiprocessor systems are nondeterministic. Each time these systems execute a multithreaded application, even if supplied with the same input, they can produce a different output. This frustrates debugging and limits the ability to properly test multithreaded code, becoming a major stumbling block to the much-needed widespread adoption of parallel programming. In this paper we make the case for fully deterministic shared memory multiprocessing (DMP). The behavior of an arbitrary multithreaded program on a DMP system is only a function of its inputs. The core idea is to make inter-thread communication fully deterministic. Previous approaches to coping with nondeterminism in multithreaded programs have focused on replay, a technique useful only for debugging. In contrast, while DMP systems are directly useful for debugging by offering repeatability by default, we argue that parallel programs should execute deterministically in the field as well. This has the potential to make testing more assuring and increase the reliability of deployed multithreaded software. We propose a range of approaches to enforcing determinism and discuss their implementation trade-offs. We show that determinism can be provided with little performance cost using our architecture proposals on future hardware, and that software-only approaches can be utilized on existing systems.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508255},
 doi = {http://doi.acm.org/10.1145/1508244.1508255},
 acmid = {1508255},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, determinism, multicores, parallel programming},
} 

@article{Devietti:2009:DDS:1508284.1508255,
 author = {Devietti, Joseph and Lucia, Brandon and Ceze, Luis and Oskin, Mark},
 title = {DMP: deterministic shared memory multiprocessing},
 abstract = {Current shared memory multicore and multiprocessor systems are nondeterministic. Each time these systems execute a multithreaded application, even if supplied with the same input, they can produce a different output. This frustrates debugging and limits the ability to properly test multithreaded code, becoming a major stumbling block to the much-needed widespread adoption of parallel programming. In this paper we make the case for fully deterministic shared memory multiprocessing (DMP). The behavior of an arbitrary multithreaded program on a DMP system is only a function of its inputs. The core idea is to make inter-thread communication fully deterministic. Previous approaches to coping with nondeterminism in multithreaded programs have focused on replay, a technique useful only for debugging. In contrast, while DMP systems are directly useful for debugging by offering repeatability by default, we argue that parallel programs should execute deterministically in the field as well. This has the potential to make testing more assuring and increase the reliability of deployed multithreaded software. We propose a range of approaches to enforcing determinism and discuss their implementation trade-offs. We show that determinism can be provided with little performance cost using our architecture proposals on future hardware, and that software-only approaches can be utilized on existing systems.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508255},
 doi = {http://doi.acm.org/10.1145/1508284.1508255},
 acmid = {1508255},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, determinism, multicores, parallel programming},
} 

@article{Olszewski:2009:KED:1508284.1508256,
 author = {Olszewski, Marek and Ansel, Jason and Amarasinghe, Saman},
 title = {Kendo: efficient deterministic multithreading in software},
 abstract = {Although chip-multiprocessors have become the industry standard, developing parallel applications that target them remains a daunting task. Non-determinism, inherent in threaded applications, causes significant challenges for parallel programmers by hindering their ability to create parallel applications with repeatable results. As a consequence, parallel applications are significantly harder to debug, test, and maintain than sequential programs. This paper introduces Kendo: a new software-only system that provides deterministic multithreading of parallel applications. Kendo enforces a deterministic interleaving of lock acquisitions and specially declared non-protected reads through a novel dynamically load-balanced deterministic scheduling algorithm. The algorithm tracks the progress of each thread using performance counters to construct a deterministic logical time that is used to compute an interleaving of shared data accesses that is both deterministic and provides good load balancing. Kendo can run on today's commodity hardware while incurring only a modest performance cost. Experimental results on the SPLASH-2 applications yield a geometric mean overhead of only 16\% when running on 4 processors. This low overhead makes it possible to benefit from Kendo even after an application is deployed. Programmers can start using Kendo today to program parallel applications that are easier to develop, debug, and test.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508256},
 doi = {http://doi.acm.org/10.1145/1508284.1508256},
 acmid = {1508256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, determinism, deterministic multithreading, multicore, parallel programming},
} 

@inproceedings{Olszewski:2009:KED:1508244.1508256,
 author = {Olszewski, Marek and Ansel, Jason and Amarasinghe, Saman},
 title = {Kendo: efficient deterministic multithreading in software},
 abstract = {Although chip-multiprocessors have become the industry standard, developing parallel applications that target them remains a daunting task. Non-determinism, inherent in threaded applications, causes significant challenges for parallel programmers by hindering their ability to create parallel applications with repeatable results. As a consequence, parallel applications are significantly harder to debug, test, and maintain than sequential programs. This paper introduces Kendo: a new software-only system that provides deterministic multithreading of parallel applications. Kendo enforces a deterministic interleaving of lock acquisitions and specially declared non-protected reads through a novel dynamically load-balanced deterministic scheduling algorithm. The algorithm tracks the progress of each thread using performance counters to construct a deterministic logical time that is used to compute an interleaving of shared data accesses that is both deterministic and provides good load balancing. Kendo can run on today's commodity hardware while incurring only a modest performance cost. Experimental results on the SPLASH-2 applications yield a geometric mean overhead of only 16\% when running on 4 processors. This low overhead makes it possible to benefit from Kendo even after an application is deployed. Programmers can start using Kendo today to program parallel applications that are easier to develop, debug, and test.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508256},
 doi = {http://doi.acm.org/10.1145/1508244.1508256},
 acmid = {1508256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, determinism, deterministic multithreading, multicore, parallel programming},
} 

@article{Tiwari:2009:CIF:1508284.1508258,
 author = {Tiwari, Mohit and Wassel, Hassan M.G. and Mazloom, Bita and Mysore, Shashidhar and Chong, Frederic T. and Sherwood, Timothy},
 title = {Complete information flow tracking from the gates up},
 abstract = {For many mission-critical tasks, tight guarantees on the flow of information are desirable, for example, when handling important cryptographic keys or sensitive financial data. We present a novel architecture capable of tracking all information flow within the machine, including all explicit data transfers and all implicit flows (those subtly devious flows caused by not performing conditional operations). While the problem is impossible to solve in the general case, we have created a machine that avoids the general-purpose programmability that leads to this impossibility result, yet is still programmable enough to handle a variety of critical operations such as public-key encryption and authentication. Through the application of our novel gate-level information flow tracking method, we show how all flows of information can be precisely tracked. From this foundation, we then describe how a class of architectures can be constructed, from the gates up, to completely capture all information flows and we measure the impact of doing so on the hardware implementation, the ISA, and the programmer.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508258},
 doi = {http://doi.acm.org/10.1145/1508284.1508258},
 acmid = {1508258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gate level, information flow tracking, provably sound},
} 

@inproceedings{Tiwari:2009:CIF:1508244.1508258,
 author = {Tiwari, Mohit and Wassel, Hassan M.G. and Mazloom, Bita and Mysore, Shashidhar and Chong, Frederic T. and Sherwood, Timothy},
 title = {Complete information flow tracking from the gates up},
 abstract = {For many mission-critical tasks, tight guarantees on the flow of information are desirable, for example, when handling important cryptographic keys or sensitive financial data. We present a novel architecture capable of tracking all information flow within the machine, including all explicit data transfers and all implicit flows (those subtly devious flows caused by not performing conditional operations). While the problem is impossible to solve in the general case, we have created a machine that avoids the general-purpose programmability that leads to this impossibility result, yet is still programmable enough to handle a variety of critical operations such as public-key encryption and authentication. Through the application of our novel gate-level information flow tracking method, we show how all flows of information can be precisely tracked. From this foundation, we then describe how a class of architectures can be constructed, from the gates up, to completely capture all information flows and we measure the impact of doing so on the hardware implementation, the ISA, and the programmer.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508258},
 doi = {http://doi.acm.org/10.1145/1508244.1508258},
 acmid = {1508258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gate level, information flow tracking, provably sound},
} 

@article{Tam:2009:RAL:1508284.1508259,
 author = {Tam, David K. and Azimi, Reza and Soares, Livio B. and Stumm, Michael},
 title = {RapidMRC: approximating L2 miss rate curves on commodity systems for online optimizations},
 abstract = {Miss rate curves (MRCs) are useful in a number of contexts. In our research, online L2 cache MRCs enable us to dynamically identify optimal cache sizes when cache-partitioning a shared-cache multicore processor. Obtaining L2 MRCs has generally been assumed to be expensive when done in software and consequently, their usage for online optimizations has been limited. To address these problems and opportunities, we have developed a low-overhead software technique to obtain L2 MRCs online on current processors, exploiting features available in their performance monitoring units so that no changes to the application source code or binaries are required. Our technique, called RapidMRC, requires a single probing period of roughly 221 million processor cycles (147 ms), and subsequently 124 million cycles (83 ms) to process the data. We demonstrate its accuracy by comparing the obtained MRCs to the actual L2 MRCs of 30 applications taken from SPECcpu2006, SPECcpu2000, and SPECjbb2000. We show that RapidMRC can be applied to sizing cache partitions, helping to achieve performance improvements of up to 27\%.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508259},
 doi = {http://doi.acm.org/10.1145/1508284.1508259},
 acmid = {1508259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache management, cache partitioning, chip multiprocessor, dynamic optimization, hardware performance counters, miss rate curve, multicore processor, online optimization, performance monitoring unit, shared cache, shared cache management},
} 

@inproceedings{Tam:2009:RAL:1508244.1508259,
 author = {Tam, David K. and Azimi, Reza and Soares, Livio B. and Stumm, Michael},
 title = {RapidMRC: approximating L2 miss rate curves on commodity systems for online optimizations},
 abstract = {Miss rate curves (MRCs) are useful in a number of contexts. In our research, online L2 cache MRCs enable us to dynamically identify optimal cache sizes when cache-partitioning a shared-cache multicore processor. Obtaining L2 MRCs has generally been assumed to be expensive when done in software and consequently, their usage for online optimizations has been limited. To address these problems and opportunities, we have developed a low-overhead software technique to obtain L2 MRCs online on current processors, exploiting features available in their performance monitoring units so that no changes to the application source code or binaries are required. Our technique, called RapidMRC, requires a single probing period of roughly 221 million processor cycles (147 ms), and subsequently 124 million cycles (83 ms) to process the data. We demonstrate its accuracy by comparing the obtained MRCs to the actual L2 MRCs of 30 applications taken from SPECcpu2006, SPECcpu2000, and SPECjbb2000. We show that RapidMRC can be applied to sizing cache partitions, helping to achieve performance improvements of up to 27\%.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508259},
 doi = {http://doi.acm.org/10.1145/1508244.1508259},
 acmid = {1508259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache management, cache partitioning, chip multiprocessor, dynamic optimization, hardware performance counters, miss rate curve, multicore processor, online optimization, performance monitoring unit, shared cache, shared cache management},
} 

@inproceedings{Eyerman:2009:PCA:1508244.1508260,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {Per-thread cycle accounting in SMT processors},
 abstract = {This paper proposes a cycle accounting architecture for Simultaneous Multithreading (SMT) processors that estimates the execution times for each of the threads had they been executed alone, while they are running simultaneously on the SMT processor. This is done by accounting each cycle to either a base, miss event or waiting cycle component during multi-threaded execution. Single-threaded alone execution time is then estimated as the sum of the base and miss event components; the waiting cycle component represents the lost cycle count due to SMT execution. The cycle accounting architecture incurs reasonable hardware cost (around 1KB of storage) and estimates single-threaded performance with average prediction errors around 7.2\% for two-program workloads and 11.7\% for four-program workloads. The cycle accounting architecture has several important applications to system software and its interaction with SMT hardware. For one, the estimated single-thread alone execution time provides an accurate picture to system software of the actually consumed processor cycles per thread. The alone execution time instead of the total execution time (timeslice) may make system software scheduling policies more effective. Second, a new class of thread-progress aware SMT fetch policies based on per-thread progress indicators enable system software level priorities to be enforced at the hardware level.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508260},
 doi = {http://doi.acm.org/10.1145/1508244.1508260},
 acmid = {1508260},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cycle accounting, simultaneous multithreading (smt), thread-progress aware fetch policy},
} 

@article{Eyerman:2009:PCA:1508284.1508260,
 author = {Eyerman, Stijn and Eeckhout, Lieven},
 title = {Per-thread cycle accounting in SMT processors},
 abstract = {This paper proposes a cycle accounting architecture for Simultaneous Multithreading (SMT) processors that estimates the execution times for each of the threads had they been executed alone, while they are running simultaneously on the SMT processor. This is done by accounting each cycle to either a base, miss event or waiting cycle component during multi-threaded execution. Single-threaded alone execution time is then estimated as the sum of the base and miss event components; the waiting cycle component represents the lost cycle count due to SMT execution. The cycle accounting architecture incurs reasonable hardware cost (around 1KB of storage) and estimates single-threaded performance with average prediction errors around 7.2\% for two-program workloads and 11.7\% for four-program workloads. The cycle accounting architecture has several important applications to system software and its interaction with SMT hardware. For one, the estimated single-thread alone execution time provides an accurate picture to system software of the actually consumed processor cycles per thread. The alone execution time instead of the total execution time (timeslice) may make system software scheduling policies more effective. Second, a new class of thread-progress aware SMT fetch policies based on per-thread progress indicators enable system software level priorities to be enforced at the hardware level.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508260},
 doi = {http://doi.acm.org/10.1145/1508284.1508260},
 acmid = {1508260},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cycle accounting, simultaneous multithreading (smt), thread-progress aware fetch policy},
} 

@inproceedings{Hofmann:2009:MBM:1508244.1508262,
 author = {Hofmann, Owen S. and Rossbach, Christopher J. and Witchel, Emmett},
 title = {Maximum benefit from a minimal HTM},
 abstract = {A minimal, bounded hardware transactional memory implementation significantly improves synchronization performance when used in an operating system kernel. We add HTM to Linux 2.4, a kernel with a simple, coarse-grained synchronization structure. The transactional Linux 2.4 kernel can improve performance of user programs by as much as 40\% over the non-transactional 2.4 kernel. It closes 68\% of the performance gap with the Linux 2.6 kernel, which has had significant engineering effort applied to improve scalability. We then extend our minimal HTM to a fast, unbounded transactional memory with a novel technique for coordinating hardware transactions and software synchronization. Overflowed transactions run in software, with only a minimal coupling between hardware and software systems. There is no performance penalty for overflow rates of less than 1\%. In one instance, at 16 processors and an overflow rate of 4\%, performance degrades from an ideal 4.3x to 3.6x.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508262},
 doi = {http://doi.acm.org/10.1145/1508244.1508262},
 acmid = {1508262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware transactional memory},
} 

@article{Hofmann:2009:MBM:1508284.1508262,
 author = {Hofmann, Owen S. and Rossbach, Christopher J. and Witchel, Emmett},
 title = {Maximum benefit from a minimal HTM},
 abstract = {A minimal, bounded hardware transactional memory implementation significantly improves synchronization performance when used in an operating system kernel. We add HTM to Linux 2.4, a kernel with a simple, coarse-grained synchronization structure. The transactional Linux 2.4 kernel can improve performance of user programs by as much as 40\% over the non-transactional 2.4 kernel. It closes 68\% of the performance gap with the Linux 2.6 kernel, which has had significant engineering effort applied to improve scalability. We then extend our minimal HTM to a fast, unbounded transactional memory with a novel technique for coordinating hardware transactions and software synchronization. Overflowed transactions run in software, with only a minimal coupling between hardware and software systems. There is no performance penalty for overflow rates of less than 1\%. In one instance, at 16 processors and an overflow rate of 4\%, performance degrades from an ideal 4.3x to 3.6x.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508262},
 doi = {http://doi.acm.org/10.1145/1508284.1508262},
 acmid = {1508262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware transactional memory},
} 

@article{Dice:2009:EEC:1508284.1508263,
 author = {Dice, Dave and Lev, Yossi and Moir, Mark and Nussbaum, Daniel},
 title = {Early experience with a commercial hardware transactional memory implementation},
 abstract = {We report on our experience with the hardware transactional memory (HTM) feature of two pre-production revisions of a new commercial multicore processor. Our experience includes a number of promising results using HTM to improve performance in a variety of contexts, and also identifies some ways in which the feature could be improved to make it even better. We give detailed accounts of our experiences, sharing techniques we used to achieve the results we have, as well as describing challenges we faced in doing so.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508263},
 doi = {http://doi.acm.org/10.1145/1508284.1508263},
 acmid = {1508263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware, synchronization, transactional memory},
} 

@inproceedings{Dice:2009:EEC:1508244.1508263,
 author = {Dice, Dave and Lev, Yossi and Moir, Mark and Nussbaum, Daniel},
 title = {Early experience with a commercial hardware transactional memory implementation},
 abstract = {We report on our experience with the hardware transactional memory (HTM) feature of two pre-production revisions of a new commercial multicore processor. Our experience includes a number of promising results using HTM to improve performance in a variety of contexts, and also identifies some ways in which the feature could be improved to make it even better. We give detailed accounts of our experiences, sharing techniques we used to achieve the results we have, as well as describing challenges we faced in doing so.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508263},
 doi = {http://doi.acm.org/10.1145/1508244.1508263},
 acmid = {1508263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware, synchronization, transactional memory},
} 

@inproceedings{Wells:2009:MMR:1508244.1508265,
 author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
 title = {Mixed-mode multicore reliability},
 abstract = {Future processors are expected to observe increasing rates of hardware faults. Using Dual-Modular Redundancy (DMR), two cores of a multicore can be loosely coupled to redundantly execute a single software thread, providing very high coverage from many difference sources of faults. This reliability, however, comes at a high price in terms of per-thread IPC and overall system throughput. We make the observation that a user may want to run both applications requiring high reliability, such as financial software, and more fault tolerant applications requiring high performance, such as media or web software, on the same machine at the same time. Yet a traditional DMR system must fully operate in redundant mode whenever any application requires high reliability. This paper proposes a Mixed-Mode Multicore (MMM), which enables most applications, including the system software, to run with high reliability in DMR mode, while applications that need high performance can avoid the penalty of DMR. Though conceptually simple, two key challenges arise: 1) care must be taken to protect reliable applications from any faults occurring to applications running in high performance mode, and 2) the desire to execute additional independent software threads for a performance application complicates the scheduling of computation to cores. After solving these issues, an MMM is shown to improve overall system performance, compared to a traditional DMR system, by approximately 2X when one reliable and one performance application are concurrently executing.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508265},
 doi = {http://doi.acm.org/10.1145/1508244.1508265},
 acmid = {1508265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dual-modular redundancy, multicore},
} 

@article{Wells:2009:MMR:1508284.1508265,
 author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
 title = {Mixed-mode multicore reliability},
 abstract = {Future processors are expected to observe increasing rates of hardware faults. Using Dual-Modular Redundancy (DMR), two cores of a multicore can be loosely coupled to redundantly execute a single software thread, providing very high coverage from many difference sources of faults. This reliability, however, comes at a high price in terms of per-thread IPC and overall system throughput. We make the observation that a user may want to run both applications requiring high reliability, such as financial software, and more fault tolerant applications requiring high performance, such as media or web software, on the same machine at the same time. Yet a traditional DMR system must fully operate in redundant mode whenever any application requires high reliability. This paper proposes a Mixed-Mode Multicore (MMM), which enables most applications, including the system software, to run with high reliability in DMR mode, while applications that need high performance can avoid the penalty of DMR. Though conceptually simple, two key challenges arise: 1) care must be taken to protect reliable applications from any faults occurring to applications running in high performance mode, and 2) the desire to execute additional independent software threads for a performance application complicates the scheduling of computation to cores. After solving these issues, an MMM is shown to improve overall system performance, compared to a traditional DMR system, by approximately 2X when one reliable and one performance application are concurrently executing.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508265},
 doi = {http://doi.acm.org/10.1145/1508284.1508265},
 acmid = {1508265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dual-modular redundancy, multicore},
} 

@article{Rajamani:2009:IDE:1508284.1508266,
 author = {Rajamani, Sriram and Ramalingam, G. and Ranganath, Venkatesh Prasad and Vaswani, Kapil},
 title = {ISOLATOR: dynamically ensuring isolation in comcurrent programs},
 abstract = {In this paper, we focus on concurrent programs that use locks to achieve isolation of data accessed by critical sections of code. We present ISOLATOR, an algorithm that guarantees isolation for well-behaved threads of a program that obey a locking discipline even in the presence of ill-behaved threads that disobey the locking discipline. ISOLATOR uses code instrumentation, data replication, and virtual memory protection to detect isolation violations and delays ill-behaved threads to ensure isolation. Our instrumentation scheme requires access only to the code of well-behaved threads. We have evaluated ISOLATOR on several benchmark programs and found that ISOLATOR can ensure isolation with reasonable runtime overheads. In addition, we present three general desiderata - safety, isolation, and permissiveness - for any scheme that attempts to ensure isolation, and formally prove that ISOLATOR satisfies all of these desiderata.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508266},
 doi = {http://doi.acm.org/10.1145/1508284.1508266},
 acmid = {1508266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurreny, isolation, memory protection},
} 

@inproceedings{Rajamani:2009:IDE:1508244.1508266,
 author = {Rajamani, Sriram and Ramalingam, G. and Ranganath, Venkatesh Prasad and Vaswani, Kapil},
 title = {ISOLATOR: dynamically ensuring isolation in comcurrent programs},
 abstract = {In this paper, we focus on concurrent programs that use locks to achieve isolation of data accessed by critical sections of code. We present ISOLATOR, an algorithm that guarantees isolation for well-behaved threads of a program that obey a locking discipline even in the presence of ill-behaved threads that disobey the locking discipline. ISOLATOR uses code instrumentation, data replication, and virtual memory protection to detect isolation violations and delays ill-behaved threads to ensure isolation. Our instrumentation scheme requires access only to the code of well-behaved threads. We have evaluated ISOLATOR on several benchmark programs and found that ISOLATOR can ensure isolation with reasonable runtime overheads. In addition, we present three general desiderata - safety, isolation, and permissiveness - for any scheme that attempts to ensure isolation, and formally prove that ISOLATOR satisfies all of these desiderata.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508266},
 doi = {http://doi.acm.org/10.1145/1508244.1508266},
 acmid = {1508266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurreny, isolation, memory protection},
} 

@inproceedings{Tucek:2009:EOV:1508244.1508267,
 author = {Tucek, Joseph and Xiong, Weiwei and Zhou, Yuanyuan},
 title = {Efficient online validation with delta execution},
 abstract = {Software systems are constantly changing. Patches to fix bugs and patches to add features are all too common. Every change risks breaking a previously working system. Hence administrators loathe change, and are willing to delay even critical security patches until after fully validating their correctness. Compared to off-line validation, on-line validation has clear advantages since it tests against real life workloads. Yet unfortunately it imposes restrictive overheads as it requires running the old and new versions side-by-side. Moreover, due to spurious differences (e.g. event timing, random number generation, and thread interleavings), it is difficult to compare the two for validation. To allow more effective on-line patch validation, we propose a new mechanism, called delta execution, that is based on the observation that most patches are small. Delta execution merges the two side-by-side executions for most of the time and splits only when necessary, such as when they access different data or execute different code. This allows us to perform on-line validation not only with lower overhead but also with greatly reduced spurious differences, allowing us to effectively validate changes. We first validate the feasibility of our idea by studying the characteristics of 240 patches from 4 server programs; our examination shows that 77\% of the changes should not be expected to cause large changes and are thereby feasible for Delta execution. We then implemented Delta execution using dynamic instrumentation. Using real world patches from 7 server applications and 3 other programs, we compared our implementation of Delta execution against a traditional side-by-side on-line validation. Delta execution outperformed traditional validation by up to 128\%; further, for 3 of the changes, spurious differences caused the traditional validation to fail completely while Delta execution succeeded. This demonstrates that Delta execution can allow administrators to use on-line validation to confidently ensure the correctness of the changes they apply.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508267},
 doi = {http://doi.acm.org/10.1145/1508244.1508267},
 acmid = {1508267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delta execution, patch validation, testing},
} 

@article{Tucek:2009:EOV:1508284.1508267,
 author = {Tucek, Joseph and Xiong, Weiwei and Zhou, Yuanyuan},
 title = {Efficient online validation with delta execution},
 abstract = {Software systems are constantly changing. Patches to fix bugs and patches to add features are all too common. Every change risks breaking a previously working system. Hence administrators loathe change, and are willing to delay even critical security patches until after fully validating their correctness. Compared to off-line validation, on-line validation has clear advantages since it tests against real life workloads. Yet unfortunately it imposes restrictive overheads as it requires running the old and new versions side-by-side. Moreover, due to spurious differences (e.g. event timing, random number generation, and thread interleavings), it is difficult to compare the two for validation. To allow more effective on-line patch validation, we propose a new mechanism, called delta execution, that is based on the observation that most patches are small. Delta execution merges the two side-by-side executions for most of the time and splits only when necessary, such as when they access different data or execute different code. This allows us to perform on-line validation not only with lower overhead but also with greatly reduced spurious differences, allowing us to effectively validate changes. We first validate the feasibility of our idea by studying the characteristics of 240 patches from 4 server programs; our examination shows that 77\% of the changes should not be expected to cause large changes and are thereby feasible for Delta execution. We then implemented Delta execution using dynamic instrumentation. Using real world patches from 7 server applications and 3 other programs, we compared our implementation of Delta execution against a traditional side-by-side on-line validation. Delta execution outperformed traditional validation by up to 128\%; further, for 3 of the changes, spurious differences caused the traditional validation to fail completely while Delta execution succeeded. This demonstrates that Delta execution can allow administrators to use on-line validation to confidently ensure the correctness of the changes they apply.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508267},
 doi = {http://doi.acm.org/10.1145/1508284.1508267},
 acmid = {1508267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delta execution, patch validation, testing},
} 

@article{Meisner:2009:PES:1508284.1508269,
 author = {Meisner, David and Gold, Brian T. and Wenisch, Thomas F.},
 title = {PowerNap: eliminating server idle power},
 abstract = {Data center power consumption is growing to unprecedented levels: the EPA estimates U.S. data centers will consume 100 billion kilowatt hours annually by 2011. Much of this energy is wasted in idle systems: in typical deployments, server utilization is below 30\%, but idle servers still consume 60\% of their peak power draw. Typical idle periods though frequent--last seconds or less, confounding simple energy-conservation approaches. In this paper, we propose PowerNap, an energy-conservation approach where the entire system transitions rapidly between a high-performance active state and a near-zero-power idle state in response to instantaneous load. Rather than requiring fine-grained power-performance states and complex load-proportional operation from each system component, PowerNap instead calls for minimizing idle power and transition time, which are simpler optimization goals. Based on the PowerNap concept, we develop requirements and outline mechanisms to eliminate idle power waste in enterprise blade servers. Because PowerNap operates in low-efficiency regions of current blade center power supplies, we introduce the Redundant Array for Inexpensive Load Sharing (RAILS), a power provisioning approach that provides high conversion efficiency across the entire range of PowerNap's power demands. Using utilization traces collected from enterprise-scale commercial deployments, we demonstrate that, together, PowerNap and RAILS reduce average server power consumption by 74\%.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508269},
 doi = {http://doi.acm.org/10.1145/1508284.1508269},
 acmid = {1508269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {power management, servers},
} 

@inproceedings{Meisner:2009:PES:1508244.1508269,
 author = {Meisner, David and Gold, Brian T. and Wenisch, Thomas F.},
 title = {PowerNap: eliminating server idle power},
 abstract = {Data center power consumption is growing to unprecedented levels: the EPA estimates U.S. data centers will consume 100 billion kilowatt hours annually by 2011. Much of this energy is wasted in idle systems: in typical deployments, server utilization is below 30\%, but idle servers still consume 60\% of their peak power draw. Typical idle periods though frequent--last seconds or less, confounding simple energy-conservation approaches. In this paper, we propose PowerNap, an energy-conservation approach where the entire system transitions rapidly between a high-performance active state and a near-zero-power idle state in response to instantaneous load. Rather than requiring fine-grained power-performance states and complex load-proportional operation from each system component, PowerNap instead calls for minimizing idle power and transition time, which are simpler optimization goals. Based on the PowerNap concept, we develop requirements and outline mechanisms to eliminate idle power waste in enterprise blade servers. Because PowerNap operates in low-efficiency regions of current blade center power supplies, we introduce the Redundant Array for Inexpensive Load Sharing (RAILS), a power provisioning approach that provides high conversion efficiency across the entire range of PowerNap's power demands. Using utilization traces collected from enterprise-scale commercial deployments, we demonstrate that, together, PowerNap and RAILS reduce average server power consumption by 74\%.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508269},
 doi = {http://doi.acm.org/10.1145/1508244.1508269},
 acmid = {1508269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {power management, servers},
} 

@article{Caulfield:2009:GUF:1508284.1508270,
 author = {Caulfield, Adrian M. and Grupp, Laura M. and Swanson, Steven},
 title = {Gordon: using flash memory to build fast, power-efficient clusters for data-intensive applications},
 abstract = {As our society becomes more information-driven, we have begun to amass data at an astounding and accelerating rate. At the same time, power concerns have made it difficult to bring the necessary processing power to bear on querying, processing, and understanding this data. We describe Gordon, a system architecture for data-centric applications that combines low-power processors, flash memory, and data-centric programming systems to improve performance for data-centric applications while reducing power consumption. The paper presents an exhaustive analysis of the design space of Gordon systems, focusing on the trade-offs between power, energy, and performance that Gordon must make. It analyzes the impact of flash-storage and the Gordon architecture on the performance and power efficiency of data-centric applications. It also describes a novel flash translation layer tailored to data intensive workloads and large flash storage arrays. Our data show that, using technologies available in the near future, Gordon systems can out-perform disk-based clusters by 1.5x and deliver up to 2.5x more performance per Watt.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508270},
 doi = {http://doi.acm.org/10.1145/1508284.1508270},
 acmid = {1508270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster architecture, data centric, flash memory, solid-state storage},
} 

@inproceedings{Caulfield:2009:GUF:1508244.1508270,
 author = {Caulfield, Adrian M. and Grupp, Laura M. and Swanson, Steven},
 title = {Gordon: using flash memory to build fast, power-efficient clusters for data-intensive applications},
 abstract = {As our society becomes more information-driven, we have begun to amass data at an astounding and accelerating rate. At the same time, power concerns have made it difficult to bring the necessary processing power to bear on querying, processing, and understanding this data. We describe Gordon, a system architecture for data-centric applications that combines low-power processors, flash memory, and data-centric programming systems to improve performance for data-centric applications while reducing power consumption. The paper presents an exhaustive analysis of the design space of Gordon systems, focusing on the trade-offs between power, energy, and performance that Gordon must make. It analyzes the impact of flash-storage and the Gordon architecture on the performance and power efficiency of data-centric applications. It also describes a novel flash translation layer tailored to data intensive workloads and large flash storage arrays. Our data show that, using technologies available in the near future, Gordon systems can out-perform disk-based clusters by 1.5x and deliver up to 2.5x more performance per Watt.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508270},
 doi = {http://doi.acm.org/10.1145/1508244.1508270},
 acmid = {1508270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster architecture, data centric, flash memory, solid-state storage},
} 

@article{Gupta:2009:DFT:1508284.1508271,
 author = {Gupta, Aayush and Kim, Youngjae and Urgaonkar, Bhuvan},
 title = {DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings},
 abstract = {Recent technological advances in the development of flash-memory based devices have consolidated their leadership position as the preferred storage media in the embedded systems market and opened new vistas for deployment in enterprise-scale storage systems. Unlike hard disks, flash devices are free from any mechanical moving parts, have no seek or rotational delays and consume lower power. However, the internal idiosyncrasies of flash technology make its performance highly dependent on workload characteristics. The poor performance of random writes has been a cause of major concern, which needs to be addressed to better utilize the potential of flash in enterprise-scale environments. We examine one of the important causes of this poor performance: the design of the Flash Translation Layer (FTL), which performs the virtual-to-physical address translations and hides the erase-before-write characteristics of flash. We propose a complete paradigm shift in the design of the core FTL engine from the existing techniques with our Demand-based Flash Translation Layer (DFTL), which selectively caches page-level address mappings. We develop a flash simulation framework called FlashSim. Our experimental evaluation with realistic enterprise-scale workloads endorses the utility of DFTL in enterprise-scale storage systems by demonstrating: (i) improved performance, (ii) reduced garbage collection overhead and (iii) better overload behavior compared to state-of-the-art FTL schemes. For example, a predominantly random-write dominant I/O trace from an OLTP application running at a large financial institution shows a 78\% improvement in average response time (due to a 3-fold reduction in operations of the garbage collector), compared to a state-of-the-art FTL scheme. Even for the well-known read-dominant TPC-H benchmark, for which DFTL introduces additional overheads, we improve system response time by 56\%.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508271},
 doi = {http://doi.acm.org/10.1145/1508284.1508271},
 acmid = {1508271},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flash management, flash translation layer, storage system},
} 

@inproceedings{Gupta:2009:DFT:1508244.1508271,
 author = {Gupta, Aayush and Kim, Youngjae and Urgaonkar, Bhuvan},
 title = {DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings},
 abstract = {Recent technological advances in the development of flash-memory based devices have consolidated their leadership position as the preferred storage media in the embedded systems market and opened new vistas for deployment in enterprise-scale storage systems. Unlike hard disks, flash devices are free from any mechanical moving parts, have no seek or rotational delays and consume lower power. However, the internal idiosyncrasies of flash technology make its performance highly dependent on workload characteristics. The poor performance of random writes has been a cause of major concern, which needs to be addressed to better utilize the potential of flash in enterprise-scale environments. We examine one of the important causes of this poor performance: the design of the Flash Translation Layer (FTL), which performs the virtual-to-physical address translations and hides the erase-before-write characteristics of flash. We propose a complete paradigm shift in the design of the core FTL engine from the existing techniques with our Demand-based Flash Translation Layer (DFTL), which selectively caches page-level address mappings. We develop a flash simulation framework called FlashSim. Our experimental evaluation with realistic enterprise-scale workloads endorses the utility of DFTL in enterprise-scale storage systems by demonstrating: (i) improved performance, (ii) reduced garbage collection overhead and (iii) better overload behavior compared to state-of-the-art FTL schemes. For example, a predominantly random-write dominant I/O trace from an OLTP application running at a large financial institution shows a 78\% improvement in average response time (due to a 3-fold reduction in operations of the garbage collector), compared to a state-of-the-art FTL scheme. Even for the well-known read-dominant TPC-H benchmark, for which DFTL introduces additional overheads, we improve system response time by 56\%.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508271},
 doi = {http://doi.acm.org/10.1145/1508244.1508271},
 acmid = {1508271},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flash management, flash translation layer, storage system},
} 

@inproceedings{Aleen:2009:CAS:1508244.1508273,
 author = {Aleen, Farhana and Clark, Nathan},
 title = {Commutativity analysis for software parallelization: letting program transformations see the big picture},
 abstract = {Extracting performance from many-core architectures requires software engineers to create multi-threaded applications, which significantly complicates the already daunting task of software development. One solution to this problem is automatic compile-time parallelization, which can ease the burden on software developers in many situations. Clearly, automatic parallelization in its present form is not suitable for many application domains and new compiler analyses are needed address its shortcomings. In this paper, we present one such analysis: a new approach for detecting commutative functions. Commutative functions are sections of code that can be executed in any order without affecting the outcome of the application, e.g., inserting elements into a set. Previous research on this topic had one significant limitation, in that the results of a commutative functions must produce identical memory layouts. This prevented previous techniques from detecting functions like malloc, which may return different pointers depending on the order in which it is called, but these differing results do not affect the overall output of the application. Our new commutativity analysis correctly identify these situations to better facilitate automatic parallelization. We demonstrate that this analysis can automatically extract significant amounts of parallelism from many applications, and where it is ineffective it can provide software developers a useful list of functions that may be commutative provided semantic program changes that are not automatable.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508273},
 doi = {http://doi.acm.org/10.1145/1508244.1508273},
 acmid = {1508273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic software parallelization, commutative functions, random interpretation},
} 

@article{Aleen:2009:CAS:1508284.1508273,
 author = {Aleen, Farhana and Clark, Nathan},
 title = {Commutativity analysis for software parallelization: letting program transformations see the big picture},
 abstract = {Extracting performance from many-core architectures requires software engineers to create multi-threaded applications, which significantly complicates the already daunting task of software development. One solution to this problem is automatic compile-time parallelization, which can ease the burden on software developers in many situations. Clearly, automatic parallelization in its present form is not suitable for many application domains and new compiler analyses are needed address its shortcomings. In this paper, we present one such analysis: a new approach for detecting commutative functions. Commutative functions are sections of code that can be executed in any order without affecting the outcome of the application, e.g., inserting elements into a set. Previous research on this topic had one significant limitation, in that the results of a commutative functions must produce identical memory layouts. This prevented previous techniques from detecting functions like malloc, which may return different pointers depending on the order in which it is called, but these differing results do not affect the overall output of the application. Our new commutativity analysis correctly identify these situations to better facilitate automatic parallelization. We demonstrate that this analysis can automatically extract significant amounts of parallelism from many applications, and where it is ineffective it can provide software developers a useful list of functions that may be commutative provided semantic program changes that are not automatable.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508273},
 doi = {http://doi.acm.org/10.1145/1508284.1508273},
 acmid = {1508273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic software parallelization, commutative functions, random interpretation},
} 

@article{Suleman:2009:ACS:1508284.1508274,
 author = {Suleman, M. Aater and Mutlu, Onur and Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Accelerating critical section execution with asymmetric multi-core architectures},
 abstract = {To improve the performance of a single application on Chip Multiprocessors (CMPs), the application must be split into threads which execute concurrently on multiple cores. In multi-threaded applications, critical sections are used to ensure that only one thread accesses shared data at any given time. Critical sections can serialize the execution of threads, which significantly reduces performance and scalability. This paper proposes Accelerated Critical Sections (ACS), a technique that leverages the high-performance core(s) of an Asymmetric Chip Multiprocessor (ACMP) to accelerate the execution of critical sections. In ACS, selected critical sections are executed by a high-performance core, which can execute the critical section faster than the other, smaller cores. As a result, ACS reduces serialization: it lowers the likelihood of threads waiting for a critical section to finish. Our evaluation on a set of 12 critical-section-intensive workloads shows that ACS reduces the average execution time by 34\% compared to an equal-area 32T-core symmetric CMP and by 23\% compared to an equal-area ACMP. Moreover, for 7 out of the 12 workloads, ACS improves scalability by increasing the number of threads at which performance saturates.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508274},
 doi = {http://doi.acm.org/10.1145/1508284.1508274},
 acmid = {1508274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cmp, critical sections, heterogeneous cores, locks, multi-core, parallel programming},
} 

@inproceedings{Suleman:2009:ACS:1508244.1508274,
 author = {Suleman, M. Aater and Mutlu, Onur and Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Accelerating critical section execution with asymmetric multi-core architectures},
 abstract = {To improve the performance of a single application on Chip Multiprocessors (CMPs), the application must be split into threads which execute concurrently on multiple cores. In multi-threaded applications, critical sections are used to ensure that only one thread accesses shared data at any given time. Critical sections can serialize the execution of threads, which significantly reduces performance and scalability. This paper proposes Accelerated Critical Sections (ACS), a technique that leverages the high-performance core(s) of an Asymmetric Chip Multiprocessor (ACMP) to accelerate the execution of critical sections. In ACS, selected critical sections are executed by a high-performance core, which can execute the critical section faster than the other, smaller cores. As a result, ACS reduces serialization: it lowers the likelihood of threads waiting for a critical section to finish. Our evaluation on a set of 12 critical-section-intensive workloads shows that ACS reduces the average execution time by 34\% compared to an equal-area 32T-core symmetric CMP and by 23\% compared to an equal-area ACMP. Moreover, for 7 out of the 12 workloads, ACS improves scalability by increasing the number of threads at which performance saturates.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508274},
 doi = {http://doi.acm.org/10.1145/1508244.1508274},
 acmid = {1508274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cmp, critical sections, heterogeneous cores, locks, multi-core, parallel programming},
} 

@article{Mytkowicz:2009:PWD:1508284.1508275,
 author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
 title = {Producing wrong data without doing anything obviously wrong!},
 abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508275},
 doi = {http://doi.acm.org/10.1145/1508284.1508275},
 acmid = {1508275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bias, measurement, performance},
} 

@inproceedings{Mytkowicz:2009:PWD:1508244.1508275,
 author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
 title = {Producing wrong data without doing anything obviously wrong!},
 abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508275},
 doi = {http://doi.acm.org/10.1145/1508244.1508275},
 acmid = {1508275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bias, measurement, performance},
} 

@article{Bond:2009:LP:1508284.1508277,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Leak pruning},
 abstract = {Managed languages improve programmer productivity with type safety and garbage collection, which eliminate memory errors such as dangling pointers, double frees, and buffer overflows. However, because garbage collection uses reachability to over-approximate live objects, programs may still leak memory if programmers forget to eliminate the last reference to an object that will not be used again. Leaks slow programs by increasing collector workload and frequency. Growing leaks eventually crash programs. This paper introduces leak pruning, which keeps programs running by predicting and reclaiming leaked objects at run time. It predicts dead objects and reclaims them based on observing data structure usage patterns. Leak pruning preserves semantics because it waits for heap exhaustion before reclaiming objects and poisons references to objects it reclaims. If the program later tries to access a poisoned reference, the virtual machine (VM) throws an error. We show leak pruning has low overhead in a Java VM and evaluate it on 10 leaking programs. Leak pruning does not help two programs, executes five substantial programs 1.6-81X longer, and executes three programs, including a leak in Eclipse, for at least 24 hours. In the worst case, leak pruning defers fatal errors. In the best case, it keeps leaky programs running with preserved semantics and consistent throughput.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508277},
 doi = {http://doi.acm.org/10.1145/1508284.1508277},
 acmid = {1508277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, leak tolerance, managed languages, memory leaks},
} 

@inproceedings{Bond:2009:LP:1508244.1508277,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Leak pruning},
 abstract = {Managed languages improve programmer productivity with type safety and garbage collection, which eliminate memory errors such as dangling pointers, double frees, and buffer overflows. However, because garbage collection uses reachability to over-approximate live objects, programs may still leak memory if programmers forget to eliminate the last reference to an object that will not be used again. Leaks slow programs by increasing collector workload and frequency. Growing leaks eventually crash programs. This paper introduces leak pruning, which keeps programs running by predicting and reclaiming leaked objects at run time. It predicts dead objects and reclaims them based on observing data structure usage patterns. Leak pruning preserves semantics because it waits for heap exhaustion before reclaiming objects and poisons references to objects it reclaims. If the program later tries to access a poisoned reference, the virtual machine (VM) throws an error. We show leak pruning has low overhead in a Java VM and evaluate it on 10 leaking programs. Leak pruning does not help two programs, executes five substantial programs 1.6-81X longer, and executes three programs, including a leak in Eclipse, for at least 24 hours. In the worst case, leak pruning defers fatal errors. In the best case, it keeps leaky programs running with preserved semantics and consistent throughput.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508277},
 doi = {http://doi.acm.org/10.1145/1508244.1508277},
 acmid = {1508277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, leak tolerance, managed languages, memory leaks},
} 

@inproceedings{Wegiel:2009:DPC:1508244.1508278,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {Dynamic prediction of collection yield for managed runtimes},
 abstract = {The growth in complexity of modern systems makes it increasingly difficult to extract high-performance. The software stacks for such systems typically consist of multiple layers and include managed runtime environments (MREs). In this paper, we investigate techniques to improve cooperation between these layers and the hardware to increase the efficacy of automatic memory management in MREs. General-purpose MREs commonly implement parallel and/or concurrent garbage collection and employ compaction to eliminate heap fragmentation. Moreover, most systems trigger collection based on the amount of heap a program uses. Our analysis shows that in many cases this strategy leads to ineffective collections that are unable to reclaim sufficient space to justify the incurred cost. To avoid such collections, we exploit the observation that dead objects tend to cluster together and form large, never-referenced, regions in the address space that correlate well with virtual pages that have not recently been referenced by the application. We leverage this correlation to design a new, simple and light-weight, yield predictor that estimates the amount of reclaimable space in the heap using hardware page reference bits. Our predictor allows MREs to avoid low-yield collections and thereby improve resource management. We integrate this predictor into three state-of-the-art parallel compactors, implemented in the HotSpot JVM, that represent distinct canonical heap layouts. Our empirical evaluation, based on standard Java benchmarks and open-source applications, indicates that inexpensive and accurate yield prediction can improve performance significantly.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508278},
 doi = {http://doi.acm.org/10.1145/1508244.1508278},
 acmid = {1508278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, concurrent, garbage collection, operating system, parallel, reference bits, yield prediction},
} 

@article{Wegiel:2009:DPC:1508284.1508278,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {Dynamic prediction of collection yield for managed runtimes},
 abstract = {The growth in complexity of modern systems makes it increasingly difficult to extract high-performance. The software stacks for such systems typically consist of multiple layers and include managed runtime environments (MREs). In this paper, we investigate techniques to improve cooperation between these layers and the hardware to increase the efficacy of automatic memory management in MREs. General-purpose MREs commonly implement parallel and/or concurrent garbage collection and employ compaction to eliminate heap fragmentation. Moreover, most systems trigger collection based on the amount of heap a program uses. Our analysis shows that in many cases this strategy leads to ineffective collections that are unable to reclaim sufficient space to justify the incurred cost. To avoid such collections, we exploit the observation that dead objects tend to cluster together and form large, never-referenced, regions in the address space that correlate well with virtual pages that have not recently been referenced by the application. We leverage this correlation to design a new, simple and light-weight, yield predictor that estimates the amount of reclaimable space in the heap using hardware page reference bits. Our predictor allows MREs to avoid low-yield collections and thereby improve resource management. We integrate this predictor into three state-of-the-art parallel compactors, implemented in the HotSpot JVM, that represent distinct canonical heap layouts. Our empirical evaluation, based on standard Java benchmarks and open-source applications, indicates that inexpensive and accurate yield prediction can improve performance significantly.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508278},
 doi = {http://doi.acm.org/10.1145/1508284.1508278},
 acmid = {1508278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, concurrent, garbage collection, operating system, parallel, reference bits, yield prediction},
} 

@article{Menon:2009:TSD:1508284.1508279,
 author = {Menon, Aravind and Schubert, Simon and Zwaenepoel, Willy},
 title = {TwinDrivers: semi-automatic derivation of fast and safe hypervisor network drivers from guest OS drivers},
 abstract = {In a virtualized environment, device drivers are often run inside a virtual machine (VM) rather than in the hypervisor, for reasons of safety and reduction in software engineering effort. Unfortunately, this approach results in poor performance for I/O-intensive devices such as network cards. The alternative approach of running device drivers directly in the hypervisor yields better performance, but results in the loss of safety guarantees for the hypervisor and incurs additional software engineering costs. In this paper we present TwinDrivers, a framework which allows us to semi-automatically create safe and efficient hypervisor drivers from guest OS drivers. The hypervisor driver runs directly in the hypervisor, but its data resides completely in the driver VM address space. A Software Virtual Memory mechanism allows the driver to access its VM data efficiently from the hypervisor running in any guest context, and also protects the hypervisor from invalid memory accesses from the driver. An upcall mechanism allows the hypervisor to largely reuse the driver support infrastructure present in the VM. The TwinDriver system thus combines most of the performance benefits of hypervisor-based driver approaches with the safety and software engineering benefits of VM-based driver approaches. Using the TwinDrivers hypervisor driver, we are able to improve the guest domain networking throughput in Xen by a factor of 2.4 for transmit workloads, and 2.1 for receive workloads, both in CPU-scaled units, and achieve close to 64-67 of native Linux throughput.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508279},
 doi = {http://doi.acm.org/10.1145/1508284.1508279},
 acmid = {1508279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {virtualization},
} 

@inproceedings{Menon:2009:TSD:1508244.1508279,
 author = {Menon, Aravind and Schubert, Simon and Zwaenepoel, Willy},
 title = {TwinDrivers: semi-automatic derivation of fast and safe hypervisor network drivers from guest OS drivers},
 abstract = {In a virtualized environment, device drivers are often run inside a virtual machine (VM) rather than in the hypervisor, for reasons of safety and reduction in software engineering effort. Unfortunately, this approach results in poor performance for I/O-intensive devices such as network cards. The alternative approach of running device drivers directly in the hypervisor yields better performance, but results in the loss of safety guarantees for the hypervisor and incurs additional software engineering costs. In this paper we present TwinDrivers, a framework which allows us to semi-automatically create safe and efficient hypervisor drivers from guest OS drivers. The hypervisor driver runs directly in the hypervisor, but its data resides completely in the driver VM address space. A Software Virtual Memory mechanism allows the driver to access its VM data efficiently from the hypervisor running in any guest context, and also protects the hypervisor from invalid memory accesses from the driver. An upcall mechanism allows the hypervisor to largely reuse the driver support infrastructure present in the VM. The TwinDriver system thus combines most of the performance benefits of hypervisor-based driver approaches with the safety and software engineering benefits of VM-based driver approaches. Using the TwinDrivers hypervisor driver, we are able to improve the guest domain networking throughput in Xen by a factor of 2.4 for transmit workloads, and 2.1 for receive workloads, both in CPU-scaled units, and achieve close to 64-67 of native Linux throughput.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508279},
 doi = {http://doi.acm.org/10.1145/1508244.1508279},
 acmid = {1508279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {virtualization},
} 

@inproceedings{Burcea:2009:PVB:1508244.1508281,
 author = {Burcea, Ioana and Moshovos, Andreas},
 title = {Phantom-BTB: a virtualized branch target buffer design},
 abstract = {Modern processors use branch target buffers (BTBs) to predict the target address of branches such that they can fetch ahead in the instruction stream increasing concurrency and performance. Ideally, BTBs would be sufficiently large to capture the entire working set of the application and sufficiently small for fast access and practical on-chip dedicated storage. Depending on the application, these requirements are at odds. This work introduces a BTB design that accommodates large instruction footprints without dedicating expensive onchip resources. In the proposed Phantom-BTB (PBTB) design, a conventional BTB is augmented with a virtual table that collects branch target information as the application runs. The virtual table does not have fixed dedicated storage. Instead, it is transparently allocated, on demand, in the on-chip caches, at cache line granularity. The entries in the virtual table are proactively prefetched and installed in the dedicated conventional BTB, thus, increasing its perceived capacity. Experimental results with commercial workloads under full-system simulation demonstrate that PBTB improves IPC performance over a 1K-entry BTB by 6.9\% on average and up to 12.7\%, with a storage overhead of only 8\%. Overall, the virtualized design performs within 1\% of a conventional 4K-entry, single-cycle access BTB, while the dedicated storage is 3.6 times smaller.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {313--324},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508281},
 doi = {http://doi.acm.org/10.1145/1508244.1508281},
 acmid = {1508281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch target buffer, predictor metadata prefetching, predictor virtualization},
} 

@article{Burcea:2009:PVB:1508284.1508281,
 author = {Burcea, Ioana and Moshovos, Andreas},
 title = {Phantom-BTB: a virtualized branch target buffer design},
 abstract = {Modern processors use branch target buffers (BTBs) to predict the target address of branches such that they can fetch ahead in the instruction stream increasing concurrency and performance. Ideally, BTBs would be sufficiently large to capture the entire working set of the application and sufficiently small for fast access and practical on-chip dedicated storage. Depending on the application, these requirements are at odds. This work introduces a BTB design that accommodates large instruction footprints without dedicating expensive onchip resources. In the proposed Phantom-BTB (PBTB) design, a conventional BTB is augmented with a virtual table that collects branch target information as the application runs. The virtual table does not have fixed dedicated storage. Instead, it is transparently allocated, on demand, in the on-chip caches, at cache line granularity. The entries in the virtual table are proactively prefetched and installed in the dedicated conventional BTB, thus, increasing its perceived capacity. Experimental results with commercial workloads under full-system simulation demonstrate that PBTB improves IPC performance over a 1K-entry BTB by 6.9\% on average and up to 12.7\%, with a storage overhead of only 8\%. Overall, the virtualized design performs within 1\% of a conventional 4K-entry, single-cycle access BTB, while the dedicated storage is 3.6 times smaller.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {313--324},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508281},
 doi = {http://doi.acm.org/10.1145/1508284.1508281},
 acmid = {1508281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch target buffer, predictor metadata prefetching, predictor virtualization},
} 

@article{Ramani:2009:SSF:1508284.1508282,
 author = {Ramani, Karthik and Gribble, Christiaan P. and Davis, Al},
 title = {StreamRay: a stream filtering architecture for coherent ray tracing},
 abstract = {The wide availability of commodity graphics processors has made real-time graphics an intrinsic component of the human/computer interface. These graphics cores accelerate the z-buffer algorithm and provide a highly interactive experience at a relatively low cost. However, many applications in entertainment, science, and industry require high quality lighting effects such as accurate shadows, reflection, and refraction. These effects can be difficult to achieve with z-buffer algorithms but are straightforward to implement using ray tracing. Although ray tracing is computationally more complex, the algorithm exhibits excellent scaling and parallelism properties. Nevertheless, ray tracing memory access patterns are difficult to predict and the parallelism speedup promise is therefore hard to achieve. This paper highlights a novel approach to ray tracing based on stream filtering and presents StreamRay, a multicore wide SIMD microarchitecture that delivers interactive frame rates of 15-32 frames/second for scenes of high geometric complexity and exhibits high utilization for SIMD widths ranging from eight to 16 elements. StreamRay consists of two main components: the ray engine, which is responsible for stream assembly and employs address generation units that generate addresses to form large SIMD vectors, and the filter engine, which implements the ray tracing operations with programmable accelerators. Results demonstrate that separating address and data processing reduces data movement and resource contention. Performance improves by 56\% while simultaneously providing 11.63\% power savings per accelerator core compared to a design which does not use separate resources for address and data computations.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {325--336},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508282},
 doi = {http://doi.acm.org/10.1145/1508284.1508282},
 acmid = {1508282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graphics processors, interactive rendering, ray tracing, wide simd architectures},
} 

@inproceedings{Ramani:2009:SSF:1508244.1508282,
 author = {Ramani, Karthik and Gribble, Christiaan P. and Davis, Al},
 title = {StreamRay: a stream filtering architecture for coherent ray tracing},
 abstract = {The wide availability of commodity graphics processors has made real-time graphics an intrinsic component of the human/computer interface. These graphics cores accelerate the z-buffer algorithm and provide a highly interactive experience at a relatively low cost. However, many applications in entertainment, science, and industry require high quality lighting effects such as accurate shadows, reflection, and refraction. These effects can be difficult to achieve with z-buffer algorithms but are straightforward to implement using ray tracing. Although ray tracing is computationally more complex, the algorithm exhibits excellent scaling and parallelism properties. Nevertheless, ray tracing memory access patterns are difficult to predict and the parallelism speedup promise is therefore hard to achieve. This paper highlights a novel approach to ray tracing based on stream filtering and presents StreamRay, a multicore wide SIMD microarchitecture that delivers interactive frame rates of 15-32 frames/second for scenes of high geometric complexity and exhibits high utilization for SIMD widths ranging from eight to 16 elements. StreamRay consists of two main components: the ray engine, which is responsible for stream assembly and employs address generation units that generate addresses to form large SIMD vectors, and the filter engine, which implements the ray tracing operations with programmable accelerators. Results demonstrate that separating address and data processing reduces data movement and resource contention. Performance improves by 56\% while simultaneously providing 11.63\% power savings per accelerator core compared to a design which does not use separate resources for address and data computations.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {325--336},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508282},
 doi = {http://doi.acm.org/10.1145/1508244.1508282},
 acmid = {1508282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graphics processors, interactive rendering, ray tracing, wide simd architectures},
} 

@article{Cameron:2009:ASS:1508284.1508283,
 author = {Cameron, Robert D. and Lin, Dan},
 title = {Architectural support for SWAR text processing with parallel bit streams: the inductive doubling principle},
 abstract = {Parallel bit stream algorithms exploit the SWAR (SIMD within a register) capabilities of commodity processors in high-performance text processing applications such as UTF-8 to UTF-16 transcoding, XML parsing, string search and regular expression matching. Direct architectural support for these algorithms in future SWAR instruction sets could further increase performance as well as simplifying the programming task. A set of simple SWAR instruction set extensions are proposed for this purpose based on the principle of systematic support for inductive doubling as an algorithmic technique. These extensions are shown to significantly reduce instruction count in core parallel bit stream algorithms, often providing a 3X or better improvement. The extensions are also shown to be useful for SWAR programming in other application areas, including providing a systematic treatment for horizontal operations. An implementation model for these extensions involves relatively simple circuitry added to the operand fetch components in a pipelined processor.},
 journal = {SIGPLAN Not.},
 volume = {44},
 issue = {3},
 month = {March},
 year = {2009},
 issn = {0362-1340},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508284.1508283},
 doi = {http://doi.acm.org/10.1145/1508284.1508283},
 acmid = {1508283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inductive doubling, parallel bit streams, swar},
} 

@inproceedings{Cameron:2009:ASS:1508244.1508283,
 author = {Cameron, Robert D. and Lin, Dan},
 title = {Architectural support for SWAR text processing with parallel bit streams: the inductive doubling principle},
 abstract = {Parallel bit stream algorithms exploit the SWAR (SIMD within a register) capabilities of commodity processors in high-performance text processing applications such as UTF-8 to UTF-16 transcoding, XML parsing, string search and regular expression matching. Direct architectural support for these algorithms in future SWAR instruction sets could further increase performance as well as simplifying the programming task. A set of simple SWAR instruction set extensions are proposed for this purpose based on the principle of systematic support for inductive doubling as an algorithmic technique. These extensions are shown to significantly reduce instruction count in core parallel bit stream algorithms, often providing a 3X or better improvement. The extensions are also shown to be useful for SWAR programming in other application areas, including providing a systematic treatment for horizontal operations. An implementation model for these extensions involves relatively simple circuitry added to the operand fetch components in a pipelined processor.},
 booktitle = {Proceeding of the 14th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS '09},
 year = {2009},
 isbn = {978-1-60558-406-5},
 location = {Washington, DC, USA},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1508244.1508283},
 doi = {http://doi.acm.org/10.1145/1508244.1508283},
 acmid = {1508283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inductive doubling, parallel bit streams, swar},
} 

@article{Winfree:2008:TMP:1353534.1346282,
 author = {Winfree, Erik},
 title = {Toward molecular programming with DNA},
 abstract = {
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1353534.1346282},
 doi = {http://doi.acm.org/10.1145/1353534.1346282},
 acmid = {1346282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, molecular programming},
} 

@article{Winfree:2008:TMP:1353535.1346282,
 author = {Winfree, Erik},
 title = {Toward molecular programming with DNA},
 abstract = {
},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1353535.1346282},
 doi = {http://doi.acm.org/10.1145/1353535.1346282},
 acmid = {1346282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, molecular programming},
} 

@inproceedings{Winfree:2008:TMP:1346281.1346282,
 author = {Winfree, Erik},
 title = {Toward molecular programming with DNA},
 abstract = {
},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1346281.1346282},
 doi = {http://doi.acm.org/10.1145/1346281.1346282},
 acmid = {1346282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, molecular programming},
} 

@article{Winfree:2008:TMP:1353536.1346282,
 author = {Winfree, Erik},
 title = {Toward molecular programming with DNA},
 abstract = {
},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1353536.1346282},
 doi = {http://doi.acm.org/10.1145/1353536.1346282},
 acmid = {1346282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, molecular programming},
} 

@article{Chen:2008:OVA:1353535.1346284,
 author = {Chen, Xiaoxin and Garfinkel, Tal and Lewis, E. Christopher and Subrahmanyam, Pratap and Waldspurger, Carl A. and Boneh, Dan and Dwoskin, Jeffrey and Ports, Dan R.K.},
 title = {Overshadow: a virtualization-based approach to retrofitting protection in commodity operating systems},
 abstract = {Commodity operating systems entrusted with securing sensitive data are remarkably large and complex, and consequently, frequently prone to compromise. To address this limitation, we introduce a virtual-machine-based system called Overshadow that protects the privacy and integrity of application data, even in the event of a total OScompromise. Overshadow presents an application with a normal view of its resources, but the OS with an encrypted view. This allows the operating system to carry out the complex task of managing an application's resources, without allowing it to read or modify them. Thus, Overshadow offers a last line of defense for application data. Overshadow builds on multi-shadowing, a novel mechanism that presents different views of "physical" memory, depending on the context performing the access. This primitive offers an additional dimension of protection beyond the hierarchical protection domains implemented by traditional operating systems and processor architectures. We present the design and implementation of Overshadow and show how its new protection semantics can be integrated with existing systems. Our design has been fully implemented and used to protect a wide range of unmodified legacy applications running on an unmodified Linux operating system. We evaluate the performance of our implementation, demonstrating that this approach is practical.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346284},
 doi = {http://doi.acm.org/10.1145/1353535.1346284},
 acmid = {1346284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VMM, cloaking, hypervisors, memory protection, multi-shadowing, operating systems, virtual machine monitors},
} 

@inproceedings{Chen:2008:OVA:1346281.1346284,
 author = {Chen, Xiaoxin and Garfinkel, Tal and Lewis, E. Christopher and Subrahmanyam, Pratap and Waldspurger, Carl A. and Boneh, Dan and Dwoskin, Jeffrey and Ports, Dan R.K.},
 title = {Overshadow: a virtualization-based approach to retrofitting protection in commodity operating systems},
 abstract = {Commodity operating systems entrusted with securing sensitive data are remarkably large and complex, and consequently, frequently prone to compromise. To address this limitation, we introduce a virtual-machine-based system called Overshadow that protects the privacy and integrity of application data, even in the event of a total OScompromise. Overshadow presents an application with a normal view of its resources, but the OS with an encrypted view. This allows the operating system to carry out the complex task of managing an application's resources, without allowing it to read or modify them. Thus, Overshadow offers a last line of defense for application data. Overshadow builds on multi-shadowing, a novel mechanism that presents different views of "physical" memory, depending on the context performing the access. This primitive offers an additional dimension of protection beyond the hierarchical protection domains implemented by traditional operating systems and processor architectures. We present the design and implementation of Overshadow and show how its new protection semantics can be integrated with existing systems. Our design has been fully implemented and used to protect a wide range of unmodified legacy applications running on an unmodified Linux operating system. We evaluate the performance of our implementation, demonstrating that this approach is practical.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346284},
 doi = {http://doi.acm.org/10.1145/1346281.1346284},
 acmid = {1346284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VMM, cloaking, hypervisors, memory protection, multi-shadowing, operating systems, virtual machine monitors},
} 

@article{Chen:2008:OVA:1353536.1346284,
 author = {Chen, Xiaoxin and Garfinkel, Tal and Lewis, E. Christopher and Subrahmanyam, Pratap and Waldspurger, Carl A. and Boneh, Dan and Dwoskin, Jeffrey and Ports, Dan R.K.},
 title = {Overshadow: a virtualization-based approach to retrofitting protection in commodity operating systems},
 abstract = {Commodity operating systems entrusted with securing sensitive data are remarkably large and complex, and consequently, frequently prone to compromise. To address this limitation, we introduce a virtual-machine-based system called Overshadow that protects the privacy and integrity of application data, even in the event of a total OScompromise. Overshadow presents an application with a normal view of its resources, but the OS with an encrypted view. This allows the operating system to carry out the complex task of managing an application's resources, without allowing it to read or modify them. Thus, Overshadow offers a last line of defense for application data. Overshadow builds on multi-shadowing, a novel mechanism that presents different views of "physical" memory, depending on the context performing the access. This primitive offers an additional dimension of protection beyond the hierarchical protection domains implemented by traditional operating systems and processor architectures. We present the design and implementation of Overshadow and show how its new protection semantics can be integrated with existing systems. Our design has been fully implemented and used to protect a wide range of unmodified legacy applications running on an unmodified Linux operating system. We evaluate the performance of our implementation, demonstrating that this approach is practical.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346284},
 doi = {http://doi.acm.org/10.1145/1353536.1346284},
 acmid = {1346284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VMM, cloaking, hypervisors, memory protection, multi-shadowing, operating systems, virtual machine monitors},
} 

@article{Chen:2008:OVA:1353534.1346284,
 author = {Chen, Xiaoxin and Garfinkel, Tal and Lewis, E. Christopher and Subrahmanyam, Pratap and Waldspurger, Carl A. and Boneh, Dan and Dwoskin, Jeffrey and Ports, Dan R.K.},
 title = {Overshadow: a virtualization-based approach to retrofitting protection in commodity operating systems},
 abstract = {Commodity operating systems entrusted with securing sensitive data are remarkably large and complex, and consequently, frequently prone to compromise. To address this limitation, we introduce a virtual-machine-based system called Overshadow that protects the privacy and integrity of application data, even in the event of a total OScompromise. Overshadow presents an application with a normal view of its resources, but the OS with an encrypted view. This allows the operating system to carry out the complex task of managing an application's resources, without allowing it to read or modify them. Thus, Overshadow offers a last line of defense for application data. Overshadow builds on multi-shadowing, a novel mechanism that presents different views of "physical" memory, depending on the context performing the access. This primitive offers an additional dimension of protection beyond the hierarchical protection domains implemented by traditional operating systems and processor architectures. We present the design and implementation of Overshadow and show how its new protection semantics can be integrated with existing systems. Our design has been fully implemented and used to protect a wide range of unmodified legacy applications running on an unmodified Linux operating system. We evaluate the performance of our implementation, demonstrating that this approach is practical.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346284},
 doi = {http://doi.acm.org/10.1145/1353534.1346284},
 acmid = {1346284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VMM, cloaking, hypervisors, memory protection, multi-shadowing, operating systems, virtual machine monitors},
} 

@article{McCune:2008:LYG:1353535.1346285,
 author = {McCune, Jonathan M. and Parno, Bryan and Perrig, Adrian and Reiter, Michael K. and Seshadri, Arvind},
 title = {How low can you go?: recommendations for hardware-supported minimal TCB code execution},
 abstract = {We explore the extent to which newly available CPU-based security technology can reduce the Trusted Computing Base (TCB) for security-sensitive applications. We find that although this new technology represents a step in the right direction, significant performance issues remain. We offer several suggestions that leverage existing processor technology, retain security, and improve performance. Implementing these recommendations will finally allow application developers to focus exclusively on the security of their own code, enabling it to execute in isolation from the numerous vulnerabilities in the underlying layers of legacy code.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346285},
 doi = {http://doi.acm.org/10.1145/1353535.1346285},
 acmid = {1346285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {late launch, secure execution, trusted computing},
} 

@article{McCune:2008:LYG:1353536.1346285,
 author = {McCune, Jonathan M. and Parno, Bryan and Perrig, Adrian and Reiter, Michael K. and Seshadri, Arvind},
 title = {How low can you go?: recommendations for hardware-supported minimal TCB code execution},
 abstract = {We explore the extent to which newly available CPU-based security technology can reduce the Trusted Computing Base (TCB) for security-sensitive applications. We find that although this new technology represents a step in the right direction, significant performance issues remain. We offer several suggestions that leverage existing processor technology, retain security, and improve performance. Implementing these recommendations will finally allow application developers to focus exclusively on the security of their own code, enabling it to execute in isolation from the numerous vulnerabilities in the underlying layers of legacy code.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346285},
 doi = {http://doi.acm.org/10.1145/1353536.1346285},
 acmid = {1346285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {late launch, secure execution, trusted computing},
} 

@article{McCune:2008:LYG:1353534.1346285,
 author = {McCune, Jonathan M. and Parno, Bryan and Perrig, Adrian and Reiter, Michael K. and Seshadri, Arvind},
 title = {How low can you go?: recommendations for hardware-supported minimal TCB code execution},
 abstract = {We explore the extent to which newly available CPU-based security technology can reduce the Trusted Computing Base (TCB) for security-sensitive applications. We find that although this new technology represents a step in the right direction, significant performance issues remain. We offer several suggestions that leverage existing processor technology, retain security, and improve performance. Implementing these recommendations will finally allow application developers to focus exclusively on the security of their own code, enabling it to execute in isolation from the numerous vulnerabilities in the underlying layers of legacy code.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346285},
 doi = {http://doi.acm.org/10.1145/1353534.1346285},
 acmid = {1346285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {late launch, secure execution, trusted computing},
} 

@inproceedings{McCune:2008:LYG:1346281.1346285,
 author = {McCune, Jonathan M. and Parno, Bryan and Perrig, Adrian and Reiter, Michael K. and Seshadri, Arvind},
 title = {How low can you go?: recommendations for hardware-supported minimal TCB code execution},
 abstract = {We explore the extent to which newly available CPU-based security technology can reduce the Trusted Computing Base (TCB) for security-sensitive applications. We find that although this new technology represents a step in the right direction, significant performance issues remain. We offer several suggestions that leverage existing processor technology, retain security, and improve performance. Implementing these recommendations will finally allow application developers to focus exclusively on the security of their own code, enabling it to execute in isolation from the numerous vulnerabilities in the underlying layers of legacy code.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346285},
 doi = {http://doi.acm.org/10.1145/1346281.1346285},
 acmid = {1346285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {late launch, secure execution, trusted computing},
} 

@article{Bhargava:2008:ATP:1353535.1346286,
 author = {Bhargava, Ravi and Serebrin, Benjamin and Spadini, Francesco and Manne, Srilatha},
 title = {Accelerating two-dimensional page walks for virtualized systems},
 abstract = {Nested paging is a hardware solution for alleviating the software memory management overhead imposed by system virtualization. Nested paging complements existing page walk hardware to form a two-dimensional (2D) page walk, which reduces the need for hypervisor intervention in guest page table management. However, the extra dimension also increases the maximum number of architecturally-required page table references. This paper presents an in-depth examination of the 2D page table walk overhead and options for decreasing it. These options include using the AMD Opteron processor's page walk cache to exploit the strong reuse of page entry references. For a mix of server and SPEC benchmarks, the presented results show a 15\%-38\% improvement in guest performance by extending the existing page walk cache to also store the nested dimension of the 2D page walk. Caching nested page table translations and skipping multiple page entry references produce an additional 3\%-7\% improvement. Much of the remaining 2D page walk overhead is due to low-locality nested page entry references, which result in additional memory hierarchy misses. By using large pages, the hypervisor can eliminate many of these long-latency accesses and further improve the guest performance by 3\%-22\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {26--35},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346286},
 doi = {http://doi.acm.org/10.1145/1353535.1346286},
 acmid = {1346286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AMD, TLB, hypervisor, memory management, nested paging, page walk caching, virtual machine monitor, virtualization},
} 

@article{Bhargava:2008:ATP:1353534.1346286,
 author = {Bhargava, Ravi and Serebrin, Benjamin and Spadini, Francesco and Manne, Srilatha},
 title = {Accelerating two-dimensional page walks for virtualized systems},
 abstract = {Nested paging is a hardware solution for alleviating the software memory management overhead imposed by system virtualization. Nested paging complements existing page walk hardware to form a two-dimensional (2D) page walk, which reduces the need for hypervisor intervention in guest page table management. However, the extra dimension also increases the maximum number of architecturally-required page table references. This paper presents an in-depth examination of the 2D page table walk overhead and options for decreasing it. These options include using the AMD Opteron processor's page walk cache to exploit the strong reuse of page entry references. For a mix of server and SPEC benchmarks, the presented results show a 15\%-38\% improvement in guest performance by extending the existing page walk cache to also store the nested dimension of the 2D page walk. Caching nested page table translations and skipping multiple page entry references produce an additional 3\%-7\% improvement. Much of the remaining 2D page walk overhead is due to low-locality nested page entry references, which result in additional memory hierarchy misses. By using large pages, the hypervisor can eliminate many of these long-latency accesses and further improve the guest performance by 3\%-22\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {26--35},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346286},
 doi = {http://doi.acm.org/10.1145/1353534.1346286},
 acmid = {1346286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AMD, TLB, hypervisor, memory management, nested paging, page walk caching, virtual machine monitor, virtualization},
} 

@article{Bhargava:2008:ATP:1353536.1346286,
 author = {Bhargava, Ravi and Serebrin, Benjamin and Spadini, Francesco and Manne, Srilatha},
 title = {Accelerating two-dimensional page walks for virtualized systems},
 abstract = {Nested paging is a hardware solution for alleviating the software memory management overhead imposed by system virtualization. Nested paging complements existing page walk hardware to form a two-dimensional (2D) page walk, which reduces the need for hypervisor intervention in guest page table management. However, the extra dimension also increases the maximum number of architecturally-required page table references. This paper presents an in-depth examination of the 2D page table walk overhead and options for decreasing it. These options include using the AMD Opteron processor's page walk cache to exploit the strong reuse of page entry references. For a mix of server and SPEC benchmarks, the presented results show a 15\%-38\% improvement in guest performance by extending the existing page walk cache to also store the nested dimension of the 2D page walk. Caching nested page table translations and skipping multiple page entry references produce an additional 3\%-7\% improvement. Much of the remaining 2D page walk overhead is due to low-locality nested page entry references, which result in additional memory hierarchy misses. By using large pages, the hypervisor can eliminate many of these long-latency accesses and further improve the guest performance by 3\%-22\%.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {26--35},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346286},
 doi = {http://doi.acm.org/10.1145/1353536.1346286},
 acmid = {1346286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AMD, TLB, hypervisor, memory management, nested paging, page walk caching, virtual machine monitor, virtualization},
} 

@inproceedings{Bhargava:2008:ATP:1346281.1346286,
 author = {Bhargava, Ravi and Serebrin, Benjamin and Spadini, Francesco and Manne, Srilatha},
 title = {Accelerating two-dimensional page walks for virtualized systems},
 abstract = {Nested paging is a hardware solution for alleviating the software memory management overhead imposed by system virtualization. Nested paging complements existing page walk hardware to form a two-dimensional (2D) page walk, which reduces the need for hypervisor intervention in guest page table management. However, the extra dimension also increases the maximum number of architecturally-required page table references. This paper presents an in-depth examination of the 2D page table walk overhead and options for decreasing it. These options include using the AMD Opteron processor's page walk cache to exploit the strong reuse of page entry references. For a mix of server and SPEC benchmarks, the presented results show a 15\%-38\% improvement in guest performance by extending the existing page walk cache to also store the nested dimension of the 2D page walk. Caching nested page table translations and skipping multiple page entry references produce an additional 3\%-7\% improvement. Much of the remaining 2D page walk overhead is due to low-locality nested page entry references, which result in additional memory hierarchy misses. By using large pages, the hypervisor can eliminate many of these long-latency accesses and further improve the guest performance by 3\%-22\%.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {26--35},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346286},
 doi = {http://doi.acm.org/10.1145/1346281.1346286},
 acmid = {1346286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AMD, TLB, hypervisor, memory management, nested paging, page walk caching, virtual machine monitor, virtualization},
} 

@article{Lee:2008:ETL:1353535.1346288,
 author = {Lee, Benjamin C. and Brooks, David},
 title = {Efficiency trends and limits from comprehensive microarchitectural adaptivity},
 abstract = {Increasing demand for power-efficient, high-performance computing requires tuning applications and/or the underlying hardware to improve the mapping between workload heterogeneity and computational resources. To assess the potential benefits of hardware tuning, we propose a framework that leverages synergistic interactions between recent advances in (a) sampling, (b) predictive modeling, and (c) optimization heuristics. This framework enables qualitatively new capabilities in analyzing the performance and power characteristics of adaptive microarchitectures. For the first time, we are able to simultaneously consider high temporal and comprehensive spatial adaptivity. In particular, we optimize efficiency for many, short adaptive intervals and identify the best configuration of 15 parameters, which define a space of 240B point. With frequent sub-application reconfiguration and a fully reconfigurable hardware substrate, adaptive microarchitectures achieve bips<sup>3</sup>/w</i> efficiency gains of up to 5.3x (median 2.4x) relative to their static counterparts already optimized for a given application. This 5.3x efficiency gain is derived from a 1.6x performance gain and 0.8x power reduction. Although several applications achieve a significant fraction of their potential efficiency with as few as three adaptive parameters, the three most significant parameters differ across applications. These differences motivate a hardware substrate capable of comprehensive adaptivity to meet these diverse application requirements.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346288},
 doi = {http://doi.acm.org/10.1145/1353535.1346288},
 acmid = {1346288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptivity, efficiency, inference, microarchitecture, performance, power, reconfigurablity, regression, simulation, statistics},
} 

@article{Lee:2008:ETL:1353534.1346288,
 author = {Lee, Benjamin C. and Brooks, David},
 title = {Efficiency trends and limits from comprehensive microarchitectural adaptivity},
 abstract = {Increasing demand for power-efficient, high-performance computing requires tuning applications and/or the underlying hardware to improve the mapping between workload heterogeneity and computational resources. To assess the potential benefits of hardware tuning, we propose a framework that leverages synergistic interactions between recent advances in (a) sampling, (b) predictive modeling, and (c) optimization heuristics. This framework enables qualitatively new capabilities in analyzing the performance and power characteristics of adaptive microarchitectures. For the first time, we are able to simultaneously consider high temporal and comprehensive spatial adaptivity. In particular, we optimize efficiency for many, short adaptive intervals and identify the best configuration of 15 parameters, which define a space of 240B point. With frequent sub-application reconfiguration and a fully reconfigurable hardware substrate, adaptive microarchitectures achieve bips<sup>3</sup>/w</i> efficiency gains of up to 5.3x (median 2.4x) relative to their static counterparts already optimized for a given application. This 5.3x efficiency gain is derived from a 1.6x performance gain and 0.8x power reduction. Although several applications achieve a significant fraction of their potential efficiency with as few as three adaptive parameters, the three most significant parameters differ across applications. These differences motivate a hardware substrate capable of comprehensive adaptivity to meet these diverse application requirements.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346288},
 doi = {http://doi.acm.org/10.1145/1353534.1346288},
 acmid = {1346288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptivity, efficiency, inference, microarchitecture, performance, power, reconfigurablity, regression, simulation, statistics},
} 

@inproceedings{Lee:2008:ETL:1346281.1346288,
 author = {Lee, Benjamin C. and Brooks, David},
 title = {Efficiency trends and limits from comprehensive microarchitectural adaptivity},
 abstract = {Increasing demand for power-efficient, high-performance computing requires tuning applications and/or the underlying hardware to improve the mapping between workload heterogeneity and computational resources. To assess the potential benefits of hardware tuning, we propose a framework that leverages synergistic interactions between recent advances in (a) sampling, (b) predictive modeling, and (c) optimization heuristics. This framework enables qualitatively new capabilities in analyzing the performance and power characteristics of adaptive microarchitectures. For the first time, we are able to simultaneously consider high temporal and comprehensive spatial adaptivity. In particular, we optimize efficiency for many, short adaptive intervals and identify the best configuration of 15 parameters, which define a space of 240B point. With frequent sub-application reconfiguration and a fully reconfigurable hardware substrate, adaptive microarchitectures achieve bips<sup>3</sup>/w</i> efficiency gains of up to 5.3x (median 2.4x) relative to their static counterparts already optimized for a given application. This 5.3x efficiency gain is derived from a 1.6x performance gain and 0.8x power reduction. Although several applications achieve a significant fraction of their potential efficiency with as few as three adaptive parameters, the three most significant parameters differ across applications. These differences motivate a hardware substrate capable of comprehensive adaptivity to meet these diverse application requirements.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346288},
 doi = {http://doi.acm.org/10.1145/1346281.1346288},
 acmid = {1346288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptivity, efficiency, inference, microarchitecture, performance, power, reconfigurablity, regression, simulation, statistics},
} 

@article{Lee:2008:ETL:1353536.1346288,
 author = {Lee, Benjamin C. and Brooks, David},
 title = {Efficiency trends and limits from comprehensive microarchitectural adaptivity},
 abstract = {Increasing demand for power-efficient, high-performance computing requires tuning applications and/or the underlying hardware to improve the mapping between workload heterogeneity and computational resources. To assess the potential benefits of hardware tuning, we propose a framework that leverages synergistic interactions between recent advances in (a) sampling, (b) predictive modeling, and (c) optimization heuristics. This framework enables qualitatively new capabilities in analyzing the performance and power characteristics of adaptive microarchitectures. For the first time, we are able to simultaneously consider high temporal and comprehensive spatial adaptivity. In particular, we optimize efficiency for many, short adaptive intervals and identify the best configuration of 15 parameters, which define a space of 240B point. With frequent sub-application reconfiguration and a fully reconfigurable hardware substrate, adaptive microarchitectures achieve bips<sup>3</sup>/w</i> efficiency gains of up to 5.3x (median 2.4x) relative to their static counterparts already optimized for a given application. This 5.3x efficiency gain is derived from a 1.6x performance gain and 0.8x power reduction. Although several applications achieve a significant fraction of their potential efficiency with as few as three adaptive parameters, the three most significant parameters differ across applications. These differences motivate a hardware substrate capable of comprehensive adaptivity to meet these diverse application requirements.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346288},
 doi = {http://doi.acm.org/10.1145/1353536.1346288},
 acmid = {1346288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptivity, efficiency, inference, microarchitecture, performance, power, reconfigurablity, regression, simulation, statistics},
} 

@article{Raghavendra:2008:NPS:1353534.1346289,
 author = {Raghavendra, Ramya and Ranganathan, Parthasarathy and Talwar, Vanish and Wang, Zhikui and Zhu, Xiaoyun},
 title = {No "power" struggles: coordinated multi-level power management for the data center},
 abstract = {Power delivery, electricity consumption, and heat management are becoming key challenges in data center environments. Several past solutions have individually evaluated different techniques to address separate aspects of this problem, in hardware and software, and at local and global levels. Unfortunately, there has been no corresponding work on coordinating all these solutions. In the absence of such coordination, these solutions are likely to interfere with one another, in unpredictable (and potentially dangerous) ways. This paper seeks to address this problem. We make two key contributions. First, we propose and validate a power management solution that coordinates different individual approaches. Using simulations based on 180 server traces from nine different real-world enterprises, we demonstrate the correctness, stability, and efficiency advantages of our solution. Second, using our unified architecture as the base, we perform a detailed quantitative sensitivity analysis and draw conclusions about the impact of different architectures, implementations, workloads, and system design choices.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {48--59},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346289},
 doi = {http://doi.acm.org/10.1145/1353534.1346289},
 acmid = {1346289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capping, control theory, coordination, data center, efficiency, power management, virtualization},
} 

@inproceedings{Raghavendra:2008:NPS:1346281.1346289,
 author = {Raghavendra, Ramya and Ranganathan, Parthasarathy and Talwar, Vanish and Wang, Zhikui and Zhu, Xiaoyun},
 title = {No "power" struggles: coordinated multi-level power management for the data center},
 abstract = {Power delivery, electricity consumption, and heat management are becoming key challenges in data center environments. Several past solutions have individually evaluated different techniques to address separate aspects of this problem, in hardware and software, and at local and global levels. Unfortunately, there has been no corresponding work on coordinating all these solutions. In the absence of such coordination, these solutions are likely to interfere with one another, in unpredictable (and potentially dangerous) ways. This paper seeks to address this problem. We make two key contributions. First, we propose and validate a power management solution that coordinates different individual approaches. Using simulations based on 180 server traces from nine different real-world enterprises, we demonstrate the correctness, stability, and efficiency advantages of our solution. Second, using our unified architecture as the base, we perform a detailed quantitative sensitivity analysis and draw conclusions about the impact of different architectures, implementations, workloads, and system design choices.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {48--59},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346289},
 doi = {http://doi.acm.org/10.1145/1346281.1346289},
 acmid = {1346289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capping, control theory, coordination, data center, efficiency, power management, virtualization},
} 

@article{Raghavendra:2008:NPS:1353536.1346289,
 author = {Raghavendra, Ramya and Ranganathan, Parthasarathy and Talwar, Vanish and Wang, Zhikui and Zhu, Xiaoyun},
 title = {No "power" struggles: coordinated multi-level power management for the data center},
 abstract = {Power delivery, electricity consumption, and heat management are becoming key challenges in data center environments. Several past solutions have individually evaluated different techniques to address separate aspects of this problem, in hardware and software, and at local and global levels. Unfortunately, there has been no corresponding work on coordinating all these solutions. In the absence of such coordination, these solutions are likely to interfere with one another, in unpredictable (and potentially dangerous) ways. This paper seeks to address this problem. We make two key contributions. First, we propose and validate a power management solution that coordinates different individual approaches. Using simulations based on 180 server traces from nine different real-world enterprises, we demonstrate the correctness, stability, and efficiency advantages of our solution. Second, using our unified architecture as the base, we perform a detailed quantitative sensitivity analysis and draw conclusions about the impact of different architectures, implementations, workloads, and system design choices.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {48--59},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346289},
 doi = {http://doi.acm.org/10.1145/1353536.1346289},
 acmid = {1346289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capping, control theory, coordination, data center, efficiency, power management, virtualization},
} 

@article{Raghavendra:2008:NPS:1353535.1346289,
 author = {Raghavendra, Ramya and Ranganathan, Parthasarathy and Talwar, Vanish and Wang, Zhikui and Zhu, Xiaoyun},
 title = {No "power" struggles: coordinated multi-level power management for the data center},
 abstract = {Power delivery, electricity consumption, and heat management are becoming key challenges in data center environments. Several past solutions have individually evaluated different techniques to address separate aspects of this problem, in hardware and software, and at local and global levels. Unfortunately, there has been no corresponding work on coordinating all these solutions. In the absence of such coordination, these solutions are likely to interfere with one another, in unpredictable (and potentially dangerous) ways. This paper seeks to address this problem. We make two key contributions. First, we propose and validate a power management solution that coordinates different individual approaches. Using simulations based on 180 server traces from nine different real-world enterprises, we demonstrate the correctness, stability, and efficiency advantages of our solution. Second, using our unified architecture as the base, we perform a detailed quantitative sensitivity analysis and draw conclusions about the impact of different architectures, implementations, workloads, and system design choices.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {48--59},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346289},
 doi = {http://doi.acm.org/10.1145/1353535.1346289},
 acmid = {1346289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capping, control theory, coordination, data center, efficiency, power management, virtualization},
} 

@inproceedings{Ballapuram:2008:EAS:1346281.1346290,
 author = {Ballapuram, Chinnakrishnan S. and Sharif, Ahmad and Lee, Hsien-Hsin S.},
 title = {Exploiting access semantics and program behavior to reduce snoop power in chip multiprocessors},
 abstract = {Integrating more processor cores on-die has become the unanimous trend in the microprocessor industry. Most of the current research thrusts using chip multiprocessors (CMPs) as the baseline to analyze problems in various domains. One of the main design issues facing CMP systems is the growing number of snoops required to maintain cache coherency and to support self/cross-modifying code that leads to power and performance limitations. In this paper, we analyze the internal and external snoop behavior in a CMP system and relax the snoopy cache coherence protocol based on the program semantics and properties of the shared variables for saving power. Based on the observations and analyses, we propose two novel techniques: Selective Snoop Probe (SSP) and Essential Snoop Probe (ESP) to reduce power without compromising performance. Our simulation results show that using the SSPtechnique, 5\% to 65\% data cache energy savings per core for different processor configurations can be achieved with 1\% to 2\% performance improvement. We also show that 5\% to 82\% of data cache energy per core is spent on the non-essential snoop probes that can be saved using the ESP technique.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346290},
 doi = {http://doi.acm.org/10.1145/1346281.1346290},
 acmid = {1346290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MESI protocol, chip multiprocessors, internal and external snoops, self-modifying code},
} 

@article{Ballapuram:2008:EAS:1353534.1346290,
 author = {Ballapuram, Chinnakrishnan S. and Sharif, Ahmad and Lee, Hsien-Hsin S.},
 title = {Exploiting access semantics and program behavior to reduce snoop power in chip multiprocessors},
 abstract = {Integrating more processor cores on-die has become the unanimous trend in the microprocessor industry. Most of the current research thrusts using chip multiprocessors (CMPs) as the baseline to analyze problems in various domains. One of the main design issues facing CMP systems is the growing number of snoops required to maintain cache coherency and to support self/cross-modifying code that leads to power and performance limitations. In this paper, we analyze the internal and external snoop behavior in a CMP system and relax the snoopy cache coherence protocol based on the program semantics and properties of the shared variables for saving power. Based on the observations and analyses, we propose two novel techniques: Selective Snoop Probe (SSP) and Essential Snoop Probe (ESP) to reduce power without compromising performance. Our simulation results show that using the SSPtechnique, 5\% to 65\% data cache energy savings per core for different processor configurations can be achieved with 1\% to 2\% performance improvement. We also show that 5\% to 82\% of data cache energy per core is spent on the non-essential snoop probes that can be saved using the ESP technique.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346290},
 doi = {http://doi.acm.org/10.1145/1353534.1346290},
 acmid = {1346290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MESI protocol, chip multiprocessors, internal and external snoops, self-modifying code},
} 

@article{Ballapuram:2008:EAS:1353536.1346290,
 author = {Ballapuram, Chinnakrishnan S. and Sharif, Ahmad and Lee, Hsien-Hsin S.},
 title = {Exploiting access semantics and program behavior to reduce snoop power in chip multiprocessors},
 abstract = {Integrating more processor cores on-die has become the unanimous trend in the microprocessor industry. Most of the current research thrusts using chip multiprocessors (CMPs) as the baseline to analyze problems in various domains. One of the main design issues facing CMP systems is the growing number of snoops required to maintain cache coherency and to support self/cross-modifying code that leads to power and performance limitations. In this paper, we analyze the internal and external snoop behavior in a CMP system and relax the snoopy cache coherence protocol based on the program semantics and properties of the shared variables for saving power. Based on the observations and analyses, we propose two novel techniques: Selective Snoop Probe (SSP) and Essential Snoop Probe (ESP) to reduce power without compromising performance. Our simulation results show that using the SSPtechnique, 5\% to 65\% data cache energy savings per core for different processor configurations can be achieved with 1\% to 2\% performance improvement. We also show that 5\% to 82\% of data cache energy per core is spent on the non-essential snoop probes that can be saved using the ESP technique.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346290},
 doi = {http://doi.acm.org/10.1145/1353536.1346290},
 acmid = {1346290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MESI protocol, chip multiprocessors, internal and external snoops, self-modifying code},
} 

@article{Ballapuram:2008:EAS:1353535.1346290,
 author = {Ballapuram, Chinnakrishnan S. and Sharif, Ahmad and Lee, Hsien-Hsin S.},
 title = {Exploiting access semantics and program behavior to reduce snoop power in chip multiprocessors},
 abstract = {Integrating more processor cores on-die has become the unanimous trend in the microprocessor industry. Most of the current research thrusts using chip multiprocessors (CMPs) as the baseline to analyze problems in various domains. One of the main design issues facing CMP systems is the growing number of snoops required to maintain cache coherency and to support self/cross-modifying code that leads to power and performance limitations. In this paper, we analyze the internal and external snoop behavior in a CMP system and relax the snoopy cache coherence protocol based on the program semantics and properties of the shared variables for saving power. Based on the observations and analyses, we propose two novel techniques: Selective Snoop Probe (SSP) and Essential Snoop Probe (ESP) to reduce power without compromising performance. Our simulation results show that using the SSPtechnique, 5\% to 65\% data cache energy savings per core for different processor configurations can be achieved with 1\% to 2\% performance improvement. We also show that 5\% to 82\% of data cache energy per core is spent on the non-essential snoop probes that can be saved using the ESP technique.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346290},
 doi = {http://doi.acm.org/10.1145/1353535.1346290},
 acmid = {1346290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MESI protocol, chip multiprocessors, internal and external snoops, self-modifying code},
} 

@article{Mallik:2008:PMU:1353535.1346291,
 author = {Mallik, Arindam and Cosgrove, Jack and Dick, Robert P. and Memik, Gokhan and Dinda, Peter},
 title = {PICSEL: measuring user-perceived performance to control dynamic frequency scaling},
 abstract = {The ultimate goal of a computer system is to satisfy its users. The success of architectural or system-level optimizations depends largely on having accurate metrics for user satisfaction. We propose to derive such metrics from information that is "close to flesh" and apparent to the user rather than from information that is "close to metal" and hidden from the user. We describe and evaluate PICSEL, a dynamic voltage and frequency scaling (DVFS) technique that uses measurements of variations in the rate of change of a computer's video output to estimate user-perceived performance. Our adaptive algorithms, one conservative and one aggressive, use these estimates to dramatically reduce operating frequencies and voltages for graphically-intensive applications while maintaining performance at a satisfactory level for the user. We evaluate PICSEL through user studies conducted on a Pentium M laptop running Windows XP. Experiments performed with 20 users executing three applications indicate that the measured laptop power can be reduced by up to 12.1\%, averaged across all of our users and applications, compared to the default Windows XP DVFS policy. User studies revealed that the difference in overall user satisfaction between the more aggressive version of PICSEL and Windows DVFS were statistically insignificant, whereas the conservative version of PICSEL actually improved user satisfaction when compared to Windows DVFS.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {70--79},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346291},
 doi = {http://doi.acm.org/10.1145/1353535.1346291},
 acmid = {1346291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling, power management, thermal emergency, user-perceived performance},
} 

@inproceedings{Mallik:2008:PMU:1346281.1346291,
 author = {Mallik, Arindam and Cosgrove, Jack and Dick, Robert P. and Memik, Gokhan and Dinda, Peter},
 title = {PICSEL: measuring user-perceived performance to control dynamic frequency scaling},
 abstract = {The ultimate goal of a computer system is to satisfy its users. The success of architectural or system-level optimizations depends largely on having accurate metrics for user satisfaction. We propose to derive such metrics from information that is "close to flesh" and apparent to the user rather than from information that is "close to metal" and hidden from the user. We describe and evaluate PICSEL, a dynamic voltage and frequency scaling (DVFS) technique that uses measurements of variations in the rate of change of a computer's video output to estimate user-perceived performance. Our adaptive algorithms, one conservative and one aggressive, use these estimates to dramatically reduce operating frequencies and voltages for graphically-intensive applications while maintaining performance at a satisfactory level for the user. We evaluate PICSEL through user studies conducted on a Pentium M laptop running Windows XP. Experiments performed with 20 users executing three applications indicate that the measured laptop power can be reduced by up to 12.1\%, averaged across all of our users and applications, compared to the default Windows XP DVFS policy. User studies revealed that the difference in overall user satisfaction between the more aggressive version of PICSEL and Windows DVFS were statistically insignificant, whereas the conservative version of PICSEL actually improved user satisfaction when compared to Windows DVFS.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {70--79},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346291},
 doi = {http://doi.acm.org/10.1145/1346281.1346291},
 acmid = {1346291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling, power management, thermal emergency, user-perceived performance},
} 

@article{Mallik:2008:PMU:1353534.1346291,
 author = {Mallik, Arindam and Cosgrove, Jack and Dick, Robert P. and Memik, Gokhan and Dinda, Peter},
 title = {PICSEL: measuring user-perceived performance to control dynamic frequency scaling},
 abstract = {The ultimate goal of a computer system is to satisfy its users. The success of architectural or system-level optimizations depends largely on having accurate metrics for user satisfaction. We propose to derive such metrics from information that is "close to flesh" and apparent to the user rather than from information that is "close to metal" and hidden from the user. We describe and evaluate PICSEL, a dynamic voltage and frequency scaling (DVFS) technique that uses measurements of variations in the rate of change of a computer's video output to estimate user-perceived performance. Our adaptive algorithms, one conservative and one aggressive, use these estimates to dramatically reduce operating frequencies and voltages for graphically-intensive applications while maintaining performance at a satisfactory level for the user. We evaluate PICSEL through user studies conducted on a Pentium M laptop running Windows XP. Experiments performed with 20 users executing three applications indicate that the measured laptop power can be reduced by up to 12.1\%, averaged across all of our users and applications, compared to the default Windows XP DVFS policy. User studies revealed that the difference in overall user satisfaction between the more aggressive version of PICSEL and Windows DVFS were statistically insignificant, whereas the conservative version of PICSEL actually improved user satisfaction when compared to Windows DVFS.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {70--79},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346291},
 doi = {http://doi.acm.org/10.1145/1353534.1346291},
 acmid = {1346291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling, power management, thermal emergency, user-perceived performance},
} 

@article{Mallik:2008:PMU:1353536.1346291,
 author = {Mallik, Arindam and Cosgrove, Jack and Dick, Robert P. and Memik, Gokhan and Dinda, Peter},
 title = {PICSEL: measuring user-perceived performance to control dynamic frequency scaling},
 abstract = {The ultimate goal of a computer system is to satisfy its users. The success of architectural or system-level optimizations depends largely on having accurate metrics for user satisfaction. We propose to derive such metrics from information that is "close to flesh" and apparent to the user rather than from information that is "close to metal" and hidden from the user. We describe and evaluate PICSEL, a dynamic voltage and frequency scaling (DVFS) technique that uses measurements of variations in the rate of change of a computer's video output to estimate user-perceived performance. Our adaptive algorithms, one conservative and one aggressive, use these estimates to dramatically reduce operating frequencies and voltages for graphically-intensive applications while maintaining performance at a satisfactory level for the user. We evaluate PICSEL through user studies conducted on a Pentium M laptop running Windows XP. Experiments performed with 20 users executing three applications indicate that the measured laptop power can be reduced by up to 12.1\%, averaged across all of our users and applications, compared to the default Windows XP DVFS policy. User studies revealed that the difference in overall user satisfaction between the more aggressive version of PICSEL and Windows DVFS were statistically insignificant, whereas the conservative version of PICSEL actually improved user satisfaction when compared to Windows DVFS.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {70--79},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346291},
 doi = {http://doi.acm.org/10.1145/1353536.1346291},
 acmid = {1346291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling, power management, thermal emergency, user-perceived performance},
} 

@article{Joao:2008:IPO:1353534.1346293,
 author = {Joao, Jose A. and Mutlu, Onur and Kim, Hyesoon and Agarwal, Rishi and Patt, Yale N.},
 title = {Improving the performance of object-oriented languages with dynamic predication of indirect jumps},
 abstract = {Indirect jump instructions are used to implement increasingly-common programming constructs such as virtual function calls, switch-case statements, jump tables, and interface calls. The performance impact of indirect jumps is likely to increase because indirect jumps with multiple targets are difficult to predict even with specialized hardware. This paper proposes a new way of handling hard-to-predict indirect jumps: dynamically predicating them. The compiler (static or dynamic) identifies indirect jumps that are suitable for predication along with their control-flow merge (CFM) points. The hardware predicates theinstructions between different targets of the jump and its CFM point if the jump turns out to be hard-to-predict at run time. If the jump would actually have been mispredicted, its dynamic predication eliminates a pipeline flush, thereby improving performance. Our evaluations show that Dynamic Indirect jump Predication (DIP) improves the performance of a set of object-oriented applications including the Java DaCapo benchmark suite by 37.8\% compared to a commonly-used branch target buffer based predictor, while also reducing energy consumption by 24.8\%. We compare DIP to three previously proposed indirect jump predictors and find that it provides the best performance and energy-efficiency.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {80--90},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346293},
 doi = {http://doi.acm.org/10.1145/1353534.1346293},
 acmid = {1346293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic predication, indirect jumps, object-oriented languages, predicated execution, virtual functions},
} 

@article{Joao:2008:IPO:1353536.1346293,
 author = {Joao, Jose A. and Mutlu, Onur and Kim, Hyesoon and Agarwal, Rishi and Patt, Yale N.},
 title = {Improving the performance of object-oriented languages with dynamic predication of indirect jumps},
 abstract = {Indirect jump instructions are used to implement increasingly-common programming constructs such as virtual function calls, switch-case statements, jump tables, and interface calls. The performance impact of indirect jumps is likely to increase because indirect jumps with multiple targets are difficult to predict even with specialized hardware. This paper proposes a new way of handling hard-to-predict indirect jumps: dynamically predicating them. The compiler (static or dynamic) identifies indirect jumps that are suitable for predication along with their control-flow merge (CFM) points. The hardware predicates theinstructions between different targets of the jump and its CFM point if the jump turns out to be hard-to-predict at run time. If the jump would actually have been mispredicted, its dynamic predication eliminates a pipeline flush, thereby improving performance. Our evaluations show that Dynamic Indirect jump Predication (DIP) improves the performance of a set of object-oriented applications including the Java DaCapo benchmark suite by 37.8\% compared to a commonly-used branch target buffer based predictor, while also reducing energy consumption by 24.8\%. We compare DIP to three previously proposed indirect jump predictors and find that it provides the best performance and energy-efficiency.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {80--90},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346293},
 doi = {http://doi.acm.org/10.1145/1353536.1346293},
 acmid = {1346293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic predication, indirect jumps, object-oriented languages, predicated execution, virtual functions},
} 

@article{Joao:2008:IPO:1353535.1346293,
 author = {Joao, Jose A. and Mutlu, Onur and Kim, Hyesoon and Agarwal, Rishi and Patt, Yale N.},
 title = {Improving the performance of object-oriented languages with dynamic predication of indirect jumps},
 abstract = {Indirect jump instructions are used to implement increasingly-common programming constructs such as virtual function calls, switch-case statements, jump tables, and interface calls. The performance impact of indirect jumps is likely to increase because indirect jumps with multiple targets are difficult to predict even with specialized hardware. This paper proposes a new way of handling hard-to-predict indirect jumps: dynamically predicating them. The compiler (static or dynamic) identifies indirect jumps that are suitable for predication along with their control-flow merge (CFM) points. The hardware predicates theinstructions between different targets of the jump and its CFM point if the jump turns out to be hard-to-predict at run time. If the jump would actually have been mispredicted, its dynamic predication eliminates a pipeline flush, thereby improving performance. Our evaluations show that Dynamic Indirect jump Predication (DIP) improves the performance of a set of object-oriented applications including the Java DaCapo benchmark suite by 37.8\% compared to a commonly-used branch target buffer based predictor, while also reducing energy consumption by 24.8\%. We compare DIP to three previously proposed indirect jump predictors and find that it provides the best performance and energy-efficiency.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {80--90},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346293},
 doi = {http://doi.acm.org/10.1145/1353535.1346293},
 acmid = {1346293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic predication, indirect jumps, object-oriented languages, predicated execution, virtual functions},
} 

@inproceedings{Joao:2008:IPO:1346281.1346293,
 author = {Joao, Jose A. and Mutlu, Onur and Kim, Hyesoon and Agarwal, Rishi and Patt, Yale N.},
 title = {Improving the performance of object-oriented languages with dynamic predication of indirect jumps},
 abstract = {Indirect jump instructions are used to implement increasingly-common programming constructs such as virtual function calls, switch-case statements, jump tables, and interface calls. The performance impact of indirect jumps is likely to increase because indirect jumps with multiple targets are difficult to predict even with specialized hardware. This paper proposes a new way of handling hard-to-predict indirect jumps: dynamically predicating them. The compiler (static or dynamic) identifies indirect jumps that are suitable for predication along with their control-flow merge (CFM) points. The hardware predicates theinstructions between different targets of the jump and its CFM point if the jump turns out to be hard-to-predict at run time. If the jump would actually have been mispredicted, its dynamic predication eliminates a pipeline flush, thereby improving performance. Our evaluations show that Dynamic Indirect jump Predication (DIP) improves the performance of a set of object-oriented applications including the Java DaCapo benchmark suite by 37.8\% compared to a commonly-used branch target buffer based predictor, while also reducing energy consumption by 24.8\%. We compare DIP to three previously proposed indirect jump predictors and find that it provides the best performance and energy-efficiency.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {80--90},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346293},
 doi = {http://doi.acm.org/10.1145/1346281.1346293},
 acmid = {1346293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic predication, indirect jumps, object-oriented languages, predicated execution, virtual functions},
} 

@article{Wegiel:2008:MCV:1353536.1346294,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {The mapping collector: virtual memory support for generational, parallel, and concurrent compaction},
 abstract = {Parallel and concurrent garbage collectors are increasingly employed by managed runtime environments (MREs) to maintain scalability, as multi-core architectures and multi-threaded applications become pervasive. Moreover, state-of-the-art MREs commonly implement compaction to eliminate heap fragmentation and enable fast linear object allocation. Our empirical analysis of object demographics reveals that unreachable objects in the heap tend to form clusters large enough to be effectively managed at the granularity of virtual memory pages. Even though processes can manipulate the mapping of the virtual address space through the standard operating system (OS) interface on most platforms, extant parallel/concurrent compactors do not do so to exploit this clustering behavior and instead achieve compaction by performing, relatively expensive, object moving and pointer adjustment. We introduce the Mapping Collector (MC), which leverages virtual memory operations to reclaim and consolidate free space without moving objects and updating pointers. MC is a nearly-single-phase compactor that is simpler and more efficient than previously reported compactors that comprise two to four phases. Through effective MRE-OS coordination, MC maintains the simplicity of a non-moving collector while providing efficient parallel and concurrent compaction. We implement both stop-the-world and concurrent MC in a generational garbage collection framework within the open-source HotSpot Java Virtual Machine. Our experimental evaluation using a multiprocessor indicates that MC significantly increases throughput and scalability as well as reduces pause times, relative to state-of-the-art, parallel and concurrent compactors.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346294},
 doi = {http://doi.acm.org/10.1145/1353536.1346294},
 acmid = {1346294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, concurrent, parallel, virtual memory},
} 

@article{Wegiel:2008:MCV:1353534.1346294,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {The mapping collector: virtual memory support for generational, parallel, and concurrent compaction},
 abstract = {Parallel and concurrent garbage collectors are increasingly employed by managed runtime environments (MREs) to maintain scalability, as multi-core architectures and multi-threaded applications become pervasive. Moreover, state-of-the-art MREs commonly implement compaction to eliminate heap fragmentation and enable fast linear object allocation. Our empirical analysis of object demographics reveals that unreachable objects in the heap tend to form clusters large enough to be effectively managed at the granularity of virtual memory pages. Even though processes can manipulate the mapping of the virtual address space through the standard operating system (OS) interface on most platforms, extant parallel/concurrent compactors do not do so to exploit this clustering behavior and instead achieve compaction by performing, relatively expensive, object moving and pointer adjustment. We introduce the Mapping Collector (MC), which leverages virtual memory operations to reclaim and consolidate free space without moving objects and updating pointers. MC is a nearly-single-phase compactor that is simpler and more efficient than previously reported compactors that comprise two to four phases. Through effective MRE-OS coordination, MC maintains the simplicity of a non-moving collector while providing efficient parallel and concurrent compaction. We implement both stop-the-world and concurrent MC in a generational garbage collection framework within the open-source HotSpot Java Virtual Machine. Our experimental evaluation using a multiprocessor indicates that MC significantly increases throughput and scalability as well as reduces pause times, relative to state-of-the-art, parallel and concurrent compactors.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346294},
 doi = {http://doi.acm.org/10.1145/1353534.1346294},
 acmid = {1346294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, concurrent, parallel, virtual memory},
} 

@inproceedings{Wegiel:2008:MCV:1346281.1346294,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {The mapping collector: virtual memory support for generational, parallel, and concurrent compaction},
 abstract = {Parallel and concurrent garbage collectors are increasingly employed by managed runtime environments (MREs) to maintain scalability, as multi-core architectures and multi-threaded applications become pervasive. Moreover, state-of-the-art MREs commonly implement compaction to eliminate heap fragmentation and enable fast linear object allocation. Our empirical analysis of object demographics reveals that unreachable objects in the heap tend to form clusters large enough to be effectively managed at the granularity of virtual memory pages. Even though processes can manipulate the mapping of the virtual address space through the standard operating system (OS) interface on most platforms, extant parallel/concurrent compactors do not do so to exploit this clustering behavior and instead achieve compaction by performing, relatively expensive, object moving and pointer adjustment. We introduce the Mapping Collector (MC), which leverages virtual memory operations to reclaim and consolidate free space without moving objects and updating pointers. MC is a nearly-single-phase compactor that is simpler and more efficient than previously reported compactors that comprise two to four phases. Through effective MRE-OS coordination, MC maintains the simplicity of a non-moving collector while providing efficient parallel and concurrent compaction. We implement both stop-the-world and concurrent MC in a generational garbage collection framework within the open-source HotSpot Java Virtual Machine. Our experimental evaluation using a multiprocessor indicates that MC significantly increases throughput and scalability as well as reduces pause times, relative to state-of-the-art, parallel and concurrent compactors.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346294},
 doi = {http://doi.acm.org/10.1145/1346281.1346294},
 acmid = {1346294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, concurrent, parallel, virtual memory},
} 

@article{Wegiel:2008:MCV:1353535.1346294,
 author = {Wegiel, Michal and Krintz, Chandra},
 title = {The mapping collector: virtual memory support for generational, parallel, and concurrent compaction},
 abstract = {Parallel and concurrent garbage collectors are increasingly employed by managed runtime environments (MREs) to maintain scalability, as multi-core architectures and multi-threaded applications become pervasive. Moreover, state-of-the-art MREs commonly implement compaction to eliminate heap fragmentation and enable fast linear object allocation. Our empirical analysis of object demographics reveals that unreachable objects in the heap tend to form clusters large enough to be effectively managed at the granularity of virtual memory pages. Even though processes can manipulate the mapping of the virtual address space through the standard operating system (OS) interface on most platforms, extant parallel/concurrent compactors do not do so to exploit this clustering behavior and instead achieve compaction by performing, relatively expensive, object moving and pointer adjustment. We introduce the Mapping Collector (MC), which leverages virtual memory operations to reclaim and consolidate free space without moving objects and updating pointers. MC is a nearly-single-phase compactor that is simpler and more efficient than previously reported compactors that comprise two to four phases. Through effective MRE-OS coordination, MC maintains the simplicity of a non-moving collector while providing efficient parallel and concurrent compaction. We implement both stop-the-world and concurrent MC in a generational garbage collection framework within the open-source HotSpot Java Virtual Machine. Our experimental evaluation using a multiprocessor indicates that MC significantly increases throughput and scalability as well as reduces pause times, relative to state-of-the-art, parallel and concurrent compactors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346294},
 doi = {http://doi.acm.org/10.1145/1353535.1346294},
 acmid = {1346294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, concurrent, parallel, virtual memory},
} 

@article{Devietti:2008:HAS:1353536.1346295,
 author = {Devietti, Joe and Blundell, Colin and Martin, Milo M. K. and Zdancewic, Steve},
 title = {Hardbound: architectural support for spatial safety of the C programming language},
 abstract = {The C programming language is at least as well known for its absence of spatial memory safety guarantees (i.e., lack of bounds checking) as it is for its high performance. C's unchecked pointer arithmetic and array indexing allow simple programming mistakes to lead to erroneous executions, silent data corruption, and security vulnerabilities. Many prior proposals have tackled enforcing spatial safety in C programs by checking pointer and array accesses. However, existing software-only proposals have significant drawbacks that may prevent wide adoption, including: unacceptably high run-time overheads, lack of completeness, incompatible pointer representations, or need for non-trivial changes to existing C source code and compiler infrastructure. Inspired by the promise of these software-only approaches, this paper proposes a hardware bounded pointer architectural primitive that supports cooperative hardware/software enforcement of spatial memory safety for C programs. This bounded pointer is a new hardware primitive datatype for pointers that leaves the standard C pointer representation intact, but augments it with bounds information maintained separately and invisibly by the hardware. The bounds are initialized by the software, and they are then propagated and enforced transparently by the hardware, which automatically checks a pointer's bounds before it is dereferenced. One mode of use requires instrumenting only malloc, which enables enforcement of perallocation spatial safety for heap-allocated objects for existing binaries. When combined with simple intraprocedural compiler instrumentation, hardware bounded pointers enable a low-overhead approach for enforcing complete spatial memory safety in unmodified C programs.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346295},
 doi = {http://doi.acm.org/10.1145/1353536.1346295},
 acmid = {1346295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C programming language, spatial memory safety},
} 

@inproceedings{Devietti:2008:HAS:1346281.1346295,
 author = {Devietti, Joe and Blundell, Colin and Martin, Milo M. K. and Zdancewic, Steve},
 title = {Hardbound: architectural support for spatial safety of the C programming language},
 abstract = {The C programming language is at least as well known for its absence of spatial memory safety guarantees (i.e., lack of bounds checking) as it is for its high performance. C's unchecked pointer arithmetic and array indexing allow simple programming mistakes to lead to erroneous executions, silent data corruption, and security vulnerabilities. Many prior proposals have tackled enforcing spatial safety in C programs by checking pointer and array accesses. However, existing software-only proposals have significant drawbacks that may prevent wide adoption, including: unacceptably high run-time overheads, lack of completeness, incompatible pointer representations, or need for non-trivial changes to existing C source code and compiler infrastructure. Inspired by the promise of these software-only approaches, this paper proposes a hardware bounded pointer architectural primitive that supports cooperative hardware/software enforcement of spatial memory safety for C programs. This bounded pointer is a new hardware primitive datatype for pointers that leaves the standard C pointer representation intact, but augments it with bounds information maintained separately and invisibly by the hardware. The bounds are initialized by the software, and they are then propagated and enforced transparently by the hardware, which automatically checks a pointer's bounds before it is dereferenced. One mode of use requires instrumenting only malloc, which enables enforcement of perallocation spatial safety for heap-allocated objects for existing binaries. When combined with simple intraprocedural compiler instrumentation, hardware bounded pointers enable a low-overhead approach for enforcing complete spatial memory safety in unmodified C programs.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346295},
 doi = {http://doi.acm.org/10.1145/1346281.1346295},
 acmid = {1346295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C programming language, spatial memory safety},
} 

@article{Devietti:2008:HAS:1353535.1346295,
 author = {Devietti, Joe and Blundell, Colin and Martin, Milo M. K. and Zdancewic, Steve},
 title = {Hardbound: architectural support for spatial safety of the C programming language},
 abstract = {The C programming language is at least as well known for its absence of spatial memory safety guarantees (i.e., lack of bounds checking) as it is for its high performance. C's unchecked pointer arithmetic and array indexing allow simple programming mistakes to lead to erroneous executions, silent data corruption, and security vulnerabilities. Many prior proposals have tackled enforcing spatial safety in C programs by checking pointer and array accesses. However, existing software-only proposals have significant drawbacks that may prevent wide adoption, including: unacceptably high run-time overheads, lack of completeness, incompatible pointer representations, or need for non-trivial changes to existing C source code and compiler infrastructure. Inspired by the promise of these software-only approaches, this paper proposes a hardware bounded pointer architectural primitive that supports cooperative hardware/software enforcement of spatial memory safety for C programs. This bounded pointer is a new hardware primitive datatype for pointers that leaves the standard C pointer representation intact, but augments it with bounds information maintained separately and invisibly by the hardware. The bounds are initialized by the software, and they are then propagated and enforced transparently by the hardware, which automatically checks a pointer's bounds before it is dereferenced. One mode of use requires instrumenting only malloc, which enables enforcement of perallocation spatial safety for heap-allocated objects for existing binaries. When combined with simple intraprocedural compiler instrumentation, hardware bounded pointers enable a low-overhead approach for enforcing complete spatial memory safety in unmodified C programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346295},
 doi = {http://doi.acm.org/10.1145/1353535.1346295},
 acmid = {1346295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C programming language, spatial memory safety},
} 

@article{Devietti:2008:HAS:1353534.1346295,
 author = {Devietti, Joe and Blundell, Colin and Martin, Milo M. K. and Zdancewic, Steve},
 title = {Hardbound: architectural support for spatial safety of the C programming language},
 abstract = {The C programming language is at least as well known for its absence of spatial memory safety guarantees (i.e., lack of bounds checking) as it is for its high performance. C's unchecked pointer arithmetic and array indexing allow simple programming mistakes to lead to erroneous executions, silent data corruption, and security vulnerabilities. Many prior proposals have tackled enforcing spatial safety in C programs by checking pointer and array accesses. However, existing software-only proposals have significant drawbacks that may prevent wide adoption, including: unacceptably high run-time overheads, lack of completeness, incompatible pointer representations, or need for non-trivial changes to existing C source code and compiler infrastructure. Inspired by the promise of these software-only approaches, this paper proposes a hardware bounded pointer architectural primitive that supports cooperative hardware/software enforcement of spatial memory safety for C programs. This bounded pointer is a new hardware primitive datatype for pointers that leaves the standard C pointer representation intact, but augments it with bounds information maintained separately and invisibly by the hardware. The bounds are initialized by the software, and they are then propagated and enforced transparently by the hardware, which automatically checks a pointer's bounds before it is dereferenced. One mode of use requires instrumenting only malloc, which enables enforcement of perallocation spatial safety for heap-allocated objects for existing binaries. When combined with simple intraprocedural compiler instrumentation, hardware bounded pointers enable a low-overhead approach for enforcing complete spatial memory safety in unmodified C programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346295},
 doi = {http://doi.acm.org/10.1145/1353534.1346295},
 acmid = {1346295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {C programming language, spatial memory safety},
} 

@article{Lvin:2008:ATA:1353535.1346296,
 author = {Lvin, Vitaliy B. and Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Archipelago: trading address space for reliability and security},
 abstract = {Memory errors are a notorious source of security vulnerabilities that can lead to service interruptions, information leakage and unauthorized access. Because such errors are also difficult to debug, the absence of timely patches can leave users vulnerable to attack for long periods of time. A variety of approaches have been introduced to combat these errors, but these often incur large runtime overheads and generally abort on errors, threatening availability. This paper presents Archipelago, a runtime system that takes advantage of available address space to substantially reduce the likelihood that a memory error will affect program execution. Archipelago randomly allocates heap objects far apart in virtual address space, effectively isolating each object from buffer overflows. Archipelago also protects against dangling pointer errors by preserving the contents of freed objects after they are freed. Archipelago thus trades virtual address space---a plentiful resource on 64-bit systems---for significantly improved program reliability and security, while limiting physical memory consumption by tracking the working set of an application and compacting cold objects. We show that Archipelago allows applications to continue to run correctly in the face of thousands of memory errors. Across a suite of server applications, Archipelago's performance overhead is 6\% on average (between -7\% and 22\%), making it especially suitable to protect servers that have known security vulnerabilities due to heap memory errors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346296},
 doi = {http://doi.acm.org/10.1145/1353535.1346296},
 acmid = {1346296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Archipelago, buffer overflow, dynamic memory allocation, memory errors, probabilistic memory safety, randomized algorithms, virtual memory},
} 

@inproceedings{Lvin:2008:ATA:1346281.1346296,
 author = {Lvin, Vitaliy B. and Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Archipelago: trading address space for reliability and security},
 abstract = {Memory errors are a notorious source of security vulnerabilities that can lead to service interruptions, information leakage and unauthorized access. Because such errors are also difficult to debug, the absence of timely patches can leave users vulnerable to attack for long periods of time. A variety of approaches have been introduced to combat these errors, but these often incur large runtime overheads and generally abort on errors, threatening availability. This paper presents Archipelago, a runtime system that takes advantage of available address space to substantially reduce the likelihood that a memory error will affect program execution. Archipelago randomly allocates heap objects far apart in virtual address space, effectively isolating each object from buffer overflows. Archipelago also protects against dangling pointer errors by preserving the contents of freed objects after they are freed. Archipelago thus trades virtual address space---a plentiful resource on 64-bit systems---for significantly improved program reliability and security, while limiting physical memory consumption by tracking the working set of an application and compacting cold objects. We show that Archipelago allows applications to continue to run correctly in the face of thousands of memory errors. Across a suite of server applications, Archipelago's performance overhead is 6\% on average (between -7\% and 22\%), making it especially suitable to protect servers that have known security vulnerabilities due to heap memory errors.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346296},
 doi = {http://doi.acm.org/10.1145/1346281.1346296},
 acmid = {1346296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Archipelago, buffer overflow, dynamic memory allocation, memory errors, probabilistic memory safety, randomized algorithms, virtual memory},
} 

@article{Lvin:2008:ATA:1353534.1346296,
 author = {Lvin, Vitaliy B. and Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Archipelago: trading address space for reliability and security},
 abstract = {Memory errors are a notorious source of security vulnerabilities that can lead to service interruptions, information leakage and unauthorized access. Because such errors are also difficult to debug, the absence of timely patches can leave users vulnerable to attack for long periods of time. A variety of approaches have been introduced to combat these errors, but these often incur large runtime overheads and generally abort on errors, threatening availability. This paper presents Archipelago, a runtime system that takes advantage of available address space to substantially reduce the likelihood that a memory error will affect program execution. Archipelago randomly allocates heap objects far apart in virtual address space, effectively isolating each object from buffer overflows. Archipelago also protects against dangling pointer errors by preserving the contents of freed objects after they are freed. Archipelago thus trades virtual address space---a plentiful resource on 64-bit systems---for significantly improved program reliability and security, while limiting physical memory consumption by tracking the working set of an application and compacting cold objects. We show that Archipelago allows applications to continue to run correctly in the face of thousands of memory errors. Across a suite of server applications, Archipelago's performance overhead is 6\% on average (between -7\% and 22\%), making it especially suitable to protect servers that have known security vulnerabilities due to heap memory errors.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346296},
 doi = {http://doi.acm.org/10.1145/1353534.1346296},
 acmid = {1346296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Archipelago, buffer overflow, dynamic memory allocation, memory errors, probabilistic memory safety, randomized algorithms, virtual memory},
} 

@article{Lvin:2008:ATA:1353536.1346296,
 author = {Lvin, Vitaliy B. and Novark, Gene and Berger, Emery D. and Zorn, Benjamin G.},
 title = {Archipelago: trading address space for reliability and security},
 abstract = {Memory errors are a notorious source of security vulnerabilities that can lead to service interruptions, information leakage and unauthorized access. Because such errors are also difficult to debug, the absence of timely patches can leave users vulnerable to attack for long periods of time. A variety of approaches have been introduced to combat these errors, but these often incur large runtime overheads and generally abort on errors, threatening availability. This paper presents Archipelago, a runtime system that takes advantage of available address space to substantially reduce the likelihood that a memory error will affect program execution. Archipelago randomly allocates heap objects far apart in virtual address space, effectively isolating each object from buffer overflows. Archipelago also protects against dangling pointer errors by preserving the contents of freed objects after they are freed. Archipelago thus trades virtual address space---a plentiful resource on 64-bit systems---for significantly improved program reliability and security, while limiting physical memory consumption by tracking the working set of an application and compacting cold objects. We show that Archipelago allows applications to continue to run correctly in the face of thousands of memory errors. Across a suite of server applications, Archipelago's performance overhead is 6\% on average (between -7\% and 22\%), making it especially suitable to protect servers that have known security vulnerabilities due to heap memory errors.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346296},
 doi = {http://doi.acm.org/10.1145/1353536.1346296},
 acmid = {1346296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Archipelago, buffer overflow, dynamic memory allocation, memory errors, probabilistic memory safety, randomized algorithms, virtual memory},
} 

@article{Choi:2008:ABP:1353535.1346298,
 author = {Choi, Bumyong and Porter, Leo and Tullsen, Dean M.},
 title = {Accurate branch prediction for short threads},
 abstract = {Multi-core processors, with low communication costs and high availability of execution cores, will increase the use of execution and compilation models that use short threads to expose parallelism. Current branch predictors seek to incorporate large amounts of control flow history to maximize accuracy. However, when that history is absent the predictor fails to work as intended. Thus, modern predictors are almost useless for threads below a certain length. Using a Speculative Multithreaded (SpMT) architecture as an example of a system which generates shorter threads, this work examines techniques to improve branch prediction accuracy when a new thread begins to execute on a different core. This paper proposes a minor change to the branch predictor that gives virtually the same performance on short threads as an idealized predictor that incorporates unknowable pre-history of a spawned speculative thread. At the same time, strong performance on long threads is preserved. The proposed technique sets the global history register of the spawned thread to the initial value of the program counter. This novel and simple design reduces branch mispredicts by 29\% and provides as much as a 13\% IPC improvement on selected SPEC2000 benchmarks.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {125--134},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346298},
 doi = {http://doi.acm.org/10.1145/1353535.1346298},
 acmid = {1346298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessors},
} 

@article{Choi:2008:ABP:1353536.1346298,
 author = {Choi, Bumyong and Porter, Leo and Tullsen, Dean M.},
 title = {Accurate branch prediction for short threads},
 abstract = {Multi-core processors, with low communication costs and high availability of execution cores, will increase the use of execution and compilation models that use short threads to expose parallelism. Current branch predictors seek to incorporate large amounts of control flow history to maximize accuracy. However, when that history is absent the predictor fails to work as intended. Thus, modern predictors are almost useless for threads below a certain length. Using a Speculative Multithreaded (SpMT) architecture as an example of a system which generates shorter threads, this work examines techniques to improve branch prediction accuracy when a new thread begins to execute on a different core. This paper proposes a minor change to the branch predictor that gives virtually the same performance on short threads as an idealized predictor that incorporates unknowable pre-history of a spawned speculative thread. At the same time, strong performance on long threads is preserved. The proposed technique sets the global history register of the spawned thread to the initial value of the program counter. This novel and simple design reduces branch mispredicts by 29\% and provides as much as a 13\% IPC improvement on selected SPEC2000 benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {125--134},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346298},
 doi = {http://doi.acm.org/10.1145/1353536.1346298},
 acmid = {1346298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessors},
} 

@article{Choi:2008:ABP:1353534.1346298,
 author = {Choi, Bumyong and Porter, Leo and Tullsen, Dean M.},
 title = {Accurate branch prediction for short threads},
 abstract = {Multi-core processors, with low communication costs and high availability of execution cores, will increase the use of execution and compilation models that use short threads to expose parallelism. Current branch predictors seek to incorporate large amounts of control flow history to maximize accuracy. However, when that history is absent the predictor fails to work as intended. Thus, modern predictors are almost useless for threads below a certain length. Using a Speculative Multithreaded (SpMT) architecture as an example of a system which generates shorter threads, this work examines techniques to improve branch prediction accuracy when a new thread begins to execute on a different core. This paper proposes a minor change to the branch predictor that gives virtually the same performance on short threads as an idealized predictor that incorporates unknowable pre-history of a spawned speculative thread. At the same time, strong performance on long threads is preserved. The proposed technique sets the global history register of the spawned thread to the initial value of the program counter. This novel and simple design reduces branch mispredicts by 29\% and provides as much as a 13\% IPC improvement on selected SPEC2000 benchmarks.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {125--134},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346298},
 doi = {http://doi.acm.org/10.1145/1353534.1346298},
 acmid = {1346298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessors},
} 

@inproceedings{Choi:2008:ABP:1346281.1346298,
 author = {Choi, Bumyong and Porter, Leo and Tullsen, Dean M.},
 title = {Accurate branch prediction for short threads},
 abstract = {Multi-core processors, with low communication costs and high availability of execution cores, will increase the use of execution and compilation models that use short threads to expose parallelism. Current branch predictors seek to incorporate large amounts of control flow history to maximize accuracy. However, when that history is absent the predictor fails to work as intended. Thus, modern predictors are almost useless for threads below a certain length. Using a Speculative Multithreaded (SpMT) architecture as an example of a system which generates shorter threads, this work examines techniques to improve branch prediction accuracy when a new thread begins to execute on a different core. This paper proposes a minor change to the branch predictor that gives virtually the same performance on short threads as an idealized predictor that incorporates unknowable pre-history of a spawned speculative thread. At the same time, strong performance on long threads is preserved. The proposed technique sets the global history register of the spawned thread to the initial value of the program counter. This novel and simple design reduces branch mispredicts by 29\% and provides as much as a 13\% IPC improvement on selected SPEC2000 benchmarks.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {125--134},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346298},
 doi = {http://doi.acm.org/10.1145/1346281.1346298},
 acmid = {1346298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessors},
} 

@article{Srikantaiah:2008:ASP:1353534.1346299,
 author = {Srikantaiah, Shekhar and Kandemir, Mahmut and Irwin, Mary Jane},
 title = {Adaptive set pinning: managing shared caches in chip multiprocessors},
 abstract = {As part of the trend towards Chip Multiprocessors (CMPs) for the next leap in computing performance, many architectures have explored sharing the last level of cache among different processors for better performance-cost ratio and improved resource allocation. Shared cache management is a crucial CMP design aspect for the performance of the system. This paper first presents a new classification of cache misses - CII: Compulsory, Inter-processor and Intra-processor misses - for CMPs with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a CMP. We then propose a novel approach, called set pinning, for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive set pinning scheme improves over the benefits obtained by the set pinning scheme by significantly reducing the number of off-chip accesses. Extensive analysis of these approaches with SPEComp 2001 benchmarks is performed using a full system simulator. Our experiments indicate that the set pinning scheme achieves an average improvement of 22.18\% in the L2 miss rate while the adaptive set pinning scheme reduces the miss rates by an average of 47.94\% as compared to the traditional shared cache scheme. They also improve the performance by 7.24\% and 17.88\% respectively.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {135--144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346299},
 doi = {http://doi.acm.org/10.1145/1353534.1346299},
 acmid = {1346299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, inter-processor, intra-processor, set pinning, shared cache},
} 

@inproceedings{Srikantaiah:2008:ASP:1346281.1346299,
 author = {Srikantaiah, Shekhar and Kandemir, Mahmut and Irwin, Mary Jane},
 title = {Adaptive set pinning: managing shared caches in chip multiprocessors},
 abstract = {As part of the trend towards Chip Multiprocessors (CMPs) for the next leap in computing performance, many architectures have explored sharing the last level of cache among different processors for better performance-cost ratio and improved resource allocation. Shared cache management is a crucial CMP design aspect for the performance of the system. This paper first presents a new classification of cache misses - CII: Compulsory, Inter-processor and Intra-processor misses - for CMPs with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a CMP. We then propose a novel approach, called set pinning, for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive set pinning scheme improves over the benefits obtained by the set pinning scheme by significantly reducing the number of off-chip accesses. Extensive analysis of these approaches with SPEComp 2001 benchmarks is performed using a full system simulator. Our experiments indicate that the set pinning scheme achieves an average improvement of 22.18\% in the L2 miss rate while the adaptive set pinning scheme reduces the miss rates by an average of 47.94\% as compared to the traditional shared cache scheme. They also improve the performance by 7.24\% and 17.88\% respectively.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {135--144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346299},
 doi = {http://doi.acm.org/10.1145/1346281.1346299},
 acmid = {1346299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, inter-processor, intra-processor, set pinning, shared cache},
} 

@article{Srikantaiah:2008:ASP:1353535.1346299,
 author = {Srikantaiah, Shekhar and Kandemir, Mahmut and Irwin, Mary Jane},
 title = {Adaptive set pinning: managing shared caches in chip multiprocessors},
 abstract = {As part of the trend towards Chip Multiprocessors (CMPs) for the next leap in computing performance, many architectures have explored sharing the last level of cache among different processors for better performance-cost ratio and improved resource allocation. Shared cache management is a crucial CMP design aspect for the performance of the system. This paper first presents a new classification of cache misses - CII: Compulsory, Inter-processor and Intra-processor misses - for CMPs with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a CMP. We then propose a novel approach, called set pinning, for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive set pinning scheme improves over the benefits obtained by the set pinning scheme by significantly reducing the number of off-chip accesses. Extensive analysis of these approaches with SPEComp 2001 benchmarks is performed using a full system simulator. Our experiments indicate that the set pinning scheme achieves an average improvement of 22.18\% in the L2 miss rate while the adaptive set pinning scheme reduces the miss rates by an average of 47.94\% as compared to the traditional shared cache scheme. They also improve the performance by 7.24\% and 17.88\% respectively.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {135--144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346299},
 doi = {http://doi.acm.org/10.1145/1353535.1346299},
 acmid = {1346299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, inter-processor, intra-processor, set pinning, shared cache},
} 

@article{Srikantaiah:2008:ASP:1353536.1346299,
 author = {Srikantaiah, Shekhar and Kandemir, Mahmut and Irwin, Mary Jane},
 title = {Adaptive set pinning: managing shared caches in chip multiprocessors},
 abstract = {As part of the trend towards Chip Multiprocessors (CMPs) for the next leap in computing performance, many architectures have explored sharing the last level of cache among different processors for better performance-cost ratio and improved resource allocation. Shared cache management is a crucial CMP design aspect for the performance of the system. This paper first presents a new classification of cache misses - CII: Compulsory, Inter-processor and Intra-processor misses - for CMPs with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a CMP. We then propose a novel approach, called set pinning, for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive set pinning scheme improves over the benefits obtained by the set pinning scheme by significantly reducing the number of off-chip accesses. Extensive analysis of these approaches with SPEComp 2001 benchmarks is performed using a full system simulator. Our experiments indicate that the set pinning scheme achieves an average improvement of 22.18\% in the L2 miss rate while the adaptive set pinning scheme reduces the miss rates by an average of 47.94\% as compared to the traditional shared cache scheme. They also improve the performance by 7.24\% and 17.88\% respectively.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {135--144},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346299},
 doi = {http://doi.acm.org/10.1145/1353536.1346299},
 acmid = {1346299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, inter-processor, intra-processor, set pinning, shared cache},
} 

@article{Tuck:2008:SSH:1353536.1346300,
 author = {Tuck, James and Ahn, Wonsun and Ceze, Luis and Torrellas, Josep},
 title = {SoftSig: software-exposed hardware signatures for code analysis and optimization},
 abstract = {Many code analysis techniques for optimization, debugging, or parallelization need to perform runtime disambiguation of sets of addresses. Such operations can be supported efficiently and with low complexity with hardware signatures. To enable flexible use of signatures, this paper proposes to expose a Signature Register File to the software through a rich ISA. The software has great flexibility to decide, for each signature,which addresses to collect and which addresses to disambiguate against. We call this architecture SoftSig</i>. In addition, as an example of SoftSig use, we show how to detect redundant function calls efficiently and eliminate them dynamically. We call this algorithm MemoiSE</i>. On average for five popular applications, MemoiSE reduces the number of dynamic instructions by 9.3\%, thereby reducing the execution time of the applications by 9\%.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346300},
 doi = {http://doi.acm.org/10.1145/1353536.1346300},
 acmid = {1346300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory disambiguation, multi-core architectures, runtime optimization},
} 

@article{Tuck:2008:SSH:1353535.1346300,
 author = {Tuck, James and Ahn, Wonsun and Ceze, Luis and Torrellas, Josep},
 title = {SoftSig: software-exposed hardware signatures for code analysis and optimization},
 abstract = {Many code analysis techniques for optimization, debugging, or parallelization need to perform runtime disambiguation of sets of addresses. Such operations can be supported efficiently and with low complexity with hardware signatures. To enable flexible use of signatures, this paper proposes to expose a Signature Register File to the software through a rich ISA. The software has great flexibility to decide, for each signature,which addresses to collect and which addresses to disambiguate against. We call this architecture SoftSig</i>. In addition, as an example of SoftSig use, we show how to detect redundant function calls efficiently and eliminate them dynamically. We call this algorithm MemoiSE</i>. On average for five popular applications, MemoiSE reduces the number of dynamic instructions by 9.3\%, thereby reducing the execution time of the applications by 9\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346300},
 doi = {http://doi.acm.org/10.1145/1353535.1346300},
 acmid = {1346300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory disambiguation, multi-core architectures, runtime optimization},
} 

@inproceedings{Tuck:2008:SSH:1346281.1346300,
 author = {Tuck, James and Ahn, Wonsun and Ceze, Luis and Torrellas, Josep},
 title = {SoftSig: software-exposed hardware signatures for code analysis and optimization},
 abstract = {Many code analysis techniques for optimization, debugging, or parallelization need to perform runtime disambiguation of sets of addresses. Such operations can be supported efficiently and with low complexity with hardware signatures. To enable flexible use of signatures, this paper proposes to expose a Signature Register File to the software through a rich ISA. The software has great flexibility to decide, for each signature,which addresses to collect and which addresses to disambiguate against. We call this architecture SoftSig</i>. In addition, as an example of SoftSig use, we show how to detect redundant function calls efficiently and eliminate them dynamically. We call this algorithm MemoiSE</i>. On average for five popular applications, MemoiSE reduces the number of dynamic instructions by 9.3\%, thereby reducing the execution time of the applications by 9\%.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346300},
 doi = {http://doi.acm.org/10.1145/1346281.1346300},
 acmid = {1346300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory disambiguation, multi-core architectures, runtime optimization},
} 

@article{Tuck:2008:SSH:1353534.1346300,
 author = {Tuck, James and Ahn, Wonsun and Ceze, Luis and Torrellas, Josep},
 title = {SoftSig: software-exposed hardware signatures for code analysis and optimization},
 abstract = {Many code analysis techniques for optimization, debugging, or parallelization need to perform runtime disambiguation of sets of addresses. Such operations can be supported efficiently and with low complexity with hardware signatures. To enable flexible use of signatures, this paper proposes to expose a Signature Register File to the software through a rich ISA. The software has great flexibility to decide, for each signature,which addresses to collect and which addresses to disambiguate against. We call this architecture SoftSig</i>. In addition, as an example of SoftSig use, we show how to detect redundant function calls efficiently and eliminate them dynamically. We call this algorithm MemoiSE</i>. On average for five popular applications, MemoiSE reduces the number of dynamic instructions by 9.3\%, thereby reducing the execution time of the applications by 9\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346300},
 doi = {http://doi.acm.org/10.1145/1353534.1346300},
 acmid = {1346300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory disambiguation, multi-core architectures, runtime optimization},
} 

@article{Burcea:2008:PV:1353534.1346301,
 author = {Burcea, Ioana and Somogyi, Stephen and Moshovos, Andreas and Falsafi, Babak},
 title = {Predictor virtualization},
 abstract = {Many hardware optimizations rely on collecting information about program behavior at runtime. This information is stored in lookup tables. To be accurate and effective, these optimizations usually require large dedicated on-chip tables. Although technology advances offer an increased amount of on-chip resources, these resources are allocated to increase the size of on-chip conventional cache hierarchies. This work proposes Predictor Virtualization</i>, a technique that uses the existing memory hierarchy to emulate large predictor tables. We demonstrate the benefits of this technique by virtualizing a state-of-the-art data prefetcher. Full-system, cycle-accurate simulations demonstrate that the virtualized prefetcher preserves the performance benefits of the original design, while reducing the on-chip storage dedicated to the predictor table from 60KB down to less than one kilobyte.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {157--167},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346301},
 doi = {http://doi.acm.org/10.1145/1353534.1346301},
 acmid = {1346301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, memory hierarchy, metadata, predictor virtualization},
} 

@inproceedings{Burcea:2008:PV:1346281.1346301,
 author = {Burcea, Ioana and Somogyi, Stephen and Moshovos, Andreas and Falsafi, Babak},
 title = {Predictor virtualization},
 abstract = {Many hardware optimizations rely on collecting information about program behavior at runtime. This information is stored in lookup tables. To be accurate and effective, these optimizations usually require large dedicated on-chip tables. Although technology advances offer an increased amount of on-chip resources, these resources are allocated to increase the size of on-chip conventional cache hierarchies. This work proposes Predictor Virtualization</i>, a technique that uses the existing memory hierarchy to emulate large predictor tables. We demonstrate the benefits of this technique by virtualizing a state-of-the-art data prefetcher. Full-system, cycle-accurate simulations demonstrate that the virtualized prefetcher preserves the performance benefits of the original design, while reducing the on-chip storage dedicated to the predictor table from 60KB down to less than one kilobyte.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {157--167},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346301},
 doi = {http://doi.acm.org/10.1145/1346281.1346301},
 acmid = {1346301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, memory hierarchy, metadata, predictor virtualization},
} 

@article{Burcea:2008:PV:1353535.1346301,
 author = {Burcea, Ioana and Somogyi, Stephen and Moshovos, Andreas and Falsafi, Babak},
 title = {Predictor virtualization},
 abstract = {Many hardware optimizations rely on collecting information about program behavior at runtime. This information is stored in lookup tables. To be accurate and effective, these optimizations usually require large dedicated on-chip tables. Although technology advances offer an increased amount of on-chip resources, these resources are allocated to increase the size of on-chip conventional cache hierarchies. This work proposes Predictor Virtualization</i>, a technique that uses the existing memory hierarchy to emulate large predictor tables. We demonstrate the benefits of this technique by virtualizing a state-of-the-art data prefetcher. Full-system, cycle-accurate simulations demonstrate that the virtualized prefetcher preserves the performance benefits of the original design, while reducing the on-chip storage dedicated to the predictor table from 60KB down to less than one kilobyte.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {157--167},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346301},
 doi = {http://doi.acm.org/10.1145/1353535.1346301},
 acmid = {1346301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, memory hierarchy, metadata, predictor virtualization},
} 

@article{Burcea:2008:PV:1353536.1346301,
 author = {Burcea, Ioana and Somogyi, Stephen and Moshovos, Andreas and Falsafi, Babak},
 title = {Predictor virtualization},
 abstract = {Many hardware optimizations rely on collecting information about program behavior at runtime. This information is stored in lookup tables. To be accurate and effective, these optimizations usually require large dedicated on-chip tables. Although technology advances offer an increased amount of on-chip resources, these resources are allocated to increase the size of on-chip conventional cache hierarchies. This work proposes Predictor Virtualization</i>, a technique that uses the existing memory hierarchy to emulate large predictor tables. We demonstrate the benefits of this technique by virtualizing a state-of-the-art data prefetcher. Full-system, cycle-accurate simulations demonstrate that the virtualized prefetcher preserves the performance benefits of the original design, while reducing the on-chip storage dedicated to the predictor table from 60KB down to less than one kilobyte.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {157--167},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346301},
 doi = {http://doi.acm.org/10.1145/1353536.1346301},
 acmid = {1346301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, memory hierarchy, metadata, predictor virtualization},
} 

@inproceedings{Ganapathy:2008:DIM:1346281.1346303,
 author = {Ganapathy, Vinod and Renzelmann, Matthew J. and Balakrishnan, Arini and Swift, Michael M. and Jha, Somesh},
 title = {The design and implementation of microdrivers},
 abstract = {Device drivers commonly execute in the kernel to achieve high performance and easy access to kernel services. However, this comes at the price of decreased reliability and increased programming difficulty. Driver programmers are unable to use user-mode development tools and must instead use cumbersome kernel tools. Faults in kernel drivers can cause the entire operating system to crash. User-mode drivers have long been seen as a solution to this problem, but suffer from either poor performance or new interfaces that require a rewrite of existing drivers. This paper introduces the Microdrivers architecture that achieves high performance and compatibility by leaving critical path code in the kernel and moving the rest of the driver code to a user-mode process. This allows data-handling operations critical to I/O performance to run at full speed, while management operations such as initialization and configuration run at reduced speed in user-level. To achieve compatibility, we present DriverSlicer, a tool that splits existing kernel drivers into a kernel-level component and a user-level component using a small number of programmer annotations. Experiments show that as much as 65\% of driver code can be removed from the kernel without affecting common-case performance, and that only 1-6 percent of the code requires annotations.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {168--178},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346303},
 doi = {http://doi.acm.org/10.1145/1346281.1346303},
 acmid = {1346303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, program partitioning, reliability},
} 

@article{Ganapathy:2008:DIM:1353536.1346303,
 author = {Ganapathy, Vinod and Renzelmann, Matthew J. and Balakrishnan, Arini and Swift, Michael M. and Jha, Somesh},
 title = {The design and implementation of microdrivers},
 abstract = {Device drivers commonly execute in the kernel to achieve high performance and easy access to kernel services. However, this comes at the price of decreased reliability and increased programming difficulty. Driver programmers are unable to use user-mode development tools and must instead use cumbersome kernel tools. Faults in kernel drivers can cause the entire operating system to crash. User-mode drivers have long been seen as a solution to this problem, but suffer from either poor performance or new interfaces that require a rewrite of existing drivers. This paper introduces the Microdrivers architecture that achieves high performance and compatibility by leaving critical path code in the kernel and moving the rest of the driver code to a user-mode process. This allows data-handling operations critical to I/O performance to run at full speed, while management operations such as initialization and configuration run at reduced speed in user-level. To achieve compatibility, we present DriverSlicer, a tool that splits existing kernel drivers into a kernel-level component and a user-level component using a small number of programmer annotations. Experiments show that as much as 65\% of driver code can be removed from the kernel without affecting common-case performance, and that only 1-6 percent of the code requires annotations.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {168--178},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346303},
 doi = {http://doi.acm.org/10.1145/1353536.1346303},
 acmid = {1346303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, program partitioning, reliability},
} 

@article{Ganapathy:2008:DIM:1353535.1346303,
 author = {Ganapathy, Vinod and Renzelmann, Matthew J. and Balakrishnan, Arini and Swift, Michael M. and Jha, Somesh},
 title = {The design and implementation of microdrivers},
 abstract = {Device drivers commonly execute in the kernel to achieve high performance and easy access to kernel services. However, this comes at the price of decreased reliability and increased programming difficulty. Driver programmers are unable to use user-mode development tools and must instead use cumbersome kernel tools. Faults in kernel drivers can cause the entire operating system to crash. User-mode drivers have long been seen as a solution to this problem, but suffer from either poor performance or new interfaces that require a rewrite of existing drivers. This paper introduces the Microdrivers architecture that achieves high performance and compatibility by leaving critical path code in the kernel and moving the rest of the driver code to a user-mode process. This allows data-handling operations critical to I/O performance to run at full speed, while management operations such as initialization and configuration run at reduced speed in user-level. To achieve compatibility, we present DriverSlicer, a tool that splits existing kernel drivers into a kernel-level component and a user-level component using a small number of programmer annotations. Experiments show that as much as 65\% of driver code can be removed from the kernel without affecting common-case performance, and that only 1-6 percent of the code requires annotations.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {168--178},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346303},
 doi = {http://doi.acm.org/10.1145/1353535.1346303},
 acmid = {1346303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, program partitioning, reliability},
} 

@article{Ganapathy:2008:DIM:1353534.1346303,
 author = {Ganapathy, Vinod and Renzelmann, Matthew J. and Balakrishnan, Arini and Swift, Michael M. and Jha, Somesh},
 title = {The design and implementation of microdrivers},
 abstract = {Device drivers commonly execute in the kernel to achieve high performance and easy access to kernel services. However, this comes at the price of decreased reliability and increased programming difficulty. Driver programmers are unable to use user-mode development tools and must instead use cumbersome kernel tools. Faults in kernel drivers can cause the entire operating system to crash. User-mode drivers have long been seen as a solution to this problem, but suffer from either poor performance or new interfaces that require a rewrite of existing drivers. This paper introduces the Microdrivers architecture that achieves high performance and compatibility by leaving critical path code in the kernel and moving the rest of the driver code to a user-mode process. This allows data-handling operations critical to I/O performance to run at full speed, while management operations such as initialization and configuration run at reduced speed in user-level. To achieve compatibility, we present DriverSlicer, a tool that splits existing kernel drivers into a kernel-level component and a user-level component using a small number of programmer annotations. Experiments show that as much as 65\% of driver code can be removed from the kernel without affecting common-case performance, and that only 1-6 percent of the code requires annotations.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {168--178},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346303},
 doi = {http://doi.acm.org/10.1145/1353534.1346303},
 acmid = {1346303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, program partitioning, reliability},
} 

@article{Weinsberg:2008:TFC:1353535.1346304,
 author = {Weinsberg, Yaron and Dolev, Danny and Anker, Tal and Ben-Yehuda, Muli and Wyckoff, Pete},
 title = {Tapping into the fountain of CPUs: on operating system support for programmable devices},
 abstract = {The constant race for faster and more powerful CPUs is drawing to a close. No longer is it feasible to significantly increase the speed of the CPU without paying a crushing penalty in power consumption and production costs. Instead of increasing single thread performance, the industry is turning to multiple CPU threads or cores (such as SMT and CMP) and heterogeneous CPU architectures (such as the Cell Broadband Engine). While this is a step in the right direction, in every modern PC there is a wealth of untapped compute resources. The NIC has a CPU; the disk controller is programmable; some high-end graphics adaptersare already more powerful than host CPUs. Some of these CPUs can perform some functions more efficiently than the host CPUs. Our operating systems and programming abstractions should be expanded to let applications tap into these computational resources and make the best use of them. Therefore, we propose the H<sc>YDRA</sc> framework, which lets application developers use the combined power of every compute resource in a coherent way. H<sc>YDRA</sc> is a programming model and a runtime support layer which enables utilization of host processors as well as various programmable peripheral devices' processors. We present the frameworkand its application for a demonstrative use-case, as well as provide a thorough evaluation of its capabilities. Using H<sc>YDRA</sc> we were able to cut down the development cost of a system that uses multiple heterogenous compute resources significantly.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346304},
 doi = {http://doi.acm.org/10.1145/1353535.1346304},
 acmid = {1346304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {offloading, operating systems, programming model},
} 

@article{Weinsberg:2008:TFC:1353536.1346304,
 author = {Weinsberg, Yaron and Dolev, Danny and Anker, Tal and Ben-Yehuda, Muli and Wyckoff, Pete},
 title = {Tapping into the fountain of CPUs: on operating system support for programmable devices},
 abstract = {The constant race for faster and more powerful CPUs is drawing to a close. No longer is it feasible to significantly increase the speed of the CPU without paying a crushing penalty in power consumption and production costs. Instead of increasing single thread performance, the industry is turning to multiple CPU threads or cores (such as SMT and CMP) and heterogeneous CPU architectures (such as the Cell Broadband Engine). While this is a step in the right direction, in every modern PC there is a wealth of untapped compute resources. The NIC has a CPU; the disk controller is programmable; some high-end graphics adaptersare already more powerful than host CPUs. Some of these CPUs can perform some functions more efficiently than the host CPUs. Our operating systems and programming abstractions should be expanded to let applications tap into these computational resources and make the best use of them. Therefore, we propose the H<sc>YDRA</sc> framework, which lets application developers use the combined power of every compute resource in a coherent way. H<sc>YDRA</sc> is a programming model and a runtime support layer which enables utilization of host processors as well as various programmable peripheral devices' processors. We present the frameworkand its application for a demonstrative use-case, as well as provide a thorough evaluation of its capabilities. Using H<sc>YDRA</sc> we were able to cut down the development cost of a system that uses multiple heterogenous compute resources significantly.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346304},
 doi = {http://doi.acm.org/10.1145/1353536.1346304},
 acmid = {1346304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {offloading, operating systems, programming model},
} 

@article{Weinsberg:2008:TFC:1353534.1346304,
 author = {Weinsberg, Yaron and Dolev, Danny and Anker, Tal and Ben-Yehuda, Muli and Wyckoff, Pete},
 title = {Tapping into the fountain of CPUs: on operating system support for programmable devices},
 abstract = {The constant race for faster and more powerful CPUs is drawing to a close. No longer is it feasible to significantly increase the speed of the CPU without paying a crushing penalty in power consumption and production costs. Instead of increasing single thread performance, the industry is turning to multiple CPU threads or cores (such as SMT and CMP) and heterogeneous CPU architectures (such as the Cell Broadband Engine). While this is a step in the right direction, in every modern PC there is a wealth of untapped compute resources. The NIC has a CPU; the disk controller is programmable; some high-end graphics adaptersare already more powerful than host CPUs. Some of these CPUs can perform some functions more efficiently than the host CPUs. Our operating systems and programming abstractions should be expanded to let applications tap into these computational resources and make the best use of them. Therefore, we propose the H<sc>YDRA</sc> framework, which lets application developers use the combined power of every compute resource in a coherent way. H<sc>YDRA</sc> is a programming model and a runtime support layer which enables utilization of host processors as well as various programmable peripheral devices' processors. We present the frameworkand its application for a demonstrative use-case, as well as provide a thorough evaluation of its capabilities. Using H<sc>YDRA</sc> we were able to cut down the development cost of a system that uses multiple heterogenous compute resources significantly.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346304},
 doi = {http://doi.acm.org/10.1145/1353534.1346304},
 acmid = {1346304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {offloading, operating systems, programming model},
} 

@inproceedings{Weinsberg:2008:TFC:1346281.1346304,
 author = {Weinsberg, Yaron and Dolev, Danny and Anker, Tal and Ben-Yehuda, Muli and Wyckoff, Pete},
 title = {Tapping into the fountain of CPUs: on operating system support for programmable devices},
 abstract = {The constant race for faster and more powerful CPUs is drawing to a close. No longer is it feasible to significantly increase the speed of the CPU without paying a crushing penalty in power consumption and production costs. Instead of increasing single thread performance, the industry is turning to multiple CPU threads or cores (such as SMT and CMP) and heterogeneous CPU architectures (such as the Cell Broadband Engine). While this is a step in the right direction, in every modern PC there is a wealth of untapped compute resources. The NIC has a CPU; the disk controller is programmable; some high-end graphics adaptersare already more powerful than host CPUs. Some of these CPUs can perform some functions more efficiently than the host CPUs. Our operating systems and programming abstractions should be expanded to let applications tap into these computational resources and make the best use of them. Therefore, we propose the H<sc>YDRA</sc> framework, which lets application developers use the combined power of every compute resource in a coherent way. H<sc>YDRA</sc> is a programming model and a runtime support layer which enables utilization of host processors as well as various programmable peripheral devices' processors. We present the frameworkand its application for a demonstrative use-case, as well as provide a thorough evaluation of its capabilities. Using H<sc>YDRA</sc> we were able to cut down the development cost of a system that uses multiple heterogenous compute resources significantly.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346304},
 doi = {http://doi.acm.org/10.1145/1346281.1346304},
 acmid = {1346304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {offloading, operating systems, programming model},
} 

@article{Shen:2008:HCD:1353534.1346306,
 author = {Shen, Kai and Zhong, Ming and Dwarkadas, Sandhya and Li, Chuanpeng and Stewart, Christopher and Zhang, Xiao},
 title = {Hardware counter driven on-the-fly request signatures},
 abstract = {Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g.</i>, CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly</i> request identification and property inference allow guided operating system adaptation at request granularity (e.g.</i>, resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7\%, 3\%, 20\%, and 41\% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82\% errors). Its use for resource-aware request scheduling results in a 15-70\% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {189--200},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346306},
 doi = {http://doi.acm.org/10.1145/1353534.1346306},
 acmid = {1346306},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, hardware counter, operating system adaptation, request classification, server system},
} 

@article{Shen:2008:HCD:1353535.1346306,
 author = {Shen, Kai and Zhong, Ming and Dwarkadas, Sandhya and Li, Chuanpeng and Stewart, Christopher and Zhang, Xiao},
 title = {Hardware counter driven on-the-fly request signatures},
 abstract = {Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g.</i>, CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly</i> request identification and property inference allow guided operating system adaptation at request granularity (e.g.</i>, resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7\%, 3\%, 20\%, and 41\% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82\% errors). Its use for resource-aware request scheduling results in a 15-70\% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {189--200},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346306},
 doi = {http://doi.acm.org/10.1145/1353535.1346306},
 acmid = {1346306},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, hardware counter, operating system adaptation, request classification, server system},
} 

@article{Shen:2008:HCD:1353536.1346306,
 author = {Shen, Kai and Zhong, Ming and Dwarkadas, Sandhya and Li, Chuanpeng and Stewart, Christopher and Zhang, Xiao},
 title = {Hardware counter driven on-the-fly request signatures},
 abstract = {Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g.</i>, CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly</i> request identification and property inference allow guided operating system adaptation at request granularity (e.g.</i>, resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7\%, 3\%, 20\%, and 41\% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82\% errors). Its use for resource-aware request scheduling results in a 15-70\% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {189--200},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346306},
 doi = {http://doi.acm.org/10.1145/1353536.1346306},
 acmid = {1346306},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, hardware counter, operating system adaptation, request classification, server system},
} 

@inproceedings{Shen:2008:HCD:1346281.1346306,
 author = {Shen, Kai and Zhong, Ming and Dwarkadas, Sandhya and Li, Chuanpeng and Stewart, Christopher and Zhang, Xiao},
 title = {Hardware counter driven on-the-fly request signatures},
 abstract = {Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g.</i>, CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly</i> request identification and property inference allow guided operating system adaptation at request granularity (e.g.</i>, resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7\%, 3\%, 20\%, and 41\% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82\% errors). Its use for resource-aware request scheduling results in a 15-70\% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {189--200},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346306},
 doi = {http://doi.acm.org/10.1145/1346281.1346306},
 acmid = {1346306},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, hardware counter, operating system adaptation, request classification, server system},
} 

@inproceedings{Van Ertvelde:2008:DPA:1346281.1346307,
 author = {Van Ertvelde, Luk and Eeckhout, Lieven},
 title = {Dispersing proprietary applications as benchmarks through code mutation},
 abstract = {Industry vendors hesitate to disseminate proprietary applications to academia and third party vendors. By consequence, the benchmarking process is typically driven by standardized, open-source benchmarks which may be very different from and likely not representative of the real-life applications of interest. This paper proposes code mutation, a novel technique that mutates a proprietary application to complicate reverse engineering so that it can be distributed as a benchmark. The benchmark mutant then serves as a proxy for the proprietary application. The key idea in the proposed code mutation approach is to preserve the proprietary application's dynamic memory access and/or control flow behavior in the benchmark mutant while mutating the rest of the application code. To this end, we compute program slices for memory access operations and/or control flow operationstrimmed through constant value and branch profiles; and subsequently mutate the instructions not appearing in these slices through binary rewriting. Our experimental results using SPEC CPU2000 and MiBench benchmarks show that code mutation is a promising technique that mutates up to 90\% of the static binary, up to 50\% of the dynamically executed instructions, and up to 35\% of the at run time exposed inter-operation data dependencies. The performance characteristics of the mutant are very similar to those of the proprietary application across a wide range of microarchitectures and hardware implementations.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {201--210},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346307},
 doi = {http://doi.acm.org/10.1145/1346281.1346307},
 acmid = {1346307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark generation, code mutation},
} 

@article{Van Ertvelde:2008:DPA:1353534.1346307,
 author = {Van Ertvelde, Luk and Eeckhout, Lieven},
 title = {Dispersing proprietary applications as benchmarks through code mutation},
 abstract = {Industry vendors hesitate to disseminate proprietary applications to academia and third party vendors. By consequence, the benchmarking process is typically driven by standardized, open-source benchmarks which may be very different from and likely not representative of the real-life applications of interest. This paper proposes code mutation, a novel technique that mutates a proprietary application to complicate reverse engineering so that it can be distributed as a benchmark. The benchmark mutant then serves as a proxy for the proprietary application. The key idea in the proposed code mutation approach is to preserve the proprietary application's dynamic memory access and/or control flow behavior in the benchmark mutant while mutating the rest of the application code. To this end, we compute program slices for memory access operations and/or control flow operationstrimmed through constant value and branch profiles; and subsequently mutate the instructions not appearing in these slices through binary rewriting. Our experimental results using SPEC CPU2000 and MiBench benchmarks show that code mutation is a promising technique that mutates up to 90\% of the static binary, up to 50\% of the dynamically executed instructions, and up to 35\% of the at run time exposed inter-operation data dependencies. The performance characteristics of the mutant are very similar to those of the proprietary application across a wide range of microarchitectures and hardware implementations.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {201--210},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346307},
 doi = {http://doi.acm.org/10.1145/1353534.1346307},
 acmid = {1346307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark generation, code mutation},
} 

@article{Van Ertvelde:2008:DPA:1353535.1346307,
 author = {Van Ertvelde, Luk and Eeckhout, Lieven},
 title = {Dispersing proprietary applications as benchmarks through code mutation},
 abstract = {Industry vendors hesitate to disseminate proprietary applications to academia and third party vendors. By consequence, the benchmarking process is typically driven by standardized, open-source benchmarks which may be very different from and likely not representative of the real-life applications of interest. This paper proposes code mutation, a novel technique that mutates a proprietary application to complicate reverse engineering so that it can be distributed as a benchmark. The benchmark mutant then serves as a proxy for the proprietary application. The key idea in the proposed code mutation approach is to preserve the proprietary application's dynamic memory access and/or control flow behavior in the benchmark mutant while mutating the rest of the application code. To this end, we compute program slices for memory access operations and/or control flow operationstrimmed through constant value and branch profiles; and subsequently mutate the instructions not appearing in these slices through binary rewriting. Our experimental results using SPEC CPU2000 and MiBench benchmarks show that code mutation is a promising technique that mutates up to 90\% of the static binary, up to 50\% of the dynamically executed instructions, and up to 35\% of the at run time exposed inter-operation data dependencies. The performance characteristics of the mutant are very similar to those of the proprietary application across a wide range of microarchitectures and hardware implementations.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {201--210},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346307},
 doi = {http://doi.acm.org/10.1145/1353535.1346307},
 acmid = {1346307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark generation, code mutation},
} 

@article{Van Ertvelde:2008:DPA:1353536.1346307,
 author = {Van Ertvelde, Luk and Eeckhout, Lieven},
 title = {Dispersing proprietary applications as benchmarks through code mutation},
 abstract = {Industry vendors hesitate to disseminate proprietary applications to academia and third party vendors. By consequence, the benchmarking process is typically driven by standardized, open-source benchmarks which may be very different from and likely not representative of the real-life applications of interest. This paper proposes code mutation, a novel technique that mutates a proprietary application to complicate reverse engineering so that it can be distributed as a benchmark. The benchmark mutant then serves as a proxy for the proprietary application. The key idea in the proposed code mutation approach is to preserve the proprietary application's dynamic memory access and/or control flow behavior in the benchmark mutant while mutating the rest of the application code. To this end, we compute program slices for memory access operations and/or control flow operationstrimmed through constant value and branch profiles; and subsequently mutate the instructions not appearing in these slices through binary rewriting. Our experimental results using SPEC CPU2000 and MiBench benchmarks show that code mutation is a promising technique that mutates up to 90\% of the static binary, up to 50\% of the dynamically executed instructions, and up to 35\% of the at run time exposed inter-operation data dependencies. The performance characteristics of the mutant are very similar to those of the proprietary application across a wide range of microarchitectures and hardware implementations.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {201--210},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346307},
 doi = {http://doi.acm.org/10.1145/1353536.1346307},
 acmid = {1346307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark generation, code mutation},
} 

@article{Mysore:2008:UVF:1353536.1346308,
 author = {Mysore, Shashidhar and Mazloom, Bita and Agrawal, Banit and Sherwood, Timothy},
 title = {Understanding and visualizing full systems with data flow tomography},
 abstract = {It is not uncommon for modern systems to be composed of a variety of interacting services, running across multiple machines in such a way that most developers do not really understand the whole system. As abstraction is layered atop abstraction, developers gain the ability to compose systems of extraordinary complexity with relative ease. However, many software properties, especially those that cut across abstraction layers, become very difficult to understand in such compositions. The communication patterns involved, the privacy of critical data, and the provenance of information, can be difficult to find and understand, even with access to all of the source code. The goal of Data Flow Tomography is to use the inherent information flow of such systems to help visualize the interactions between complex and interwoven components across multiple layers of abstraction. In the same way that the injection of short-lived radioactive isotopes help doctors trace problems in the cardiovascular system, the use of "data tagging" can help developers slice through the extraneous layers of software and pin-point those portions of the system interacting with the data of interest. To demonstrate the feasibility of this approach we have developed a prototype system in which tags are tracked both through the machine and in between machines over the network, and from which novel visualizations of the whole system can be derived. We describe the system-level challenges in creating a working system tomography tool and we qualitatively evaluate our system by examining several example real world scenarios.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {211--221},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346308},
 doi = {http://doi.acm.org/10.1145/1353536.1346308},
 acmid = {1346308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow tracking, tomography, virtual machine},
} 

@inproceedings{Mysore:2008:UVF:1346281.1346308,
 author = {Mysore, Shashidhar and Mazloom, Bita and Agrawal, Banit and Sherwood, Timothy},
 title = {Understanding and visualizing full systems with data flow tomography},
 abstract = {It is not uncommon for modern systems to be composed of a variety of interacting services, running across multiple machines in such a way that most developers do not really understand the whole system. As abstraction is layered atop abstraction, developers gain the ability to compose systems of extraordinary complexity with relative ease. However, many software properties, especially those that cut across abstraction layers, become very difficult to understand in such compositions. The communication patterns involved, the privacy of critical data, and the provenance of information, can be difficult to find and understand, even with access to all of the source code. The goal of Data Flow Tomography is to use the inherent information flow of such systems to help visualize the interactions between complex and interwoven components across multiple layers of abstraction. In the same way that the injection of short-lived radioactive isotopes help doctors trace problems in the cardiovascular system, the use of "data tagging" can help developers slice through the extraneous layers of software and pin-point those portions of the system interacting with the data of interest. To demonstrate the feasibility of this approach we have developed a prototype system in which tags are tracked both through the machine and in between machines over the network, and from which novel visualizations of the whole system can be derived. We describe the system-level challenges in creating a working system tomography tool and we qualitatively evaluate our system by examining several example real world scenarios.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {211--221},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346308},
 doi = {http://doi.acm.org/10.1145/1346281.1346308},
 acmid = {1346308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow tracking, tomography, virtual machine},
} 

@article{Mysore:2008:UVF:1353534.1346308,
 author = {Mysore, Shashidhar and Mazloom, Bita and Agrawal, Banit and Sherwood, Timothy},
 title = {Understanding and visualizing full systems with data flow tomography},
 abstract = {It is not uncommon for modern systems to be composed of a variety of interacting services, running across multiple machines in such a way that most developers do not really understand the whole system. As abstraction is layered atop abstraction, developers gain the ability to compose systems of extraordinary complexity with relative ease. However, many software properties, especially those that cut across abstraction layers, become very difficult to understand in such compositions. The communication patterns involved, the privacy of critical data, and the provenance of information, can be difficult to find and understand, even with access to all of the source code. The goal of Data Flow Tomography is to use the inherent information flow of such systems to help visualize the interactions between complex and interwoven components across multiple layers of abstraction. In the same way that the injection of short-lived radioactive isotopes help doctors trace problems in the cardiovascular system, the use of "data tagging" can help developers slice through the extraneous layers of software and pin-point those portions of the system interacting with the data of interest. To demonstrate the feasibility of this approach we have developed a prototype system in which tags are tracked both through the machine and in between machines over the network, and from which novel visualizations of the whole system can be derived. We describe the system-level challenges in creating a working system tomography tool and we qualitatively evaluate our system by examining several example real world scenarios.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {211--221},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346308},
 doi = {http://doi.acm.org/10.1145/1353534.1346308},
 acmid = {1346308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow tracking, tomography, virtual machine},
} 

@article{Mysore:2008:UVF:1353535.1346308,
 author = {Mysore, Shashidhar and Mazloom, Bita and Agrawal, Banit and Sherwood, Timothy},
 title = {Understanding and visualizing full systems with data flow tomography},
 abstract = {It is not uncommon for modern systems to be composed of a variety of interacting services, running across multiple machines in such a way that most developers do not really understand the whole system. As abstraction is layered atop abstraction, developers gain the ability to compose systems of extraordinary complexity with relative ease. However, many software properties, especially those that cut across abstraction layers, become very difficult to understand in such compositions. The communication patterns involved, the privacy of critical data, and the provenance of information, can be difficult to find and understand, even with access to all of the source code. The goal of Data Flow Tomography is to use the inherent information flow of such systems to help visualize the interactions between complex and interwoven components across multiple layers of abstraction. In the same way that the injection of short-lived radioactive isotopes help doctors trace problems in the cardiovascular system, the use of "data tagging" can help developers slice through the extraneous layers of software and pin-point those portions of the system interacting with the data of interest. To demonstrate the feasibility of this approach we have developed a prototype system in which tags are tracked both through the machine and in between machines over the network, and from which novel visualizations of the whole system can be derived. We describe the system-level challenges in creating a working system tomography tool and we qualitatively evaluate our system by examining several example real world scenarios.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {211--221},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346308},
 doi = {http://doi.acm.org/10.1145/1353535.1346308},
 acmid = {1346308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow tracking, tomography, virtual machine},
} 

@article{Ottoni:2008:COG:1353535.1346310,
 author = {Ottoni, Guilherme and August, David I.},
 title = {Communication optimizations for global multi-threaded instruction scheduling},
 abstract = {The recent shift in the industry towards chip multiprocessor (CMP) designs has brought the need for multi-threaded applications to mainstream computing. As observed in several limit studies, most of the parallelization opportunities require looking for parallelism beyond local regions of code. To exploit these opportunities, especially for sequential applications, researchers have recently proposed global multi-threaded instruction scheduling techniques, including DSWP and GREMIO. These techniques simultaneously schedule instructions from large regions of code, such as arbitrary loop nests or whole procedures, and have been shown to be effective at extracting threads for many applications. A key enabler of these global instruction scheduling techniques is the Multi-Threaded Code Generation (MTCG) algorithm proposed in [16], which generates multi-threaded code for any partition of the instructions into threads. This algorithm inserts communication and synchronization instructions in order to satisfy all inter-thread dependences. In this paper, we present a general compiler framework, COCO, to optimize the communication and synchronization instructions inserted by the MTCG algorithm. This framework, based on thread-aware data-flow analyses and graph min-cut algorithms, appropriately models andoptimizes all kinds of inter-thread dependences, including register, memory, and control dependences. Our experiments, using a fully automatic compiler implementation of these techniques, demonstrate significant reductions (about 30\% on average) in the number of dynamic communication instructions in code parallelized with DSWP and GREMIO. This reduction in communication translates to performance gains of up to 40\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346310},
 doi = {http://doi.acm.org/10.1145/1353535.1346310},
 acmid = {1346310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication, data-flow analysis, graph min-cut, instruction scheduling, multi-threading, synchronization},
} 

@article{Ottoni:2008:COG:1353534.1346310,
 author = {Ottoni, Guilherme and August, David I.},
 title = {Communication optimizations for global multi-threaded instruction scheduling},
 abstract = {The recent shift in the industry towards chip multiprocessor (CMP) designs has brought the need for multi-threaded applications to mainstream computing. As observed in several limit studies, most of the parallelization opportunities require looking for parallelism beyond local regions of code. To exploit these opportunities, especially for sequential applications, researchers have recently proposed global multi-threaded instruction scheduling techniques, including DSWP and GREMIO. These techniques simultaneously schedule instructions from large regions of code, such as arbitrary loop nests or whole procedures, and have been shown to be effective at extracting threads for many applications. A key enabler of these global instruction scheduling techniques is the Multi-Threaded Code Generation (MTCG) algorithm proposed in [16], which generates multi-threaded code for any partition of the instructions into threads. This algorithm inserts communication and synchronization instructions in order to satisfy all inter-thread dependences. In this paper, we present a general compiler framework, COCO, to optimize the communication and synchronization instructions inserted by the MTCG algorithm. This framework, based on thread-aware data-flow analyses and graph min-cut algorithms, appropriately models andoptimizes all kinds of inter-thread dependences, including register, memory, and control dependences. Our experiments, using a fully automatic compiler implementation of these techniques, demonstrate significant reductions (about 30\% on average) in the number of dynamic communication instructions in code parallelized with DSWP and GREMIO. This reduction in communication translates to performance gains of up to 40\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346310},
 doi = {http://doi.acm.org/10.1145/1353534.1346310},
 acmid = {1346310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication, data-flow analysis, graph min-cut, instruction scheduling, multi-threading, synchronization},
} 

@inproceedings{Ottoni:2008:COG:1346281.1346310,
 author = {Ottoni, Guilherme and August, David I.},
 title = {Communication optimizations for global multi-threaded instruction scheduling},
 abstract = {The recent shift in the industry towards chip multiprocessor (CMP) designs has brought the need for multi-threaded applications to mainstream computing. As observed in several limit studies, most of the parallelization opportunities require looking for parallelism beyond local regions of code. To exploit these opportunities, especially for sequential applications, researchers have recently proposed global multi-threaded instruction scheduling techniques, including DSWP and GREMIO. These techniques simultaneously schedule instructions from large regions of code, such as arbitrary loop nests or whole procedures, and have been shown to be effective at extracting threads for many applications. A key enabler of these global instruction scheduling techniques is the Multi-Threaded Code Generation (MTCG) algorithm proposed in [16], which generates multi-threaded code for any partition of the instructions into threads. This algorithm inserts communication and synchronization instructions in order to satisfy all inter-thread dependences. In this paper, we present a general compiler framework, COCO, to optimize the communication and synchronization instructions inserted by the MTCG algorithm. This framework, based on thread-aware data-flow analyses and graph min-cut algorithms, appropriately models andoptimizes all kinds of inter-thread dependences, including register, memory, and control dependences. Our experiments, using a fully automatic compiler implementation of these techniques, demonstrate significant reductions (about 30\% on average) in the number of dynamic communication instructions in code parallelized with DSWP and GREMIO. This reduction in communication translates to performance gains of up to 40\%.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346310},
 doi = {http://doi.acm.org/10.1145/1346281.1346310},
 acmid = {1346310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication, data-flow analysis, graph min-cut, instruction scheduling, multi-threading, synchronization},
} 

@article{Ottoni:2008:COG:1353536.1346310,
 author = {Ottoni, Guilherme and August, David I.},
 title = {Communication optimizations for global multi-threaded instruction scheduling},
 abstract = {The recent shift in the industry towards chip multiprocessor (CMP) designs has brought the need for multi-threaded applications to mainstream computing. As observed in several limit studies, most of the parallelization opportunities require looking for parallelism beyond local regions of code. To exploit these opportunities, especially for sequential applications, researchers have recently proposed global multi-threaded instruction scheduling techniques, including DSWP and GREMIO. These techniques simultaneously schedule instructions from large regions of code, such as arbitrary loop nests or whole procedures, and have been shown to be effective at extracting threads for many applications. A key enabler of these global instruction scheduling techniques is the Multi-Threaded Code Generation (MTCG) algorithm proposed in [16], which generates multi-threaded code for any partition of the instructions into threads. This algorithm inserts communication and synchronization instructions in order to satisfy all inter-thread dependences. In this paper, we present a general compiler framework, COCO, to optimize the communication and synchronization instructions inserted by the MTCG algorithm. This framework, based on thread-aware data-flow analyses and graph min-cut algorithms, appropriately models andoptimizes all kinds of inter-thread dependences, including register, memory, and control dependences. Our experiments, using a fully automatic compiler implementation of these techniques, demonstrate significant reductions (about 30\% on average) in the number of dynamic communication instructions in code parallelized with DSWP and GREMIO. This reduction in communication translates to performance gains of up to 40\%.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346310},
 doi = {http://doi.acm.org/10.1145/1353536.1346310},
 acmid = {1346310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication, data-flow analysis, graph min-cut, instruction scheduling, multi-threading, synchronization},
} 

@article{Kulkarni:2008:OPB:1353534.1346311,
 author = {Kulkarni, Milind and Pingali, Keshav and Ramanarayanan, Ganesh and Walter, Bruce and Bala, Kavita and Chew, L. Paul},
 title = {Optimistic parallelism benefits from data partitioning},
 abstract = {Recent studies of irregular applications such as finite-element mesh generators and data-clustering codes have shown that these applications have a generalized data parallelism arising from the use of iterative algorithms that perform computations on elements of worklists. In some irregular applications, the computations on different elements are independent. In other applications, there may be complex patterns of dependences between these computations. The Galois system was designed to exploit this kind of irregular data parallelism on multicore processors. Its main features are (i) two kinds of set iterators for expressing worklist-based data parallelism, and (ii) a runtime system that performs optimistic parallelization of these iterators, detecting conflicts and rolling back computations as needed. Detection of conflicts and rolling back iterations requires information from class implementors. In this paper, we introduce mechanisms to improve the execution efficiency of Galois programs: data partitioning, data-centric work assignment, lock coarsening, and over-decomposition. These mechanisms can be used to exploit locality of reference, reduce mis-speculation, and lower synchronization overhead. We also argue that the design of the Galois system permits these mechanisms to be used with relatively little modification to the user code. Finally, we present experimental results that demonstrate the utility of these mechanisms.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346311},
 doi = {http://doi.acm.org/10.1145/1353534.1346311},
 acmid = {1346311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data partitioning, irregular programs, locality, lock coarsening, optimistic parallelism, over-decomposition},
} 

@inproceedings{Kulkarni:2008:OPB:1346281.1346311,
 author = {Kulkarni, Milind and Pingali, Keshav and Ramanarayanan, Ganesh and Walter, Bruce and Bala, Kavita and Chew, L. Paul},
 title = {Optimistic parallelism benefits from data partitioning},
 abstract = {Recent studies of irregular applications such as finite-element mesh generators and data-clustering codes have shown that these applications have a generalized data parallelism arising from the use of iterative algorithms that perform computations on elements of worklists. In some irregular applications, the computations on different elements are independent. In other applications, there may be complex patterns of dependences between these computations. The Galois system was designed to exploit this kind of irregular data parallelism on multicore processors. Its main features are (i) two kinds of set iterators for expressing worklist-based data parallelism, and (ii) a runtime system that performs optimistic parallelization of these iterators, detecting conflicts and rolling back computations as needed. Detection of conflicts and rolling back iterations requires information from class implementors. In this paper, we introduce mechanisms to improve the execution efficiency of Galois programs: data partitioning, data-centric work assignment, lock coarsening, and over-decomposition. These mechanisms can be used to exploit locality of reference, reduce mis-speculation, and lower synchronization overhead. We also argue that the design of the Galois system permits these mechanisms to be used with relatively little modification to the user code. Finally, we present experimental results that demonstrate the utility of these mechanisms.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346311},
 doi = {http://doi.acm.org/10.1145/1346281.1346311},
 acmid = {1346311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data partitioning, irregular programs, locality, lock coarsening, optimistic parallelism, over-decomposition},
} 

@article{Kulkarni:2008:OPB:1353536.1346311,
 author = {Kulkarni, Milind and Pingali, Keshav and Ramanarayanan, Ganesh and Walter, Bruce and Bala, Kavita and Chew, L. Paul},
 title = {Optimistic parallelism benefits from data partitioning},
 abstract = {Recent studies of irregular applications such as finite-element mesh generators and data-clustering codes have shown that these applications have a generalized data parallelism arising from the use of iterative algorithms that perform computations on elements of worklists. In some irregular applications, the computations on different elements are independent. In other applications, there may be complex patterns of dependences between these computations. The Galois system was designed to exploit this kind of irregular data parallelism on multicore processors. Its main features are (i) two kinds of set iterators for expressing worklist-based data parallelism, and (ii) a runtime system that performs optimistic parallelization of these iterators, detecting conflicts and rolling back computations as needed. Detection of conflicts and rolling back iterations requires information from class implementors. In this paper, we introduce mechanisms to improve the execution efficiency of Galois programs: data partitioning, data-centric work assignment, lock coarsening, and over-decomposition. These mechanisms can be used to exploit locality of reference, reduce mis-speculation, and lower synchronization overhead. We also argue that the design of the Galois system permits these mechanisms to be used with relatively little modification to the user code. Finally, we present experimental results that demonstrate the utility of these mechanisms.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346311},
 doi = {http://doi.acm.org/10.1145/1353536.1346311},
 acmid = {1346311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data partitioning, irregular programs, locality, lock coarsening, optimistic parallelism, over-decomposition},
} 

@article{Kulkarni:2008:OPB:1353535.1346311,
 author = {Kulkarni, Milind and Pingali, Keshav and Ramanarayanan, Ganesh and Walter, Bruce and Bala, Kavita and Chew, L. Paul},
 title = {Optimistic parallelism benefits from data partitioning},
 abstract = {Recent studies of irregular applications such as finite-element mesh generators and data-clustering codes have shown that these applications have a generalized data parallelism arising from the use of iterative algorithms that perform computations on elements of worklists. In some irregular applications, the computations on different elements are independent. In other applications, there may be complex patterns of dependences between these computations. The Galois system was designed to exploit this kind of irregular data parallelism on multicore processors. Its main features are (i) two kinds of set iterators for expressing worklist-based data parallelism, and (ii) a runtime system that performs optimistic parallelization of these iterators, detecting conflicts and rolling back computations as needed. Detection of conflicts and rolling back iterations requires information from class implementors. In this paper, we introduce mechanisms to improve the execution efficiency of Galois programs: data partitioning, data-centric work assignment, lock coarsening, and over-decomposition. These mechanisms can be used to exploit locality of reference, reduce mis-speculation, and lower synchronization overhead. We also argue that the design of the Galois system permits these mechanisms to be used with relatively little modification to the user code. Finally, we present experimental results that demonstrate the utility of these mechanisms.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346311},
 doi = {http://doi.acm.org/10.1145/1353535.1346311},
 acmid = {1346311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data partitioning, irregular programs, locality, lock coarsening, optimistic parallelism, over-decomposition},
} 

@article{Cox:2008:XEC:1353536.1346312,
 author = {Cox, Russ and Bergan, Tom and Clements, Austin T. and Kaashoek, Frans and Kohler, Eddie},
 title = {Xoc, an extension-oriented compiler for systems programming},
 abstract = {Today's system programmers go to great lengths to extend the languages in which they program. For instance, system-specific compilers find errors in Linux and other systems, and add support for specialized control flow to Qt and event-based programs. These compilers are difficult to build and cannot always understand each other's language changes. However, they can greatly improve code understandability and correctness, advantages that should be accessible to all programmers. We describe an extension-oriented compiler for C called xoc. An extension-oriented compiler, unlike a conventional extensible compiler, implements new features via many small extensions that are loaded together as needed. Xoc gives extension writers full control over program syntax and semantics while hiding many compiler internals. Xoc programmers concisely define powerful compiler extensions that, by construction, can be combined; even some parts of the base compiler, such as GNU C compatibility, are structured as extensions. Xoc is based on two key interfaces. Syntax patterns allow extension writers to manipulate language fragments using concrete syntax. Lazy computation of attributes allows extension writers to use the results of analyses by other extensions or the core without needing to worry about pass scheduling. Extensions built using xoc include xsparse, a 345-line extension that mimics Sparse, Linux's C front end, and xlambda, a 170-line extension that adds function expressions to C. An evaluation of xoc using these and 13 other extensions shows that xoc extensions are typically more concise than equivalent extensions written for conventional extensible compilers and that it is possible to compose extensions.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {244--254},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346312},
 doi = {http://doi.acm.org/10.1145/1353536.1346312},
 acmid = {1346312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extension-oriented compilers},
} 

@inproceedings{Cox:2008:XEC:1346281.1346312,
 author = {Cox, Russ and Bergan, Tom and Clements, Austin T. and Kaashoek, Frans and Kohler, Eddie},
 title = {Xoc, an extension-oriented compiler for systems programming},
 abstract = {Today's system programmers go to great lengths to extend the languages in which they program. For instance, system-specific compilers find errors in Linux and other systems, and add support for specialized control flow to Qt and event-based programs. These compilers are difficult to build and cannot always understand each other's language changes. However, they can greatly improve code understandability and correctness, advantages that should be accessible to all programmers. We describe an extension-oriented compiler for C called xoc. An extension-oriented compiler, unlike a conventional extensible compiler, implements new features via many small extensions that are loaded together as needed. Xoc gives extension writers full control over program syntax and semantics while hiding many compiler internals. Xoc programmers concisely define powerful compiler extensions that, by construction, can be combined; even some parts of the base compiler, such as GNU C compatibility, are structured as extensions. Xoc is based on two key interfaces. Syntax patterns allow extension writers to manipulate language fragments using concrete syntax. Lazy computation of attributes allows extension writers to use the results of analyses by other extensions or the core without needing to worry about pass scheduling. Extensions built using xoc include xsparse, a 345-line extension that mimics Sparse, Linux's C front end, and xlambda, a 170-line extension that adds function expressions to C. An evaluation of xoc using these and 13 other extensions shows that xoc extensions are typically more concise than equivalent extensions written for conventional extensible compilers and that it is possible to compose extensions.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {244--254},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346312},
 doi = {http://doi.acm.org/10.1145/1346281.1346312},
 acmid = {1346312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extension-oriented compilers},
} 

@article{Cox:2008:XEC:1353534.1346312,
 author = {Cox, Russ and Bergan, Tom and Clements, Austin T. and Kaashoek, Frans and Kohler, Eddie},
 title = {Xoc, an extension-oriented compiler for systems programming},
 abstract = {Today's system programmers go to great lengths to extend the languages in which they program. For instance, system-specific compilers find errors in Linux and other systems, and add support for specialized control flow to Qt and event-based programs. These compilers are difficult to build and cannot always understand each other's language changes. However, they can greatly improve code understandability and correctness, advantages that should be accessible to all programmers. We describe an extension-oriented compiler for C called xoc. An extension-oriented compiler, unlike a conventional extensible compiler, implements new features via many small extensions that are loaded together as needed. Xoc gives extension writers full control over program syntax and semantics while hiding many compiler internals. Xoc programmers concisely define powerful compiler extensions that, by construction, can be combined; even some parts of the base compiler, such as GNU C compatibility, are structured as extensions. Xoc is based on two key interfaces. Syntax patterns allow extension writers to manipulate language fragments using concrete syntax. Lazy computation of attributes allows extension writers to use the results of analyses by other extensions or the core without needing to worry about pass scheduling. Extensions built using xoc include xsparse, a 345-line extension that mimics Sparse, Linux's C front end, and xlambda, a 170-line extension that adds function expressions to C. An evaluation of xoc using these and 13 other extensions shows that xoc extensions are typically more concise than equivalent extensions written for conventional extensible compilers and that it is possible to compose extensions.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {244--254},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346312},
 doi = {http://doi.acm.org/10.1145/1353534.1346312},
 acmid = {1346312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extension-oriented compilers},
} 

@article{Cox:2008:XEC:1353535.1346312,
 author = {Cox, Russ and Bergan, Tom and Clements, Austin T. and Kaashoek, Frans and Kohler, Eddie},
 title = {Xoc, an extension-oriented compiler for systems programming},
 abstract = {Today's system programmers go to great lengths to extend the languages in which they program. For instance, system-specific compilers find errors in Linux and other systems, and add support for specialized control flow to Qt and event-based programs. These compilers are difficult to build and cannot always understand each other's language changes. However, they can greatly improve code understandability and correctness, advantages that should be accessible to all programmers. We describe an extension-oriented compiler for C called xoc. An extension-oriented compiler, unlike a conventional extensible compiler, implements new features via many small extensions that are loaded together as needed. Xoc gives extension writers full control over program syntax and semantics while hiding many compiler internals. Xoc programmers concisely define powerful compiler extensions that, by construction, can be combined; even some parts of the base compiler, such as GNU C compatibility, are structured as extensions. Xoc is based on two key interfaces. Syntax patterns allow extension writers to manipulate language fragments using concrete syntax. Lazy computation of attributes allows extension writers to use the results of analyses by other extensions or the core without needing to worry about pass scheduling. Extensions built using xoc include xsparse, a 345-line extension that mimics Sparse, Linux's C front end, and xlambda, a 170-line extension that adds function expressions to C. An evaluation of xoc using these and 13 other extensions shows that xoc extensions are typically more concise than equivalent extensions written for conventional extensible compilers and that it is possible to compose extensions.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {244--254},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346312},
 doi = {http://doi.acm.org/10.1145/1353535.1346312},
 acmid = {1346312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extension-oriented compilers},
} 

@article{Wells:2008:AIF:1353534.1346314,
 author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
 title = {Adapting to intermittent faults in multicore systems},
 abstract = {Future multicore processors will be more susceptible to a variety of hardware failures. In particular, intermittent faults</i>, caused in part by manufacturing, thermal, and voltage variations, can cause bursts of frequent faults that last from several cycles to several seconds or more. Due to practical limitations of circuit techniques, cost-effective reliability will likely require the ability to temporarily suspend execution on a core during periods of intermittent faults. We investigate three of the most obvious techniques for adapting to the dynamically changing resource availability caused by intermittent faults, and demonstrate their different system-level implications. We show that system software reconfiguration has very high overhead, that temporarily pausing execution on a faulty core can lead to cascading livelock, and that using spare cores has high fault-free cost. To remedy these and other drawbacks of the three baseline techniques, we propose using a thin hardware/firmware layer to manage an overcommitted system</i> -- one where the OS is configured to use more virtual processors than the number of currently available physical cores. We show that this proposed technique can gracefully degrade performance during intermittent faults of various duration with low overhead, without involving system software, and without requiring spare cores.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {255--264},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346314},
 doi = {http://doi.acm.org/10.1145/1353534.1346314},
 acmid = {1346314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {intermittent faults, overcommitted system},
} 

@article{Wells:2008:AIF:1353536.1346314,
 author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
 title = {Adapting to intermittent faults in multicore systems},
 abstract = {Future multicore processors will be more susceptible to a variety of hardware failures. In particular, intermittent faults</i>, caused in part by manufacturing, thermal, and voltage variations, can cause bursts of frequent faults that last from several cycles to several seconds or more. Due to practical limitations of circuit techniques, cost-effective reliability will likely require the ability to temporarily suspend execution on a core during periods of intermittent faults. We investigate three of the most obvious techniques for adapting to the dynamically changing resource availability caused by intermittent faults, and demonstrate their different system-level implications. We show that system software reconfiguration has very high overhead, that temporarily pausing execution on a faulty core can lead to cascading livelock, and that using spare cores has high fault-free cost. To remedy these and other drawbacks of the three baseline techniques, we propose using a thin hardware/firmware layer to manage an overcommitted system</i> -- one where the OS is configured to use more virtual processors than the number of currently available physical cores. We show that this proposed technique can gracefully degrade performance during intermittent faults of various duration with low overhead, without involving system software, and without requiring spare cores.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {255--264},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346314},
 doi = {http://doi.acm.org/10.1145/1353536.1346314},
 acmid = {1346314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {intermittent faults, overcommitted system},
} 

@article{Wells:2008:AIF:1353535.1346314,
 author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
 title = {Adapting to intermittent faults in multicore systems},
 abstract = {Future multicore processors will be more susceptible to a variety of hardware failures. In particular, intermittent faults</i>, caused in part by manufacturing, thermal, and voltage variations, can cause bursts of frequent faults that last from several cycles to several seconds or more. Due to practical limitations of circuit techniques, cost-effective reliability will likely require the ability to temporarily suspend execution on a core during periods of intermittent faults. We investigate three of the most obvious techniques for adapting to the dynamically changing resource availability caused by intermittent faults, and demonstrate their different system-level implications. We show that system software reconfiguration has very high overhead, that temporarily pausing execution on a faulty core can lead to cascading livelock, and that using spare cores has high fault-free cost. To remedy these and other drawbacks of the three baseline techniques, we propose using a thin hardware/firmware layer to manage an overcommitted system</i> -- one where the OS is configured to use more virtual processors than the number of currently available physical cores. We show that this proposed technique can gracefully degrade performance during intermittent faults of various duration with low overhead, without involving system software, and without requiring spare cores.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {255--264},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346314},
 doi = {http://doi.acm.org/10.1145/1353535.1346314},
 acmid = {1346314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {intermittent faults, overcommitted system},
} 

@inproceedings{Wells:2008:AIF:1346281.1346314,
 author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
 title = {Adapting to intermittent faults in multicore systems},
 abstract = {Future multicore processors will be more susceptible to a variety of hardware failures. In particular, intermittent faults</i>, caused in part by manufacturing, thermal, and voltage variations, can cause bursts of frequent faults that last from several cycles to several seconds or more. Due to practical limitations of circuit techniques, cost-effective reliability will likely require the ability to temporarily suspend execution on a core during periods of intermittent faults. We investigate three of the most obvious techniques for adapting to the dynamically changing resource availability caused by intermittent faults, and demonstrate their different system-level implications. We show that system software reconfiguration has very high overhead, that temporarily pausing execution on a faulty core can lead to cascading livelock, and that using spare cores has high fault-free cost. To remedy these and other drawbacks of the three baseline techniques, we propose using a thin hardware/firmware layer to manage an overcommitted system</i> -- one where the OS is configured to use more virtual processors than the number of currently available physical cores. We show that this proposed technique can gracefully degrade performance during intermittent faults of various duration with low overhead, without involving system software, and without requiring spare cores.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {255--264},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346314},
 doi = {http://doi.acm.org/10.1145/1346281.1346314},
 acmid = {1346314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {intermittent faults, overcommitted system},
} 

@article{Li:2008:UPH:1353536.1346315,
 author = {Li, Man-Lap and Ramachandran, Pradeep and Sahoo, Swarup Kumar and Adve, Sarita V. and Adve, Vikram S. and Zhou, Yuanyuan},
 title = {Understanding the propagation of hard errors to software and implications for resilient system design},
 abstract = {With continued CMOS scaling, future shipped hardware will be increasingly vulnerable to in-the-field faults. To be broadly deployable, the hardware reliability solution must incur low overheads, precluding use of expensive redundancy. We explore a cooperative hardware-software solution that watches for anomalous software behavior to indicate the presence of hardware faults. Fundamental to such a solution is a characterization of how hardware faults indifferent microarchitectural structures of a modern processor propagate through the application and OS. This paper aims to provide such a characterization, resulting in identifying low-cost detection methods and providing guidelines for implementation of the recovery and diagnosis components of such a reliability solution. We focus on hard faults because they are increasingly important and have different system implications than the much studied transients. We achieve our goals through fault injection experiments with a microarchitecture-level full system timing simulator. Our main results are: (1) we are able to detect 95\% of the unmasked faults in 7 out of 8 studied microarchitectural structures with simple detectors that incur zero to little hardware overhead; (2) over 86\% of these detections are within latencies that existing hardware checkpointing schemes can handle, while others require software checkpointing; and (3) a surprisingly large fraction of the detected faults corrupt OS state, but almost all of these are detected with latencies short enough to use hardware checkpointing, thereby enabling OS recovery in virtually all such cases.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353536.1346315},
 doi = {http://doi.acm.org/10.1145/1353536.1346315},
 acmid = {1346315},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, error detection, fault injection, permanent fault},
} 

@inproceedings{Li:2008:UPH:1346281.1346315,
 author = {Li, Man-Lap and Ramachandran, Pradeep and Sahoo, Swarup Kumar and Adve, Sarita V. and Adve, Vikram S. and Zhou, Yuanyuan},
 title = {Understanding the propagation of hard errors to software and implications for resilient system design},
 abstract = {With continued CMOS scaling, future shipped hardware will be increasingly vulnerable to in-the-field faults. To be broadly deployable, the hardware reliability solution must incur low overheads, precluding use of expensive redundancy. We explore a cooperative hardware-software solution that watches for anomalous software behavior to indicate the presence of hardware faults. Fundamental to such a solution is a characterization of how hardware faults indifferent microarchitectural structures of a modern processor propagate through the application and OS. This paper aims to provide such a characterization, resulting in identifying low-cost detection methods and providing guidelines for implementation of the recovery and diagnosis components of such a reliability solution. We focus on hard faults because they are increasingly important and have different system implications than the much studied transients. We achieve our goals through fault injection experiments with a microarchitecture-level full system timing simulator. Our main results are: (1) we are able to detect 95\% of the unmasked faults in 7 out of 8 studied microarchitectural structures with simple detectors that incur zero to little hardware overhead; (2) over 86\% of these detections are within latencies that existing hardware checkpointing schemes can handle, while others require software checkpointing; and (3) a surprisingly large fraction of the detected faults corrupt OS state, but almost all of these are detected with latencies short enough to use hardware checkpointing, thereby enabling OS recovery in virtually all such cases.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1346281.1346315},
 doi = {http://doi.acm.org/10.1145/1346281.1346315},
 acmid = {1346315},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, error detection, fault injection, permanent fault},
} 

@article{Li:2008:UPH:1353535.1346315,
 author = {Li, Man-Lap and Ramachandran, Pradeep and Sahoo, Swarup Kumar and Adve, Sarita V. and Adve, Vikram S. and Zhou, Yuanyuan},
 title = {Understanding the propagation of hard errors to software and implications for resilient system design},
 abstract = {With continued CMOS scaling, future shipped hardware will be increasingly vulnerable to in-the-field faults. To be broadly deployable, the hardware reliability solution must incur low overheads, precluding use of expensive redundancy. We explore a cooperative hardware-software solution that watches for anomalous software behavior to indicate the presence of hardware faults. Fundamental to such a solution is a characterization of how hardware faults indifferent microarchitectural structures of a modern processor propagate through the application and OS. This paper aims to provide such a characterization, resulting in identifying low-cost detection methods and providing guidelines for implementation of the recovery and diagnosis components of such a reliability solution. We focus on hard faults because they are increasingly important and have different system implications than the much studied transients. We achieve our goals through fault injection experiments with a microarchitecture-level full system timing simulator. Our main results are: (1) we are able to detect 95\% of the unmasked faults in 7 out of 8 studied microarchitectural structures with simple detectors that incur zero to little hardware overhead; (2) over 86\% of these detections are within latencies that existing hardware checkpointing schemes can handle, while others require software checkpointing; and (3) a surprisingly large fraction of the detected faults corrupt OS state, but almost all of these are detected with latencies short enough to use hardware checkpointing, thereby enabling OS recovery in virtually all such cases.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353535.1346315},
 doi = {http://doi.acm.org/10.1145/1353535.1346315},
 acmid = {1346315},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, error detection, fault injection, permanent fault},
} 

@article{Li:2008:UPH:1353534.1346315,
 author = {Li, Man-Lap and Ramachandran, Pradeep and Sahoo, Swarup Kumar and Adve, Sarita V. and Adve, Vikram S. and Zhou, Yuanyuan},
 title = {Understanding the propagation of hard errors to software and implications for resilient system design},
 abstract = {With continued CMOS scaling, future shipped hardware will be increasingly vulnerable to in-the-field faults. To be broadly deployable, the hardware reliability solution must incur low overheads, precluding use of expensive redundancy. We explore a cooperative hardware-software solution that watches for anomalous software behavior to indicate the presence of hardware faults. Fundamental to such a solution is a characterization of how hardware faults indifferent microarchitectural structures of a modern processor propagate through the application and OS. This paper aims to provide such a characterization, resulting in identifying low-cost detection methods and providing guidelines for implementation of the recovery and diagnosis components of such a reliability solution. We focus on hard faults because they are increasingly important and have different system implications than the much studied transients. We achieve our goals through fault injection experiments with a microarchitecture-level full system timing simulator. Our main results are: (1) we are able to detect 95\% of the unmasked faults in 7 out of 8 studied microarchitectural structures with simple detectors that incur zero to little hardware overhead; (2) over 86\% of these detections are within latencies that existing hardware checkpointing schemes can handle, while others require software checkpointing; and (3) a surprisingly large fraction of the detected faults corrupt OS state, but almost all of these are detected with latencies short enough to use hardware checkpointing, thereby enabling OS recovery in virtually all such cases.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1353534.1346315},
 doi = {http://doi.acm.org/10.1145/1353534.1346315},
 acmid = {1346315},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, error detection, fault injection, permanent fault},
} 

@article{Suleman:2008:FTP:1353534.1346317,
 author = {Suleman, M. Aater and Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Feedback-driven threading: power-efficient and high-performance execution of multi-threaded workloads on CMPs},
 abstract = {Extracting high-performance from the emerging Chip Multiprocessors (CMPs) requires that the application be divided into multiple threads. Each thread executes on a separate core thereby increasing concurrency and improving performance. As the number of cores on a CMP continues to increase, the performance of some multi-threaded applications will benefit from the increased number of threads, whereas, the performance of other multi-threaded applications will become limited by data-synchronization and off-chip bandwidth. For applications that get limited by data-synchronization, increasing the number of threads significantly degrades performance and increases on-chip power. Similarly, for applications that get limited by off-chip bandwidth, increasing the number of threads increases on-chip power without providing any performance improvement. Furthermore, whether an application gets limited by data-synchronization, or bandwidth, or neither depends not only on the application but also on the input set and the machine configuration. Therefore, controlling the number of threads based on the run-time behavior of the application can significantly improve performance and reduce power. This paper proposes Feedback-Driven Threading (FDT)</i>, a framework to dynamically control the number of threads using run-time information. FDT can be used to implement Synchronization-Aware Threading (SAT)</i>, which predicts the optimal number of threads depending on the amount of data-synchronization. Our evaluation shows that SAT can reduce both execution time and power by up to 66\% and 78\% respectively. Similarly, FDT can be used to implement Bandwidth-Aware Threading (BAT)</i>, which predicts the minimum number of threads required to saturate the off-chip bus. Our evaluation shows that BAT reduces on-chip power by up to 78\%. When SAT and BAT are combined, the average execution time reduces by 17\% and power reduces by 59\%. The proposed techniques leverage existing performance counters and require minimal support from the threading library.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {277--286},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346317},
 doi = {http://doi.acm.org/10.1145/1353534.1346317},
 acmid = {1346317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, bandwidth, multi-threaded, synchronization},
} 

@inproceedings{Suleman:2008:FTP:1346281.1346317,
 author = {Suleman, M. Aater and Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Feedback-driven threading: power-efficient and high-performance execution of multi-threaded workloads on CMPs},
 abstract = {Extracting high-performance from the emerging Chip Multiprocessors (CMPs) requires that the application be divided into multiple threads. Each thread executes on a separate core thereby increasing concurrency and improving performance. As the number of cores on a CMP continues to increase, the performance of some multi-threaded applications will benefit from the increased number of threads, whereas, the performance of other multi-threaded applications will become limited by data-synchronization and off-chip bandwidth. For applications that get limited by data-synchronization, increasing the number of threads significantly degrades performance and increases on-chip power. Similarly, for applications that get limited by off-chip bandwidth, increasing the number of threads increases on-chip power without providing any performance improvement. Furthermore, whether an application gets limited by data-synchronization, or bandwidth, or neither depends not only on the application but also on the input set and the machine configuration. Therefore, controlling the number of threads based on the run-time behavior of the application can significantly improve performance and reduce power. This paper proposes Feedback-Driven Threading (FDT)</i>, a framework to dynamically control the number of threads using run-time information. FDT can be used to implement Synchronization-Aware Threading (SAT)</i>, which predicts the optimal number of threads depending on the amount of data-synchronization. Our evaluation shows that SAT can reduce both execution time and power by up to 66\% and 78\% respectively. Similarly, FDT can be used to implement Bandwidth-Aware Threading (BAT)</i>, which predicts the minimum number of threads required to saturate the off-chip bus. Our evaluation shows that BAT reduces on-chip power by up to 78\%. When SAT and BAT are combined, the average execution time reduces by 17\% and power reduces by 59\%. The proposed techniques leverage existing performance counters and require minimal support from the threading library.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {277--286},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346317},
 doi = {http://doi.acm.org/10.1145/1346281.1346317},
 acmid = {1346317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, bandwidth, multi-threaded, synchronization},
} 

@article{Suleman:2008:FTP:1353535.1346317,
 author = {Suleman, M. Aater and Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Feedback-driven threading: power-efficient and high-performance execution of multi-threaded workloads on CMPs},
 abstract = {Extracting high-performance from the emerging Chip Multiprocessors (CMPs) requires that the application be divided into multiple threads. Each thread executes on a separate core thereby increasing concurrency and improving performance. As the number of cores on a CMP continues to increase, the performance of some multi-threaded applications will benefit from the increased number of threads, whereas, the performance of other multi-threaded applications will become limited by data-synchronization and off-chip bandwidth. For applications that get limited by data-synchronization, increasing the number of threads significantly degrades performance and increases on-chip power. Similarly, for applications that get limited by off-chip bandwidth, increasing the number of threads increases on-chip power without providing any performance improvement. Furthermore, whether an application gets limited by data-synchronization, or bandwidth, or neither depends not only on the application but also on the input set and the machine configuration. Therefore, controlling the number of threads based on the run-time behavior of the application can significantly improve performance and reduce power. This paper proposes Feedback-Driven Threading (FDT)</i>, a framework to dynamically control the number of threads using run-time information. FDT can be used to implement Synchronization-Aware Threading (SAT)</i>, which predicts the optimal number of threads depending on the amount of data-synchronization. Our evaluation shows that SAT can reduce both execution time and power by up to 66\% and 78\% respectively. Similarly, FDT can be used to implement Bandwidth-Aware Threading (BAT)</i>, which predicts the minimum number of threads required to saturate the off-chip bus. Our evaluation shows that BAT reduces on-chip power by up to 78\%. When SAT and BAT are combined, the average execution time reduces by 17\% and power reduces by 59\%. The proposed techniques leverage existing performance counters and require minimal support from the threading library.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {277--286},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346317},
 doi = {http://doi.acm.org/10.1145/1353535.1346317},
 acmid = {1346317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, bandwidth, multi-threaded, synchronization},
} 

@article{Suleman:2008:FTP:1353536.1346317,
 author = {Suleman, M. Aater and Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Feedback-driven threading: power-efficient and high-performance execution of multi-threaded workloads on CMPs},
 abstract = {Extracting high-performance from the emerging Chip Multiprocessors (CMPs) requires that the application be divided into multiple threads. Each thread executes on a separate core thereby increasing concurrency and improving performance. As the number of cores on a CMP continues to increase, the performance of some multi-threaded applications will benefit from the increased number of threads, whereas, the performance of other multi-threaded applications will become limited by data-synchronization and off-chip bandwidth. For applications that get limited by data-synchronization, increasing the number of threads significantly degrades performance and increases on-chip power. Similarly, for applications that get limited by off-chip bandwidth, increasing the number of threads increases on-chip power without providing any performance improvement. Furthermore, whether an application gets limited by data-synchronization, or bandwidth, or neither depends not only on the application but also on the input set and the machine configuration. Therefore, controlling the number of threads based on the run-time behavior of the application can significantly improve performance and reduce power. This paper proposes Feedback-Driven Threading (FDT)</i>, a framework to dynamically control the number of threads using run-time information. FDT can be used to implement Synchronization-Aware Threading (SAT)</i>, which predicts the optimal number of threads depending on the amount of data-synchronization. Our evaluation shows that SAT can reduce both execution time and power by up to 66\% and 78\% respectively. Similarly, FDT can be used to implement Bandwidth-Aware Threading (BAT)</i>, which predicts the minimum number of threads required to saturate the off-chip bus. Our evaluation shows that BAT reduces on-chip power by up to 78\%. When SAT and BAT are combined, the average execution time reduces by 17\% and power reduces by 59\%. The proposed techniques leverage existing performance counters and require minimal support from the threading library.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {277--286},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346317},
 doi = {http://doi.acm.org/10.1145/1353536.1346317},
 acmid = {1346317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, bandwidth, multi-threaded, synchronization},
} 

@article{Linderman:2008:MPM:1353536.1346318,
 author = {Linderman, Michael D. and Collins, Jamison D. and Wang, Hong and Meng, Teresa H.},
 title = {Merge: a programming model for heterogeneous multi-core systems},
 abstract = {In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the X3000 and 5.2x -- 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {287--296},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346318},
 doi = {http://doi.acm.org/10.1145/1353536.1346318},
 acmid = {1346318},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, heterogeneous multi-core, predicate dispatch},
} 

@article{Linderman:2008:MPM:1353535.1346318,
 author = {Linderman, Michael D. and Collins, Jamison D. and Wang, Hong and Meng, Teresa H.},
 title = {Merge: a programming model for heterogeneous multi-core systems},
 abstract = {In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the X3000 and 5.2x -- 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {287--296},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346318},
 doi = {http://doi.acm.org/10.1145/1353535.1346318},
 acmid = {1346318},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, heterogeneous multi-core, predicate dispatch},
} 

@article{Linderman:2008:MPM:1353534.1346318,
 author = {Linderman, Michael D. and Collins, Jamison D. and Wang, Hong and Meng, Teresa H.},
 title = {Merge: a programming model for heterogeneous multi-core systems},
 abstract = {In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the X3000 and 5.2x -- 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {287--296},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346318},
 doi = {http://doi.acm.org/10.1145/1353534.1346318},
 acmid = {1346318},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, heterogeneous multi-core, predicate dispatch},
} 

@inproceedings{Linderman:2008:MPM:1346281.1346318,
 author = {Linderman, Michael D. and Collins, Jamison D. and Wang, Hong and Meng, Teresa H.},
 title = {Merge: a programming model for heterogeneous multi-core systems},
 abstract = {In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the X3000 and 5.2x -- 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {287--296},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346318},
 doi = {http://doi.acm.org/10.1145/1346281.1346318},
 acmid = {1346318},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, heterogeneous multi-core, predicate dispatch},
} 

@article{Gummaraju:2008:SPG:1353535.1346319,
 author = {Gummaraju, Jayanth and Coburn, Joel and Turner, Yoshio and Rosenblum, Mendel},
 title = {Streamware: programming general-purpose multicore processors using streams},
 abstract = {Recently, the number of cores on general-purpose processors has been increasing rapidly. Using conventional programming models, it is challenging to effectively exploit these cores for maximal performance. An interesting alternative candidate for programming multiple cores is the stream programming model, which provides a framework for writing programs in a sequential-style while greatly simplifying the task of automatic parallelization. It has been shown that not only traditional media/image applications but also more general-purpose data-intensive applications can be expressed in the stream programming style. In this paper, we investigate the potential to use the stream programming model to efficiently utilize commodity multicore general-purpose processors (e.g., Intel/AMD). Although several stream languages and stream compilers have recently been developed, they typically target special-purpose stream processors. In contrast, we propose a flexible software system, Streamware, which automatically maps stream programs onto a wide variety of general-purpose multicore processor configurations. We leverage existing compilation framework for stream processors and design a runtime environment which takes as input the output of these stream compilers in the form of machine-independent stream virtual machine code. The runtime environment assigns work to processor cores considering processor/cache configurations and adapts to workload variations. We evaluate this approach for a few general-purpose scientific applications on real hardware and a cycle-level simulator set-up to showcase scaling and contention issues. The results show that the stream programming model is a good choice for efficiently exploiting modern and future multicore CPUs for an important class of applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {297--307},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346319},
 doi = {http://doi.acm.org/10.1145/1353535.1346319},
 acmid = {1346319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {general-purpose multicore processors, programming, runtime system, streams},
} 

@inproceedings{Gummaraju:2008:SPG:1346281.1346319,
 author = {Gummaraju, Jayanth and Coburn, Joel and Turner, Yoshio and Rosenblum, Mendel},
 title = {Streamware: programming general-purpose multicore processors using streams},
 abstract = {Recently, the number of cores on general-purpose processors has been increasing rapidly. Using conventional programming models, it is challenging to effectively exploit these cores for maximal performance. An interesting alternative candidate for programming multiple cores is the stream programming model, which provides a framework for writing programs in a sequential-style while greatly simplifying the task of automatic parallelization. It has been shown that not only traditional media/image applications but also more general-purpose data-intensive applications can be expressed in the stream programming style. In this paper, we investigate the potential to use the stream programming model to efficiently utilize commodity multicore general-purpose processors (e.g., Intel/AMD). Although several stream languages and stream compilers have recently been developed, they typically target special-purpose stream processors. In contrast, we propose a flexible software system, Streamware, which automatically maps stream programs onto a wide variety of general-purpose multicore processor configurations. We leverage existing compilation framework for stream processors and design a runtime environment which takes as input the output of these stream compilers in the form of machine-independent stream virtual machine code. The runtime environment assigns work to processor cores considering processor/cache configurations and adapts to workload variations. We evaluate this approach for a few general-purpose scientific applications on real hardware and a cycle-level simulator set-up to showcase scaling and contention issues. The results show that the stream programming model is a good choice for efficiently exploiting modern and future multicore CPUs for an important class of applications.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {297--307},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346319},
 doi = {http://doi.acm.org/10.1145/1346281.1346319},
 acmid = {1346319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {general-purpose multicore processors, programming, runtime system, streams},
} 

@article{Gummaraju:2008:SPG:1353536.1346319,
 author = {Gummaraju, Jayanth and Coburn, Joel and Turner, Yoshio and Rosenblum, Mendel},
 title = {Streamware: programming general-purpose multicore processors using streams},
 abstract = {Recently, the number of cores on general-purpose processors has been increasing rapidly. Using conventional programming models, it is challenging to effectively exploit these cores for maximal performance. An interesting alternative candidate for programming multiple cores is the stream programming model, which provides a framework for writing programs in a sequential-style while greatly simplifying the task of automatic parallelization. It has been shown that not only traditional media/image applications but also more general-purpose data-intensive applications can be expressed in the stream programming style. In this paper, we investigate the potential to use the stream programming model to efficiently utilize commodity multicore general-purpose processors (e.g., Intel/AMD). Although several stream languages and stream compilers have recently been developed, they typically target special-purpose stream processors. In contrast, we propose a flexible software system, Streamware, which automatically maps stream programs onto a wide variety of general-purpose multicore processor configurations. We leverage existing compilation framework for stream processors and design a runtime environment which takes as input the output of these stream compilers in the form of machine-independent stream virtual machine code. The runtime environment assigns work to processor cores considering processor/cache configurations and adapts to workload variations. We evaluate this approach for a few general-purpose scientific applications on real hardware and a cycle-level simulator set-up to showcase scaling and contention issues. The results show that the stream programming model is a good choice for efficiently exploiting modern and future multicore CPUs for an important class of applications.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {297--307},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346319},
 doi = {http://doi.acm.org/10.1145/1353536.1346319},
 acmid = {1346319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {general-purpose multicore processors, programming, runtime system, streams},
} 

@article{Gummaraju:2008:SPG:1353534.1346319,
 author = {Gummaraju, Jayanth and Coburn, Joel and Turner, Yoshio and Rosenblum, Mendel},
 title = {Streamware: programming general-purpose multicore processors using streams},
 abstract = {Recently, the number of cores on general-purpose processors has been increasing rapidly. Using conventional programming models, it is challenging to effectively exploit these cores for maximal performance. An interesting alternative candidate for programming multiple cores is the stream programming model, which provides a framework for writing programs in a sequential-style while greatly simplifying the task of automatic parallelization. It has been shown that not only traditional media/image applications but also more general-purpose data-intensive applications can be expressed in the stream programming style. In this paper, we investigate the potential to use the stream programming model to efficiently utilize commodity multicore general-purpose processors (e.g., Intel/AMD). Although several stream languages and stream compilers have recently been developed, they typically target special-purpose stream processors. In contrast, we propose a flexible software system, Streamware, which automatically maps stream programs onto a wide variety of general-purpose multicore processor configurations. We leverage existing compilation framework for stream processors and design a runtime environment which takes as input the output of these stream compilers in the form of machine-independent stream virtual machine code. The runtime environment assigns work to processor cores considering processor/cache configurations and adapts to workload variations. We evaluate this approach for a few general-purpose scientific applications on real hardware and a cycle-level simulator set-up to showcase scaling and contention issues. The results show that the stream programming model is a good choice for efficiently exploiting modern and future multicore CPUs for an important class of applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {297--307},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346319},
 doi = {http://doi.acm.org/10.1145/1353534.1346319},
 acmid = {1346319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {general-purpose multicore processors, programming, runtime system, streams},
} 

@article{Nightingale:2008:PSC:1353535.1346321,
 author = {Nightingale, Edmund B. and Peek, Daniel and Chen, Peter M. and Flinn, Jason},
 title = {Parallelizing security checks on commodity hardware},
 abstract = {Speck (Speculative Parallel Check) is a system thataccelerates powerful security checks on commodity hardware by executing them in parallel on multiple cores. Speck provides an infrastructure that allows sequential</i> invocations of a particular security check to run in parallel without sacrificing the safety of the system. Speck creates parallelism in two ways. First, Speck decouples a security check from an application by continuing the application, using speculative execution, while the security check executes in parallel on another core. Second, Speck creates parallelism between sequential invocations of a security check by running later checks in parallel with earlier ones. Speck provides a process-level replay system to deterministically and efficiently synchronize state between a security check and the original process.We use Speck to parallelize three security checks: sensitive data analysis, on-access virus scanning, and taint propagation. Running on a 4-core and an 8-core computer, Speck improves performance 4x and 7.5x for the sensitive data analysis check, 3.3x and 2.8x for theon-access virus scanning check, and 1.6x and 2x for the taint propagation check.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346321},
 doi = {http://doi.acm.org/10.1145/1353535.1346321},
 acmid = {1346321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating systems, parallel, performance, security, speculative execution},
} 

@inproceedings{Nightingale:2008:PSC:1346281.1346321,
 author = {Nightingale, Edmund B. and Peek, Daniel and Chen, Peter M. and Flinn, Jason},
 title = {Parallelizing security checks on commodity hardware},
 abstract = {Speck (Speculative Parallel Check) is a system thataccelerates powerful security checks on commodity hardware by executing them in parallel on multiple cores. Speck provides an infrastructure that allows sequential</i> invocations of a particular security check to run in parallel without sacrificing the safety of the system. Speck creates parallelism in two ways. First, Speck decouples a security check from an application by continuing the application, using speculative execution, while the security check executes in parallel on another core. Second, Speck creates parallelism between sequential invocations of a security check by running later checks in parallel with earlier ones. Speck provides a process-level replay system to deterministically and efficiently synchronize state between a security check and the original process.We use Speck to parallelize three security checks: sensitive data analysis, on-access virus scanning, and taint propagation. Running on a 4-core and an 8-core computer, Speck improves performance 4x and 7.5x for the sensitive data analysis check, 3.3x and 2.8x for theon-access virus scanning check, and 1.6x and 2x for the taint propagation check.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346321},
 doi = {http://doi.acm.org/10.1145/1346281.1346321},
 acmid = {1346321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating systems, parallel, performance, security, speculative execution},
} 

@article{Nightingale:2008:PSC:1353534.1346321,
 author = {Nightingale, Edmund B. and Peek, Daniel and Chen, Peter M. and Flinn, Jason},
 title = {Parallelizing security checks on commodity hardware},
 abstract = {Speck (Speculative Parallel Check) is a system thataccelerates powerful security checks on commodity hardware by executing them in parallel on multiple cores. Speck provides an infrastructure that allows sequential</i> invocations of a particular security check to run in parallel without sacrificing the safety of the system. Speck creates parallelism in two ways. First, Speck decouples a security check from an application by continuing the application, using speculative execution, while the security check executes in parallel on another core. Second, Speck creates parallelism between sequential invocations of a security check by running later checks in parallel with earlier ones. Speck provides a process-level replay system to deterministically and efficiently synchronize state between a security check and the original process.We use Speck to parallelize three security checks: sensitive data analysis, on-access virus scanning, and taint propagation. Running on a 4-core and an 8-core computer, Speck improves performance 4x and 7.5x for the sensitive data analysis check, 3.3x and 2.8x for theon-access virus scanning check, and 1.6x and 2x for the taint propagation check.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346321},
 doi = {http://doi.acm.org/10.1145/1353534.1346321},
 acmid = {1346321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating systems, parallel, performance, security, speculative execution},
} 

@article{Nightingale:2008:PSC:1353536.1346321,
 author = {Nightingale, Edmund B. and Peek, Daniel and Chen, Peter M. and Flinn, Jason},
 title = {Parallelizing security checks on commodity hardware},
 abstract = {Speck (Speculative Parallel Check) is a system thataccelerates powerful security checks on commodity hardware by executing them in parallel on multiple cores. Speck provides an infrastructure that allows sequential</i> invocations of a particular security check to run in parallel without sacrificing the safety of the system. Speck creates parallelism in two ways. First, Speck decouples a security check from an application by continuing the application, using speculative execution, while the security check executes in parallel on another core. Second, Speck creates parallelism between sequential invocations of a security check by running later checks in parallel with earlier ones. Speck provides a process-level replay system to deterministically and efficiently synchronize state between a security check and the original process.We use Speck to parallelize three security checks: sensitive data analysis, on-access virus scanning, and taint propagation. Running on a 4-core and an 8-core computer, Speck improves performance 4x and 7.5x for the sensitive data analysis check, 3.3x and 2.8x for theon-access virus scanning check, and 1.6x and 2x for the taint propagation check.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346321},
 doi = {http://doi.acm.org/10.1145/1353536.1346321},
 acmid = {1346321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating systems, parallel, performance, security, speculative execution},
} 

@article{Castro:2008:BBR:1353534.1346322,
 author = {Castro, Miguel and Costa, Manuel and Martin, Jean-Philippe},
 title = {Better bug reporting with better privacy},
 abstract = {Software vendors collect bug reports from customers to improve the quality of their software. These reports should include the inputs that make the software fail, to enable vendors to reproduce the bug. However, vendors rarely include these inputs in reports because they may contain private user data. We describe a solution to this problem that provides software vendors with new input values that satisfy the conditions required to make the software follow the same execution path until it fails, but are otherwise unrelated with the original inputs. These new inputs allow vendors to reproduce the bug while revealing less private information than existing approaches. Additionally, we provide a mechanism to measure the amount of information revealed in an error report. This mechanism allows users to perform informed decisions on whether or not to submit reports. We implemented a prototype of our solution and evaluated it with real errors in real programs. The results show that we can produce error reports that allow software vendors to reproduce bugs while revealing almost no private information.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {319--328},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353534.1346322},
 doi = {http://doi.acm.org/10.1145/1353534.1346322},
 acmid = {1346322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug reports, constraint solving, privacy, symbolic execution},
} 

@article{Castro:2008:BBR:1353535.1346322,
 author = {Castro, Miguel and Costa, Manuel and Martin, Jean-Philippe},
 title = {Better bug reporting with better privacy},
 abstract = {Software vendors collect bug reports from customers to improve the quality of their software. These reports should include the inputs that make the software fail, to enable vendors to reproduce the bug. However, vendors rarely include these inputs in reports because they may contain private user data. We describe a solution to this problem that provides software vendors with new input values that satisfy the conditions required to make the software follow the same execution path until it fails, but are otherwise unrelated with the original inputs. These new inputs allow vendors to reproduce the bug while revealing less private information than existing approaches. Additionally, we provide a mechanism to measure the amount of information revealed in an error report. This mechanism allows users to perform informed decisions on whether or not to submit reports. We implemented a prototype of our solution and evaluated it with real errors in real programs. The results show that we can produce error reports that allow software vendors to reproduce bugs while revealing almost no private information.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {319--328},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353535.1346322},
 doi = {http://doi.acm.org/10.1145/1353535.1346322},
 acmid = {1346322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug reports, constraint solving, privacy, symbolic execution},
} 

@article{Castro:2008:BBR:1353536.1346322,
 author = {Castro, Miguel and Costa, Manuel and Martin, Jean-Philippe},
 title = {Better bug reporting with better privacy},
 abstract = {Software vendors collect bug reports from customers to improve the quality of their software. These reports should include the inputs that make the software fail, to enable vendors to reproduce the bug. However, vendors rarely include these inputs in reports because they may contain private user data. We describe a solution to this problem that provides software vendors with new input values that satisfy the conditions required to make the software follow the same execution path until it fails, but are otherwise unrelated with the original inputs. These new inputs allow vendors to reproduce the bug while revealing less private information than existing approaches. Additionally, we provide a mechanism to measure the amount of information revealed in an error report. This mechanism allows users to perform informed decisions on whether or not to submit reports. We implemented a prototype of our solution and evaluated it with real errors in real programs. The results show that we can produce error reports that allow software vendors to reproduce bugs while revealing almost no private information.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {319--328},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1353536.1346322},
 doi = {http://doi.acm.org/10.1145/1353536.1346322},
 acmid = {1346322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug reports, constraint solving, privacy, symbolic execution},
} 

@inproceedings{Castro:2008:BBR:1346281.1346322,
 author = {Castro, Miguel and Costa, Manuel and Martin, Jean-Philippe},
 title = {Better bug reporting with better privacy},
 abstract = {Software vendors collect bug reports from customers to improve the quality of their software. These reports should include the inputs that make the software fail, to enable vendors to reproduce the bug. However, vendors rarely include these inputs in reports because they may contain private user data. We describe a solution to this problem that provides software vendors with new input values that satisfy the conditions required to make the software follow the same execution path until it fails, but are otherwise unrelated with the original inputs. These new inputs allow vendors to reproduce the bug while revealing less private information than existing approaches. Additionally, we provide a mechanism to measure the amount of information revealed in an error report. This mechanism allows users to perform informed decisions on whether or not to submit reports. We implemented a prototype of our solution and evaluated it with real errors in real programs. The results show that we can produce error reports that allow software vendors to reproduce bugs while revealing almost no private information.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {319--328},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346322},
 doi = {http://doi.acm.org/10.1145/1346281.1346322},
 acmid = {1346322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug reports, constraint solving, privacy, symbolic execution},
} 

@article{Lu:2008:LMC:1353535.1346323,
 author = {Lu, Shan and Park, Soyeon and Seo, Eunsoo and Zhou, Yuanyuan},
 title = {Learning from mistakes: a comprehensive study on real world concurrency bug characteristics},
 abstract = {The reality of multi-core hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent program testing, concurrent programming model design, etc. Designing effective techniques in all these directions will significantly benefit from a deep understanding of real world concurrency bug characteristics. This paper provides the first (to the best of our knowledge) comprehensive real world concurrency bug characteristic study. Specifically, we have carefully examined concurrency bug patterns, manifestation, and fix strategies of 105 randomly selected real world concurrency bugs from 4 representative server and client open-source applications (MySQL, Apache, Mozilla and OpenOffice). Our study reveals several interesting findings and provides useful guidance for concurrency bug detection, testing, and concurrent programming language design. Some of our findings are as follows: (1) Around one third of the examined non-deadlock concurrency bugs are caused by violation to programmers' order intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34\% of the examined non-deadlock concurrency bugs involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92\% of the examined concurrency bugs canbe reliably triggered by enforcing certain orders among no more than 4 memory accesses. This indicates that testing concurrent programs can target at exploring possible orders among every small groups of memory accesses, instead of among all memory accesses; (4) About 73\% of the examinednon-deadlock concurrency bugs were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {42},
 issue = {2},
 month = {March},
 year = {2008},
 issn = {0163-5980},
 pages = {329--339},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353535.1346323},
 doi = {http://doi.acm.org/10.1145/1353535.1346323},
 acmid = {1346323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug characteristics, concurrency bug, concurrent program},
} 

@inproceedings{Lu:2008:LMC:1346281.1346323,
 author = {Lu, Shan and Park, Soyeon and Seo, Eunsoo and Zhou, Yuanyuan},
 title = {Learning from mistakes: a comprehensive study on real world concurrency bug characteristics},
 abstract = {The reality of multi-core hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent program testing, concurrent programming model design, etc. Designing effective techniques in all these directions will significantly benefit from a deep understanding of real world concurrency bug characteristics. This paper provides the first (to the best of our knowledge) comprehensive real world concurrency bug characteristic study. Specifically, we have carefully examined concurrency bug patterns, manifestation, and fix strategies of 105 randomly selected real world concurrency bugs from 4 representative server and client open-source applications (MySQL, Apache, Mozilla and OpenOffice). Our study reveals several interesting findings and provides useful guidance for concurrency bug detection, testing, and concurrent programming language design. Some of our findings are as follows: (1) Around one third of the examined non-deadlock concurrency bugs are caused by violation to programmers' order intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34\% of the examined non-deadlock concurrency bugs involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92\% of the examined concurrency bugs canbe reliably triggered by enforcing certain orders among no more than 4 memory accesses. This indicates that testing concurrent programs can target at exploring possible orders among every small groups of memory accesses, instead of among all memory accesses; (4) About 73\% of the examinednon-deadlock concurrency bugs were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.},
 booktitle = {Proceedings of the 13th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {329--339},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346323},
 doi = {http://doi.acm.org/10.1145/1346281.1346323},
 acmid = {1346323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug characteristics, concurrency bug, concurrent program},
} 

@article{Lu:2008:LMC:1353536.1346323,
 author = {Lu, Shan and Park, Soyeon and Seo, Eunsoo and Zhou, Yuanyuan},
 title = {Learning from mistakes: a comprehensive study on real world concurrency bug characteristics},
 abstract = {The reality of multi-core hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent program testing, concurrent programming model design, etc. Designing effective techniques in all these directions will significantly benefit from a deep understanding of real world concurrency bug characteristics. This paper provides the first (to the best of our knowledge) comprehensive real world concurrency bug characteristic study. Specifically, we have carefully examined concurrency bug patterns, manifestation, and fix strategies of 105 randomly selected real world concurrency bugs from 4 representative server and client open-source applications (MySQL, Apache, Mozilla and OpenOffice). Our study reveals several interesting findings and provides useful guidance for concurrency bug detection, testing, and concurrent programming language design. Some of our findings are as follows: (1) Around one third of the examined non-deadlock concurrency bugs are caused by violation to programmers' order intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34\% of the examined non-deadlock concurrency bugs involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92\% of the examined concurrency bugs canbe reliably triggered by enforcing certain orders among no more than 4 memory accesses. This indicates that testing concurrent programs can target at exploring possible orders among every small groups of memory accesses, instead of among all memory accesses; (4) About 73\% of the examinednon-deadlock concurrency bugs were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.},
 journal = {SIGPLAN Not.},
 volume = {43},
 issue = {3},
 month = {March},
 year = {2008},
 issn = {0362-1340},
 pages = {329--339},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353536.1346323},
 doi = {http://doi.acm.org/10.1145/1353536.1346323},
 acmid = {1346323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug characteristics, concurrency bug, concurrent program},
} 

@article{Lu:2008:LMC:1353534.1346323,
 author = {Lu, Shan and Park, Soyeon and Seo, Eunsoo and Zhou, Yuanyuan},
 title = {Learning from mistakes: a comprehensive study on real world concurrency bug characteristics},
 abstract = {The reality of multi-core hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent program testing, concurrent programming model design, etc. Designing effective techniques in all these directions will significantly benefit from a deep understanding of real world concurrency bug characteristics. This paper provides the first (to the best of our knowledge) comprehensive real world concurrency bug characteristic study. Specifically, we have carefully examined concurrency bug patterns, manifestation, and fix strategies of 105 randomly selected real world concurrency bugs from 4 representative server and client open-source applications (MySQL, Apache, Mozilla and OpenOffice). Our study reveals several interesting findings and provides useful guidance for concurrency bug detection, testing, and concurrent programming language design. Some of our findings are as follows: (1) Around one third of the examined non-deadlock concurrency bugs are caused by violation to programmers' order intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34\% of the examined non-deadlock concurrency bugs involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92\% of the examined concurrency bugs canbe reliably triggered by enforcing certain orders among no more than 4 memory accesses. This indicates that testing concurrent programs can target at exploring possible orders among every small groups of memory accesses, instead of among all memory accesses; (4) About 73\% of the examinednon-deadlock concurrency bugs were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {36},
 issue = {1},
 month = {March},
 year = {2008},
 issn = {0163-5964},
 pages = {329--339},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1353534.1346323},
 doi = {http://doi.acm.org/10.1145/1353534.1346323},
 acmid = {1346323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bug characteristics, concurrency bug, concurrent program},
} 

@article{Rosenblum:2006:IVC:1168918.1168858,
 author = {Rosenblum, Mendel},
 title = {Impact of virtualization on computer architecture and operating systems},
 abstract = {Abstract This talk describes how virtualization is changing the way computing is done in the industry today and how it is causing users to rethink how they view hardware, operating systems, and application programs. The talk will describe this new view on computing and the benefits driving users to adopt it. The changing roles for hardware and operating systems will be discussed along with what changes will be needed to efficiently and simply support this new computing model. I will conclude with a discussion of areas where industry could use input from the ASPLOS research community.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1168918.1168858},
 doi = {http://doi.acm.org/10.1145/1168918.1168858},
 acmid = {1168858},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rosenblum:2006:IVC:1168919.1168858,
 author = {Rosenblum, Mendel},
 title = {Impact of virtualization on computer architecture and operating systems},
 abstract = {Abstract This talk describes how virtualization is changing the way computing is done in the industry today and how it is causing users to rethink how they view hardware, operating systems, and application programs. The talk will describe this new view on computing and the benefits driving users to adopt it. The changing roles for hardware and operating systems will be discussed along with what changes will be needed to efficiently and simply support this new computing model. I will conclude with a discussion of areas where industry could use input from the ASPLOS research community.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1168919.1168858},
 doi = {http://doi.acm.org/10.1145/1168919.1168858},
 acmid = {1168858},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rosenblum:2006:IVC:1168857.1168858,
 author = {Rosenblum, Mendel},
 title = {Impact of virtualization on computer architecture and operating systems},
 abstract = {Abstract This talk describes how virtualization is changing the way computing is done in the industry today and how it is causing users to rethink how they view hardware, operating systems, and application programs. The talk will describe this new view on computing and the benefits driving users to adopt it. The changing roles for hardware and operating systems will be discussed along with what changes will be needed to efficiently and simply support this new computing model. I will conclude with a discussion of areas where industry could use input from the ASPLOS research community.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1168857.1168858},
 doi = {http://doi.acm.org/10.1145/1168857.1168858},
 acmid = {1168858},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rosenblum:2006:IVC:1168917.1168858,
 author = {Rosenblum, Mendel},
 title = {Impact of virtualization on computer architecture and operating systems},
 abstract = {Abstract This talk describes how virtualization is changing the way computing is done in the industry today and how it is causing users to rethink how they view hardware, operating systems, and application programs. The talk will describe this new view on computing and the benefits driving users to adopt it. The changing roles for hardware and operating systems will be discussed along with what changes will be needed to efficiently and simply support this new computing model. I will conclude with a discussion of areas where industry could use input from the ASPLOS research community.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1168917.1168858},
 doi = {http://doi.acm.org/10.1145/1168917.1168858},
 acmid = {1168858},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adams:2006:CSH:1168857.1168860,
 author = {Adams, Keith and Agesen, Ole},
 title = {A comparison of software and hardware techniques for x86 virtualization},
 abstract = {Until recently, the x86 architecture has not permitted classical trap-and-emulate virtualization. Virtual Machine Monitors for x86, such as VMware \&#174; Workstation and Virtual PC, have instead used binary translation of the guest kernel code. However, both Intel and AMD have now introduced architectural extensions to support classical virtualization.We compare an existing software VMM with a new VMM designed for the emerging hardware support. Surprisingly, the hardware VMM often suffers lower performance than the pure software VMM. To determine why, we study architecture-level events such as page table updates, context switches and I/O, and find their costs vastly different among native, software VMM and hardware VMM execution.We find that the hardware support fails to provide an unambiguous performance advantage for two primary reasons: first, it offers no support for MMU virtualization; second, it fails to co-exist with existing software techniques for MMU virtualization. We look ahead to emerging techniques for addressing this MMU virtualization problem in the context of hardware-assisted virtualization.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168860},
 doi = {http://doi.acm.org/10.1145/1168857.1168860},
 acmid = {1168860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MMU, SVM, TLB, VT, dynamic binary translation, nested paging, virtual machine monitor, virtualization, x86},
} 

@article{Adams:2006:CSH:1168919.1168860,
 author = {Adams, Keith and Agesen, Ole},
 title = {A comparison of software and hardware techniques for x86 virtualization},
 abstract = {Until recently, the x86 architecture has not permitted classical trap-and-emulate virtualization. Virtual Machine Monitors for x86, such as VMware \&#174; Workstation and Virtual PC, have instead used binary translation of the guest kernel code. However, both Intel and AMD have now introduced architectural extensions to support classical virtualization.We compare an existing software VMM with a new VMM designed for the emerging hardware support. Surprisingly, the hardware VMM often suffers lower performance than the pure software VMM. To determine why, we study architecture-level events such as page table updates, context switches and I/O, and find their costs vastly different among native, software VMM and hardware VMM execution.We find that the hardware support fails to provide an unambiguous performance advantage for two primary reasons: first, it offers no support for MMU virtualization; second, it fails to co-exist with existing software techniques for MMU virtualization. We look ahead to emerging techniques for addressing this MMU virtualization problem in the context of hardware-assisted virtualization.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168860},
 doi = {http://doi.acm.org/10.1145/1168919.1168860},
 acmid = {1168860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MMU, SVM, TLB, VT, dynamic binary translation, nested paging, virtual machine monitor, virtualization, x86},
} 

@article{Adams:2006:CSH:1168918.1168860,
 author = {Adams, Keith and Agesen, Ole},
 title = {A comparison of software and hardware techniques for x86 virtualization},
 abstract = {Until recently, the x86 architecture has not permitted classical trap-and-emulate virtualization. Virtual Machine Monitors for x86, such as VMware \&#174; Workstation and Virtual PC, have instead used binary translation of the guest kernel code. However, both Intel and AMD have now introduced architectural extensions to support classical virtualization.We compare an existing software VMM with a new VMM designed for the emerging hardware support. Surprisingly, the hardware VMM often suffers lower performance than the pure software VMM. To determine why, we study architecture-level events such as page table updates, context switches and I/O, and find their costs vastly different among native, software VMM and hardware VMM execution.We find that the hardware support fails to provide an unambiguous performance advantage for two primary reasons: first, it offers no support for MMU virtualization; second, it fails to co-exist with existing software techniques for MMU virtualization. We look ahead to emerging techniques for addressing this MMU virtualization problem in the context of hardware-assisted virtualization.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168860},
 doi = {http://doi.acm.org/10.1145/1168918.1168860},
 acmid = {1168860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MMU, SVM, TLB, VT, dynamic binary translation, nested paging, virtual machine monitor, virtualization, x86},
} 

@article{Adams:2006:CSH:1168917.1168860,
 author = {Adams, Keith and Agesen, Ole},
 title = {A comparison of software and hardware techniques for x86 virtualization},
 abstract = {Until recently, the x86 architecture has not permitted classical trap-and-emulate virtualization. Virtual Machine Monitors for x86, such as VMware \&#174; Workstation and Virtual PC, have instead used binary translation of the guest kernel code. However, both Intel and AMD have now introduced architectural extensions to support classical virtualization.We compare an existing software VMM with a new VMM designed for the emerging hardware support. Surprisingly, the hardware VMM often suffers lower performance than the pure software VMM. To determine why, we study architecture-level events such as page table updates, context switches and I/O, and find their costs vastly different among native, software VMM and hardware VMM execution.We find that the hardware support fails to provide an unambiguous performance advantage for two primary reasons: first, it offers no support for MMU virtualization; second, it fails to co-exist with existing software techniques for MMU virtualization. We look ahead to emerging techniques for addressing this MMU virtualization problem in the context of hardware-assisted virtualization.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168860},
 doi = {http://doi.acm.org/10.1145/1168917.1168860},
 acmid = {1168860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MMU, SVM, TLB, VT, dynamic binary translation, nested paging, virtual machine monitor, virtualization, x86},
} 

@article{Jones:2006:GMB:1168919.1168861,
 author = {Jones, Stephen T. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Geiger: monitoring the buffer cache in a virtual machine environment},
 abstract = {Virtualization is increasingly being used to address server management and administration issues like flexible resource allocation, service isolation and workload migration. In a virtualized environment, the virtual machine monitor (VMM) is the primary resource manager and is an attractive target for implementing system features like scheduling, caching, and monitoring. However, the lackof runtime information within the VMM about guest operating systems, sometimes called the semantic gap, is a significant obstacle to efficiently implementing some kinds of services.In this paper we explore techniques that can be used by a VMM to passively infer useful information about a guest operating system's unified buffer cache and virtual memory system. We have created a prototype implementation of these techniques inside the Xen VMM called Geiger and show that it can accurately infer when pages are inserted into and evicted from a system's buffer cache. We explore several nuances involved in passively implementing eviction detection that have not previously been addressed, such as the importance of tracking disk block liveness, the effect of file system journaling, and the importance of accounting for the unified caches found in modern operating systems.Using case studies we show that the information provided by Geiger enables a VMM to implement useful VMM-level services. We implement a novel working set size estimator which allows the VMM to make more informed memory allocation decisions. We also show that a VMM can be used to drastically improve the hit rate in remote storage caches by using eviction-based cache placement without modifying the application or operating system storage interface. Both case studies hint at a future where inference techniques enable a broad new class of VMM-level functionality.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168861},
 doi = {http://doi.acm.org/10.1145/1168919.1168861},
 acmid = {1168861},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, virtual machine},
} 

@article{Jones:2006:GMB:1168918.1168861,
 author = {Jones, Stephen T. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Geiger: monitoring the buffer cache in a virtual machine environment},
 abstract = {Virtualization is increasingly being used to address server management and administration issues like flexible resource allocation, service isolation and workload migration. In a virtualized environment, the virtual machine monitor (VMM) is the primary resource manager and is an attractive target for implementing system features like scheduling, caching, and monitoring. However, the lackof runtime information within the VMM about guest operating systems, sometimes called the semantic gap, is a significant obstacle to efficiently implementing some kinds of services.In this paper we explore techniques that can be used by a VMM to passively infer useful information about a guest operating system's unified buffer cache and virtual memory system. We have created a prototype implementation of these techniques inside the Xen VMM called Geiger and show that it can accurately infer when pages are inserted into and evicted from a system's buffer cache. We explore several nuances involved in passively implementing eviction detection that have not previously been addressed, such as the importance of tracking disk block liveness, the effect of file system journaling, and the importance of accounting for the unified caches found in modern operating systems.Using case studies we show that the information provided by Geiger enables a VMM to implement useful VMM-level services. We implement a novel working set size estimator which allows the VMM to make more informed memory allocation decisions. We also show that a VMM can be used to drastically improve the hit rate in remote storage caches by using eviction-based cache placement without modifying the application or operating system storage interface. Both case studies hint at a future where inference techniques enable a broad new class of VMM-level functionality.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168861},
 doi = {http://doi.acm.org/10.1145/1168918.1168861},
 acmid = {1168861},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, virtual machine},
} 

@inproceedings{Jones:2006:GMB:1168857.1168861,
 author = {Jones, Stephen T. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Geiger: monitoring the buffer cache in a virtual machine environment},
 abstract = {Virtualization is increasingly being used to address server management and administration issues like flexible resource allocation, service isolation and workload migration. In a virtualized environment, the virtual machine monitor (VMM) is the primary resource manager and is an attractive target for implementing system features like scheduling, caching, and monitoring. However, the lackof runtime information within the VMM about guest operating systems, sometimes called the semantic gap, is a significant obstacle to efficiently implementing some kinds of services.In this paper we explore techniques that can be used by a VMM to passively infer useful information about a guest operating system's unified buffer cache and virtual memory system. We have created a prototype implementation of these techniques inside the Xen VMM called Geiger and show that it can accurately infer when pages are inserted into and evicted from a system's buffer cache. We explore several nuances involved in passively implementing eviction detection that have not previously been addressed, such as the importance of tracking disk block liveness, the effect of file system journaling, and the importance of accounting for the unified caches found in modern operating systems.Using case studies we show that the information provided by Geiger enables a VMM to implement useful VMM-level services. We implement a novel working set size estimator which allows the VMM to make more informed memory allocation decisions. We also show that a VMM can be used to drastically improve the hit rate in remote storage caches by using eviction-based cache placement without modifying the application or operating system storage interface. Both case studies hint at a future where inference techniques enable a broad new class of VMM-level functionality.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168861},
 doi = {http://doi.acm.org/10.1145/1168857.1168861},
 acmid = {1168861},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, virtual machine},
} 

@article{Jones:2006:GMB:1168917.1168861,
 author = {Jones, Stephen T. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Geiger: monitoring the buffer cache in a virtual machine environment},
 abstract = {Virtualization is increasingly being used to address server management and administration issues like flexible resource allocation, service isolation and workload migration. In a virtualized environment, the virtual machine monitor (VMM) is the primary resource manager and is an attractive target for implementing system features like scheduling, caching, and monitoring. However, the lackof runtime information within the VMM about guest operating systems, sometimes called the semantic gap, is a significant obstacle to efficiently implementing some kinds of services.In this paper we explore techniques that can be used by a VMM to passively infer useful information about a guest operating system's unified buffer cache and virtual memory system. We have created a prototype implementation of these techniques inside the Xen VMM called Geiger and show that it can accurately infer when pages are inserted into and evicted from a system's buffer cache. We explore several nuances involved in passively implementing eviction detection that have not previously been addressed, such as the importance of tracking disk block liveness, the effect of file system journaling, and the importance of accounting for the unified caches found in modern operating systems.Using case studies we show that the information provided by Geiger enables a VMM to implement useful VMM-level services. We implement a novel working set size estimator which allows the VMM to make more informed memory allocation decisions. We also show that a VMM can be used to drastically improve the hit rate in remote storage caches by using eviction-based cache placement without modifying the application or operating system storage interface. Both case studies hint at a future where inference techniques enable a broad new class of VMM-level functionality.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168861},
 doi = {http://doi.acm.org/10.1145/1168917.1168861},
 acmid = {1168861},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, virtual machine},
} 

@article{Crandall:2006:TSD:1168917.1168862,
 author = {Crandall, Jedidiah R. and Wassermann, Gary and de Oliveira, Daniela A. S. and Su, Zhendong and Wu, S. Felix and Chong, Frederic T.},
 title = {Temporal search: detecting hidden malware timebombs with virtual machines},
 abstract = {Worms, viruses, and other malware can be ticking bombs counting down to a specific time, when they might, for example, delete files or download new instructions from a public web server. We propose a novel virtual-machine-based analysis technique to automatically discover the timetable</i> of a piece of malware, or when events will be triggered, so that other types of analysis can discern what those events are. This information can be invaluable for responding to rapid malware, and automating its discovery can provide more accurate information with less delay than careful human analysis.Developing an automated system that produces the timetable of a piece of malware is a challenging research problem. In this paper, we describe our implementation of a key component of such a system: the discovery of timers without making assumptions about the integrity of the infected system's kernel. Our technique runs a virtual machine at slightly different rates of perceived time</i> (time as seen by the virtual machine), and identifies time counters by correlating memory write frequency to timer interrupt frequency.We also analyze real malware to assess the feasibility of using full-system, machine-level symbolic execution on these timers to discover predicates. Because of the intricacies of the Gregorian calendar (leap years, different number of days in each month, etc.) these predicates will not be direct expressions on the timer but instead an annotated trace; so we formalize the calculation of a timetable as a weakest precondition calculation. Our analysis of six real worms sheds light on two challenges for future work: 1) time-dependent malware behavior often does not follow a linear timetable; and 2) that an attacker with knowledge of the analysis technique can evade analysis. Our current results are promising in that with simple symbolic execution we are able to discover predicates on the day of the month for four real worms. Then through more traditional manual analysis we conclude that a more control-flow-sensitive symbolic execution implementation would discover all predicates for the malware we analyzed.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168862},
 doi = {http://doi.acm.org/10.1145/1168917.1168862},
 acmid = {1168862},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {malware, virtual machines, worms},
} 

@inproceedings{Crandall:2006:TSD:1168857.1168862,
 author = {Crandall, Jedidiah R. and Wassermann, Gary and de Oliveira, Daniela A. S. and Su, Zhendong and Wu, S. Felix and Chong, Frederic T.},
 title = {Temporal search: detecting hidden malware timebombs with virtual machines},
 abstract = {Worms, viruses, and other malware can be ticking bombs counting down to a specific time, when they might, for example, delete files or download new instructions from a public web server. We propose a novel virtual-machine-based analysis technique to automatically discover the timetable</i> of a piece of malware, or when events will be triggered, so that other types of analysis can discern what those events are. This information can be invaluable for responding to rapid malware, and automating its discovery can provide more accurate information with less delay than careful human analysis.Developing an automated system that produces the timetable of a piece of malware is a challenging research problem. In this paper, we describe our implementation of a key component of such a system: the discovery of timers without making assumptions about the integrity of the infected system's kernel. Our technique runs a virtual machine at slightly different rates of perceived time</i> (time as seen by the virtual machine), and identifies time counters by correlating memory write frequency to timer interrupt frequency.We also analyze real malware to assess the feasibility of using full-system, machine-level symbolic execution on these timers to discover predicates. Because of the intricacies of the Gregorian calendar (leap years, different number of days in each month, etc.) these predicates will not be direct expressions on the timer but instead an annotated trace; so we formalize the calculation of a timetable as a weakest precondition calculation. Our analysis of six real worms sheds light on two challenges for future work: 1) time-dependent malware behavior often does not follow a linear timetable; and 2) that an attacker with knowledge of the analysis technique can evade analysis. Our current results are promising in that with simple symbolic execution we are able to discover predicates on the day of the month for four real worms. Then through more traditional manual analysis we conclude that a more control-flow-sensitive symbolic execution implementation would discover all predicates for the malware we analyzed.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168862},
 doi = {http://doi.acm.org/10.1145/1168857.1168862},
 acmid = {1168862},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {malware, virtual machines, worms},
} 

@article{Crandall:2006:TSD:1168918.1168862,
 author = {Crandall, Jedidiah R. and Wassermann, Gary and de Oliveira, Daniela A. S. and Su, Zhendong and Wu, S. Felix and Chong, Frederic T.},
 title = {Temporal search: detecting hidden malware timebombs with virtual machines},
 abstract = {Worms, viruses, and other malware can be ticking bombs counting down to a specific time, when they might, for example, delete files or download new instructions from a public web server. We propose a novel virtual-machine-based analysis technique to automatically discover the timetable</i> of a piece of malware, or when events will be triggered, so that other types of analysis can discern what those events are. This information can be invaluable for responding to rapid malware, and automating its discovery can provide more accurate information with less delay than careful human analysis.Developing an automated system that produces the timetable of a piece of malware is a challenging research problem. In this paper, we describe our implementation of a key component of such a system: the discovery of timers without making assumptions about the integrity of the infected system's kernel. Our technique runs a virtual machine at slightly different rates of perceived time</i> (time as seen by the virtual machine), and identifies time counters by correlating memory write frequency to timer interrupt frequency.We also analyze real malware to assess the feasibility of using full-system, machine-level symbolic execution on these timers to discover predicates. Because of the intricacies of the Gregorian calendar (leap years, different number of days in each month, etc.) these predicates will not be direct expressions on the timer but instead an annotated trace; so we formalize the calculation of a timetable as a weakest precondition calculation. Our analysis of six real worms sheds light on two challenges for future work: 1) time-dependent malware behavior often does not follow a linear timetable; and 2) that an attacker with knowledge of the analysis technique can evade analysis. Our current results are promising in that with simple symbolic execution we are able to discover predicates on the day of the month for four real worms. Then through more traditional manual analysis we conclude that a more control-flow-sensitive symbolic execution implementation would discover all predicates for the malware we analyzed.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168862},
 doi = {http://doi.acm.org/10.1145/1168918.1168862},
 acmid = {1168862},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {malware, virtual machines, worms},
} 

@article{Crandall:2006:TSD:1168919.1168862,
 author = {Crandall, Jedidiah R. and Wassermann, Gary and de Oliveira, Daniela A. S. and Su, Zhendong and Wu, S. Felix and Chong, Frederic T.},
 title = {Temporal search: detecting hidden malware timebombs with virtual machines},
 abstract = {Worms, viruses, and other malware can be ticking bombs counting down to a specific time, when they might, for example, delete files or download new instructions from a public web server. We propose a novel virtual-machine-based analysis technique to automatically discover the timetable</i> of a piece of malware, or when events will be triggered, so that other types of analysis can discern what those events are. This information can be invaluable for responding to rapid malware, and automating its discovery can provide more accurate information with less delay than careful human analysis.Developing an automated system that produces the timetable of a piece of malware is a challenging research problem. In this paper, we describe our implementation of a key component of such a system: the discovery of timers without making assumptions about the integrity of the infected system's kernel. Our technique runs a virtual machine at slightly different rates of perceived time</i> (time as seen by the virtual machine), and identifies time counters by correlating memory write frequency to timer interrupt frequency.We also analyze real malware to assess the feasibility of using full-system, machine-level symbolic execution on these timers to discover predicates. Because of the intricacies of the Gregorian calendar (leap years, different number of days in each month, etc.) these predicates will not be direct expressions on the timer but instead an annotated trace; so we formalize the calculation of a timetable as a weakest precondition calculation. Our analysis of six real worms sheds light on two challenges for future work: 1) time-dependent malware behavior often does not follow a linear timetable; and 2) that an attacker with knowledge of the analysis technique can evade analysis. Our current results are promising in that with simple symbolic execution we are able to discover predicates on the day of the month for four real worms. Then through more traditional manual analysis we conclude that a more control-flow-sensitive symbolic execution implementation would discover all predicates for the malware we analyzed.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168862},
 doi = {http://doi.acm.org/10.1145/1168919.1168862},
 acmid = {1168862},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {malware, virtual machines, worms},
} 

@article{Lu:2006:ADA:1168919.1168864,
 author = {Lu, Shan and Tucek, Joseph and Qin, Feng and Zhou, Yuanyuan},
 title = {AVIO: detecting atomicity violations via access interleaving invariants},
 abstract = {Concurrency bugs are among the most difficult to test and diagnose of all software bugs. The multicore technology trend worsens this problem. Most previous concurrency bug detection work focuses on one bug subclass, data races, and neglects many other important ones such as atomicity violations</i>, which will soon become increasingly important due to the emerging trend of transactional memory models.This paper proposes an innovative, comprehensive, invariantbased approach called AVIO to detect atomicity violations. Our idea is based on a novel observation called access interleaving invariant</i>, which is a good indication of programmers' assumptions about the atomicity of certain code regions. By automatically extracting such invariants and detecting violations of these invariants at run time, AVIO can detect a variety of atomicity violations.Based on this idea, we have designed and built <b>two</b> implementations of AVIO and evaluated the <b>trade-offs</b> between them. The first implementation, AVIO-S, is purely in software, while the second, AVIO-H, requires some simple extensions to the cache coherence hardware. AVIO-S is cheaper and more accurate but incurs much higher overhead and thus more run-time perturbation than AVIOH. Therefore, AVIO-S is more suitable for in-house bug detection and postmortem bug diagnosis, while AVIO-H can be used for bug detection during production runs.We evaluate both implementations of AVIO using large realworld server applications (Apache and MySQL) with six representative real atomicity violation bugs, and SPLASH-2 benchmarks. Our results show that AVIO detects more tested atomicity violations of various types and has 25 times fewer false positives than previous solutions on average.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168864},
 doi = {http://doi.acm.org/10.1145/1168919.1168864},
 acmid = {1168864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity violation, bug detection, concurrency bug, concurrent program, hardware support, program invariant},
} 

@article{Lu:2006:ADA:1168917.1168864,
 author = {Lu, Shan and Tucek, Joseph and Qin, Feng and Zhou, Yuanyuan},
 title = {AVIO: detecting atomicity violations via access interleaving invariants},
 abstract = {Concurrency bugs are among the most difficult to test and diagnose of all software bugs. The multicore technology trend worsens this problem. Most previous concurrency bug detection work focuses on one bug subclass, data races, and neglects many other important ones such as atomicity violations</i>, which will soon become increasingly important due to the emerging trend of transactional memory models.This paper proposes an innovative, comprehensive, invariantbased approach called AVIO to detect atomicity violations. Our idea is based on a novel observation called access interleaving invariant</i>, which is a good indication of programmers' assumptions about the atomicity of certain code regions. By automatically extracting such invariants and detecting violations of these invariants at run time, AVIO can detect a variety of atomicity violations.Based on this idea, we have designed and built <b>two</b> implementations of AVIO and evaluated the <b>trade-offs</b> between them. The first implementation, AVIO-S, is purely in software, while the second, AVIO-H, requires some simple extensions to the cache coherence hardware. AVIO-S is cheaper and more accurate but incurs much higher overhead and thus more run-time perturbation than AVIOH. Therefore, AVIO-S is more suitable for in-house bug detection and postmortem bug diagnosis, while AVIO-H can be used for bug detection during production runs.We evaluate both implementations of AVIO using large realworld server applications (Apache and MySQL) with six representative real atomicity violation bugs, and SPLASH-2 benchmarks. Our results show that AVIO detects more tested atomicity violations of various types and has 25 times fewer false positives than previous solutions on average.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168864},
 doi = {http://doi.acm.org/10.1145/1168917.1168864},
 acmid = {1168864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity violation, bug detection, concurrency bug, concurrent program, hardware support, program invariant},
} 

@article{Lu:2006:ADA:1168918.1168864,
 author = {Lu, Shan and Tucek, Joseph and Qin, Feng and Zhou, Yuanyuan},
 title = {AVIO: detecting atomicity violations via access interleaving invariants},
 abstract = {Concurrency bugs are among the most difficult to test and diagnose of all software bugs. The multicore technology trend worsens this problem. Most previous concurrency bug detection work focuses on one bug subclass, data races, and neglects many other important ones such as atomicity violations</i>, which will soon become increasingly important due to the emerging trend of transactional memory models.This paper proposes an innovative, comprehensive, invariantbased approach called AVIO to detect atomicity violations. Our idea is based on a novel observation called access interleaving invariant</i>, which is a good indication of programmers' assumptions about the atomicity of certain code regions. By automatically extracting such invariants and detecting violations of these invariants at run time, AVIO can detect a variety of atomicity violations.Based on this idea, we have designed and built <b>two</b> implementations of AVIO and evaluated the <b>trade-offs</b> between them. The first implementation, AVIO-S, is purely in software, while the second, AVIO-H, requires some simple extensions to the cache coherence hardware. AVIO-S is cheaper and more accurate but incurs much higher overhead and thus more run-time perturbation than AVIOH. Therefore, AVIO-S is more suitable for in-house bug detection and postmortem bug diagnosis, while AVIO-H can be used for bug detection during production runs.We evaluate both implementations of AVIO using large realworld server applications (Apache and MySQL) with six representative real atomicity violation bugs, and SPLASH-2 benchmarks. Our results show that AVIO detects more tested atomicity violations of various types and has 25 times fewer false positives than previous solutions on average.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168864},
 doi = {http://doi.acm.org/10.1145/1168918.1168864},
 acmid = {1168864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity violation, bug detection, concurrency bug, concurrent program, hardware support, program invariant},
} 

@inproceedings{Lu:2006:ADA:1168857.1168864,
 author = {Lu, Shan and Tucek, Joseph and Qin, Feng and Zhou, Yuanyuan},
 title = {AVIO: detecting atomicity violations via access interleaving invariants},
 abstract = {Concurrency bugs are among the most difficult to test and diagnose of all software bugs. The multicore technology trend worsens this problem. Most previous concurrency bug detection work focuses on one bug subclass, data races, and neglects many other important ones such as atomicity violations</i>, which will soon become increasingly important due to the emerging trend of transactional memory models.This paper proposes an innovative, comprehensive, invariantbased approach called AVIO to detect atomicity violations. Our idea is based on a novel observation called access interleaving invariant</i>, which is a good indication of programmers' assumptions about the atomicity of certain code regions. By automatically extracting such invariants and detecting violations of these invariants at run time, AVIO can detect a variety of atomicity violations.Based on this idea, we have designed and built <b>two</b> implementations of AVIO and evaluated the <b>trade-offs</b> between them. The first implementation, AVIO-S, is purely in software, while the second, AVIO-H, requires some simple extensions to the cache coherence hardware. AVIO-S is cheaper and more accurate but incurs much higher overhead and thus more run-time perturbation than AVIOH. Therefore, AVIO-S is more suitable for in-house bug detection and postmortem bug diagnosis, while AVIO-H can be used for bug detection during production runs.We evaluate both implementations of AVIO using large realworld server applications (Apache and MySQL) with six representative real atomicity violation bugs, and SPLASH-2 benchmarks. Our results show that AVIO detects more tested atomicity violations of various types and has 25 times fewer false positives than previous solutions on average.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168864},
 doi = {http://doi.acm.org/10.1145/1168857.1168864},
 acmid = {1168864},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomicity violation, bug detection, concurrency bug, concurrent program, hardware support, program invariant},
} 

@article{Xu:2006:RTR:1168919.1168865,
 author = {Xu, Min and Hill, Mark D. and Bodik, Rastislav},
 title = {A regulated transitive reduction (RTR) for longer memory race recording},
 abstract = {Now at VMware. Multithreaded deterministic replay has important applications in cyclic debugging, fault tolerance and intrusion analysis. Memory race recording is a key technology for multithreaded deterministic replay. In this paper, we considerably improve our previous always-on Flight Data Recorder (FDR) in four ways: <b>\&#8226;Longer recording</b> by reducing the log size growth rate to approximately one byte per thousand dynamic instructions. <b>\&#8226;Lower hardware cost</b> by reducing the cost to 24 KB per processor core. <b>\&#8226;Simpler design</b> by modifying only the cache coherence protocol, but not the cache. <b>\&#8226;Broader applicability</b> by supporting both Sequential Consistency (SC) and Total Store Order (TSO) memory consistency models (existing recorders support only SC).These improvements stem from several ideas: (1) a <b>Regulated Transitive Reduction (RTR)</b> recording algorithm that creates stricter and vectorizable dependencies to reduce the log growth rate; (2) a <b>Set/LRU</b> timestamp approximation method that better approximates timestamps of uncached memory locations to reduce the hardware cost; (3) an <b>order-value-hybrid</b> recording methodthat explicitly logs the value of potential SC-violating load instructions to support multiprocessor systems with TSO.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168865},
 doi = {http://doi.acm.org/10.1145/1168919.1168865},
 acmid = {1168865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {determinism, multithreading, race recording},
} 

@article{Xu:2006:RTR:1168917.1168865,
 author = {Xu, Min and Hill, Mark D. and Bodik, Rastislav},
 title = {A regulated transitive reduction (RTR) for longer memory race recording},
 abstract = {Now at VMware. Multithreaded deterministic replay has important applications in cyclic debugging, fault tolerance and intrusion analysis. Memory race recording is a key technology for multithreaded deterministic replay. In this paper, we considerably improve our previous always-on Flight Data Recorder (FDR) in four ways: <b>\&#8226;Longer recording</b> by reducing the log size growth rate to approximately one byte per thousand dynamic instructions. <b>\&#8226;Lower hardware cost</b> by reducing the cost to 24 KB per processor core. <b>\&#8226;Simpler design</b> by modifying only the cache coherence protocol, but not the cache. <b>\&#8226;Broader applicability</b> by supporting both Sequential Consistency (SC) and Total Store Order (TSO) memory consistency models (existing recorders support only SC).These improvements stem from several ideas: (1) a <b>Regulated Transitive Reduction (RTR)</b> recording algorithm that creates stricter and vectorizable dependencies to reduce the log growth rate; (2) a <b>Set/LRU</b> timestamp approximation method that better approximates timestamps of uncached memory locations to reduce the hardware cost; (3) an <b>order-value-hybrid</b> recording methodthat explicitly logs the value of potential SC-violating load instructions to support multiprocessor systems with TSO.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168865},
 doi = {http://doi.acm.org/10.1145/1168917.1168865},
 acmid = {1168865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {determinism, multithreading, race recording},
} 

@article{Xu:2006:RTR:1168918.1168865,
 author = {Xu, Min and Hill, Mark D. and Bodik, Rastislav},
 title = {A regulated transitive reduction (RTR) for longer memory race recording},
 abstract = {Now at VMware. Multithreaded deterministic replay has important applications in cyclic debugging, fault tolerance and intrusion analysis. Memory race recording is a key technology for multithreaded deterministic replay. In this paper, we considerably improve our previous always-on Flight Data Recorder (FDR) in four ways: <b>\&#8226;Longer recording</b> by reducing the log size growth rate to approximately one byte per thousand dynamic instructions. <b>\&#8226;Lower hardware cost</b> by reducing the cost to 24 KB per processor core. <b>\&#8226;Simpler design</b> by modifying only the cache coherence protocol, but not the cache. <b>\&#8226;Broader applicability</b> by supporting both Sequential Consistency (SC) and Total Store Order (TSO) memory consistency models (existing recorders support only SC).These improvements stem from several ideas: (1) a <b>Regulated Transitive Reduction (RTR)</b> recording algorithm that creates stricter and vectorizable dependencies to reduce the log growth rate; (2) a <b>Set/LRU</b> timestamp approximation method that better approximates timestamps of uncached memory locations to reduce the hardware cost; (3) an <b>order-value-hybrid</b> recording methodthat explicitly logs the value of potential SC-violating load instructions to support multiprocessor systems with TSO.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168865},
 doi = {http://doi.acm.org/10.1145/1168918.1168865},
 acmid = {1168865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {determinism, multithreading, race recording},
} 

@inproceedings{Xu:2006:RTR:1168857.1168865,
 author = {Xu, Min and Hill, Mark D. and Bodik, Rastislav},
 title = {A regulated transitive reduction (RTR) for longer memory race recording},
 abstract = {Now at VMware. Multithreaded deterministic replay has important applications in cyclic debugging, fault tolerance and intrusion analysis. Memory race recording is a key technology for multithreaded deterministic replay. In this paper, we considerably improve our previous always-on Flight Data Recorder (FDR) in four ways: <b>\&#8226;Longer recording</b> by reducing the log size growth rate to approximately one byte per thousand dynamic instructions. <b>\&#8226;Lower hardware cost</b> by reducing the cost to 24 KB per processor core. <b>\&#8226;Simpler design</b> by modifying only the cache coherence protocol, but not the cache. <b>\&#8226;Broader applicability</b> by supporting both Sequential Consistency (SC) and Total Store Order (TSO) memory consistency models (existing recorders support only SC).These improvements stem from several ideas: (1) a <b>Regulated Transitive Reduction (RTR)</b> recording algorithm that creates stricter and vectorizable dependencies to reduce the log growth rate; (2) a <b>Set/LRU</b> timestamp approximation method that better approximates timestamps of uncached memory locations to reduce the hardware cost; (3) an <b>order-value-hybrid</b> recording methodthat explicitly logs the value of potential SC-violating load instructions to support multiprocessor systems with TSO.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168865},
 doi = {http://doi.acm.org/10.1145/1168857.1168865},
 acmid = {1168865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {determinism, multithreading, race recording},
} 

@article{Bond:2006:BBO:1168918.1168866,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Bell: bit-encoding online memory leak detection},
 abstract = {Memory leaks compromise availability and security by crippling performance and crashing programs. Leaks are difficult to diagnose because they have no immediate symptoms. Online leak detection tools benefit from storing and reporting per-object sites</i> (e.g., allocation sites) for potentially leaking objects. In programs with many small objects, per-object sites add high space overhead, limiting their use in production environments.This paper introduces Bit-Encoding Leak Location</i> (Bell), a statistical approach that encodes</i> per-object sites to a single bit per object. A bit loses information about a site, but given sufficient objects that use the site and a known, finite set of possible sites, Bell uses brute-force decoding</i> to recover the site with high accuracy.We use this approach to encode object allocation and last-use sites in Sleigh</i>, a new leak detection tool. Sleigh detects stale</i> objects (objects unused for a long time) and uses Bell decoding to report their allocation and last-use sites. Our implementation steals four unused bits in the object header and thus incurs no per-object space overhead. Sleigh's instrumentation adds 29\% execution time overhead, which adaptive profiling reduces to 11\%. Sleigh's output is directly useful for finding and fixing leaks in SPEC JBB2000 and Eclipse, although sufficiently many objects must leak before Bell decoding can report sites with confidence. Bell is suitable for other leak detection approaches that store per-object sites, and for other problems amenable to statistical per-object metadata.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168866},
 doi = {http://doi.acm.org/10.1145/1168918.1168866},
 acmid = {1168866},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, managed languages, memory leaks, probabilistic approaches},
} 

@article{Bond:2006:BBO:1168919.1168866,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Bell: bit-encoding online memory leak detection},
 abstract = {Memory leaks compromise availability and security by crippling performance and crashing programs. Leaks are difficult to diagnose because they have no immediate symptoms. Online leak detection tools benefit from storing and reporting per-object sites</i> (e.g., allocation sites) for potentially leaking objects. In programs with many small objects, per-object sites add high space overhead, limiting their use in production environments.This paper introduces Bit-Encoding Leak Location</i> (Bell), a statistical approach that encodes</i> per-object sites to a single bit per object. A bit loses information about a site, but given sufficient objects that use the site and a known, finite set of possible sites, Bell uses brute-force decoding</i> to recover the site with high accuracy.We use this approach to encode object allocation and last-use sites in Sleigh</i>, a new leak detection tool. Sleigh detects stale</i> objects (objects unused for a long time) and uses Bell decoding to report their allocation and last-use sites. Our implementation steals four unused bits in the object header and thus incurs no per-object space overhead. Sleigh's instrumentation adds 29\% execution time overhead, which adaptive profiling reduces to 11\%. Sleigh's output is directly useful for finding and fixing leaks in SPEC JBB2000 and Eclipse, although sufficiently many objects must leak before Bell decoding can report sites with confidence. Bell is suitable for other leak detection approaches that store per-object sites, and for other problems amenable to statistical per-object metadata.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168866},
 doi = {http://doi.acm.org/10.1145/1168919.1168866},
 acmid = {1168866},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, managed languages, memory leaks, probabilistic approaches},
} 

@article{Bond:2006:BBO:1168917.1168866,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Bell: bit-encoding online memory leak detection},
 abstract = {Memory leaks compromise availability and security by crippling performance and crashing programs. Leaks are difficult to diagnose because they have no immediate symptoms. Online leak detection tools benefit from storing and reporting per-object sites</i> (e.g., allocation sites) for potentially leaking objects. In programs with many small objects, per-object sites add high space overhead, limiting their use in production environments.This paper introduces Bit-Encoding Leak Location</i> (Bell), a statistical approach that encodes</i> per-object sites to a single bit per object. A bit loses information about a site, but given sufficient objects that use the site and a known, finite set of possible sites, Bell uses brute-force decoding</i> to recover the site with high accuracy.We use this approach to encode object allocation and last-use sites in Sleigh</i>, a new leak detection tool. Sleigh detects stale</i> objects (objects unused for a long time) and uses Bell decoding to report their allocation and last-use sites. Our implementation steals four unused bits in the object header and thus incurs no per-object space overhead. Sleigh's instrumentation adds 29\% execution time overhead, which adaptive profiling reduces to 11\%. Sleigh's output is directly useful for finding and fixing leaks in SPEC JBB2000 and Eclipse, although sufficiently many objects must leak before Bell decoding can report sites with confidence. Bell is suitable for other leak detection approaches that store per-object sites, and for other problems amenable to statistical per-object metadata.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168866},
 doi = {http://doi.acm.org/10.1145/1168917.1168866},
 acmid = {1168866},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, managed languages, memory leaks, probabilistic approaches},
} 

@inproceedings{Bond:2006:BBO:1168857.1168866,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Bell: bit-encoding online memory leak detection},
 abstract = {Memory leaks compromise availability and security by crippling performance and crashing programs. Leaks are difficult to diagnose because they have no immediate symptoms. Online leak detection tools benefit from storing and reporting per-object sites</i> (e.g., allocation sites) for potentially leaking objects. In programs with many small objects, per-object sites add high space overhead, limiting their use in production environments.This paper introduces Bit-Encoding Leak Location</i> (Bell), a statistical approach that encodes</i> per-object sites to a single bit per object. A bit loses information about a site, but given sufficient objects that use the site and a known, finite set of possible sites, Bell uses brute-force decoding</i> to recover the site with high accuracy.We use this approach to encode object allocation and last-use sites in Sleigh</i>, a new leak detection tool. Sleigh detects stale</i> objects (objects unused for a long time) and uses Bell decoding to report their allocation and last-use sites. Our implementation steals four unused bits in the object header and thus incurs no per-object space overhead. Sleigh's instrumentation adds 29\% execution time overhead, which adaptive profiling reduces to 11\%. Sleigh's output is directly useful for finding and fixing leaks in SPEC JBB2000 and Eclipse, although sufficiently many objects must leak before Bell decoding can report sites with confidence. Bell is suitable for other leak detection approaches that store per-object sites, and for other problems amenable to statistical per-object metadata.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168866},
 doi = {http://doi.acm.org/10.1145/1168857.1168866},
 acmid = {1168866},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, managed languages, memory leaks, probabilistic approaches},
} 

@article{Shyam:2006:ULD:1168919.1168868,
 author = {Shyam, Smitha and Constantinides, Kypros and Phadke, Sujay and Bertacco, Valeria and Austin, Todd},
 title = {Ultra low-cost defect protection for microprocessor pipelines},
 abstract = {The sustained push toward smaller and smaller technology sizes has reached a point where device reliability has moved to the forefront of concerns for next-generation designs. Silicon failure mechanisms, such as transistor wearout and manufacturing defects, are a growing challenge that threatens the yield and product lifetime of future systems. In this paper we introduce the BulletProof</i> pipeline, the first ultra low-cost mechanism to protect a microprocessor pipeline and on-chip memory system from silicon defects. To achieve this goal we combine area-frugal on-line testing techniques and system-level checkpointing to provide the same guarantees of reliability found in traditional solutions, but at much lower cost. Our approach utilizes a microarchitectural checkpointing mechanism which creates coarse-grained epochs of execution, during which distributed on-line built in self-test (BIST) mechanisms validate the integrity of the underlying hardware. In case a failure is detected, we rely on the natural redundancy of instructionlevel parallel processors to repair the system so that it can still operate in a degraded performance mode. Using detailed circuit-level and architectural simulation, we find that our approach provides very high coverage of silicon defects (89\%) with little area cost (5.8\%). In addition, when a defect occurs, the subsequent degraded mode of operation was found to have only moderate performance impacts, (from 4\% to 18\% slowdown).},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {73--82},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168868},
 doi = {http://doi.acm.org/10.1145/1168919.1168868},
 acmid = {1168868},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect-protection, low-cost, pipelines, reliability},
} 

@article{Shyam:2006:ULD:1168918.1168868,
 author = {Shyam, Smitha and Constantinides, Kypros and Phadke, Sujay and Bertacco, Valeria and Austin, Todd},
 title = {Ultra low-cost defect protection for microprocessor pipelines},
 abstract = {The sustained push toward smaller and smaller technology sizes has reached a point where device reliability has moved to the forefront of concerns for next-generation designs. Silicon failure mechanisms, such as transistor wearout and manufacturing defects, are a growing challenge that threatens the yield and product lifetime of future systems. In this paper we introduce the BulletProof</i> pipeline, the first ultra low-cost mechanism to protect a microprocessor pipeline and on-chip memory system from silicon defects. To achieve this goal we combine area-frugal on-line testing techniques and system-level checkpointing to provide the same guarantees of reliability found in traditional solutions, but at much lower cost. Our approach utilizes a microarchitectural checkpointing mechanism which creates coarse-grained epochs of execution, during which distributed on-line built in self-test (BIST) mechanisms validate the integrity of the underlying hardware. In case a failure is detected, we rely on the natural redundancy of instructionlevel parallel processors to repair the system so that it can still operate in a degraded performance mode. Using detailed circuit-level and architectural simulation, we find that our approach provides very high coverage of silicon defects (89\%) with little area cost (5.8\%). In addition, when a defect occurs, the subsequent degraded mode of operation was found to have only moderate performance impacts, (from 4\% to 18\% slowdown).},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {73--82},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168868},
 doi = {http://doi.acm.org/10.1145/1168918.1168868},
 acmid = {1168868},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect-protection, low-cost, pipelines, reliability},
} 

@article{Shyam:2006:ULD:1168917.1168868,
 author = {Shyam, Smitha and Constantinides, Kypros and Phadke, Sujay and Bertacco, Valeria and Austin, Todd},
 title = {Ultra low-cost defect protection for microprocessor pipelines},
 abstract = {The sustained push toward smaller and smaller technology sizes has reached a point where device reliability has moved to the forefront of concerns for next-generation designs. Silicon failure mechanisms, such as transistor wearout and manufacturing defects, are a growing challenge that threatens the yield and product lifetime of future systems. In this paper we introduce the BulletProof</i> pipeline, the first ultra low-cost mechanism to protect a microprocessor pipeline and on-chip memory system from silicon defects. To achieve this goal we combine area-frugal on-line testing techniques and system-level checkpointing to provide the same guarantees of reliability found in traditional solutions, but at much lower cost. Our approach utilizes a microarchitectural checkpointing mechanism which creates coarse-grained epochs of execution, during which distributed on-line built in self-test (BIST) mechanisms validate the integrity of the underlying hardware. In case a failure is detected, we rely on the natural redundancy of instructionlevel parallel processors to repair the system so that it can still operate in a degraded performance mode. Using detailed circuit-level and architectural simulation, we find that our approach provides very high coverage of silicon defects (89\%) with little area cost (5.8\%). In addition, when a defect occurs, the subsequent degraded mode of operation was found to have only moderate performance impacts, (from 4\% to 18\% slowdown).},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {73--82},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168868},
 doi = {http://doi.acm.org/10.1145/1168917.1168868},
 acmid = {1168868},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect-protection, low-cost, pipelines, reliability},
} 

@inproceedings{Shyam:2006:ULD:1168857.1168868,
 author = {Shyam, Smitha and Constantinides, Kypros and Phadke, Sujay and Bertacco, Valeria and Austin, Todd},
 title = {Ultra low-cost defect protection for microprocessor pipelines},
 abstract = {The sustained push toward smaller and smaller technology sizes has reached a point where device reliability has moved to the forefront of concerns for next-generation designs. Silicon failure mechanisms, such as transistor wearout and manufacturing defects, are a growing challenge that threatens the yield and product lifetime of future systems. In this paper we introduce the BulletProof</i> pipeline, the first ultra low-cost mechanism to protect a microprocessor pipeline and on-chip memory system from silicon defects. To achieve this goal we combine area-frugal on-line testing techniques and system-level checkpointing to provide the same guarantees of reliability found in traditional solutions, but at much lower cost. Our approach utilizes a microarchitectural checkpointing mechanism which creates coarse-grained epochs of execution, during which distributed on-line built in self-test (BIST) mechanisms validate the integrity of the underlying hardware. In case a failure is detected, we rely on the natural redundancy of instructionlevel parallel processors to repair the system so that it can still operate in a degraded performance mode. Using detailed circuit-level and architectural simulation, we find that our approach provides very high coverage of silicon defects (89\%) with little area cost (5.8\%). In addition, when a defect occurs, the subsequent degraded mode of operation was found to have only moderate performance impacts, (from 4\% to 18\% slowdown).},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {73--82},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168868},
 doi = {http://doi.acm.org/10.1145/1168857.1168868},
 acmid = {1168868},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {defect-protection, low-cost, pipelines, reliability},
} 

@inproceedings{Reddy:2006:UPP:1168857.1168869,
 author = {Reddy, Vimal K. and Rotenberg, Eric and Parthasarathy, Sailashri},
 title = {Understanding prediction-based partial redundant threading for low-overhead, high- coverage fault tolerance},
 abstract = {Redundant threading architectures duplicate all instructions to detect and possibly recover from transient faults. Several lighter weight Partial Redundant Threading (PRT) architectures have been proposed recently. (i) Opportunistic Fault Tolerance duplicates instructions only during periods of poor single-thread performance. (ii) ReStore does not explicitly duplicate instructions and instead exploits mispredictions among highly confident branch predictions as symptoms of faults. (iii) Slipstream creates a reduced alternate thread by replacing many instructions with highly confident predictions. We explore PRT as a possible direction for achieving the fault tolerance of full duplication with the performance of single-thread execution. Opportunistic and ReStore yield partial coverage since they are restricted to using only partial duplication or only confident predictions, respectively. Previous analysis of Slipstream fault tolerance was cursory and concluded that only duplicated instructions are covered. In this paper, we attempt to better understand Slipstream's fault tolerance, conjecturing that the mixture of partial duplication and confident predictions actually closely approximates the coverage of full duplication. A thorough dissection of prediction scenarios confirms that faults in nearly 100\% of instructions are detectable. Fewer than 0.1\% of faulty instructions are not detectable due to coincident faults and mispredictions. Next we show that the current recovery implementation fails to leverage excellent detection capability, since recovery sometimes initiates belatedly, after already retiring a detected faulty instruction. We propose and evaluate a suite of simple microarchitectural alterations to recovery and checking. Using the best alterations, Slipstream can recover from faults in 99\% of instructions, compared to only 78\% of instructions without alterations. Both results are much higher than predicted by past research, which claims coverage for only duplicated instructions, or 65\% of instructions. On an 8-issue SMT processor, Slipstream performs within 1.3\% of single-thread execution whereas full duplication slows performance by 14\%.A key byproduct of this paper is a novel analysis framework in which every dynamic instruction is considered to be hypothetically faulty, thus not requiring explicit fault injection. Fault coverage is measured in terms of the fraction of candidate faulty instructions that are directly or indirectly detectable before.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168869},
 doi = {http://doi.acm.org/10.1145/1168857.1168869},
 acmid = {1168869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessor (CMP), redundant multithreading, simultaneous multithreading (SMT), slipstream processor, time redundancy, transient faults, value prediction},
} 

@article{Reddy:2006:UPP:1168919.1168869,
 author = {Reddy, Vimal K. and Rotenberg, Eric and Parthasarathy, Sailashri},
 title = {Understanding prediction-based partial redundant threading for low-overhead, high- coverage fault tolerance},
 abstract = {Redundant threading architectures duplicate all instructions to detect and possibly recover from transient faults. Several lighter weight Partial Redundant Threading (PRT) architectures have been proposed recently. (i) Opportunistic Fault Tolerance duplicates instructions only during periods of poor single-thread performance. (ii) ReStore does not explicitly duplicate instructions and instead exploits mispredictions among highly confident branch predictions as symptoms of faults. (iii) Slipstream creates a reduced alternate thread by replacing many instructions with highly confident predictions. We explore PRT as a possible direction for achieving the fault tolerance of full duplication with the performance of single-thread execution. Opportunistic and ReStore yield partial coverage since they are restricted to using only partial duplication or only confident predictions, respectively. Previous analysis of Slipstream fault tolerance was cursory and concluded that only duplicated instructions are covered. In this paper, we attempt to better understand Slipstream's fault tolerance, conjecturing that the mixture of partial duplication and confident predictions actually closely approximates the coverage of full duplication. A thorough dissection of prediction scenarios confirms that faults in nearly 100\% of instructions are detectable. Fewer than 0.1\% of faulty instructions are not detectable due to coincident faults and mispredictions. Next we show that the current recovery implementation fails to leverage excellent detection capability, since recovery sometimes initiates belatedly, after already retiring a detected faulty instruction. We propose and evaluate a suite of simple microarchitectural alterations to recovery and checking. Using the best alterations, Slipstream can recover from faults in 99\% of instructions, compared to only 78\% of instructions without alterations. Both results are much higher than predicted by past research, which claims coverage for only duplicated instructions, or 65\% of instructions. On an 8-issue SMT processor, Slipstream performs within 1.3\% of single-thread execution whereas full duplication slows performance by 14\%.A key byproduct of this paper is a novel analysis framework in which every dynamic instruction is considered to be hypothetically faulty, thus not requiring explicit fault injection. Fault coverage is measured in terms of the fraction of candidate faulty instructions that are directly or indirectly detectable before.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168869},
 doi = {http://doi.acm.org/10.1145/1168919.1168869},
 acmid = {1168869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessor (CMP), redundant multithreading, simultaneous multithreading (SMT), slipstream processor, time redundancy, transient faults, value prediction},
} 

@article{Reddy:2006:UPP:1168917.1168869,
 author = {Reddy, Vimal K. and Rotenberg, Eric and Parthasarathy, Sailashri},
 title = {Understanding prediction-based partial redundant threading for low-overhead, high- coverage fault tolerance},
 abstract = {Redundant threading architectures duplicate all instructions to detect and possibly recover from transient faults. Several lighter weight Partial Redundant Threading (PRT) architectures have been proposed recently. (i) Opportunistic Fault Tolerance duplicates instructions only during periods of poor single-thread performance. (ii) ReStore does not explicitly duplicate instructions and instead exploits mispredictions among highly confident branch predictions as symptoms of faults. (iii) Slipstream creates a reduced alternate thread by replacing many instructions with highly confident predictions. We explore PRT as a possible direction for achieving the fault tolerance of full duplication with the performance of single-thread execution. Opportunistic and ReStore yield partial coverage since they are restricted to using only partial duplication or only confident predictions, respectively. Previous analysis of Slipstream fault tolerance was cursory and concluded that only duplicated instructions are covered. In this paper, we attempt to better understand Slipstream's fault tolerance, conjecturing that the mixture of partial duplication and confident predictions actually closely approximates the coverage of full duplication. A thorough dissection of prediction scenarios confirms that faults in nearly 100\% of instructions are detectable. Fewer than 0.1\% of faulty instructions are not detectable due to coincident faults and mispredictions. Next we show that the current recovery implementation fails to leverage excellent detection capability, since recovery sometimes initiates belatedly, after already retiring a detected faulty instruction. We propose and evaluate a suite of simple microarchitectural alterations to recovery and checking. Using the best alterations, Slipstream can recover from faults in 99\% of instructions, compared to only 78\% of instructions without alterations. Both results are much higher than predicted by past research, which claims coverage for only duplicated instructions, or 65\% of instructions. On an 8-issue SMT processor, Slipstream performs within 1.3\% of single-thread execution whereas full duplication slows performance by 14\%.A key byproduct of this paper is a novel analysis framework in which every dynamic instruction is considered to be hypothetically faulty, thus not requiring explicit fault injection. Fault coverage is measured in terms of the fraction of candidate faulty instructions that are directly or indirectly detectable before.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168869},
 doi = {http://doi.acm.org/10.1145/1168917.1168869},
 acmid = {1168869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessor (CMP), redundant multithreading, simultaneous multithreading (SMT), slipstream processor, time redundancy, transient faults, value prediction},
} 

@article{Reddy:2006:UPP:1168918.1168869,
 author = {Reddy, Vimal K. and Rotenberg, Eric and Parthasarathy, Sailashri},
 title = {Understanding prediction-based partial redundant threading for low-overhead, high- coverage fault tolerance},
 abstract = {Redundant threading architectures duplicate all instructions to detect and possibly recover from transient faults. Several lighter weight Partial Redundant Threading (PRT) architectures have been proposed recently. (i) Opportunistic Fault Tolerance duplicates instructions only during periods of poor single-thread performance. (ii) ReStore does not explicitly duplicate instructions and instead exploits mispredictions among highly confident branch predictions as symptoms of faults. (iii) Slipstream creates a reduced alternate thread by replacing many instructions with highly confident predictions. We explore PRT as a possible direction for achieving the fault tolerance of full duplication with the performance of single-thread execution. Opportunistic and ReStore yield partial coverage since they are restricted to using only partial duplication or only confident predictions, respectively. Previous analysis of Slipstream fault tolerance was cursory and concluded that only duplicated instructions are covered. In this paper, we attempt to better understand Slipstream's fault tolerance, conjecturing that the mixture of partial duplication and confident predictions actually closely approximates the coverage of full duplication. A thorough dissection of prediction scenarios confirms that faults in nearly 100\% of instructions are detectable. Fewer than 0.1\% of faulty instructions are not detectable due to coincident faults and mispredictions. Next we show that the current recovery implementation fails to leverage excellent detection capability, since recovery sometimes initiates belatedly, after already retiring a detected faulty instruction. We propose and evaluate a suite of simple microarchitectural alterations to recovery and checking. Using the best alterations, Slipstream can recover from faults in 99\% of instructions, compared to only 78\% of instructions without alterations. Both results are much higher than predicted by past research, which claims coverage for only duplicated instructions, or 65\% of instructions. On an 8-issue SMT processor, Slipstream performs within 1.3\% of single-thread execution whereas full duplication slows performance by 14\%.A key byproduct of this paper is a novel analysis framework in which every dynamic instruction is considered to be hypothetically faulty, thus not requiring explicit fault injection. Fault coverage is measured in terms of the fraction of candidate faulty instructions that are directly or indirectly detectable before.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168869},
 doi = {http://doi.acm.org/10.1145/1168918.1168869},
 acmid = {1168869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, chip multiprocessor (CMP), redundant multithreading, simultaneous multithreading (SMT), slipstream processor, time redundancy, transient faults, value prediction},
} 

@article{Parashar:2006:SSL:1168919.1168870,
 author = {Parashar, Angshuman and Sivasubramaniam, Anand and Gurumurthi, Sudhanva},
 title = {SlicK: slice-based locality exploitation for efficient redundant multithreading},
 abstract = {Transient faults are expected a be a major design consideration in future microprocessors. Recent proposals for transient fault detection in processor cores have revolved around the idea of redundant threading, which involves redundant execution of a program across multiple execution contexts. This paper presents a new approach to redundant threading by bringing together the concepts of slice-level execution and value and control-flow locality into a novel partial redundant threading mechanism called SlicK</i>.The purpose of redundant execution is to check the integrity of the outputs propagating out of the core (typically through stores). SlicK implements redundancy at the granularity of backward-slices of these output instructions and exploits value and control-flow locality to avoid redundantly executing slices that lead to predictable outputs, thereby avoiding redundant execution of a significant fraction of instructions while maintaining extremely low vulnerabilities for critical processor structures.We propose the microarchitecture of a backward-slice extractor called SliceEM that is able to identify backward slices without interrupting the instruction flow, and show how this extractor and a set of predictors can be integrated into a redundant threading mechanism to form SlicK. Detailed simulations with SPEC CPU2000 benchmarks show that SlicK can provide around 10.2\% performance improvement over a well known redundant threading mechanism, buying back over 50\% of the loss suffered due to redundant execution. SlicK can keep the Architectural Vulnerability Factors of processor structures to typically 0\%-2\%. More importantly, SlicK's slice-based mechanisms provide future opportunities for exploring interesting points in the performance-reliability design space based on market segment needs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {95--105},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168870},
 doi = {http://doi.acm.org/10.1145/1168919.1168870},
 acmid = {1168870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backward slice extraction, microarchitecture, redundant threading, transient faults},
} 

@article{Parashar:2006:SSL:1168918.1168870,
 author = {Parashar, Angshuman and Sivasubramaniam, Anand and Gurumurthi, Sudhanva},
 title = {SlicK: slice-based locality exploitation for efficient redundant multithreading},
 abstract = {Transient faults are expected a be a major design consideration in future microprocessors. Recent proposals for transient fault detection in processor cores have revolved around the idea of redundant threading, which involves redundant execution of a program across multiple execution contexts. This paper presents a new approach to redundant threading by bringing together the concepts of slice-level execution and value and control-flow locality into a novel partial redundant threading mechanism called SlicK</i>.The purpose of redundant execution is to check the integrity of the outputs propagating out of the core (typically through stores). SlicK implements redundancy at the granularity of backward-slices of these output instructions and exploits value and control-flow locality to avoid redundantly executing slices that lead to predictable outputs, thereby avoiding redundant execution of a significant fraction of instructions while maintaining extremely low vulnerabilities for critical processor structures.We propose the microarchitecture of a backward-slice extractor called SliceEM that is able to identify backward slices without interrupting the instruction flow, and show how this extractor and a set of predictors can be integrated into a redundant threading mechanism to form SlicK. Detailed simulations with SPEC CPU2000 benchmarks show that SlicK can provide around 10.2\% performance improvement over a well known redundant threading mechanism, buying back over 50\% of the loss suffered due to redundant execution. SlicK can keep the Architectural Vulnerability Factors of processor structures to typically 0\%-2\%. More importantly, SlicK's slice-based mechanisms provide future opportunities for exploring interesting points in the performance-reliability design space based on market segment needs.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {95--105},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168870},
 doi = {http://doi.acm.org/10.1145/1168918.1168870},
 acmid = {1168870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backward slice extraction, microarchitecture, redundant threading, transient faults},
} 

@inproceedings{Parashar:2006:SSL:1168857.1168870,
 author = {Parashar, Angshuman and Sivasubramaniam, Anand and Gurumurthi, Sudhanva},
 title = {SlicK: slice-based locality exploitation for efficient redundant multithreading},
 abstract = {Transient faults are expected a be a major design consideration in future microprocessors. Recent proposals for transient fault detection in processor cores have revolved around the idea of redundant threading, which involves redundant execution of a program across multiple execution contexts. This paper presents a new approach to redundant threading by bringing together the concepts of slice-level execution and value and control-flow locality into a novel partial redundant threading mechanism called SlicK</i>.The purpose of redundant execution is to check the integrity of the outputs propagating out of the core (typically through stores). SlicK implements redundancy at the granularity of backward-slices of these output instructions and exploits value and control-flow locality to avoid redundantly executing slices that lead to predictable outputs, thereby avoiding redundant execution of a significant fraction of instructions while maintaining extremely low vulnerabilities for critical processor structures.We propose the microarchitecture of a backward-slice extractor called SliceEM that is able to identify backward slices without interrupting the instruction flow, and show how this extractor and a set of predictors can be integrated into a redundant threading mechanism to form SlicK. Detailed simulations with SPEC CPU2000 benchmarks show that SlicK can provide around 10.2\% performance improvement over a well known redundant threading mechanism, buying back over 50\% of the loss suffered due to redundant execution. SlicK can keep the Architectural Vulnerability Factors of processor structures to typically 0\%-2\%. More importantly, SlicK's slice-based mechanisms provide future opportunities for exploring interesting points in the performance-reliability design space based on market segment needs.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {95--105},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168870},
 doi = {http://doi.acm.org/10.1145/1168857.1168870},
 acmid = {1168870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backward slice extraction, microarchitecture, redundant threading, transient faults},
} 

@article{Parashar:2006:SSL:1168917.1168870,
 author = {Parashar, Angshuman and Sivasubramaniam, Anand and Gurumurthi, Sudhanva},
 title = {SlicK: slice-based locality exploitation for efficient redundant multithreading},
 abstract = {Transient faults are expected a be a major design consideration in future microprocessors. Recent proposals for transient fault detection in processor cores have revolved around the idea of redundant threading, which involves redundant execution of a program across multiple execution contexts. This paper presents a new approach to redundant threading by bringing together the concepts of slice-level execution and value and control-flow locality into a novel partial redundant threading mechanism called SlicK</i>.The purpose of redundant execution is to check the integrity of the outputs propagating out of the core (typically through stores). SlicK implements redundancy at the granularity of backward-slices of these output instructions and exploits value and control-flow locality to avoid redundantly executing slices that lead to predictable outputs, thereby avoiding redundant execution of a significant fraction of instructions while maintaining extremely low vulnerabilities for critical processor structures.We propose the microarchitecture of a backward-slice extractor called SliceEM that is able to identify backward slices without interrupting the instruction flow, and show how this extractor and a set of predictors can be integrated into a redundant threading mechanism to form SlicK. Detailed simulations with SPEC CPU2000 benchmarks show that SlicK can provide around 10.2\% performance improvement over a well known redundant threading mechanism, buying back over 50\% of the loss suffered due to redundant execution. SlicK can keep the Architectural Vulnerability Factors of processor structures to typically 0\%-2\%. More importantly, SlicK's slice-based mechanisms provide future opportunities for exploring interesting points in the performance-reliability design space based on market segment needs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {95--105},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168870},
 doi = {http://doi.acm.org/10.1145/1168917.1168870},
 acmid = {1168870},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backward slice extraction, microarchitecture, redundant threading, transient faults},
} 

@article{Heath:2006:MFT:1168919.1168872,
 author = {Heath, Taliver and Centeno, Ana Paula and George, Pradeep and Ramos, Luiz and Jaluria, Yogesh and Bianchini, Ricardo},
 title = {Mercury and freon: temperature emulation and management for server systems},
 abstract = {Power densities have been increasing rapidly at all levels of server systems. To counter the high temperatures resulting from these densities, systems researchers have recently started work on softwarebased thermal management</i>. Unfortunately, research in this new area has been hindered by the limitations imposed by simulators and real measurements. In this paper, we introduce Mercury, a software suite that avoids these limitations by accurately emulating temperatures based on simple layout, hardware, and componentutilization data. Most importantly, Mercury runs the entire software stack natively, enables repeatable experiments, and allows the study of thermal emergencies without harming hardware reliability. We validate Mercury using real measurements and a widely used commercial simulator. We use Mercury to develop Freon, a system that manages thermal emergencies in a server cluster without unnecessary performance degradation. Mercury will soon become available from http://www.darklab.rutgers.edu.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {106--116},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168872},
 doi = {http://doi.acm.org/10.1145/1168919.1168872},
 acmid = {1168872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy conservation, server clusters, temperature modeling, thermal management},
} 

@inproceedings{Heath:2006:MFT:1168857.1168872,
 author = {Heath, Taliver and Centeno, Ana Paula and George, Pradeep and Ramos, Luiz and Jaluria, Yogesh and Bianchini, Ricardo},
 title = {Mercury and freon: temperature emulation and management for server systems},
 abstract = {Power densities have been increasing rapidly at all levels of server systems. To counter the high temperatures resulting from these densities, systems researchers have recently started work on softwarebased thermal management</i>. Unfortunately, research in this new area has been hindered by the limitations imposed by simulators and real measurements. In this paper, we introduce Mercury, a software suite that avoids these limitations by accurately emulating temperatures based on simple layout, hardware, and componentutilization data. Most importantly, Mercury runs the entire software stack natively, enables repeatable experiments, and allows the study of thermal emergencies without harming hardware reliability. We validate Mercury using real measurements and a widely used commercial simulator. We use Mercury to develop Freon, a system that manages thermal emergencies in a server cluster without unnecessary performance degradation. Mercury will soon become available from http://www.darklab.rutgers.edu.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {106--116},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168872},
 doi = {http://doi.acm.org/10.1145/1168857.1168872},
 acmid = {1168872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy conservation, server clusters, temperature modeling, thermal management},
} 

@article{Heath:2006:MFT:1168918.1168872,
 author = {Heath, Taliver and Centeno, Ana Paula and George, Pradeep and Ramos, Luiz and Jaluria, Yogesh and Bianchini, Ricardo},
 title = {Mercury and freon: temperature emulation and management for server systems},
 abstract = {Power densities have been increasing rapidly at all levels of server systems. To counter the high temperatures resulting from these densities, systems researchers have recently started work on softwarebased thermal management</i>. Unfortunately, research in this new area has been hindered by the limitations imposed by simulators and real measurements. In this paper, we introduce Mercury, a software suite that avoids these limitations by accurately emulating temperatures based on simple layout, hardware, and componentutilization data. Most importantly, Mercury runs the entire software stack natively, enables repeatable experiments, and allows the study of thermal emergencies without harming hardware reliability. We validate Mercury using real measurements and a widely used commercial simulator. We use Mercury to develop Freon, a system that manages thermal emergencies in a server cluster without unnecessary performance degradation. Mercury will soon become available from http://www.darklab.rutgers.edu.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {106--116},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168872},
 doi = {http://doi.acm.org/10.1145/1168918.1168872},
 acmid = {1168872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy conservation, server clusters, temperature modeling, thermal management},
} 

@article{Heath:2006:MFT:1168917.1168872,
 author = {Heath, Taliver and Centeno, Ana Paula and George, Pradeep and Ramos, Luiz and Jaluria, Yogesh and Bianchini, Ricardo},
 title = {Mercury and freon: temperature emulation and management for server systems},
 abstract = {Power densities have been increasing rapidly at all levels of server systems. To counter the high temperatures resulting from these densities, systems researchers have recently started work on softwarebased thermal management</i>. Unfortunately, research in this new area has been hindered by the limitations imposed by simulators and real measurements. In this paper, we introduce Mercury, a software suite that avoids these limitations by accurately emulating temperatures based on simple layout, hardware, and componentutilization data. Most importantly, Mercury runs the entire software stack natively, enables repeatable experiments, and allows the study of thermal emergencies without harming hardware reliability. We validate Mercury using real measurements and a widely used commercial simulator. We use Mercury to develop Freon, a system that manages thermal emergencies in a server cluster without unnecessary performance degradation. Mercury will soon become available from http://www.darklab.rutgers.edu.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {106--116},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168872},
 doi = {http://doi.acm.org/10.1145/1168917.1168872},
 acmid = {1168872},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy conservation, server clusters, temperature modeling, thermal management},
} 

@inproceedings{Kgil:2006:PUS:1168857.1168873,
 author = {Kgil, Taeho and D'Souza, Shaun and Saidi, Ali and Binkert, Nathan and Dreslinski, Ronald and Mudge, Trevor and Reinhardt, Steven and Flautner, Krisztian},
 title = {PicoServer: using 3D stacking technology to enable a compact energy efficient chip multiprocessor},
 abstract = {In this paper, we show how 3D stacking technology can be used to implement a simple, low-power, high-performance chip multiprocessor suitable for throughput processing. Our proposed architecture, PicoServer, employs 3D technology to bond one die containing several simple slow processing cores to multiple DRAM dies sufficient for a primary memory. The 3D technology also enables wide low-latency buses between processors and memory. These remove the need for an L2 cache allowing its area to be re-allocated to additional simple cores. The additional cores allow the clock frequency to be lowered without impairing throughput. Lower clock frequency in turn reduces power and means that thermal constraints, a concern with 3D stacking, are easily satisfied.The PicoServer architecture specifically targets Tier 1 server applications, which exhibit a high degree of thread level parallelism. An architecture targeted to efficient throughput is ideal for this application domain. We find for a similar logic die area, a 12 CPU system with 3D stacking and no L2 cache outperforms an 8 CPU system with a large on-chip L2 cache by about 14\% while consuming 55\% less power. In addition, we show that a PicoServer performs comparably to a Pentium 4-like class machine while consuming only about 1/10 of the power, even when conservative assumptions are made about the power consumption of the PicoServer.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168873},
 doi = {http://doi.acm.org/10.1145/1168857.1168873},
 acmid = {1168873},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D stacking technology, chip multiprocessor, full-system simulation, low power, tier 1 server, web/file/streaming server},
} 

@article{Kgil:2006:PUS:1168918.1168873,
 author = {Kgil, Taeho and D'Souza, Shaun and Saidi, Ali and Binkert, Nathan and Dreslinski, Ronald and Mudge, Trevor and Reinhardt, Steven and Flautner, Krisztian},
 title = {PicoServer: using 3D stacking technology to enable a compact energy efficient chip multiprocessor},
 abstract = {In this paper, we show how 3D stacking technology can be used to implement a simple, low-power, high-performance chip multiprocessor suitable for throughput processing. Our proposed architecture, PicoServer, employs 3D technology to bond one die containing several simple slow processing cores to multiple DRAM dies sufficient for a primary memory. The 3D technology also enables wide low-latency buses between processors and memory. These remove the need for an L2 cache allowing its area to be re-allocated to additional simple cores. The additional cores allow the clock frequency to be lowered without impairing throughput. Lower clock frequency in turn reduces power and means that thermal constraints, a concern with 3D stacking, are easily satisfied.The PicoServer architecture specifically targets Tier 1 server applications, which exhibit a high degree of thread level parallelism. An architecture targeted to efficient throughput is ideal for this application domain. We find for a similar logic die area, a 12 CPU system with 3D stacking and no L2 cache outperforms an 8 CPU system with a large on-chip L2 cache by about 14\% while consuming 55\% less power. In addition, we show that a PicoServer performs comparably to a Pentium 4-like class machine while consuming only about 1/10 of the power, even when conservative assumptions are made about the power consumption of the PicoServer.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168873},
 doi = {http://doi.acm.org/10.1145/1168918.1168873},
 acmid = {1168873},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D stacking technology, chip multiprocessor, full-system simulation, low power, tier 1 server, web/file/streaming server},
} 

@article{Kgil:2006:PUS:1168917.1168873,
 author = {Kgil, Taeho and D'Souza, Shaun and Saidi, Ali and Binkert, Nathan and Dreslinski, Ronald and Mudge, Trevor and Reinhardt, Steven and Flautner, Krisztian},
 title = {PicoServer: using 3D stacking technology to enable a compact energy efficient chip multiprocessor},
 abstract = {In this paper, we show how 3D stacking technology can be used to implement a simple, low-power, high-performance chip multiprocessor suitable for throughput processing. Our proposed architecture, PicoServer, employs 3D technology to bond one die containing several simple slow processing cores to multiple DRAM dies sufficient for a primary memory. The 3D technology also enables wide low-latency buses between processors and memory. These remove the need for an L2 cache allowing its area to be re-allocated to additional simple cores. The additional cores allow the clock frequency to be lowered without impairing throughput. Lower clock frequency in turn reduces power and means that thermal constraints, a concern with 3D stacking, are easily satisfied.The PicoServer architecture specifically targets Tier 1 server applications, which exhibit a high degree of thread level parallelism. An architecture targeted to efficient throughput is ideal for this application domain. We find for a similar logic die area, a 12 CPU system with 3D stacking and no L2 cache outperforms an 8 CPU system with a large on-chip L2 cache by about 14\% while consuming 55\% less power. In addition, we show that a PicoServer performs comparably to a Pentium 4-like class machine while consuming only about 1/10 of the power, even when conservative assumptions are made about the power consumption of the PicoServer.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168873},
 doi = {http://doi.acm.org/10.1145/1168917.1168873},
 acmid = {1168873},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D stacking technology, chip multiprocessor, full-system simulation, low power, tier 1 server, web/file/streaming server},
} 

@article{Kgil:2006:PUS:1168919.1168873,
 author = {Kgil, Taeho and D'Souza, Shaun and Saidi, Ali and Binkert, Nathan and Dreslinski, Ronald and Mudge, Trevor and Reinhardt, Steven and Flautner, Krisztian},
 title = {PicoServer: using 3D stacking technology to enable a compact energy efficient chip multiprocessor},
 abstract = {In this paper, we show how 3D stacking technology can be used to implement a simple, low-power, high-performance chip multiprocessor suitable for throughput processing. Our proposed architecture, PicoServer, employs 3D technology to bond one die containing several simple slow processing cores to multiple DRAM dies sufficient for a primary memory. The 3D technology also enables wide low-latency buses between processors and memory. These remove the need for an L2 cache allowing its area to be re-allocated to additional simple cores. The additional cores allow the clock frequency to be lowered without impairing throughput. Lower clock frequency in turn reduces power and means that thermal constraints, a concern with 3D stacking, are easily satisfied.The PicoServer architecture specifically targets Tier 1 server applications, which exhibit a high degree of thread level parallelism. An architecture targeted to efficient throughput is ideal for this application domain. We find for a similar logic die area, a 12 CPU system with 3D stacking and no L2 cache outperforms an 8 CPU system with a large on-chip L2 cache by about 14\% while consuming 55\% less power. In addition, we show that a PicoServer performs comparably to a Pentium 4-like class machine while consuming only about 1/10 of the power, even when conservative assumptions are made about the power consumption of the PicoServer.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168873},
 doi = {http://doi.acm.org/10.1145/1168919.1168873},
 acmid = {1168873},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D stacking technology, chip multiprocessor, full-system simulation, low power, tier 1 server, web/file/streaming server},
} 

@article{Coons:2006:SPS:1168918.1168875,
 author = {Coons, Katherine E. and Chen, Xia and Burger, Doug and McKinley, Kathryn S. and Kushwaha, Sundeep K.},
 title = {A spatial path scheduling algorithm for EDGE architectures},
 abstract = {Growing on-chip wire delays are motivating architectural features that expose on-chip communication to the compiler. EDGE architectures are one example of communication-exposed microarchitectures in which the compiler forms dataflow graphs that specify how the microarchitecture maps instructions onto a distributed execution substrate. This paper describes a compiler scheduling algorithm called spatial path scheduling</i> that factors in previously fixed locations - called anchor points - for each placement. This algorithm extends easily to different spatial topologies. We augment this basic algorithm with three heuristics: (1) local and global ALU and network link contention modeling, (2) global critical path estimates, and (3) dependence chain path reservation. We use simulated annealing to explore possible performance improvements and to motivate the augmented heuristics and their weighting functions. We show that the spatial path scheduling algorithm augmented with these three heuristics achieves a 21\% average performance improvement over the best prior algorithm and comes within an average of 5\% of the annealed performance for our benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168875},
 doi = {http://doi.acm.org/10.1145/1168918.1168875},
 acmid = {1168875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architecture, instruction scheduling, path scheduling, simulated annealing},
} 

@inproceedings{Coons:2006:SPS:1168857.1168875,
 author = {Coons, Katherine E. and Chen, Xia and Burger, Doug and McKinley, Kathryn S. and Kushwaha, Sundeep K.},
 title = {A spatial path scheduling algorithm for EDGE architectures},
 abstract = {Growing on-chip wire delays are motivating architectural features that expose on-chip communication to the compiler. EDGE architectures are one example of communication-exposed microarchitectures in which the compiler forms dataflow graphs that specify how the microarchitecture maps instructions onto a distributed execution substrate. This paper describes a compiler scheduling algorithm called spatial path scheduling</i> that factors in previously fixed locations - called anchor points - for each placement. This algorithm extends easily to different spatial topologies. We augment this basic algorithm with three heuristics: (1) local and global ALU and network link contention modeling, (2) global critical path estimates, and (3) dependence chain path reservation. We use simulated annealing to explore possible performance improvements and to motivate the augmented heuristics and their weighting functions. We show that the spatial path scheduling algorithm augmented with these three heuristics achieves a 21\% average performance improvement over the best prior algorithm and comes within an average of 5\% of the annealed performance for our benchmarks.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168875},
 doi = {http://doi.acm.org/10.1145/1168857.1168875},
 acmid = {1168875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architecture, instruction scheduling, path scheduling, simulated annealing},
} 

@article{Coons:2006:SPS:1168917.1168875,
 author = {Coons, Katherine E. and Chen, Xia and Burger, Doug and McKinley, Kathryn S. and Kushwaha, Sundeep K.},
 title = {A spatial path scheduling algorithm for EDGE architectures},
 abstract = {Growing on-chip wire delays are motivating architectural features that expose on-chip communication to the compiler. EDGE architectures are one example of communication-exposed microarchitectures in which the compiler forms dataflow graphs that specify how the microarchitecture maps instructions onto a distributed execution substrate. This paper describes a compiler scheduling algorithm called spatial path scheduling</i> that factors in previously fixed locations - called anchor points - for each placement. This algorithm extends easily to different spatial topologies. We augment this basic algorithm with three heuristics: (1) local and global ALU and network link contention modeling, (2) global critical path estimates, and (3) dependence chain path reservation. We use simulated annealing to explore possible performance improvements and to motivate the augmented heuristics and their weighting functions. We show that the spatial path scheduling algorithm augmented with these three heuristics achieves a 21\% average performance improvement over the best prior algorithm and comes within an average of 5\% of the annealed performance for our benchmarks.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168875},
 doi = {http://doi.acm.org/10.1145/1168917.1168875},
 acmid = {1168875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architecture, instruction scheduling, path scheduling, simulated annealing},
} 

@article{Coons:2006:SPS:1168919.1168875,
 author = {Coons, Katherine E. and Chen, Xia and Burger, Doug and McKinley, Kathryn S. and Kushwaha, Sundeep K.},
 title = {A spatial path scheduling algorithm for EDGE architectures},
 abstract = {Growing on-chip wire delays are motivating architectural features that expose on-chip communication to the compiler. EDGE architectures are one example of communication-exposed microarchitectures in which the compiler forms dataflow graphs that specify how the microarchitecture maps instructions onto a distributed execution substrate. This paper describes a compiler scheduling algorithm called spatial path scheduling</i> that factors in previously fixed locations - called anchor points - for each placement. This algorithm extends easily to different spatial topologies. We augment this basic algorithm with three heuristics: (1) local and global ALU and network link contention modeling, (2) global critical path estimates, and (3) dependence chain path reservation. We use simulated annealing to explore possible performance improvements and to motivate the augmented heuristics and their weighting functions. We show that the spatial path scheduling algorithm augmented with these three heuristics achieves a 21\% average performance improvement over the best prior algorithm and comes within an average of 5\% of the annealed performance for our benchmarks.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168875},
 doi = {http://doi.acm.org/10.1145/1168919.1168875},
 acmid = {1168875},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architecture, instruction scheduling, path scheduling, simulated annealing},
} 

@inproceedings{Mercaldi:2006:IST:1168857.1168876,
 author = {Mercaldi, Martha and Swanson, Steven and Petersen, Andrew and Putnam, Andrew and Schwerin, Andrew and Oskin, Mark and Eggers, Susan J.},
 title = {Instruction scheduling for a tiled dataflow architecture},
 abstract = {This paper explores hierarchical instruction scheduling for a tiled processor. Our results show that at the top level of the hierarchy, a simple profile-driven algorithm effectively minimizes operand latency. After this schedule has been partitioned into large sections, the bottom-level algorithm must more carefully analyze program structure when producing the final schedule.Our analysis reveals that at this bottom level, good scheduling depends upon carefully balancing instruction contention for processing elements and operand latency between producer and consumer instructions. We develop a parameterizable instruction scheduler that more effectively optimizes this trade-off. We use this scheduler to determine the contention-latency sweet spot that generates the best instruction schedule for each application. To avoid this application-specific tuning, we also determine the parameters that produce the best performance across all applications. The result is a contention-latency setting that generates instruction schedules for all applications in our workload that come within 17\% of the best schedule for each.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168876},
 doi = {http://doi.acm.org/10.1145/1168857.1168876},
 acmid = {1168876},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow, instruction scheduling, tiled architectures},
} 

@article{Mercaldi:2006:IST:1168919.1168876,
 author = {Mercaldi, Martha and Swanson, Steven and Petersen, Andrew and Putnam, Andrew and Schwerin, Andrew and Oskin, Mark and Eggers, Susan J.},
 title = {Instruction scheduling for a tiled dataflow architecture},
 abstract = {This paper explores hierarchical instruction scheduling for a tiled processor. Our results show that at the top level of the hierarchy, a simple profile-driven algorithm effectively minimizes operand latency. After this schedule has been partitioned into large sections, the bottom-level algorithm must more carefully analyze program structure when producing the final schedule.Our analysis reveals that at this bottom level, good scheduling depends upon carefully balancing instruction contention for processing elements and operand latency between producer and consumer instructions. We develop a parameterizable instruction scheduler that more effectively optimizes this trade-off. We use this scheduler to determine the contention-latency sweet spot that generates the best instruction schedule for each application. To avoid this application-specific tuning, we also determine the parameters that produce the best performance across all applications. The result is a contention-latency setting that generates instruction schedules for all applications in our workload that come within 17\% of the best schedule for each.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168876},
 doi = {http://doi.acm.org/10.1145/1168919.1168876},
 acmid = {1168876},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow, instruction scheduling, tiled architectures},
} 

@article{Mercaldi:2006:IST:1168918.1168876,
 author = {Mercaldi, Martha and Swanson, Steven and Petersen, Andrew and Putnam, Andrew and Schwerin, Andrew and Oskin, Mark and Eggers, Susan J.},
 title = {Instruction scheduling for a tiled dataflow architecture},
 abstract = {This paper explores hierarchical instruction scheduling for a tiled processor. Our results show that at the top level of the hierarchy, a simple profile-driven algorithm effectively minimizes operand latency. After this schedule has been partitioned into large sections, the bottom-level algorithm must more carefully analyze program structure when producing the final schedule.Our analysis reveals that at this bottom level, good scheduling depends upon carefully balancing instruction contention for processing elements and operand latency between producer and consumer instructions. We develop a parameterizable instruction scheduler that more effectively optimizes this trade-off. We use this scheduler to determine the contention-latency sweet spot that generates the best instruction schedule for each application. To avoid this application-specific tuning, we also determine the parameters that produce the best performance across all applications. The result is a contention-latency setting that generates instruction schedules for all applications in our workload that come within 17\% of the best schedule for each.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168876},
 doi = {http://doi.acm.org/10.1145/1168918.1168876},
 acmid = {1168876},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow, instruction scheduling, tiled architectures},
} 

@article{Mercaldi:2006:IST:1168917.1168876,
 author = {Mercaldi, Martha and Swanson, Steven and Petersen, Andrew and Putnam, Andrew and Schwerin, Andrew and Oskin, Mark and Eggers, Susan J.},
 title = {Instruction scheduling for a tiled dataflow architecture},
 abstract = {This paper explores hierarchical instruction scheduling for a tiled processor. Our results show that at the top level of the hierarchy, a simple profile-driven algorithm effectively minimizes operand latency. After this schedule has been partitioned into large sections, the bottom-level algorithm must more carefully analyze program structure when producing the final schedule.Our analysis reveals that at this bottom level, good scheduling depends upon carefully balancing instruction contention for processing elements and operand latency between producer and consumer instructions. We develop a parameterizable instruction scheduler that more effectively optimizes this trade-off. We use this scheduler to determine the contention-latency sweet spot that generates the best instruction schedule for each application. To avoid this application-specific tuning, we also determine the parameters that produce the best performance across all applications. The result is a contention-latency setting that generates instruction schedules for all applications in our workload that come within 17\% of the best schedule for each.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168876},
 doi = {http://doi.acm.org/10.1145/1168917.1168876},
 acmid = {1168876},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow, instruction scheduling, tiled architectures},
} 

@article{Gordon:2006:ECT:1168917.1168877,
 author = {Gordon, Michael I. and Thies, William and Amarasinghe, Saman},
 title = {Exploiting coarse-grained task, data, and pipeline parallelism in stream programs},
 abstract = {As multicore architectures enter the mainstream, there is a pressing demand for high-level programming models that can effectively map to them. Stream programming offers an attractive way to expose coarse-grained parallelism, as streaming applications (image, video, DSP, etc.) are naturally represented by independent filters that communicate over explicit data channels.In this paper, we demonstrate an end-to-end stream compiler that attains robust multicore performance in the face of varying application characteristics. As benchmarks exhibit different amounts of task, data, and pipeline parallelism, we exploit all types of parallelism in a unified manner in order to achieve this generality. Our compiler, which maps from the StreamIt language to the 16-core Raw architecture, attains a 11.2x mean speedup over a single-core baseline, and a 1.84x speedup over our previous work.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168877},
 doi = {http://doi.acm.org/10.1145/1168917.1168877},
 acmid = {1168877},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Raw, StreamIt, coarse-grained dataflow, multicore, software pipelining, streams},
} 

@article{Gordon:2006:ECT:1168919.1168877,
 author = {Gordon, Michael I. and Thies, William and Amarasinghe, Saman},
 title = {Exploiting coarse-grained task, data, and pipeline parallelism in stream programs},
 abstract = {As multicore architectures enter the mainstream, there is a pressing demand for high-level programming models that can effectively map to them. Stream programming offers an attractive way to expose coarse-grained parallelism, as streaming applications (image, video, DSP, etc.) are naturally represented by independent filters that communicate over explicit data channels.In this paper, we demonstrate an end-to-end stream compiler that attains robust multicore performance in the face of varying application characteristics. As benchmarks exhibit different amounts of task, data, and pipeline parallelism, we exploit all types of parallelism in a unified manner in order to achieve this generality. Our compiler, which maps from the StreamIt language to the 16-core Raw architecture, attains a 11.2x mean speedup over a single-core baseline, and a 1.84x speedup over our previous work.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168877},
 doi = {http://doi.acm.org/10.1145/1168919.1168877},
 acmid = {1168877},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Raw, StreamIt, coarse-grained dataflow, multicore, software pipelining, streams},
} 

@article{Gordon:2006:ECT:1168918.1168877,
 author = {Gordon, Michael I. and Thies, William and Amarasinghe, Saman},
 title = {Exploiting coarse-grained task, data, and pipeline parallelism in stream programs},
 abstract = {As multicore architectures enter the mainstream, there is a pressing demand for high-level programming models that can effectively map to them. Stream programming offers an attractive way to expose coarse-grained parallelism, as streaming applications (image, video, DSP, etc.) are naturally represented by independent filters that communicate over explicit data channels.In this paper, we demonstrate an end-to-end stream compiler that attains robust multicore performance in the face of varying application characteristics. As benchmarks exhibit different amounts of task, data, and pipeline parallelism, we exploit all types of parallelism in a unified manner in order to achieve this generality. Our compiler, which maps from the StreamIt language to the 16-core Raw architecture, attains a 11.2x mean speedup over a single-core baseline, and a 1.84x speedup over our previous work.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168877},
 doi = {http://doi.acm.org/10.1145/1168918.1168877},
 acmid = {1168877},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Raw, StreamIt, coarse-grained dataflow, multicore, software pipelining, streams},
} 

@inproceedings{Gordon:2006:ECT:1168857.1168877,
 author = {Gordon, Michael I. and Thies, William and Amarasinghe, Saman},
 title = {Exploiting coarse-grained task, data, and pipeline parallelism in stream programs},
 abstract = {As multicore architectures enter the mainstream, there is a pressing demand for high-level programming models that can effectively map to them. Stream programming offers an attractive way to expose coarse-grained parallelism, as streaming applications (image, video, DSP, etc.) are naturally represented by independent filters that communicate over explicit data channels.In this paper, we demonstrate an end-to-end stream compiler that attains robust multicore performance in the face of varying application characteristics. As benchmarks exhibit different amounts of task, data, and pipeline parallelism, we exploit all types of parallelism in a unified manner in order to achieve this generality. Our compiler, which maps from the StreamIt language to the 16-core Raw architecture, attains a 11.2x mean speedup over a single-core baseline, and a 1.84x speedup over our previous work.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168877},
 doi = {http://doi.acm.org/10.1145/1168857.1168877},
 acmid = {1168877},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Raw, StreamIt, coarse-grained dataflow, multicore, software pipelining, streams},
} 

@article{Mishra:2006:TES:1168919.1168878,
 author = {Mishra, Mahim and Callahan, Timothy J. and Chelcea, Tiberiu and Venkataramani, Girish and Goldstein, Seth C. and Budiu, Mihai},
 title = {Tartan: evaluating spatial computation for whole program execution},
 abstract = {Spatial Computing (SC) has been shown to be an energy-efficient model for implementing program kernels. In this paper we explore the feasibility of using SC for more than small kernels. To this end, we evaluate the performance and energy efficiency of entire applications on Tartan, a general-purpose architecture which integrates a reconfigurable fabric (RF) with a superscalar core. Our compiler automatically partitions and compiles an application into an instruction stream for the core and a configuration for the RF. We use a detailed simulator to capture both timing and energy numbers for all parts of the system.Our results indicate that a hierarchical RF architecture, designed around a scalable interconnect, is instrumental in harnessing the benefits of spatial computation. The interconnect uses static configuration and routing at the lower levels and a packet-switched, dynamically-routed network at the top level. Tartan is most energyefficient when almost all of the application is mapped to the RF, indicating the need for the RF to support most general-purpose programming constructs. Our initial investigation reveals that such a system can provide, on average, an order of magnitude improvement in energy-delay compared to an aggressive superscalar core on single-threaded workloads.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {163--174},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168878},
 doi = {http://doi.acm.org/10.1145/1168919.1168878},
 acmid = {1168878},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous circuits, dataflow machine, defect tolerance, low power, reconfigurable hardware, spatial computation},
} 

@article{Mishra:2006:TES:1168918.1168878,
 author = {Mishra, Mahim and Callahan, Timothy J. and Chelcea, Tiberiu and Venkataramani, Girish and Goldstein, Seth C. and Budiu, Mihai},
 title = {Tartan: evaluating spatial computation for whole program execution},
 abstract = {Spatial Computing (SC) has been shown to be an energy-efficient model for implementing program kernels. In this paper we explore the feasibility of using SC for more than small kernels. To this end, we evaluate the performance and energy efficiency of entire applications on Tartan, a general-purpose architecture which integrates a reconfigurable fabric (RF) with a superscalar core. Our compiler automatically partitions and compiles an application into an instruction stream for the core and a configuration for the RF. We use a detailed simulator to capture both timing and energy numbers for all parts of the system.Our results indicate that a hierarchical RF architecture, designed around a scalable interconnect, is instrumental in harnessing the benefits of spatial computation. The interconnect uses static configuration and routing at the lower levels and a packet-switched, dynamically-routed network at the top level. Tartan is most energyefficient when almost all of the application is mapped to the RF, indicating the need for the RF to support most general-purpose programming constructs. Our initial investigation reveals that such a system can provide, on average, an order of magnitude improvement in energy-delay compared to an aggressive superscalar core on single-threaded workloads.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {163--174},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168878},
 doi = {http://doi.acm.org/10.1145/1168918.1168878},
 acmid = {1168878},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous circuits, dataflow machine, defect tolerance, low power, reconfigurable hardware, spatial computation},
} 

@article{Mishra:2006:TES:1168917.1168878,
 author = {Mishra, Mahim and Callahan, Timothy J. and Chelcea, Tiberiu and Venkataramani, Girish and Goldstein, Seth C. and Budiu, Mihai},
 title = {Tartan: evaluating spatial computation for whole program execution},
 abstract = {Spatial Computing (SC) has been shown to be an energy-efficient model for implementing program kernels. In this paper we explore the feasibility of using SC for more than small kernels. To this end, we evaluate the performance and energy efficiency of entire applications on Tartan, a general-purpose architecture which integrates a reconfigurable fabric (RF) with a superscalar core. Our compiler automatically partitions and compiles an application into an instruction stream for the core and a configuration for the RF. We use a detailed simulator to capture both timing and energy numbers for all parts of the system.Our results indicate that a hierarchical RF architecture, designed around a scalable interconnect, is instrumental in harnessing the benefits of spatial computation. The interconnect uses static configuration and routing at the lower levels and a packet-switched, dynamically-routed network at the top level. Tartan is most energyefficient when almost all of the application is mapped to the RF, indicating the need for the RF to support most general-purpose programming constructs. Our initial investigation reveals that such a system can provide, on average, an order of magnitude improvement in energy-delay compared to an aggressive superscalar core on single-threaded workloads.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {163--174},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168878},
 doi = {http://doi.acm.org/10.1145/1168917.1168878},
 acmid = {1168878},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous circuits, dataflow machine, defect tolerance, low power, reconfigurable hardware, spatial computation},
} 

@inproceedings{Mishra:2006:TES:1168857.1168878,
 author = {Mishra, Mahim and Callahan, Timothy J. and Chelcea, Tiberiu and Venkataramani, Girish and Goldstein, Seth C. and Budiu, Mihai},
 title = {Tartan: evaluating spatial computation for whole program execution},
 abstract = {Spatial Computing (SC) has been shown to be an energy-efficient model for implementing program kernels. In this paper we explore the feasibility of using SC for more than small kernels. To this end, we evaluate the performance and energy efficiency of entire applications on Tartan, a general-purpose architecture which integrates a reconfigurable fabric (RF) with a superscalar core. Our compiler automatically partitions and compiles an application into an instruction stream for the core and a configuration for the RF. We use a detailed simulator to capture both timing and energy numbers for all parts of the system.Our results indicate that a hierarchical RF architecture, designed around a scalable interconnect, is instrumental in harnessing the benefits of spatial computation. The interconnect uses static configuration and routing at the lower levels and a packet-switched, dynamically-routed network at the top level. Tartan is most energyefficient when almost all of the application is mapped to the RF, indicating the need for the RF to support most general-purpose programming constructs. Our initial investigation reveals that such a system can provide, on average, an order of magnitude improvement in energy-delay compared to an aggressive superscalar core on single-threaded workloads.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {163--174},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168878},
 doi = {http://doi.acm.org/10.1145/1168857.1168878},
 acmid = {1168878},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous circuits, dataflow machine, defect tolerance, low power, reconfigurable hardware, spatial computation},
} 

@article{Eyerman:2006:PCA:1168919.1168880,
 author = {Eyerman, Stijn and Eeckhout, Lieven and Karkhanis, Tejas and Smith, James E.},
 title = {A performance counter architecture for computing accurate CPI components},
 abstract = {A common way of representing processor performance is to use Cycles per Instruction (CPI) `stacks' which break performance into a baseline CPI plus a number of individual miss event CPI components. CPI stacks can be very helpful in gaining insight into the behavior of an application on a given microprocessor; consequently, they are widely used by software application developers and computer architects. However, computing CPI stacks on superscalar out-of-order processors is challenging because of various overlaps among execution and miss events (cache misses, TLB misses, and branch mispredictions).This paper shows that meaningful and accurate CPI stacks can be computed for superscalar out-of-order processors. Using interval analysis, a novel method for analyzing out-of-order processor performance, we gain understanding into the performance impact of the various miss events. Based on this understanding, we propose a novel way of architecting hardware performance counters for building accurate CPI stacks. The additional hardware for implementing these counters is limited and comparable to existing hardware performance counter architectures while being significantly more accurate than previous approaches.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168880},
 doi = {http://doi.acm.org/10.1145/1168919.1168880},
 acmid = {1168880},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware performance counter architecture, superscalar processor performance modeling},
} 

@inproceedings{Eyerman:2006:PCA:1168857.1168880,
 author = {Eyerman, Stijn and Eeckhout, Lieven and Karkhanis, Tejas and Smith, James E.},
 title = {A performance counter architecture for computing accurate CPI components},
 abstract = {A common way of representing processor performance is to use Cycles per Instruction (CPI) `stacks' which break performance into a baseline CPI plus a number of individual miss event CPI components. CPI stacks can be very helpful in gaining insight into the behavior of an application on a given microprocessor; consequently, they are widely used by software application developers and computer architects. However, computing CPI stacks on superscalar out-of-order processors is challenging because of various overlaps among execution and miss events (cache misses, TLB misses, and branch mispredictions).This paper shows that meaningful and accurate CPI stacks can be computed for superscalar out-of-order processors. Using interval analysis, a novel method for analyzing out-of-order processor performance, we gain understanding into the performance impact of the various miss events. Based on this understanding, we propose a novel way of architecting hardware performance counters for building accurate CPI stacks. The additional hardware for implementing these counters is limited and comparable to existing hardware performance counter architectures while being significantly more accurate than previous approaches.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168880},
 doi = {http://doi.acm.org/10.1145/1168857.1168880},
 acmid = {1168880},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware performance counter architecture, superscalar processor performance modeling},
} 

@article{Eyerman:2006:PCA:1168917.1168880,
 author = {Eyerman, Stijn and Eeckhout, Lieven and Karkhanis, Tejas and Smith, James E.},
 title = {A performance counter architecture for computing accurate CPI components},
 abstract = {A common way of representing processor performance is to use Cycles per Instruction (CPI) `stacks' which break performance into a baseline CPI plus a number of individual miss event CPI components. CPI stacks can be very helpful in gaining insight into the behavior of an application on a given microprocessor; consequently, they are widely used by software application developers and computer architects. However, computing CPI stacks on superscalar out-of-order processors is challenging because of various overlaps among execution and miss events (cache misses, TLB misses, and branch mispredictions).This paper shows that meaningful and accurate CPI stacks can be computed for superscalar out-of-order processors. Using interval analysis, a novel method for analyzing out-of-order processor performance, we gain understanding into the performance impact of the various miss events. Based on this understanding, we propose a novel way of architecting hardware performance counters for building accurate CPI stacks. The additional hardware for implementing these counters is limited and comparable to existing hardware performance counter architectures while being significantly more accurate than previous approaches.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168880},
 doi = {http://doi.acm.org/10.1145/1168917.1168880},
 acmid = {1168880},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware performance counter architecture, superscalar processor performance modeling},
} 

@article{Eyerman:2006:PCA:1168918.1168880,
 author = {Eyerman, Stijn and Eeckhout, Lieven and Karkhanis, Tejas and Smith, James E.},
 title = {A performance counter architecture for computing accurate CPI components},
 abstract = {A common way of representing processor performance is to use Cycles per Instruction (CPI) `stacks' which break performance into a baseline CPI plus a number of individual miss event CPI components. CPI stacks can be very helpful in gaining insight into the behavior of an application on a given microprocessor; consequently, they are widely used by software application developers and computer architects. However, computing CPI stacks on superscalar out-of-order processors is challenging because of various overlaps among execution and miss events (cache misses, TLB misses, and branch mispredictions).This paper shows that meaningful and accurate CPI stacks can be computed for superscalar out-of-order processors. Using interval analysis, a novel method for analyzing out-of-order processor performance, we gain understanding into the performance impact of the various miss events. Based on this understanding, we propose a novel way of architecting hardware performance counters for building accurate CPI stacks. The additional hardware for implementing these counters is limited and comparable to existing hardware performance counter architectures while being significantly more accurate than previous approaches.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168880},
 doi = {http://doi.acm.org/10.1145/1168918.1168880},
 acmid = {1168880},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware performance counter architecture, superscalar processor performance modeling},
} 

@article{Lee:2006:AER:1168917.1168881,
 author = {Lee, Benjamin C. and Brooks, David M.},
 title = {Accurate and efficient regression modeling for microarchitectural performance and power prediction},
 abstract = {We propose regression modeling as an efficient approach for accurately predicting performance and power for various applications executing on any microprocessor configuration in a large microarchitectural design space. This paper addresses fundamental challenges in microarchitectural simulation cost by reducing the number of required simulations and using simulated results more effectively via statistical modeling and inference.Specifically, we derive and validate regression models for performance and power. Such models enable computationally efficient statistical inference, requiring the simulation of only 1 in 5 million points of a joint microarchitecture-application design space while achieving median error rates as low as 4.1 percent for performance and 4.3 percent for power. Although both models achieve similar accuracy, the sources of accuracy are strikingly different. We present optimizations for a baseline regression model to obtain (1) application-specific models to maximize accuracy in performance prediction and (2) regional power models leveraging only the most relevant samples from the microarchitectural design space to maximize accuracy in power prediction. Assessing sensitivity to the number of samples simulated for model formulation, we find fewer than 4,000 samples from a design space of approximately 22 billion points are sufficient. Collectively, our results suggest significant potential in accurate and efficient statistical inference for microarchitectural design space exploration via regression models.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168881},
 doi = {http://doi.acm.org/10.1145/1168917.1168881},
 acmid = {1168881},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, microarchitecture, regression, simulation, statistics},
} 

@article{Lee:2006:AER:1168918.1168881,
 author = {Lee, Benjamin C. and Brooks, David M.},
 title = {Accurate and efficient regression modeling for microarchitectural performance and power prediction},
 abstract = {We propose regression modeling as an efficient approach for accurately predicting performance and power for various applications executing on any microprocessor configuration in a large microarchitectural design space. This paper addresses fundamental challenges in microarchitectural simulation cost by reducing the number of required simulations and using simulated results more effectively via statistical modeling and inference.Specifically, we derive and validate regression models for performance and power. Such models enable computationally efficient statistical inference, requiring the simulation of only 1 in 5 million points of a joint microarchitecture-application design space while achieving median error rates as low as 4.1 percent for performance and 4.3 percent for power. Although both models achieve similar accuracy, the sources of accuracy are strikingly different. We present optimizations for a baseline regression model to obtain (1) application-specific models to maximize accuracy in performance prediction and (2) regional power models leveraging only the most relevant samples from the microarchitectural design space to maximize accuracy in power prediction. Assessing sensitivity to the number of samples simulated for model formulation, we find fewer than 4,000 samples from a design space of approximately 22 billion points are sufficient. Collectively, our results suggest significant potential in accurate and efficient statistical inference for microarchitectural design space exploration via regression models.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168881},
 doi = {http://doi.acm.org/10.1145/1168918.1168881},
 acmid = {1168881},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, microarchitecture, regression, simulation, statistics},
} 

@article{Lee:2006:AER:1168919.1168881,
 author = {Lee, Benjamin C. and Brooks, David M.},
 title = {Accurate and efficient regression modeling for microarchitectural performance and power prediction},
 abstract = {We propose regression modeling as an efficient approach for accurately predicting performance and power for various applications executing on any microprocessor configuration in a large microarchitectural design space. This paper addresses fundamental challenges in microarchitectural simulation cost by reducing the number of required simulations and using simulated results more effectively via statistical modeling and inference.Specifically, we derive and validate regression models for performance and power. Such models enable computationally efficient statistical inference, requiring the simulation of only 1 in 5 million points of a joint microarchitecture-application design space while achieving median error rates as low as 4.1 percent for performance and 4.3 percent for power. Although both models achieve similar accuracy, the sources of accuracy are strikingly different. We present optimizations for a baseline regression model to obtain (1) application-specific models to maximize accuracy in performance prediction and (2) regional power models leveraging only the most relevant samples from the microarchitectural design space to maximize accuracy in power prediction. Assessing sensitivity to the number of samples simulated for model formulation, we find fewer than 4,000 samples from a design space of approximately 22 billion points are sufficient. Collectively, our results suggest significant potential in accurate and efficient statistical inference for microarchitectural design space exploration via regression models.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168881},
 doi = {http://doi.acm.org/10.1145/1168919.1168881},
 acmid = {1168881},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, microarchitecture, regression, simulation, statistics},
} 

@inproceedings{Lee:2006:AER:1168857.1168881,
 author = {Lee, Benjamin C. and Brooks, David M.},
 title = {Accurate and efficient regression modeling for microarchitectural performance and power prediction},
 abstract = {We propose regression modeling as an efficient approach for accurately predicting performance and power for various applications executing on any microprocessor configuration in a large microarchitectural design space. This paper addresses fundamental challenges in microarchitectural simulation cost by reducing the number of required simulations and using simulated results more effectively via statistical modeling and inference.Specifically, we derive and validate regression models for performance and power. Such models enable computationally efficient statistical inference, requiring the simulation of only 1 in 5 million points of a joint microarchitecture-application design space while achieving median error rates as low as 4.1 percent for performance and 4.3 percent for power. Although both models achieve similar accuracy, the sources of accuracy are strikingly different. We present optimizations for a baseline regression model to obtain (1) application-specific models to maximize accuracy in performance prediction and (2) regional power models leveraging only the most relevant samples from the microarchitectural design space to maximize accuracy in power prediction. Assessing sensitivity to the number of samples simulated for model formulation, we find fewer than 4,000 samples from a design space of approximately 22 billion points are sufficient. Collectively, our results suggest significant potential in accurate and efficient statistical inference for microarchitectural design space exploration via regression models.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168881},
 doi = {http://doi.acm.org/10.1145/1168857.1168881},
 acmid = {1168881},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, microarchitecture, regression, simulation, statistics},
} 

@inproceedings{Ipek:2006:EEA:1168857.1168882,
 author = {\"{I}pek, Engin and McKee, Sally A. and Caruana, Rich and de Supinski, Bronis R. and Schulz, Martin},
 title = {Efficiently exploring architectural design spaces via predictive modeling},
 abstract = {Architects use cycle-by-cycle simulation to evaluate design choices and understand tradeoffs and interactions among design parameters. Efficiently exploring exponential-size design spaces with many interacting parameters remains an open problem: the sheer number of experiments renders detailed simulation intractable. We attack this problem via an automated approach that builds accurate, confident predictive design-space models. We simulate sampled points, using the results to teach our models the function describing relationships among design parameters. The models produce highly accurate performance estimates for other points in the space, can be queried to predict performance impacts of architectural changes, and are very fast compared to simulation, enabling efficient discovery of tradeoffs among parameters in different regions. We validate our approach via sensitivity studies on memory hierarchy and CPU design spaces: our models generally predict IPC with only 1-2\% error and reduce required simulation by two orders of magnitude. We also show the efficacy of our technique for exploring chip multiprocessor (CMP) design spaces: when trained on a 1\% sample drawn from a CMP design space with 250K points and up to 55x performance swings among different system configurations, our models predict performance with only 4-5\% error on average. Our approach combines with techniques to reduce time per simulation, achieving net time savings of three-four orders of magnitude.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168882},
 doi = {http://doi.acm.org/10.1145/1168857.1168882},
 acmid = {1168882},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial neural networks, design space exploration, performance prediction, sensitivity studies},
} 

@article{Ipek:2006:EEA:1168917.1168882,
 author = {\"{I}pek, Engin and McKee, Sally A. and Caruana, Rich and de Supinski, Bronis R. and Schulz, Martin},
 title = {Efficiently exploring architectural design spaces via predictive modeling},
 abstract = {Architects use cycle-by-cycle simulation to evaluate design choices and understand tradeoffs and interactions among design parameters. Efficiently exploring exponential-size design spaces with many interacting parameters remains an open problem: the sheer number of experiments renders detailed simulation intractable. We attack this problem via an automated approach that builds accurate, confident predictive design-space models. We simulate sampled points, using the results to teach our models the function describing relationships among design parameters. The models produce highly accurate performance estimates for other points in the space, can be queried to predict performance impacts of architectural changes, and are very fast compared to simulation, enabling efficient discovery of tradeoffs among parameters in different regions. We validate our approach via sensitivity studies on memory hierarchy and CPU design spaces: our models generally predict IPC with only 1-2\% error and reduce required simulation by two orders of magnitude. We also show the efficacy of our technique for exploring chip multiprocessor (CMP) design spaces: when trained on a 1\% sample drawn from a CMP design space with 250K points and up to 55x performance swings among different system configurations, our models predict performance with only 4-5\% error on average. Our approach combines with techniques to reduce time per simulation, achieving net time savings of three-four orders of magnitude.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168882},
 doi = {http://doi.acm.org/10.1145/1168917.1168882},
 acmid = {1168882},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial neural networks, design space exploration, performance prediction, sensitivity studies},
} 

@article{Ipek:2006:EEA:1168918.1168882,
 author = {\"{I}pek, Engin and McKee, Sally A. and Caruana, Rich and de Supinski, Bronis R. and Schulz, Martin},
 title = {Efficiently exploring architectural design spaces via predictive modeling},
 abstract = {Architects use cycle-by-cycle simulation to evaluate design choices and understand tradeoffs and interactions among design parameters. Efficiently exploring exponential-size design spaces with many interacting parameters remains an open problem: the sheer number of experiments renders detailed simulation intractable. We attack this problem via an automated approach that builds accurate, confident predictive design-space models. We simulate sampled points, using the results to teach our models the function describing relationships among design parameters. The models produce highly accurate performance estimates for other points in the space, can be queried to predict performance impacts of architectural changes, and are very fast compared to simulation, enabling efficient discovery of tradeoffs among parameters in different regions. We validate our approach via sensitivity studies on memory hierarchy and CPU design spaces: our models generally predict IPC with only 1-2\% error and reduce required simulation by two orders of magnitude. We also show the efficacy of our technique for exploring chip multiprocessor (CMP) design spaces: when trained on a 1\% sample drawn from a CMP design space with 250K points and up to 55x performance swings among different system configurations, our models predict performance with only 4-5\% error on average. Our approach combines with techniques to reduce time per simulation, achieving net time savings of three-four orders of magnitude.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168882},
 doi = {http://doi.acm.org/10.1145/1168918.1168882},
 acmid = {1168882},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial neural networks, design space exploration, performance prediction, sensitivity studies},
} 

@article{Ipek:2006:EEA:1168919.1168882,
 author = {\"{I}pek, Engin and McKee, Sally A. and Caruana, Rich and de Supinski, Bronis R. and Schulz, Martin},
 title = {Efficiently exploring architectural design spaces via predictive modeling},
 abstract = {Architects use cycle-by-cycle simulation to evaluate design choices and understand tradeoffs and interactions among design parameters. Efficiently exploring exponential-size design spaces with many interacting parameters remains an open problem: the sheer number of experiments renders detailed simulation intractable. We attack this problem via an automated approach that builds accurate, confident predictive design-space models. We simulate sampled points, using the results to teach our models the function describing relationships among design parameters. The models produce highly accurate performance estimates for other points in the space, can be queried to predict performance impacts of architectural changes, and are very fast compared to simulation, enabling efficient discovery of tradeoffs among parameters in different regions. We validate our approach via sensitivity studies on memory hierarchy and CPU design spaces: our models generally predict IPC with only 1-2\% error and reduce required simulation by two orders of magnitude. We also show the efficacy of our technique for exploring chip multiprocessor (CMP) design spaces: when trained on a 1\% sample drawn from a CMP design space with 250K points and up to 55x performance swings among different system configurations, our models predict performance with only 4-5\% error on average. Our approach combines with techniques to reduce time per simulation, achieving net time savings of three-four orders of magnitude.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168882},
 doi = {http://doi.acm.org/10.1145/1168919.1168882},
 acmid = {1168882},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {artificial neural networks, design space exploration, performance prediction, sensitivity studies},
} 

@article{Kharbutli:2006:CEP:1168919.1168884,
 author = {Kharbutli, Mazen and Jiang, Xiaowei and Solihin, Yan and Venkataramani, Guru and Prvulovic, Milos},
 title = {Comprehensively and efficiently protecting the heap},
 abstract = {The goal of this paper is to propose a scheme that provides comprehensive security protection for the heap. Heap vulnerabilities are increasingly being exploited for attacks on computer programs. In most implementations, the heap management library keeps the heap meta-data (heap structure information) and the application's heap data in an interleaved fashion and does not protect them against each other. Such implementations are inherently unsafe: vulnerabilities in the application can cause the heap library to perform unintended actions to achieve control-flow and non-control attacks.Unfortunately, current heap protection techniques are limited in that they use too many assumptions on how the attacks will be performed, require new hardware support, or require too many changes to the software developers' toolchain. We propose Heap Server, a new solution that does not have such drawbacks. Through existing virtual memory and inter-process protection mechanisms, Heap Server prevents the heap meta-data from being illegally overwritten, and heap data from being meaningfully overwritten. We show that through aggressive optimizations and parallelism, Heap Server protects the heap with nearly-negligible performance overheads even on heap-intensive applications. We also verify the protection against several real-world exploits and attack kernels.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168884},
 doi = {http://doi.acm.org/10.1145/1168919.1168884},
 acmid = {1168884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer security, heap attacks, heap security, heap server},
} 

@inproceedings{Kharbutli:2006:CEP:1168857.1168884,
 author = {Kharbutli, Mazen and Jiang, Xiaowei and Solihin, Yan and Venkataramani, Guru and Prvulovic, Milos},
 title = {Comprehensively and efficiently protecting the heap},
 abstract = {The goal of this paper is to propose a scheme that provides comprehensive security protection for the heap. Heap vulnerabilities are increasingly being exploited for attacks on computer programs. In most implementations, the heap management library keeps the heap meta-data (heap structure information) and the application's heap data in an interleaved fashion and does not protect them against each other. Such implementations are inherently unsafe: vulnerabilities in the application can cause the heap library to perform unintended actions to achieve control-flow and non-control attacks.Unfortunately, current heap protection techniques are limited in that they use too many assumptions on how the attacks will be performed, require new hardware support, or require too many changes to the software developers' toolchain. We propose Heap Server, a new solution that does not have such drawbacks. Through existing virtual memory and inter-process protection mechanisms, Heap Server prevents the heap meta-data from being illegally overwritten, and heap data from being meaningfully overwritten. We show that through aggressive optimizations and parallelism, Heap Server protects the heap with nearly-negligible performance overheads even on heap-intensive applications. We also verify the protection against several real-world exploits and attack kernels.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168884},
 doi = {http://doi.acm.org/10.1145/1168857.1168884},
 acmid = {1168884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer security, heap attacks, heap security, heap server},
} 

@article{Kharbutli:2006:CEP:1168918.1168884,
 author = {Kharbutli, Mazen and Jiang, Xiaowei and Solihin, Yan and Venkataramani, Guru and Prvulovic, Milos},
 title = {Comprehensively and efficiently protecting the heap},
 abstract = {The goal of this paper is to propose a scheme that provides comprehensive security protection for the heap. Heap vulnerabilities are increasingly being exploited for attacks on computer programs. In most implementations, the heap management library keeps the heap meta-data (heap structure information) and the application's heap data in an interleaved fashion and does not protect them against each other. Such implementations are inherently unsafe: vulnerabilities in the application can cause the heap library to perform unintended actions to achieve control-flow and non-control attacks.Unfortunately, current heap protection techniques are limited in that they use too many assumptions on how the attacks will be performed, require new hardware support, or require too many changes to the software developers' toolchain. We propose Heap Server, a new solution that does not have such drawbacks. Through existing virtual memory and inter-process protection mechanisms, Heap Server prevents the heap meta-data from being illegally overwritten, and heap data from being meaningfully overwritten. We show that through aggressive optimizations and parallelism, Heap Server protects the heap with nearly-negligible performance overheads even on heap-intensive applications. We also verify the protection against several real-world exploits and attack kernels.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168884},
 doi = {http://doi.acm.org/10.1145/1168918.1168884},
 acmid = {1168884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer security, heap attacks, heap security, heap server},
} 

@article{Kharbutli:2006:CEP:1168917.1168884,
 author = {Kharbutli, Mazen and Jiang, Xiaowei and Solihin, Yan and Venkataramani, Guru and Prvulovic, Milos},
 title = {Comprehensively and efficiently protecting the heap},
 abstract = {The goal of this paper is to propose a scheme that provides comprehensive security protection for the heap. Heap vulnerabilities are increasingly being exploited for attacks on computer programs. In most implementations, the heap management library keeps the heap meta-data (heap structure information) and the application's heap data in an interleaved fashion and does not protect them against each other. Such implementations are inherently unsafe: vulnerabilities in the application can cause the heap library to perform unintended actions to achieve control-flow and non-control attacks.Unfortunately, current heap protection techniques are limited in that they use too many assumptions on how the attacks will be performed, require new hardware support, or require too many changes to the software developers' toolchain. We propose Heap Server, a new solution that does not have such drawbacks. Through existing virtual memory and inter-process protection mechanisms, Heap Server prevents the heap meta-data from being illegally overwritten, and heap data from being meaningfully overwritten. We show that through aggressive optimizations and parallelism, Heap Server protects the heap with nearly-negligible performance overheads even on heap-intensive applications. We also verify the protection against several real-world exploits and attack kernels.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168884},
 doi = {http://doi.acm.org/10.1145/1168917.1168884},
 acmid = {1168884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer security, heap attacks, heap security, heap server},
} 

@article{Chilimbi:2006:HIH:1168917.1168885,
 author = {Chilimbi, Trishul M. and Ganapathy, Vinod},
 title = {HeapMD: identifying heap-based bugs using anomaly detection},
 abstract = {We present the design, implementation, and evaluation of HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly detection. HeapMD is based upon the observation that, in spite of the evolving nature of the heap, several of its properties remain stable. HeapMD uses this observation in a novel way: periodically, during the execution of the program, it computes a suite of metrics which are sensitive to the state of the heap. These metrics track heap behavior, and the stability of the heap reflects quantitatively in the values of these metrics. The "normal" ranges of stable metrics, obtained by running a program on multiple inputs, are then treated as indicators of correct behaviour, and are used in conjunction with an anomaly detector to find heap-based bugs. Using HeapMD, we were able to find 40 heap-based bugs, 31 of them previously unknown, in 5 large, commercial applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168885},
 doi = {http://doi.acm.org/10.1145/1168917.1168885},
 acmid = {1168885},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, bugs, debugging, heap, metrics},
} 

@article{Chilimbi:2006:HIH:1168918.1168885,
 author = {Chilimbi, Trishul M. and Ganapathy, Vinod},
 title = {HeapMD: identifying heap-based bugs using anomaly detection},
 abstract = {We present the design, implementation, and evaluation of HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly detection. HeapMD is based upon the observation that, in spite of the evolving nature of the heap, several of its properties remain stable. HeapMD uses this observation in a novel way: periodically, during the execution of the program, it computes a suite of metrics which are sensitive to the state of the heap. These metrics track heap behavior, and the stability of the heap reflects quantitatively in the values of these metrics. The "normal" ranges of stable metrics, obtained by running a program on multiple inputs, are then treated as indicators of correct behaviour, and are used in conjunction with an anomaly detector to find heap-based bugs. Using HeapMD, we were able to find 40 heap-based bugs, 31 of them previously unknown, in 5 large, commercial applications.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168885},
 doi = {http://doi.acm.org/10.1145/1168918.1168885},
 acmid = {1168885},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, bugs, debugging, heap, metrics},
} 

@inproceedings{Chilimbi:2006:HIH:1168857.1168885,
 author = {Chilimbi, Trishul M. and Ganapathy, Vinod},
 title = {HeapMD: identifying heap-based bugs using anomaly detection},
 abstract = {We present the design, implementation, and evaluation of HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly detection. HeapMD is based upon the observation that, in spite of the evolving nature of the heap, several of its properties remain stable. HeapMD uses this observation in a novel way: periodically, during the execution of the program, it computes a suite of metrics which are sensitive to the state of the heap. These metrics track heap behavior, and the stability of the heap reflects quantitatively in the values of these metrics. The "normal" ranges of stable metrics, obtained by running a program on multiple inputs, are then treated as indicators of correct behaviour, and are used in conjunction with an anomaly detector to find heap-based bugs. Using HeapMD, we were able to find 40 heap-based bugs, 31 of them previously unknown, in 5 large, commercial applications.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168885},
 doi = {http://doi.acm.org/10.1145/1168857.1168885},
 acmid = {1168885},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, bugs, debugging, heap, metrics},
} 

@article{Chilimbi:2006:HIH:1168919.1168885,
 author = {Chilimbi, Trishul M. and Ganapathy, Vinod},
 title = {HeapMD: identifying heap-based bugs using anomaly detection},
 abstract = {We present the design, implementation, and evaluation of HeapMD, a dynamic analysis tool that finds heap-based bugs using anomaly detection. HeapMD is based upon the observation that, in spite of the evolving nature of the heap, several of its properties remain stable. HeapMD uses this observation in a novel way: periodically, during the execution of the program, it computes a suite of metrics which are sensitive to the state of the heap. These metrics track heap behavior, and the stability of the heap reflects quantitatively in the values of these metrics. The "normal" ranges of stable metrics, obtained by running a program on multiple inputs, are then treated as indicators of correct behaviour, and are used in conjunction with an anomaly detector to find heap-based bugs. Using HeapMD, we were able to find 40 heap-based bugs, 31 of them previously unknown, in 5 large, commercial applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168885},
 doi = {http://doi.acm.org/10.1145/1168919.1168885},
 acmid = {1168885},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, bugs, debugging, heap, metrics},
} 

@article{Narayanasamy:2006:RSM:1168918.1168886,
 author = {Narayanasamy, Satish and Pereira, Cristiano and Calder, Brad},
 title = {Recording shared memory dependencies using strata},
 abstract = {Significant time is spent by companies trying to reproduce and fix bugs. BugNet and FDR are recent architecture proposals that provide architecture support for deterministic replay debugging. They focus on continuously recording information about the program's execution, which can be communicated back to the developer. Using that information, the developer can deterministically replay the program's execution to reproduce and fix the bugs.In this paper, we propose using Strata</i> to efficiently capture the shared memory dependencies. A stratum creates a time layer across all the logs for the running threads, which separates all the memory operations executed before and after the stratum. A strata log allows us to determine all the shared memory dependencies during replay and thereby supports deterministic replay debugging for multi-threaded programs.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168886},
 doi = {http://doi.acm.org/10.1145/1168918.1168886},
 acmid = {1168886},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dependencies, logging, replay, shared memory, strata},
} 

@article{Narayanasamy:2006:RSM:1168917.1168886,
 author = {Narayanasamy, Satish and Pereira, Cristiano and Calder, Brad},
 title = {Recording shared memory dependencies using strata},
 abstract = {Significant time is spent by companies trying to reproduce and fix bugs. BugNet and FDR are recent architecture proposals that provide architecture support for deterministic replay debugging. They focus on continuously recording information about the program's execution, which can be communicated back to the developer. Using that information, the developer can deterministically replay the program's execution to reproduce and fix the bugs.In this paper, we propose using Strata</i> to efficiently capture the shared memory dependencies. A stratum creates a time layer across all the logs for the running threads, which separates all the memory operations executed before and after the stratum. A strata log allows us to determine all the shared memory dependencies during replay and thereby supports deterministic replay debugging for multi-threaded programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168886},
 doi = {http://doi.acm.org/10.1145/1168917.1168886},
 acmid = {1168886},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dependencies, logging, replay, shared memory, strata},
} 

@article{Narayanasamy:2006:RSM:1168919.1168886,
 author = {Narayanasamy, Satish and Pereira, Cristiano and Calder, Brad},
 title = {Recording shared memory dependencies using strata},
 abstract = {Significant time is spent by companies trying to reproduce and fix bugs. BugNet and FDR are recent architecture proposals that provide architecture support for deterministic replay debugging. They focus on continuously recording information about the program's execution, which can be communicated back to the developer. Using that information, the developer can deterministically replay the program's execution to reproduce and fix the bugs.In this paper, we propose using Strata</i> to efficiently capture the shared memory dependencies. A stratum creates a time layer across all the logs for the running threads, which separates all the memory operations executed before and after the stratum. A strata log allows us to determine all the shared memory dependencies during replay and thereby supports deterministic replay debugging for multi-threaded programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168886},
 doi = {http://doi.acm.org/10.1145/1168919.1168886},
 acmid = {1168886},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dependencies, logging, replay, shared memory, strata},
} 

@inproceedings{Narayanasamy:2006:RSM:1168857.1168886,
 author = {Narayanasamy, Satish and Pereira, Cristiano and Calder, Brad},
 title = {Recording shared memory dependencies using strata},
 abstract = {Significant time is spent by companies trying to reproduce and fix bugs. BugNet and FDR are recent architecture proposals that provide architecture support for deterministic replay debugging. They focus on continuously recording information about the program's execution, which can be communicated back to the developer. Using that information, the developer can deterministically replay the program's execution to reproduce and fix the bugs.In this paper, we propose using Strata</i> to efficiently capture the shared memory dependencies. A stratum creates a time layer across all the logs for the running threads, which separates all the memory operations executed before and after the stratum. A strata log allows us to determine all the shared memory dependencies during replay and thereby supports deterministic replay debugging for multi-threaded programs.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168886},
 doi = {http://doi.acm.org/10.1145/1168857.1168886},
 acmid = {1168886},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dependencies, logging, replay, shared memory, strata},
} 

@article{Patwardhan:2006:DTS:1168919.1168888,
 author = {Patwardhan, Jaidev P. and Johri, Vijeta and Dwyer, Chris and Lebeck, Alvin R.},
 title = {A defect tolerant self-organizing nanoscale SIMD architecture},
 abstract = {The continual decrease in transistor size (through either scaled CMOS or emerging nano-technologies) promises to usher in an era of tera to peta-scale integration. However, this decrease in size is also likely to increase defect densities, contributing to the exponentially increasing cost of top-down lithography. Bottom-up manufacturing techniques, like self assembly, may provide a viable lower-cost alternative to top-down lithography, but may also be prone to higher defects. Therefore, regardless of fabrication methodology, defect tolerant architectures are necessary to exploit the full potential of future increased device densities.This paper explores a defect tolerant SIMD architecture. A key feature of our design is the ability of a large number of limited capability nodes with high defect rates (up to 30\%) to self-organize into a set of SIMD processing elements. Despite node simplicity and high defect rates, we show that by supporting the familiar data parallel programming model the architecture can execute a variety of programs. The architecture efficiently exploits a large number of nodes and higher device densities to keep device switching speeds and power density low. On a medium sized system (~1cm<sup>2</sup> area), the performance of the proposed architecture on our data parallel programs matches or exceeds the performance of an aggressively scaled out-of-order processor (128-wide, 8k reorder buffer, perfect memory system). For larger systems (\&gt;1cm<sup>2</sup>), the proposed architecture can match the performance of a chip multiprocessor with 16 aggressively scaled out-of-order cores.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168888},
 doi = {http://doi.acm.org/10.1145/1168919.1168888},
 acmid = {1168888},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, SIMD, bit-serial, data parallel, defect tolerance, nanocomputing, self-organizing},
} 

@article{Patwardhan:2006:DTS:1168917.1168888,
 author = {Patwardhan, Jaidev P. and Johri, Vijeta and Dwyer, Chris and Lebeck, Alvin R.},
 title = {A defect tolerant self-organizing nanoscale SIMD architecture},
 abstract = {The continual decrease in transistor size (through either scaled CMOS or emerging nano-technologies) promises to usher in an era of tera to peta-scale integration. However, this decrease in size is also likely to increase defect densities, contributing to the exponentially increasing cost of top-down lithography. Bottom-up manufacturing techniques, like self assembly, may provide a viable lower-cost alternative to top-down lithography, but may also be prone to higher defects. Therefore, regardless of fabrication methodology, defect tolerant architectures are necessary to exploit the full potential of future increased device densities.This paper explores a defect tolerant SIMD architecture. A key feature of our design is the ability of a large number of limited capability nodes with high defect rates (up to 30\%) to self-organize into a set of SIMD processing elements. Despite node simplicity and high defect rates, we show that by supporting the familiar data parallel programming model the architecture can execute a variety of programs. The architecture efficiently exploits a large number of nodes and higher device densities to keep device switching speeds and power density low. On a medium sized system (~1cm<sup>2</sup> area), the performance of the proposed architecture on our data parallel programs matches or exceeds the performance of an aggressively scaled out-of-order processor (128-wide, 8k reorder buffer, perfect memory system). For larger systems (\&gt;1cm<sup>2</sup>), the proposed architecture can match the performance of a chip multiprocessor with 16 aggressively scaled out-of-order cores.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168888},
 doi = {http://doi.acm.org/10.1145/1168917.1168888},
 acmid = {1168888},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, SIMD, bit-serial, data parallel, defect tolerance, nanocomputing, self-organizing},
} 

@inproceedings{Patwardhan:2006:DTS:1168857.1168888,
 author = {Patwardhan, Jaidev P. and Johri, Vijeta and Dwyer, Chris and Lebeck, Alvin R.},
 title = {A defect tolerant self-organizing nanoscale SIMD architecture},
 abstract = {The continual decrease in transistor size (through either scaled CMOS or emerging nano-technologies) promises to usher in an era of tera to peta-scale integration. However, this decrease in size is also likely to increase defect densities, contributing to the exponentially increasing cost of top-down lithography. Bottom-up manufacturing techniques, like self assembly, may provide a viable lower-cost alternative to top-down lithography, but may also be prone to higher defects. Therefore, regardless of fabrication methodology, defect tolerant architectures are necessary to exploit the full potential of future increased device densities.This paper explores a defect tolerant SIMD architecture. A key feature of our design is the ability of a large number of limited capability nodes with high defect rates (up to 30\%) to self-organize into a set of SIMD processing elements. Despite node simplicity and high defect rates, we show that by supporting the familiar data parallel programming model the architecture can execute a variety of programs. The architecture efficiently exploits a large number of nodes and higher device densities to keep device switching speeds and power density low. On a medium sized system (~1cm<sup>2</sup> area), the performance of the proposed architecture on our data parallel programs matches or exceeds the performance of an aggressively scaled out-of-order processor (128-wide, 8k reorder buffer, perfect memory system). For larger systems (\&gt;1cm<sup>2</sup>), the proposed architecture can match the performance of a chip multiprocessor with 16 aggressively scaled out-of-order cores.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168888},
 doi = {http://doi.acm.org/10.1145/1168857.1168888},
 acmid = {1168888},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, SIMD, bit-serial, data parallel, defect tolerance, nanocomputing, self-organizing},
} 

@article{Patwardhan:2006:DTS:1168918.1168888,
 author = {Patwardhan, Jaidev P. and Johri, Vijeta and Dwyer, Chris and Lebeck, Alvin R.},
 title = {A defect tolerant self-organizing nanoscale SIMD architecture},
 abstract = {The continual decrease in transistor size (through either scaled CMOS or emerging nano-technologies) promises to usher in an era of tera to peta-scale integration. However, this decrease in size is also likely to increase defect densities, contributing to the exponentially increasing cost of top-down lithography. Bottom-up manufacturing techniques, like self assembly, may provide a viable lower-cost alternative to top-down lithography, but may also be prone to higher defects. Therefore, regardless of fabrication methodology, defect tolerant architectures are necessary to exploit the full potential of future increased device densities.This paper explores a defect tolerant SIMD architecture. A key feature of our design is the ability of a large number of limited capability nodes with high defect rates (up to 30\%) to self-organize into a set of SIMD processing elements. Despite node simplicity and high defect rates, we show that by supporting the familiar data parallel programming model the architecture can execute a variety of programs. The architecture efficiently exploits a large number of nodes and higher device densities to keep device switching speeds and power density low. On a medium sized system (~1cm<sup>2</sup> area), the performance of the proposed architecture on our data parallel programs matches or exceeds the performance of an aggressively scaled out-of-order processor (128-wide, 8k reorder buffer, perfect memory system). For larger systems (\&gt;1cm<sup>2</sup>), the proposed architecture can match the performance of a chip multiprocessor with 16 aggressively scaled out-of-order cores.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168888},
 doi = {http://doi.acm.org/10.1145/1168918.1168888},
 acmid = {1168888},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, SIMD, bit-serial, data parallel, defect tolerance, nanocomputing, self-organizing},
} 

@article{Schuchman:2006:PTA:1168919.1168889,
 author = {Schuchman, Ethan and Vijaykumar, T. N.},
 title = {A program transformation and architecture support for quantum uncomputation},
 abstract = {Quantum computing's power comes from new algorithms that exploit quantum mechanical phenomena for computation. Quantum algorithms are different from their classical counterparts in that quantum algorithms rely on algorithmic structures that are simply not present in classical computing. Just as classical program transformations and architectures have been designed for common classical algorithm structures, quantum program transformations and quantum architectures should be designed with quantum algorithms in mind. Because quantum algorithms come with these new algorithmic structures, resultant quantum program transformations and architectures may look very different from their classical counterparts.This paper focuses on uncomputation, a critical and prevalent structure in quantum algorithms, and considers how program transformations, and architecture support should be designed to accommodate uncomputation. In this paper,we show a simple quantum program transformation that exposes independence between uncomputation and later computation. We then propose a multicore architecture tailored to this exposed parallelism and propose a scheduling policy that efficiently maps such parallelism to the multicore architecture. Our policy achieves parallelism between uncomputation and later computation while reducing cumulative communication distance. Our scheduling and architecture allows significant speedup of quantum programs (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while reducing cumulative communication distance 26\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168889},
 doi = {http://doi.acm.org/10.1145/1168919.1168889},
 acmid = {1168889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QLA, quantum computing, uncomputation},
} 

@article{Schuchman:2006:PTA:1168918.1168889,
 author = {Schuchman, Ethan and Vijaykumar, T. N.},
 title = {A program transformation and architecture support for quantum uncomputation},
 abstract = {Quantum computing's power comes from new algorithms that exploit quantum mechanical phenomena for computation. Quantum algorithms are different from their classical counterparts in that quantum algorithms rely on algorithmic structures that are simply not present in classical computing. Just as classical program transformations and architectures have been designed for common classical algorithm structures, quantum program transformations and quantum architectures should be designed with quantum algorithms in mind. Because quantum algorithms come with these new algorithmic structures, resultant quantum program transformations and architectures may look very different from their classical counterparts.This paper focuses on uncomputation, a critical and prevalent structure in quantum algorithms, and considers how program transformations, and architecture support should be designed to accommodate uncomputation. In this paper,we show a simple quantum program transformation that exposes independence between uncomputation and later computation. We then propose a multicore architecture tailored to this exposed parallelism and propose a scheduling policy that efficiently maps such parallelism to the multicore architecture. Our policy achieves parallelism between uncomputation and later computation while reducing cumulative communication distance. Our scheduling and architecture allows significant speedup of quantum programs (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while reducing cumulative communication distance 26\%.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168889},
 doi = {http://doi.acm.org/10.1145/1168918.1168889},
 acmid = {1168889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QLA, quantum computing, uncomputation},
} 

@inproceedings{Schuchman:2006:PTA:1168857.1168889,
 author = {Schuchman, Ethan and Vijaykumar, T. N.},
 title = {A program transformation and architecture support for quantum uncomputation},
 abstract = {Quantum computing's power comes from new algorithms that exploit quantum mechanical phenomena for computation. Quantum algorithms are different from their classical counterparts in that quantum algorithms rely on algorithmic structures that are simply not present in classical computing. Just as classical program transformations and architectures have been designed for common classical algorithm structures, quantum program transformations and quantum architectures should be designed with quantum algorithms in mind. Because quantum algorithms come with these new algorithmic structures, resultant quantum program transformations and architectures may look very different from their classical counterparts.This paper focuses on uncomputation, a critical and prevalent structure in quantum algorithms, and considers how program transformations, and architecture support should be designed to accommodate uncomputation. In this paper,we show a simple quantum program transformation that exposes independence between uncomputation and later computation. We then propose a multicore architecture tailored to this exposed parallelism and propose a scheduling policy that efficiently maps such parallelism to the multicore architecture. Our policy achieves parallelism between uncomputation and later computation while reducing cumulative communication distance. Our scheduling and architecture allows significant speedup of quantum programs (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while reducing cumulative communication distance 26\%.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168889},
 doi = {http://doi.acm.org/10.1145/1168857.1168889},
 acmid = {1168889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QLA, quantum computing, uncomputation},
} 

@article{Schuchman:2006:PTA:1168917.1168889,
 author = {Schuchman, Ethan and Vijaykumar, T. N.},
 title = {A program transformation and architecture support for quantum uncomputation},
 abstract = {Quantum computing's power comes from new algorithms that exploit quantum mechanical phenomena for computation. Quantum algorithms are different from their classical counterparts in that quantum algorithms rely on algorithmic structures that are simply not present in classical computing. Just as classical program transformations and architectures have been designed for common classical algorithm structures, quantum program transformations and quantum architectures should be designed with quantum algorithms in mind. Because quantum algorithms come with these new algorithmic structures, resultant quantum program transformations and architectures may look very different from their classical counterparts.This paper focuses on uncomputation, a critical and prevalent structure in quantum algorithms, and considers how program transformations, and architecture support should be designed to accommodate uncomputation. In this paper,we show a simple quantum program transformation that exposes independence between uncomputation and later computation. We then propose a multicore architecture tailored to this exposed parallelism and propose a scheduling policy that efficiently maps such parallelism to the multicore architecture. Our policy achieves parallelism between uncomputation and later computation while reducing cumulative communication distance. Our scheduling and architecture allows significant speedup of quantum programs (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while reducing cumulative communication distance 26\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168889},
 doi = {http://doi.acm.org/10.1145/1168917.1168889},
 acmid = {1168889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QLA, quantum computing, uncomputation},
} 

@article{Mysore:2006:IC:1168917.1168890,
 author = {Mysore, Shashidhar and Agrawal, Banit and Srivastava, Navin and Lin, Sheng-Chih and Banerjee, Kaustav and Sherwood, Tim},
 title = {Introspective 3D chips},
 abstract = {While the number of transistors on a chip increases exponentially over time, the productivity that can be realized from these systems has not kept pace. To deal with the complexity of modern systems, software developers are increasingly dependent on specialized development tools such as security profilers, memory leak identifiers, data flight recorders, and dynamic type analysis. Many of these tools require full-system data which covers multiple interacting threads, processes, and processors. Reducing the performance penalty and complexity of these software tools is critical to those developing next generation applications, and many researchers have proposed adding specialized hardware to assist in profiling and introspection. Unfortunately, while this additional hardware would be incredibly beneficial to developers, the cost of this hardware must be paid on every single die that is manufactured.In this paper, we argue that a new way to attack this problem is with the addition of specialized analysis hardware built on separate active layers stacked vertically on the processor die using 3D IC technology. This provides a modular "snap-on" functionality that could be included with developer systems, and omitted from consumer systems to keep the cost impact to a minimum. In this paper we describe the advantage of using inter-die vias for introspection and we quantify the impact they can have in terms of the area, power, temperature, and routability of the resulting systems. We show that hardware stubs could be inserted into commodity processors at design time that would allow analysis layers to be bonded to development chips, and that these stubs would increase area and power by no more than 0.021mm<sup>2</sup> and 0.9\% respectively.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168890},
 doi = {http://doi.acm.org/10.1145/1168917.1168890},
 acmid = {1168890},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D Architectures, hardware support for profiling, introspection},
} 

@article{Mysore:2006:IC:1168918.1168890,
 author = {Mysore, Shashidhar and Agrawal, Banit and Srivastava, Navin and Lin, Sheng-Chih and Banerjee, Kaustav and Sherwood, Tim},
 title = {Introspective 3D chips},
 abstract = {While the number of transistors on a chip increases exponentially over time, the productivity that can be realized from these systems has not kept pace. To deal with the complexity of modern systems, software developers are increasingly dependent on specialized development tools such as security profilers, memory leak identifiers, data flight recorders, and dynamic type analysis. Many of these tools require full-system data which covers multiple interacting threads, processes, and processors. Reducing the performance penalty and complexity of these software tools is critical to those developing next generation applications, and many researchers have proposed adding specialized hardware to assist in profiling and introspection. Unfortunately, while this additional hardware would be incredibly beneficial to developers, the cost of this hardware must be paid on every single die that is manufactured.In this paper, we argue that a new way to attack this problem is with the addition of specialized analysis hardware built on separate active layers stacked vertically on the processor die using 3D IC technology. This provides a modular "snap-on" functionality that could be included with developer systems, and omitted from consumer systems to keep the cost impact to a minimum. In this paper we describe the advantage of using inter-die vias for introspection and we quantify the impact they can have in terms of the area, power, temperature, and routability of the resulting systems. We show that hardware stubs could be inserted into commodity processors at design time that would allow analysis layers to be bonded to development chips, and that these stubs would increase area and power by no more than 0.021mm<sup>2</sup> and 0.9\% respectively.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168890},
 doi = {http://doi.acm.org/10.1145/1168918.1168890},
 acmid = {1168890},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D Architectures, hardware support for profiling, introspection},
} 

@inproceedings{Mysore:2006:IC:1168857.1168890,
 author = {Mysore, Shashidhar and Agrawal, Banit and Srivastava, Navin and Lin, Sheng-Chih and Banerjee, Kaustav and Sherwood, Tim},
 title = {Introspective 3D chips},
 abstract = {While the number of transistors on a chip increases exponentially over time, the productivity that can be realized from these systems has not kept pace. To deal with the complexity of modern systems, software developers are increasingly dependent on specialized development tools such as security profilers, memory leak identifiers, data flight recorders, and dynamic type analysis. Many of these tools require full-system data which covers multiple interacting threads, processes, and processors. Reducing the performance penalty and complexity of these software tools is critical to those developing next generation applications, and many researchers have proposed adding specialized hardware to assist in profiling and introspection. Unfortunately, while this additional hardware would be incredibly beneficial to developers, the cost of this hardware must be paid on every single die that is manufactured.In this paper, we argue that a new way to attack this problem is with the addition of specialized analysis hardware built on separate active layers stacked vertically on the processor die using 3D IC technology. This provides a modular "snap-on" functionality that could be included with developer systems, and omitted from consumer systems to keep the cost impact to a minimum. In this paper we describe the advantage of using inter-die vias for introspection and we quantify the impact they can have in terms of the area, power, temperature, and routability of the resulting systems. We show that hardware stubs could be inserted into commodity processors at design time that would allow analysis layers to be bonded to development chips, and that these stubs would increase area and power by no more than 0.021mm<sup>2</sup> and 0.9\% respectively.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168890},
 doi = {http://doi.acm.org/10.1145/1168857.1168890},
 acmid = {1168890},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D Architectures, hardware support for profiling, introspection},
} 

@article{Mysore:2006:IC:1168919.1168890,
 author = {Mysore, Shashidhar and Agrawal, Banit and Srivastava, Navin and Lin, Sheng-Chih and Banerjee, Kaustav and Sherwood, Tim},
 title = {Introspective 3D chips},
 abstract = {While the number of transistors on a chip increases exponentially over time, the productivity that can be realized from these systems has not kept pace. To deal with the complexity of modern systems, software developers are increasingly dependent on specialized development tools such as security profilers, memory leak identifiers, data flight recorders, and dynamic type analysis. Many of these tools require full-system data which covers multiple interacting threads, processes, and processors. Reducing the performance penalty and complexity of these software tools is critical to those developing next generation applications, and many researchers have proposed adding specialized hardware to assist in profiling and introspection. Unfortunately, while this additional hardware would be incredibly beneficial to developers, the cost of this hardware must be paid on every single die that is manufactured.In this paper, we argue that a new way to attack this problem is with the addition of specialized analysis hardware built on separate active layers stacked vertically on the processor die using 3D IC technology. This provides a modular "snap-on" functionality that could be included with developer systems, and omitted from consumer systems to keep the cost impact to a minimum. In this paper we describe the advantage of using inter-die vias for introspection and we quantify the impact they can have in terms of the area, power, temperature, and routability of the resulting systems. We show that hardware stubs could be inserted into commodity processors at design time that would allow analysis layers to be bonded to development chips, and that these stubs would increase area and power by no more than 0.021mm<sup>2</sup> and 0.9\% respectively.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {264--273},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168890},
 doi = {http://doi.acm.org/10.1145/1168919.1168890},
 acmid = {1168890},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D Architectures, hardware support for profiling, introspection},
} 

@article{Cantin:2006:SP:1168917.1168892,
 author = {Cantin, Jason F. and Lipasti, Mikko H. and Smith, James E.},
 title = {Stealth prefetching},
 abstract = {Prefetching in shared-memory multiprocessor systems is an increasingly difficult problem. As system designs grow to incorporate larger numbers of faster processors, memory latency and interconnect traffic increase. While aggressive prefetching techniques can mitigate the increasing memory latency, they can harm performance by wasting precious interconnect bandwidth and prematurely accessing shared data, causing state downgrades at remote nodes that force later upgrades.This paper investigates Stealth Prefetching, a new technique that utilizes information from Coarse-Grain Coherence Tracking (CGCT) for prefetching data aggressively, stealthily, and efficiently in a broadcast-based shared-memory multiprocessor system. Stealth Prefetching utilizes CGCT to identify regions of memory that are not shared by other processors, aggressively fetches these lines from DRAM in open-page mode, and moves them close to the processor in anticipation of future references. Our analysis with commercial, scientific, and multiprogrammed workloads show that Stealth Prefetching provides an average speedup of 20\% over an aggressive baseline system with conventional prefetching.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {274--282},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1168917.1168892},
 doi = {http://doi.acm.org/10.1145/1168917.1168892},
 acmid = {1168892},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence, multiprocessors, prefetching},
} 

@article{Cantin:2006:SP:1168919.1168892,
 author = {Cantin, Jason F. and Lipasti, Mikko H. and Smith, James E.},
 title = {Stealth prefetching},
 abstract = {Prefetching in shared-memory multiprocessor systems is an increasingly difficult problem. As system designs grow to incorporate larger numbers of faster processors, memory latency and interconnect traffic increase. While aggressive prefetching techniques can mitigate the increasing memory latency, they can harm performance by wasting precious interconnect bandwidth and prematurely accessing shared data, causing state downgrades at remote nodes that force later upgrades.This paper investigates Stealth Prefetching, a new technique that utilizes information from Coarse-Grain Coherence Tracking (CGCT) for prefetching data aggressively, stealthily, and efficiently in a broadcast-based shared-memory multiprocessor system. Stealth Prefetching utilizes CGCT to identify regions of memory that are not shared by other processors, aggressively fetches these lines from DRAM in open-page mode, and moves them close to the processor in anticipation of future references. Our analysis with commercial, scientific, and multiprogrammed workloads show that Stealth Prefetching provides an average speedup of 20\% over an aggressive baseline system with conventional prefetching.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {274--282},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1168919.1168892},
 doi = {http://doi.acm.org/10.1145/1168919.1168892},
 acmid = {1168892},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence, multiprocessors, prefetching},
} 

@article{Cantin:2006:SP:1168918.1168892,
 author = {Cantin, Jason F. and Lipasti, Mikko H. and Smith, James E.},
 title = {Stealth prefetching},
 abstract = {Prefetching in shared-memory multiprocessor systems is an increasingly difficult problem. As system designs grow to incorporate larger numbers of faster processors, memory latency and interconnect traffic increase. While aggressive prefetching techniques can mitigate the increasing memory latency, they can harm performance by wasting precious interconnect bandwidth and prematurely accessing shared data, causing state downgrades at remote nodes that force later upgrades.This paper investigates Stealth Prefetching, a new technique that utilizes information from Coarse-Grain Coherence Tracking (CGCT) for prefetching data aggressively, stealthily, and efficiently in a broadcast-based shared-memory multiprocessor system. Stealth Prefetching utilizes CGCT to identify regions of memory that are not shared by other processors, aggressively fetches these lines from DRAM in open-page mode, and moves them close to the processor in anticipation of future references. Our analysis with commercial, scientific, and multiprogrammed workloads show that Stealth Prefetching provides an average speedup of 20\% over an aggressive baseline system with conventional prefetching.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {274--282},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1168918.1168892},
 doi = {http://doi.acm.org/10.1145/1168918.1168892},
 acmid = {1168892},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence, multiprocessors, prefetching},
} 

@inproceedings{Cantin:2006:SP:1168857.1168892,
 author = {Cantin, Jason F. and Lipasti, Mikko H. and Smith, James E.},
 title = {Stealth prefetching},
 abstract = {Prefetching in shared-memory multiprocessor systems is an increasingly difficult problem. As system designs grow to incorporate larger numbers of faster processors, memory latency and interconnect traffic increase. While aggressive prefetching techniques can mitigate the increasing memory latency, they can harm performance by wasting precious interconnect bandwidth and prematurely accessing shared data, causing state downgrades at remote nodes that force later upgrades.This paper investigates Stealth Prefetching, a new technique that utilizes information from Coarse-Grain Coherence Tracking (CGCT) for prefetching data aggressively, stealthily, and efficiently in a broadcast-based shared-memory multiprocessor system. Stealth Prefetching utilizes CGCT to identify regions of memory that are not shared by other processors, aggressively fetches these lines from DRAM in open-page mode, and moves them close to the processor in anticipation of future references. Our analysis with commercial, scientific, and multiprogrammed workloads show that Stealth Prefetching provides an average speedup of 20\% over an aggressive baseline system with conventional prefetching.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {274--282},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1168857.1168892},
 doi = {http://doi.acm.org/10.1145/1168857.1168892},
 acmid = {1168892},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence, multiprocessors, prefetching},
} 

@inproceedings{Chakraborty:2006:CSE:1168857.1168893,
 author = {Chakraborty, Koushik and Wells, Philip M. and Sohi, Gurindar S.},
 title = {Computation spreading: employing hardware migration to specialize CMP cores on-the-fly},
 abstract = {In canonical parallel processing, the operating system (OS) assigns a processing core to a single thread from a multithreaded server application. Since different threads from the same application often carry out similar computation, albeit at different times, we observe extensive code reuse among different processors, causing redundancy (e.g., in our server workloads, 45-65\% of all instruction blocks are accessed by all processors). Moreover, largely independent fragments of computation compete for the same private resources causing destructive interference. Together, this redundancy and interference lead to poor utilization of private microarchitecture resources such as caches and branch predictors.We present Computation Spreading</i> (CSP), which employs hardware migration to distribute a thread's dissimilar fragments of computation across the multiple processing cores of a chip multiprocessor (CMP), while grouping similar computation fragments from different threads together. This paper focuses on a specific example of CSP for OS intensive server applications: separating application level (user) computation from the OS calls it makes.When performing CSP, each core becomes temporally specialized to execute certain computation fragments, and the same core is repeatedly used for such fragments. We examine two specific thread assignment policies for CSP, and show that these policies, across four server workloads, are able to reduce instruction misses in private L2 caches by 27-58\%, private L2 load misses by 0-19\%, and branch mispredictions by 9-25\%.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {283--292},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168893},
 doi = {http://doi.acm.org/10.1145/1168857.1168893},
 acmid = {1168893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache locality, dynamic specialization},
} 

@article{Chakraborty:2006:CSE:1168918.1168893,
 author = {Chakraborty, Koushik and Wells, Philip M. and Sohi, Gurindar S.},
 title = {Computation spreading: employing hardware migration to specialize CMP cores on-the-fly},
 abstract = {In canonical parallel processing, the operating system (OS) assigns a processing core to a single thread from a multithreaded server application. Since different threads from the same application often carry out similar computation, albeit at different times, we observe extensive code reuse among different processors, causing redundancy (e.g., in our server workloads, 45-65\% of all instruction blocks are accessed by all processors). Moreover, largely independent fragments of computation compete for the same private resources causing destructive interference. Together, this redundancy and interference lead to poor utilization of private microarchitecture resources such as caches and branch predictors.We present Computation Spreading</i> (CSP), which employs hardware migration to distribute a thread's dissimilar fragments of computation across the multiple processing cores of a chip multiprocessor (CMP), while grouping similar computation fragments from different threads together. This paper focuses on a specific example of CSP for OS intensive server applications: separating application level (user) computation from the OS calls it makes.When performing CSP, each core becomes temporally specialized to execute certain computation fragments, and the same core is repeatedly used for such fragments. We examine two specific thread assignment policies for CSP, and show that these policies, across four server workloads, are able to reduce instruction misses in private L2 caches by 27-58\%, private L2 load misses by 0-19\%, and branch mispredictions by 9-25\%.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {283--292},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168893},
 doi = {http://doi.acm.org/10.1145/1168918.1168893},
 acmid = {1168893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache locality, dynamic specialization},
} 

@article{Chakraborty:2006:CSE:1168919.1168893,
 author = {Chakraborty, Koushik and Wells, Philip M. and Sohi, Gurindar S.},
 title = {Computation spreading: employing hardware migration to specialize CMP cores on-the-fly},
 abstract = {In canonical parallel processing, the operating system (OS) assigns a processing core to a single thread from a multithreaded server application. Since different threads from the same application often carry out similar computation, albeit at different times, we observe extensive code reuse among different processors, causing redundancy (e.g., in our server workloads, 45-65\% of all instruction blocks are accessed by all processors). Moreover, largely independent fragments of computation compete for the same private resources causing destructive interference. Together, this redundancy and interference lead to poor utilization of private microarchitecture resources such as caches and branch predictors.We present Computation Spreading</i> (CSP), which employs hardware migration to distribute a thread's dissimilar fragments of computation across the multiple processing cores of a chip multiprocessor (CMP), while grouping similar computation fragments from different threads together. This paper focuses on a specific example of CSP for OS intensive server applications: separating application level (user) computation from the OS calls it makes.When performing CSP, each core becomes temporally specialized to execute certain computation fragments, and the same core is repeatedly used for such fragments. We examine two specific thread assignment policies for CSP, and show that these policies, across four server workloads, are able to reduce instruction misses in private L2 caches by 27-58\%, private L2 load misses by 0-19\%, and branch mispredictions by 9-25\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {283--292},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168893},
 doi = {http://doi.acm.org/10.1145/1168919.1168893},
 acmid = {1168893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache locality, dynamic specialization},
} 

@article{Chakraborty:2006:CSE:1168917.1168893,
 author = {Chakraborty, Koushik and Wells, Philip M. and Sohi, Gurindar S.},
 title = {Computation spreading: employing hardware migration to specialize CMP cores on-the-fly},
 abstract = {In canonical parallel processing, the operating system (OS) assigns a processing core to a single thread from a multithreaded server application. Since different threads from the same application often carry out similar computation, albeit at different times, we observe extensive code reuse among different processors, causing redundancy (e.g., in our server workloads, 45-65\% of all instruction blocks are accessed by all processors). Moreover, largely independent fragments of computation compete for the same private resources causing destructive interference. Together, this redundancy and interference lead to poor utilization of private microarchitecture resources such as caches and branch predictors.We present Computation Spreading</i> (CSP), which employs hardware migration to distribute a thread's dissimilar fragments of computation across the multiple processing cores of a chip multiprocessor (CMP), while grouping similar computation fragments from different threads together. This paper focuses on a specific example of CSP for OS intensive server applications: separating application level (user) computation from the OS calls it makes.When performing CSP, each core becomes temporally specialized to execute certain computation fragments, and the same core is repeatedly used for such fragments. We examine two specific thread assignment policies for CSP, and show that these policies, across four server workloads, are able to reduce instruction misses in private L2 caches by 27-58\%, private L2 load misses by 0-19\%, and branch mispredictions by 9-25\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {283--292},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168893},
 doi = {http://doi.acm.org/10.1145/1168917.1168893},
 acmid = {1168893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache locality, dynamic specialization},
} 

@article{Miller:2006:SIC:1168918.1168894,
 author = {Miller, Jason E. and Agarwal, Anant},
 title = {Software-based instruction caching for embedded processors},
 abstract = {While hardware instruction caches are present in virtually all general-purpose and high-performance microprocessors today, many embedded processors use SRAM or scratchpad memories instead. These are simple array memory structures that are directly addressed and explicitly managed by software. Compared to hardware caches of the same data capacity, they are smaller, have shorter access times and consume less energy per access. Access times are also easier to predict with simple memories since there is no possibility of a "miss." On the other hand, they are more difficult for the programmer to use since they are not automatically managed.In this paper, we present a software system that allows all or part of an SRAM or scratchpad memory to be automatically managed as a cache. This system provides the programming convenience of a cache for processors that lack dedicated caching hardware. It has been implemented for an actual processor and runs on real hardware. Our results show that a software-based instruction cache can be built that provides performance within 10\% of a traditional hardware cache on many benchmarks while using a cheaper, simpler, SRAM memory. On these same benchmarks, energy consumption is up to 3\% lower than it would be using a hardware cache.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {293--302},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168894},
 doi = {http://doi.acm.org/10.1145/1168918.1168894},
 acmid = {1168894},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chaining, instruction cache, software caching},
} 

@inproceedings{Miller:2006:SIC:1168857.1168894,
 author = {Miller, Jason E. and Agarwal, Anant},
 title = {Software-based instruction caching for embedded processors},
 abstract = {While hardware instruction caches are present in virtually all general-purpose and high-performance microprocessors today, many embedded processors use SRAM or scratchpad memories instead. These are simple array memory structures that are directly addressed and explicitly managed by software. Compared to hardware caches of the same data capacity, they are smaller, have shorter access times and consume less energy per access. Access times are also easier to predict with simple memories since there is no possibility of a "miss." On the other hand, they are more difficult for the programmer to use since they are not automatically managed.In this paper, we present a software system that allows all or part of an SRAM or scratchpad memory to be automatically managed as a cache. This system provides the programming convenience of a cache for processors that lack dedicated caching hardware. It has been implemented for an actual processor and runs on real hardware. Our results show that a software-based instruction cache can be built that provides performance within 10\% of a traditional hardware cache on many benchmarks while using a cheaper, simpler, SRAM memory. On these same benchmarks, energy consumption is up to 3\% lower than it would be using a hardware cache.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {293--302},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168894},
 doi = {http://doi.acm.org/10.1145/1168857.1168894},
 acmid = {1168894},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chaining, instruction cache, software caching},
} 

@article{Miller:2006:SIC:1168917.1168894,
 author = {Miller, Jason E. and Agarwal, Anant},
 title = {Software-based instruction caching for embedded processors},
 abstract = {While hardware instruction caches are present in virtually all general-purpose and high-performance microprocessors today, many embedded processors use SRAM or scratchpad memories instead. These are simple array memory structures that are directly addressed and explicitly managed by software. Compared to hardware caches of the same data capacity, they are smaller, have shorter access times and consume less energy per access. Access times are also easier to predict with simple memories since there is no possibility of a "miss." On the other hand, they are more difficult for the programmer to use since they are not automatically managed.In this paper, we present a software system that allows all or part of an SRAM or scratchpad memory to be automatically managed as a cache. This system provides the programming convenience of a cache for processors that lack dedicated caching hardware. It has been implemented for an actual processor and runs on real hardware. Our results show that a software-based instruction cache can be built that provides performance within 10\% of a traditional hardware cache on many benchmarks while using a cheaper, simpler, SRAM memory. On these same benchmarks, energy consumption is up to 3\% lower than it would be using a hardware cache.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {293--302},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168894},
 doi = {http://doi.acm.org/10.1145/1168917.1168894},
 acmid = {1168894},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chaining, instruction cache, software caching},
} 

@article{Miller:2006:SIC:1168919.1168894,
 author = {Miller, Jason E. and Agarwal, Anant},
 title = {Software-based instruction caching for embedded processors},
 abstract = {While hardware instruction caches are present in virtually all general-purpose and high-performance microprocessors today, many embedded processors use SRAM or scratchpad memories instead. These are simple array memory structures that are directly addressed and explicitly managed by software. Compared to hardware caches of the same data capacity, they are smaller, have shorter access times and consume less energy per access. Access times are also easier to predict with simple memories since there is no possibility of a "miss." On the other hand, they are more difficult for the programmer to use since they are not automatically managed.In this paper, we present a software system that allows all or part of an SRAM or scratchpad memory to be automatically managed as a cache. This system provides the programming convenience of a cache for processors that lack dedicated caching hardware. It has been implemented for an actual processor and runs on real hardware. Our results show that a software-based instruction cache can be built that provides performance within 10\% of a traditional hardware cache on many benchmarks while using a cheaper, simpler, SRAM memory. On these same benchmarks, energy consumption is up to 3\% lower than it would be using a hardware cache.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {293--302},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168894},
 doi = {http://doi.acm.org/10.1145/1168919.1168894},
 acmid = {1168894},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chaining, instruction cache, software caching},
} 

@article{Li:2006:MEM:1168919.1168896,
 author = {Li, Xin and Boldt, Marian and von Hanxleden, Reinhard},
 title = {Mapping esterel onto a multi-threaded embedded processor},
 abstract = {The synchronous language Esterel is well-suited for programming control-dominated reactive systems at the system level. It provides non-traditional control structures, in particular concurrency and various forms of preemption, which allow to concisely express reactive behavior. As these control structures cannot be mapped easily onto traditional, sequential processors, an alternative approach that has emerged recently makes use of special-purpose reactive processors. However, the designs proposed so far have limitations regarding completeness of the language support, and did not really take advantage of compile-time knowledge to optimize resource usage.This paper presents a reactive processor, the Kiel Esterel Processor 3a (KEP3a), and its compiler. The KEP3a improves on earlier designs in several areas; most notable are the support for exception handling and the provision of context-dependent preemption handling instructions. The KEP3a compiler presented here is to our knowledge the first for multi-threaded reactive processors. The translation of Esterel's preemption constructs onto KEP3a assembler is straightforward; however, a challenge is the correct and efficient representation of Esterel's concurrency. The compiler generates code that respects data and control dependencies using the KEP3a priority-based scheduling mechanism. We present a priority assignment approach that makes use of a novel concurrent control flow graph and has a complexity that in practice tends to be linear in the size of the program. Unlike earlier Esterel compilation schemes, this approach avoids unnecessary context switches by considering each thread's actual execution state at run time. Furthermore, it avoids code replication present in other approaches.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {303--314},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168896},
 doi = {http://doi.acm.org/10.1145/1168919.1168896},
 acmid = {1168896},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, esterel, low-power processing, multi-threading, reactive systems},
} 

@article{Li:2006:MEM:1168917.1168896,
 author = {Li, Xin and Boldt, Marian and von Hanxleden, Reinhard},
 title = {Mapping esterel onto a multi-threaded embedded processor},
 abstract = {The synchronous language Esterel is well-suited for programming control-dominated reactive systems at the system level. It provides non-traditional control structures, in particular concurrency and various forms of preemption, which allow to concisely express reactive behavior. As these control structures cannot be mapped easily onto traditional, sequential processors, an alternative approach that has emerged recently makes use of special-purpose reactive processors. However, the designs proposed so far have limitations regarding completeness of the language support, and did not really take advantage of compile-time knowledge to optimize resource usage.This paper presents a reactive processor, the Kiel Esterel Processor 3a (KEP3a), and its compiler. The KEP3a improves on earlier designs in several areas; most notable are the support for exception handling and the provision of context-dependent preemption handling instructions. The KEP3a compiler presented here is to our knowledge the first for multi-threaded reactive processors. The translation of Esterel's preemption constructs onto KEP3a assembler is straightforward; however, a challenge is the correct and efficient representation of Esterel's concurrency. The compiler generates code that respects data and control dependencies using the KEP3a priority-based scheduling mechanism. We present a priority assignment approach that makes use of a novel concurrent control flow graph and has a complexity that in practice tends to be linear in the size of the program. Unlike earlier Esterel compilation schemes, this approach avoids unnecessary context switches by considering each thread's actual execution state at run time. Furthermore, it avoids code replication present in other approaches.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {303--314},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168896},
 doi = {http://doi.acm.org/10.1145/1168917.1168896},
 acmid = {1168896},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, esterel, low-power processing, multi-threading, reactive systems},
} 

@article{Li:2006:MEM:1168918.1168896,
 author = {Li, Xin and Boldt, Marian and von Hanxleden, Reinhard},
 title = {Mapping esterel onto a multi-threaded embedded processor},
 abstract = {The synchronous language Esterel is well-suited for programming control-dominated reactive systems at the system level. It provides non-traditional control structures, in particular concurrency and various forms of preemption, which allow to concisely express reactive behavior. As these control structures cannot be mapped easily onto traditional, sequential processors, an alternative approach that has emerged recently makes use of special-purpose reactive processors. However, the designs proposed so far have limitations regarding completeness of the language support, and did not really take advantage of compile-time knowledge to optimize resource usage.This paper presents a reactive processor, the Kiel Esterel Processor 3a (KEP3a), and its compiler. The KEP3a improves on earlier designs in several areas; most notable are the support for exception handling and the provision of context-dependent preemption handling instructions. The KEP3a compiler presented here is to our knowledge the first for multi-threaded reactive processors. The translation of Esterel's preemption constructs onto KEP3a assembler is straightforward; however, a challenge is the correct and efficient representation of Esterel's concurrency. The compiler generates code that respects data and control dependencies using the KEP3a priority-based scheduling mechanism. We present a priority assignment approach that makes use of a novel concurrent control flow graph and has a complexity that in practice tends to be linear in the size of the program. Unlike earlier Esterel compilation schemes, this approach avoids unnecessary context switches by considering each thread's actual execution state at run time. Furthermore, it avoids code replication present in other approaches.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {303--314},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168896},
 doi = {http://doi.acm.org/10.1145/1168918.1168896},
 acmid = {1168896},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, esterel, low-power processing, multi-threading, reactive systems},
} 

@inproceedings{Li:2006:MEM:1168857.1168896,
 author = {Li, Xin and Boldt, Marian and von Hanxleden, Reinhard},
 title = {Mapping esterel onto a multi-threaded embedded processor},
 abstract = {The synchronous language Esterel is well-suited for programming control-dominated reactive systems at the system level. It provides non-traditional control structures, in particular concurrency and various forms of preemption, which allow to concisely express reactive behavior. As these control structures cannot be mapped easily onto traditional, sequential processors, an alternative approach that has emerged recently makes use of special-purpose reactive processors. However, the designs proposed so far have limitations regarding completeness of the language support, and did not really take advantage of compile-time knowledge to optimize resource usage.This paper presents a reactive processor, the Kiel Esterel Processor 3a (KEP3a), and its compiler. The KEP3a improves on earlier designs in several areas; most notable are the support for exception handling and the provision of context-dependent preemption handling instructions. The KEP3a compiler presented here is to our knowledge the first for multi-threaded reactive processors. The translation of Esterel's preemption constructs onto KEP3a assembler is straightforward; however, a challenge is the correct and efficient representation of Esterel's concurrency. The compiler generates code that respects data and control dependencies using the KEP3a priority-based scheduling mechanism. We present a priority assignment approach that makes use of a novel concurrent control flow graph and has a complexity that in practice tends to be linear in the size of the program. Unlike earlier Esterel compilation schemes, this approach avoids unnecessary context switches by considering each thread's actual execution state at run time. Furthermore, it avoids code replication present in other approaches.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {303--314},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168896},
 doi = {http://doi.acm.org/10.1145/1168857.1168896},
 acmid = {1168896},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, esterel, low-power processing, multi-threading, reactive systems},
} 

@article{Binkert:2006:INI:1168917.1168897,
 author = {Binkert, Nathan L. and Saidi, Ali G. and Reinhardt, Steven K.},
 title = {Integrated network interfaces for high-bandwidth TCP/IP},
 abstract = {This paper proposes new network interface controller (NIC) designs that take advantage of integration with the host CPU to provide increased flexibility for operating system kernel-based performance optimization.We believe that this approach is more likely to meet the needs of current and future high-bandwidth TCP/IP networking on end hosts than the current trend of putting more complexity in the NIC, while avoiding the need to modify applications and protocols. This paper presents two such NICs. The first, the simple integrated NIC (SINIC), is a minimally complex design that moves the responsibility for managing the network FIFOs from the NIC to the kernel. Despite this closer interaction between the kernel and the NIC, SINIC provides performance equivalent to a conventional DMA-based NIC without increasing CPU overhead. The second design, V-SINIC, adds virtual per-packet registers to SINIC, enabling parallel packet processing while maintaining a FIFO model. V-SINIC allows the kernel to decouple examining a packet's header from copying its payload to memory. We exploit this capability to implement a true zero-copy receive optimization in the Linux 2.6 kernel, providing bandwidth improvements of over 50\% on unmodified sockets-based receive-intensive benchmarks.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {315--324},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168897},
 doi = {http://doi.acm.org/10.1145/1168917.1168897},
 acmid = {1168897},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP/IP performance, interfaces, network, zero-copy},
} 

@article{Binkert:2006:INI:1168918.1168897,
 author = {Binkert, Nathan L. and Saidi, Ali G. and Reinhardt, Steven K.},
 title = {Integrated network interfaces for high-bandwidth TCP/IP},
 abstract = {This paper proposes new network interface controller (NIC) designs that take advantage of integration with the host CPU to provide increased flexibility for operating system kernel-based performance optimization.We believe that this approach is more likely to meet the needs of current and future high-bandwidth TCP/IP networking on end hosts than the current trend of putting more complexity in the NIC, while avoiding the need to modify applications and protocols. This paper presents two such NICs. The first, the simple integrated NIC (SINIC), is a minimally complex design that moves the responsibility for managing the network FIFOs from the NIC to the kernel. Despite this closer interaction between the kernel and the NIC, SINIC provides performance equivalent to a conventional DMA-based NIC without increasing CPU overhead. The second design, V-SINIC, adds virtual per-packet registers to SINIC, enabling parallel packet processing while maintaining a FIFO model. V-SINIC allows the kernel to decouple examining a packet's header from copying its payload to memory. We exploit this capability to implement a true zero-copy receive optimization in the Linux 2.6 kernel, providing bandwidth improvements of over 50\% on unmodified sockets-based receive-intensive benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {315--324},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168897},
 doi = {http://doi.acm.org/10.1145/1168918.1168897},
 acmid = {1168897},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP/IP performance, interfaces, network, zero-copy},
} 

@inproceedings{Binkert:2006:INI:1168857.1168897,
 author = {Binkert, Nathan L. and Saidi, Ali G. and Reinhardt, Steven K.},
 title = {Integrated network interfaces for high-bandwidth TCP/IP},
 abstract = {This paper proposes new network interface controller (NIC) designs that take advantage of integration with the host CPU to provide increased flexibility for operating system kernel-based performance optimization.We believe that this approach is more likely to meet the needs of current and future high-bandwidth TCP/IP networking on end hosts than the current trend of putting more complexity in the NIC, while avoiding the need to modify applications and protocols. This paper presents two such NICs. The first, the simple integrated NIC (SINIC), is a minimally complex design that moves the responsibility for managing the network FIFOs from the NIC to the kernel. Despite this closer interaction between the kernel and the NIC, SINIC provides performance equivalent to a conventional DMA-based NIC without increasing CPU overhead. The second design, V-SINIC, adds virtual per-packet registers to SINIC, enabling parallel packet processing while maintaining a FIFO model. V-SINIC allows the kernel to decouple examining a packet's header from copying its payload to memory. We exploit this capability to implement a true zero-copy receive optimization in the Linux 2.6 kernel, providing bandwidth improvements of over 50\% on unmodified sockets-based receive-intensive benchmarks.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {315--324},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168897},
 doi = {http://doi.acm.org/10.1145/1168857.1168897},
 acmid = {1168897},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP/IP performance, interfaces, network, zero-copy},
} 

@article{Binkert:2006:INI:1168919.1168897,
 author = {Binkert, Nathan L. and Saidi, Ali G. and Reinhardt, Steven K.},
 title = {Integrated network interfaces for high-bandwidth TCP/IP},
 abstract = {This paper proposes new network interface controller (NIC) designs that take advantage of integration with the host CPU to provide increased flexibility for operating system kernel-based performance optimization.We believe that this approach is more likely to meet the needs of current and future high-bandwidth TCP/IP networking on end hosts than the current trend of putting more complexity in the NIC, while avoiding the need to modify applications and protocols. This paper presents two such NICs. The first, the simple integrated NIC (SINIC), is a minimally complex design that moves the responsibility for managing the network FIFOs from the NIC to the kernel. Despite this closer interaction between the kernel and the NIC, SINIC provides performance equivalent to a conventional DMA-based NIC without increasing CPU overhead. The second design, V-SINIC, adds virtual per-packet registers to SINIC, enabling parallel packet processing while maintaining a FIFO model. V-SINIC allows the kernel to decouple examining a packet's header from copying its payload to memory. We exploit this capability to implement a true zero-copy receive optimization in the Linux 2.6 kernel, providing bandwidth improvements of over 50\% on unmodified sockets-based receive-intensive benchmarks.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {315--324},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168897},
 doi = {http://doi.acm.org/10.1145/1168919.1168897},
 acmid = {1168897},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP/IP performance, interfaces, network, zero-copy},
} 

@article{Tarditi:2006:AUD:1168917.1168898,
 author = {Tarditi, David and Puri, Sidd and Oglesby, Jose},
 title = {Accelerator: using data parallelism to program GPUs for general-purpose uses},
 abstract = {GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls.We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50\% of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {325--335},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168898},
 doi = {http://doi.acm.org/10.1145/1168917.1168898},
 acmid = {1168898},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data parallelism, graphics processing units, just-in time compilation},
} 

@inproceedings{Tarditi:2006:AUD:1168857.1168898,
 author = {Tarditi, David and Puri, Sidd and Oglesby, Jose},
 title = {Accelerator: using data parallelism to program GPUs for general-purpose uses},
 abstract = {GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls.We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50\% of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {325--335},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168898},
 doi = {http://doi.acm.org/10.1145/1168857.1168898},
 acmid = {1168898},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data parallelism, graphics processing units, just-in time compilation},
} 

@article{Tarditi:2006:AUD:1168918.1168898,
 author = {Tarditi, David and Puri, Sidd and Oglesby, Jose},
 title = {Accelerator: using data parallelism to program GPUs for general-purpose uses},
 abstract = {GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls.We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50\% of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {325--335},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168898},
 doi = {http://doi.acm.org/10.1145/1168918.1168898},
 acmid = {1168898},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data parallelism, graphics processing units, just-in time compilation},
} 

@article{Tarditi:2006:AUD:1168919.1168898,
 author = {Tarditi, David and Puri, Sidd and Oglesby, Jose},
 title = {Accelerator: using data parallelism to program GPUs for general-purpose uses},
 abstract = {GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls.We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50\% of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {325--335},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168898},
 doi = {http://doi.acm.org/10.1145/1168919.1168898},
 acmid = {1168898},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data parallelism, graphics processing units, just-in time compilation},
} 

@article{Damron:2006:HTM:1168917.1168900,
 author = {Damron, Peter and Fedorova, Alexandra and Lev, Yossi and Luchangco, Victor and Moir, Mark and Nussbaum, Daniel},
 title = {Hybrid transactional memory},
 abstract = {Transactional memory (TM) promises to substantially reduce the difficulty of writing correct, efficient, and scalable concurrent programs. But "bounded" and "best-effort" hardware TM proposals impose unreasonable constraints on programmers, while more flexible software TM implementations are considered too slow. Proposals for supporting "unbounded" transactions in hardware entail significantly higher complexity and risk than best-effort designs.We introduce Hybrid Transactional Memory</i> (HyTM), an approach to implementing TMin software so that it can use best effort hardware TM (HTM) to boost performance but does not depend</i> on HTM. Thus programmers can develop and test transactional programs in existing systems today, and can enjoy the performance benefits of HTM support when it becomes available.We describe our prototype HyTM system, comprising a compiler and a library. The compiler allows a transaction to be attempted using best-effort HTM, and retried using the software library if it fails. We have used our prototype to "transactify" part of the Berkeley DB system, as well as several benchmarks. By disabling the optional use of HTM, we can run all of these tests on existing systems. Furthermore, by using a simulated multiprocessor with HTM support, we demonstrate the viability of the HyTM approach: it can provide performance and scalability approaching that of an unbounded HTM implementation, without the need to support all transactions with complicated HTM support.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {336--346},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168900},
 doi = {http://doi.acm.org/10.1145/1168917.1168900},
 acmid = {1168900},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {transactional memory},
} 

@article{Damron:2006:HTM:1168919.1168900,
 author = {Damron, Peter and Fedorova, Alexandra and Lev, Yossi and Luchangco, Victor and Moir, Mark and Nussbaum, Daniel},
 title = {Hybrid transactional memory},
 abstract = {Transactional memory (TM) promises to substantially reduce the difficulty of writing correct, efficient, and scalable concurrent programs. But "bounded" and "best-effort" hardware TM proposals impose unreasonable constraints on programmers, while more flexible software TM implementations are considered too slow. Proposals for supporting "unbounded" transactions in hardware entail significantly higher complexity and risk than best-effort designs.We introduce Hybrid Transactional Memory</i> (HyTM), an approach to implementing TMin software so that it can use best effort hardware TM (HTM) to boost performance but does not depend</i> on HTM. Thus programmers can develop and test transactional programs in existing systems today, and can enjoy the performance benefits of HTM support when it becomes available.We describe our prototype HyTM system, comprising a compiler and a library. The compiler allows a transaction to be attempted using best-effort HTM, and retried using the software library if it fails. We have used our prototype to "transactify" part of the Berkeley DB system, as well as several benchmarks. By disabling the optional use of HTM, we can run all of these tests on existing systems. Furthermore, by using a simulated multiprocessor with HTM support, we demonstrate the viability of the HyTM approach: it can provide performance and scalability approaching that of an unbounded HTM implementation, without the need to support all transactions with complicated HTM support.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {336--346},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168900},
 doi = {http://doi.acm.org/10.1145/1168919.1168900},
 acmid = {1168900},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {transactional memory},
} 

@inproceedings{Damron:2006:HTM:1168857.1168900,
 author = {Damron, Peter and Fedorova, Alexandra and Lev, Yossi and Luchangco, Victor and Moir, Mark and Nussbaum, Daniel},
 title = {Hybrid transactional memory},
 abstract = {Transactional memory (TM) promises to substantially reduce the difficulty of writing correct, efficient, and scalable concurrent programs. But "bounded" and "best-effort" hardware TM proposals impose unreasonable constraints on programmers, while more flexible software TM implementations are considered too slow. Proposals for supporting "unbounded" transactions in hardware entail significantly higher complexity and risk than best-effort designs.We introduce Hybrid Transactional Memory</i> (HyTM), an approach to implementing TMin software so that it can use best effort hardware TM (HTM) to boost performance but does not depend</i> on HTM. Thus programmers can develop and test transactional programs in existing systems today, and can enjoy the performance benefits of HTM support when it becomes available.We describe our prototype HyTM system, comprising a compiler and a library. The compiler allows a transaction to be attempted using best-effort HTM, and retried using the software library if it fails. We have used our prototype to "transactify" part of the Berkeley DB system, as well as several benchmarks. By disabling the optional use of HTM, we can run all of these tests on existing systems. Furthermore, by using a simulated multiprocessor with HTM support, we demonstrate the viability of the HyTM approach: it can provide performance and scalability approaching that of an unbounded HTM implementation, without the need to support all transactions with complicated HTM support.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {336--346},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168900},
 doi = {http://doi.acm.org/10.1145/1168857.1168900},
 acmid = {1168900},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {transactional memory},
} 

@article{Damron:2006:HTM:1168918.1168900,
 author = {Damron, Peter and Fedorova, Alexandra and Lev, Yossi and Luchangco, Victor and Moir, Mark and Nussbaum, Daniel},
 title = {Hybrid transactional memory},
 abstract = {Transactional memory (TM) promises to substantially reduce the difficulty of writing correct, efficient, and scalable concurrent programs. But "bounded" and "best-effort" hardware TM proposals impose unreasonable constraints on programmers, while more flexible software TM implementations are considered too slow. Proposals for supporting "unbounded" transactions in hardware entail significantly higher complexity and risk than best-effort designs.We introduce Hybrid Transactional Memory</i> (HyTM), an approach to implementing TMin software so that it can use best effort hardware TM (HTM) to boost performance but does not depend</i> on HTM. Thus programmers can develop and test transactional programs in existing systems today, and can enjoy the performance benefits of HTM support when it becomes available.We describe our prototype HyTM system, comprising a compiler and a library. The compiler allows a transaction to be attempted using best-effort HTM, and retried using the software library if it fails. We have used our prototype to "transactify" part of the Berkeley DB system, as well as several benchmarks. By disabling the optional use of HTM, we can run all of these tests on existing systems. Furthermore, by using a simulated multiprocessor with HTM support, we demonstrate the viability of the HyTM approach: it can provide performance and scalability approaching that of an unbounded HTM implementation, without the need to support all transactions with complicated HTM support.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {336--346},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168900},
 doi = {http://doi.acm.org/10.1145/1168918.1168900},
 acmid = {1168900},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {transactional memory},
} 

@article{Chuang:2006:UPT:1168917.1168901,
 author = {Chuang, Weihaw and Narayanasamy, Satish and Venkatesh, Ganesh and Sampson, Jack and Van Biesbrouck, Michael and Pokam, Gilles and Calder, Brad and Colavin, Osvaldo},
 title = {Unbounded page-based transactional memory},
 abstract = {Exploiting thread level parallelism is paramount in the multicore era. Transactions enable programmers to expose such parallelism by greatly simplifying the multi-threaded programming model. Virtualized transactions (unbounded in space and time) are desirable, as they can increase the scope of transactions' use, and thereby further simplify a programmer's job. However, hardware support is essential to support efficient execution of unbounded transactions. In this paper, we introduce Page-based Transactional Memory</i> to support unbounded transactions. We combine transaction bookkeeping with the virtual memory system to support fast transaction conflict detection, commit, abort, and to maintain transactions' speculative data.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168901},
 doi = {http://doi.acm.org/10.1145/1168917.1168901},
 acmid = {1168901},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, parallel programming, transactional memory, transactions, virtual memory},
} 

@article{Chuang:2006:UPT:1168919.1168901,
 author = {Chuang, Weihaw and Narayanasamy, Satish and Venkatesh, Ganesh and Sampson, Jack and Van Biesbrouck, Michael and Pokam, Gilles and Calder, Brad and Colavin, Osvaldo},
 title = {Unbounded page-based transactional memory},
 abstract = {Exploiting thread level parallelism is paramount in the multicore era. Transactions enable programmers to expose such parallelism by greatly simplifying the multi-threaded programming model. Virtualized transactions (unbounded in space and time) are desirable, as they can increase the scope of transactions' use, and thereby further simplify a programmer's job. However, hardware support is essential to support efficient execution of unbounded transactions. In this paper, we introduce Page-based Transactional Memory</i> to support unbounded transactions. We combine transaction bookkeeping with the virtual memory system to support fast transaction conflict detection, commit, abort, and to maintain transactions' speculative data.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168901},
 doi = {http://doi.acm.org/10.1145/1168919.1168901},
 acmid = {1168901},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, parallel programming, transactional memory, transactions, virtual memory},
} 

@inproceedings{Chuang:2006:UPT:1168857.1168901,
 author = {Chuang, Weihaw and Narayanasamy, Satish and Venkatesh, Ganesh and Sampson, Jack and Van Biesbrouck, Michael and Pokam, Gilles and Calder, Brad and Colavin, Osvaldo},
 title = {Unbounded page-based transactional memory},
 abstract = {Exploiting thread level parallelism is paramount in the multicore era. Transactions enable programmers to expose such parallelism by greatly simplifying the multi-threaded programming model. Virtualized transactions (unbounded in space and time) are desirable, as they can increase the scope of transactions' use, and thereby further simplify a programmer's job. However, hardware support is essential to support efficient execution of unbounded transactions. In this paper, we introduce Page-based Transactional Memory</i> to support unbounded transactions. We combine transaction bookkeeping with the virtual memory system to support fast transaction conflict detection, commit, abort, and to maintain transactions' speculative data.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168901},
 doi = {http://doi.acm.org/10.1145/1168857.1168901},
 acmid = {1168901},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, parallel programming, transactional memory, transactions, virtual memory},
} 

@article{Chuang:2006:UPT:1168918.1168901,
 author = {Chuang, Weihaw and Narayanasamy, Satish and Venkatesh, Ganesh and Sampson, Jack and Van Biesbrouck, Michael and Pokam, Gilles and Calder, Brad and Colavin, Osvaldo},
 title = {Unbounded page-based transactional memory},
 abstract = {Exploiting thread level parallelism is paramount in the multicore era. Transactions enable programmers to expose such parallelism by greatly simplifying the multi-threaded programming model. Virtualized transactions (unbounded in space and time) are desirable, as they can increase the scope of transactions' use, and thereby further simplify a programmer's job. However, hardware support is essential to support efficient execution of unbounded transactions. In this paper, we introduce Page-based Transactional Memory</i> to support unbounded transactions. We combine transaction bookkeeping with the virtual memory system to support fast transaction conflict detection, commit, abort, and to maintain transactions' speculative data.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168901},
 doi = {http://doi.acm.org/10.1145/1168918.1168901},
 acmid = {1168901},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, parallel programming, transactional memory, transactions, virtual memory},
} 

@article{Moravan:2006:SNT:1168917.1168902,
 author = {Moravan, Michelle J. and Bobba, Jayaram and Moore, Kevin E. and Yen, Luke and Hill, Mark D. and Liblit, Ben and Swift, Michael M. and Wood, David A.},
 title = {Supporting nested transactional memory in logTM},
 abstract = {Nested transactional memory (TM) facilitates software composition by letting one module invoke another without either knowing whether the other uses transactions. <b>Closed nested transactions</b> extend isolation of an inner transaction until the toplevel transaction commits. Implementations may flatten nested transactions into the top-level one, resulting in a complete abort on conflict, or allow partial abort of inner transactions. <b>Open nested transactions</b> allow a committing inner transaction to immediately release isolation, which increases parallelism and expressiveness at the cost of both software and hardware complexity.This paper extends the recently-proposed flat Log-based Transactional Memory (LogTM) with nested transactions. Flat LogTM saves pre-transaction values in a log, detects conflicts with read (R) and write (W) bits per cache block, and, on abort, invokes a software handler to unroll the log. Nested LogTM supports nesting by segmenting the log into a stack of activation records and modestly replicating R/W bits. To facilitate composition with nontransactional code, such as language runtime and operating system services, we propose <b>escape actions</b> that allow trusted code to run outside the confines of the transactional memory system.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168902},
 doi = {http://doi.acm.org/10.1145/1168917.1168902},
 acmid = {1168902},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {logTM, nesting, transactional memory},
} 

@inproceedings{Moravan:2006:SNT:1168857.1168902,
 author = {Moravan, Michelle J. and Bobba, Jayaram and Moore, Kevin E. and Yen, Luke and Hill, Mark D. and Liblit, Ben and Swift, Michael M. and Wood, David A.},
 title = {Supporting nested transactional memory in logTM},
 abstract = {Nested transactional memory (TM) facilitates software composition by letting one module invoke another without either knowing whether the other uses transactions. <b>Closed nested transactions</b> extend isolation of an inner transaction until the toplevel transaction commits. Implementations may flatten nested transactions into the top-level one, resulting in a complete abort on conflict, or allow partial abort of inner transactions. <b>Open nested transactions</b> allow a committing inner transaction to immediately release isolation, which increases parallelism and expressiveness at the cost of both software and hardware complexity.This paper extends the recently-proposed flat Log-based Transactional Memory (LogTM) with nested transactions. Flat LogTM saves pre-transaction values in a log, detects conflicts with read (R) and write (W) bits per cache block, and, on abort, invokes a software handler to unroll the log. Nested LogTM supports nesting by segmenting the log into a stack of activation records and modestly replicating R/W bits. To facilitate composition with nontransactional code, such as language runtime and operating system services, we propose <b>escape actions</b> that allow trusted code to run outside the confines of the transactional memory system.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168902},
 doi = {http://doi.acm.org/10.1145/1168857.1168902},
 acmid = {1168902},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {logTM, nesting, transactional memory},
} 

@article{Moravan:2006:SNT:1168919.1168902,
 author = {Moravan, Michelle J. and Bobba, Jayaram and Moore, Kevin E. and Yen, Luke and Hill, Mark D. and Liblit, Ben and Swift, Michael M. and Wood, David A.},
 title = {Supporting nested transactional memory in logTM},
 abstract = {Nested transactional memory (TM) facilitates software composition by letting one module invoke another without either knowing whether the other uses transactions. <b>Closed nested transactions</b> extend isolation of an inner transaction until the toplevel transaction commits. Implementations may flatten nested transactions into the top-level one, resulting in a complete abort on conflict, or allow partial abort of inner transactions. <b>Open nested transactions</b> allow a committing inner transaction to immediately release isolation, which increases parallelism and expressiveness at the cost of both software and hardware complexity.This paper extends the recently-proposed flat Log-based Transactional Memory (LogTM) with nested transactions. Flat LogTM saves pre-transaction values in a log, detects conflicts with read (R) and write (W) bits per cache block, and, on abort, invokes a software handler to unroll the log. Nested LogTM supports nesting by segmenting the log into a stack of activation records and modestly replicating R/W bits. To facilitate composition with nontransactional code, such as language runtime and operating system services, we propose <b>escape actions</b> that allow trusted code to run outside the confines of the transactional memory system.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168902},
 doi = {http://doi.acm.org/10.1145/1168919.1168902},
 acmid = {1168902},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {logTM, nesting, transactional memory},
} 

@article{Moravan:2006:SNT:1168918.1168902,
 author = {Moravan, Michelle J. and Bobba, Jayaram and Moore, Kevin E. and Yen, Luke and Hill, Mark D. and Liblit, Ben and Swift, Michael M. and Wood, David A.},
 title = {Supporting nested transactional memory in logTM},
 abstract = {Nested transactional memory (TM) facilitates software composition by letting one module invoke another without either knowing whether the other uses transactions. <b>Closed nested transactions</b> extend isolation of an inner transaction until the toplevel transaction commits. Implementations may flatten nested transactions into the top-level one, resulting in a complete abort on conflict, or allow partial abort of inner transactions. <b>Open nested transactions</b> allow a committing inner transaction to immediately release isolation, which increases parallelism and expressiveness at the cost of both software and hardware complexity.This paper extends the recently-proposed flat Log-based Transactional Memory (LogTM) with nested transactions. Flat LogTM saves pre-transaction values in a log, detects conflicts with read (R) and write (W) bits per cache block, and, on abort, invokes a software handler to unroll the log. Nested LogTM supports nesting by segmenting the log into a stack of activation records and modestly replicating R/W bits. To facilitate composition with nontransactional code, such as language runtime and operating system services, we propose <b>escape actions</b> that allow trusted code to run outside the confines of the transactional memory system.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168902},
 doi = {http://doi.acm.org/10.1145/1168918.1168902},
 acmid = {1168902},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {logTM, nesting, transactional memory},
} 

@article{Chung:2006:TTM:1168917.1168903,
 author = {Chung, JaeWoong and Minh, Chi Cao and McDonald, Austen and Skare, Travis and Chafi, Hassan and Carlstrom, Brian D. and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Tradeoffs in transactional memory virtualization},
 abstract = {For transactional memory (TM) to achieve widespread acceptance, transactions should not be limited to the physical resources of any specific hardware implementation. TM systems should guarantee correct execution even when transactions exceed scheduling quanta, overflow the capacity of hardware caches and physical memory, or include more independent nesting levels than what is supported in hardware. Existing proposals for TM virtualization are either incomplete or rely on complex hardware implementations, which are an overkill if virtualization is invoked infrequently in the common case.We present eXtended Transactional Memory</i> (XTM), the first TM virtualization system that virtualizes all aspects of transactional execution (time, space, and nesting depth). XTM is implemented in software using virtual memory support. It operates at page granularity, using private copies of overflowed pages to buffer memory updates until the transaction commits and snapshots of pages to detect interference between transactions. We also describe two enhancements to XTM that use limited hardware support to address key performance bottlenecks.We compare XTM to hardwarebased virtualization using both real applications and synthetic microbenchmarks. We show that despite being software-based, XTM and its enhancements are competitive with hardware-based alternatives. Overall, we demonstrate that XTM provides a complete, flexible, and low-cost mechanism for practical TM virtualization.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {371--381},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168917.1168903},
 doi = {http://doi.acm.org/10.1145/1168917.1168903},
 acmid = {1168903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OS support, chip multi-processor, transactional memory, virtualization},
} 

@article{Chung:2006:TTM:1168918.1168903,
 author = {Chung, JaeWoong and Minh, Chi Cao and McDonald, Austen and Skare, Travis and Chafi, Hassan and Carlstrom, Brian D. and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Tradeoffs in transactional memory virtualization},
 abstract = {For transactional memory (TM) to achieve widespread acceptance, transactions should not be limited to the physical resources of any specific hardware implementation. TM systems should guarantee correct execution even when transactions exceed scheduling quanta, overflow the capacity of hardware caches and physical memory, or include more independent nesting levels than what is supported in hardware. Existing proposals for TM virtualization are either incomplete or rely on complex hardware implementations, which are an overkill if virtualization is invoked infrequently in the common case.We present eXtended Transactional Memory</i> (XTM), the first TM virtualization system that virtualizes all aspects of transactional execution (time, space, and nesting depth). XTM is implemented in software using virtual memory support. It operates at page granularity, using private copies of overflowed pages to buffer memory updates until the transaction commits and snapshots of pages to detect interference between transactions. We also describe two enhancements to XTM that use limited hardware support to address key performance bottlenecks.We compare XTM to hardwarebased virtualization using both real applications and synthetic microbenchmarks. We show that despite being software-based, XTM and its enhancements are competitive with hardware-based alternatives. Overall, we demonstrate that XTM provides a complete, flexible, and low-cost mechanism for practical TM virtualization.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {371--381},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168918.1168903},
 doi = {http://doi.acm.org/10.1145/1168918.1168903},
 acmid = {1168903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OS support, chip multi-processor, transactional memory, virtualization},
} 

@inproceedings{Chung:2006:TTM:1168857.1168903,
 author = {Chung, JaeWoong and Minh, Chi Cao and McDonald, Austen and Skare, Travis and Chafi, Hassan and Carlstrom, Brian D. and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Tradeoffs in transactional memory virtualization},
 abstract = {For transactional memory (TM) to achieve widespread acceptance, transactions should not be limited to the physical resources of any specific hardware implementation. TM systems should guarantee correct execution even when transactions exceed scheduling quanta, overflow the capacity of hardware caches and physical memory, or include more independent nesting levels than what is supported in hardware. Existing proposals for TM virtualization are either incomplete or rely on complex hardware implementations, which are an overkill if virtualization is invoked infrequently in the common case.We present eXtended Transactional Memory</i> (XTM), the first TM virtualization system that virtualizes all aspects of transactional execution (time, space, and nesting depth). XTM is implemented in software using virtual memory support. It operates at page granularity, using private copies of overflowed pages to buffer memory updates until the transaction commits and snapshots of pages to detect interference between transactions. We also describe two enhancements to XTM that use limited hardware support to address key performance bottlenecks.We compare XTM to hardwarebased virtualization using both real applications and synthetic microbenchmarks. We show that despite being software-based, XTM and its enhancements are competitive with hardware-based alternatives. Overall, we demonstrate that XTM provides a complete, flexible, and low-cost mechanism for practical TM virtualization.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {371--381},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168857.1168903},
 doi = {http://doi.acm.org/10.1145/1168857.1168903},
 acmid = {1168903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OS support, chip multi-processor, transactional memory, virtualization},
} 

@article{Chung:2006:TTM:1168919.1168903,
 author = {Chung, JaeWoong and Minh, Chi Cao and McDonald, Austen and Skare, Travis and Chafi, Hassan and Carlstrom, Brian D. and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Tradeoffs in transactional memory virtualization},
 abstract = {For transactional memory (TM) to achieve widespread acceptance, transactions should not be limited to the physical resources of any specific hardware implementation. TM systems should guarantee correct execution even when transactions exceed scheduling quanta, overflow the capacity of hardware caches and physical memory, or include more independent nesting levels than what is supported in hardware. Existing proposals for TM virtualization are either incomplete or rely on complex hardware implementations, which are an overkill if virtualization is invoked infrequently in the common case.We present eXtended Transactional Memory</i> (XTM), the first TM virtualization system that virtualizes all aspects of transactional execution (time, space, and nesting depth). XTM is implemented in software using virtual memory support. It operates at page granularity, using private copies of overflowed pages to buffer memory updates until the transaction commits and snapshots of pages to detect interference between transactions. We also describe two enhancements to XTM that use limited hardware support to address key performance bottlenecks.We compare XTM to hardwarebased virtualization using both real applications and synthetic microbenchmarks. We show that despite being software-based, XTM and its enhancements are competitive with hardware-based alternatives. Overall, we demonstrate that XTM provides a complete, flexible, and low-cost mechanism for practical TM virtualization.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {371--381},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1168919.1168903},
 doi = {http://doi.acm.org/10.1145/1168919.1168903},
 acmid = {1168903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OS support, chip multi-processor, transactional memory, virtualization},
} 

@article{Kawahito:2006:NIR:1168917.1168905,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Moriyama, Takao and Inoue, Hiroshi and Nakatani, Toshio},
 title = {A new idiom recognition framework for exploiting hardware-assist instructions},
 abstract = {Modern processors support hardware-assist instructions (such as TRT and TROT instructions on IBM zSeries) to accelerate certain functions such as delimiter search and character conversion. Such special instructions have often been used in high performance libraries, but they have not been exploited well in optimizing compilers except for some limited cases. We propose a new idiom recognition technique derived from a topological embedding algorithm [4] to detect idiom patterns in the input program more aggressively than in previous approaches. Our approach can detect a pattern even if the code segment does not exactly match the idiom. For example, we can detect a code segment that includes additional code within the idiom pattern. We implemented our new idiom recognition approach based on the Java Just-In-Time (JIT) compiler that is part of the J9 Java Virtual Machine, and we supported several important idioms for special hardware-assist instructions on the IBM zSeries and on some models of the IBM pSeries. To demonstrate the effectiveness of our technique, we performed two experiments. The first one is to see how many more patterns we can detect compared to the previous approach. The second one is to see how much performance improvement we can achieve over the previous approach. For the first experiment, we used the Java Compatibility Kit (JCK) API tests. For the second one we used IBM XML parser, SPECjvm98, and SPCjbb2000. In summary, relative to a baseline implementation using exact pattern matching, our algorithm converted 75\% more loops in JCK tests. We also observed significant performance improvement of the XML parser by 64\%, of SPECjvm98 by 1\%, and of SPECjbb2000 by 2\% on average on a z990. Finally, we observed the JIT compilation time increases by only 0.32\% to 0.44\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {382--393},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168905},
 doi = {http://doi.acm.org/10.1145/1168917.1168905},
 acmid = {1168905},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, VMX, hardware-assist instructions, idiom recognition, java, topological embedding},
} 

@article{Kawahito:2006:NIR:1168919.1168905,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Moriyama, Takao and Inoue, Hiroshi and Nakatani, Toshio},
 title = {A new idiom recognition framework for exploiting hardware-assist instructions},
 abstract = {Modern processors support hardware-assist instructions (such as TRT and TROT instructions on IBM zSeries) to accelerate certain functions such as delimiter search and character conversion. Such special instructions have often been used in high performance libraries, but they have not been exploited well in optimizing compilers except for some limited cases. We propose a new idiom recognition technique derived from a topological embedding algorithm [4] to detect idiom patterns in the input program more aggressively than in previous approaches. Our approach can detect a pattern even if the code segment does not exactly match the idiom. For example, we can detect a code segment that includes additional code within the idiom pattern. We implemented our new idiom recognition approach based on the Java Just-In-Time (JIT) compiler that is part of the J9 Java Virtual Machine, and we supported several important idioms for special hardware-assist instructions on the IBM zSeries and on some models of the IBM pSeries. To demonstrate the effectiveness of our technique, we performed two experiments. The first one is to see how many more patterns we can detect compared to the previous approach. The second one is to see how much performance improvement we can achieve over the previous approach. For the first experiment, we used the Java Compatibility Kit (JCK) API tests. For the second one we used IBM XML parser, SPECjvm98, and SPCjbb2000. In summary, relative to a baseline implementation using exact pattern matching, our algorithm converted 75\% more loops in JCK tests. We also observed significant performance improvement of the XML parser by 64\%, of SPECjvm98 by 1\%, and of SPECjbb2000 by 2\% on average on a z990. Finally, we observed the JIT compilation time increases by only 0.32\% to 0.44\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {382--393},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168905},
 doi = {http://doi.acm.org/10.1145/1168919.1168905},
 acmid = {1168905},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, VMX, hardware-assist instructions, idiom recognition, java, topological embedding},
} 

@article{Kawahito:2006:NIR:1168918.1168905,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Moriyama, Takao and Inoue, Hiroshi and Nakatani, Toshio},
 title = {A new idiom recognition framework for exploiting hardware-assist instructions},
 abstract = {Modern processors support hardware-assist instructions (such as TRT and TROT instructions on IBM zSeries) to accelerate certain functions such as delimiter search and character conversion. Such special instructions have often been used in high performance libraries, but they have not been exploited well in optimizing compilers except for some limited cases. We propose a new idiom recognition technique derived from a topological embedding algorithm [4] to detect idiom patterns in the input program more aggressively than in previous approaches. Our approach can detect a pattern even if the code segment does not exactly match the idiom. For example, we can detect a code segment that includes additional code within the idiom pattern. We implemented our new idiom recognition approach based on the Java Just-In-Time (JIT) compiler that is part of the J9 Java Virtual Machine, and we supported several important idioms for special hardware-assist instructions on the IBM zSeries and on some models of the IBM pSeries. To demonstrate the effectiveness of our technique, we performed two experiments. The first one is to see how many more patterns we can detect compared to the previous approach. The second one is to see how much performance improvement we can achieve over the previous approach. For the first experiment, we used the Java Compatibility Kit (JCK) API tests. For the second one we used IBM XML parser, SPECjvm98, and SPCjbb2000. In summary, relative to a baseline implementation using exact pattern matching, our algorithm converted 75\% more loops in JCK tests. We also observed significant performance improvement of the XML parser by 64\%, of SPECjvm98 by 1\%, and of SPECjbb2000 by 2\% on average on a z990. Finally, we observed the JIT compilation time increases by only 0.32\% to 0.44\%.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {382--393},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168905},
 doi = {http://doi.acm.org/10.1145/1168918.1168905},
 acmid = {1168905},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, VMX, hardware-assist instructions, idiom recognition, java, topological embedding},
} 

@inproceedings{Kawahito:2006:NIR:1168857.1168905,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Moriyama, Takao and Inoue, Hiroshi and Nakatani, Toshio},
 title = {A new idiom recognition framework for exploiting hardware-assist instructions},
 abstract = {Modern processors support hardware-assist instructions (such as TRT and TROT instructions on IBM zSeries) to accelerate certain functions such as delimiter search and character conversion. Such special instructions have often been used in high performance libraries, but they have not been exploited well in optimizing compilers except for some limited cases. We propose a new idiom recognition technique derived from a topological embedding algorithm [4] to detect idiom patterns in the input program more aggressively than in previous approaches. Our approach can detect a pattern even if the code segment does not exactly match the idiom. For example, we can detect a code segment that includes additional code within the idiom pattern. We implemented our new idiom recognition approach based on the Java Just-In-Time (JIT) compiler that is part of the J9 Java Virtual Machine, and we supported several important idioms for special hardware-assist instructions on the IBM zSeries and on some models of the IBM pSeries. To demonstrate the effectiveness of our technique, we performed two experiments. The first one is to see how many more patterns we can detect compared to the previous approach. The second one is to see how much performance improvement we can achieve over the previous approach. For the first experiment, we used the Java Compatibility Kit (JCK) API tests. For the second one we used IBM XML parser, SPECjvm98, and SPCjbb2000. In summary, relative to a baseline implementation using exact pattern matching, our algorithm converted 75\% more loops in JCK tests. We also observed significant performance improvement of the XML parser by 64\%, of SPECjvm98 by 1\%, and of SPECjbb2000 by 2\% on average on a z990. Finally, we observed the JIT compilation time increases by only 0.32\% to 0.44\%.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {382--393},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168905},
 doi = {http://doi.acm.org/10.1145/1168857.1168905},
 acmid = {1168905},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, VMX, hardware-assist instructions, idiom recognition, java, topological embedding},
} 

@article{Bansal:2006:AGP:1168918.1168906,
 author = {Bansal, Sorav and Aiken, Alex},
 title = {Automatic generation of peephole superoptimizers},
 abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {394--403},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168906},
 doi = {http://doi.acm.org/10.1145/1168918.1168906},
 acmid = {1168906},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code selection, peephole optimization, superoptimization},
} 

@inproceedings{Bansal:2006:AGP:1168857.1168906,
 author = {Bansal, Sorav and Aiken, Alex},
 title = {Automatic generation of peephole superoptimizers},
 abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {394--403},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168906},
 doi = {http://doi.acm.org/10.1145/1168857.1168906},
 acmid = {1168906},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code selection, peephole optimization, superoptimization},
} 

@article{Bansal:2006:AGP:1168919.1168906,
 author = {Bansal, Sorav and Aiken, Alex},
 title = {Automatic generation of peephole superoptimizers},
 abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {394--403},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168906},
 doi = {http://doi.acm.org/10.1145/1168919.1168906},
 acmid = {1168906},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code selection, peephole optimization, superoptimization},
} 

@article{Bansal:2006:AGP:1168917.1168906,
 author = {Bansal, Sorav and Aiken, Alex},
 title = {Automatic generation of peephole superoptimizers},
 abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {394--403},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168906},
 doi = {http://doi.acm.org/10.1145/1168917.1168906},
 acmid = {1168906},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code selection, peephole optimization, superoptimization},
} 

@article{Solar-Lezama:2006:CSF:1168917.1168907,
 author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
 title = {Combinatorial sketching for finite programs},
 abstract = {Sketching is a software synthesis approach where the programmer develops a partial implementation - a sketch - and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the specification. The correctness of the synthesized implementation is guaranteed by the compiler, which allows, among other benefits, rapid development of highly tuned implementations without the fear of introducing bugs.We develop SKETCH, a language for finite programs with linguistic support for sketching. Finite programs include many highperformance kernels, including cryptocodes. In contrast to prior synthesizers, which had to be equipped with domain-specific rules, SKETCH completes sketches by means of a combinatorial search based on generalized boolean satisfiability. Consequently, our combinatorial synthesizer is complete for the class of finite programs: it is guaranteed to complete any sketch in theory, and in practice has scaled to realistic programming problems.Freed from domain rules, we can now write sketches as simpleto-understand partial programs, which are regular programs in which difficult code fragments are replaced with holes</i> to be filled by the synthesizer. Holes may stand for index expressions, lookup tables, or bitmasks, but the programmer can easily define new kinds of holes using a single versatile synthesis operator.We have used SKETCH to synthesize an efficient implementation of the AES cipher standard. The synthesizer produces the most complex part of the implementation and runs in about an hour.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {404--415},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168917.1168907},
 doi = {http://doi.acm.org/10.1145/1168917.1168907},
 acmid = {1168907},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, sketching},
} 

@inproceedings{Solar-Lezama:2006:CSF:1168857.1168907,
 author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
 title = {Combinatorial sketching for finite programs},
 abstract = {Sketching is a software synthesis approach where the programmer develops a partial implementation - a sketch - and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the specification. The correctness of the synthesized implementation is guaranteed by the compiler, which allows, among other benefits, rapid development of highly tuned implementations without the fear of introducing bugs.We develop SKETCH, a language for finite programs with linguistic support for sketching. Finite programs include many highperformance kernels, including cryptocodes. In contrast to prior synthesizers, which had to be equipped with domain-specific rules, SKETCH completes sketches by means of a combinatorial search based on generalized boolean satisfiability. Consequently, our combinatorial synthesizer is complete for the class of finite programs: it is guaranteed to complete any sketch in theory, and in practice has scaled to realistic programming problems.Freed from domain rules, we can now write sketches as simpleto-understand partial programs, which are regular programs in which difficult code fragments are replaced with holes</i> to be filled by the synthesizer. Holes may stand for index expressions, lookup tables, or bitmasks, but the programmer can easily define new kinds of holes using a single versatile synthesis operator.We have used SKETCH to synthesize an efficient implementation of the AES cipher standard. The synthesizer produces the most complex part of the implementation and runs in about an hour.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {404--415},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168907},
 doi = {http://doi.acm.org/10.1145/1168857.1168907},
 acmid = {1168907},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, sketching},
} 

@article{Solar-Lezama:2006:CSF:1168918.1168907,
 author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
 title = {Combinatorial sketching for finite programs},
 abstract = {Sketching is a software synthesis approach where the programmer develops a partial implementation - a sketch - and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the specification. The correctness of the synthesized implementation is guaranteed by the compiler, which allows, among other benefits, rapid development of highly tuned implementations without the fear of introducing bugs.We develop SKETCH, a language for finite programs with linguistic support for sketching. Finite programs include many highperformance kernels, including cryptocodes. In contrast to prior synthesizers, which had to be equipped with domain-specific rules, SKETCH completes sketches by means of a combinatorial search based on generalized boolean satisfiability. Consequently, our combinatorial synthesizer is complete for the class of finite programs: it is guaranteed to complete any sketch in theory, and in practice has scaled to realistic programming problems.Freed from domain rules, we can now write sketches as simpleto-understand partial programs, which are regular programs in which difficult code fragments are replaced with holes</i> to be filled by the synthesizer. Holes may stand for index expressions, lookup tables, or bitmasks, but the programmer can easily define new kinds of holes using a single versatile synthesis operator.We have used SKETCH to synthesize an efficient implementation of the AES cipher standard. The synthesizer produces the most complex part of the implementation and runs in about an hour.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {404--415},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168918.1168907},
 doi = {http://doi.acm.org/10.1145/1168918.1168907},
 acmid = {1168907},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, sketching},
} 

@article{Solar-Lezama:2006:CSF:1168919.1168907,
 author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
 title = {Combinatorial sketching for finite programs},
 abstract = {Sketching is a software synthesis approach where the programmer develops a partial implementation - a sketch - and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the specification. The correctness of the synthesized implementation is guaranteed by the compiler, which allows, among other benefits, rapid development of highly tuned implementations without the fear of introducing bugs.We develop SKETCH, a language for finite programs with linguistic support for sketching. Finite programs include many highperformance kernels, including cryptocodes. In contrast to prior synthesizers, which had to be equipped with domain-specific rules, SKETCH completes sketches by means of a combinatorial search based on generalized boolean satisfiability. Consequently, our combinatorial synthesizer is complete for the class of finite programs: it is guaranteed to complete any sketch in theory, and in practice has scaled to realistic programming problems.Freed from domain rules, we can now write sketches as simpleto-understand partial programs, which are regular programs in which difficult code fragments are replaced with holes</i> to be filled by the synthesizer. Holes may stand for index expressions, lookup tables, or bitmasks, but the programmer can easily define new kinds of holes using a single versatile synthesis operator.We have used SKETCH to synthesize an efficient implementation of the AES cipher standard. The synthesizer produces the most complex part of the implementation and runs in about an hour.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {404--415},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168919.1168907},
 doi = {http://doi.acm.org/10.1145/1168919.1168907},
 acmid = {1168907},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SAT, sketching},
} 

@inproceedings{Da Silva:2006:PPA:1168857.1168908,
 author = {Da Silva, Jeff and Steffan, J. Gregory},
 title = {A probabilistic pointer analysis for speculative optimizations},
 abstract = {Pointer analysis is a critical compiler analysis used to disambiguate the indirect memory references that result from the use of pointers and pointer-based data structures. A conventional pointer analysis deduces for every pair of pointers, at any program point, whether a points-to relation between them (i) definitely</i> exists, (ii) definitely does not</i> exist, or (iii) maybe</i> exists. Many compiler optimizations rely on accurate pointer analysis, and to ensure correctness cannot optimize in the maybe</i> case. In contrast, recently-proposed speculative optimizations</i> can aggressively exploit the maybe</i> case, especially if the likelihood that two pointers alias can be quantified. This paper proposes a Probabilistic Pointer Analysis</i> (PPA) algorithm that statically predicts the probability of each points-to relation at every program point. Building on simple control-flow edge profiling, our analysis is both one-level context and flow sensitive-yet can still scale to large programs including the SPEC 2000 integer benchmark suite. The key to our approach is to compute points-to probabilities through the use of linear transfer functions that are efficiently encoded as sparse matrices.We demonstrate that our analysis can provide accurate probabilities, even without edge-profile information. We also find that-even without considering probability information-our analysis provides an accurate approach to performing pointer analysis.},
 booktitle = {Proceedings of the 12th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {416--425},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168857.1168908},
 doi = {http://doi.acm.org/10.1145/1168857.1168908},
 acmid = {1168908},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependence analysis, pointer analysis, speculative optimization},
} 

@article{Da Silva:2006:PPA:1168919.1168908,
 author = {Da Silva, Jeff and Steffan, J. Gregory},
 title = {A probabilistic pointer analysis for speculative optimizations},
 abstract = {Pointer analysis is a critical compiler analysis used to disambiguate the indirect memory references that result from the use of pointers and pointer-based data structures. A conventional pointer analysis deduces for every pair of pointers, at any program point, whether a points-to relation between them (i) definitely</i> exists, (ii) definitely does not</i> exist, or (iii) maybe</i> exists. Many compiler optimizations rely on accurate pointer analysis, and to ensure correctness cannot optimize in the maybe</i> case. In contrast, recently-proposed speculative optimizations</i> can aggressively exploit the maybe</i> case, especially if the likelihood that two pointers alias can be quantified. This paper proposes a Probabilistic Pointer Analysis</i> (PPA) algorithm that statically predicts the probability of each points-to relation at every program point. Building on simple control-flow edge profiling, our analysis is both one-level context and flow sensitive-yet can still scale to large programs including the SPEC 2000 integer benchmark suite. The key to our approach is to compute points-to probabilities through the use of linear transfer functions that are efficiently encoded as sparse matrices.We demonstrate that our analysis can provide accurate probabilities, even without edge-profile information. We also find that-even without considering probability information-our analysis provides an accurate approach to performing pointer analysis.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {34},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5964},
 pages = {416--425},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168919.1168908},
 doi = {http://doi.acm.org/10.1145/1168919.1168908},
 acmid = {1168908},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependence analysis, pointer analysis, speculative optimization},
} 

@article{Da Silva:2006:PPA:1168917.1168908,
 author = {Da Silva, Jeff and Steffan, J. Gregory},
 title = {A probabilistic pointer analysis for speculative optimizations},
 abstract = {Pointer analysis is a critical compiler analysis used to disambiguate the indirect memory references that result from the use of pointers and pointer-based data structures. A conventional pointer analysis deduces for every pair of pointers, at any program point, whether a points-to relation between them (i) definitely</i> exists, (ii) definitely does not</i> exist, or (iii) maybe</i> exists. Many compiler optimizations rely on accurate pointer analysis, and to ensure correctness cannot optimize in the maybe</i> case. In contrast, recently-proposed speculative optimizations</i> can aggressively exploit the maybe</i> case, especially if the likelihood that two pointers alias can be quantified. This paper proposes a Probabilistic Pointer Analysis</i> (PPA) algorithm that statically predicts the probability of each points-to relation at every program point. Building on simple control-flow edge profiling, our analysis is both one-level context and flow sensitive-yet can still scale to large programs including the SPEC 2000 integer benchmark suite. The key to our approach is to compute points-to probabilities through the use of linear transfer functions that are efficiently encoded as sparse matrices.We demonstrate that our analysis can provide accurate probabilities, even without edge-profile information. We also find that-even without considering probability information-our analysis provides an accurate approach to performing pointer analysis.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {40},
 issue = {5},
 month = {October},
 year = {2006},
 issn = {0163-5980},
 pages = {416--425},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168917.1168908},
 doi = {http://doi.acm.org/10.1145/1168917.1168908},
 acmid = {1168908},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependence analysis, pointer analysis, speculative optimization},
} 

@article{Da Silva:2006:PPA:1168918.1168908,
 author = {Da Silva, Jeff and Steffan, J. Gregory},
 title = {A probabilistic pointer analysis for speculative optimizations},
 abstract = {Pointer analysis is a critical compiler analysis used to disambiguate the indirect memory references that result from the use of pointers and pointer-based data structures. A conventional pointer analysis deduces for every pair of pointers, at any program point, whether a points-to relation between them (i) definitely</i> exists, (ii) definitely does not</i> exist, or (iii) maybe</i> exists. Many compiler optimizations rely on accurate pointer analysis, and to ensure correctness cannot optimize in the maybe</i> case. In contrast, recently-proposed speculative optimizations</i> can aggressively exploit the maybe</i> case, especially if the likelihood that two pointers alias can be quantified. This paper proposes a Probabilistic Pointer Analysis</i> (PPA) algorithm that statically predicts the probability of each points-to relation at every program point. Building on simple control-flow edge profiling, our analysis is both one-level context and flow sensitive-yet can still scale to large programs including the SPEC 2000 integer benchmark suite. The key to our approach is to compute points-to probabilities through the use of linear transfer functions that are efficiently encoded as sparse matrices.We demonstrate that our analysis can provide accurate probabilities, even without edge-profile information. We also find that-even without considering probability information-our analysis provides an accurate approach to performing pointer analysis.},
 journal = {SIGPLAN Not.},
 volume = {41},
 issue = {11},
 month = {October},
 year = {2006},
 issn = {0362-1340},
 pages = {416--425},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1168918.1168908},
 doi = {http://doi.acm.org/10.1145/1168918.1168908},
 acmid = {1168908},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependence analysis, pointer analysis, speculative optimization},
} 

@article{Hammond:2004:PTC:1037949.1024395,
 author = {Hammond, Lance and Carlstrom, Brian D. and Wong, Vicky and Hertzberg, Ben and Chen, Mike and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Programming with transactional coherence and consistency (TCC)},
 abstract = {Transactional Coherence and Consistency (TCC) offers a way to simplify parallel programming by executing all code within transactions. In TCC systems, transactions serve as the fundamental unit of parallel work, communication and coherence. As each transaction completes, it writes all of its newly produced state to shared memory atomically, while restarting other processors that have speculatively read stale data. With this mechanism, a TCC-based system automatically handles data synchronization correctly, without programmer intervention. To gain the benefits of TCC, programs must be decomposed into transactions. We describe two basic programming language constructs for decomposing programs into transactions, a loop conversion syntax and a general transaction-forking mechanism. With these constructs, writing correct parallel programs requires only small, incremental changes to correct sequential programs. The performance of these programs may then easily be optimized, based on feedback from real program execution, using a few simple techniques.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024395},
 doi = {http://doi.acm.org/10.1145/1037949.1024395},
 acmid = {1024395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feedback optimization, multiprocessor architecture, transactions},
} 

@article{Hammond:2004:PTC:1037187.1024395,
 author = {Hammond, Lance and Carlstrom, Brian D. and Wong, Vicky and Hertzberg, Ben and Chen, Mike and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Programming with transactional coherence and consistency (TCC)},
 abstract = {Transactional Coherence and Consistency (TCC) offers a way to simplify parallel programming by executing all code within transactions. In TCC systems, transactions serve as the fundamental unit of parallel work, communication and coherence. As each transaction completes, it writes all of its newly produced state to shared memory atomically, while restarting other processors that have speculatively read stale data. With this mechanism, a TCC-based system automatically handles data synchronization correctly, without programmer intervention. To gain the benefits of TCC, programs must be decomposed into transactions. We describe two basic programming language constructs for decomposing programs into transactions, a loop conversion syntax and a general transaction-forking mechanism. With these constructs, writing correct parallel programs requires only small, incremental changes to correct sequential programs. The performance of these programs may then easily be optimized, based on feedback from real program execution, using a few simple techniques.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024395},
 doi = {http://doi.acm.org/10.1145/1037187.1024395},
 acmid = {1024395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feedback optimization, multiprocessor architecture, transactions},
} 

@article{Hammond:2004:PTC:1037947.1024395,
 author = {Hammond, Lance and Carlstrom, Brian D. and Wong, Vicky and Hertzberg, Ben and Chen, Mike and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Programming with transactional coherence and consistency (TCC)},
 abstract = {Transactional Coherence and Consistency (TCC) offers a way to simplify parallel programming by executing all code within transactions. In TCC systems, transactions serve as the fundamental unit of parallel work, communication and coherence. As each transaction completes, it writes all of its newly produced state to shared memory atomically, while restarting other processors that have speculatively read stale data. With this mechanism, a TCC-based system automatically handles data synchronization correctly, without programmer intervention. To gain the benefits of TCC, programs must be decomposed into transactions. We describe two basic programming language constructs for decomposing programs into transactions, a loop conversion syntax and a general transaction-forking mechanism. With these constructs, writing correct parallel programs requires only small, incremental changes to correct sequential programs. The performance of these programs may then easily be optimized, based on feedback from real program execution, using a few simple techniques.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024395},
 doi = {http://doi.acm.org/10.1145/1037947.1024395},
 acmid = {1024395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feedback optimization, multiprocessor architecture, transactions},
} 

@inproceedings{Hammond:2004:PTC:1024393.1024395,
 author = {Hammond, Lance and Carlstrom, Brian D. and Wong, Vicky and Hertzberg, Ben and Chen, Mike and Kozyrakis, Christos and Olukotun, Kunle},
 title = {Programming with transactional coherence and consistency (TCC)},
 abstract = {Transactional Coherence and Consistency (TCC) offers a way to simplify parallel programming by executing all code within transactions. In TCC systems, transactions serve as the fundamental unit of parallel work, communication and coherence. As each transaction completes, it writes all of its newly produced state to shared memory atomically, while restarting other processors that have speculatively read stale data. With this mechanism, a TCC-based system automatically handles data synchronization correctly, without programmer intervention. To gain the benefits of TCC, programs must be decomposed into transactions. We describe two basic programming language constructs for decomposing programs into transactions, a loop conversion syntax and a general transaction-forking mechanism. With these constructs, writing correct parallel programs requires only small, incremental changes to correct sequential programs. The performance of these programs may then easily be optimized, based on feedback from real program execution, using a few simple techniques.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024395},
 doi = {http://doi.acm.org/10.1145/1024393.1024395},
 acmid = {1024395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feedback optimization, multiprocessor architecture, transactions},
} 

@article{Budiu:2004:SC:1037187.1024396,
 author = {Budiu, Mihai and Venkataramani, Girish and Chelcea, Tiberiu and Goldstein, Seth Copen},
 title = {Spatial computation},
 abstract = {This paper describes a computer architecture, Spatial Computation</i> (SC), which is based on the translation of high-level language programs directly into hardware structures. SC program implementations are completely distributed, with no centralized control. SC circuits are optimized for wires</i> at the expense of computation units.In this paper we investigate a particular implementation of SC: ASH (Application-Specific Hardware). Under the assumption that computation is cheaper than communication, ASH replicates computation units to simplify interconnect, building a system which uses very simple, completely dedicated communication channels. As a consequence, communication on the datapath never requires arbitration; the only arbitration required is for accessing memory. ASH relies on very simple hardware primitives, using no associative structures, no multiported register files, no scheduling logic, no broadcast, and no clocks. As a consequence, ASH hardware is fast and extremely power efficient.In this work we demonstrate three features of ASH: (1) that such architectures can be built by automatic compilation of C programs; (2) that distributed computation is in some respects fundamentally different from monolithic superscalar processors; and (3) that ASIC implementations of ASH use three orders of magnitude less energy compared to high-end superscalar processors, while being on average only 33\% slower in performance (3.5x worst-case).},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {14--26},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024396},
 doi = {http://doi.acm.org/10.1145/1037187.1024396},
 acmid = {1024396},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application-specific hardware, dataflow machine, low-power, spatial computation},
} 

@article{Budiu:2004:SC:1037949.1024396,
 author = {Budiu, Mihai and Venkataramani, Girish and Chelcea, Tiberiu and Goldstein, Seth Copen},
 title = {Spatial computation},
 abstract = {This paper describes a computer architecture, Spatial Computation</i> (SC), which is based on the translation of high-level language programs directly into hardware structures. SC program implementations are completely distributed, with no centralized control. SC circuits are optimized for wires</i> at the expense of computation units.In this paper we investigate a particular implementation of SC: ASH (Application-Specific Hardware). Under the assumption that computation is cheaper than communication, ASH replicates computation units to simplify interconnect, building a system which uses very simple, completely dedicated communication channels. As a consequence, communication on the datapath never requires arbitration; the only arbitration required is for accessing memory. ASH relies on very simple hardware primitives, using no associative structures, no multiported register files, no scheduling logic, no broadcast, and no clocks. As a consequence, ASH hardware is fast and extremely power efficient.In this work we demonstrate three features of ASH: (1) that such architectures can be built by automatic compilation of C programs; (2) that distributed computation is in some respects fundamentally different from monolithic superscalar processors; and (3) that ASIC implementations of ASH use three orders of magnitude less energy compared to high-end superscalar processors, while being on average only 33\% slower in performance (3.5x worst-case).},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {14--26},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024396},
 doi = {http://doi.acm.org/10.1145/1037949.1024396},
 acmid = {1024396},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application-specific hardware, dataflow machine, low-power, spatial computation},
} 

@article{Budiu:2004:SC:1037947.1024396,
 author = {Budiu, Mihai and Venkataramani, Girish and Chelcea, Tiberiu and Goldstein, Seth Copen},
 title = {Spatial computation},
 abstract = {This paper describes a computer architecture, Spatial Computation</i> (SC), which is based on the translation of high-level language programs directly into hardware structures. SC program implementations are completely distributed, with no centralized control. SC circuits are optimized for wires</i> at the expense of computation units.In this paper we investigate a particular implementation of SC: ASH (Application-Specific Hardware). Under the assumption that computation is cheaper than communication, ASH replicates computation units to simplify interconnect, building a system which uses very simple, completely dedicated communication channels. As a consequence, communication on the datapath never requires arbitration; the only arbitration required is for accessing memory. ASH relies on very simple hardware primitives, using no associative structures, no multiported register files, no scheduling logic, no broadcast, and no clocks. As a consequence, ASH hardware is fast and extremely power efficient.In this work we demonstrate three features of ASH: (1) that such architectures can be built by automatic compilation of C programs; (2) that distributed computation is in some respects fundamentally different from monolithic superscalar processors; and (3) that ASIC implementations of ASH use three orders of magnitude less energy compared to high-end superscalar processors, while being on average only 33\% slower in performance (3.5x worst-case).},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {14--26},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024396},
 doi = {http://doi.acm.org/10.1145/1037947.1024396},
 acmid = {1024396},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application-specific hardware, dataflow machine, low-power, spatial computation},
} 

@inproceedings{Budiu:2004:SC:1024393.1024396,
 author = {Budiu, Mihai and Venkataramani, Girish and Chelcea, Tiberiu and Goldstein, Seth Copen},
 title = {Spatial computation},
 abstract = {This paper describes a computer architecture, Spatial Computation</i> (SC), which is based on the translation of high-level language programs directly into hardware structures. SC program implementations are completely distributed, with no centralized control. SC circuits are optimized for wires</i> at the expense of computation units.In this paper we investigate a particular implementation of SC: ASH (Application-Specific Hardware). Under the assumption that computation is cheaper than communication, ASH replicates computation units to simplify interconnect, building a system which uses very simple, completely dedicated communication channels. As a consequence, communication on the datapath never requires arbitration; the only arbitration required is for accessing memory. ASH relies on very simple hardware primitives, using no associative structures, no multiported register files, no scheduling logic, no broadcast, and no clocks. As a consequence, ASH hardware is fast and extremely power efficient.In this work we demonstrate three features of ASH: (1) that such architectures can be built by automatic compilation of C programs; (2) that distributed computation is in some respects fundamentally different from monolithic superscalar processors; and (3) that ASIC implementations of ASH use three orders of magnitude less energy compared to high-end superscalar processors, while being on average only 33\% slower in performance (3.5x worst-case).},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {14--26},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024396},
 doi = {http://doi.acm.org/10.1145/1024393.1024396},
 acmid = {1024396},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application-specific hardware, dataflow machine, low-power, spatial computation},
} 

@inproceedings{Ekanayake:2004:ULP:1024393.1024397,
 author = {Ekanayake, Virantha and Kelly,IV, Clinton and Manohar, Rajit},
 title = {An ultra low-power processor for sensor networks},
 abstract = {We present a novel processor architecture designed specifically for use in low-power wireless sensor-network nodes. Our sensor network asynchronous processor (SNAP/LE) is based on an asynchronous data-driven 16-bit RISC core with an extremely low-power idle state, and a wakeup response latency on the order of tens of nanoseconds. The processor instruction set is optimized for sensor-network applications, with support for event scheduling, pseudo-random number generation, bitfield operations, and radio/sensor interfaces. SNAP/LE has a hardware event queue and event coprocessors, which allow the processor to avoid the overhead of operating system software (such as task schedulers and external interrupt servicing), while still providing a straightforward programming interface to the designer. The processor can meet performance levels required for data monitoring applications while executing instructions with tens of picojoules of energy.We evaluate the energy consumption of SNAP/LE with several applications representative of the workload found in data-gathering wireless sensor networks. We compare our architecture and software against existing platforms for sensor networks, quantifying both the software and hardware benefits of our approach.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {27--36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1024393.1024397},
 doi = {http://doi.acm.org/10.1145/1024393.1024397},
 acmid = {1024397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous, event-driven, low-energy, picojoule computing, sensor network processor, sensor networks, wireless},
} 

@article{Ekanayake:2004:ULP:1037949.1024397,
 author = {Ekanayake, Virantha and Kelly,IV, Clinton and Manohar, Rajit},
 title = {An ultra low-power processor for sensor networks},
 abstract = {We present a novel processor architecture designed specifically for use in low-power wireless sensor-network nodes. Our sensor network asynchronous processor (SNAP/LE) is based on an asynchronous data-driven 16-bit RISC core with an extremely low-power idle state, and a wakeup response latency on the order of tens of nanoseconds. The processor instruction set is optimized for sensor-network applications, with support for event scheduling, pseudo-random number generation, bitfield operations, and radio/sensor interfaces. SNAP/LE has a hardware event queue and event coprocessors, which allow the processor to avoid the overhead of operating system software (such as task schedulers and external interrupt servicing), while still providing a straightforward programming interface to the designer. The processor can meet performance levels required for data monitoring applications while executing instructions with tens of picojoules of energy.We evaluate the energy consumption of SNAP/LE with several applications representative of the workload found in data-gathering wireless sensor networks. We compare our architecture and software against existing platforms for sensor networks, quantifying both the software and hardware benefits of our approach.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {27--36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037949.1024397},
 doi = {http://doi.acm.org/10.1145/1037949.1024397},
 acmid = {1024397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous, event-driven, low-energy, picojoule computing, sensor network processor, sensor networks, wireless},
} 

@article{Ekanayake:2004:ULP:1037187.1024397,
 author = {Ekanayake, Virantha and Kelly,IV, Clinton and Manohar, Rajit},
 title = {An ultra low-power processor for sensor networks},
 abstract = {We present a novel processor architecture designed specifically for use in low-power wireless sensor-network nodes. Our sensor network asynchronous processor (SNAP/LE) is based on an asynchronous data-driven 16-bit RISC core with an extremely low-power idle state, and a wakeup response latency on the order of tens of nanoseconds. The processor instruction set is optimized for sensor-network applications, with support for event scheduling, pseudo-random number generation, bitfield operations, and radio/sensor interfaces. SNAP/LE has a hardware event queue and event coprocessors, which allow the processor to avoid the overhead of operating system software (such as task schedulers and external interrupt servicing), while still providing a straightforward programming interface to the designer. The processor can meet performance levels required for data monitoring applications while executing instructions with tens of picojoules of energy.We evaluate the energy consumption of SNAP/LE with several applications representative of the workload found in data-gathering wireless sensor networks. We compare our architecture and software against existing platforms for sensor networks, quantifying both the software and hardware benefits of our approach.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {27--36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037187.1024397},
 doi = {http://doi.acm.org/10.1145/1037187.1024397},
 acmid = {1024397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous, event-driven, low-energy, picojoule computing, sensor network processor, sensor networks, wireless},
} 

@article{Ekanayake:2004:ULP:1037947.1024397,
 author = {Ekanayake, Virantha and Kelly,IV, Clinton and Manohar, Rajit},
 title = {An ultra low-power processor for sensor networks},
 abstract = {We present a novel processor architecture designed specifically for use in low-power wireless sensor-network nodes. Our sensor network asynchronous processor (SNAP/LE) is based on an asynchronous data-driven 16-bit RISC core with an extremely low-power idle state, and a wakeup response latency on the order of tens of nanoseconds. The processor instruction set is optimized for sensor-network applications, with support for event scheduling, pseudo-random number generation, bitfield operations, and radio/sensor interfaces. SNAP/LE has a hardware event queue and event coprocessors, which allow the processor to avoid the overhead of operating system software (such as task schedulers and external interrupt servicing), while still providing a straightforward programming interface to the designer. The processor can meet performance levels required for data monitoring applications while executing instructions with tens of picojoules of energy.We evaluate the energy consumption of SNAP/LE with several applications representative of the workload found in data-gathering wireless sensor networks. We compare our architecture and software against existing platforms for sensor networks, quantifying both the software and hardware benefits of our approach.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {27--36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037947.1024397},
 doi = {http://doi.acm.org/10.1145/1037947.1024397},
 acmid = {1024397},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous, event-driven, low-energy, picojoule computing, sensor network processor, sensor networks, wireless},
} 

@article{Lumb:2004:DDR:1037187.1024399,
 author = {Lumb, Christopher R. and Golding, Richard},
 title = {D-SPTF: decentralized request distribution in brick-based storage systems},
 abstract = {Distributed Shortest-Positioning Time First (D-SPTF) is a request distribution protocol for decentralized systems of storage servers. D-SPTF exploits high-speed interconnects to dynamically select which server, among those with a replica, should service each read request. In doing so, it simultaneously balances load, exploits the aggregate cache capacity, and reduces positioning times for cache misses. For network latencies expected in storage clusters (e.g., 10--200\&#956;s), D-SPTF performs as well as would a hypothetical centralized system with the same collection of CPU, cache, and disk resources. Compared to popular decentralized approaches, D-SPTF achieves up to 65\% higher throughput and adapts more cleanly to heterogenous server capabilities.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037187.1024399},
 doi = {http://doi.acm.org/10.1145/1037187.1024399},
 acmid = {1024399},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {brick based storage, decentralized systems, disk scheduling, distributed systems, storage systems},
} 

@article{Lumb:2004:DDR:1037949.1024399,
 author = {Lumb, Christopher R. and Golding, Richard},
 title = {D-SPTF: decentralized request distribution in brick-based storage systems},
 abstract = {Distributed Shortest-Positioning Time First (D-SPTF) is a request distribution protocol for decentralized systems of storage servers. D-SPTF exploits high-speed interconnects to dynamically select which server, among those with a replica, should service each read request. In doing so, it simultaneously balances load, exploits the aggregate cache capacity, and reduces positioning times for cache misses. For network latencies expected in storage clusters (e.g., 10--200\&#956;s), D-SPTF performs as well as would a hypothetical centralized system with the same collection of CPU, cache, and disk resources. Compared to popular decentralized approaches, D-SPTF achieves up to 65\% higher throughput and adapts more cleanly to heterogenous server capabilities.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037949.1024399},
 doi = {http://doi.acm.org/10.1145/1037949.1024399},
 acmid = {1024399},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {brick based storage, decentralized systems, disk scheduling, distributed systems, storage systems},
} 

@inproceedings{Lumb:2004:DDR:1024393.1024399,
 author = {Lumb, Christopher R. and Golding, Richard},
 title = {D-SPTF: decentralized request distribution in brick-based storage systems},
 abstract = {Distributed Shortest-Positioning Time First (D-SPTF) is a request distribution protocol for decentralized systems of storage servers. D-SPTF exploits high-speed interconnects to dynamically select which server, among those with a replica, should service each read request. In doing so, it simultaneously balances load, exploits the aggregate cache capacity, and reduces positioning times for cache misses. For network latencies expected in storage clusters (e.g., 10--200\&#956;s), D-SPTF performs as well as would a hypothetical centralized system with the same collection of CPU, cache, and disk resources. Compared to popular decentralized approaches, D-SPTF achieves up to 65\% higher throughput and adapts more cleanly to heterogenous server capabilities.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1024393.1024399},
 doi = {http://doi.acm.org/10.1145/1024393.1024399},
 acmid = {1024399},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {brick based storage, decentralized systems, disk scheduling, distributed systems, storage systems},
} 

@article{Lumb:2004:DDR:1037947.1024399,
 author = {Lumb, Christopher R. and Golding, Richard},
 title = {D-SPTF: decentralized request distribution in brick-based storage systems},
 abstract = {Distributed Shortest-Positioning Time First (D-SPTF) is a request distribution protocol for decentralized systems of storage servers. D-SPTF exploits high-speed interconnects to dynamically select which server, among those with a replica, should service each read request. In doing so, it simultaneously balances load, exploits the aggregate cache capacity, and reduces positioning times for cache misses. For network latencies expected in storage clusters (e.g., 10--200\&#956;s), D-SPTF performs as well as would a hypothetical centralized system with the same collection of CPU, cache, and disk resources. Compared to popular decentralized approaches, D-SPTF achieves up to 65\% higher throughput and adapts more cleanly to heterogenous server capabilities.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037947.1024399},
 doi = {http://doi.acm.org/10.1145/1037947.1024399},
 acmid = {1024399},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {brick based storage, decentralized systems, disk scheduling, distributed systems, storage systems},
} 

@inproceedings{Saito:2004:FBD:1024393.1024400,
 author = {Saito, Yasushi and Fr{\o}lund, Svend and Veitch, Alistair and Merchant, Arif and Spence, Susan},
 title = {FAB: building distributed enterprise disk arrays from commodity components},
 abstract = {This paper describes the design, implementation, and evaluation of a Federated Array of Bricks (FAB), a distributed disk array that provides the reliability of traditional enterprise arrays with lower cost and better scalability. FAB is built from a collection of bricks</i>, small storage appliances containing commodity disks, CPU, NVRAM, and network interface cards. FAB deploys a new majority-voting-based algorithm to replicate or erasure-code logical blocks across bricks and a reconfiguration algorithm to move data in the background when bricks are added or decommissioned. We argue that voting is practical and necessary for reliable, high-throughput storage systems such as FAB. We have implemented a FAB prototype on a 22-node Linux cluster. This prototype sustains 85MB/second of throughput for a database workload, and 270MB/second for a bulk-read workload. In addition, it can outperform traditional master-slave replication through performance decoupling and can handle brick failures and recoveries smoothly without disturbing client requests.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {48--58},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1024393.1024400},
 doi = {http://doi.acm.org/10.1145/1024393.1024400},
 acmid = {1024400},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consensus, disk array, erasure coding, replication, storage, voting},
} 

@article{Saito:2004:FBD:1037949.1024400,
 author = {Saito, Yasushi and Fr{\o}lund, Svend and Veitch, Alistair and Merchant, Arif and Spence, Susan},
 title = {FAB: building distributed enterprise disk arrays from commodity components},
 abstract = {This paper describes the design, implementation, and evaluation of a Federated Array of Bricks (FAB), a distributed disk array that provides the reliability of traditional enterprise arrays with lower cost and better scalability. FAB is built from a collection of bricks</i>, small storage appliances containing commodity disks, CPU, NVRAM, and network interface cards. FAB deploys a new majority-voting-based algorithm to replicate or erasure-code logical blocks across bricks and a reconfiguration algorithm to move data in the background when bricks are added or decommissioned. We argue that voting is practical and necessary for reliable, high-throughput storage systems such as FAB. We have implemented a FAB prototype on a 22-node Linux cluster. This prototype sustains 85MB/second of throughput for a database workload, and 270MB/second for a bulk-read workload. In addition, it can outperform traditional master-slave replication through performance decoupling and can handle brick failures and recoveries smoothly without disturbing client requests.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {48--58},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037949.1024400},
 doi = {http://doi.acm.org/10.1145/1037949.1024400},
 acmid = {1024400},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consensus, disk array, erasure coding, replication, storage, voting},
} 

@article{Saito:2004:FBD:1037947.1024400,
 author = {Saito, Yasushi and Fr{\o}lund, Svend and Veitch, Alistair and Merchant, Arif and Spence, Susan},
 title = {FAB: building distributed enterprise disk arrays from commodity components},
 abstract = {This paper describes the design, implementation, and evaluation of a Federated Array of Bricks (FAB), a distributed disk array that provides the reliability of traditional enterprise arrays with lower cost and better scalability. FAB is built from a collection of bricks</i>, small storage appliances containing commodity disks, CPU, NVRAM, and network interface cards. FAB deploys a new majority-voting-based algorithm to replicate or erasure-code logical blocks across bricks and a reconfiguration algorithm to move data in the background when bricks are added or decommissioned. We argue that voting is practical and necessary for reliable, high-throughput storage systems such as FAB. We have implemented a FAB prototype on a 22-node Linux cluster. This prototype sustains 85MB/second of throughput for a database workload, and 270MB/second for a bulk-read workload. In addition, it can outperform traditional master-slave replication through performance decoupling and can handle brick failures and recoveries smoothly without disturbing client requests.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {48--58},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037947.1024400},
 doi = {http://doi.acm.org/10.1145/1037947.1024400},
 acmid = {1024400},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consensus, disk array, erasure coding, replication, storage, voting},
} 

@article{Saito:2004:FBD:1037187.1024400,
 author = {Saito, Yasushi and Fr{\o}lund, Svend and Veitch, Alistair and Merchant, Arif and Spence, Susan},
 title = {FAB: building distributed enterprise disk arrays from commodity components},
 abstract = {This paper describes the design, implementation, and evaluation of a Federated Array of Bricks (FAB), a distributed disk array that provides the reliability of traditional enterprise arrays with lower cost and better scalability. FAB is built from a collection of bricks</i>, small storage appliances containing commodity disks, CPU, NVRAM, and network interface cards. FAB deploys a new majority-voting-based algorithm to replicate or erasure-code logical blocks across bricks and a reconfiguration algorithm to move data in the background when bricks are added or decommissioned. We argue that voting is practical and necessary for reliable, high-throughput storage systems such as FAB. We have implemented a FAB prototype on a 22-node Linux cluster. This prototype sustains 85MB/second of throughput for a database workload, and 270MB/second for a bulk-read workload. In addition, it can outperform traditional master-slave replication through performance decoupling and can handle brick failures and recoveries smoothly without disturbing client requests.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {48--58},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037187.1024400},
 doi = {http://doi.acm.org/10.1145/1037187.1024400},
 acmid = {1024400},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consensus, disk array, erasure coding, replication, storage, voting},
} 

@article{Denehy:2004:DSA:1037949.1024401,
 author = {Denehy, Timothy E. and Bent, John and Popovici, Florentina I. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Deconstructing storage arrays},
 abstract = {We introduce Shear, a user-level software tool that characterizes RAID storage arrays. Shear employs a set of controlled algorithms combined with statistical techniques to automatically determine the important properties of a RAID system, including the number of disks, chunk size, level of redundancy, and layout scheme. We illustrate the correctness of Shear by running it upon numerous simulated configurations, and then verify its real-world applicability by running Shear on both software-based and hardware-based RAID systems. Finally, we demonstrate the utility of Shear through three case studies. First, we show how Shear can be used in a storage management environment to verify RAID construction and detect failures. Second, we demonstrate how Shear can be used to extract detailed characteristics about the individual disks within an array. Third, we show how an operating system can use Shear to automatically tune its storage subsystems to specific RAID configurations.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {59--71},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024401},
 doi = {http://doi.acm.org/10.1145/1037949.1024401},
 acmid = {1024401},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, storage},
} 

@inproceedings{Denehy:2004:DSA:1024393.1024401,
 author = {Denehy, Timothy E. and Bent, John and Popovici, Florentina I. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Deconstructing storage arrays},
 abstract = {We introduce Shear, a user-level software tool that characterizes RAID storage arrays. Shear employs a set of controlled algorithms combined with statistical techniques to automatically determine the important properties of a RAID system, including the number of disks, chunk size, level of redundancy, and layout scheme. We illustrate the correctness of Shear by running it upon numerous simulated configurations, and then verify its real-world applicability by running Shear on both software-based and hardware-based RAID systems. Finally, we demonstrate the utility of Shear through three case studies. First, we show how Shear can be used in a storage management environment to verify RAID construction and detect failures. Second, we demonstrate how Shear can be used to extract detailed characteristics about the individual disks within an array. Third, we show how an operating system can use Shear to automatically tune its storage subsystems to specific RAID configurations.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {59--71},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024401},
 doi = {http://doi.acm.org/10.1145/1024393.1024401},
 acmid = {1024401},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, storage},
} 

@article{Denehy:2004:DSA:1037947.1024401,
 author = {Denehy, Timothy E. and Bent, John and Popovici, Florentina I. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Deconstructing storage arrays},
 abstract = {We introduce Shear, a user-level software tool that characterizes RAID storage arrays. Shear employs a set of controlled algorithms combined with statistical techniques to automatically determine the important properties of a RAID system, including the number of disks, chunk size, level of redundancy, and layout scheme. We illustrate the correctness of Shear by running it upon numerous simulated configurations, and then verify its real-world applicability by running Shear on both software-based and hardware-based RAID systems. Finally, we demonstrate the utility of Shear through three case studies. First, we show how Shear can be used in a storage management environment to verify RAID construction and detect failures. Second, we demonstrate how Shear can be used to extract detailed characteristics about the individual disks within an array. Third, we show how an operating system can use Shear to automatically tune its storage subsystems to specific RAID configurations.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {59--71},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024401},
 doi = {http://doi.acm.org/10.1145/1037947.1024401},
 acmid = {1024401},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, storage},
} 

@article{Denehy:2004:DSA:1037187.1024401,
 author = {Denehy, Timothy E. and Bent, John and Popovici, Florentina I. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Deconstructing storage arrays},
 abstract = {We introduce Shear, a user-level software tool that characterizes RAID storage arrays. Shear employs a set of controlled algorithms combined with statistical techniques to automatically determine the important properties of a RAID system, including the number of disks, chunk size, level of redundancy, and layout scheme. We illustrate the correctness of Shear by running it upon numerous simulated configurations, and then verify its real-world applicability by running Shear on both software-based and hardware-based RAID systems. Finally, we demonstrate the utility of Shear through three case studies. First, we show how Shear can be used in a storage management environment to verify RAID construction and detect failures. Second, we demonstrate how Shear can be used to extract detailed characteristics about the individual disks within an array. Third, we show how an operating system can use Shear to automatically tune its storage subsystems to specific RAID configurations.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {59--71},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024401},
 doi = {http://doi.acm.org/10.1145/1037187.1024401},
 acmid = {1024401},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, storage},
} 

@article{Zhuang:2004:HIE:1037949.1024403,
 author = {Zhuang, Xiaotong and Zhang, Tao and Pande, Santosh},
 title = {HIDE: an infrastructure for efficiently protecting information leakage on the address bus},
 abstract = {XOM-based secure processor has recently been introduced as a mechanism to provide copy and tamper resistant execution. XOM provides support for encryption/decryption and integrity checking. However, neither XOM nor any other current approach adequately addresses the problem of information leakage via the address bus. This paper shows that without address bus protection, the XOM model is severely crippled. Two realistic attacks are shown and experiments show that 70\% of the code might be cracked and sensitive data might be exposed leading to serious security breaches.Although the problem of address bus leakage has been widely acknowledged both in industry and academia, no practical solution has ever been proposed that can provide an adequate security guarantee. The main reason is that the problem is very difficult to solve in practice due to severe performance degradation which accompanies most of the solutions. This paper presents an infrastructure called HIDE (Hardware-support for leakage-Immune Dynamic Execution) which provides a solution consisting of chunk-level protection with hardware support and a flexible interface which can be orchestrated through the proposed compiler optimization and user specifications that allow utilizing underlying hardware solution more efficiently to provide better security guarantees.Our results show that protecting both data and code with a high level of security guarantee is possible with negligible performance penalty (1.3\% slowdown).},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {72--84},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024403},
 doi = {http://doi.acm.org/10.1145/1037949.1024403},
 acmid = {1024403},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address bus leakage protection, secure processor},
} 

@article{Zhuang:2004:HIE:1037947.1024403,
 author = {Zhuang, Xiaotong and Zhang, Tao and Pande, Santosh},
 title = {HIDE: an infrastructure for efficiently protecting information leakage on the address bus},
 abstract = {XOM-based secure processor has recently been introduced as a mechanism to provide copy and tamper resistant execution. XOM provides support for encryption/decryption and integrity checking. However, neither XOM nor any other current approach adequately addresses the problem of information leakage via the address bus. This paper shows that without address bus protection, the XOM model is severely crippled. Two realistic attacks are shown and experiments show that 70\% of the code might be cracked and sensitive data might be exposed leading to serious security breaches.Although the problem of address bus leakage has been widely acknowledged both in industry and academia, no practical solution has ever been proposed that can provide an adequate security guarantee. The main reason is that the problem is very difficult to solve in practice due to severe performance degradation which accompanies most of the solutions. This paper presents an infrastructure called HIDE (Hardware-support for leakage-Immune Dynamic Execution) which provides a solution consisting of chunk-level protection with hardware support and a flexible interface which can be orchestrated through the proposed compiler optimization and user specifications that allow utilizing underlying hardware solution more efficiently to provide better security guarantees.Our results show that protecting both data and code with a high level of security guarantee is possible with negligible performance penalty (1.3\% slowdown).},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {72--84},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024403},
 doi = {http://doi.acm.org/10.1145/1037947.1024403},
 acmid = {1024403},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address bus leakage protection, secure processor},
} 

@article{Zhuang:2004:HIE:1037187.1024403,
 author = {Zhuang, Xiaotong and Zhang, Tao and Pande, Santosh},
 title = {HIDE: an infrastructure for efficiently protecting information leakage on the address bus},
 abstract = {XOM-based secure processor has recently been introduced as a mechanism to provide copy and tamper resistant execution. XOM provides support for encryption/decryption and integrity checking. However, neither XOM nor any other current approach adequately addresses the problem of information leakage via the address bus. This paper shows that without address bus protection, the XOM model is severely crippled. Two realistic attacks are shown and experiments show that 70\% of the code might be cracked and sensitive data might be exposed leading to serious security breaches.Although the problem of address bus leakage has been widely acknowledged both in industry and academia, no practical solution has ever been proposed that can provide an adequate security guarantee. The main reason is that the problem is very difficult to solve in practice due to severe performance degradation which accompanies most of the solutions. This paper presents an infrastructure called HIDE (Hardware-support for leakage-Immune Dynamic Execution) which provides a solution consisting of chunk-level protection with hardware support and a flexible interface which can be orchestrated through the proposed compiler optimization and user specifications that allow utilizing underlying hardware solution more efficiently to provide better security guarantees.Our results show that protecting both data and code with a high level of security guarantee is possible with negligible performance penalty (1.3\% slowdown).},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {72--84},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024403},
 doi = {http://doi.acm.org/10.1145/1037187.1024403},
 acmid = {1024403},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address bus leakage protection, secure processor},
} 

@inproceedings{Zhuang:2004:HIE:1024393.1024403,
 author = {Zhuang, Xiaotong and Zhang, Tao and Pande, Santosh},
 title = {HIDE: an infrastructure for efficiently protecting information leakage on the address bus},
 abstract = {XOM-based secure processor has recently been introduced as a mechanism to provide copy and tamper resistant execution. XOM provides support for encryption/decryption and integrity checking. However, neither XOM nor any other current approach adequately addresses the problem of information leakage via the address bus. This paper shows that without address bus protection, the XOM model is severely crippled. Two realistic attacks are shown and experiments show that 70\% of the code might be cracked and sensitive data might be exposed leading to serious security breaches.Although the problem of address bus leakage has been widely acknowledged both in industry and academia, no practical solution has ever been proposed that can provide an adequate security guarantee. The main reason is that the problem is very difficult to solve in practice due to severe performance degradation which accompanies most of the solutions. This paper presents an infrastructure called HIDE (Hardware-support for leakage-Immune Dynamic Execution) which provides a solution consisting of chunk-level protection with hardware support and a flexible interface which can be orchestrated through the proposed compiler optimization and user specifications that allow utilizing underlying hardware solution more efficiently to provide better security guarantees.Our results show that protecting both data and code with a high level of security guarantee is possible with negligible performance penalty (1.3\% slowdown).},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {72--84},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024403},
 doi = {http://doi.acm.org/10.1145/1024393.1024403},
 acmid = {1024403},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address bus leakage protection, secure processor},
} 

@article{Suh:2004:SPE:1037947.1024404,
 author = {Suh, G. Edward and Lee, Jae W. and Zhang, David and Devadas, Srinivas},
 title = {Secure program execution via dynamic information flow tracking},
 abstract = {We present a simple architectural mechanism called dynamic information flow tracking that can significantly improve the security of computing systems with negligible performance overhead. Dynamic information flow tracking protects programs against malicious software attacks by identifying spurious information flows from untrusted I/O and restricting the usage of the spurious information.Every security attack to take control of a program needs to transfer the program's control to malevolent code. In our approach, the operating system identifies a set of input channels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks are effectively defeated by checking the use of the spurious values as instructions and pointers.Our protection is transparent to users or application programmers; the executables can be used without any modification. Also, our scheme only incurs, on average, a memory overhead of 1.4\% and a performance overhead of 1.1\%.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037947.1024404},
 doi = {http://doi.acm.org/10.1145/1037947.1024404},
 acmid = {1024404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflow, format string, hardware tagging},
} 

@article{Suh:2004:SPE:1037949.1024404,
 author = {Suh, G. Edward and Lee, Jae W. and Zhang, David and Devadas, Srinivas},
 title = {Secure program execution via dynamic information flow tracking},
 abstract = {We present a simple architectural mechanism called dynamic information flow tracking that can significantly improve the security of computing systems with negligible performance overhead. Dynamic information flow tracking protects programs against malicious software attacks by identifying spurious information flows from untrusted I/O and restricting the usage of the spurious information.Every security attack to take control of a program needs to transfer the program's control to malevolent code. In our approach, the operating system identifies a set of input channels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks are effectively defeated by checking the use of the spurious values as instructions and pointers.Our protection is transparent to users or application programmers; the executables can be used without any modification. Also, our scheme only incurs, on average, a memory overhead of 1.4\% and a performance overhead of 1.1\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037949.1024404},
 doi = {http://doi.acm.org/10.1145/1037949.1024404},
 acmid = {1024404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflow, format string, hardware tagging},
} 

@inproceedings{Suh:2004:SPE:1024393.1024404,
 author = {Suh, G. Edward and Lee, Jae W. and Zhang, David and Devadas, Srinivas},
 title = {Secure program execution via dynamic information flow tracking},
 abstract = {We present a simple architectural mechanism called dynamic information flow tracking that can significantly improve the security of computing systems with negligible performance overhead. Dynamic information flow tracking protects programs against malicious software attacks by identifying spurious information flows from untrusted I/O and restricting the usage of the spurious information.Every security attack to take control of a program needs to transfer the program's control to malevolent code. In our approach, the operating system identifies a set of input channels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks are effectively defeated by checking the use of the spurious values as instructions and pointers.Our protection is transparent to users or application programmers; the executables can be used without any modification. Also, our scheme only incurs, on average, a memory overhead of 1.4\% and a performance overhead of 1.1\%.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1024393.1024404},
 doi = {http://doi.acm.org/10.1145/1024393.1024404},
 acmid = {1024404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflow, format string, hardware tagging},
} 

@article{Suh:2004:SPE:1037187.1024404,
 author = {Suh, G. Edward and Lee, Jae W. and Zhang, David and Devadas, Srinivas},
 title = {Secure program execution via dynamic information flow tracking},
 abstract = {We present a simple architectural mechanism called dynamic information flow tracking that can significantly improve the security of computing systems with negligible performance overhead. Dynamic information flow tracking protects programs against malicious software attacks by identifying spurious information flows from untrusted I/O and restricting the usage of the spurious information.Every security attack to take control of a program needs to transfer the program's control to malevolent code. In our approach, the operating system identifies a set of input channels as spurious, and the processor tracks all information flows from those inputs. A broad range of attacks are effectively defeated by checking the use of the spurious values as instructions and pointers.Our protection is transparent to users or application programmers; the executables can be used without any modification. Also, our scheme only incurs, on average, a memory overhead of 1.4\% and a performance overhead of 1.1\%.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037187.1024404},
 doi = {http://doi.acm.org/10.1145/1037187.1024404},
 acmid = {1024404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflow, format string, hardware tagging},
} 

@article{Huh:2004:CDM:1037947.1024406,
 author = {Huh, Jaehyuk and Chang, Jichuan and Burger, Doug and Sohi, Gurindar S.},
 title = {Coherence decoupling: making use of incoherence},
 abstract = {This paper explores a new technique called coherence decoupling</i>, which breaks a traditional cache coherence protocol into two protocols: a Speculative Cache Lookup (SCL) protocol and a safe, backing coherence protocol. The SCL protocol produces a speculative load value, typically from an invalid cache line, permitting the processor to compute with incoherent data. In parallel, the coherence protocol obtains the necessary coherence permissions and the correct value. Eventually, the speculative use of the incoherent data can be verified against the coherent data. Thus, coherence decoupling can greatly reduce --- if not eliminate --- the effects of false sharing. Furthermore, coherence decoupling can also reduce latencies incurred by true sharing. SCL protocols reduce those latencies by speculatively writing updates into invalid lines, thereby increasing the accuracy of speculation, without complicating the simple, underlying coherence protocol that guarantees correctness.The performance benefits of coherence decoupling are evaluated using a full-system simulator and a mix of commercial and scientific benchmarks. Our results show that 40\% to 90\% of all coherence misses can be speculated correctly, and therefore their latencies partially or fully hidden. This capability results in performance improvements ranging from 3\% to over 16\%, in most cases where the latencies of coherence misses have an effect on performance.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037947.1024406},
 doi = {http://doi.acm.org/10.1145/1037947.1024406},
 acmid = {1024406},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence decoupling, coherence misses, false sharing, speculative cache lookup},
} 

@article{Huh:2004:CDM:1037187.1024406,
 author = {Huh, Jaehyuk and Chang, Jichuan and Burger, Doug and Sohi, Gurindar S.},
 title = {Coherence decoupling: making use of incoherence},
 abstract = {This paper explores a new technique called coherence decoupling</i>, which breaks a traditional cache coherence protocol into two protocols: a Speculative Cache Lookup (SCL) protocol and a safe, backing coherence protocol. The SCL protocol produces a speculative load value, typically from an invalid cache line, permitting the processor to compute with incoherent data. In parallel, the coherence protocol obtains the necessary coherence permissions and the correct value. Eventually, the speculative use of the incoherent data can be verified against the coherent data. Thus, coherence decoupling can greatly reduce --- if not eliminate --- the effects of false sharing. Furthermore, coherence decoupling can also reduce latencies incurred by true sharing. SCL protocols reduce those latencies by speculatively writing updates into invalid lines, thereby increasing the accuracy of speculation, without complicating the simple, underlying coherence protocol that guarantees correctness.The performance benefits of coherence decoupling are evaluated using a full-system simulator and a mix of commercial and scientific benchmarks. Our results show that 40\% to 90\% of all coherence misses can be speculated correctly, and therefore their latencies partially or fully hidden. This capability results in performance improvements ranging from 3\% to over 16\%, in most cases where the latencies of coherence misses have an effect on performance.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037187.1024406},
 doi = {http://doi.acm.org/10.1145/1037187.1024406},
 acmid = {1024406},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence decoupling, coherence misses, false sharing, speculative cache lookup},
} 

@inproceedings{Huh:2004:CDM:1024393.1024406,
 author = {Huh, Jaehyuk and Chang, Jichuan and Burger, Doug and Sohi, Gurindar S.},
 title = {Coherence decoupling: making use of incoherence},
 abstract = {This paper explores a new technique called coherence decoupling</i>, which breaks a traditional cache coherence protocol into two protocols: a Speculative Cache Lookup (SCL) protocol and a safe, backing coherence protocol. The SCL protocol produces a speculative load value, typically from an invalid cache line, permitting the processor to compute with incoherent data. In parallel, the coherence protocol obtains the necessary coherence permissions and the correct value. Eventually, the speculative use of the incoherent data can be verified against the coherent data. Thus, coherence decoupling can greatly reduce --- if not eliminate --- the effects of false sharing. Furthermore, coherence decoupling can also reduce latencies incurred by true sharing. SCL protocols reduce those latencies by speculatively writing updates into invalid lines, thereby increasing the accuracy of speculation, without complicating the simple, underlying coherence protocol that guarantees correctness.The performance benefits of coherence decoupling are evaluated using a full-system simulator and a mix of commercial and scientific benchmarks. Our results show that 40\% to 90\% of all coherence misses can be speculated correctly, and therefore their latencies partially or fully hidden. This capability results in performance improvements ranging from 3\% to over 16\%, in most cases where the latencies of coherence misses have an effect on performance.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1024393.1024406},
 doi = {http://doi.acm.org/10.1145/1024393.1024406},
 acmid = {1024406},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence decoupling, coherence misses, false sharing, speculative cache lookup},
} 

@article{Huh:2004:CDM:1037949.1024406,
 author = {Huh, Jaehyuk and Chang, Jichuan and Burger, Doug and Sohi, Gurindar S.},
 title = {Coherence decoupling: making use of incoherence},
 abstract = {This paper explores a new technique called coherence decoupling</i>, which breaks a traditional cache coherence protocol into two protocols: a Speculative Cache Lookup (SCL) protocol and a safe, backing coherence protocol. The SCL protocol produces a speculative load value, typically from an invalid cache line, permitting the processor to compute with incoherent data. In parallel, the coherence protocol obtains the necessary coherence permissions and the correct value. Eventually, the speculative use of the incoherent data can be verified against the coherent data. Thus, coherence decoupling can greatly reduce --- if not eliminate --- the effects of false sharing. Furthermore, coherence decoupling can also reduce latencies incurred by true sharing. SCL protocols reduce those latencies by speculatively writing updates into invalid lines, thereby increasing the accuracy of speculation, without complicating the simple, underlying coherence protocol that guarantees correctness.The performance benefits of coherence decoupling are evaluated using a full-system simulator and a mix of commercial and scientific benchmarks. Our results show that 40\% to 90\% of all coherence misses can be speculated correctly, and therefore their latencies partially or fully hidden. This capability results in performance improvements ranging from 3\% to over 16\%, in most cases where the latencies of coherence misses have an effect on performance.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {97--106},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037949.1024406},
 doi = {http://doi.acm.org/10.1145/1037949.1024406},
 acmid = {1024406},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coherence decoupling, coherence misses, false sharing, speculative cache lookup},
} 

@article{Srinivasan:2004:CFP:1037187.1024407,
 author = {Srinivasan, Srikanth T. and Rajwar, Ravi and Akkary, Haitham and Gandhi, Amit and Upton, Mike},
 title = {Continual flow pipelines},
 abstract = {Increased integration in the form of multiple processor cores on a single die, relatively constant die sizes, shrinking power envelopes, and emerging applications create a new challenge for processor architects. How to build a processor that provides high single-thread performance and enables multiple of these to be placed on the same die for high throughput while dynamically adapting for future applications? Conventional approaches for high single-thread performance rely on large and complex cores to sustain a large instruction window for memory tolerance, making them unsuitable for multi-core chips. We present Continual Flow Pipelines</i> (CFP) as a new non-blocking processor pipeline architecture that achieves the performance of a large instruction window without requiring cycle-critical structures such as the scheduler and register file to be large. We show that to achieve benefits of a large instruction window, inefficiencies in management of both the scheduler and register file must be addressed, and we propose a unified solution. The non-blocking property of CFP keeps key processor structures affecting cycle time and power (scheduler, register file), and die size (second level cache) small. The memory latency-tolerant CFP core allows multiple cores on a single die while outperforming current processor cores for single-thread applications.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {107--119},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024407},
 doi = {http://doi.acm.org/10.1145/1037187.1024407},
 acmid = {1024407},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CFP, instruction window, latency tolerance, non-blocking},
} 

@article{Srinivasan:2004:CFP:1037949.1024407,
 author = {Srinivasan, Srikanth T. and Rajwar, Ravi and Akkary, Haitham and Gandhi, Amit and Upton, Mike},
 title = {Continual flow pipelines},
 abstract = {Increased integration in the form of multiple processor cores on a single die, relatively constant die sizes, shrinking power envelopes, and emerging applications create a new challenge for processor architects. How to build a processor that provides high single-thread performance and enables multiple of these to be placed on the same die for high throughput while dynamically adapting for future applications? Conventional approaches for high single-thread performance rely on large and complex cores to sustain a large instruction window for memory tolerance, making them unsuitable for multi-core chips. We present Continual Flow Pipelines</i> (CFP) as a new non-blocking processor pipeline architecture that achieves the performance of a large instruction window without requiring cycle-critical structures such as the scheduler and register file to be large. We show that to achieve benefits of a large instruction window, inefficiencies in management of both the scheduler and register file must be addressed, and we propose a unified solution. The non-blocking property of CFP keeps key processor structures affecting cycle time and power (scheduler, register file), and die size (second level cache) small. The memory latency-tolerant CFP core allows multiple cores on a single die while outperforming current processor cores for single-thread applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {107--119},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024407},
 doi = {http://doi.acm.org/10.1145/1037949.1024407},
 acmid = {1024407},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CFP, instruction window, latency tolerance, non-blocking},
} 

@article{Srinivasan:2004:CFP:1037947.1024407,
 author = {Srinivasan, Srikanth T. and Rajwar, Ravi and Akkary, Haitham and Gandhi, Amit and Upton, Mike},
 title = {Continual flow pipelines},
 abstract = {Increased integration in the form of multiple processor cores on a single die, relatively constant die sizes, shrinking power envelopes, and emerging applications create a new challenge for processor architects. How to build a processor that provides high single-thread performance and enables multiple of these to be placed on the same die for high throughput while dynamically adapting for future applications? Conventional approaches for high single-thread performance rely on large and complex cores to sustain a large instruction window for memory tolerance, making them unsuitable for multi-core chips. We present Continual Flow Pipelines</i> (CFP) as a new non-blocking processor pipeline architecture that achieves the performance of a large instruction window without requiring cycle-critical structures such as the scheduler and register file to be large. We show that to achieve benefits of a large instruction window, inefficiencies in management of both the scheduler and register file must be addressed, and we propose a unified solution. The non-blocking property of CFP keeps key processor structures affecting cycle time and power (scheduler, register file), and die size (second level cache) small. The memory latency-tolerant CFP core allows multiple cores on a single die while outperforming current processor cores for single-thread applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {107--119},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024407},
 doi = {http://doi.acm.org/10.1145/1037947.1024407},
 acmid = {1024407},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CFP, instruction window, latency tolerance, non-blocking},
} 

@inproceedings{Srinivasan:2004:CFP:1024393.1024407,
 author = {Srinivasan, Srikanth T. and Rajwar, Ravi and Akkary, Haitham and Gandhi, Amit and Upton, Mike},
 title = {Continual flow pipelines},
 abstract = {Increased integration in the form of multiple processor cores on a single die, relatively constant die sizes, shrinking power envelopes, and emerging applications create a new challenge for processor architects. How to build a processor that provides high single-thread performance and enables multiple of these to be placed on the same die for high throughput while dynamically adapting for future applications? Conventional approaches for high single-thread performance rely on large and complex cores to sustain a large instruction window for memory tolerance, making them unsuitable for multi-core chips. We present Continual Flow Pipelines</i> (CFP) as a new non-blocking processor pipeline architecture that achieves the performance of a large instruction window without requiring cycle-critical structures such as the scheduler and register file to be large. We show that to achieve benefits of a large instruction window, inefficiencies in management of both the scheduler and register file must be addressed, and we propose a unified solution. The non-blocking property of CFP keeps key processor structures affecting cycle time and power (scheduler, register file), and die size (second level cache) small. The memory latency-tolerant CFP core allows multiple cores on a single die while outperforming current processor cores for single-thread applications.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {107--119},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024407},
 doi = {http://doi.acm.org/10.1145/1024393.1024407},
 acmid = {1024407},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CFP, instruction window, latency tolerance, non-blocking},
} 

@inproceedings{Desikan:2004:SSR:1024393.1024408,
 author = {Desikan, Rajagopalan and Sethumadhavan, Simha and Burger, Doug and Keckler, Stephen W.},
 title = {Scalable selective re-execution for EDGE architectures},
 abstract = {Pipeline flushes are becoming increasingly expensive in modern microprocessors with large instruction windows and deep pipelines. Selective re-execution is a technique that can reduce the penalty of mis-speculations by re-executing only instructions affected by the mis-speculation, instead of all instructions. In this paper we introduce a new selective re-execution mechanism that exploits the properties of a dataflow-like Explicit Data Graph Execution (EDGE) architecture to support efficient mis-speculation recovery, while scaling to window sizes of thousands of instructions with high performance. This distributed selective re-execution (DSRE) protocol permits multiple speculative waves of computation to be traversing a dataflow graph simultaneously, with a commit wave propagating behind them to ensure correct execution. We evaluate one application of this protocol to provide efficient recovery for load-store dependence speculation. Unlike traditional dataflow architectures which resorted to single-assignment memory semantics, the DSRE protocol combines dataflow execution with speculation to enable high performance and</i> conventional sequential memory semantics. Our experiments show that the DSRE protocol results in an average 17\% speedup over the best dependence predictor proposed to date, and obtains 82\% of the performance possible with a perfect oracle directing the issue of loads.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {120--132},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024408},
 doi = {http://doi.acm.org/10.1145/1024393.1024408},
 acmid = {1024408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architectures, load-store dependence prediction, mis-speculation recovery, selective re-execution, selective replay, speculative dataflow machines},
} 

@article{Desikan:2004:SSR:1037949.1024408,
 author = {Desikan, Rajagopalan and Sethumadhavan, Simha and Burger, Doug and Keckler, Stephen W.},
 title = {Scalable selective re-execution for EDGE architectures},
 abstract = {Pipeline flushes are becoming increasingly expensive in modern microprocessors with large instruction windows and deep pipelines. Selective re-execution is a technique that can reduce the penalty of mis-speculations by re-executing only instructions affected by the mis-speculation, instead of all instructions. In this paper we introduce a new selective re-execution mechanism that exploits the properties of a dataflow-like Explicit Data Graph Execution (EDGE) architecture to support efficient mis-speculation recovery, while scaling to window sizes of thousands of instructions with high performance. This distributed selective re-execution (DSRE) protocol permits multiple speculative waves of computation to be traversing a dataflow graph simultaneously, with a commit wave propagating behind them to ensure correct execution. We evaluate one application of this protocol to provide efficient recovery for load-store dependence speculation. Unlike traditional dataflow architectures which resorted to single-assignment memory semantics, the DSRE protocol combines dataflow execution with speculation to enable high performance and</i> conventional sequential memory semantics. Our experiments show that the DSRE protocol results in an average 17\% speedup over the best dependence predictor proposed to date, and obtains 82\% of the performance possible with a perfect oracle directing the issue of loads.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {120--132},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024408},
 doi = {http://doi.acm.org/10.1145/1037949.1024408},
 acmid = {1024408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architectures, load-store dependence prediction, mis-speculation recovery, selective re-execution, selective replay, speculative dataflow machines},
} 

@article{Desikan:2004:SSR:1037947.1024408,
 author = {Desikan, Rajagopalan and Sethumadhavan, Simha and Burger, Doug and Keckler, Stephen W.},
 title = {Scalable selective re-execution for EDGE architectures},
 abstract = {Pipeline flushes are becoming increasingly expensive in modern microprocessors with large instruction windows and deep pipelines. Selective re-execution is a technique that can reduce the penalty of mis-speculations by re-executing only instructions affected by the mis-speculation, instead of all instructions. In this paper we introduce a new selective re-execution mechanism that exploits the properties of a dataflow-like Explicit Data Graph Execution (EDGE) architecture to support efficient mis-speculation recovery, while scaling to window sizes of thousands of instructions with high performance. This distributed selective re-execution (DSRE) protocol permits multiple speculative waves of computation to be traversing a dataflow graph simultaneously, with a commit wave propagating behind them to ensure correct execution. We evaluate one application of this protocol to provide efficient recovery for load-store dependence speculation. Unlike traditional dataflow architectures which resorted to single-assignment memory semantics, the DSRE protocol combines dataflow execution with speculation to enable high performance and</i> conventional sequential memory semantics. Our experiments show that the DSRE protocol results in an average 17\% speedup over the best dependence predictor proposed to date, and obtains 82\% of the performance possible with a perfect oracle directing the issue of loads.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {120--132},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024408},
 doi = {http://doi.acm.org/10.1145/1037947.1024408},
 acmid = {1024408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architectures, load-store dependence prediction, mis-speculation recovery, selective re-execution, selective replay, speculative dataflow machines},
} 

@article{Desikan:2004:SSR:1037187.1024408,
 author = {Desikan, Rajagopalan and Sethumadhavan, Simha and Burger, Doug and Keckler, Stephen W.},
 title = {Scalable selective re-execution for EDGE architectures},
 abstract = {Pipeline flushes are becoming increasingly expensive in modern microprocessors with large instruction windows and deep pipelines. Selective re-execution is a technique that can reduce the penalty of mis-speculations by re-executing only instructions affected by the mis-speculation, instead of all instructions. In this paper we introduce a new selective re-execution mechanism that exploits the properties of a dataflow-like Explicit Data Graph Execution (EDGE) architecture to support efficient mis-speculation recovery, while scaling to window sizes of thousands of instructions with high performance. This distributed selective re-execution (DSRE) protocol permits multiple speculative waves of computation to be traversing a dataflow graph simultaneously, with a commit wave propagating behind them to ensure correct execution. We evaluate one application of this protocol to provide efficient recovery for load-store dependence speculation. Unlike traditional dataflow architectures which resorted to single-assignment memory semantics, the DSRE protocol combines dataflow execution with speculation to enable high performance and</i> conventional sequential memory semantics. Our experiments show that the DSRE protocol results in an average 17\% speedup over the best dependence predictor proposed to date, and obtains 82\% of the performance possible with a perfect oracle directing the issue of loads.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {120--132},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024408},
 doi = {http://doi.acm.org/10.1145/1037187.1024408},
 acmid = {1024408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDGE architectures, load-store dependence prediction, mis-speculation recovery, selective re-execution, selective replay, speculative dataflow machines},
} 

@article{Regehr:2004:HSA:1037949.1024410,
 author = {Regehr, John and Reid, Alastair},
 title = {HOIST: a system for automatically deriving static analyzers for embedded systems},
 abstract = {Embedded software must meet conflicting requirements such as be-ing highly reliable, running on resource-constrained platforms, and being developed rapidly. Static program analysis can help meet all of these goals. People developing analyzers for embedded object code face a difficult problem: writing an abstract version of each instruction in the target architecture(s). This is currently done by hand, resulting in abstract operations that are both buggy and im-precise. We have developed Hoist: a novel system that solves these problems by automatically constructing abstract operations using a microprocessor (or simulator) as its own specification. With almost no input from a human, Hoist generates a collection of C func-tions that are ready to be linked into an abstract interpreter. We demonstrate that Hoist generates abstract operations that are cor-rect, having been extensively tested, sufficiently fast, and substan-tially more precise than manually written abstract operations. Hoist is currently limited to eight-bit machines due to costs exponential in the word size of the target architecture. It is essential to be able to analyze software running on these small processors: they are important and ubiquitous, with many embedded and safety-critical systems being based on them.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037949.1024410},
 doi = {http://doi.acm.org/10.1145/1037949.1024410},
 acmid = {1024410},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, object code, program verification, static analysis},
} 

@article{Regehr:2004:HSA:1037947.1024410,
 author = {Regehr, John and Reid, Alastair},
 title = {HOIST: a system for automatically deriving static analyzers for embedded systems},
 abstract = {Embedded software must meet conflicting requirements such as be-ing highly reliable, running on resource-constrained platforms, and being developed rapidly. Static program analysis can help meet all of these goals. People developing analyzers for embedded object code face a difficult problem: writing an abstract version of each instruction in the target architecture(s). This is currently done by hand, resulting in abstract operations that are both buggy and im-precise. We have developed Hoist: a novel system that solves these problems by automatically constructing abstract operations using a microprocessor (or simulator) as its own specification. With almost no input from a human, Hoist generates a collection of C func-tions that are ready to be linked into an abstract interpreter. We demonstrate that Hoist generates abstract operations that are cor-rect, having been extensively tested, sufficiently fast, and substan-tially more precise than manually written abstract operations. Hoist is currently limited to eight-bit machines due to costs exponential in the word size of the target architecture. It is essential to be able to analyze software running on these small processors: they are important and ubiquitous, with many embedded and safety-critical systems being based on them.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037947.1024410},
 doi = {http://doi.acm.org/10.1145/1037947.1024410},
 acmid = {1024410},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, object code, program verification, static analysis},
} 

@article{Regehr:2004:HSA:1037187.1024410,
 author = {Regehr, John and Reid, Alastair},
 title = {HOIST: a system for automatically deriving static analyzers for embedded systems},
 abstract = {Embedded software must meet conflicting requirements such as be-ing highly reliable, running on resource-constrained platforms, and being developed rapidly. Static program analysis can help meet all of these goals. People developing analyzers for embedded object code face a difficult problem: writing an abstract version of each instruction in the target architecture(s). This is currently done by hand, resulting in abstract operations that are both buggy and im-precise. We have developed Hoist: a novel system that solves these problems by automatically constructing abstract operations using a microprocessor (or simulator) as its own specification. With almost no input from a human, Hoist generates a collection of C func-tions that are ready to be linked into an abstract interpreter. We demonstrate that Hoist generates abstract operations that are cor-rect, having been extensively tested, sufficiently fast, and substan-tially more precise than manually written abstract operations. Hoist is currently limited to eight-bit machines due to costs exponential in the word size of the target architecture. It is essential to be able to analyze software running on these small processors: they are important and ubiquitous, with many embedded and safety-critical systems being based on them.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037187.1024410},
 doi = {http://doi.acm.org/10.1145/1037187.1024410},
 acmid = {1024410},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, object code, program verification, static analysis},
} 

@inproceedings{Regehr:2004:HSA:1024393.1024410,
 author = {Regehr, John and Reid, Alastair},
 title = {HOIST: a system for automatically deriving static analyzers for embedded systems},
 abstract = {Embedded software must meet conflicting requirements such as be-ing highly reliable, running on resource-constrained platforms, and being developed rapidly. Static program analysis can help meet all of these goals. People developing analyzers for embedded object code face a difficult problem: writing an abstract version of each instruction in the target architecture(s). This is currently done by hand, resulting in abstract operations that are both buggy and im-precise. We have developed Hoist: a novel system that solves these problems by automatically constructing abstract operations using a microprocessor (or simulator) as its own specification. With almost no input from a human, Hoist generates a collection of C func-tions that are ready to be linked into an abstract interpreter. We demonstrate that Hoist generates abstract operations that are cor-rect, having been extensively tested, sufficiently fast, and substan-tially more precise than manually written abstract operations. Hoist is currently limited to eight-bit machines due to costs exponential in the word size of the target architecture. It is essential to be able to analyze software running on these small processors: they are important and ubiquitous, with many embedded and safety-critical systems being based on them.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1024393.1024410},
 doi = {http://doi.acm.org/10.1145/1024393.1024410},
 acmid = {1024410},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract interpretation, object code, program verification, static analysis},
} 

@article{Wang:2004:HTV:1037187.1024411,
 author = {Wang, Perry H. and Collins, Jamison D. and Wang, Hong and Kim, Dongkeun and Greene, Bill and Chan, Kai-Ming and Yunus, Aamir B. and Sych, Terry and Moore, Stephen F. and Shen, John P.},
 title = {Helper threads via virtual multithreading on an experimental itanium<sup>\&\#174;</sup> 2 processor-based platform},
 abstract = {Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run ``assist'' threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled.The concept of VMT is prototyped on an Itanium <sup>\&#174;</sup> 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8\% to 38.5\% for the workstation benchmarks, and 5.0\% to 12.7\% on various queries in the DSS workload.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037187.1024411},
 doi = {http://doi.acm.org/10.1145/1037187.1024411},
 acmid = {1024411},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DB2 database, PAL, cache miss prefetching, helper thread, itanium processor, multithreading, switch-on-event},
} 

@article{Wang:2004:HTV:1037947.1024411,
 author = {Wang, Perry H. and Collins, Jamison D. and Wang, Hong and Kim, Dongkeun and Greene, Bill and Chan, Kai-Ming and Yunus, Aamir B. and Sych, Terry and Moore, Stephen F. and Shen, John P.},
 title = {Helper threads via virtual multithreading on an experimental itanium<sup>\&\#174;</sup> 2 processor-based platform},
 abstract = {Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run ``assist'' threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled.The concept of VMT is prototyped on an Itanium <sup>\&#174;</sup> 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8\% to 38.5\% for the workstation benchmarks, and 5.0\% to 12.7\% on various queries in the DSS workload.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037947.1024411},
 doi = {http://doi.acm.org/10.1145/1037947.1024411},
 acmid = {1024411},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DB2 database, PAL, cache miss prefetching, helper thread, itanium processor, multithreading, switch-on-event},
} 

@inproceedings{Wang:2004:HTV:1024393.1024411,
 author = {Wang, Perry H. and Collins, Jamison D. and Wang, Hong and Kim, Dongkeun and Greene, Bill and Chan, Kai-Ming and Yunus, Aamir B. and Sych, Terry and Moore, Stephen F. and Shen, John P.},
 title = {Helper threads via virtual multithreading on an experimental itanium<sup>\&\#174;</sup> 2 processor-based platform},
 abstract = {Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run ``assist'' threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled.The concept of VMT is prototyped on an Itanium <sup>\&#174;</sup> 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8\% to 38.5\% for the workstation benchmarks, and 5.0\% to 12.7\% on various queries in the DSS workload.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1024393.1024411},
 doi = {http://doi.acm.org/10.1145/1024393.1024411},
 acmid = {1024411},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DB2 database, PAL, cache miss prefetching, helper thread, itanium processor, multithreading, switch-on-event},
} 

@article{Wang:2004:HTV:1037949.1024411,
 author = {Wang, Perry H. and Collins, Jamison D. and Wang, Hong and Kim, Dongkeun and Greene, Bill and Chan, Kai-Ming and Yunus, Aamir B. and Sych, Terry and Moore, Stephen F. and Shen, John P.},
 title = {Helper threads via virtual multithreading on an experimental itanium<sup>\&\#174;</sup> 2 processor-based platform},
 abstract = {Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run ``assist'' threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled.The concept of VMT is prototyped on an Itanium <sup>\&#174;</sup> 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8\% to 38.5\% for the workstation benchmarks, and 5.0\% to 12.7\% on various queries in the DSS workload.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037949.1024411},
 doi = {http://doi.acm.org/10.1145/1037949.1024411},
 acmid = {1024411},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DB2 database, PAL, cache miss prefetching, helper thread, itanium processor, multithreading, switch-on-event},
} 

@inproceedings{Hauswirth:2004:LML:1024393.1024412,
 author = {Hauswirth, Matthias and Chilimbi, Trishul M.},
 title = {Low-overhead memory leak detection using adaptive statistical profiling},
 abstract = {Sampling has been successfully used to identify performance optimization opportunities. We would like to apply similar techniques to check program correctness. Unfortunately, sampling provides poor coverage of infrequently executed code, where bugs often lurk. We describe an adaptive profiling scheme that addresses this by sampling executions of code segments at a rate inversely proportional to their execution frequency. To validate our ideas, we have implemented SWAT, a novel memory leak detection tool. SWAT traces program allocations/ frees to construct a heap model and uses our adaptive profiling infrastructure to monitor loads/stores to these objects with low overhead. SWAT reports 'stale' objects that have not been accessed for a 'long' time as leaks. This allows it to find all leaks that manifest during the current program execution. Since SWAT has low runtime overhead (\&#8249;5\%), and low space overhead (\&#8249;10\% in most cases and often less than 5\%), it can be used to track leaks in production code that take days to manifest. In addition to identifying the allocations that leak memory, SWAT exposes where the program last accessed the leaked data, which facilitates debugging and fixing the leak. SWAT has been used by several product groups at Microsoft for the past 18 months and has proved effective at detecting leaks with a low false positive rate (\&#8249;10\%).},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {156--164},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1024393.1024412},
 doi = {http://doi.acm.org/10.1145/1024393.1024412},
 acmid = {1024412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, memory leaks, runtime analysis},
} 

@article{Hauswirth:2004:LML:1037187.1024412,
 author = {Hauswirth, Matthias and Chilimbi, Trishul M.},
 title = {Low-overhead memory leak detection using adaptive statistical profiling},
 abstract = {Sampling has been successfully used to identify performance optimization opportunities. We would like to apply similar techniques to check program correctness. Unfortunately, sampling provides poor coverage of infrequently executed code, where bugs often lurk. We describe an adaptive profiling scheme that addresses this by sampling executions of code segments at a rate inversely proportional to their execution frequency. To validate our ideas, we have implemented SWAT, a novel memory leak detection tool. SWAT traces program allocations/ frees to construct a heap model and uses our adaptive profiling infrastructure to monitor loads/stores to these objects with low overhead. SWAT reports 'stale' objects that have not been accessed for a 'long' time as leaks. This allows it to find all leaks that manifest during the current program execution. Since SWAT has low runtime overhead (\&#8249;5\%), and low space overhead (\&#8249;10\% in most cases and often less than 5\%), it can be used to track leaks in production code that take days to manifest. In addition to identifying the allocations that leak memory, SWAT exposes where the program last accessed the leaked data, which facilitates debugging and fixing the leak. SWAT has been used by several product groups at Microsoft for the past 18 months and has proved effective at detecting leaks with a low false positive rate (\&#8249;10\%).},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {156--164},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1037187.1024412},
 doi = {http://doi.acm.org/10.1145/1037187.1024412},
 acmid = {1024412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, memory leaks, runtime analysis},
} 

@article{Hauswirth:2004:LML:1037947.1024412,
 author = {Hauswirth, Matthias and Chilimbi, Trishul M.},
 title = {Low-overhead memory leak detection using adaptive statistical profiling},
 abstract = {Sampling has been successfully used to identify performance optimization opportunities. We would like to apply similar techniques to check program correctness. Unfortunately, sampling provides poor coverage of infrequently executed code, where bugs often lurk. We describe an adaptive profiling scheme that addresses this by sampling executions of code segments at a rate inversely proportional to their execution frequency. To validate our ideas, we have implemented SWAT, a novel memory leak detection tool. SWAT traces program allocations/ frees to construct a heap model and uses our adaptive profiling infrastructure to monitor loads/stores to these objects with low overhead. SWAT reports 'stale' objects that have not been accessed for a 'long' time as leaks. This allows it to find all leaks that manifest during the current program execution. Since SWAT has low runtime overhead (\&#8249;5\%), and low space overhead (\&#8249;10\% in most cases and often less than 5\%), it can be used to track leaks in production code that take days to manifest. In addition to identifying the allocations that leak memory, SWAT exposes where the program last accessed the leaked data, which facilitates debugging and fixing the leak. SWAT has been used by several product groups at Microsoft for the past 18 months and has proved effective at detecting leaks with a low false positive rate (\&#8249;10\%).},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {156--164},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1037947.1024412},
 doi = {http://doi.acm.org/10.1145/1037947.1024412},
 acmid = {1024412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, memory leaks, runtime analysis},
} 

@article{Hauswirth:2004:LML:1037949.1024412,
 author = {Hauswirth, Matthias and Chilimbi, Trishul M.},
 title = {Low-overhead memory leak detection using adaptive statistical profiling},
 abstract = {Sampling has been successfully used to identify performance optimization opportunities. We would like to apply similar techniques to check program correctness. Unfortunately, sampling provides poor coverage of infrequently executed code, where bugs often lurk. We describe an adaptive profiling scheme that addresses this by sampling executions of code segments at a rate inversely proportional to their execution frequency. To validate our ideas, we have implemented SWAT, a novel memory leak detection tool. SWAT traces program allocations/ frees to construct a heap model and uses our adaptive profiling infrastructure to monitor loads/stores to these objects with low overhead. SWAT reports 'stale' objects that have not been accessed for a 'long' time as leaks. This allows it to find all leaks that manifest during the current program execution. Since SWAT has low runtime overhead (\&#8249;5\%), and low space overhead (\&#8249;10\% in most cases and often less than 5\%), it can be used to track leaks in production code that take days to manifest. In addition to identifying the allocations that leak memory, SWAT exposes where the program last accessed the leaked data, which facilitates debugging and fixing the leak. SWAT has been used by several product groups at Microsoft for the past 18 months and has proved effective at detecting leaks with a low false positive rate (\&#8249;10\%).},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {156--164},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1037949.1024412},
 doi = {http://doi.acm.org/10.1145/1037949.1024412},
 acmid = {1024412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low-overhead monitoring, memory leaks, runtime analysis},
} 

@article{Shen:2004:LPP:1037949.1024414,
 author = {Shen, Xipeng and Zhong, Yutao and Ding, Chen},
 title = {Locality phase prediction},
 abstract = {As computer memory hierarchy becomes adaptive, its performance increasingly depends on forecasting the dynamic program locality. This paper presents a method that predicts the locality phases of a program by a combination of locality profiling and run-time prediction. By profiling a training input, it identifies locality phases by sifting through all accesses to all data elements using variable-distance sampling, wavelet filtering, and optimal phase partitioning. It then constructs a phase hierarchy through grammar compression. Finally, it inserts phase markers into the program using binary rewriting. When the instrumented program runs, it uses the first few executions of a phase to predict all its later executions.Compared with existing methods based on program code and execution intervals, locality phase prediction is unique because it uses locality profiles, and it marks phase boundaries in program code. The second half of the paper presents a comprehensive evaluation. It measures the accuracy and the coverage of the new technique and compares it with best known run-time methods. It measures its benefit in adaptive cache resizing and memory remapping. Finally, it compares the automatic analysis with manual phase marking. The results show that locality phase prediction is well suited for identifying large, recurring phases in complex programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037949.1024414},
 doi = {http://doi.acm.org/10.1145/1037949.1024414},
 acmid = {1024414},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic optimization, locality analysis and optimization, phase hierarchy, program phase analysis and prediction, reconfigurable architecture},
} 

@inproceedings{Shen:2004:LPP:1024393.1024414,
 author = {Shen, Xipeng and Zhong, Yutao and Ding, Chen},
 title = {Locality phase prediction},
 abstract = {As computer memory hierarchy becomes adaptive, its performance increasingly depends on forecasting the dynamic program locality. This paper presents a method that predicts the locality phases of a program by a combination of locality profiling and run-time prediction. By profiling a training input, it identifies locality phases by sifting through all accesses to all data elements using variable-distance sampling, wavelet filtering, and optimal phase partitioning. It then constructs a phase hierarchy through grammar compression. Finally, it inserts phase markers into the program using binary rewriting. When the instrumented program runs, it uses the first few executions of a phase to predict all its later executions.Compared with existing methods based on program code and execution intervals, locality phase prediction is unique because it uses locality profiles, and it marks phase boundaries in program code. The second half of the paper presents a comprehensive evaluation. It measures the accuracy and the coverage of the new technique and compares it with best known run-time methods. It measures its benefit in adaptive cache resizing and memory remapping. Finally, it compares the automatic analysis with manual phase marking. The results show that locality phase prediction is well suited for identifying large, recurring phases in complex programs.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1024393.1024414},
 doi = {http://doi.acm.org/10.1145/1024393.1024414},
 acmid = {1024414},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic optimization, locality analysis and optimization, phase hierarchy, program phase analysis and prediction, reconfigurable architecture},
} 

@article{Shen:2004:LPP:1037947.1024414,
 author = {Shen, Xipeng and Zhong, Yutao and Ding, Chen},
 title = {Locality phase prediction},
 abstract = {As computer memory hierarchy becomes adaptive, its performance increasingly depends on forecasting the dynamic program locality. This paper presents a method that predicts the locality phases of a program by a combination of locality profiling and run-time prediction. By profiling a training input, it identifies locality phases by sifting through all accesses to all data elements using variable-distance sampling, wavelet filtering, and optimal phase partitioning. It then constructs a phase hierarchy through grammar compression. Finally, it inserts phase markers into the program using binary rewriting. When the instrumented program runs, it uses the first few executions of a phase to predict all its later executions.Compared with existing methods based on program code and execution intervals, locality phase prediction is unique because it uses locality profiles, and it marks phase boundaries in program code. The second half of the paper presents a comprehensive evaluation. It measures the accuracy and the coverage of the new technique and compares it with best known run-time methods. It measures its benefit in adaptive cache resizing and memory remapping. Finally, it compares the automatic analysis with manual phase marking. The results show that locality phase prediction is well suited for identifying large, recurring phases in complex programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037947.1024414},
 doi = {http://doi.acm.org/10.1145/1037947.1024414},
 acmid = {1024414},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic optimization, locality analysis and optimization, phase hierarchy, program phase analysis and prediction, reconfigurable architecture},
} 

@article{Shen:2004:LPP:1037187.1024414,
 author = {Shen, Xipeng and Zhong, Yutao and Ding, Chen},
 title = {Locality phase prediction},
 abstract = {As computer memory hierarchy becomes adaptive, its performance increasingly depends on forecasting the dynamic program locality. This paper presents a method that predicts the locality phases of a program by a combination of locality profiling and run-time prediction. By profiling a training input, it identifies locality phases by sifting through all accesses to all data elements using variable-distance sampling, wavelet filtering, and optimal phase partitioning. It then constructs a phase hierarchy through grammar compression. Finally, it inserts phase markers into the program using binary rewriting. When the instrumented program runs, it uses the first few executions of a phase to predict all its later executions.Compared with existing methods based on program code and execution intervals, locality phase prediction is unique because it uses locality profiles, and it marks phase boundaries in program code. The second half of the paper presents a comprehensive evaluation. It measures the accuracy and the coverage of the new technique and compares it with best known run-time methods. It measures its benefit in adaptive cache resizing and memory remapping. Finally, it compares the automatic analysis with manual phase marking. The results show that locality phase prediction is well suited for identifying large, recurring phases in complex programs.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037187.1024414},
 doi = {http://doi.acm.org/10.1145/1037187.1024414},
 acmid = {1024414},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic optimization, locality analysis and optimization, phase hierarchy, program phase analysis and prediction, reconfigurable architecture},
} 

@article{Zhou:2004:DTP:1037949.1024415,
 author = {Zhou, Pin and Pandey, Vivek and Sundaresan, Jagadeesan and Raghuraman, Anand and Zhou, Yuanyuan and Kumar, Sanjeev},
 title = {Dynamic tracking of page miss ratio curve for memory management},
 abstract = {Memory can be efficiently utilized if the dynamic memory demands of applications can be determined and analyzed at run-time. The page miss ratio curve(MRC), i.e. page miss rate vs. memory size curve, is a good performance-directed metric to serve this purpose. However, dynamically tracking MRC at run time is challenging in systems with virtual memory because not every memory reference passes through the operating system (OS).This paper proposes two methods to dynamically track MRC of applications at run time. The first method is using a hardware MRC monitor that can track MRC at fine time granularity. Our simulation results show that this monitor has negligible performance and energy overheads. The second method is an OS-only implementation that can track MRC at coarse time granularity. Our implementation results on Linux</i> show that it adds only 7--10\% overhead.We have also used the dynamic MRC to guide both memory allocation for multiprogramming systems and memory energy management. Our real system</i> experiments on Linux with applications including Apache Web Server show that the MRC-directed memory allocation can speed up the applications' execution/response time by up to a factor of 5.86 and reduce the number of page faults by up to 63.1\%. Our execution-driven simulation results with SPEC2000 benchmarks show that the MRC-directed memory energy management can improve the Energy * Delay metric by 27--58\% over previously proposed static and dynamic schemes.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037949.1024415},
 doi = {http://doi.acm.org/10.1145/1037949.1024415},
 acmid = {1024415},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory management, power management, resource allocation},
} 

@article{Zhou:2004:DTP:1037187.1024415,
 author = {Zhou, Pin and Pandey, Vivek and Sundaresan, Jagadeesan and Raghuraman, Anand and Zhou, Yuanyuan and Kumar, Sanjeev},
 title = {Dynamic tracking of page miss ratio curve for memory management},
 abstract = {Memory can be efficiently utilized if the dynamic memory demands of applications can be determined and analyzed at run-time. The page miss ratio curve(MRC), i.e. page miss rate vs. memory size curve, is a good performance-directed metric to serve this purpose. However, dynamically tracking MRC at run time is challenging in systems with virtual memory because not every memory reference passes through the operating system (OS).This paper proposes two methods to dynamically track MRC of applications at run time. The first method is using a hardware MRC monitor that can track MRC at fine time granularity. Our simulation results show that this monitor has negligible performance and energy overheads. The second method is an OS-only implementation that can track MRC at coarse time granularity. Our implementation results on Linux</i> show that it adds only 7--10\% overhead.We have also used the dynamic MRC to guide both memory allocation for multiprogramming systems and memory energy management. Our real system</i> experiments on Linux with applications including Apache Web Server show that the MRC-directed memory allocation can speed up the applications' execution/response time by up to a factor of 5.86 and reduce the number of page faults by up to 63.1\%. Our execution-driven simulation results with SPEC2000 benchmarks show that the MRC-directed memory energy management can improve the Energy * Delay metric by 27--58\% over previously proposed static and dynamic schemes.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037187.1024415},
 doi = {http://doi.acm.org/10.1145/1037187.1024415},
 acmid = {1024415},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory management, power management, resource allocation},
} 

@article{Zhou:2004:DTP:1037947.1024415,
 author = {Zhou, Pin and Pandey, Vivek and Sundaresan, Jagadeesan and Raghuraman, Anand and Zhou, Yuanyuan and Kumar, Sanjeev},
 title = {Dynamic tracking of page miss ratio curve for memory management},
 abstract = {Memory can be efficiently utilized if the dynamic memory demands of applications can be determined and analyzed at run-time. The page miss ratio curve(MRC), i.e. page miss rate vs. memory size curve, is a good performance-directed metric to serve this purpose. However, dynamically tracking MRC at run time is challenging in systems with virtual memory because not every memory reference passes through the operating system (OS).This paper proposes two methods to dynamically track MRC of applications at run time. The first method is using a hardware MRC monitor that can track MRC at fine time granularity. Our simulation results show that this monitor has negligible performance and energy overheads. The second method is an OS-only implementation that can track MRC at coarse time granularity. Our implementation results on Linux</i> show that it adds only 7--10\% overhead.We have also used the dynamic MRC to guide both memory allocation for multiprogramming systems and memory energy management. Our real system</i> experiments on Linux with applications including Apache Web Server show that the MRC-directed memory allocation can speed up the applications' execution/response time by up to a factor of 5.86 and reduce the number of page faults by up to 63.1\%. Our execution-driven simulation results with SPEC2000 benchmarks show that the MRC-directed memory energy management can improve the Energy * Delay metric by 27--58\% over previously proposed static and dynamic schemes.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037947.1024415},
 doi = {http://doi.acm.org/10.1145/1037947.1024415},
 acmid = {1024415},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory management, power management, resource allocation},
} 

@inproceedings{Zhou:2004:DTP:1024393.1024415,
 author = {Zhou, Pin and Pandey, Vivek and Sundaresan, Jagadeesan and Raghuraman, Anand and Zhou, Yuanyuan and Kumar, Sanjeev},
 title = {Dynamic tracking of page miss ratio curve for memory management},
 abstract = {Memory can be efficiently utilized if the dynamic memory demands of applications can be determined and analyzed at run-time. The page miss ratio curve(MRC), i.e. page miss rate vs. memory size curve, is a good performance-directed metric to serve this purpose. However, dynamically tracking MRC at run time is challenging in systems with virtual memory because not every memory reference passes through the operating system (OS).This paper proposes two methods to dynamically track MRC of applications at run time. The first method is using a hardware MRC monitor that can track MRC at fine time granularity. Our simulation results show that this monitor has negligible performance and energy overheads. The second method is an OS-only implementation that can track MRC at coarse time granularity. Our implementation results on Linux</i> show that it adds only 7--10\% overhead.We have also used the dynamic MRC to guide both memory allocation for multiprogramming systems and memory energy management. Our real system</i> experiments on Linux with applications including Apache Web Server show that the MRC-directed memory allocation can speed up the applications' execution/response time by up to a factor of 5.86 and reduce the number of page faults by up to 63.1\%. Our execution-driven simulation results with SPEC2000 benchmarks show that the MRC-directed memory energy management can improve the Energy * Delay metric by 27--58\% over previously proposed static and dynamic schemes.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1024393.1024415},
 doi = {http://doi.acm.org/10.1145/1024393.1024415},
 acmid = {1024415},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory management, power management, resource allocation},
} 

@article{Rabbah:2004:COP:1037187.1024416,
 author = {Rabbah, Rodric M. and Sandanagobalane, Hariharan and Ekpanyapong, Mongkol and Wong, Weng-Fai},
 title = {Compiler orchestrated prefetching via speculation and predication},
 abstract = {This paper introduces a compiler orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution</i> can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27\% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of it sentineled prefetching</i>. Our results for twelve SPEC benchmarks demonstrate that 45\% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037187.1024416},
 doi = {http://doi.acm.org/10.1145/1037187.1024416},
 acmid = {1024416},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {precomputation, predicated execution, prefetching, speculation},
} 

@article{Rabbah:2004:COP:1037947.1024416,
 author = {Rabbah, Rodric M. and Sandanagobalane, Hariharan and Ekpanyapong, Mongkol and Wong, Weng-Fai},
 title = {Compiler orchestrated prefetching via speculation and predication},
 abstract = {This paper introduces a compiler orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution</i> can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27\% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of it sentineled prefetching</i>. Our results for twelve SPEC benchmarks demonstrate that 45\% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037947.1024416},
 doi = {http://doi.acm.org/10.1145/1037947.1024416},
 acmid = {1024416},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {precomputation, predicated execution, prefetching, speculation},
} 

@inproceedings{Rabbah:2004:COP:1024393.1024416,
 author = {Rabbah, Rodric M. and Sandanagobalane, Hariharan and Ekpanyapong, Mongkol and Wong, Weng-Fai},
 title = {Compiler orchestrated prefetching via speculation and predication},
 abstract = {This paper introduces a compiler orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution</i> can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27\% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of it sentineled prefetching</i>. Our results for twelve SPEC benchmarks demonstrate that 45\% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1024393.1024416},
 doi = {http://doi.acm.org/10.1145/1024393.1024416},
 acmid = {1024416},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {precomputation, predicated execution, prefetching, speculation},
} 

@article{Rabbah:2004:COP:1037949.1024416,
 author = {Rabbah, Rodric M. and Sandanagobalane, Hariharan and Ekpanyapong, Mongkol and Wong, Weng-Fai},
 title = {Compiler orchestrated prefetching via speculation and predication},
 abstract = {This paper introduces a compiler orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution</i> can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27\% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of it sentineled prefetching</i>. Our results for twelve SPEC benchmarks demonstrate that 45\% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1037949.1024416},
 doi = {http://doi.acm.org/10.1145/1037949.1024416},
 acmid = {1024416},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {precomputation, predicated execution, prefetching, speculation},
} 

@article{Cher:2004:SPM:1037187.1024417,
 author = {Cher, Chen-Yong and Hosking, Antony L. and Vijaykumar, T. N.},
 title = {Software prefetching for mark-sweep garbage collection: hardware analysis and software redesign},
 abstract = {Tracing garbage collectors traverse references from live program variables, transitively tracing out the closure of live objects. Memory accesses incurred during tracing are essentially random: a given object may contain references to any other object. Since application heaps are typically much larger than hardware caches, tracing results in many cache misses. Technology trends will make cache misses more important, so tracing is a prime target for prefetching.Simulation of Java benchmarks running with the Boehm-De-mers-Weiser mark-sweep garbage collector for a projected hardware platform reveal high tracing overhead (up to 65\% of elapsed time), and that cache misses are a problem. Applying Boehm's default prefetching strategy yields improvements in execution time (16\% on average with incremental/generational collection for GC-intensive benchmarks), but analysis shows that his strategy suffers from significant timing problems: prefetches that occur too early or too late relative to their matching loads. This analysis drives development of a new prefetching strategy that yields up to three times</i> the performance improvement of Boehm's strategy for GC-intensive benchmark (27\% average speedup), and achieves performance close to that of perfect timing ie</i>, few misses for tracing accesses) on some benchmarks. Validating these simulation results with live runs on current hardware produces average speedup of 6\% for the new strategy on GC-intensive benchmarks with a GC configuration that tightly controls heap growth. In contrast, Boehm's default prefetching strategy is ineffective on this platform.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037187.1024417},
 doi = {http://doi.acm.org/10.1145/1037187.1024417},
 acmid = {1024417},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {breadth-first, buffered prefetch, cache architecture, depth-first, garbage collection, mark-sweep, prefetch-on-grey, prefetching},
} 

@article{Cher:2004:SPM:1037947.1024417,
 author = {Cher, Chen-Yong and Hosking, Antony L. and Vijaykumar, T. N.},
 title = {Software prefetching for mark-sweep garbage collection: hardware analysis and software redesign},
 abstract = {Tracing garbage collectors traverse references from live program variables, transitively tracing out the closure of live objects. Memory accesses incurred during tracing are essentially random: a given object may contain references to any other object. Since application heaps are typically much larger than hardware caches, tracing results in many cache misses. Technology trends will make cache misses more important, so tracing is a prime target for prefetching.Simulation of Java benchmarks running with the Boehm-De-mers-Weiser mark-sweep garbage collector for a projected hardware platform reveal high tracing overhead (up to 65\% of elapsed time), and that cache misses are a problem. Applying Boehm's default prefetching strategy yields improvements in execution time (16\% on average with incremental/generational collection for GC-intensive benchmarks), but analysis shows that his strategy suffers from significant timing problems: prefetches that occur too early or too late relative to their matching loads. This analysis drives development of a new prefetching strategy that yields up to three times</i> the performance improvement of Boehm's strategy for GC-intensive benchmark (27\% average speedup), and achieves performance close to that of perfect timing ie</i>, few misses for tracing accesses) on some benchmarks. Validating these simulation results with live runs on current hardware produces average speedup of 6\% for the new strategy on GC-intensive benchmarks with a GC configuration that tightly controls heap growth. In contrast, Boehm's default prefetching strategy is ineffective on this platform.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037947.1024417},
 doi = {http://doi.acm.org/10.1145/1037947.1024417},
 acmid = {1024417},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {breadth-first, buffered prefetch, cache architecture, depth-first, garbage collection, mark-sweep, prefetch-on-grey, prefetching},
} 

@inproceedings{Cher:2004:SPM:1024393.1024417,
 author = {Cher, Chen-Yong and Hosking, Antony L. and Vijaykumar, T. N.},
 title = {Software prefetching for mark-sweep garbage collection: hardware analysis and software redesign},
 abstract = {Tracing garbage collectors traverse references from live program variables, transitively tracing out the closure of live objects. Memory accesses incurred during tracing are essentially random: a given object may contain references to any other object. Since application heaps are typically much larger than hardware caches, tracing results in many cache misses. Technology trends will make cache misses more important, so tracing is a prime target for prefetching.Simulation of Java benchmarks running with the Boehm-De-mers-Weiser mark-sweep garbage collector for a projected hardware platform reveal high tracing overhead (up to 65\% of elapsed time), and that cache misses are a problem. Applying Boehm's default prefetching strategy yields improvements in execution time (16\% on average with incremental/generational collection for GC-intensive benchmarks), but analysis shows that his strategy suffers from significant timing problems: prefetches that occur too early or too late relative to their matching loads. This analysis drives development of a new prefetching strategy that yields up to three times</i> the performance improvement of Boehm's strategy for GC-intensive benchmark (27\% average speedup), and achieves performance close to that of perfect timing ie</i>, few misses for tracing accesses) on some benchmarks. Validating these simulation results with live runs on current hardware produces average speedup of 6\% for the new strategy on GC-intensive benchmarks with a GC configuration that tightly controls heap growth. In contrast, Boehm's default prefetching strategy is ineffective on this platform.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1024393.1024417},
 doi = {http://doi.acm.org/10.1145/1024393.1024417},
 acmid = {1024417},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {breadth-first, buffered prefetch, cache architecture, depth-first, garbage collection, mark-sweep, prefetch-on-grey, prefetching},
} 

@article{Cher:2004:SPM:1037949.1024417,
 author = {Cher, Chen-Yong and Hosking, Antony L. and Vijaykumar, T. N.},
 title = {Software prefetching for mark-sweep garbage collection: hardware analysis and software redesign},
 abstract = {Tracing garbage collectors traverse references from live program variables, transitively tracing out the closure of live objects. Memory accesses incurred during tracing are essentially random: a given object may contain references to any other object. Since application heaps are typically much larger than hardware caches, tracing results in many cache misses. Technology trends will make cache misses more important, so tracing is a prime target for prefetching.Simulation of Java benchmarks running with the Boehm-De-mers-Weiser mark-sweep garbage collector for a projected hardware platform reveal high tracing overhead (up to 65\% of elapsed time), and that cache misses are a problem. Applying Boehm's default prefetching strategy yields improvements in execution time (16\% on average with incremental/generational collection for GC-intensive benchmarks), but analysis shows that his strategy suffers from significant timing problems: prefetches that occur too early or too late relative to their matching loads. This analysis drives development of a new prefetching strategy that yields up to three times</i> the performance improvement of Boehm's strategy for GC-intensive benchmark (27\% average speedup), and achieves performance close to that of perfect timing ie</i>, few misses for tracing accesses) on some benchmarks. Validating these simulation results with live runs on current hardware produces average speedup of 6\% for the new strategy on GC-intensive benchmarks with a GC configuration that tightly controls heap growth. In contrast, Boehm's default prefetching strategy is ineffective on this platform.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037949.1024417},
 doi = {http://doi.acm.org/10.1145/1037949.1024417},
 acmid = {1024417},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {breadth-first, buffered prefetch, cache architecture, depth-first, garbage collection, mark-sweep, prefetch-on-grey, prefetching},
} 

@article{Lowell:2004:DVM:1037949.1024419,
 author = {Lowell, David E. and Saito, Yasushi and Samberg, Eileen J.},
 title = {Devirtualizable virtual machines enabling general, single-node, online maintenance},
 abstract = {Maintenance is the dominant source of downtime at high availability sites. Unfortunately, the dominant mechanism for reducing this downtime, cluster rolling upgrade, has two shortcomings that have prevented its broad acceptance. First, cluster-style maintenance over many nodes is typically performed a few nodes at a time, mak-ing maintenance slow and often impractical. Second, cluster-style maintenance does not work on single-node systems, despite the fact that their unavailability during maintenance can be painful for organizations. In this paper, we propose a novel technique for online maintenance that uses virtual machines to provide maintenance on single nodes, allowing parallel maintenance over multiple nodes, and online maintenance for standalone servers. We present the Microvisor, our prototype virtual machine system that is custom tailored to the needs of online maintenance. Unlike general purpose virtual machine environments that induce continual 10-20\% over-head, the Microvisor virtualizes the hardware only during periods of active maintenance, letting the guest OS run at full speed most of the time. Unlike past attempts at virtual machine optimization, we do not compromise OS transparency. We instead give up generality and tailor our virtual machine system to the minimum needs of online maintenance, eschewing features, such as I/O and memory virtualization, that it does not strictly require. The result is a very thin virtual machine system that induces only 5.6\% CPU overhead when virtualizing the hardware, and zero CPU overhead when devirtualized</i>. Using the Microvisor, we demonstrate an online OS upgrade on a live, single-node web server, reducing downtime from one hour to less than one minute.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {211--223},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024419},
 doi = {http://doi.acm.org/10.1145/1037949.1024419},
 acmid = {1024419},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability, online maintenance, planned downtime, virtual machines},
} 

@article{Lowell:2004:DVM:1037187.1024419,
 author = {Lowell, David E. and Saito, Yasushi and Samberg, Eileen J.},
 title = {Devirtualizable virtual machines enabling general, single-node, online maintenance},
 abstract = {Maintenance is the dominant source of downtime at high availability sites. Unfortunately, the dominant mechanism for reducing this downtime, cluster rolling upgrade, has two shortcomings that have prevented its broad acceptance. First, cluster-style maintenance over many nodes is typically performed a few nodes at a time, mak-ing maintenance slow and often impractical. Second, cluster-style maintenance does not work on single-node systems, despite the fact that their unavailability during maintenance can be painful for organizations. In this paper, we propose a novel technique for online maintenance that uses virtual machines to provide maintenance on single nodes, allowing parallel maintenance over multiple nodes, and online maintenance for standalone servers. We present the Microvisor, our prototype virtual machine system that is custom tailored to the needs of online maintenance. Unlike general purpose virtual machine environments that induce continual 10-20\% over-head, the Microvisor virtualizes the hardware only during periods of active maintenance, letting the guest OS run at full speed most of the time. Unlike past attempts at virtual machine optimization, we do not compromise OS transparency. We instead give up generality and tailor our virtual machine system to the minimum needs of online maintenance, eschewing features, such as I/O and memory virtualization, that it does not strictly require. The result is a very thin virtual machine system that induces only 5.6\% CPU overhead when virtualizing the hardware, and zero CPU overhead when devirtualized</i>. Using the Microvisor, we demonstrate an online OS upgrade on a live, single-node web server, reducing downtime from one hour to less than one minute.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {211--223},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024419},
 doi = {http://doi.acm.org/10.1145/1037187.1024419},
 acmid = {1024419},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability, online maintenance, planned downtime, virtual machines},
} 

@inproceedings{Lowell:2004:DVM:1024393.1024419,
 author = {Lowell, David E. and Saito, Yasushi and Samberg, Eileen J.},
 title = {Devirtualizable virtual machines enabling general, single-node, online maintenance},
 abstract = {Maintenance is the dominant source of downtime at high availability sites. Unfortunately, the dominant mechanism for reducing this downtime, cluster rolling upgrade, has two shortcomings that have prevented its broad acceptance. First, cluster-style maintenance over many nodes is typically performed a few nodes at a time, mak-ing maintenance slow and often impractical. Second, cluster-style maintenance does not work on single-node systems, despite the fact that their unavailability during maintenance can be painful for organizations. In this paper, we propose a novel technique for online maintenance that uses virtual machines to provide maintenance on single nodes, allowing parallel maintenance over multiple nodes, and online maintenance for standalone servers. We present the Microvisor, our prototype virtual machine system that is custom tailored to the needs of online maintenance. Unlike general purpose virtual machine environments that induce continual 10-20\% over-head, the Microvisor virtualizes the hardware only during periods of active maintenance, letting the guest OS run at full speed most of the time. Unlike past attempts at virtual machine optimization, we do not compromise OS transparency. We instead give up generality and tailor our virtual machine system to the minimum needs of online maintenance, eschewing features, such as I/O and memory virtualization, that it does not strictly require. The result is a very thin virtual machine system that induces only 5.6\% CPU overhead when virtualizing the hardware, and zero CPU overhead when devirtualized</i>. Using the Microvisor, we demonstrate an online OS upgrade on a live, single-node web server, reducing downtime from one hour to less than one minute.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {211--223},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024419},
 doi = {http://doi.acm.org/10.1145/1024393.1024419},
 acmid = {1024419},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability, online maintenance, planned downtime, virtual machines},
} 

@article{Lowell:2004:DVM:1037947.1024419,
 author = {Lowell, David E. and Saito, Yasushi and Samberg, Eileen J.},
 title = {Devirtualizable virtual machines enabling general, single-node, online maintenance},
 abstract = {Maintenance is the dominant source of downtime at high availability sites. Unfortunately, the dominant mechanism for reducing this downtime, cluster rolling upgrade, has two shortcomings that have prevented its broad acceptance. First, cluster-style maintenance over many nodes is typically performed a few nodes at a time, mak-ing maintenance slow and often impractical. Second, cluster-style maintenance does not work on single-node systems, despite the fact that their unavailability during maintenance can be painful for organizations. In this paper, we propose a novel technique for online maintenance that uses virtual machines to provide maintenance on single nodes, allowing parallel maintenance over multiple nodes, and online maintenance for standalone servers. We present the Microvisor, our prototype virtual machine system that is custom tailored to the needs of online maintenance. Unlike general purpose virtual machine environments that induce continual 10-20\% over-head, the Microvisor virtualizes the hardware only during periods of active maintenance, letting the guest OS run at full speed most of the time. Unlike past attempts at virtual machine optimization, we do not compromise OS transparency. We instead give up generality and tailor our virtual machine system to the minimum needs of online maintenance, eschewing features, such as I/O and memory virtualization, that it does not strictly require. The result is a very thin virtual machine system that induces only 5.6\% CPU overhead when virtualizing the hardware, and zero CPU overhead when devirtualized</i>. Using the Microvisor, we demonstrate an online OS upgrade on a live, single-node web server, reducing downtime from one hour to less than one minute.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {211--223},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024419},
 doi = {http://doi.acm.org/10.1145/1037947.1024419},
 acmid = {1024419},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability, online maintenance, planned downtime, virtual machines},
} 

@inproceedings{Smolens:2004:FBS:1024393.1024420,
 author = {Smolens, Jared C. and Gold, Brian T. and Kim, Jangwoo and Falsafi, Babak and Hoe, James C. and Nowatzyk, Andreas G.},
 title = {Fingerprinting: bounding soft-error detection latency and bandwidth},
 abstract = {Recent studies have suggested that the soft-error rate in microprocessor logic will become a reliability concern by 2010. This paper proposes an efficient error detection technique, called fingerprinting</i>, that detects differences in execution across a dual modular redundant (DMR) processor pair. Fingerprinting summarizes a processor's execution history in a hash-based signature; differences between two mirrored processors are exposed by comparing their fingerprints. Fingerprinting tightly bounds detection latency and greatly reduces the interprocessor communication bandwidth required for checking. This paper presents a study that evaluates fingerprinting against a range of current approaches to error detection. The result of this study shows that fingerprinting is the only error detection mechanism that simultaneously allows high-error coverage, low error detection bandwidth, and high I/O performance.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1024393.1024420},
 doi = {http://doi.acm.org/10.1145/1024393.1024420},
 acmid = {1024420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backwards error recovery (BER), dual modular redundancy (DMR), error detection, soft errors},
} 

@article{Smolens:2004:FBS:1037187.1024420,
 author = {Smolens, Jared C. and Gold, Brian T. and Kim, Jangwoo and Falsafi, Babak and Hoe, James C. and Nowatzyk, Andreas G.},
 title = {Fingerprinting: bounding soft-error detection latency and bandwidth},
 abstract = {Recent studies have suggested that the soft-error rate in microprocessor logic will become a reliability concern by 2010. This paper proposes an efficient error detection technique, called fingerprinting</i>, that detects differences in execution across a dual modular redundant (DMR) processor pair. Fingerprinting summarizes a processor's execution history in a hash-based signature; differences between two mirrored processors are exposed by comparing their fingerprints. Fingerprinting tightly bounds detection latency and greatly reduces the interprocessor communication bandwidth required for checking. This paper presents a study that evaluates fingerprinting against a range of current approaches to error detection. The result of this study shows that fingerprinting is the only error detection mechanism that simultaneously allows high-error coverage, low error detection bandwidth, and high I/O performance.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037187.1024420},
 doi = {http://doi.acm.org/10.1145/1037187.1024420},
 acmid = {1024420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backwards error recovery (BER), dual modular redundancy (DMR), error detection, soft errors},
} 

@article{Smolens:2004:FBS:1037949.1024420,
 author = {Smolens, Jared C. and Gold, Brian T. and Kim, Jangwoo and Falsafi, Babak and Hoe, James C. and Nowatzyk, Andreas G.},
 title = {Fingerprinting: bounding soft-error detection latency and bandwidth},
 abstract = {Recent studies have suggested that the soft-error rate in microprocessor logic will become a reliability concern by 2010. This paper proposes an efficient error detection technique, called fingerprinting</i>, that detects differences in execution across a dual modular redundant (DMR) processor pair. Fingerprinting summarizes a processor's execution history in a hash-based signature; differences between two mirrored processors are exposed by comparing their fingerprints. Fingerprinting tightly bounds detection latency and greatly reduces the interprocessor communication bandwidth required for checking. This paper presents a study that evaluates fingerprinting against a range of current approaches to error detection. The result of this study shows that fingerprinting is the only error detection mechanism that simultaneously allows high-error coverage, low error detection bandwidth, and high I/O performance.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037949.1024420},
 doi = {http://doi.acm.org/10.1145/1037949.1024420},
 acmid = {1024420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backwards error recovery (BER), dual modular redundancy (DMR), error detection, soft errors},
} 

@article{Smolens:2004:FBS:1037947.1024420,
 author = {Smolens, Jared C. and Gold, Brian T. and Kim, Jangwoo and Falsafi, Babak and Hoe, James C. and Nowatzyk, Andreas G.},
 title = {Fingerprinting: bounding soft-error detection latency and bandwidth},
 abstract = {Recent studies have suggested that the soft-error rate in microprocessor logic will become a reliability concern by 2010. This paper proposes an efficient error detection technique, called fingerprinting</i>, that detects differences in execution across a dual modular redundant (DMR) processor pair. Fingerprinting summarizes a processor's execution history in a hash-based signature; differences between two mirrored processors are exposed by comparing their fingerprints. Fingerprinting tightly bounds detection latency and greatly reduces the interprocessor communication bandwidth required for checking. This paper presents a study that evaluates fingerprinting against a range of current approaches to error detection. The result of this study shows that fingerprinting is the only error detection mechanism that simultaneously allows high-error coverage, low error detection bandwidth, and high I/O performance.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037947.1024420},
 doi = {http://doi.acm.org/10.1145/1037947.1024420},
 acmid = {1024420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backwards error recovery (BER), dual modular redundancy (DMR), error detection, soft errors},
} 

@article{Bronevetsky:2004:ACS:1037949.1024421,
 author = {Bronevetsky, Greg and Marques, Daniel and Pingali, Keshav and Szwed, Peter and Schulz, Martin},
 title = {Application-level checkpointing for shared memory programs},
 abstract = {Trends in high-performance computing are making it necessary for long-running applications to tolerate hardware faults. The most commonly used approach is checkpoint and restart (CPR) - the state of the computation is saved periodically on disk, and when a failure occurs, the computation is restarted from the last saved state. At present, it is the responsibility of the programmer to instrument applications for CPR.Our group is investigating the use of compiler technology to instrument codes to make them self-checkpointing and self-restarting, thereby providing an automatic solution to the problem of making long-running scientific applications resilient to hardware faults. Our previous work focused on message-passing programs.In this paper, we describe such a system for shared-memory programs running on symmetric multiprocessors. This system has two components: (i) a pre-compiler for source-to-source modification of applications, and (ii) a runtime system that implements a protocol for coordinating CPR among the threads of the parallel application. For the sake of concreteness, we focus on a non-trivial subset of OpenMP that includes barriers and locks.One of the advantages of this approach is that the ability to tolerate faults becomes embedded within the application itself, so applications become self-checkpointing and self-restarting on any platform. We demonstrate this by showing that our transformed benchmarks can checkpoint and restart on three different platforms (Windows/x86, Linux/x86, and Tru64/Alpha). Our experiments show that the overhead introduced by this approach is usually quite small; they also suggest ways in which the current implementation can be tuned to reduced overheads further.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {235--247},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024421},
 doi = {http://doi.acm.org/10.1145/1037949.1024421},
 acmid = {1024421},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointing, fault-tolerance, openMP, shared-memory programs},
} 

@article{Bronevetsky:2004:ACS:1037947.1024421,
 author = {Bronevetsky, Greg and Marques, Daniel and Pingali, Keshav and Szwed, Peter and Schulz, Martin},
 title = {Application-level checkpointing for shared memory programs},
 abstract = {Trends in high-performance computing are making it necessary for long-running applications to tolerate hardware faults. The most commonly used approach is checkpoint and restart (CPR) - the state of the computation is saved periodically on disk, and when a failure occurs, the computation is restarted from the last saved state. At present, it is the responsibility of the programmer to instrument applications for CPR.Our group is investigating the use of compiler technology to instrument codes to make them self-checkpointing and self-restarting, thereby providing an automatic solution to the problem of making long-running scientific applications resilient to hardware faults. Our previous work focused on message-passing programs.In this paper, we describe such a system for shared-memory programs running on symmetric multiprocessors. This system has two components: (i) a pre-compiler for source-to-source modification of applications, and (ii) a runtime system that implements a protocol for coordinating CPR among the threads of the parallel application. For the sake of concreteness, we focus on a non-trivial subset of OpenMP that includes barriers and locks.One of the advantages of this approach is that the ability to tolerate faults becomes embedded within the application itself, so applications become self-checkpointing and self-restarting on any platform. We demonstrate this by showing that our transformed benchmarks can checkpoint and restart on three different platforms (Windows/x86, Linux/x86, and Tru64/Alpha). Our experiments show that the overhead introduced by this approach is usually quite small; they also suggest ways in which the current implementation can be tuned to reduced overheads further.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {235--247},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024421},
 doi = {http://doi.acm.org/10.1145/1037947.1024421},
 acmid = {1024421},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointing, fault-tolerance, openMP, shared-memory programs},
} 

@inproceedings{Bronevetsky:2004:ACS:1024393.1024421,
 author = {Bronevetsky, Greg and Marques, Daniel and Pingali, Keshav and Szwed, Peter and Schulz, Martin},
 title = {Application-level checkpointing for shared memory programs},
 abstract = {Trends in high-performance computing are making it necessary for long-running applications to tolerate hardware faults. The most commonly used approach is checkpoint and restart (CPR) - the state of the computation is saved periodically on disk, and when a failure occurs, the computation is restarted from the last saved state. At present, it is the responsibility of the programmer to instrument applications for CPR.Our group is investigating the use of compiler technology to instrument codes to make them self-checkpointing and self-restarting, thereby providing an automatic solution to the problem of making long-running scientific applications resilient to hardware faults. Our previous work focused on message-passing programs.In this paper, we describe such a system for shared-memory programs running on symmetric multiprocessors. This system has two components: (i) a pre-compiler for source-to-source modification of applications, and (ii) a runtime system that implements a protocol for coordinating CPR among the threads of the parallel application. For the sake of concreteness, we focus on a non-trivial subset of OpenMP that includes barriers and locks.One of the advantages of this approach is that the ability to tolerate faults becomes embedded within the application itself, so applications become self-checkpointing and self-restarting on any platform. We demonstrate this by showing that our transformed benchmarks can checkpoint and restart on three different platforms (Windows/x86, Linux/x86, and Tru64/Alpha). Our experiments show that the overhead introduced by this approach is usually quite small; they also suggest ways in which the current implementation can be tuned to reduced overheads further.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {235--247},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024421},
 doi = {http://doi.acm.org/10.1145/1024393.1024421},
 acmid = {1024421},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointing, fault-tolerance, openMP, shared-memory programs},
} 

@article{Bronevetsky:2004:ACS:1037187.1024421,
 author = {Bronevetsky, Greg and Marques, Daniel and Pingali, Keshav and Szwed, Peter and Schulz, Martin},
 title = {Application-level checkpointing for shared memory programs},
 abstract = {Trends in high-performance computing are making it necessary for long-running applications to tolerate hardware faults. The most commonly used approach is checkpoint and restart (CPR) - the state of the computation is saved periodically on disk, and when a failure occurs, the computation is restarted from the last saved state. At present, it is the responsibility of the programmer to instrument applications for CPR.Our group is investigating the use of compiler technology to instrument codes to make them self-checkpointing and self-restarting, thereby providing an automatic solution to the problem of making long-running scientific applications resilient to hardware faults. Our previous work focused on message-passing programs.In this paper, we describe such a system for shared-memory programs running on symmetric multiprocessors. This system has two components: (i) a pre-compiler for source-to-source modification of applications, and (ii) a runtime system that implements a protocol for coordinating CPR among the threads of the parallel application. For the sake of concreteness, we focus on a non-trivial subset of OpenMP that includes barriers and locks.One of the advantages of this approach is that the ability to tolerate faults becomes embedded within the application itself, so applications become self-checkpointing and self-restarting on any platform. We demonstrate this by showing that our transformed benchmarks can checkpoint and restart on three different platforms (Windows/x86, Linux/x86, and Tru64/Alpha). Our experiments show that the overhead introduced by this approach is usually quite small; they also suggest ways in which the current implementation can be tuned to reduced overheads further.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {235--247},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024421},
 doi = {http://doi.acm.org/10.1145/1037187.1024421},
 acmid = {1024421},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointing, fault-tolerance, openMP, shared-memory programs},
} 

@article{Wu:2004:FOM:1037949.1024423,
 author = {Wu, Qiang and Juang, Philo and Martonosi, Margaret and Clark, Douglas W.},
 title = {Formal online methods for voltage/frequency control in multiple clock domain microprocessors},
 abstract = {Multiple Clock Domain (MCD) processors are a promising future alternative to today's fully synchronous designs. Dynamic Voltage and Frequency Scaling (DVFS) in an MCD processor has the extra flexibility to adjust the voltage and frequency in each domain independently. Most existing DVFS approaches are profile-based offline schemes which are mainly suitable for applications whose execution char-acteristics are constrained and repeatable. While some work has been published about online DVFS schemes, the prior approaches are typically heuristic-based. In this paper, we present an effective online DVFS scheme for an MCD processor which takes a formal analytic approach, is driven by dynamic workloads, and is suitable for all applications. In our approach, we model an MCD processor as a queue-domain network and the online DVFS as a feedback control problem with issue queue occupancies as feedback signals. A dynamic stochastic queuing model is first proposed and linearized through an accu-rate linearization technique. A controller is then designed and verified by stability analysis. Finally we evaluate our DVFS scheme through a cycle-accurate simulation with a broad set of applications selected from MediaBench and SPEC2000 benchmark suites. Compared to the best-known prior approach, which is heuristic-based, the proposed online DVFS scheme is substantially more effective due to its automatic regulation ability. For example, we have achieved a 2-3 fold increase in efficiency in terms of energy-delay product improvement. In addition, our control theoretic technique is more resilient, requires less tuning effort, and has better scalability as compared to prior online DVFS schemes.We believe that the techniques and methodology described in this paper can be generalized for energy control in processors other than MCD, such as tiled stream processors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037949.1024423},
 doi = {http://doi.acm.org/10.1145/1037949.1024423},
 acmid = {1024423},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MCD processors, dynamic voltage/frequency scaling, formal methods},
} 

@article{Wu:2004:FOM:1037947.1024423,
 author = {Wu, Qiang and Juang, Philo and Martonosi, Margaret and Clark, Douglas W.},
 title = {Formal online methods for voltage/frequency control in multiple clock domain microprocessors},
 abstract = {Multiple Clock Domain (MCD) processors are a promising future alternative to today's fully synchronous designs. Dynamic Voltage and Frequency Scaling (DVFS) in an MCD processor has the extra flexibility to adjust the voltage and frequency in each domain independently. Most existing DVFS approaches are profile-based offline schemes which are mainly suitable for applications whose execution char-acteristics are constrained and repeatable. While some work has been published about online DVFS schemes, the prior approaches are typically heuristic-based. In this paper, we present an effective online DVFS scheme for an MCD processor which takes a formal analytic approach, is driven by dynamic workloads, and is suitable for all applications. In our approach, we model an MCD processor as a queue-domain network and the online DVFS as a feedback control problem with issue queue occupancies as feedback signals. A dynamic stochastic queuing model is first proposed and linearized through an accu-rate linearization technique. A controller is then designed and verified by stability analysis. Finally we evaluate our DVFS scheme through a cycle-accurate simulation with a broad set of applications selected from MediaBench and SPEC2000 benchmark suites. Compared to the best-known prior approach, which is heuristic-based, the proposed online DVFS scheme is substantially more effective due to its automatic regulation ability. For example, we have achieved a 2-3 fold increase in efficiency in terms of energy-delay product improvement. In addition, our control theoretic technique is more resilient, requires less tuning effort, and has better scalability as compared to prior online DVFS schemes.We believe that the techniques and methodology described in this paper can be generalized for energy control in processors other than MCD, such as tiled stream processors.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037947.1024423},
 doi = {http://doi.acm.org/10.1145/1037947.1024423},
 acmid = {1024423},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MCD processors, dynamic voltage/frequency scaling, formal methods},
} 

@article{Wu:2004:FOM:1037187.1024423,
 author = {Wu, Qiang and Juang, Philo and Martonosi, Margaret and Clark, Douglas W.},
 title = {Formal online methods for voltage/frequency control in multiple clock domain microprocessors},
 abstract = {Multiple Clock Domain (MCD) processors are a promising future alternative to today's fully synchronous designs. Dynamic Voltage and Frequency Scaling (DVFS) in an MCD processor has the extra flexibility to adjust the voltage and frequency in each domain independently. Most existing DVFS approaches are profile-based offline schemes which are mainly suitable for applications whose execution char-acteristics are constrained and repeatable. While some work has been published about online DVFS schemes, the prior approaches are typically heuristic-based. In this paper, we present an effective online DVFS scheme for an MCD processor which takes a formal analytic approach, is driven by dynamic workloads, and is suitable for all applications. In our approach, we model an MCD processor as a queue-domain network and the online DVFS as a feedback control problem with issue queue occupancies as feedback signals. A dynamic stochastic queuing model is first proposed and linearized through an accu-rate linearization technique. A controller is then designed and verified by stability analysis. Finally we evaluate our DVFS scheme through a cycle-accurate simulation with a broad set of applications selected from MediaBench and SPEC2000 benchmark suites. Compared to the best-known prior approach, which is heuristic-based, the proposed online DVFS scheme is substantially more effective due to its automatic regulation ability. For example, we have achieved a 2-3 fold increase in efficiency in terms of energy-delay product improvement. In addition, our control theoretic technique is more resilient, requires less tuning effort, and has better scalability as compared to prior online DVFS schemes.We believe that the techniques and methodology described in this paper can be generalized for energy control in processors other than MCD, such as tiled stream processors.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1037187.1024423},
 doi = {http://doi.acm.org/10.1145/1037187.1024423},
 acmid = {1024423},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MCD processors, dynamic voltage/frequency scaling, formal methods},
} 

@inproceedings{Wu:2004:FOM:1024393.1024423,
 author = {Wu, Qiang and Juang, Philo and Martonosi, Margaret and Clark, Douglas W.},
 title = {Formal online methods for voltage/frequency control in multiple clock domain microprocessors},
 abstract = {Multiple Clock Domain (MCD) processors are a promising future alternative to today's fully synchronous designs. Dynamic Voltage and Frequency Scaling (DVFS) in an MCD processor has the extra flexibility to adjust the voltage and frequency in each domain independently. Most existing DVFS approaches are profile-based offline schemes which are mainly suitable for applications whose execution char-acteristics are constrained and repeatable. While some work has been published about online DVFS schemes, the prior approaches are typically heuristic-based. In this paper, we present an effective online DVFS scheme for an MCD processor which takes a formal analytic approach, is driven by dynamic workloads, and is suitable for all applications. In our approach, we model an MCD processor as a queue-domain network and the online DVFS as a feedback control problem with issue queue occupancies as feedback signals. A dynamic stochastic queuing model is first proposed and linearized through an accu-rate linearization technique. A controller is then designed and verified by stability analysis. Finally we evaluate our DVFS scheme through a cycle-accurate simulation with a broad set of applications selected from MediaBench and SPEC2000 benchmark suites. Compared to the best-known prior approach, which is heuristic-based, the proposed online DVFS scheme is substantially more effective due to its automatic regulation ability. For example, we have achieved a 2-3 fold increase in efficiency in terms of energy-delay product improvement. In addition, our control theoretic technique is more resilient, requires less tuning effort, and has better scalability as compared to prior online DVFS schemes.We believe that the techniques and methodology described in this paper can be generalized for energy control in processors other than MCD, such as tiled stream processors.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1024393.1024423},
 doi = {http://doi.acm.org/10.1145/1024393.1024423},
 acmid = {1024423},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MCD processors, dynamic voltage/frequency scaling, formal methods},
} 

@article{Gomaa:2004:HLS:1037949.1024424,
 author = {Gomaa, Mohamed and Powell, Michael D. and Vijaykumar, T. N.},
 title = {Heat-and-run: leveraging SMT and CMP to manage power density through the operating system},
 abstract = {Power density in high-performance processors continues to increase with technology generations as scaling of current, clock speed, and device density outpaces the downscaling of supply voltage and thermal ability of packages to dissipate heat. Power density is characterized by localized chip hot spots that can reach critical temperatures and cause failure. Previous architectural approaches to power density have used global clock gating, fetch toggling, dynamic frequency scaling, or resource duplication to either prevent heating or relieve overheated resources in a superscalar processor. Previous approaches also evaluate design technologies where power density is not a major problem and most applications do not overheat the processor. Future processors, however, are likely to be chip multiprocessors (CMPs) with simultaneously-multithreaded (SMT) cores. SMT CMPs pose unique challenges and opportunities for power density. SMT and CMP increase throughput and thus on-chip heat, but also provide natural granularities for managing power-density. This paper is the first work to leverage SMT and CMP to address power density. We propose heat-and-run SMT thread assignment to increase processor-resource utilization before cooling becomes necessary by co-scheduling threads that use complimentary resources. We propose heat-and-run CMP thread migration to migrate threads away from overheated cores and assign them to free SMT contexts on alternate cores, leveraging availability of SMT contexts on alternate CMP cores to maintain throughput while allowing overheated cores to cool. We show that our proposal has an average of 9\% and up to 34\% higher throughput than a previous superscalar technique running the same number of threads.</i>},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {260--270},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037949.1024424},
 doi = {http://doi.acm.org/10.1145/1037949.1024424},
 acmid = {1024424},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, SMT, heat, migration, power density},
} 

@inproceedings{Gomaa:2004:HLS:1024393.1024424,
 author = {Gomaa, Mohamed and Powell, Michael D. and Vijaykumar, T. N.},
 title = {Heat-and-run: leveraging SMT and CMP to manage power density through the operating system},
 abstract = {Power density in high-performance processors continues to increase with technology generations as scaling of current, clock speed, and device density outpaces the downscaling of supply voltage and thermal ability of packages to dissipate heat. Power density is characterized by localized chip hot spots that can reach critical temperatures and cause failure. Previous architectural approaches to power density have used global clock gating, fetch toggling, dynamic frequency scaling, or resource duplication to either prevent heating or relieve overheated resources in a superscalar processor. Previous approaches also evaluate design technologies where power density is not a major problem and most applications do not overheat the processor. Future processors, however, are likely to be chip multiprocessors (CMPs) with simultaneously-multithreaded (SMT) cores. SMT CMPs pose unique challenges and opportunities for power density. SMT and CMP increase throughput and thus on-chip heat, but also provide natural granularities for managing power-density. This paper is the first work to leverage SMT and CMP to address power density. We propose heat-and-run SMT thread assignment to increase processor-resource utilization before cooling becomes necessary by co-scheduling threads that use complimentary resources. We propose heat-and-run CMP thread migration to migrate threads away from overheated cores and assign them to free SMT contexts on alternate cores, leveraging availability of SMT contexts on alternate CMP cores to maintain throughput while allowing overheated cores to cool. We show that our proposal has an average of 9\% and up to 34\% higher throughput than a previous superscalar technique running the same number of threads.</i>},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {260--270},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1024393.1024424},
 doi = {http://doi.acm.org/10.1145/1024393.1024424},
 acmid = {1024424},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, SMT, heat, migration, power density},
} 

@article{Gomaa:2004:HLS:1037947.1024424,
 author = {Gomaa, Mohamed and Powell, Michael D. and Vijaykumar, T. N.},
 title = {Heat-and-run: leveraging SMT and CMP to manage power density through the operating system},
 abstract = {Power density in high-performance processors continues to increase with technology generations as scaling of current, clock speed, and device density outpaces the downscaling of supply voltage and thermal ability of packages to dissipate heat. Power density is characterized by localized chip hot spots that can reach critical temperatures and cause failure. Previous architectural approaches to power density have used global clock gating, fetch toggling, dynamic frequency scaling, or resource duplication to either prevent heating or relieve overheated resources in a superscalar processor. Previous approaches also evaluate design technologies where power density is not a major problem and most applications do not overheat the processor. Future processors, however, are likely to be chip multiprocessors (CMPs) with simultaneously-multithreaded (SMT) cores. SMT CMPs pose unique challenges and opportunities for power density. SMT and CMP increase throughput and thus on-chip heat, but also provide natural granularities for managing power-density. This paper is the first work to leverage SMT and CMP to address power density. We propose heat-and-run SMT thread assignment to increase processor-resource utilization before cooling becomes necessary by co-scheduling threads that use complimentary resources. We propose heat-and-run CMP thread migration to migrate threads away from overheated cores and assign them to free SMT contexts on alternate cores, leveraging availability of SMT contexts on alternate CMP cores to maintain throughput while allowing overheated cores to cool. We show that our proposal has an average of 9\% and up to 34\% higher throughput than a previous superscalar technique running the same number of threads.</i>},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {260--270},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037947.1024424},
 doi = {http://doi.acm.org/10.1145/1037947.1024424},
 acmid = {1024424},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, SMT, heat, migration, power density},
} 

@article{Gomaa:2004:HLS:1037187.1024424,
 author = {Gomaa, Mohamed and Powell, Michael D. and Vijaykumar, T. N.},
 title = {Heat-and-run: leveraging SMT and CMP to manage power density through the operating system},
 abstract = {Power density in high-performance processors continues to increase with technology generations as scaling of current, clock speed, and device density outpaces the downscaling of supply voltage and thermal ability of packages to dissipate heat. Power density is characterized by localized chip hot spots that can reach critical temperatures and cause failure. Previous architectural approaches to power density have used global clock gating, fetch toggling, dynamic frequency scaling, or resource duplication to either prevent heating or relieve overheated resources in a superscalar processor. Previous approaches also evaluate design technologies where power density is not a major problem and most applications do not overheat the processor. Future processors, however, are likely to be chip multiprocessors (CMPs) with simultaneously-multithreaded (SMT) cores. SMT CMPs pose unique challenges and opportunities for power density. SMT and CMP increase throughput and thus on-chip heat, but also provide natural granularities for managing power-density. This paper is the first work to leverage SMT and CMP to address power density. We propose heat-and-run SMT thread assignment to increase processor-resource utilization before cooling becomes necessary by co-scheduling threads that use complimentary resources. We propose heat-and-run CMP thread migration to migrate threads away from overheated cores and assign them to free SMT contexts on alternate cores, leveraging availability of SMT contexts on alternate CMP cores to maintain throughput while allowing overheated cores to cool. We show that our proposal has an average of 9\% and up to 34\% higher throughput than a previous superscalar technique running the same number of threads.</i>},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {260--270},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1037187.1024424},
 doi = {http://doi.acm.org/10.1145/1037187.1024424},
 acmid = {1024424},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, SMT, heat, migration, power density},
} 

@article{Li:2004:PDE:1037947.1024425,
 author = {Li, Xiaodong and Li, Zhenmin and David, Francis and Zhou, Pin and Zhou, Yuanyuan and Adve, Sarita and Kumar, Sanjeev},
 title = {Performance directed energy management for main memory and disks},
 abstract = {Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees. In one case, they slowed down an application by 835.This paper addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make three contributions: (1) We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-directed Dynamic</i> (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy, when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simple, optimization-based, threshold-free control algorithm, Performance-directed Static</i> (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performanceguaranteed disk algorithms, including hand-tuned versions.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {32},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5964},
 pages = {271--283},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037947.1024425},
 doi = {http://doi.acm.org/10.1145/1037947.1024425},
 acmid = {1024425},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptation algorithms, control algorithms, low power design, memory and disk energy management, multiple power mode device},
} 

@inproceedings{Li:2004:PDE:1024393.1024425,
 author = {Li, Xiaodong and Li, Zhenmin and David, Francis and Zhou, Pin and Zhou, Yuanyuan and Adve, Sarita and Kumar, Sanjeev},
 title = {Performance directed energy management for main memory and disks},
 abstract = {Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees. In one case, they slowed down an application by 835.This paper addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make three contributions: (1) We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-directed Dynamic</i> (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy, when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simple, optimization-based, threshold-free control algorithm, Performance-directed Static</i> (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performanceguaranteed disk algorithms, including hand-tuned versions.},
 booktitle = {Proceedings of the 11th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-XI},
 year = {2004},
 isbn = {1-58113-804-0},
 location = {Boston, MA, USA},
 pages = {271--283},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1024393.1024425},
 doi = {http://doi.acm.org/10.1145/1024393.1024425},
 acmid = {1024425},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptation algorithms, control algorithms, low power design, memory and disk energy management, multiple power mode device},
} 

@article{Li:2004:PDE:1037187.1024425,
 author = {Li, Xiaodong and Li, Zhenmin and David, Francis and Zhou, Pin and Zhou, Yuanyuan and Adve, Sarita and Kumar, Sanjeev},
 title = {Performance directed energy management for main memory and disks},
 abstract = {Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees. In one case, they slowed down an application by 835.This paper addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make three contributions: (1) We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-directed Dynamic</i> (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy, when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simple, optimization-based, threshold-free control algorithm, Performance-directed Static</i> (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performanceguaranteed disk algorithms, including hand-tuned versions.},
 journal = {SIGPLAN Not.},
 volume = {39},
 issue = {11},
 month = {October},
 year = {2004},
 issn = {0362-1340},
 pages = {271--283},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037187.1024425},
 doi = {http://doi.acm.org/10.1145/1037187.1024425},
 acmid = {1024425},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptation algorithms, control algorithms, low power design, memory and disk energy management, multiple power mode device},
} 

@article{Li:2004:PDE:1037949.1024425,
 author = {Li, Xiaodong and Li, Zhenmin and David, Francis and Zhou, Pin and Zhou, Yuanyuan and Adve, Sarita and Kumar, Sanjeev},
 title = {Performance directed energy management for main memory and disks},
 abstract = {Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees. In one case, they slowed down an application by 835.This paper addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make three contributions: (1) We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-directed Dynamic</i> (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy, when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simple, optimization-based, threshold-free control algorithm, Performance-directed Static</i> (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performanceguaranteed disk algorithms, including hand-tuned versions.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {38},
 issue = {5},
 month = {October},
 year = {2004},
 issn = {0163-5980},
 pages = {271--283},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1037949.1024425},
 doi = {http://doi.acm.org/10.1145/1037949.1024425},
 acmid = {1024425},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptation algorithms, control algorithms, low power design, memory and disk energy management, multiple power mode device},
} 

@inproceedings{Estrin:2002:KAS:605397.1090192,
 author = {Estrin, Deborah},
 title = {Keynote address: Sensor network research: emerging challenges for architecture, systems, and languages},
 abstract = {},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/605397.1090192},
 doi = {http://doi.acm.org/10.1145/605397.1090192},
 acmid = {1090192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Estrin:2002:KAS:635506.1090192,
 author = {Estrin, Deborah},
 title = {Keynote address: Sensor network research: emerging challenges for architecture, systems, and languages},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/635506.1090192},
 doi = {http://doi.acm.org/10.1145/635506.1090192},
 acmid = {1090192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Estrin:2002:KAS:635508.1090192,
 author = {Estrin, Deborah},
 title = {Keynote address: Sensor network research: emerging challenges for architecture, systems, and languages},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/635508.1090192},
 doi = {http://doi.acm.org/10.1145/635508.1090192},
 acmid = {1090192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Estrin:2002:KAS:605432.1090192,
 author = {Estrin, Deborah},
 title = {Keynote address: Sensor network research: emerging challenges for architecture, systems, and languages},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/605432.1090192},
 doi = {http://doi.acm.org/10.1145/605432.1090192},
 acmid = {1090192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rajwar:2002:TLE:635508.605399,
 author = {Rajwar, Ravi and Goodman, James R.},
 title = {Transactional lock-free execution of lock-based programs},
 abstract = {This paper is motivated by the difficulty in writing correct high-performance programs. Writing shared-memory multi-threaded programs imposes a complex trade-off between programming ease and performance, largely due to subtleties in coordinating access to shared data. To ensure correctness programmers often rely on conservative locking at the expense of performance. The resulting serialization of threads is a performance bottleneck. Locks also interact poorly with thread scheduling and faults, resulting in poor system performance.We seek to improve multithreaded programming trade-offs by providing architectural support for optimistic lock-free execution. In a lock-free execution, shared objects are never locked when accessed by various threads. We propose Transactional Lock Removal</i> (TLR) and show how a program that uses lock-based synchronization can be executed by the hardware in a lock-free manner, even in the presence of conflicts, without programmer support or software changes. TLR uses timestamps for conflict resolution, modest hardware, and features already present in many modern computer systems.TLR's benefits include improved programmability, stability, and performance. Programmers can obtain benefits of lock-free data structures, such as non-blocking behavior and wait-freedom, while using lock-protected critical sections for writing programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {5--17},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605399},
 doi = {http://doi.acm.org/10.1145/635508.605399},
 acmid = {605399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rajwar:2002:TLE:635506.605399,
 author = {Rajwar, Ravi and Goodman, James R.},
 title = {Transactional lock-free execution of lock-based programs},
 abstract = {This paper is motivated by the difficulty in writing correct high-performance programs. Writing shared-memory multi-threaded programs imposes a complex trade-off between programming ease and performance, largely due to subtleties in coordinating access to shared data. To ensure correctness programmers often rely on conservative locking at the expense of performance. The resulting serialization of threads is a performance bottleneck. Locks also interact poorly with thread scheduling and faults, resulting in poor system performance.We seek to improve multithreaded programming trade-offs by providing architectural support for optimistic lock-free execution. In a lock-free execution, shared objects are never locked when accessed by various threads. We propose Transactional Lock Removal</i> (TLR) and show how a program that uses lock-based synchronization can be executed by the hardware in a lock-free manner, even in the presence of conflicts, without programmer support or software changes. TLR uses timestamps for conflict resolution, modest hardware, and features already present in many modern computer systems.TLR's benefits include improved programmability, stability, and performance. Programmers can obtain benefits of lock-free data structures, such as non-blocking behavior and wait-freedom, while using lock-protected critical sections for writing programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {5--17},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605399},
 doi = {http://doi.acm.org/10.1145/635506.605399},
 acmid = {605399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rajwar:2002:TLE:605397.605399,
 author = {Rajwar, Ravi and Goodman, James R.},
 title = {Transactional lock-free execution of lock-based programs},
 abstract = {This paper is motivated by the difficulty in writing correct high-performance programs. Writing shared-memory multi-threaded programs imposes a complex trade-off between programming ease and performance, largely due to subtleties in coordinating access to shared data. To ensure correctness programmers often rely on conservative locking at the expense of performance. The resulting serialization of threads is a performance bottleneck. Locks also interact poorly with thread scheduling and faults, resulting in poor system performance.We seek to improve multithreaded programming trade-offs by providing architectural support for optimistic lock-free execution. In a lock-free execution, shared objects are never locked when accessed by various threads. We propose Transactional Lock Removal</i> (TLR) and show how a program that uses lock-based synchronization can be executed by the hardware in a lock-free manner, even in the presence of conflicts, without programmer support or software changes. TLR uses timestamps for conflict resolution, modest hardware, and features already present in many modern computer systems.TLR's benefits include improved programmability, stability, and performance. Programmers can obtain benefits of lock-free data structures, such as non-blocking behavior and wait-freedom, while using lock-protected critical sections for writing programs.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {5--17},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605399},
 doi = {http://doi.acm.org/10.1145/605397.605399},
 acmid = {605399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rajwar:2002:TLE:605432.605399,
 author = {Rajwar, Ravi and Goodman, James R.},
 title = {Transactional lock-free execution of lock-based programs},
 abstract = {This paper is motivated by the difficulty in writing correct high-performance programs. Writing shared-memory multi-threaded programs imposes a complex trade-off between programming ease and performance, largely due to subtleties in coordinating access to shared data. To ensure correctness programmers often rely on conservative locking at the expense of performance. The resulting serialization of threads is a performance bottleneck. Locks also interact poorly with thread scheduling and faults, resulting in poor system performance.We seek to improve multithreaded programming trade-offs by providing architectural support for optimistic lock-free execution. In a lock-free execution, shared objects are never locked when accessed by various threads. We propose Transactional Lock Removal</i> (TLR) and show how a program that uses lock-based synchronization can be executed by the hardware in a lock-free manner, even in the presence of conflicts, without programmer support or software changes. TLR uses timestamps for conflict resolution, modest hardware, and features already present in many modern computer systems.TLR's benefits include improved programmability, stability, and performance. Programmers can obtain benefits of lock-free data structures, such as non-blocking behavior and wait-freedom, while using lock-protected critical sections for writing programs.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {5--17},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605399},
 doi = {http://doi.acm.org/10.1145/605432.605399},
 acmid = {605399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martinez:2002:SSA:605432.605400,
 author = {Mart\'{\i}nez, Jos\'{e} F. and Torrellas, Josep},
 title = {Speculative synchronization: applying thread-level speculation to explicitly parallel applications},
 abstract = {Barriers, locks, and flags are synchronizing operations widely used programmers and parallelizing compilers to produce race-free parallel programs. Often times, these operations are placed suboptimally, either because of conservative assumptions about the program, or merely for code simplicity.We propose Speculative Synchronization,</i> which applies the philosophy behind Thread-Level Speculation (TLS) to explicitly parallel applications. Speculative threads execute past active barriers, busy locks, and unset flags instead of waiting. The proposed hardware checks for conflicting accesses and, if a violation is detected, offending speculative thread is rolled back to the synchronization point and restarted on the fly. TLS's principle of always keeping a safe thread</i> is key to our proposal: in any speculative barrier, lock, or flag, the existence of one or more safe threads at all times guarantees forward progress, even in the presence of access conflicts or speculative buffer overflow. Our proposal requires simple hardware and no programming effort. Furthermore, it can coexist with conventional synchronization at run time.We use simulations to evaluate 5 compiler- and hand-parallelized applications. Our results show a reduction in the time lost to synchronization of 34\% on average, and a reduction in overall program execution time of 7.4\% on average.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {18--29},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605400},
 doi = {http://doi.acm.org/10.1145/605432.605400},
 acmid = {605400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martinez:2002:SSA:635508.605400,
 author = {Mart\'{\i}nez, Jos\'{e} F. and Torrellas, Josep},
 title = {Speculative synchronization: applying thread-level speculation to explicitly parallel applications},
 abstract = {Barriers, locks, and flags are synchronizing operations widely used programmers and parallelizing compilers to produce race-free parallel programs. Often times, these operations are placed suboptimally, either because of conservative assumptions about the program, or merely for code simplicity.We propose Speculative Synchronization,</i> which applies the philosophy behind Thread-Level Speculation (TLS) to explicitly parallel applications. Speculative threads execute past active barriers, busy locks, and unset flags instead of waiting. The proposed hardware checks for conflicting accesses and, if a violation is detected, offending speculative thread is rolled back to the synchronization point and restarted on the fly. TLS's principle of always keeping a safe thread</i> is key to our proposal: in any speculative barrier, lock, or flag, the existence of one or more safe threads at all times guarantees forward progress, even in the presence of access conflicts or speculative buffer overflow. Our proposal requires simple hardware and no programming effort. Furthermore, it can coexist with conventional synchronization at run time.We use simulations to evaluate 5 compiler- and hand-parallelized applications. Our results show a reduction in the time lost to synchronization of 34\% on average, and a reduction in overall program execution time of 7.4\% on average.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {18--29},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605400},
 doi = {http://doi.acm.org/10.1145/635508.605400},
 acmid = {605400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martinez:2002:SSA:635506.605400,
 author = {Mart\'{\i}nez, Jos\'{e} F. and Torrellas, Josep},
 title = {Speculative synchronization: applying thread-level speculation to explicitly parallel applications},
 abstract = {Barriers, locks, and flags are synchronizing operations widely used programmers and parallelizing compilers to produce race-free parallel programs. Often times, these operations are placed suboptimally, either because of conservative assumptions about the program, or merely for code simplicity.We propose Speculative Synchronization,</i> which applies the philosophy behind Thread-Level Speculation (TLS) to explicitly parallel applications. Speculative threads execute past active barriers, busy locks, and unset flags instead of waiting. The proposed hardware checks for conflicting accesses and, if a violation is detected, offending speculative thread is rolled back to the synchronization point and restarted on the fly. TLS's principle of always keeping a safe thread</i> is key to our proposal: in any speculative barrier, lock, or flag, the existence of one or more safe threads at all times guarantees forward progress, even in the presence of access conflicts or speculative buffer overflow. Our proposal requires simple hardware and no programming effort. Furthermore, it can coexist with conventional synchronization at run time.We use simulations to evaluate 5 compiler- and hand-parallelized applications. Our results show a reduction in the time lost to synchronization of 34\% on average, and a reduction in overall program execution time of 7.4\% on average.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {18--29},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605400},
 doi = {http://doi.acm.org/10.1145/635506.605400},
 acmid = {605400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Martinez:2002:SSA:605397.605400,
 author = {Mart\'{\i}nez, Jos\'{e} F. and Torrellas, Josep},
 title = {Speculative synchronization: applying thread-level speculation to explicitly parallel applications},
 abstract = {Barriers, locks, and flags are synchronizing operations widely used programmers and parallelizing compilers to produce race-free parallel programs. Often times, these operations are placed suboptimally, either because of conservative assumptions about the program, or merely for code simplicity.We propose Speculative Synchronization,</i> which applies the philosophy behind Thread-Level Speculation (TLS) to explicitly parallel applications. Speculative threads execute past active barriers, busy locks, and unset flags instead of waiting. The proposed hardware checks for conflicting accesses and, if a violation is detected, offending speculative thread is rolled back to the synchronization point and restarted on the fly. TLS's principle of always keeping a safe thread</i> is key to our proposal: in any speculative barrier, lock, or flag, the existence of one or more safe threads at all times guarantees forward progress, even in the presence of access conflicts or speculative buffer overflow. Our proposal requires simple hardware and no programming effort. Furthermore, it can coexist with conventional synchronization at run time.We use simulations to evaluate 5 compiler- and hand-parallelized applications. Our results show a reduction in the time lost to synchronization of 34\% on average, and a reduction in overall program execution time of 7.4\% on average.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {18--29},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605400},
 doi = {http://doi.acm.org/10.1145/605397.605400},
 acmid = {605400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lepak:2002:TSS:635508.605401,
 author = {Lepak, Kevin M. and Lipasti, Mikko H.},
 title = {Temporally silent stores},
 abstract = {Recent work has shown that silent stores--stores which write a value matching the one already stored at the memory location--occur quite frequently and can be exploited to reduce memory traffic and improve performance. This paper extends the definition of silent stores to encompass sets of stores that change the value stored at a memory location, but only temporarily, and subsequently return a previous value of interest to the memory location. The stores that cause the value to revert are called temporally silent stores. We redefine multiprocessor sharing to account for temporal silence and show that in the limit, up to 45\% of communication misses in scientific and commercial applications can be eliminated by exploiting values that change only temporarily. We describe a practical mechanism that detects temporally silent stores and removes the coherence traffic they cause in conventional multiprocessors. We find that up to 42\% of communication misses can be eliminated with a simple extension to the MESI protocol. Further, we examine application and operating system code to provide insight into the temporal silence phenomenon and characterize temporal silence by examining value frequencies and dynamic instruction distances between temporally silent pairs. These studies indicate that the operating system is involved heavily in temporal silence, in both commercial and scientific workloads, and that while detectable synchronization primitives provide substantial contributions, significant opportunity exists outside these references.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {30--41},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605401},
 doi = {http://doi.acm.org/10.1145/635508.605401},
 acmid = {605401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lepak:2002:TSS:605432.605401,
 author = {Lepak, Kevin M. and Lipasti, Mikko H.},
 title = {Temporally silent stores},
 abstract = {Recent work has shown that silent stores--stores which write a value matching the one already stored at the memory location--occur quite frequently and can be exploited to reduce memory traffic and improve performance. This paper extends the definition of silent stores to encompass sets of stores that change the value stored at a memory location, but only temporarily, and subsequently return a previous value of interest to the memory location. The stores that cause the value to revert are called temporally silent stores. We redefine multiprocessor sharing to account for temporal silence and show that in the limit, up to 45\% of communication misses in scientific and commercial applications can be eliminated by exploiting values that change only temporarily. We describe a practical mechanism that detects temporally silent stores and removes the coherence traffic they cause in conventional multiprocessors. We find that up to 42\% of communication misses can be eliminated with a simple extension to the MESI protocol. Further, we examine application and operating system code to provide insight into the temporal silence phenomenon and characterize temporal silence by examining value frequencies and dynamic instruction distances between temporally silent pairs. These studies indicate that the operating system is involved heavily in temporal silence, in both commercial and scientific workloads, and that while detectable synchronization primitives provide substantial contributions, significant opportunity exists outside these references.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {30--41},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605401},
 doi = {http://doi.acm.org/10.1145/605432.605401},
 acmid = {605401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lepak:2002:TSS:605397.605401,
 author = {Lepak, Kevin M. and Lipasti, Mikko H.},
 title = {Temporally silent stores},
 abstract = {Recent work has shown that silent stores--stores which write a value matching the one already stored at the memory location--occur quite frequently and can be exploited to reduce memory traffic and improve performance. This paper extends the definition of silent stores to encompass sets of stores that change the value stored at a memory location, but only temporarily, and subsequently return a previous value of interest to the memory location. The stores that cause the value to revert are called temporally silent stores. We redefine multiprocessor sharing to account for temporal silence and show that in the limit, up to 45\% of communication misses in scientific and commercial applications can be eliminated by exploiting values that change only temporarily. We describe a practical mechanism that detects temporally silent stores and removes the coherence traffic they cause in conventional multiprocessors. We find that up to 42\% of communication misses can be eliminated with a simple extension to the MESI protocol. Further, we examine application and operating system code to provide insight into the temporal silence phenomenon and characterize temporal silence by examining value frequencies and dynamic instruction distances between temporally silent pairs. These studies indicate that the operating system is involved heavily in temporal silence, in both commercial and scientific workloads, and that while detectable synchronization primitives provide substantial contributions, significant opportunity exists outside these references.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {30--41},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605401},
 doi = {http://doi.acm.org/10.1145/605397.605401},
 acmid = {605401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lepak:2002:TSS:635506.605401,
 author = {Lepak, Kevin M. and Lipasti, Mikko H.},
 title = {Temporally silent stores},
 abstract = {Recent work has shown that silent stores--stores which write a value matching the one already stored at the memory location--occur quite frequently and can be exploited to reduce memory traffic and improve performance. This paper extends the definition of silent stores to encompass sets of stores that change the value stored at a memory location, but only temporarily, and subsequently return a previous value of interest to the memory location. The stores that cause the value to revert are called temporally silent stores. We redefine multiprocessor sharing to account for temporal silence and show that in the limit, up to 45\% of communication misses in scientific and commercial applications can be eliminated by exploiting values that change only temporarily. We describe a practical mechanism that detects temporally silent stores and removes the coherence traffic they cause in conventional multiprocessors. We find that up to 42\% of communication misses can be eliminated with a simple extension to the MESI protocol. Further, we examine application and operating system code to provide insight into the temporal silence phenomenon and characterize temporal silence by examining value frequencies and dynamic instruction distances between temporally silent pairs. These studies indicate that the operating system is involved heavily in temporal silence, in both commercial and scientific workloads, and that while detectable synchronization primitives provide substantial contributions, significant opportunity exists outside these references.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {30--41},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605401},
 doi = {http://doi.acm.org/10.1145/635506.605401},
 acmid = {605401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sherwood:2002:ACL:635508.605403,
 author = {Sherwood, Timothy and Perelman, Erez and Hamerly, Greg and Calder, Brad},
 title = {Automatically characterizing large scale program behavior},
 abstract = {Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the very largest of scales (over the complete execution of the program). This realization has ramifications for many architectural and compiler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we must first develop the analytical tools necessary to automatically and efficiently analyze program behavior over large sections of execution.Our goal is to develop automatic techniques that are capable of finding and exploiting the Large Scale Behavior</i> of programs (behavior seen over billions of instructions). The first step towards this goal is the development of a hardware independent metric that can concisely summarize the behavior of an arbitrary section of execution in a program. To this end we examine the use of Basic Block Vectors.</i> We quantify the effectiveness of Basic Block Vectors in capturing program behavior across several different architectural metrics, explore the large scale behavior of several programs, and develop a set of algorithms based on clustering capable of analyzing this behavior. We then demonstrate an application of this technology to automatically determine where to simulate for a program to help guide computer architecture research.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {45--57},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605403},
 doi = {http://doi.acm.org/10.1145/635508.605403},
 acmid = {605403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sherwood:2002:ACL:605432.605403,
 author = {Sherwood, Timothy and Perelman, Erez and Hamerly, Greg and Calder, Brad},
 title = {Automatically characterizing large scale program behavior},
 abstract = {Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the very largest of scales (over the complete execution of the program). This realization has ramifications for many architectural and compiler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we must first develop the analytical tools necessary to automatically and efficiently analyze program behavior over large sections of execution.Our goal is to develop automatic techniques that are capable of finding and exploiting the Large Scale Behavior</i> of programs (behavior seen over billions of instructions). The first step towards this goal is the development of a hardware independent metric that can concisely summarize the behavior of an arbitrary section of execution in a program. To this end we examine the use of Basic Block Vectors.</i> We quantify the effectiveness of Basic Block Vectors in capturing program behavior across several different architectural metrics, explore the large scale behavior of several programs, and develop a set of algorithms based on clustering capable of analyzing this behavior. We then demonstrate an application of this technology to automatically determine where to simulate for a program to help guide computer architecture research.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {45--57},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605403},
 doi = {http://doi.acm.org/10.1145/605432.605403},
 acmid = {605403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sherwood:2002:ACL:635506.605403,
 author = {Sherwood, Timothy and Perelman, Erez and Hamerly, Greg and Calder, Brad},
 title = {Automatically characterizing large scale program behavior},
 abstract = {Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the very largest of scales (over the complete execution of the program). This realization has ramifications for many architectural and compiler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we must first develop the analytical tools necessary to automatically and efficiently analyze program behavior over large sections of execution.Our goal is to develop automatic techniques that are capable of finding and exploiting the Large Scale Behavior</i> of programs (behavior seen over billions of instructions). The first step towards this goal is the development of a hardware independent metric that can concisely summarize the behavior of an arbitrary section of execution in a program. To this end we examine the use of Basic Block Vectors.</i> We quantify the effectiveness of Basic Block Vectors in capturing program behavior across several different architectural metrics, explore the large scale behavior of several programs, and develop a set of algorithms based on clustering capable of analyzing this behavior. We then demonstrate an application of this technology to automatically determine where to simulate for a program to help guide computer architecture research.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {45--57},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605403},
 doi = {http://doi.acm.org/10.1145/635506.605403},
 acmid = {605403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sherwood:2002:ACL:605397.605403,
 author = {Sherwood, Timothy and Perelman, Erez and Hamerly, Greg and Calder, Brad},
 title = {Automatically characterizing large scale program behavior},
 abstract = {Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the very largest of scales (over the complete execution of the program). This realization has ramifications for many architectural and compiler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we must first develop the analytical tools necessary to automatically and efficiently analyze program behavior over large sections of execution.Our goal is to develop automatic techniques that are capable of finding and exploiting the Large Scale Behavior</i> of programs (behavior seen over billions of instructions). The first step towards this goal is the development of a hardware independent metric that can concisely summarize the behavior of an arbitrary section of execution in a program. To this end we examine the use of Basic Block Vectors.</i> We quantify the effectiveness of Basic Block Vectors in capturing program behavior across several different architectural metrics, explore the large scale behavior of several programs, and develop a set of algorithms based on clustering capable of analyzing this behavior. We then demonstrate an application of this technology to automatically determine where to simulate for a program to help guide computer architecture research.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {45--57},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605403},
 doi = {http://doi.acm.org/10.1145/605397.605403},
 acmid = {605403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ogata:2002:BFO:605432.605404,
 author = {Ogata, Kazunori and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Bytecode fetch optimization for a Java interpreter},
 abstract = {Interpreters play an important role in many languages, and their performance is critical particularly for the popular language Java. The performance of the interpreter is important even for high-performance virtual machines that employ just-in-time compiler technology, because there are advantages in delaying the start of compilation and in reducing the number of the target methods to be compiled. Many techniques have been proposed to improve the performance of various interpreters, but none of them has fully addressed the issues of minimizing redundant memory accesses and the overhead of indirect branches inherent to interpreters running on superscalar processors. These issues are especially serious for Java because each bytecode is typically one or a few bytes long and the execution routine for each bytecode is also short due to the low-level, stack-based semantics of Java bytecode. In this paper, we describe three novel techniques of our Java bytecode interpreter, write-through top-of-stack caching (WT), position-based handler customization (PHC), and position-based speculative decoding (PSD),</i> which ameliorate these problems for the PowerPC processors. We show how each technique contributes to improving the overall performance of the interpreter for major Java benchmark programs on an IBM POWER3 processor. Among three, PHC is the most effective one. We also show that the main source of memory accesses is due to bytecode fetches and that PHC successfully eliminates the majority of them, while it keeps the instruction cache miss ratios small.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {58--67},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/605432.605404},
 doi = {http://doi.acm.org/10.1145/605432.605404},
 acmid = {605404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, PowerPC, bytecode interpreter, performance, pipelined interpreter, stack caching, superscalar processor},
} 

@article{Ogata:2002:BFO:635506.605404,
 author = {Ogata, Kazunori and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Bytecode fetch optimization for a Java interpreter},
 abstract = {Interpreters play an important role in many languages, and their performance is critical particularly for the popular language Java. The performance of the interpreter is important even for high-performance virtual machines that employ just-in-time compiler technology, because there are advantages in delaying the start of compilation and in reducing the number of the target methods to be compiled. Many techniques have been proposed to improve the performance of various interpreters, but none of them has fully addressed the issues of minimizing redundant memory accesses and the overhead of indirect branches inherent to interpreters running on superscalar processors. These issues are especially serious for Java because each bytecode is typically one or a few bytes long and the execution routine for each bytecode is also short due to the low-level, stack-based semantics of Java bytecode. In this paper, we describe three novel techniques of our Java bytecode interpreter, write-through top-of-stack caching (WT), position-based handler customization (PHC), and position-based speculative decoding (PSD),</i> which ameliorate these problems for the PowerPC processors. We show how each technique contributes to improving the overall performance of the interpreter for major Java benchmark programs on an IBM POWER3 processor. Among three, PHC is the most effective one. We also show that the main source of memory accesses is due to bytecode fetches and that PHC successfully eliminates the majority of them, while it keeps the instruction cache miss ratios small.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {58--67},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/635506.605404},
 doi = {http://doi.acm.org/10.1145/635506.605404},
 acmid = {605404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, PowerPC, bytecode interpreter, performance, pipelined interpreter, stack caching, superscalar processor},
} 

@inproceedings{Ogata:2002:BFO:605397.605404,
 author = {Ogata, Kazunori and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Bytecode fetch optimization for a Java interpreter},
 abstract = {Interpreters play an important role in many languages, and their performance is critical particularly for the popular language Java. The performance of the interpreter is important even for high-performance virtual machines that employ just-in-time compiler technology, because there are advantages in delaying the start of compilation and in reducing the number of the target methods to be compiled. Many techniques have been proposed to improve the performance of various interpreters, but none of them has fully addressed the issues of minimizing redundant memory accesses and the overhead of indirect branches inherent to interpreters running on superscalar processors. These issues are especially serious for Java because each bytecode is typically one or a few bytes long and the execution routine for each bytecode is also short due to the low-level, stack-based semantics of Java bytecode. In this paper, we describe three novel techniques of our Java bytecode interpreter, write-through top-of-stack caching (WT), position-based handler customization (PHC), and position-based speculative decoding (PSD),</i> which ameliorate these problems for the PowerPC processors. We show how each technique contributes to improving the overall performance of the interpreter for major Java benchmark programs on an IBM POWER3 processor. Among three, PHC is the most effective one. We also show that the main source of memory accesses is due to bytecode fetches and that PHC successfully eliminates the majority of them, while it keeps the instruction cache miss ratios small.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {58--67},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/605397.605404},
 doi = {http://doi.acm.org/10.1145/605397.605404},
 acmid = {605404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, PowerPC, bytecode interpreter, performance, pipelined interpreter, stack caching, superscalar processor},
} 

@article{Ogata:2002:BFO:635508.605404,
 author = {Ogata, Kazunori and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Bytecode fetch optimization for a Java interpreter},
 abstract = {Interpreters play an important role in many languages, and their performance is critical particularly for the popular language Java. The performance of the interpreter is important even for high-performance virtual machines that employ just-in-time compiler technology, because there are advantages in delaying the start of compilation and in reducing the number of the target methods to be compiled. Many techniques have been proposed to improve the performance of various interpreters, but none of them has fully addressed the issues of minimizing redundant memory accesses and the overhead of indirect branches inherent to interpreters running on superscalar processors. These issues are especially serious for Java because each bytecode is typically one or a few bytes long and the execution routine for each bytecode is also short due to the low-level, stack-based semantics of Java bytecode. In this paper, we describe three novel techniques of our Java bytecode interpreter, write-through top-of-stack caching (WT), position-based handler customization (PHC), and position-based speculative decoding (PSD),</i> which ameliorate these problems for the PowerPC processors. We show how each technique contributes to improving the overall performance of the interpreter for major Java benchmark programs on an IBM POWER3 processor. Among three, PHC is the most effective one. We also show that the main source of memory accesses is due to bytecode fetches and that PHC successfully eliminates the majority of them, while it keeps the instruction cache miss ratios small.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {58--67},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/635508.605404},
 doi = {http://doi.acm.org/10.1145/635508.605404},
 acmid = {605404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, PowerPC, bytecode interpreter, performance, pipelined interpreter, stack caching, superscalar processor},
} 

@article{Li:2002:UIO:605432.605405,
 author = {Li, Tao and John, Lizy Kurian and Sivasubramaniam, Anand and Vijaykrishnan, N. and Rubio, Juan},
 title = {Understanding and improving operating system effects in control flow prediction},
 abstract = {Many modern applications result in a significant operating system (OS) component. The OS component has several implications including affecting the control flow transfer in the execution environment. This paper focuses on understanding the operating system effects on control flow transfer and prediction, and designing architectural support to alleviate the bottlenecks. We characterize the control flow transfer of several emerging applications on a commercial operating system. We find that the exception-driven, intermittent invocation of OS code and the user/OS branch history interference increase the misprediction in both user and kernel code.We propose two simple OS-aware control flow prediction techniques to alleviate the destructive impact of user/OS branch interference. The first one consists of capturing separate branch correlation information for user and kernel code. The second one involves using separate branch prediction tables for user and kernel code. We study the improvement contributed by the OS-aware prediction to various branch predictors ranging from simple Gshare to more elegant Agree, Multi-Hybrid and Bi-Mode predictors. On 32K entries predictors, incorporating OS-aware techniques yields up to 34\%, 23\%, 27\% and 9\% prediction accuracy improvement in Gshare, Multi-Hybrid, Agree and Bi-Mode predictors, resulting in up to 8\% execution speedup.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {68--80},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605405},
 doi = {http://doi.acm.org/10.1145/605432.605405},
 acmid = {605405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Li:2002:UIO:635508.605405,
 author = {Li, Tao and John, Lizy Kurian and Sivasubramaniam, Anand and Vijaykrishnan, N. and Rubio, Juan},
 title = {Understanding and improving operating system effects in control flow prediction},
 abstract = {Many modern applications result in a significant operating system (OS) component. The OS component has several implications including affecting the control flow transfer in the execution environment. This paper focuses on understanding the operating system effects on control flow transfer and prediction, and designing architectural support to alleviate the bottlenecks. We characterize the control flow transfer of several emerging applications on a commercial operating system. We find that the exception-driven, intermittent invocation of OS code and the user/OS branch history interference increase the misprediction in both user and kernel code.We propose two simple OS-aware control flow prediction techniques to alleviate the destructive impact of user/OS branch interference. The first one consists of capturing separate branch correlation information for user and kernel code. The second one involves using separate branch prediction tables for user and kernel code. We study the improvement contributed by the OS-aware prediction to various branch predictors ranging from simple Gshare to more elegant Agree, Multi-Hybrid and Bi-Mode predictors. On 32K entries predictors, incorporating OS-aware techniques yields up to 34\%, 23\%, 27\% and 9\% prediction accuracy improvement in Gshare, Multi-Hybrid, Agree and Bi-Mode predictors, resulting in up to 8\% execution speedup.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {68--80},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605405},
 doi = {http://doi.acm.org/10.1145/635508.605405},
 acmid = {605405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Li:2002:UIO:605397.605405,
 author = {Li, Tao and John, Lizy Kurian and Sivasubramaniam, Anand and Vijaykrishnan, N. and Rubio, Juan},
 title = {Understanding and improving operating system effects in control flow prediction},
 abstract = {Many modern applications result in a significant operating system (OS) component. The OS component has several implications including affecting the control flow transfer in the execution environment. This paper focuses on understanding the operating system effects on control flow transfer and prediction, and designing architectural support to alleviate the bottlenecks. We characterize the control flow transfer of several emerging applications on a commercial operating system. We find that the exception-driven, intermittent invocation of OS code and the user/OS branch history interference increase the misprediction in both user and kernel code.We propose two simple OS-aware control flow prediction techniques to alleviate the destructive impact of user/OS branch interference. The first one consists of capturing separate branch correlation information for user and kernel code. The second one involves using separate branch prediction tables for user and kernel code. We study the improvement contributed by the OS-aware prediction to various branch predictors ranging from simple Gshare to more elegant Agree, Multi-Hybrid and Bi-Mode predictors. On 32K entries predictors, incorporating OS-aware techniques yields up to 34\%, 23\%, 27\% and 9\% prediction accuracy improvement in Gshare, Multi-Hybrid, Agree and Bi-Mode predictors, resulting in up to 8\% execution speedup.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {68--80},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605405},
 doi = {http://doi.acm.org/10.1145/605397.605405},
 acmid = {605405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Li:2002:UIO:635506.605405,
 author = {Li, Tao and John, Lizy Kurian and Sivasubramaniam, Anand and Vijaykrishnan, N. and Rubio, Juan},
 title = {Understanding and improving operating system effects in control flow prediction},
 abstract = {Many modern applications result in a significant operating system (OS) component. The OS component has several implications including affecting the control flow transfer in the execution environment. This paper focuses on understanding the operating system effects on control flow transfer and prediction, and designing architectural support to alleviate the bottlenecks. We characterize the control flow transfer of several emerging applications on a commercial operating system. We find that the exception-driven, intermittent invocation of OS code and the user/OS branch history interference increase the misprediction in both user and kernel code.We propose two simple OS-aware control flow prediction techniques to alleviate the destructive impact of user/OS branch interference. The first one consists of capturing separate branch correlation information for user and kernel code. The second one involves using separate branch prediction tables for user and kernel code. We study the improvement contributed by the OS-aware prediction to various branch predictors ranging from simple Gshare to more elegant Agree, Multi-Hybrid and Bi-Mode predictors. On 32K entries predictors, incorporating OS-aware techniques yields up to 34\%, 23\%, 27\% and 9\% prediction accuracy improvement in Gshare, Multi-Hybrid, Agree and Bi-Mode predictors, resulting in up to 8\% execution speedup.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {68--80},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605405},
 doi = {http://doi.acm.org/10.1145/635506.605405},
 acmid = {605405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levis:2002:MTV:605432.605407,
 author = {Levis, Philip and Culler, David},
 title = {Mat\&eacute;: a tiny virtual machine for sensor networks},
 abstract = {Composed of tens of thousands of tiny devices with very limited resources ("motes"), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopulate. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost.We present Mat\&eacute;, a tiny communication-centric virtual machine designed for sensor networks. Mat\&eacute;'s high-level interface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deployment of ad-hoc routing and data aggregation algorithms. Mat\&eacute;'s concise, high-level program representation simplifies programming and allows large networks to be frequently reprogrammed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual machines to provide the user/kernel boundary on motes that have no hardware protection mechanisms.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/605432.605407},
 doi = {http://doi.acm.org/10.1145/605432.605407},
 acmid = {605407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levis:2002:MTV:635506.605407,
 author = {Levis, Philip and Culler, David},
 title = {Mat\&eacute;: a tiny virtual machine for sensor networks},
 abstract = {Composed of tens of thousands of tiny devices with very limited resources ("motes"), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopulate. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost.We present Mat\&eacute;, a tiny communication-centric virtual machine designed for sensor networks. Mat\&eacute;'s high-level interface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deployment of ad-hoc routing and data aggregation algorithms. Mat\&eacute;'s concise, high-level program representation simplifies programming and allows large networks to be frequently reprogrammed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual machines to provide the user/kernel boundary on motes that have no hardware protection mechanisms.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/635506.605407},
 doi = {http://doi.acm.org/10.1145/635506.605407},
 acmid = {605407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levis:2002:MTV:635508.605407,
 author = {Levis, Philip and Culler, David},
 title = {Mat\&eacute;: a tiny virtual machine for sensor networks},
 abstract = {Composed of tens of thousands of tiny devices with very limited resources ("motes"), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopulate. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost.We present Mat\&eacute;, a tiny communication-centric virtual machine designed for sensor networks. Mat\&eacute;'s high-level interface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deployment of ad-hoc routing and data aggregation algorithms. Mat\&eacute;'s concise, high-level program representation simplifies programming and allows large networks to be frequently reprogrammed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual machines to provide the user/kernel boundary on motes that have no hardware protection mechanisms.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/635508.605407},
 doi = {http://doi.acm.org/10.1145/635508.605407},
 acmid = {605407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Levis:2002:MTV:605397.605407,
 author = {Levis, Philip and Culler, David},
 title = {Mat\&eacute;: a tiny virtual machine for sensor networks},
 abstract = {Composed of tens of thousands of tiny devices with very limited resources ("motes"), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopulate. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost.We present Mat\&eacute;, a tiny communication-centric virtual machine designed for sensor networks. Mat\&eacute;'s high-level interface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deployment of ad-hoc routing and data aggregation algorithms. Mat\&eacute;'s concise, high-level program representation simplifies programming and allows large networks to be frequently reprogrammed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual machines to provide the user/kernel boundary on motes that have no hardware protection mechanisms.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/605397.605407},
 doi = {http://doi.acm.org/10.1145/605397.605407},
 acmid = {605407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Juang:2002:ECW:605432.605408,
 author = {Juang, Philo and Oki, Hidekazu and Wang, Yong and Martonosi, Margaret and Peh, Li Shiuan and Rubenstein, Daniel},
 title = {Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet},
 abstract = {Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605408},
 doi = {http://doi.acm.org/10.1145/605432.605408},
 acmid = {605408},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Juang:2002:ECW:635506.605408,
 author = {Juang, Philo and Oki, Hidekazu and Wang, Yong and Martonosi, Margaret and Peh, Li Shiuan and Rubenstein, Daniel},
 title = {Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet},
 abstract = {Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605408},
 doi = {http://doi.acm.org/10.1145/635506.605408},
 acmid = {605408},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Juang:2002:ECW:605397.605408,
 author = {Juang, Philo and Oki, Hidekazu and Wang, Yong and Martonosi, Margaret and Peh, Li Shiuan and Rubenstein, Daniel},
 title = {Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet},
 abstract = {Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605408},
 doi = {http://doi.acm.org/10.1145/605397.605408},
 acmid = {605408},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Juang:2002:ECW:635508.605408,
 author = {Juang, Philo and Oki, Hidekazu and Wang, Yong and Martonosi, Margaret and Peh, Li Shiuan and Rubenstein, Daniel},
 title = {Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet},
 abstract = {Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605408},
 doi = {http://doi.acm.org/10.1145/635508.605408},
 acmid = {605408},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kirovski:2002:ETS:635506.605409,
 author = {Kirovski, Darko and Drini\'{c}, Milenko and Potkonjak, Miodrag},
 title = {Enabling trusted software integrity},
 abstract = {Preventing execution of unauthorized software on a given computer plays a pivotal role in system security. The key problem is that although a program at the beginning of its execution can be verified as authentic, while running, its execution flow can be redirected to externally injected malicious code using, for example, a buffer overflow exploit. Existing techniques address this problem by trying to detect the intrusion at run-time or by formally verifying that the software is not prone to a particular attack.We take a radically different approach to this problem. We aim at intrusion prevention as the core technology for enabling secure computing systems. Intrusion prevention systems force an adversary to solve a computationally hard task in order to create a binary that can be executed on a given machine. In this paper, we present an exemplary system--SPEF--a combination of architectural and compilation techniques that ensure software integrity at run-time. SPEF embeds encrypted, processor-specific constraints into each block of instructions at software installation time and then verifies their existence at run-time. Thus, the processor can execute only properly installed programs, which makes installation the only system gate that needs to be protected. We have designed a SPEF prototype based on the ARM instruction set and validated its impact on security and performance using the MediaBench suite of applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605409},
 doi = {http://doi.acm.org/10.1145/635506.605409},
 acmid = {605409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kirovski:2002:ETS:605432.605409,
 author = {Kirovski, Darko and Drini\'{c}, Milenko and Potkonjak, Miodrag},
 title = {Enabling trusted software integrity},
 abstract = {Preventing execution of unauthorized software on a given computer plays a pivotal role in system security. The key problem is that although a program at the beginning of its execution can be verified as authentic, while running, its execution flow can be redirected to externally injected malicious code using, for example, a buffer overflow exploit. Existing techniques address this problem by trying to detect the intrusion at run-time or by formally verifying that the software is not prone to a particular attack.We take a radically different approach to this problem. We aim at intrusion prevention as the core technology for enabling secure computing systems. Intrusion prevention systems force an adversary to solve a computationally hard task in order to create a binary that can be executed on a given machine. In this paper, we present an exemplary system--SPEF--a combination of architectural and compilation techniques that ensure software integrity at run-time. SPEF embeds encrypted, processor-specific constraints into each block of instructions at software installation time and then verifies their existence at run-time. Thus, the processor can execute only properly installed programs, which makes installation the only system gate that needs to be protected. We have designed a SPEF prototype based on the ARM instruction set and validated its impact on security and performance using the MediaBench suite of applications.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605409},
 doi = {http://doi.acm.org/10.1145/605432.605409},
 acmid = {605409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kirovski:2002:ETS:635508.605409,
 author = {Kirovski, Darko and Drini\'{c}, Milenko and Potkonjak, Miodrag},
 title = {Enabling trusted software integrity},
 abstract = {Preventing execution of unauthorized software on a given computer plays a pivotal role in system security. The key problem is that although a program at the beginning of its execution can be verified as authentic, while running, its execution flow can be redirected to externally injected malicious code using, for example, a buffer overflow exploit. Existing techniques address this problem by trying to detect the intrusion at run-time or by formally verifying that the software is not prone to a particular attack.We take a radically different approach to this problem. We aim at intrusion prevention as the core technology for enabling secure computing systems. Intrusion prevention systems force an adversary to solve a computationally hard task in order to create a binary that can be executed on a given machine. In this paper, we present an exemplary system--SPEF--a combination of architectural and compilation techniques that ensure software integrity at run-time. SPEF embeds encrypted, processor-specific constraints into each block of instructions at software installation time and then verifies their existence at run-time. Thus, the processor can execute only properly installed programs, which makes installation the only system gate that needs to be protected. We have designed a SPEF prototype based on the ARM instruction set and validated its impact on security and performance using the MediaBench suite of applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605409},
 doi = {http://doi.acm.org/10.1145/635508.605409},
 acmid = {605409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kirovski:2002:ETS:605397.605409,
 author = {Kirovski, Darko and Drini\'{c}, Milenko and Potkonjak, Miodrag},
 title = {Enabling trusted software integrity},
 abstract = {Preventing execution of unauthorized software on a given computer plays a pivotal role in system security. The key problem is that although a program at the beginning of its execution can be verified as authentic, while running, its execution flow can be redirected to externally injected malicious code using, for example, a buffer overflow exploit. Existing techniques address this problem by trying to detect the intrusion at run-time or by formally verifying that the software is not prone to a particular attack.We take a radically different approach to this problem. We aim at intrusion prevention as the core technology for enabling secure computing systems. Intrusion prevention systems force an adversary to solve a computationally hard task in order to create a binary that can be executed on a given machine. In this paper, we present an exemplary system--SPEF--a combination of architectural and compilation techniques that ensure software integrity at run-time. SPEF embeds encrypted, processor-specific constraints into each block of instructions at software installation time and then verifies their existence at run-time. Thus, the processor can execute only properly installed programs, which makes installation the only system gate that needs to be protected. We have designed a SPEF prototype based on the ARM instruction set and validated its impact on security and performance using the MediaBench suite of applications.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605409},
 doi = {http://doi.acm.org/10.1145/605397.605409},
 acmid = {605409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zeng:2002:EME:635508.605411,
 author = {Zeng, Heng and Ellis, Carla S. and Lebeck, Alvin R. and Vahdat, Amin},
 title = {ECOSystem: managing energy as a first class operating system resource},
 abstract = {Energy consumption has recently been widely recognized as a major challenge of computer systems design. This paper explores how to support energy as a first-class operating system resource. Energy, because of its global system nature, presents challenges beyond those of conventional resource management. To meet these challenges we propose the Currentcy Model that unifies energy accounting over diverse hardware components and enables fair allocation of available energy among applications. Our particular goal is to extend battery lifetime by limiting the average discharge rate and to share this limited resource among competing task according to user preferences. To demonstrate how our framework supports explicit control over the battery resource we implemented ECOSystem, a modified Linux, that incorporates our currentcy model. Experimental results show that ECOSystem accurately accounts for the energy consumed by asynchronous device operation, can achieve a target battery lifetime, and proportionally shares the limited energy resource among competing tasks.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/635508.605411},
 doi = {http://doi.acm.org/10.1145/635508.605411},
 acmid = {605411},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zeng:2002:EME:635506.605411,
 author = {Zeng, Heng and Ellis, Carla S. and Lebeck, Alvin R. and Vahdat, Amin},
 title = {ECOSystem: managing energy as a first class operating system resource},
 abstract = {Energy consumption has recently been widely recognized as a major challenge of computer systems design. This paper explores how to support energy as a first-class operating system resource. Energy, because of its global system nature, presents challenges beyond those of conventional resource management. To meet these challenges we propose the Currentcy Model that unifies energy accounting over diverse hardware components and enables fair allocation of available energy among applications. Our particular goal is to extend battery lifetime by limiting the average discharge rate and to share this limited resource among competing task according to user preferences. To demonstrate how our framework supports explicit control over the battery resource we implemented ECOSystem, a modified Linux, that incorporates our currentcy model. Experimental results show that ECOSystem accurately accounts for the energy consumed by asynchronous device operation, can achieve a target battery lifetime, and proportionally shares the limited energy resource among competing tasks.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/635506.605411},
 doi = {http://doi.acm.org/10.1145/635506.605411},
 acmid = {605411},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zeng:2002:EME:605397.605411,
 author = {Zeng, Heng and Ellis, Carla S. and Lebeck, Alvin R. and Vahdat, Amin},
 title = {ECOSystem: managing energy as a first class operating system resource},
 abstract = {Energy consumption has recently been widely recognized as a major challenge of computer systems design. This paper explores how to support energy as a first-class operating system resource. Energy, because of its global system nature, presents challenges beyond those of conventional resource management. To meet these challenges we propose the Currentcy Model that unifies energy accounting over diverse hardware components and enables fair allocation of available energy among applications. Our particular goal is to extend battery lifetime by limiting the average discharge rate and to share this limited resource among competing task according to user preferences. To demonstrate how our framework supports explicit control over the battery resource we implemented ECOSystem, a modified Linux, that incorporates our currentcy model. Experimental results show that ECOSystem accurately accounts for the energy consumed by asynchronous device operation, can achieve a target battery lifetime, and proportionally shares the limited energy resource among competing tasks.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/605397.605411},
 doi = {http://doi.acm.org/10.1145/605397.605411},
 acmid = {605411},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zeng:2002:EME:605432.605411,
 author = {Zeng, Heng and Ellis, Carla S. and Lebeck, Alvin R. and Vahdat, Amin},
 title = {ECOSystem: managing energy as a first class operating system resource},
 abstract = {Energy consumption has recently been widely recognized as a major challenge of computer systems design. This paper explores how to support energy as a first-class operating system resource. Energy, because of its global system nature, presents challenges beyond those of conventional resource management. To meet these challenges we propose the Currentcy Model that unifies energy accounting over diverse hardware components and enables fair allocation of available energy among applications. Our particular goal is to extend battery lifetime by limiting the average discharge rate and to share this limited resource among competing task according to user preferences. To demonstrate how our framework supports explicit control over the battery resource we implemented ECOSystem, a modified Linux, that incorporates our currentcy model. Experimental results show that ECOSystem accurately accounts for the energy consumed by asynchronous device operation, can achieve a target battery lifetime, and proportionally shares the limited energy resource among competing tasks.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/605432.605411},
 doi = {http://doi.acm.org/10.1145/605432.605411},
 acmid = {605411},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ashok:2002:CCS:605397.605412,
 author = {Ashok, Raksit and Chheda, Saurabh and Moritz, Csaba Andras},
 title = {Cool-Mem: combining statically speculative memory accessing with selective address translation for energy efficiency},
 abstract = {This paper presents Cool-Mem, a family of memory system architectures that integrate conventional memory system mechanisms, energy-aware address translation, and compiler-enabled cache disambiguation techniques, to reduce energy consumption in general purpose architectures. It combines statically speculative cache access modes, a dynamic CAM based Tag-Cache used as backup for statically mispredicted accesses, various conventional multi-level associative cache organizations, embedded protection checking along all cache access mechanisms, as well as architectural organizations to reduce the power consumed by address translation in virtual memory. Because it is based on speculative static information, the approach removes the burden of provable correctness in compiler analysis passes that extract static information. This makes Cool-Mem applicable for large and complex applications, without having any limitations due to complexity issues in the compiler passes or the presence of precompiled static libraries. Based on extensive evaluation, for both SPEC2000 and Mediabench applications, 12\% to 20\% total energy savings are obtained in the processor, with performance ranging from 1.2\% degradation to 8\% improvement, for the applications studied.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/605397.605412},
 doi = {http://doi.acm.org/10.1145/605397.605412},
 acmid = {605412},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ashok:2002:CCS:635508.605412,
 author = {Ashok, Raksit and Chheda, Saurabh and Moritz, Csaba Andras},
 title = {Cool-Mem: combining statically speculative memory accessing with selective address translation for energy efficiency},
 abstract = {This paper presents Cool-Mem, a family of memory system architectures that integrate conventional memory system mechanisms, energy-aware address translation, and compiler-enabled cache disambiguation techniques, to reduce energy consumption in general purpose architectures. It combines statically speculative cache access modes, a dynamic CAM based Tag-Cache used as backup for statically mispredicted accesses, various conventional multi-level associative cache organizations, embedded protection checking along all cache access mechanisms, as well as architectural organizations to reduce the power consumed by address translation in virtual memory. Because it is based on speculative static information, the approach removes the burden of provable correctness in compiler analysis passes that extract static information. This makes Cool-Mem applicable for large and complex applications, without having any limitations due to complexity issues in the compiler passes or the presence of precompiled static libraries. Based on extensive evaluation, for both SPEC2000 and Mediabench applications, 12\% to 20\% total energy savings are obtained in the processor, with performance ranging from 1.2\% degradation to 8\% improvement, for the applications studied.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/635508.605412},
 doi = {http://doi.acm.org/10.1145/635508.605412},
 acmid = {605412},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ashok:2002:CCS:635506.605412,
 author = {Ashok, Raksit and Chheda, Saurabh and Moritz, Csaba Andras},
 title = {Cool-Mem: combining statically speculative memory accessing with selective address translation for energy efficiency},
 abstract = {This paper presents Cool-Mem, a family of memory system architectures that integrate conventional memory system mechanisms, energy-aware address translation, and compiler-enabled cache disambiguation techniques, to reduce energy consumption in general purpose architectures. It combines statically speculative cache access modes, a dynamic CAM based Tag-Cache used as backup for statically mispredicted accesses, various conventional multi-level associative cache organizations, embedded protection checking along all cache access mechanisms, as well as architectural organizations to reduce the power consumed by address translation in virtual memory. Because it is based on speculative static information, the approach removes the burden of provable correctness in compiler analysis passes that extract static information. This makes Cool-Mem applicable for large and complex applications, without having any limitations due to complexity issues in the compiler passes or the presence of precompiled static libraries. Based on extensive evaluation, for both SPEC2000 and Mediabench applications, 12\% to 20\% total energy savings are obtained in the processor, with performance ranging from 1.2\% degradation to 8\% improvement, for the applications studied.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/635506.605412},
 doi = {http://doi.acm.org/10.1145/635506.605412},
 acmid = {605412},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ashok:2002:CCS:605432.605412,
 author = {Ashok, Raksit and Chheda, Saurabh and Moritz, Csaba Andras},
 title = {Cool-Mem: combining statically speculative memory accessing with selective address translation for energy efficiency},
 abstract = {This paper presents Cool-Mem, a family of memory system architectures that integrate conventional memory system mechanisms, energy-aware address translation, and compiler-enabled cache disambiguation techniques, to reduce energy consumption in general purpose architectures. It combines statically speculative cache access modes, a dynamic CAM based Tag-Cache used as backup for statically mispredicted accesses, various conventional multi-level associative cache organizations, embedded protection checking along all cache access mechanisms, as well as architectural organizations to reduce the power consumed by address translation in virtual memory. Because it is based on speculative static information, the approach removes the burden of provable correctness in compiler analysis passes that extract static information. This makes Cool-Mem applicable for large and complex applications, without having any limitations due to complexity issues in the compiler passes or the presence of precompiled static libraries. Based on extensive evaluation, for both SPEC2000 and Mediabench applications, 12\% to 20\% total energy savings are obtained in the processor, with performance ranging from 1.2\% degradation to 8\% improvement, for the applications studied.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/605432.605412},
 doi = {http://doi.acm.org/10.1145/605432.605412},
 acmid = {605412},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sasanka:2002:JLG:605432.605413,
 author = {Sasanka, Ruchira and Hughes, Christopher J. and Adve, Sarita V.},
 title = {Joint local and global hardware adaptations for energy},
 abstract = {This work concerns algorithms to control energy-driven architecture adaptations for multimedia applications, without and with dynamic voltage scaling (DVS). We identify a broad design space for adaptation control algorithms based on two attributes: (1) when to adapt or temporal</i> granularity and (2) what structures to adapt or spatial</i> granularity. For each attribute, adaptation may be global</i> or local.</i> Our previous work developed a temporally and spatially global algorithm. It invokes adaptation at the granularity of a full frame of a multimedia application (temporally global) and considers the entire hardware configuration at a time (spatially global). It exploits inter-</i>frame execution time variability, slowing computation just enough to eliminate idle time before the real-time deadline.This paper explores temporally and spatially local algorithms and their integration with the previous global algorithm. The local algorithms invoke architectural adaptation within an application frame to exploit intra-</i>frame execution variability, and attempt to save energy without affecting execution time. We consider local algorithms previously studied for non-real-time applications as well as propose new algorithms. We find that, for systems without and with DVS, the local algorithms are effective in saving energy for multimedia applications, but the new integrated global and local algorithm is best for the systems and applications studied.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605413},
 doi = {http://doi.acm.org/10.1145/605432.605413},
 acmid = {605413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sasanka:2002:JLG:605397.605413,
 author = {Sasanka, Ruchira and Hughes, Christopher J. and Adve, Sarita V.},
 title = {Joint local and global hardware adaptations for energy},
 abstract = {This work concerns algorithms to control energy-driven architecture adaptations for multimedia applications, without and with dynamic voltage scaling (DVS). We identify a broad design space for adaptation control algorithms based on two attributes: (1) when to adapt or temporal</i> granularity and (2) what structures to adapt or spatial</i> granularity. For each attribute, adaptation may be global</i> or local.</i> Our previous work developed a temporally and spatially global algorithm. It invokes adaptation at the granularity of a full frame of a multimedia application (temporally global) and considers the entire hardware configuration at a time (spatially global). It exploits inter-</i>frame execution time variability, slowing computation just enough to eliminate idle time before the real-time deadline.This paper explores temporally and spatially local algorithms and their integration with the previous global algorithm. The local algorithms invoke architectural adaptation within an application frame to exploit intra-</i>frame execution variability, and attempt to save energy without affecting execution time. We consider local algorithms previously studied for non-real-time applications as well as propose new algorithms. We find that, for systems without and with DVS, the local algorithms are effective in saving energy for multimedia applications, but the new integrated global and local algorithm is best for the systems and applications studied.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605413},
 doi = {http://doi.acm.org/10.1145/605397.605413},
 acmid = {605413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sasanka:2002:JLG:635508.605413,
 author = {Sasanka, Ruchira and Hughes, Christopher J. and Adve, Sarita V.},
 title = {Joint local and global hardware adaptations for energy},
 abstract = {This work concerns algorithms to control energy-driven architecture adaptations for multimedia applications, without and with dynamic voltage scaling (DVS). We identify a broad design space for adaptation control algorithms based on two attributes: (1) when to adapt or temporal</i> granularity and (2) what structures to adapt or spatial</i> granularity. For each attribute, adaptation may be global</i> or local.</i> Our previous work developed a temporally and spatially global algorithm. It invokes adaptation at the granularity of a full frame of a multimedia application (temporally global) and considers the entire hardware configuration at a time (spatially global). It exploits inter-</i>frame execution time variability, slowing computation just enough to eliminate idle time before the real-time deadline.This paper explores temporally and spatially local algorithms and their integration with the previous global algorithm. The local algorithms invoke architectural adaptation within an application frame to exploit intra-</i>frame execution variability, and attempt to save energy without affecting execution time. We consider local algorithms previously studied for non-real-time applications as well as propose new algorithms. We find that, for systems without and with DVS, the local algorithms are effective in saving energy for multimedia applications, but the new integrated global and local algorithm is best for the systems and applications studied.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605413},
 doi = {http://doi.acm.org/10.1145/635508.605413},
 acmid = {605413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sasanka:2002:JLG:635506.605413,
 author = {Sasanka, Ruchira and Hughes, Christopher J. and Adve, Sarita V.},
 title = {Joint local and global hardware adaptations for energy},
 abstract = {This work concerns algorithms to control energy-driven architecture adaptations for multimedia applications, without and with dynamic voltage scaling (DVS). We identify a broad design space for adaptation control algorithms based on two attributes: (1) when to adapt or temporal</i> granularity and (2) what structures to adapt or spatial</i> granularity. For each attribute, adaptation may be global</i> or local.</i> Our previous work developed a temporally and spatially global algorithm. It invokes adaptation at the granularity of a full frame of a multimedia application (temporally global) and considers the entire hardware configuration at a time (spatially global). It exploits inter-</i>frame execution time variability, slowing computation just enough to eliminate idle time before the real-time deadline.This paper explores temporally and spatially local algorithms and their integration with the previous global algorithm. The local algorithms invoke architectural adaptation within an application frame to exploit intra-</i>frame execution variability, and attempt to save energy without affecting execution time. We consider local algorithms previously studied for non-real-time applications as well as propose new algorithms. We find that, for systems without and with DVS, the local algorithms are effective in saving energy for multimedia applications, but the new integrated global and local algorithm is best for the systems and applications studied.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {144--155},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605413},
 doi = {http://doi.acm.org/10.1145/635506.605413},
 acmid = {605413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:2002:DEC:605397.605415,
 author = {Kim, Dongkeun and Yeung, Donald},
 title = {Design and evaluation of compiler algorithms for pre-execution},
 abstract = {Pre-execution is a promising latency tolerance technique that uses one or more helper threads running in spare hardware contexts ahead of the main computation to trigger long-latency memory operations early, hence absorbing their latency on behalf of the main computation. This paper investigates a source-to-source C compiler for extracting pre-execution thread code automatically, thus relieving the programmer or hardware from this onerous task. At the heart of our compiler are three algorithms. First, program slicing</i> removes non-critical code for computing cache-missing memory references, reducing pre-execution overhead. Second, prefetch conversion</i> replaces blocking memory references with non-blocking prefetch instructions to minimize pre-execution thread stalls. Finally, threading scheme selection</i> chooses the best scheme for initiating pre-execution threads, speculatively parallelizing loops to generate thread-level parallelism when necessary for latency tolerance. We prototyped our algorithms using the Stanford University Intermediate Format (SUIF) framework and a publicly available program slicer, called Unravel</i> [13], and we evaluated our compiler on a detailed architectural simulator of an SMT processor. Our results show compiler-based pre-execution improves the performance of 9 out of 13 applications, reducing execution time by 22.7\%. Across all 13 applications, our technique delivers an average speedup of 17.0\%. These performance gains are achieved fully automatically on conventional SMT hardware, with only minimal modifications to support pre-execution threads.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605415},
 doi = {http://doi.acm.org/10.1145/605397.605415},
 acmid = {605415},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:DEC:605432.605415,
 author = {Kim, Dongkeun and Yeung, Donald},
 title = {Design and evaluation of compiler algorithms for pre-execution},
 abstract = {Pre-execution is a promising latency tolerance technique that uses one or more helper threads running in spare hardware contexts ahead of the main computation to trigger long-latency memory operations early, hence absorbing their latency on behalf of the main computation. This paper investigates a source-to-source C compiler for extracting pre-execution thread code automatically, thus relieving the programmer or hardware from this onerous task. At the heart of our compiler are three algorithms. First, program slicing</i> removes non-critical code for computing cache-missing memory references, reducing pre-execution overhead. Second, prefetch conversion</i> replaces blocking memory references with non-blocking prefetch instructions to minimize pre-execution thread stalls. Finally, threading scheme selection</i> chooses the best scheme for initiating pre-execution threads, speculatively parallelizing loops to generate thread-level parallelism when necessary for latency tolerance. We prototyped our algorithms using the Stanford University Intermediate Format (SUIF) framework and a publicly available program slicer, called Unravel</i> [13], and we evaluated our compiler on a detailed architectural simulator of an SMT processor. Our results show compiler-based pre-execution improves the performance of 9 out of 13 applications, reducing execution time by 22.7\%. Across all 13 applications, our technique delivers an average speedup of 17.0\%. These performance gains are achieved fully automatically on conventional SMT hardware, with only minimal modifications to support pre-execution threads.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605415},
 doi = {http://doi.acm.org/10.1145/605432.605415},
 acmid = {605415},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:DEC:635508.605415,
 author = {Kim, Dongkeun and Yeung, Donald},
 title = {Design and evaluation of compiler algorithms for pre-execution},
 abstract = {Pre-execution is a promising latency tolerance technique that uses one or more helper threads running in spare hardware contexts ahead of the main computation to trigger long-latency memory operations early, hence absorbing their latency on behalf of the main computation. This paper investigates a source-to-source C compiler for extracting pre-execution thread code automatically, thus relieving the programmer or hardware from this onerous task. At the heart of our compiler are three algorithms. First, program slicing</i> removes non-critical code for computing cache-missing memory references, reducing pre-execution overhead. Second, prefetch conversion</i> replaces blocking memory references with non-blocking prefetch instructions to minimize pre-execution thread stalls. Finally, threading scheme selection</i> chooses the best scheme for initiating pre-execution threads, speculatively parallelizing loops to generate thread-level parallelism when necessary for latency tolerance. We prototyped our algorithms using the Stanford University Intermediate Format (SUIF) framework and a publicly available program slicer, called Unravel</i> [13], and we evaluated our compiler on a detailed architectural simulator of an SMT processor. Our results show compiler-based pre-execution improves the performance of 9 out of 13 applications, reducing execution time by 22.7\%. Across all 13 applications, our technique delivers an average speedup of 17.0\%. These performance gains are achieved fully automatically on conventional SMT hardware, with only minimal modifications to support pre-execution threads.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605415},
 doi = {http://doi.acm.org/10.1145/635508.605415},
 acmid = {605415},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:DEC:635506.605415,
 author = {Kim, Dongkeun and Yeung, Donald},
 title = {Design and evaluation of compiler algorithms for pre-execution},
 abstract = {Pre-execution is a promising latency tolerance technique that uses one or more helper threads running in spare hardware contexts ahead of the main computation to trigger long-latency memory operations early, hence absorbing their latency on behalf of the main computation. This paper investigates a source-to-source C compiler for extracting pre-execution thread code automatically, thus relieving the programmer or hardware from this onerous task. At the heart of our compiler are three algorithms. First, program slicing</i> removes non-critical code for computing cache-missing memory references, reducing pre-execution overhead. Second, prefetch conversion</i> replaces blocking memory references with non-blocking prefetch instructions to minimize pre-execution thread stalls. Finally, threading scheme selection</i> chooses the best scheme for initiating pre-execution threads, speculatively parallelizing loops to generate thread-level parallelism when necessary for latency tolerance. We prototyped our algorithms using the Stanford University Intermediate Format (SUIF) framework and a publicly available program slicer, called Unravel</i> [13], and we evaluated our compiler on a detailed architectural simulator of an SMT processor. Our results show compiler-based pre-execution improves the performance of 9 out of 13 applications, reducing execution time by 22.7\%. Across all 13 applications, our technique delivers an average speedup of 17.0\%. These performance gains are achieved fully automatically on conventional SMT hardware, with only minimal modifications to support pre-execution threads.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605415},
 doi = {http://doi.acm.org/10.1145/635506.605415},
 acmid = {605415},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhai:2002:COS:605432.605416,
 author = {Zhai, Antonia and Colohan, Christopher B. and Steffan, J. Gregory and Mowry, Todd C.},
 title = {Compiler optimization of scalar value communication between speculative threads},
 abstract = {While there have been many recent proposals for hardware that supports Thread-Level Speculation</i> (TLS), there has been relatively little work on compiler optimizations to fully exploit this potential for parallelizing programs optimistically. In this paper, we focus on one important limitation of program performance under TLS, which is stalls due to forwarding scalar values between threads that would otherwise cause frequent data dependences. We present and evaluate dataflow algorithms for three increasingly-aggressive instruction scheduling techniques that reduce the critical forwarding path</i> introduced by the synchronization associated with this data forwarding. In addition, we contrast our compiler techniques with related hardware-only approaches. With our most aggressive compiler and hardware techniques, we improve performance under TLS by 6.2-28.5\% for 6 of 14 applications, and by at least 2.7\% for half of the other applications.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {171--183},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605416},
 doi = {http://doi.acm.org/10.1145/605432.605416},
 acmid = {605416},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhai:2002:COS:635506.605416,
 author = {Zhai, Antonia and Colohan, Christopher B. and Steffan, J. Gregory and Mowry, Todd C.},
 title = {Compiler optimization of scalar value communication between speculative threads},
 abstract = {While there have been many recent proposals for hardware that supports Thread-Level Speculation</i> (TLS), there has been relatively little work on compiler optimizations to fully exploit this potential for parallelizing programs optimistically. In this paper, we focus on one important limitation of program performance under TLS, which is stalls due to forwarding scalar values between threads that would otherwise cause frequent data dependences. We present and evaluate dataflow algorithms for three increasingly-aggressive instruction scheduling techniques that reduce the critical forwarding path</i> introduced by the synchronization associated with this data forwarding. In addition, we contrast our compiler techniques with related hardware-only approaches. With our most aggressive compiler and hardware techniques, we improve performance under TLS by 6.2-28.5\% for 6 of 14 applications, and by at least 2.7\% for half of the other applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {171--183},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605416},
 doi = {http://doi.acm.org/10.1145/635506.605416},
 acmid = {605416},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhai:2002:COS:635508.605416,
 author = {Zhai, Antonia and Colohan, Christopher B. and Steffan, J. Gregory and Mowry, Todd C.},
 title = {Compiler optimization of scalar value communication between speculative threads},
 abstract = {While there have been many recent proposals for hardware that supports Thread-Level Speculation</i> (TLS), there has been relatively little work on compiler optimizations to fully exploit this potential for parallelizing programs optimistically. In this paper, we focus on one important limitation of program performance under TLS, which is stalls due to forwarding scalar values between threads that would otherwise cause frequent data dependences. We present and evaluate dataflow algorithms for three increasingly-aggressive instruction scheduling techniques that reduce the critical forwarding path</i> introduced by the synchronization associated with this data forwarding. In addition, we contrast our compiler techniques with related hardware-only approaches. With our most aggressive compiler and hardware techniques, we improve performance under TLS by 6.2-28.5\% for 6 of 14 applications, and by at least 2.7\% for half of the other applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {171--183},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605416},
 doi = {http://doi.acm.org/10.1145/635508.605416},
 acmid = {605416},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhai:2002:COS:605397.605416,
 author = {Zhai, Antonia and Colohan, Christopher B. and Steffan, J. Gregory and Mowry, Todd C.},
 title = {Compiler optimization of scalar value communication between speculative threads},
 abstract = {While there have been many recent proposals for hardware that supports Thread-Level Speculation</i> (TLS), there has been relatively little work on compiler optimizations to fully exploit this potential for parallelizing programs optimistically. In this paper, we focus on one important limitation of program performance under TLS, which is stalls due to forwarding scalar values between threads that would otherwise cause frequent data dependences. We present and evaluate dataflow algorithms for three increasingly-aggressive instruction scheduling techniques that reduce the critical forwarding path</i> introduced by the synchronization associated with this data forwarding. In addition, we contrast our compiler techniques with related hardware-only approaches. With our most aggressive compiler and hardware techniques, we improve performance under TLS by 6.2-28.5\% for 6 of 14 applications, and by at least 2.7\% for half of the other applications.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {171--183},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605416},
 doi = {http://doi.acm.org/10.1145/605397.605416},
 acmid = {605416},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Oplinger:2002:ESR:635506.605417,
 author = {Oplinger, Jeffrey and Lam, Monica S.},
 title = {Enhancing software reliability with speculative threads},
 abstract = {This paper advocates the use of a monitor-and-recover programming paradigm to enhance the reliability of software, and proposes an architectural design that allows software and hardware to cooperate in making this paradigm more efficient and easier to program.We propose that programmers write monitoring functions assuming simple sequential execution semantics. Our architecture speeds up the computation by executing the monitoring functions speculatively in parallel with the main computation. For recovery, programmers can define fine-grain transactions whose side effects, including all register modifications and memory writes, can either be committed or aborted under program control. Transactions are implemented efficiently by treating them as speculative threads.Our experimental results suggest that monitored execution is more amenable to parallelization than regular program execution. Code monitoring is sped up by a factor of 1.5 by exploiting single-thread instruction-level parallelism, and by an additional factor of 1.6 using thread-level speculation. This results in an overall improvement of 2.5 times and a sustained 5.4 instructions-per-cycle performance. A monitored execution that used to be 2.5 times slower executes with a degradation of only 12\% when compared to the performance on the baseline machine. We also show that the concept of fine-grain transactional programming is useful in catching buffer overrun errors through a number of real-life examples.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {184--196},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605417},
 doi = {http://doi.acm.org/10.1145/635506.605417},
 acmid = {605417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Oplinger:2002:ESR:605397.605417,
 author = {Oplinger, Jeffrey and Lam, Monica S.},
 title = {Enhancing software reliability with speculative threads},
 abstract = {This paper advocates the use of a monitor-and-recover programming paradigm to enhance the reliability of software, and proposes an architectural design that allows software and hardware to cooperate in making this paradigm more efficient and easier to program.We propose that programmers write monitoring functions assuming simple sequential execution semantics. Our architecture speeds up the computation by executing the monitoring functions speculatively in parallel with the main computation. For recovery, programmers can define fine-grain transactions whose side effects, including all register modifications and memory writes, can either be committed or aborted under program control. Transactions are implemented efficiently by treating them as speculative threads.Our experimental results suggest that monitored execution is more amenable to parallelization than regular program execution. Code monitoring is sped up by a factor of 1.5 by exploiting single-thread instruction-level parallelism, and by an additional factor of 1.6 using thread-level speculation. This results in an overall improvement of 2.5 times and a sustained 5.4 instructions-per-cycle performance. A monitored execution that used to be 2.5 times slower executes with a degradation of only 12\% when compared to the performance on the baseline machine. We also show that the concept of fine-grain transactional programming is useful in catching buffer overrun errors through a number of real-life examples.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {184--196},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605417},
 doi = {http://doi.acm.org/10.1145/605397.605417},
 acmid = {605417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Oplinger:2002:ESR:605432.605417,
 author = {Oplinger, Jeffrey and Lam, Monica S.},
 title = {Enhancing software reliability with speculative threads},
 abstract = {This paper advocates the use of a monitor-and-recover programming paradigm to enhance the reliability of software, and proposes an architectural design that allows software and hardware to cooperate in making this paradigm more efficient and easier to program.We propose that programmers write monitoring functions assuming simple sequential execution semantics. Our architecture speeds up the computation by executing the monitoring functions speculatively in parallel with the main computation. For recovery, programmers can define fine-grain transactions whose side effects, including all register modifications and memory writes, can either be committed or aborted under program control. Transactions are implemented efficiently by treating them as speculative threads.Our experimental results suggest that monitored execution is more amenable to parallelization than regular program execution. Code monitoring is sped up by a factor of 1.5 by exploiting single-thread instruction-level parallelism, and by an additional factor of 1.6 using thread-level speculation. This results in an overall improvement of 2.5 times and a sustained 5.4 instructions-per-cycle performance. A monitored execution that used to be 2.5 times slower executes with a degradation of only 12\% when compared to the performance on the baseline machine. We also show that the concept of fine-grain transactional programming is useful in catching buffer overrun errors through a number of real-life examples.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {184--196},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605417},
 doi = {http://doi.acm.org/10.1145/605432.605417},
 acmid = {605417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Oplinger:2002:ESR:635508.605417,
 author = {Oplinger, Jeffrey and Lam, Monica S.},
 title = {Enhancing software reliability with speculative threads},
 abstract = {This paper advocates the use of a monitor-and-recover programming paradigm to enhance the reliability of software, and proposes an architectural design that allows software and hardware to cooperate in making this paradigm more efficient and easier to program.We propose that programmers write monitoring functions assuming simple sequential execution semantics. Our architecture speeds up the computation by executing the monitoring functions speculatively in parallel with the main computation. For recovery, programmers can define fine-grain transactions whose side effects, including all register modifications and memory writes, can either be committed or aborted under program control. Transactions are implemented efficiently by treating them as speculative threads.Our experimental results suggest that monitored execution is more amenable to parallelization than regular program execution. Code monitoring is sped up by a factor of 1.5 by exploiting single-thread instruction-level parallelism, and by an additional factor of 1.6 using thread-level speculation. This results in an overall improvement of 2.5 times and a sustained 5.4 instructions-per-cycle performance. A monitored execution that used to be 2.5 times slower executes with a degradation of only 12\% when compared to the performance on the baseline machine. We also show that the concept of fine-grain transactional programming is useful in catching buffer overrun errors through a number of real-life examples.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {184--196},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605417},
 doi = {http://doi.acm.org/10.1145/635508.605417},
 acmid = {605417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Butts:2002:DDD:605432.605419,
 author = {Butts, J. Adam and Sohi, Guri},
 title = {Dynamic dead-instruction detection and elimination},
 abstract = {We observe a non-negligible fraction--3 to 16\% in our benchmarks--of dynamically dead instructions,</i> dynamic instruction instances that generate unused results. The majority of these instructions arise from static instructions that also produce useful results. We find that compiler optimization (specifically instruction scheduling) creates a significant portion of these partially dead</i> static instructions. We show that most of the dynamically instructions arise from a small set of static instructions that produce dead values most of the time.We leverage this locality by proposing a dead instruction predictor and presenting a scheme to avoid the execution of predicted-dead instructions. Our predictor achieves an accuracy of 93\% while identifying over 91\% of the dead instructions using less than 5 KB of state. We achieve such high accuracies by leveraging future control flow information (i.e., branch predictions) to distinguish between useless and useful instances of the same static instruction.We then present a mechanism to avoid the register allocation, instruction scheduling, and execution of predicted dead instructions. We measure reductions in resource utilization averaging over 5\% and sometimes exceeding 10\%, covering physical register management (allocation and freeing), register file read and write traffic, and data cache accesses. Performance improves by an average of 3.6\% on an architecture exhibiting resource contention. Additionally, our scheme frees future compilers from the need to consider the costs of dead instructions, enabling more aggressive code motion and optimization. Simultaneously, it mitigates the need for good path profiling information in making inter-block code motion decisions.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605419},
 doi = {http://doi.acm.org/10.1145/605432.605419},
 acmid = {605419},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Butts:2002:DDD:605397.605419,
 author = {Butts, J. Adam and Sohi, Guri},
 title = {Dynamic dead-instruction detection and elimination},
 abstract = {We observe a non-negligible fraction--3 to 16\% in our benchmarks--of dynamically dead instructions,</i> dynamic instruction instances that generate unused results. The majority of these instructions arise from static instructions that also produce useful results. We find that compiler optimization (specifically instruction scheduling) creates a significant portion of these partially dead</i> static instructions. We show that most of the dynamically instructions arise from a small set of static instructions that produce dead values most of the time.We leverage this locality by proposing a dead instruction predictor and presenting a scheme to avoid the execution of predicted-dead instructions. Our predictor achieves an accuracy of 93\% while identifying over 91\% of the dead instructions using less than 5 KB of state. We achieve such high accuracies by leveraging future control flow information (i.e., branch predictions) to distinguish between useless and useful instances of the same static instruction.We then present a mechanism to avoid the register allocation, instruction scheduling, and execution of predicted dead instructions. We measure reductions in resource utilization averaging over 5\% and sometimes exceeding 10\%, covering physical register management (allocation and freeing), register file read and write traffic, and data cache accesses. Performance improves by an average of 3.6\% on an architecture exhibiting resource contention. Additionally, our scheme frees future compilers from the need to consider the costs of dead instructions, enabling more aggressive code motion and optimization. Simultaneously, it mitigates the need for good path profiling information in making inter-block code motion decisions.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605419},
 doi = {http://doi.acm.org/10.1145/605397.605419},
 acmid = {605419},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Butts:2002:DDD:635506.605419,
 author = {Butts, J. Adam and Sohi, Guri},
 title = {Dynamic dead-instruction detection and elimination},
 abstract = {We observe a non-negligible fraction--3 to 16\% in our benchmarks--of dynamically dead instructions,</i> dynamic instruction instances that generate unused results. The majority of these instructions arise from static instructions that also produce useful results. We find that compiler optimization (specifically instruction scheduling) creates a significant portion of these partially dead</i> static instructions. We show that most of the dynamically instructions arise from a small set of static instructions that produce dead values most of the time.We leverage this locality by proposing a dead instruction predictor and presenting a scheme to avoid the execution of predicted-dead instructions. Our predictor achieves an accuracy of 93\% while identifying over 91\% of the dead instructions using less than 5 KB of state. We achieve such high accuracies by leveraging future control flow information (i.e., branch predictions) to distinguish between useless and useful instances of the same static instruction.We then present a mechanism to avoid the register allocation, instruction scheduling, and execution of predicted dead instructions. We measure reductions in resource utilization averaging over 5\% and sometimes exceeding 10\%, covering physical register management (allocation and freeing), register file read and write traffic, and data cache accesses. Performance improves by an average of 3.6\% on an architecture exhibiting resource contention. Additionally, our scheme frees future compilers from the need to consider the costs of dead instructions, enabling more aggressive code motion and optimization. Simultaneously, it mitigates the need for good path profiling information in making inter-block code motion decisions.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605419},
 doi = {http://doi.acm.org/10.1145/635506.605419},
 acmid = {605419},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Butts:2002:DDD:635508.605419,
 author = {Butts, J. Adam and Sohi, Guri},
 title = {Dynamic dead-instruction detection and elimination},
 abstract = {We observe a non-negligible fraction--3 to 16\% in our benchmarks--of dynamically dead instructions,</i> dynamic instruction instances that generate unused results. The majority of these instructions arise from static instructions that also produce useful results. We find that compiler optimization (specifically instruction scheduling) creates a significant portion of these partially dead</i> static instructions. We show that most of the dynamically instructions arise from a small set of static instructions that produce dead values most of the time.We leverage this locality by proposing a dead instruction predictor and presenting a scheme to avoid the execution of predicted-dead instructions. Our predictor achieves an accuracy of 93\% while identifying over 91\% of the dead instructions using less than 5 KB of state. We achieve such high accuracies by leveraging future control flow information (i.e., branch predictions) to distinguish between useless and useful instances of the same static instruction.We then present a mechanism to avoid the register allocation, instruction scheduling, and execution of predicted dead instructions. We measure reductions in resource utilization averaging over 5\% and sometimes exceeding 10\%, covering physical register management (allocation and freeing), register file read and write traffic, and data cache accesses. Performance improves by an average of 3.6\% on an architecture exhibiting resource contention. Additionally, our scheme frees future compilers from the need to consider the costs of dead instructions, enabling more aggressive code motion and optimization. Simultaneously, it mitigates the need for good path profiling information in making inter-block code motion decisions.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605419},
 doi = {http://doi.acm.org/10.1145/635508.605419},
 acmid = {605419},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:ANC:635508.605420,
 author = {Kim, Changkyu and Burger, Doug and Keckler, Stephen W.},
 title = {An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches},
 abstract = {Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. Increases in on-chip communication delays will make the hit time of large on-chip caches a function of a line's physical location within the cache. Consequently, cache access times will become a continuum of latencies rather than a single discrete latency. This non-uniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11\%, outperforms the best three-level hierarchy--while using less silicon area--by 13\%, and comes within 13\% of an ideal minimal hit latency solution.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605420},
 doi = {http://doi.acm.org/10.1145/635508.605420},
 acmid = {605420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:ANC:635506.605420,
 author = {Kim, Changkyu and Burger, Doug and Keckler, Stephen W.},
 title = {An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches},
 abstract = {Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. Increases in on-chip communication delays will make the hit time of large on-chip caches a function of a line's physical location within the cache. Consequently, cache access times will become a continuum of latencies rather than a single discrete latency. This non-uniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11\%, outperforms the best three-level hierarchy--while using less silicon area--by 13\%, and comes within 13\% of an ideal minimal hit latency solution.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605420},
 doi = {http://doi.acm.org/10.1145/635506.605420},
 acmid = {605420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:ANC:605432.605420,
 author = {Kim, Changkyu and Burger, Doug and Keckler, Stephen W.},
 title = {An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches},
 abstract = {Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. Increases in on-chip communication delays will make the hit time of large on-chip caches a function of a line's physical location within the cache. Consequently, cache access times will become a continuum of latencies rather than a single discrete latency. This non-uniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11\%, outperforms the best three-level hierarchy--while using less silicon area--by 13\%, and comes within 13\% of an ideal minimal hit latency solution.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605420},
 doi = {http://doi.acm.org/10.1145/605432.605420},
 acmid = {605420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:2002:ANC:605397.605420,
 author = {Kim, Changkyu and Burger, Doug and Keckler, Stephen W.},
 title = {An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches},
 abstract = {Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. Increases in on-chip communication delays will make the hit time of large on-chip caches a function of a line's physical location within the cache. Consequently, cache access times will become a continuum of latencies rather than a single discrete latency. This non-uniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11\%, outperforms the best three-level hierarchy--while using less silicon area--by 13\%, and comes within 13\% of an ideal minimal hit latency solution.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605420},
 doi = {http://doi.acm.org/10.1145/605397.605420},
 acmid = {605420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mukherjee:2002:CSA:605397.605421,
 author = {Mukherjee, Shubhendu S. and Silla, Federico and Bannon, Peter and Emer, Joel and Lang, Steve and Webb, David},
 title = {A comparative study of arbitration algorithms for the Alpha 21364 pipelined router},
 abstract = {Interconnection networks usually consist of a fabric of interconnected routers, which receive packets arriving at their input ports and forward them to appropriate output ports. Unfortunately, network packets moving through these routers are often delayed due to conflicting demand for resources, such as output ports or buffer space. Hence, routers typically employ arbiters</i> that resolve conflicting resource demands to maximize the number of matches between packets waiting at input ports and free output ports. Efficient design and implementation of the algorithm running on these arbiters is critical to maximize network performance.This paper proposes a new arbitration algorithm called SPAA</i> (Simple Pipelined Arbitration Algorithm), which is implemented in the Alpha 21364 processor's on-chip router pipeline. Simulation results show that SPAA significantly outperforms two earlier well-known arbitration algorithms: PIM (Parallel Iterative Matching) and WFA (Wave-Front Arbiter) implemented in the SGI Spider switch. SPAA outperforms PIM and WFA because SPAA exhibits matching capabilities similar to PIM and WFA under realistic conditions when many output ports are busy, incurs fewer clock cycles to perform the arbitration, and can be pipelined effectively. Additionally, we propose a new prioritization policy called the Rotary Rule,</i> which prevents the network's adverse performance degradation from saturation at high network loads by prioritizing packets already in the network over new packets generated by caches or memory.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605421},
 doi = {http://doi.acm.org/10.1145/605397.605421},
 acmid = {605421},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukherjee:2002:CSA:635508.605421,
 author = {Mukherjee, Shubhendu S. and Silla, Federico and Bannon, Peter and Emer, Joel and Lang, Steve and Webb, David},
 title = {A comparative study of arbitration algorithms for the Alpha 21364 pipelined router},
 abstract = {Interconnection networks usually consist of a fabric of interconnected routers, which receive packets arriving at their input ports and forward them to appropriate output ports. Unfortunately, network packets moving through these routers are often delayed due to conflicting demand for resources, such as output ports or buffer space. Hence, routers typically employ arbiters</i> that resolve conflicting resource demands to maximize the number of matches between packets waiting at input ports and free output ports. Efficient design and implementation of the algorithm running on these arbiters is critical to maximize network performance.This paper proposes a new arbitration algorithm called SPAA</i> (Simple Pipelined Arbitration Algorithm), which is implemented in the Alpha 21364 processor's on-chip router pipeline. Simulation results show that SPAA significantly outperforms two earlier well-known arbitration algorithms: PIM (Parallel Iterative Matching) and WFA (Wave-Front Arbiter) implemented in the SGI Spider switch. SPAA outperforms PIM and WFA because SPAA exhibits matching capabilities similar to PIM and WFA under realistic conditions when many output ports are busy, incurs fewer clock cycles to perform the arbitration, and can be pipelined effectively. Additionally, we propose a new prioritization policy called the Rotary Rule,</i> which prevents the network's adverse performance degradation from saturation at high network loads by prioritizing packets already in the network over new packets generated by caches or memory.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605421},
 doi = {http://doi.acm.org/10.1145/635508.605421},
 acmid = {605421},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukherjee:2002:CSA:635506.605421,
 author = {Mukherjee, Shubhendu S. and Silla, Federico and Bannon, Peter and Emer, Joel and Lang, Steve and Webb, David},
 title = {A comparative study of arbitration algorithms for the Alpha 21364 pipelined router},
 abstract = {Interconnection networks usually consist of a fabric of interconnected routers, which receive packets arriving at their input ports and forward them to appropriate output ports. Unfortunately, network packets moving through these routers are often delayed due to conflicting demand for resources, such as output ports or buffer space. Hence, routers typically employ arbiters</i> that resolve conflicting resource demands to maximize the number of matches between packets waiting at input ports and free output ports. Efficient design and implementation of the algorithm running on these arbiters is critical to maximize network performance.This paper proposes a new arbitration algorithm called SPAA</i> (Simple Pipelined Arbitration Algorithm), which is implemented in the Alpha 21364 processor's on-chip router pipeline. Simulation results show that SPAA significantly outperforms two earlier well-known arbitration algorithms: PIM (Parallel Iterative Matching) and WFA (Wave-Front Arbiter) implemented in the SGI Spider switch. SPAA outperforms PIM and WFA because SPAA exhibits matching capabilities similar to PIM and WFA under realistic conditions when many output ports are busy, incurs fewer clock cycles to perform the arbitration, and can be pipelined effectively. Additionally, we propose a new prioritization policy called the Rotary Rule,</i> which prevents the network's adverse performance degradation from saturation at high network loads by prioritizing packets already in the network over new packets generated by caches or memory.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605421},
 doi = {http://doi.acm.org/10.1145/635506.605421},
 acmid = {605421},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukherjee:2002:CSA:605432.605421,
 author = {Mukherjee, Shubhendu S. and Silla, Federico and Bannon, Peter and Emer, Joel and Lang, Steve and Webb, David},
 title = {A comparative study of arbitration algorithms for the Alpha 21364 pipelined router},
 abstract = {Interconnection networks usually consist of a fabric of interconnected routers, which receive packets arriving at their input ports and forward them to appropriate output ports. Unfortunately, network packets moving through these routers are often delayed due to conflicting demand for resources, such as output ports or buffer space. Hence, routers typically employ arbiters</i> that resolve conflicting resource demands to maximize the number of matches between packets waiting at input ports and free output ports. Efficient design and implementation of the algorithm running on these arbiters is critical to maximize network performance.This paper proposes a new arbitration algorithm called SPAA</i> (Simple Pipelined Arbitration Algorithm), which is implemented in the Alpha 21364 processor's on-chip router pipeline. Simulation results show that SPAA significantly outperforms two earlier well-known arbitration algorithms: PIM (Parallel Iterative Matching) and WFA (Wave-Front Arbiter) implemented in the SGI Spider switch. SPAA outperforms PIM and WFA because SPAA exhibits matching capabilities similar to PIM and WFA under realistic conditions when many output ports are busy, incurs fewer clock cycles to perform the arbitration, and can be pipelined effectively. Additionally, we propose a new prioritization policy called the Rotary Rule,</i> which prevents the network's adverse performance degradation from saturation at high network loads by prioritizing packets already in the network over new packets generated by caches or memory.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605421},
 doi = {http://doi.acm.org/10.1145/605432.605421},
 acmid = {605421},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:2002:IWS:605397.605423,
 author = {Kim, Hyong-youb and Pai, Vijay S. and Rixner, Scott},
 title = {Increasing web server throughput with network interface data caching},
 abstract = {This paper introduces network interface data caching, a new technique to reduce local interconnect traffic on networking servers by caching frequently-requested content on a programmable network interface. The operating system on the host CPU determines which data to store in the cache and for which packets it should use data from the cache. To facilitate data reuse across multiple packets and connections, the cache only stores application-level response content (such as HTTP data), with application-level and networking headers generated by the host CPU. Network interface data caching can reduce PCI traffic by up to 57\% on a prototype implementation of a uniprocessor web server. This traffic reduction results in up to 31\% performance improvement, leading to a peak server throughput of 1571 Mb/s.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605423},
 doi = {http://doi.acm.org/10.1145/605397.605423},
 acmid = {605423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:IWS:635506.605423,
 author = {Kim, Hyong-youb and Pai, Vijay S. and Rixner, Scott},
 title = {Increasing web server throughput with network interface data caching},
 abstract = {This paper introduces network interface data caching, a new technique to reduce local interconnect traffic on networking servers by caching frequently-requested content on a programmable network interface. The operating system on the host CPU determines which data to store in the cache and for which packets it should use data from the cache. To facilitate data reuse across multiple packets and connections, the cache only stores application-level response content (such as HTTP data), with application-level and networking headers generated by the host CPU. Network interface data caching can reduce PCI traffic by up to 57\% on a prototype implementation of a uniprocessor web server. This traffic reduction results in up to 31\% performance improvement, leading to a peak server throughput of 1571 Mb/s.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605423},
 doi = {http://doi.acm.org/10.1145/635506.605423},
 acmid = {605423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:IWS:635508.605423,
 author = {Kim, Hyong-youb and Pai, Vijay S. and Rixner, Scott},
 title = {Increasing web server throughput with network interface data caching},
 abstract = {This paper introduces network interface data caching, a new technique to reduce local interconnect traffic on networking servers by caching frequently-requested content on a programmable network interface. The operating system on the host CPU determines which data to store in the cache and for which packets it should use data from the cache. To facilitate data reuse across multiple packets and connections, the cache only stores application-level response content (such as HTTP data), with application-level and networking headers generated by the host CPU. Network interface data caching can reduce PCI traffic by up to 57\% on a prototype implementation of a uniprocessor web server. This traffic reduction results in up to 31\% performance improvement, leading to a peak server throughput of 1571 Mb/s.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605423},
 doi = {http://doi.acm.org/10.1145/635508.605423},
 acmid = {605423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2002:IWS:605432.605423,
 author = {Kim, Hyong-youb and Pai, Vijay S. and Rixner, Scott},
 title = {Increasing web server throughput with network interface data caching},
 abstract = {This paper introduces network interface data caching, a new technique to reduce local interconnect traffic on networking servers by caching frequently-requested content on a programmable network interface. The operating system on the host CPU determines which data to store in the cache and for which packets it should use data from the cache. To facilitate data reuse across multiple packets and connections, the cache only stores application-level response content (such as HTTP data), with application-level and networking headers generated by the host CPU. Network interface data caching can reduce PCI traffic by up to 57\% on a prototype implementation of a uniprocessor web server. This traffic reduction results in up to 31\% performance improvement, leading to a peak server throughput of 1571 Mb/s.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605423},
 doi = {http://doi.acm.org/10.1145/605432.605423},
 acmid = {605423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kohler:2002:PLO:605397.605424,
 author = {Kohler, Eddie and Morris, Robert and Chen, Benjie},
 title = {Programming language optimizations for modular router configurations},
 abstract = {Networking systems such as Ensemble, the x</i>-kernel, Scout, and Click achieve flexibility by building routers and other packet processors from modular components. Unfortunately, component designs are often slower than purpose-built code, and routers in particular have stringent efficiency requirements. This paper addresses the efficiency problems of one component-based router, Click, through optimization tools inspired in part by compiler optimization passes. This pragmatic approach can result in significant performance improvements; for example, the combination of three optimizations reduces the amount of CPU time Click requires to process a packet in a simple IP router by 34\%. We present several optimization tools, describe how those tools affected the design of Click itself, and present detailed evaluations of Click's performance with and without optimization.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {251--263},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605424},
 doi = {http://doi.acm.org/10.1145/605397.605424},
 acmid = {605424},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kohler:2002:PLO:635508.605424,
 author = {Kohler, Eddie and Morris, Robert and Chen, Benjie},
 title = {Programming language optimizations for modular router configurations},
 abstract = {Networking systems such as Ensemble, the x</i>-kernel, Scout, and Click achieve flexibility by building routers and other packet processors from modular components. Unfortunately, component designs are often slower than purpose-built code, and routers in particular have stringent efficiency requirements. This paper addresses the efficiency problems of one component-based router, Click, through optimization tools inspired in part by compiler optimization passes. This pragmatic approach can result in significant performance improvements; for example, the combination of three optimizations reduces the amount of CPU time Click requires to process a packet in a simple IP router by 34\%. We present several optimization tools, describe how those tools affected the design of Click itself, and present detailed evaluations of Click's performance with and without optimization.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {251--263},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605424},
 doi = {http://doi.acm.org/10.1145/635508.605424},
 acmid = {605424},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kohler:2002:PLO:635506.605424,
 author = {Kohler, Eddie and Morris, Robert and Chen, Benjie},
 title = {Programming language optimizations for modular router configurations},
 abstract = {Networking systems such as Ensemble, the x</i>-kernel, Scout, and Click achieve flexibility by building routers and other packet processors from modular components. Unfortunately, component designs are often slower than purpose-built code, and routers in particular have stringent efficiency requirements. This paper addresses the efficiency problems of one component-based router, Click, through optimization tools inspired in part by compiler optimization passes. This pragmatic approach can result in significant performance improvements; for example, the combination of three optimizations reduces the amount of CPU time Click requires to process a packet in a simple IP router by 34\%. We present several optimization tools, describe how those tools affected the design of Click itself, and present detailed evaluations of Click's performance with and without optimization.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {251--263},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605424},
 doi = {http://doi.acm.org/10.1145/635506.605424},
 acmid = {605424},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kohler:2002:PLO:605432.605424,
 author = {Kohler, Eddie and Morris, Robert and Chen, Benjie},
 title = {Programming language optimizations for modular router configurations},
 abstract = {Networking systems such as Ensemble, the x</i>-kernel, Scout, and Click achieve flexibility by building routers and other packet processors from modular components. Unfortunately, component designs are often slower than purpose-built code, and routers in particular have stringent efficiency requirements. This paper addresses the efficiency problems of one component-based router, Click, through optimization tools inspired in part by compiler optimization passes. This pragmatic approach can result in significant performance improvements; for example, the combination of three optimizations reduces the amount of CPU time Click requires to process a packet in a simple IP router by 34\%. We present several optimization tools, describe how those tools affected the design of Click itself, and present detailed evaluations of Click's performance with and without optimization.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {251--263},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605424},
 doi = {http://doi.acm.org/10.1145/605432.605424},
 acmid = {605424},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sivathanu:2002:ERA:605397.605425,
 author = {Sivathanu, Muthian and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Evolving RPC for active storage},
 abstract = {We introduce Scriptable RPC (SRPC), an RPC-based framework that enables distributed system services to take advantage of active components. Technology trends point to a world where each component in a system (whether disk, network interface, or memory) has substantial computational capabilities; however, traditional methods of building distributed services are not designed to take advantage of these new architectures, mandating wholesale change of the software base to exploit more powerful hardware. In contrast, SRPC provides a direct and simple migration path for traditional services into the active environment.We demonstrate the power and flexibility of the SRPC framework through a series of case studies, with a focus on active storage servers. Specifically, we find three advantages to our approach. First, SRPC improves the performance of distributed file servers, reducing latency by combining the execution of operations at the file server. Second, SRPC enables the ready addition of new functionality; for example, more powerful cache consistency models can be realized on top of a server that exports a simple NFS-like interface. Third, SRPC simplifies the construction of distributed services; operations that are difficult to coordinate across client and server can now be co-executed at the server, thus avoiding costly agreement and crash-recovery protocols.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {264--276},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605425},
 doi = {http://doi.acm.org/10.1145/605397.605425},
 acmid = {605425},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sivathanu:2002:ERA:605432.605425,
 author = {Sivathanu, Muthian and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Evolving RPC for active storage},
 abstract = {We introduce Scriptable RPC (SRPC), an RPC-based framework that enables distributed system services to take advantage of active components. Technology trends point to a world where each component in a system (whether disk, network interface, or memory) has substantial computational capabilities; however, traditional methods of building distributed services are not designed to take advantage of these new architectures, mandating wholesale change of the software base to exploit more powerful hardware. In contrast, SRPC provides a direct and simple migration path for traditional services into the active environment.We demonstrate the power and flexibility of the SRPC framework through a series of case studies, with a focus on active storage servers. Specifically, we find three advantages to our approach. First, SRPC improves the performance of distributed file servers, reducing latency by combining the execution of operations at the file server. Second, SRPC enables the ready addition of new functionality; for example, more powerful cache consistency models can be realized on top of a server that exports a simple NFS-like interface. Third, SRPC simplifies the construction of distributed services; operations that are difficult to coordinate across client and server can now be co-executed at the server, thus avoiding costly agreement and crash-recovery protocols.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {264--276},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605425},
 doi = {http://doi.acm.org/10.1145/605432.605425},
 acmid = {605425},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sivathanu:2002:ERA:635508.605425,
 author = {Sivathanu, Muthian and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Evolving RPC for active storage},
 abstract = {We introduce Scriptable RPC (SRPC), an RPC-based framework that enables distributed system services to take advantage of active components. Technology trends point to a world where each component in a system (whether disk, network interface, or memory) has substantial computational capabilities; however, traditional methods of building distributed services are not designed to take advantage of these new architectures, mandating wholesale change of the software base to exploit more powerful hardware. In contrast, SRPC provides a direct and simple migration path for traditional services into the active environment.We demonstrate the power and flexibility of the SRPC framework through a series of case studies, with a focus on active storage servers. Specifically, we find three advantages to our approach. First, SRPC improves the performance of distributed file servers, reducing latency by combining the execution of operations at the file server. Second, SRPC enables the ready addition of new functionality; for example, more powerful cache consistency models can be realized on top of a server that exports a simple NFS-like interface. Third, SRPC simplifies the construction of distributed services; operations that are difficult to coordinate across client and server can now be co-executed at the server, thus avoiding costly agreement and crash-recovery protocols.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {264--276},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605425},
 doi = {http://doi.acm.org/10.1145/635508.605425},
 acmid = {605425},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sivathanu:2002:ERA:635506.605425,
 author = {Sivathanu, Muthian and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Evolving RPC for active storage},
 abstract = {We introduce Scriptable RPC (SRPC), an RPC-based framework that enables distributed system services to take advantage of active components. Technology trends point to a world where each component in a system (whether disk, network interface, or memory) has substantial computational capabilities; however, traditional methods of building distributed services are not designed to take advantage of these new architectures, mandating wholesale change of the software base to exploit more powerful hardware. In contrast, SRPC provides a direct and simple migration path for traditional services into the active environment.We demonstrate the power and flexibility of the SRPC framework through a series of case studies, with a focus on active storage servers. Specifically, we find three advantages to our approach. First, SRPC improves the performance of distributed file servers, reducing latency by combining the execution of operations at the file server. Second, SRPC enables the ready addition of new functionality; for example, more powerful cache consistency models can be realized on top of a server that exports a simple NFS-like interface. Third, SRPC simplifies the construction of distributed services; operations that are difficult to coordinate across client and server can now be co-executed at the server, thus avoiding costly agreement and crash-recovery protocols.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {264--276},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605425},
 doi = {http://doi.acm.org/10.1145/635506.605425},
 acmid = {605425},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooksey:2002:SCD:635506.605427,
 author = {Cooksey, Robert and Jourdan, Stephan and Grunwald, Dirk},
 title = {A stateless, content-directed data prefetching mechanism},
 abstract = {Although central processor speeds continues to improve, improvements in overall system performance are increasingly hampered by memory latency, especially for pointer-intensive applications. To counter this loss of performance, numerous data and instruction prefetch mechanisms have been proposed. Recently, several proposals have posited a memory-side</i> prefetcher; typically, these prefetchers involve a distinct processor that executes a program slice that would effectively prefetch data needed by the primary program. Alternative designs embody large state tables that learn the miss reference behavior of the processor and attempt to prefetch likely misses.This paper proposes Content-Directed Data Prefetching,</i> a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modern language systems. This technique is modeled after conservative garbage collection, and prefetches "likely" virtual addresses observed in memory references. This prefetching mechanism uses the underlying data of the application, and provides an 11.3\% speedup using no additional processor state.</i> By adding less than \&frac12;\% space overhead to the second level cache, performance can be further increased to 12.6\% across a range of "real world" applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635506.605427},
 doi = {http://doi.acm.org/10.1145/635506.605427},
 acmid = {605427},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooksey:2002:SCD:635508.605427,
 author = {Cooksey, Robert and Jourdan, Stephan and Grunwald, Dirk},
 title = {A stateless, content-directed data prefetching mechanism},
 abstract = {Although central processor speeds continues to improve, improvements in overall system performance are increasingly hampered by memory latency, especially for pointer-intensive applications. To counter this loss of performance, numerous data and instruction prefetch mechanisms have been proposed. Recently, several proposals have posited a memory-side</i> prefetcher; typically, these prefetchers involve a distinct processor that executes a program slice that would effectively prefetch data needed by the primary program. Alternative designs embody large state tables that learn the miss reference behavior of the processor and attempt to prefetch likely misses.This paper proposes Content-Directed Data Prefetching,</i> a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modern language systems. This technique is modeled after conservative garbage collection, and prefetches "likely" virtual addresses observed in memory references. This prefetching mechanism uses the underlying data of the application, and provides an 11.3\% speedup using no additional processor state.</i> By adding less than \&frac12;\% space overhead to the second level cache, performance can be further increased to 12.6\% across a range of "real world" applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/635508.605427},
 doi = {http://doi.acm.org/10.1145/635508.605427},
 acmid = {605427},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooksey:2002:SCD:605432.605427,
 author = {Cooksey, Robert and Jourdan, Stephan and Grunwald, Dirk},
 title = {A stateless, content-directed data prefetching mechanism},
 abstract = {Although central processor speeds continues to improve, improvements in overall system performance are increasingly hampered by memory latency, especially for pointer-intensive applications. To counter this loss of performance, numerous data and instruction prefetch mechanisms have been proposed. Recently, several proposals have posited a memory-side</i> prefetcher; typically, these prefetchers involve a distinct processor that executes a program slice that would effectively prefetch data needed by the primary program. Alternative designs embody large state tables that learn the miss reference behavior of the processor and attempt to prefetch likely misses.This paper proposes Content-Directed Data Prefetching,</i> a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modern language systems. This technique is modeled after conservative garbage collection, and prefetches "likely" virtual addresses observed in memory references. This prefetching mechanism uses the underlying data of the application, and provides an 11.3\% speedup using no additional processor state.</i> By adding less than \&frac12;\% space overhead to the second level cache, performance can be further increased to 12.6\% across a range of "real world" applications.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605432.605427},
 doi = {http://doi.acm.org/10.1145/605432.605427},
 acmid = {605427},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cooksey:2002:SCD:605397.605427,
 author = {Cooksey, Robert and Jourdan, Stephan and Grunwald, Dirk},
 title = {A stateless, content-directed data prefetching mechanism},
 abstract = {Although central processor speeds continues to improve, improvements in overall system performance are increasingly hampered by memory latency, especially for pointer-intensive applications. To counter this loss of performance, numerous data and instruction prefetch mechanisms have been proposed. Recently, several proposals have posited a memory-side</i> prefetcher; typically, these prefetchers involve a distinct processor that executes a program slice that would effectively prefetch data needed by the primary program. Alternative designs embody large state tables that learn the miss reference behavior of the processor and attempt to prefetch likely misses.This paper proposes Content-Directed Data Prefetching,</i> a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modern language systems. This technique is modeled after conservative garbage collection, and prefetches "likely" virtual addresses observed in memory references. This prefetching mechanism uses the underlying data of the application, and provides an 11.3\% speedup using no additional processor state.</i> By adding less than \&frac12;\% space overhead to the second level cache, performance can be further increased to 12.6\% across a range of "real world" applications.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/605397.605427},
 doi = {http://doi.acm.org/10.1145/605397.605427},
 acmid = {605427},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gordon:2002:SCC:635506.605428,
 author = {Gordon, Michael I. and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S. and Lamb, Andrew A. and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman},
 title = {A stream compiler for communication-exposed architectures},
 abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {291--303},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605428},
 doi = {http://doi.acm.org/10.1145/635506.605428},
 acmid = {605428},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gordon:2002:SCC:605397.605428,
 author = {Gordon, Michael I. and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S. and Lamb, Andrew A. and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman},
 title = {A stream compiler for communication-exposed architectures},
 abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {291--303},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605428},
 doi = {http://doi.acm.org/10.1145/605397.605428},
 acmid = {605428},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gordon:2002:SCC:635508.605428,
 author = {Gordon, Michael I. and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S. and Lamb, Andrew A. and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman},
 title = {A stream compiler for communication-exposed architectures},
 abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {291--303},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605428},
 doi = {http://doi.acm.org/10.1145/635508.605428},
 acmid = {605428},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gordon:2002:SCC:605432.605428,
 author = {Gordon, Michael I. and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S. and Lamb, Andrew A. and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman},
 title = {A stream compiler for communication-exposed architectures},
 abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {291--303},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605428},
 doi = {http://doi.acm.org/10.1145/605432.605428},
 acmid = {605428},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Witchel:2002:MMP:635508.605429,
 author = {Witchel, Emmett and Cates, Josh and Asanovi\'{c}, Krste},
 title = {Mondrian memory protection},
 abstract = {Mondrian memory protection (MMP) is a fine-grained protection scheme that allows multiple protection domains to flexibly share memory and export protected services. In contrast to earlier page-based systems, MMP allows arbitrary permissions control at the granularity of individual words. We use a compressed permissions table to reduce space overheads and employ two levels of permissions caching to reduce run-time overheads. The protection tables in our implementation add less than 9\% overhead to the memory space used by the application. Accessing the protection tables adds than 8\% additional memory references to the accesses made by the application. Although it can be layered on top of demand-paged virtual memory, MMP is also well-suited to embedded systems with a single physical address space. We extend MMP to support segment translation which allows a memory segment to appear at another location in the address space. We use this translation to implement zero-copy networking underneath the standard read system call interface, where packet payload fragments are connected together by the translation system to avoid data copying. This saves 52\% of the memory references used by a traditional copying network stack.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {36},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5980},
 pages = {304--316},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635508.605429},
 doi = {http://doi.acm.org/10.1145/635508.605429},
 acmid = {605429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Witchel:2002:MMP:635506.605429,
 author = {Witchel, Emmett and Cates, Josh and Asanovi\'{c}, Krste},
 title = {Mondrian memory protection},
 abstract = {Mondrian memory protection (MMP) is a fine-grained protection scheme that allows multiple protection domains to flexibly share memory and export protected services. In contrast to earlier page-based systems, MMP allows arbitrary permissions control at the granularity of individual words. We use a compressed permissions table to reduce space overheads and employ two levels of permissions caching to reduce run-time overheads. The protection tables in our implementation add less than 9\% overhead to the memory space used by the application. Accessing the protection tables adds than 8\% additional memory references to the accesses made by the application. Although it can be layered on top of demand-paged virtual memory, MMP is also well-suited to embedded systems with a single physical address space. We extend MMP to support segment translation which allows a memory segment to appear at another location in the address space. We use this translation to implement zero-copy networking underneath the standard read system call interface, where packet payload fragments are connected together by the translation system to avoid data copying. This saves 52\% of the memory references used by a traditional copying network stack.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {30},
 issue = {5},
 month = {October},
 year = {2002},
 issn = {0163-5964},
 pages = {304--316},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/635506.605429},
 doi = {http://doi.acm.org/10.1145/635506.605429},
 acmid = {605429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Witchel:2002:MMP:605397.605429,
 author = {Witchel, Emmett and Cates, Josh and Asanovi\'{c}, Krste},
 title = {Mondrian memory protection},
 abstract = {Mondrian memory protection (MMP) is a fine-grained protection scheme that allows multiple protection domains to flexibly share memory and export protected services. In contrast to earlier page-based systems, MMP allows arbitrary permissions control at the granularity of individual words. We use a compressed permissions table to reduce space overheads and employ two levels of permissions caching to reduce run-time overheads. The protection tables in our implementation add less than 9\% overhead to the memory space used by the application. Accessing the protection tables adds than 8\% additional memory references to the accesses made by the application. Although it can be layered on top of demand-paged virtual memory, MMP is also well-suited to embedded systems with a single physical address space. We extend MMP to support segment translation which allows a memory segment to appear at another location in the address space. We use this translation to implement zero-copy networking underneath the standard read system call interface, where packet payload fragments are connected together by the translation system to avoid data copying. This saves 52\% of the memory references used by a traditional copying network stack.},
 booktitle = {Proceedings of the 10th international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-X},
 year = {2002},
 isbn = {1-58113-574-2},
 location = {San Jose, California},
 pages = {304--316},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605397.605429},
 doi = {http://doi.acm.org/10.1145/605397.605429},
 acmid = {605429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Witchel:2002:MMP:605432.605429,
 author = {Witchel, Emmett and Cates, Josh and Asanovi\'{c}, Krste},
 title = {Mondrian memory protection},
 abstract = {Mondrian memory protection (MMP) is a fine-grained protection scheme that allows multiple protection domains to flexibly share memory and export protected services. In contrast to earlier page-based systems, MMP allows arbitrary permissions control at the granularity of individual words. We use a compressed permissions table to reduce space overheads and employ two levels of permissions caching to reduce run-time overheads. The protection tables in our implementation add less than 9\% overhead to the memory space used by the application. Accessing the protection tables adds than 8\% additional memory references to the accesses made by the application. Although it can be layered on top of demand-paged virtual memory, MMP is also well-suited to embedded systems with a single physical address space. We extend MMP to support segment translation which allows a memory segment to appear at another location in the address space. We use this translation to implement zero-copy networking underneath the standard read system call interface, where packet payload fragments are connected together by the translation system to avoid data copying. This saves 52\% of the memory references used by a traditional copying network stack.},
 journal = {SIGPLAN Not.},
 volume = {37},
 issue = {10},
 month = {October},
 year = {2002},
 issn = {0362-1340},
 pages = {304--316},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/605432.605429},
 doi = {http://doi.acm.org/10.1145/605432.605429},
 acmid = {605429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schlosser:2000:DCS:378995.378996,
 author = {Schlosser, Steven W. and Griffin, John Linwood and Nagle, David F. and Ganger, Gregory R.},
 title = {Designing computer systems with MEMS-based storage},
 abstract = {For decades the RAM-to-disk memory hierarchy gap has plagued computer architects. An exciting new storage technology based on microelectromechanical systems (MEMS) is poised to fill a large portion of this performance gap, significantly reduce system power consumption, and enable many new applications. This paper explores the system-level implications of integrating MEMS-based storage into the memory hierarchy. Results show that standalone MEMS-based storage reduces I/O stall times by 4-74X over disks and improves overall application runtimes by 1.9-4.4X. When used as on-board caches for disks, MEMS-based storage improves I/O response time by up to 3.5X. Further, the energy consumption of MEMS-based storage is 10-54X less than that of state-of-the-art low-power disk drives. The combination of the high-level physical characteristics of MEMS-based storage (small footprints, high shock tolerance) and the ability to directly integrate MEMS-based storage with processing leads to such new applications as portable gigabit storage systems and ubiquitous active storage nodes.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.378996},
 doi = {http://doi.acm.org/10.1145/378995.378996},
 acmid = {378996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schlosser:2000:DCS:384264.378996,
 author = {Schlosser, Steven W. and Griffin, John Linwood and Nagle, David F. and Ganger, Gregory R.},
 title = {Designing computer systems with MEMS-based storage},
 abstract = {For decades the RAM-to-disk memory hierarchy gap has plagued computer architects. An exciting new storage technology based on microelectromechanical systems (MEMS) is poised to fill a large portion of this performance gap, significantly reduce system power consumption, and enable many new applications. This paper explores the system-level implications of integrating MEMS-based storage into the memory hierarchy. Results show that standalone MEMS-based storage reduces I/O stall times by 4-74X over disks and improves overall application runtimes by 1.9-4.4X. When used as on-board caches for disks, MEMS-based storage improves I/O response time by up to 3.5X. Further, the energy consumption of MEMS-based storage is 10-54X less than that of state-of-the-art low-power disk drives. The combination of the high-level physical characteristics of MEMS-based storage (small footprints, high shock tolerance) and the ability to directly integrate MEMS-based storage with processing leads to such new applications as portable gigabit storage systems and ubiquitous active storage nodes.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.378996},
 doi = {http://doi.acm.org/10.1145/384264.378996},
 acmid = {378996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schlosser:2000:DCS:378993.378996,
 author = {Schlosser, Steven W. and Griffin, John Linwood and Nagle, David F. and Ganger, Gregory R.},
 title = {Designing computer systems with MEMS-based storage},
 abstract = {For decades the RAM-to-disk memory hierarchy gap has plagued computer architects. An exciting new storage technology based on microelectromechanical systems (MEMS) is poised to fill a large portion of this performance gap, significantly reduce system power consumption, and enable many new applications. This paper explores the system-level implications of integrating MEMS-based storage into the memory hierarchy. Results show that standalone MEMS-based storage reduces I/O stall times by 4-74X over disks and improves overall application runtimes by 1.9-4.4X. When used as on-board caches for disks, MEMS-based storage improves I/O response time by up to 3.5X. Further, the energy consumption of MEMS-based storage is 10-54X less than that of state-of-the-art low-power disk drives. The combination of the high-level physical characteristics of MEMS-based storage (small footprints, high shock tolerance) and the ability to directly integrate MEMS-based storage with processing leads to such new applications as portable gigabit storage systems and ubiquitous active storage nodes.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.378996},
 doi = {http://doi.acm.org/10.1145/378993.378996},
 acmid = {378996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gharachorloo:2000:ADA:384264.378997,
 author = {Gharachorloo, Kourosh and Sharma, Madhu and Steely, Simon and Van Doren, Stephen},
 title = {Architecture and design of AlphaServer GS320},
 abstract = {This paper describes the architecture and implementation of the AlphaServer GS320, a cache-coherent non-uniform memory access multiprocessor developed at Compaq. The AlphaServer GS320 architecture is specifically targeted at medium-scale multiprocessing with 32 to 64 processors. Each node in the design consists of four Alpha 21264 processors, up to 32GB of coherent memory, and an aggressive IO subsystem. The current implementation supports up to 8 such nodes for a total of 32 processors. While snoopy-based designs have been stretched to medium-scale multiprocessors by some vendors, providing sufficient snoop bandwidth remains a major challenge especially in systems with aggressive processors. At the same time, directory protocols targeted at larger scale designs lead to a number of inherent inefficiencies relative to snoopy designs. A key goal of the AlphaServer GS320 architecture has been to achieve the best-of-both-worlds, partly by exploiting the bounded scale of the target systems.This paper focuses on the unique design features used in the AlphaServer GS320 to efficiently implement coherence and consistency. The guiding principle for our directory-based protocol is to address correctness issues related to rare protocol races without burdening the common transaction flows. Our protocol exhibits lower occupancy and lower message counts compared to previous designs, and provides more efficient handling of 3-hop transactions. Furthermore, our design naturally lends itself to elegant solutions for deadlock, livelock, starvation, and fairness. The AlphaServer GS320 architecture also incorporates a couple of innovative techniques that extend previous approaches for efficiently implementing memory consistency models. These techniques allow us to generate commit events (which are used for ordering purposes) well in advance of formulating the reply to a transaction. Furthermore, the separation of the commit event allows time-critical replies to by-pass inbound requests without violating ordering properties. Even though our design specifically targets medium-scale servers, many of the same techniques can be applied to larger-scale directory-based and smaller-scale snoopy-based designs. Finally, we evaluate the performance impact of some of the above optimizations and present a few competitive benchmark results.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.378997},
 doi = {http://doi.acm.org/10.1145/384264.378997},
 acmid = {378997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gharachorloo:2000:ADA:378993.378997,
 author = {Gharachorloo, Kourosh and Sharma, Madhu and Steely, Simon and Van Doren, Stephen},
 title = {Architecture and design of AlphaServer GS320},
 abstract = {This paper describes the architecture and implementation of the AlphaServer GS320, a cache-coherent non-uniform memory access multiprocessor developed at Compaq. The AlphaServer GS320 architecture is specifically targeted at medium-scale multiprocessing with 32 to 64 processors. Each node in the design consists of four Alpha 21264 processors, up to 32GB of coherent memory, and an aggressive IO subsystem. The current implementation supports up to 8 such nodes for a total of 32 processors. While snoopy-based designs have been stretched to medium-scale multiprocessors by some vendors, providing sufficient snoop bandwidth remains a major challenge especially in systems with aggressive processors. At the same time, directory protocols targeted at larger scale designs lead to a number of inherent inefficiencies relative to snoopy designs. A key goal of the AlphaServer GS320 architecture has been to achieve the best-of-both-worlds, partly by exploiting the bounded scale of the target systems.This paper focuses on the unique design features used in the AlphaServer GS320 to efficiently implement coherence and consistency. The guiding principle for our directory-based protocol is to address correctness issues related to rare protocol races without burdening the common transaction flows. Our protocol exhibits lower occupancy and lower message counts compared to previous designs, and provides more efficient handling of 3-hop transactions. Furthermore, our design naturally lends itself to elegant solutions for deadlock, livelock, starvation, and fairness. The AlphaServer GS320 architecture also incorporates a couple of innovative techniques that extend previous approaches for efficiently implementing memory consistency models. These techniques allow us to generate commit events (which are used for ordering purposes) well in advance of formulating the reply to a transaction. Furthermore, the separation of the commit event allows time-critical replies to by-pass inbound requests without violating ordering properties. Even though our design specifically targets medium-scale servers, many of the same techniques can be applied to larger-scale directory-based and smaller-scale snoopy-based designs. Finally, we evaluate the performance impact of some of the above optimizations and present a few competitive benchmark results.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.378997},
 doi = {http://doi.acm.org/10.1145/378993.378997},
 acmid = {378997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gharachorloo:2000:ADA:378995.378997,
 author = {Gharachorloo, Kourosh and Sharma, Madhu and Steely, Simon and Van Doren, Stephen},
 title = {Architecture and design of AlphaServer GS320},
 abstract = {This paper describes the architecture and implementation of the AlphaServer GS320, a cache-coherent non-uniform memory access multiprocessor developed at Compaq. The AlphaServer GS320 architecture is specifically targeted at medium-scale multiprocessing with 32 to 64 processors. Each node in the design consists of four Alpha 21264 processors, up to 32GB of coherent memory, and an aggressive IO subsystem. The current implementation supports up to 8 such nodes for a total of 32 processors. While snoopy-based designs have been stretched to medium-scale multiprocessors by some vendors, providing sufficient snoop bandwidth remains a major challenge especially in systems with aggressive processors. At the same time, directory protocols targeted at larger scale designs lead to a number of inherent inefficiencies relative to snoopy designs. A key goal of the AlphaServer GS320 architecture has been to achieve the best-of-both-worlds, partly by exploiting the bounded scale of the target systems.This paper focuses on the unique design features used in the AlphaServer GS320 to efficiently implement coherence and consistency. The guiding principle for our directory-based protocol is to address correctness issues related to rare protocol races without burdening the common transaction flows. Our protocol exhibits lower occupancy and lower message counts compared to previous designs, and provides more efficient handling of 3-hop transactions. Furthermore, our design naturally lends itself to elegant solutions for deadlock, livelock, starvation, and fairness. The AlphaServer GS320 architecture also incorporates a couple of innovative techniques that extend previous approaches for efficiently implementing memory consistency models. These techniques allow us to generate commit events (which are used for ordering purposes) well in advance of formulating the reply to a transaction. Furthermore, the separation of the commit event allows time-critical replies to by-pass inbound requests without violating ordering properties. Even though our design specifically targets medium-scale servers, many of the same techniques can be applied to larger-scale directory-based and smaller-scale snoopy-based designs. Finally, we evaluate the performance impact of some of the above optimizations and present a few competitive benchmark results.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.378997},
 doi = {http://doi.acm.org/10.1145/378995.378997},
 acmid = {378997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martin:2000:TSA:378995.378998,
 author = {Martin, Milo M. K. and Sorin, Daniel J. and Ailamaki, Anatassia and Alameldeen, Alaa R. and Dickson, Ross M. and Mauer, Carl J. and Moore, Kevin E. and Plakal, Manoj and Hill, Mark D. and Wood, David A.},
 title = {Timestamp snooping: an approach for extending SMPs},
 abstract = {Symmetric muultiprocessor (SMP) servers provide superior performance for the commercial workloads that dominate the Internet. Our simulation results show that over one-third of cache misses by these applications result in cache-to-cache transfers, where the data is found in another processor's cache rather than in memory. SMPs are optimized for this case by using snooping protocols that broadcast address transactions to all processors. Conversely, directory-based shared-memory systems must indirectly locate the owner and sharers through a directory, resulting in larger average miss latencies.This paper proposes timestamp snooping, a technique that allows SMPs to i) utilize high-speed switched interconnection networks and ii) exploit physical locality by delivering address transactions to processors and memories without regard to order. Traditional snooping requires physical ordering of transactions. Timestamp snooping works by processing address transactions in a logical order. Logical time is maintained by adding a few bits per address transaction and having network switches perform a handshake to ensure on-time delivery. Processors and memories then reorder transactions based on their timestamps to establish a total order.We evaluate timestamp snooping with commercial workloads on a 16-processor SPARC system using the Simics full-system simulator. We simulate both an indirect (butterfly) and a direct (torus) network design. For OLTP, DSS, web serving, web searching, and one scientific application, timestamp snooping with the butterfly network runs 6-28\% faster than directories, at a cost of 13-43\% more link traffic. Similarly, with the torus network, timestamp snooping runs 6-29\% faster for 17-37\% more link traffic. Thus, timestamp snooping is worth considering when buying more interconnect bandwidth is easier than reducing interconnect latency.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.378998},
 doi = {http://doi.acm.org/10.1145/378995.378998},
 acmid = {378998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Martin:2000:TSA:378993.378998,
 author = {Martin, Milo M. K. and Sorin, Daniel J. and Ailamaki, Anatassia and Alameldeen, Alaa R. and Dickson, Ross M. and Mauer, Carl J. and Moore, Kevin E. and Plakal, Manoj and Hill, Mark D. and Wood, David A.},
 title = {Timestamp snooping: an approach for extending SMPs},
 abstract = {Symmetric muultiprocessor (SMP) servers provide superior performance for the commercial workloads that dominate the Internet. Our simulation results show that over one-third of cache misses by these applications result in cache-to-cache transfers, where the data is found in another processor's cache rather than in memory. SMPs are optimized for this case by using snooping protocols that broadcast address transactions to all processors. Conversely, directory-based shared-memory systems must indirectly locate the owner and sharers through a directory, resulting in larger average miss latencies.This paper proposes timestamp snooping, a technique that allows SMPs to i) utilize high-speed switched interconnection networks and ii) exploit physical locality by delivering address transactions to processors and memories without regard to order. Traditional snooping requires physical ordering of transactions. Timestamp snooping works by processing address transactions in a logical order. Logical time is maintained by adding a few bits per address transaction and having network switches perform a handshake to ensure on-time delivery. Processors and memories then reorder transactions based on their timestamps to establish a total order.We evaluate timestamp snooping with commercial workloads on a 16-processor SPARC system using the Simics full-system simulator. We simulate both an indirect (butterfly) and a direct (torus) network design. For OLTP, DSS, web serving, web searching, and one scientific application, timestamp snooping with the butterfly network runs 6-28\% faster than directories, at a cost of 13-43\% more link traffic. Similarly, with the torus network, timestamp snooping runs 6-29\% faster for 17-37\% more link traffic. Thus, timestamp snooping is worth considering when buying more interconnect bandwidth is easier than reducing interconnect latency.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.378998},
 doi = {http://doi.acm.org/10.1145/378993.378998},
 acmid = {378998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martin:2000:TSA:384264.378998,
 author = {Martin, Milo M. K. and Sorin, Daniel J. and Ailamaki, Anatassia and Alameldeen, Alaa R. and Dickson, Ross M. and Mauer, Carl J. and Moore, Kevin E. and Plakal, Manoj and Hill, Mark D. and Wood, David A.},
 title = {Timestamp snooping: an approach for extending SMPs},
 abstract = {Symmetric muultiprocessor (SMP) servers provide superior performance for the commercial workloads that dominate the Internet. Our simulation results show that over one-third of cache misses by these applications result in cache-to-cache transfers, where the data is found in another processor's cache rather than in memory. SMPs are optimized for this case by using snooping protocols that broadcast address transactions to all processors. Conversely, directory-based shared-memory systems must indirectly locate the owner and sharers through a directory, resulting in larger average miss latencies.This paper proposes timestamp snooping, a technique that allows SMPs to i) utilize high-speed switched interconnection networks and ii) exploit physical locality by delivering address transactions to processors and memories without regard to order. Traditional snooping requires physical ordering of transactions. Timestamp snooping works by processing address transactions in a logical order. Logical time is maintained by adding a few bits per address transaction and having network switches perform a handshake to ensure on-time delivery. Processors and memories then reorder transactions based on their timestamps to establish a total order.We evaluate timestamp snooping with commercial workloads on a 16-processor SPARC system using the Simics full-system simulator. We simulate both an indirect (butterfly) and a direct (torus) network design. For OLTP, DSS, web serving, web searching, and one scientific application, timestamp snooping with the butterfly network runs 6-28\% faster than directories, at a cost of 13-43\% more link traffic. Similarly, with the torus network, timestamp snooping runs 6-29\% faster for 17-37\% more link traffic. Thus, timestamp snooping is worth considering when buying more interconnect bandwidth is easier than reducing interconnect latency.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.378998},
 doi = {http://doi.acm.org/10.1145/384264.378998},
 acmid = {378998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nanda:2000:MPR:378993.378999,
 author = {Nanda, Ashwini and Mak, Kwok-Ken and Sugarvanam, Krishnan and Sahoo, Ramendra K. and Soundarararjan, Vijayaraghavan and Smith, T. Basil},
 title = {MemorIES3: a programmable, real-time hardware emulation tool for multiprocessor server design},
 abstract = {Modern system design often requires multiple levels of simulation for design validation and performance debugging. However, while machines have gotten faster, and simulators have become more detailed, simulation speeds have not tracked machine speeds, As a result, it is difficult to simulate realistic problem sizes and hardware configurations for a target machine. Instead, researchers have focussed on developing sealing methodologies and running smaller problem sizes and configurations that attempt to represent the behavior of the real problem. Given the increasing size of problems today, it is unclear whether such an approach yields accurate results. Moreover, although commercial workloads are prevalent and important in today's marketplace, many simulation tools are unable to adequately profile such applications, let alone for realistic sizes.In this paper we present a hardware-based emulation tool that can be used to aid memory system designers. Our focus is on the memory system because the ever-widening gap between processor and memory speeds means that optimizing the memory subsystem is critical for performance. We present the design of the Memory</i> I</i>nstrumentation and E</i>mulation S</i>ystem (MemoriES). MemoriES is a programmable tool designed using FPGAs and SDRAMs. It plugs into an SMP bus to perform on-line emulation of several cache configurations, structures and protocols while the system is running real-life workloads in real-time, without any slowdown in application execution speed. We demonstrate its usefulness in several case studies, and find several important results. First, using traces to perform system evaluation can lead to incorrect results (off by 100\% or more in some cases) if the trace size is not sufficiently large. Second. MemoriES is able to detect performance problems by profiling miss behavior over the entire course of a run, rather than relying on a small interval of time. Finally, we observe that previous studies of SPLASH2 applications using scaled application sizes can result in optimistic miss rates relative to real sizes on real machines, providing potentially misleading data when used for design evaluation.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.378999},
 doi = {http://doi.acm.org/10.1145/378993.378999},
 acmid = {378999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nanda:2000:MPR:384264.378999,
 author = {Nanda, Ashwini and Mak, Kwok-Ken and Sugarvanam, Krishnan and Sahoo, Ramendra K. and Soundarararjan, Vijayaraghavan and Smith, T. Basil},
 title = {MemorIES3: a programmable, real-time hardware emulation tool for multiprocessor server design},
 abstract = {Modern system design often requires multiple levels of simulation for design validation and performance debugging. However, while machines have gotten faster, and simulators have become more detailed, simulation speeds have not tracked machine speeds, As a result, it is difficult to simulate realistic problem sizes and hardware configurations for a target machine. Instead, researchers have focussed on developing sealing methodologies and running smaller problem sizes and configurations that attempt to represent the behavior of the real problem. Given the increasing size of problems today, it is unclear whether such an approach yields accurate results. Moreover, although commercial workloads are prevalent and important in today's marketplace, many simulation tools are unable to adequately profile such applications, let alone for realistic sizes.In this paper we present a hardware-based emulation tool that can be used to aid memory system designers. Our focus is on the memory system because the ever-widening gap between processor and memory speeds means that optimizing the memory subsystem is critical for performance. We present the design of the Memory</i> I</i>nstrumentation and E</i>mulation S</i>ystem (MemoriES). MemoriES is a programmable tool designed using FPGAs and SDRAMs. It plugs into an SMP bus to perform on-line emulation of several cache configurations, structures and protocols while the system is running real-life workloads in real-time, without any slowdown in application execution speed. We demonstrate its usefulness in several case studies, and find several important results. First, using traces to perform system evaluation can lead to incorrect results (off by 100\% or more in some cases) if the trace size is not sufficiently large. Second. MemoriES is able to detect performance problems by profiling miss behavior over the entire course of a run, rather than relying on a small interval of time. Finally, we observe that previous studies of SPLASH2 applications using scaled application sizes can result in optimistic miss rates relative to real sizes on real machines, providing potentially misleading data when used for design evaluation.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.378999},
 doi = {http://doi.acm.org/10.1145/384264.378999},
 acmid = {378999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nanda:2000:MPR:378995.378999,
 author = {Nanda, Ashwini and Mak, Kwok-Ken and Sugarvanam, Krishnan and Sahoo, Ramendra K. and Soundarararjan, Vijayaraghavan and Smith, T. Basil},
 title = {MemorIES3: a programmable, real-time hardware emulation tool for multiprocessor server design},
 abstract = {Modern system design often requires multiple levels of simulation for design validation and performance debugging. However, while machines have gotten faster, and simulators have become more detailed, simulation speeds have not tracked machine speeds, As a result, it is difficult to simulate realistic problem sizes and hardware configurations for a target machine. Instead, researchers have focussed on developing sealing methodologies and running smaller problem sizes and configurations that attempt to represent the behavior of the real problem. Given the increasing size of problems today, it is unclear whether such an approach yields accurate results. Moreover, although commercial workloads are prevalent and important in today's marketplace, many simulation tools are unable to adequately profile such applications, let alone for realistic sizes.In this paper we present a hardware-based emulation tool that can be used to aid memory system designers. Our focus is on the memory system because the ever-widening gap between processor and memory speeds means that optimizing the memory subsystem is critical for performance. We present the design of the Memory</i> I</i>nstrumentation and E</i>mulation S</i>ystem (MemoriES). MemoriES is a programmable tool designed using FPGAs and SDRAMs. It plugs into an SMP bus to perform on-line emulation of several cache configurations, structures and protocols while the system is running real-life workloads in real-time, without any slowdown in application execution speed. We demonstrate its usefulness in several case studies, and find several important results. First, using traces to perform system evaluation can lead to incorrect results (off by 100\% or more in some cases) if the trace size is not sufficiently large. Second. MemoriES is able to detect performance problems by profiling miss behavior over the entire course of a run, rather than relying on a small interval of time. Finally, we observe that previous studies of SPLASH2 applications using scaled application sizes can result in optimistic miss rates relative to real sizes on real machines, providing potentially misleading data when used for design evaluation.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.378999},
 doi = {http://doi.acm.org/10.1145/378995.378999},
 acmid = {378999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gibson:2000:FVF:378995.379000,
 author = {Gibson, Jeff and Kunz, Robert and Ofelt, David and Horowitz, Mark and Hennessy, John and Heinrich, Mark},
 title = {FLASH vs. (Simulated) FLASH: closing the simulation loop},
 abstract = {Simulation is the primary method for evaluating computer systems during all phases of the design process. One significant problem with simulation is that it rarely models the system exactly, and quantifying the resulting simulator error can be difficult. More importantly, architects often assume without proof that although their simulator may make inaccurate absolute performance predictions, it will still accurately predict architectural trends.This paper studies the source and magnitude of error in a range of architectural simulators by comparing the simulated execution time of several applications and microbenchmarks to their execution time on the actual hardware being modeled. The existence of a hardware gold standard allows us to find, quantify, and fix simulator inaccuracies. We then use the simulators to predict architectural trends and analyze the sensitivity of the results to the simulator configuration. We find that most of our simulators predict trends accurately, as long as they model all of the important performance effects for the application in question. Unfortunately, it is difficult to know what these effects are without having a hardware reference, as they can be quite subtle. This calls into question the value, for architectural studies, of highly detailed simulators whose characteristics are not carefully validated against s real hardware design.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {49--58},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378995.379000},
 doi = {http://doi.acm.org/10.1145/378995.379000},
 acmid = {379000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gibson:2000:FVF:384264.379000,
 author = {Gibson, Jeff and Kunz, Robert and Ofelt, David and Horowitz, Mark and Hennessy, John and Heinrich, Mark},
 title = {FLASH vs. (Simulated) FLASH: closing the simulation loop},
 abstract = {Simulation is the primary method for evaluating computer systems during all phases of the design process. One significant problem with simulation is that it rarely models the system exactly, and quantifying the resulting simulator error can be difficult. More importantly, architects often assume without proof that although their simulator may make inaccurate absolute performance predictions, it will still accurately predict architectural trends.This paper studies the source and magnitude of error in a range of architectural simulators by comparing the simulated execution time of several applications and microbenchmarks to their execution time on the actual hardware being modeled. The existence of a hardware gold standard allows us to find, quantify, and fix simulator inaccuracies. We then use the simulators to predict architectural trends and analyze the sensitivity of the results to the simulator configuration. We find that most of our simulators predict trends accurately, as long as they model all of the important performance effects for the application in question. Unfortunately, it is difficult to know what these effects are without having a hardware reference, as they can be quite subtle. This calls into question the value, for architectural studies, of highly detailed simulators whose characteristics are not carefully validated against s real hardware design.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {49--58},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384264.379000},
 doi = {http://doi.acm.org/10.1145/384264.379000},
 acmid = {379000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gibson:2000:FVF:378993.379000,
 author = {Gibson, Jeff and Kunz, Robert and Ofelt, David and Horowitz, Mark and Hennessy, John and Heinrich, Mark},
 title = {FLASH vs. (Simulated) FLASH: closing the simulation loop},
 abstract = {Simulation is the primary method for evaluating computer systems during all phases of the design process. One significant problem with simulation is that it rarely models the system exactly, and quantifying the resulting simulator error can be difficult. More importantly, architects often assume without proof that although their simulator may make inaccurate absolute performance predictions, it will still accurately predict architectural trends.This paper studies the source and magnitude of error in a range of architectural simulators by comparing the simulated execution time of several applications and microbenchmarks to their execution time on the actual hardware being modeled. The existence of a hardware gold standard allows us to find, quantify, and fix simulator inaccuracies. We then use the simulators to predict architectural trends and analyze the sensitivity of the results to the simulator configuration. We find that most of our simulators predict trends accurately, as long as they model all of the important performance effects for the application in question. Unfortunately, it is difficult to know what these effects are without having a hardware reference, as they can be quite subtle. This calls into question the value, for architectural studies, of highly detailed simulators whose characteristics are not carefully validated against s real hardware design.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {49--58},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378993.379000},
 doi = {http://doi.acm.org/10.1145/378993.379000},
 acmid = {379000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chou:2000:UMC:378995.379002,
 author = {Chou, Andy and Chelf, Benjamin and Engler, Dawson and Heinrich, Mark},
 title = {Using meta-level compilation to check FLASH protocol code},
 abstract = {Building systems such as OS kernels and embedded software is difficult. An important source of this difficulty is the numerous rules they must obey: interrupts cannot be disabled for ~too long," global variables must be protected by locks, user pointers passed to OS code must be checked for safety before use, etc. A single violation can crash the system, yet typically these invariants are unchecked, existing only on paper or in the implementor's mind.This paper is a case study in how system implementors can use a new programming methodology, meta-level compilation (MC), to easily check such invariants. It focuses on using MC to check for errors in the code used to manage cache coherence on the FLASH shared memory multiprocessor. The only real practical method known for verifying such code is testing and simulation. We show that simple, system-specific checkers can dramatically improve this situation by statically pinpointing errors in the program source. These checkers can be written by implementors themselves and, by exploiting the system-specific information this allows, can detect errors unreachable with other methods. The checkers in this paper found 34 bugs in FLASH code despite the care used in building it and the years of testing it has undergone. Many of these errors fall in the worst category of systems bugs: those that show up sporadically only after days of continuous use. The case study is interesting because it shows that the MC approach finds serious errors in well-tested, non-toy systems code. Further, the code to find such bugs is usually 10-100 lines long, written in a few hours, and exactly locates errors that, if discovered during testing, would require several days of investigation by an experienced implementor.The paper presents 8 checkers we wrote, their application to five different protocol implementations, and a discussion of the errors that we found.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379002},
 doi = {http://doi.acm.org/10.1145/378995.379002},
 acmid = {379002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chou:2000:UMC:378993.379002,
 author = {Chou, Andy and Chelf, Benjamin and Engler, Dawson and Heinrich, Mark},
 title = {Using meta-level compilation to check FLASH protocol code},
 abstract = {Building systems such as OS kernels and embedded software is difficult. An important source of this difficulty is the numerous rules they must obey: interrupts cannot be disabled for ~too long," global variables must be protected by locks, user pointers passed to OS code must be checked for safety before use, etc. A single violation can crash the system, yet typically these invariants are unchecked, existing only on paper or in the implementor's mind.This paper is a case study in how system implementors can use a new programming methodology, meta-level compilation (MC), to easily check such invariants. It focuses on using MC to check for errors in the code used to manage cache coherence on the FLASH shared memory multiprocessor. The only real practical method known for verifying such code is testing and simulation. We show that simple, system-specific checkers can dramatically improve this situation by statically pinpointing errors in the program source. These checkers can be written by implementors themselves and, by exploiting the system-specific information this allows, can detect errors unreachable with other methods. The checkers in this paper found 34 bugs in FLASH code despite the care used in building it and the years of testing it has undergone. Many of these errors fall in the worst category of systems bugs: those that show up sporadically only after days of continuous use. The case study is interesting because it shows that the MC approach finds serious errors in well-tested, non-toy systems code. Further, the code to find such bugs is usually 10-100 lines long, written in a few hours, and exactly locates errors that, if discovered during testing, would require several days of investigation by an experienced implementor.The paper presents 8 checkers we wrote, their application to five different protocol implementations, and a discussion of the errors that we found.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379002},
 doi = {http://doi.acm.org/10.1145/378993.379002},
 acmid = {379002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chou:2000:UMC:384264.379002,
 author = {Chou, Andy and Chelf, Benjamin and Engler, Dawson and Heinrich, Mark},
 title = {Using meta-level compilation to check FLASH protocol code},
 abstract = {Building systems such as OS kernels and embedded software is difficult. An important source of this difficulty is the numerous rules they must obey: interrupts cannot be disabled for ~too long," global variables must be protected by locks, user pointers passed to OS code must be checked for safety before use, etc. A single violation can crash the system, yet typically these invariants are unchecked, existing only on paper or in the implementor's mind.This paper is a case study in how system implementors can use a new programming methodology, meta-level compilation (MC), to easily check such invariants. It focuses on using MC to check for errors in the code used to manage cache coherence on the FLASH shared memory multiprocessor. The only real practical method known for verifying such code is testing and simulation. We show that simple, system-specific checkers can dramatically improve this situation by statically pinpointing errors in the program source. These checkers can be written by implementors themselves and, by exploiting the system-specific information this allows, can detect errors unreachable with other methods. The checkers in this paper found 34 bugs in FLASH code despite the care used in building it and the years of testing it has undergone. Many of these errors fall in the worst category of systems bugs: those that show up sporadically only after days of continuous use. The case study is interesting because it shows that the MC approach finds serious errors in well-tested, non-toy systems code. Further, the code to find such bugs is usually 10-100 lines long, written in a few hours, and exactly locates errors that, if discovered during testing, would require several days of investigation by an experienced implementor.The paper presents 8 checkers we wrote, their application to five different protocol implementations, and a discussion of the errors that we found.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379002},
 doi = {http://doi.acm.org/10.1145/384264.379002},
 acmid = {379002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhoedjang:2000:EDA:384264.379004,
 author = {Bhoedjang, Raoul A. F. and Verstoep, Kees and R\"{u}hl, Tim and Bal, Henri E. and Hofman, Rutger F. H.},
 title = {Evaluating design alternatives for reliable communication on high-speed networks},
 abstract = {We systematically evaluate the performance of five implementations of a single, user-level communication interface. Each implementation makes different architectural assumptions about the reliability of the network hardware and the capabilities of the network interface. The implementations differ accordingly in their division of protocol tasks between host software, network-interface firmware, and network hardware. Using microbenchmarks, parallel-programming systems, and parallel applications, we assess the performance impact of different protocol decompositions. We show how moving protocol tasks to a relatively slow network interface yields both performance advantages and disadvantages, depending on the characteristics of the application and the underlying parallel-programming system. In particular, we show that a communication system that assumes highly reliable network hardware and that uses network-interface support to process multicast traffic performs best for all applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384264.379004},
 doi = {http://doi.acm.org/10.1145/384264.379004},
 acmid = {379004},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bhoedjang:2000:EDA:378993.379004,
 author = {Bhoedjang, Raoul A. F. and Verstoep, Kees and R\"{u}hl, Tim and Bal, Henri E. and Hofman, Rutger F. H.},
 title = {Evaluating design alternatives for reliable communication on high-speed networks},
 abstract = {We systematically evaluate the performance of five implementations of a single, user-level communication interface. Each implementation makes different architectural assumptions about the reliability of the network hardware and the capabilities of the network interface. The implementations differ accordingly in their division of protocol tasks between host software, network-interface firmware, and network hardware. Using microbenchmarks, parallel-programming systems, and parallel applications, we assess the performance impact of different protocol decompositions. We show how moving protocol tasks to a relatively slow network interface yields both performance advantages and disadvantages, depending on the characteristics of the application and the underlying parallel-programming system. In particular, we show that a communication system that assumes highly reliable network hardware and that uses network-interface support to process multicast traffic performs best for all applications.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378993.379004},
 doi = {http://doi.acm.org/10.1145/378993.379004},
 acmid = {379004},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhoedjang:2000:EDA:378995.379004,
 author = {Bhoedjang, Raoul A. F. and Verstoep, Kees and R\"{u}hl, Tim and Bal, Henri E. and Hofman, Rutger F. H.},
 title = {Evaluating design alternatives for reliable communication on high-speed networks},
 abstract = {We systematically evaluate the performance of five implementations of a single, user-level communication interface. Each implementation makes different architectural assumptions about the reliability of the network hardware and the capabilities of the network interface. The implementations differ accordingly in their division of protocol tasks between host software, network-interface firmware, and network hardware. Using microbenchmarks, parallel-programming systems, and parallel applications, we assess the performance impact of different protocol decompositions. We show how moving protocol tasks to a relatively slow network interface yields both performance advantages and disadvantages, depending on the characteristics of the application and the underlying parallel-programming system. In particular, we show that a communication system that assumes highly reliable network hardware and that uses network-interface support to process multicast traffic performs best for all applications.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378995.379004},
 doi = {http://doi.acm.org/10.1145/378995.379004},
 acmid = {379004},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mattson:2000:CS:378995.379005,
 author = {Mattson, Peter and Dally, William J. and Rixner, Scott and Kapasi, Ujval J. and Owens, John D.},
 title = {Communication scheduling},
 abstract = {The high arithmetic rates of media processing applications require architectures with tens to hundreds of functional units, multiple register files, and explicit interconnect between functional units and register files. Communication scheduling enables scheduling to these emerging architectures, including those that use shared buses and register file ports. Scheduling to these shared interconnect architectures is difficult because it requires simultaneously allocating functional units to operations and buses and register file ports to the communications between operations. Prior VLIW scheduling algorithms are limited to clustered register file architectures with no shared buses or register file ports. Communication scheduling extends the range of target architectures by making each communication explicit and decomposing it into three components: a write stub, zero or more copy operations, and a read stub. Communication scheduling allows media processing kernels to achieve 98\% of the performance of a central register file architecture on a distributed register file architecture with only 9\% of the area, 6\% of the power consumption, and 37\% of the access delay, and 120\% of the performance of a clustered register file architecture on a distributed register file architecture with 56\% of the area and 50\% of the power consumption.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {82--92},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378995.379005},
 doi = {http://doi.acm.org/10.1145/378995.379005},
 acmid = {379005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mattson:2000:CS:384264.379005,
 author = {Mattson, Peter and Dally, William J. and Rixner, Scott and Kapasi, Ujval J. and Owens, John D.},
 title = {Communication scheduling},
 abstract = {The high arithmetic rates of media processing applications require architectures with tens to hundreds of functional units, multiple register files, and explicit interconnect between functional units and register files. Communication scheduling enables scheduling to these emerging architectures, including those that use shared buses and register file ports. Scheduling to these shared interconnect architectures is difficult because it requires simultaneously allocating functional units to operations and buses and register file ports to the communications between operations. Prior VLIW scheduling algorithms are limited to clustered register file architectures with no shared buses or register file ports. Communication scheduling extends the range of target architectures by making each communication explicit and decomposing it into three components: a write stub, zero or more copy operations, and a read stub. Communication scheduling allows media processing kernels to achieve 98\% of the performance of a central register file architecture on a distributed register file architecture with only 9\% of the area, 6\% of the power consumption, and 37\% of the access delay, and 120\% of the performance of a clustered register file architecture on a distributed register file architecture with 56\% of the area and 50\% of the power consumption.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {82--92},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384264.379005},
 doi = {http://doi.acm.org/10.1145/384264.379005},
 acmid = {379005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mattson:2000:CS:378993.379005,
 author = {Mattson, Peter and Dally, William J. and Rixner, Scott and Kapasi, Ujval J. and Owens, John D.},
 title = {Communication scheduling},
 abstract = {The high arithmetic rates of media processing applications require architectures with tens to hundreds of functional units, multiple register files, and explicit interconnect between functional units and register files. Communication scheduling enables scheduling to these emerging architectures, including those that use shared buses and register file ports. Scheduling to these shared interconnect architectures is difficult because it requires simultaneously allocating functional units to operations and buses and register file ports to the communications between operations. Prior VLIW scheduling algorithms are limited to clustered register file architectures with no shared buses or register file ports. Communication scheduling extends the range of target architectures by making each communication explicit and decomposing it into three components: a write stub, zero or more copy operations, and a read stub. Communication scheduling allows media processing kernels to achieve 98\% of the performance of a central register file architecture on a distributed register file architecture with only 9\% of the area, 6\% of the power consumption, and 37\% of the access delay, and 120\% of the performance of a clustered register file architecture on a distributed register file architecture with 56\% of the area and 50\% of the power consumption.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {82--92},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378993.379005},
 doi = {http://doi.acm.org/10.1145/378993.379005},
 acmid = {379005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hill:2000:SAD:378993.379006,
 author = {Hill, Jason and Szewczyk, Robert and Woo, Alec and Hollar, Seth and Culler, David and Pister, Kristofer},
 title = {System architecture directions for networked sensors},
 abstract = {Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {93--104},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379006},
 doi = {http://doi.acm.org/10.1145/378993.379006},
 acmid = {379006},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hill:2000:SAD:378995.379006,
 author = {Hill, Jason and Szewczyk, Robert and Woo, Alec and Hollar, Seth and Culler, David and Pister, Kristofer},
 title = {System architecture directions for networked sensors},
 abstract = {Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {93--104},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379006},
 doi = {http://doi.acm.org/10.1145/378995.379006},
 acmid = {379006},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hill:2000:SAD:384264.379006,
 author = {Hill, Jason and Szewczyk, Robert and Woo, Alec and Hollar, Seth and Culler, David and Pister, Kristofer},
 title = {System architecture directions for networked sensors},
 abstract = {Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {93--104},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379006},
 doi = {http://doi.acm.org/10.1145/384264.379006},
 acmid = {379006},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lebeck:2000:PAP:378995.379007,
 author = {Lebeck, Alvin R. and Fan, Xiaobo and Zeng, Heng and Ellis, Carla},
 title = {Power aware page allocation},
 abstract = {One of the major challenges of post-PC computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that power these mobile devices. Memory is a particularly important target for efforts to improve energy efficiency. Memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes. In this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features. In particular, we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies. We perform experiments using two complementary simulation environments: a trace-driven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detailed processor/memory model and a more memory-intensive set of benchmarks (SPEC2000). Our results make a compelling case for a cooperative hardware/software approach for exploiting power-aware memory, with down to as little as 45\% of the Energy\&amp;bull; Delay for the best static policy and 1\% to 20\% of the Energy\&amp;bull; Delay for a traditional full-power memory.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {105--116},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379007},
 doi = {http://doi.acm.org/10.1145/378995.379007},
 acmid = {379007},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lebeck:2000:PAP:384264.379007,
 author = {Lebeck, Alvin R. and Fan, Xiaobo and Zeng, Heng and Ellis, Carla},
 title = {Power aware page allocation},
 abstract = {One of the major challenges of post-PC computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that power these mobile devices. Memory is a particularly important target for efforts to improve energy efficiency. Memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes. In this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features. In particular, we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies. We perform experiments using two complementary simulation environments: a trace-driven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detailed processor/memory model and a more memory-intensive set of benchmarks (SPEC2000). Our results make a compelling case for a cooperative hardware/software approach for exploiting power-aware memory, with down to as little as 45\% of the Energy\&amp;bull; Delay for the best static policy and 1\% to 20\% of the Energy\&amp;bull; Delay for a traditional full-power memory.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {105--116},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379007},
 doi = {http://doi.acm.org/10.1145/384264.379007},
 acmid = {379007},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lebeck:2000:PAP:378993.379007,
 author = {Lebeck, Alvin R. and Fan, Xiaobo and Zeng, Heng and Ellis, Carla},
 title = {Power aware page allocation},
 abstract = {One of the major challenges of post-PC computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that power these mobile devices. Memory is a particularly important target for efforts to improve energy efficiency. Memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes. In this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features. In particular, we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies. We perform experiments using two complementary simulation environments: a trace-driven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detailed processor/memory model and a more memory-intensive set of benchmarks (SPEC2000). Our results make a compelling case for a cooperative hardware/software approach for exploiting power-aware memory, with down to as little as 45\% of the Energy\&amp;bull; Delay for the best static policy and 1\% to 20\% of the Energy\&amp;bull; Delay for a traditional full-power memory.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {105--116},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379007},
 doi = {http://doi.acm.org/10.1145/378993.379007},
 acmid = {379007},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Berger:2000:HSM:378993.379232,
 author = {Berger, Emery D. and McKinley, Kathryn S. and Blumofe, Robert D. and Wilson, Paul R.},
 title = {Hoard: a scalable memory allocator for multithreaded applications},
 abstract = {Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P</i> (the number of processors) to unbounded memory consumption.This paper introduces Hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. Hoard is the first allocator to simultaneously solve the above problems. Hoard combines one global heap and per-processor heaps with a novel discipline that provably bounds memory consumption and has very low synchronization costs in the common case. Our results on eleven programs demonstrate that Hoard yields low average fragmentation and improves overall program performance over the standard Solaris allocator by up to a factor of 60 on 14 processors, and up to a factor of 18 over the next best allocator we tested.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379232},
 doi = {http://doi.acm.org/10.1145/378993.379232},
 acmid = {379232},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berger:2000:HSM:378995.379232,
 author = {Berger, Emery D. and McKinley, Kathryn S. and Blumofe, Robert D. and Wilson, Paul R.},
 title = {Hoard: a scalable memory allocator for multithreaded applications},
 abstract = {Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P</i> (the number of processors) to unbounded memory consumption.This paper introduces Hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. Hoard is the first allocator to simultaneously solve the above problems. Hoard combines one global heap and per-processor heaps with a novel discipline that provably bounds memory consumption and has very low synchronization costs in the common case. Our results on eleven programs demonstrate that Hoard yields low average fragmentation and improves overall program performance over the standard Solaris allocator by up to a factor of 60 on 14 processors, and up to a factor of 18 over the next best allocator we tested.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379232},
 doi = {http://doi.acm.org/10.1145/378995.379232},
 acmid = {379232},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berger:2000:HSM:384264.379232,
 author = {Berger, Emery D. and McKinley, Kathryn S. and Blumofe, Robert D. and Wilson, Paul R.},
 title = {Hoard: a scalable memory allocator for multithreaded applications},
 abstract = {Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P</i> (the number of processors) to unbounded memory consumption.This paper introduces Hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. Hoard is the first allocator to simultaneously solve the above problems. Hoard combines one global heap and per-processor heaps with a novel discipline that provably bounds memory consumption and has very low synchronization costs in the common case. Our results on eleven programs demonstrate that Hoard yields low average fragmentation and improves overall program performance over the standard Solaris allocator by up to a factor of 60 on 14 processors, and up to a factor of 18 over the next best allocator we tested.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379232},
 doi = {http://doi.acm.org/10.1145/384264.379232},
 acmid = {379232},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Flautner:2000:TPI:378993.379233,
 author = {Flautner, Kristi\'{a}n and Uhlig, Rich and Reinhardt, Steve and Mudge, Trevor},
 title = {Thread-level parallelism and interactive performance of desktop applications},
 abstract = {Multiprocessing is already prevalent in servers where multiple clients present an obvious source of thread-level parallelism. However, the case for multiprocessing is less clear for desktop applications. Nevertheless, architects are designing processors that count on the availability of thread-level parallelism. Unlike server workloads, the primary requirement of interactive applications is to respond to user events under human perception bounds rather than to maximize end-to-end throughput. In this paper we report on the thread-level parallelism and interactive response time of a variety of desktop applications. By tracking the communication between tasks, we can focus our measurements on the portions of the benchmark's execution that have the greatest impact on the user. We find that running our benchmarks on a dual-processor machine improves response time of mouse-click events by as much as 36\% and 22\% on average---out of a maximum possible 50\%. The benefits of multiprocessing are even more apparent when background tasks are considered. In our experiments, running a simple MP3 playback program in the background increases response time by 14\% on a uniprocessor while it only increases the response time on a dual processor by 4\%. When response times are fast enough for further improvements to be imperceptible, the increased idle time after interactive episodes could be exploited to build systems that are more power efficient.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {129--138},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378993.379233},
 doi = {http://doi.acm.org/10.1145/378993.379233},
 acmid = {379233},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flautner:2000:TPI:384264.379233,
 author = {Flautner, Kristi\'{a}n and Uhlig, Rich and Reinhardt, Steve and Mudge, Trevor},
 title = {Thread-level parallelism and interactive performance of desktop applications},
 abstract = {Multiprocessing is already prevalent in servers where multiple clients present an obvious source of thread-level parallelism. However, the case for multiprocessing is less clear for desktop applications. Nevertheless, architects are designing processors that count on the availability of thread-level parallelism. Unlike server workloads, the primary requirement of interactive applications is to respond to user events under human perception bounds rather than to maximize end-to-end throughput. In this paper we report on the thread-level parallelism and interactive response time of a variety of desktop applications. By tracking the communication between tasks, we can focus our measurements on the portions of the benchmark's execution that have the greatest impact on the user. We find that running our benchmarks on a dual-processor machine improves response time of mouse-click events by as much as 36\% and 22\% on average---out of a maximum possible 50\%. The benefits of multiprocessing are even more apparent when background tasks are considered. In our experiments, running a simple MP3 playback program in the background increases response time by 14\% on a uniprocessor while it only increases the response time on a dual processor by 4\%. When response times are fast enough for further improvements to be imperceptible, the increased idle time after interactive episodes could be exploited to build systems that are more power efficient.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {129--138},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384264.379233},
 doi = {http://doi.acm.org/10.1145/384264.379233},
 acmid = {379233},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Flautner:2000:TPI:378995.379233,
 author = {Flautner, Kristi\'{a}n and Uhlig, Rich and Reinhardt, Steve and Mudge, Trevor},
 title = {Thread-level parallelism and interactive performance of desktop applications},
 abstract = {Multiprocessing is already prevalent in servers where multiple clients present an obvious source of thread-level parallelism. However, the case for multiprocessing is less clear for desktop applications. Nevertheless, architects are designing processors that count on the availability of thread-level parallelism. Unlike server workloads, the primary requirement of interactive applications is to respond to user events under human perception bounds rather than to maximize end-to-end throughput. In this paper we report on the thread-level parallelism and interactive response time of a variety of desktop applications. By tracking the communication between tasks, we can focus our measurements on the portions of the benchmark's execution that have the greatest impact on the user. We find that running our benchmarks on a dual-processor machine improves response time of mouse-click events by as much as 36\% and 22\% on average---out of a maximum possible 50\%. The benefits of multiprocessing are even more apparent when background tasks are considered. In our experiments, running a simple MP3 playback program in the background increases response time by 14\% on a uniprocessor while it only increases the response time on a dual processor by 4\%. When response times are fast enough for further improvements to be imperceptible, the increased idle time after interactive episodes could be exploited to build systems that are more power efficient.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {129--138},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378995.379233},
 doi = {http://doi.acm.org/10.1145/378995.379233},
 acmid = {379233},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kawahito:2000:ENP:384264.379234,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Effective null pointer check elimination utilizing hardware trap},
 abstract = {We present a new algorithm for eliminating null pointer checks from programs written in Java\&amp;trade;. Our new algorithm is split into two phases. In the first phase, it moves null checks backward, and it is iterated for a few times with other optimizations to eliminate redundant null checks and maximize the effectiveness of other optimizations. In the second phase, it moves null checks forward and converts many null checks to hardware traps in order to minimize the execution cost of the remaining null checks. As a result, it eliminates many null checks effectively and exploits the maximum use of hardware traps. This algorithm has been implemented in the IBM cross-platform Java Just-in-Time (JIT) compiler. Our experimental results show that our approach improves performance by up to 71\% for jBYTEmark and up to 10\% for SPECjvm98 over the previously known best algorithm. They also show that it increases JIT compilation time by only 2.3\%. Although we implemented our algorithm for Java, it is also applicable for other languages requiring null checking.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384264.379234},
 doi = {http://doi.acm.org/10.1145/384264.379234},
 acmid = {379234},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kawahito:2000:ENP:378995.379234,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Effective null pointer check elimination utilizing hardware trap},
 abstract = {We present a new algorithm for eliminating null pointer checks from programs written in Java\&amp;trade;. Our new algorithm is split into two phases. In the first phase, it moves null checks backward, and it is iterated for a few times with other optimizations to eliminate redundant null checks and maximize the effectiveness of other optimizations. In the second phase, it moves null checks forward and converts many null checks to hardware traps in order to minimize the execution cost of the remaining null checks. As a result, it eliminates many null checks effectively and exploits the maximum use of hardware traps. This algorithm has been implemented in the IBM cross-platform Java Just-in-Time (JIT) compiler. Our experimental results show that our approach improves performance by up to 71\% for jBYTEmark and up to 10\% for SPECjvm98 over the previously known best algorithm. They also show that it increases JIT compilation time by only 2.3\%. Although we implemented our algorithm for Java, it is also applicable for other languages requiring null checking.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378995.379234},
 doi = {http://doi.acm.org/10.1145/378995.379234},
 acmid = {379234},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kawahito:2000:ENP:378993.379234,
 author = {Kawahito, Motohiro and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Effective null pointer check elimination utilizing hardware trap},
 abstract = {We present a new algorithm for eliminating null pointer checks from programs written in Java\&amp;trade;. Our new algorithm is split into two phases. In the first phase, it moves null checks backward, and it is iterated for a few times with other optimizations to eliminate redundant null checks and maximize the effectiveness of other optimizations. In the second phase, it moves null checks forward and converts many null checks to hardware traps in order to minimize the execution cost of the remaining null checks. As a result, it eliminates many null checks effectively and exploits the maximum use of hardware traps. This algorithm has been implemented in the IBM cross-platform Java Just-in-Time (JIT) compiler. Our experimental results show that our approach improves performance by up to 71\% for jBYTEmark and up to 10\% for SPECjvm98 over the previously known best algorithm. They also show that it increases JIT compilation time by only 2.3\%. Although we implemented our algorithm for Java, it is also applicable for other languages requiring null checking.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378993.379234},
 doi = {http://doi.acm.org/10.1145/378993.379234},
 acmid = {379234},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhang:2000:FVL:378995.379235,
 author = {Zhang, Youtao and Yang, Jun and Gupta, Rajiv},
 title = {Frequent value locality and value-centric data cache design},
 abstract = {By studying the behavior of programs in the SPECint95 suite we observed that six out of eight programs exhibit a new kind of value locality, the frequent value locality,</i> according to which a few values appear very frequently in memory locations and are therefore involved in a large fraction of memory accesses. In these six programs ten distinct values occupy over 50\% of all memory locations and on an average account for nearly 50\% of all memory accesses during program execution. This observation holds for smaller blocks of consecutive memory locations and the set of frequent values remains quite stable over the execution of the program.In the six benchmarks with frequent value locality, on an average 50\% of all cache misses occur during the reading or writing of the ten most frequently accessed values. We propose a new data cache structure, the frequent value cache</i> (FVC), which employs a value-centric</i> approach to caching data locations for exploiting the frequent value locality phenomenon. FVC is a small direct-mapped cache which is dedicated to holding only frequently occurring values. The value-centric nature of FVC enables us to store data in a compressed form where the compression is achieved by encoding the frequent values using a few bits. Moreover this simple compression scheme preserves the random access to data values in a cache line.Our experiments demonstrate that by augmenting a direct mapped cache (DMC) with a direct mapped FVC of size no more than 3 Kbytes we can obtain reductions in miss rates ranging from 1\% to 68\%. In fact we observed that higher reductions in miss rates can he achieved by augmenting a DMC with a small FVC as opposed to doubling the size of DMC for the 124.m88ksim and 134.perl benchmarks.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {150--159},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378995.379235},
 doi = {http://doi.acm.org/10.1145/378995.379235},
 acmid = {379235},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhang:2000:FVL:378993.379235,
 author = {Zhang, Youtao and Yang, Jun and Gupta, Rajiv},
 title = {Frequent value locality and value-centric data cache design},
 abstract = {By studying the behavior of programs in the SPECint95 suite we observed that six out of eight programs exhibit a new kind of value locality, the frequent value locality,</i> according to which a few values appear very frequently in memory locations and are therefore involved in a large fraction of memory accesses. In these six programs ten distinct values occupy over 50\% of all memory locations and on an average account for nearly 50\% of all memory accesses during program execution. This observation holds for smaller blocks of consecutive memory locations and the set of frequent values remains quite stable over the execution of the program.In the six benchmarks with frequent value locality, on an average 50\% of all cache misses occur during the reading or writing of the ten most frequently accessed values. We propose a new data cache structure, the frequent value cache</i> (FVC), which employs a value-centric</i> approach to caching data locations for exploiting the frequent value locality phenomenon. FVC is a small direct-mapped cache which is dedicated to holding only frequently occurring values. The value-centric nature of FVC enables us to store data in a compressed form where the compression is achieved by encoding the frequent values using a few bits. Moreover this simple compression scheme preserves the random access to data values in a cache line.Our experiments demonstrate that by augmenting a direct mapped cache (DMC) with a direct mapped FVC of size no more than 3 Kbytes we can obtain reductions in miss rates ranging from 1\% to 68\%. In fact we observed that higher reductions in miss rates can he achieved by augmenting a DMC with a small FVC as opposed to doubling the size of DMC for the 124.m88ksim and 134.perl benchmarks.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {150--159},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378993.379235},
 doi = {http://doi.acm.org/10.1145/378993.379235},
 acmid = {379235},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhang:2000:FVL:384264.379235,
 author = {Zhang, Youtao and Yang, Jun and Gupta, Rajiv},
 title = {Frequent value locality and value-centric data cache design},
 abstract = {By studying the behavior of programs in the SPECint95 suite we observed that six out of eight programs exhibit a new kind of value locality, the frequent value locality,</i> according to which a few values appear very frequently in memory locations and are therefore involved in a large fraction of memory accesses. In these six programs ten distinct values occupy over 50\% of all memory locations and on an average account for nearly 50\% of all memory accesses during program execution. This observation holds for smaller blocks of consecutive memory locations and the set of frequent values remains quite stable over the execution of the program.In the six benchmarks with frequent value locality, on an average 50\% of all cache misses occur during the reading or writing of the ten most frequently accessed values. We propose a new data cache structure, the frequent value cache</i> (FVC), which employs a value-centric</i> approach to caching data locations for exploiting the frequent value locality phenomenon. FVC is a small direct-mapped cache which is dedicated to holding only frequently occurring values. The value-centric nature of FVC enables us to store data in a compressed form where the compression is achieved by encoding the frequent values using a few bits. Moreover this simple compression scheme preserves the random access to data values in a cache line.Our experiments demonstrate that by augmenting a direct mapped cache (DMC) with a direct mapped FVC of size no more than 3 Kbytes we can obtain reductions in miss rates ranging from 1\% to 68\%. In fact we observed that higher reductions in miss rates can he achieved by augmenting a DMC with a small FVC as opposed to doubling the size of DMC for the 124.m88ksim and 134.perl benchmarks.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {150--159},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384264.379235},
 doi = {http://doi.acm.org/10.1145/384264.379235},
 acmid = {379235},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Burrows:2000:EFV:378993.379236,
 author = {Burrows, M. and Erlingsson, U. and Leung, S-T. A. and Vandevoorde, M. T. and Waldspurger, C. A. and Walker, K. and Weihl, W. E.},
 title = {Efficient and flexible value sampling},
 abstract = {This paper presents novel sampling-based techniques for collecting statistical profiles of register contents, data values, and other information associated with instructions, such as memory latencies. Values of interest are sampled in response to periodic interrupts. The resulting value profiles can be analyzed by programmers and optimizers to improve the performance of production uniprocessor and multiprocessor systems.Our value sampling system extends the DCPI continuous profiling infrastructure, and inherits many of its desirable properties: our value profiler has low overhead (approximately 10\% slowdown); it profiles all the code in the system, including the operating system kernel; and it operates transparently, without requiring any modifications to the profiled code.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {160--167},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/378993.379236},
 doi = {http://doi.acm.org/10.1145/378993.379236},
 acmid = {379236},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burrows:2000:EFV:378995.379236,
 author = {Burrows, M. and Erlingsson, U. and Leung, S-T. A. and Vandevoorde, M. T. and Waldspurger, C. A. and Walker, K. and Weihl, W. E.},
 title = {Efficient and flexible value sampling},
 abstract = {This paper presents novel sampling-based techniques for collecting statistical profiles of register contents, data values, and other information associated with instructions, such as memory latencies. Values of interest are sampled in response to periodic interrupts. The resulting value profiles can be analyzed by programmers and optimizers to improve the performance of production uniprocessor and multiprocessor systems.Our value sampling system extends the DCPI continuous profiling infrastructure, and inherits many of its desirable properties: our value profiler has low overhead (approximately 10\% slowdown); it profiles all the code in the system, including the operating system kernel; and it operates transparently, without requiring any modifications to the profiled code.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {160--167},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/378995.379236},
 doi = {http://doi.acm.org/10.1145/378995.379236},
 acmid = {379236},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burrows:2000:EFV:384264.379236,
 author = {Burrows, M. and Erlingsson, U. and Leung, S-T. A. and Vandevoorde, M. T. and Waldspurger, C. A. and Walker, K. and Weihl, W. E.},
 title = {Efficient and flexible value sampling},
 abstract = {This paper presents novel sampling-based techniques for collecting statistical profiles of register contents, data values, and other information associated with instructions, such as memory latencies. Values of interest are sampled in response to periodic interrupts. The resulting value profiles can be analyzed by programmers and optimizers to improve the performance of production uniprocessor and multiprocessor systems.Our value sampling system extends the DCPI continuous profiling infrastructure, and inherits many of its desirable properties: our value profiler has low overhead (approximately 10\% slowdown); it profiles all the code in the system, including the operating system kernel; and it operates transparently, without requiring any modifications to the profiled code.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {160--167},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/384264.379236},
 doi = {http://doi.acm.org/10.1145/384264.379236},
 acmid = {379236},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:2000:ASC:384264.379237,
 author = {Thekkath, David Lie Chandramohan and Mitchell, Mark and Lincoln, Patrick and Boneh, Dan and Mitchell, John and Horowitz, Mark},
 title = {Architectural support for copy and tamper resistant software},
 abstract = {Although there have been attempts to develop code transformations that yield tamper-resistant software, no reliable software-only methods are know. This paper studies the hardware implementation of a form of execute-only memory (XOM) that allows instructions stored in memory to be executed but not otherwise manipulated. To support XOM code we use a machine that supports internal compartments---a process in one compartment cannot read data from another compartment. All data that leaves the machine is encrypted, since we assume external memory is not secure. The design of this machine poses some interesting trade-offs between security, efficiency, and flexibility. We explore some of the potential security issues as one pushes the machine to become more efficient and flexible. Although security carries a performance penalty, our analysis indicates that it is possible to create a normal multi-tasking machine where nearly all applications can be run in XOM mode. While a virtual XOM machine is possible, the underlying hardware needs to support a unique private key, private memory, and traps on cache misses. For efficient operation, hardware assist to provide fast symmetric ciphers is also required.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {168--177},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384264.379237},
 doi = {http://doi.acm.org/10.1145/384264.379237},
 acmid = {379237},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:2000:ASC:378995.379237,
 author = {Thekkath, David Lie Chandramohan and Mitchell, Mark and Lincoln, Patrick and Boneh, Dan and Mitchell, John and Horowitz, Mark},
 title = {Architectural support for copy and tamper resistant software},
 abstract = {Although there have been attempts to develop code transformations that yield tamper-resistant software, no reliable software-only methods are know. This paper studies the hardware implementation of a form of execute-only memory (XOM) that allows instructions stored in memory to be executed but not otherwise manipulated. To support XOM code we use a machine that supports internal compartments---a process in one compartment cannot read data from another compartment. All data that leaves the machine is encrypted, since we assume external memory is not secure. The design of this machine poses some interesting trade-offs between security, efficiency, and flexibility. We explore some of the potential security issues as one pushes the machine to become more efficient and flexible. Although security carries a performance penalty, our analysis indicates that it is possible to create a normal multi-tasking machine where nearly all applications can be run in XOM mode. While a virtual XOM machine is possible, the underlying hardware needs to support a unique private key, private memory, and traps on cache misses. For efficient operation, hardware assist to provide fast symmetric ciphers is also required.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {168--177},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378995.379237},
 doi = {http://doi.acm.org/10.1145/378995.379237},
 acmid = {379237},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thekkath:2000:ASC:378993.379237,
 author = {Thekkath, David Lie Chandramohan and Mitchell, Mark and Lincoln, Patrick and Boneh, Dan and Mitchell, John and Horowitz, Mark},
 title = {Architectural support for copy and tamper resistant software},
 abstract = {Although there have been attempts to develop code transformations that yield tamper-resistant software, no reliable software-only methods are know. This paper studies the hardware implementation of a form of execute-only memory (XOM) that allows instructions stored in memory to be executed but not otherwise manipulated. To support XOM code we use a machine that supports internal compartments---a process in one compartment cannot read data from another compartment. All data that leaves the machine is encrypted, since we assume external memory is not secure. The design of this machine poses some interesting trade-offs between security, efficiency, and flexibility. We explore some of the potential security issues as one pushes the machine to become more efficient and flexible. Although security carries a performance penalty, our analysis indicates that it is possible to create a normal multi-tasking machine where nearly all applications can be run in XOM mode. While a virtual XOM machine is possible, the underlying hardware needs to support a unique private key, private memory, and traps on cache misses. For efficient operation, hardware assist to provide fast symmetric ciphers is also required.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {168--177},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378993.379237},
 doi = {http://doi.acm.org/10.1145/378993.379237},
 acmid = {379237},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burke:2000:ASF:384264.379238,
 author = {Burke, Jerome and McDonald, John and Austin, Todd},
 title = {Architectural support for fast symmetric-key cryptography},
 abstract = {The emergence of the Internet as a trusted medium for commerce and communication has made cryptography an essential component of modern information systems. Cryptography provides the mechanisms necessary to implement accountability, accuracy, and confidentiality in communication. As demands for secure communication bandwidth grow, efficient cryptographic processing will become increasingly vital to good system performance.In this paper, we explore techniques to improve the performance of symmetric key cipher algorithms. Eight popular strong encryption algorithms are examined in detail. Analysis reveals the algorithms are computationally complex and contain little parallelism. Overall throughput on a high-end microprocessor is quite poor, a 600 Mhz processor is incapable of saturating a T3 communication line with 3DES (triple DES) encrypted data.We introduce new instructions that improve the efficiency of the analyzed algorithms. Our approach adds instruction set support for fast substitutions, general permutations, rotates, and modular arithmetic. Performance analysis of the optimized ciphers shows an overall speedup of 59\% over a baseline machine with rotate instructions and 74\% speedup over a baseline without rotates. Even higher speedups are demonstrated with optimized substitutions (SBOXes) and additional functional unit resources. Our analyses of the original and optimized algorithms suggest future directions for the design of high-performance programmable cryptographic processors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {178--189},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379238},
 doi = {http://doi.acm.org/10.1145/384264.379238},
 acmid = {379238},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Burke:2000:ASF:378993.379238,
 author = {Burke, Jerome and McDonald, John and Austin, Todd},
 title = {Architectural support for fast symmetric-key cryptography},
 abstract = {The emergence of the Internet as a trusted medium for commerce and communication has made cryptography an essential component of modern information systems. Cryptography provides the mechanisms necessary to implement accountability, accuracy, and confidentiality in communication. As demands for secure communication bandwidth grow, efficient cryptographic processing will become increasingly vital to good system performance.In this paper, we explore techniques to improve the performance of symmetric key cipher algorithms. Eight popular strong encryption algorithms are examined in detail. Analysis reveals the algorithms are computationally complex and contain little parallelism. Overall throughput on a high-end microprocessor is quite poor, a 600 Mhz processor is incapable of saturating a T3 communication line with 3DES (triple DES) encrypted data.We introduce new instructions that improve the efficiency of the analyzed algorithms. Our approach adds instruction set support for fast substitutions, general permutations, rotates, and modular arithmetic. Performance analysis of the optimized ciphers shows an overall speedup of 59\% over a baseline machine with rotate instructions and 74\% speedup over a baseline without rotates. Even higher speedups are demonstrated with optimized substitutions (SBOXes) and additional functional unit resources. Our analyses of the original and optimized algorithms suggest future directions for the design of high-performance programmable cryptographic processors.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {178--189},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379238},
 doi = {http://doi.acm.org/10.1145/378993.379238},
 acmid = {379238},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burke:2000:ASF:378995.379238,
 author = {Burke, Jerome and McDonald, John and Austin, Todd},
 title = {Architectural support for fast symmetric-key cryptography},
 abstract = {The emergence of the Internet as a trusted medium for commerce and communication has made cryptography an essential component of modern information systems. Cryptography provides the mechanisms necessary to implement accountability, accuracy, and confidentiality in communication. As demands for secure communication bandwidth grow, efficient cryptographic processing will become increasingly vital to good system performance.In this paper, we explore techniques to improve the performance of symmetric key cipher algorithms. Eight popular strong encryption algorithms are examined in detail. Analysis reveals the algorithms are computationally complex and contain little parallelism. Overall throughput on a high-end microprocessor is quite poor, a 600 Mhz processor is incapable of saturating a T3 communication line with 3DES (triple DES) encrypted data.We introduce new instructions that improve the efficiency of the analyzed algorithms. Our approach adds instruction set support for fast substitutions, general permutations, rotates, and modular arithmetic. Performance analysis of the optimized ciphers shows an overall speedup of 59\% over a baseline machine with rotate instructions and 74\% speedup over a baseline without rotates. Even higher speedups are demonstrated with optimized substitutions (SBOXes) and additional functional unit resources. Our analyses of the original and optimized algorithms suggest future directions for the design of high-performance programmable cryptographic processors.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {178--189},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379238},
 doi = {http://doi.acm.org/10.1145/378995.379238},
 acmid = {379238},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kubiatowicz:2000:OAG:384264.379239,
 author = {Kubiatowicz, John and Bindel, David and Chen, Yan and Czerwinski, Steven and Eaton, Patrick and Geels, Dennis and Gummadi, Ramakrishna and Rhea, Sean and Weatherspoon, Hakim and Wells, Chris and Zhao, Ben},
 title = {OceanStore: an architecture for global-scale persistent storage},
 abstract = {OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {190--201},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379239},
 doi = {http://doi.acm.org/10.1145/384264.379239},
 acmid = {379239},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kubiatowicz:2000:OAG:378993.379239,
 author = {Kubiatowicz, John and Bindel, David and Chen, Yan and Czerwinski, Steven and Eaton, Patrick and Geels, Dennis and Gummadi, Ramakrishna and Rhea, Sean and Weatherspoon, Hakim and Wells, Chris and Zhao, Ben},
 title = {OceanStore: an architecture for global-scale persistent storage},
 abstract = {OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {190--201},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379239},
 doi = {http://doi.acm.org/10.1145/378993.379239},
 acmid = {379239},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kubiatowicz:2000:OAG:378995.379239,
 author = {Kubiatowicz, John and Bindel, David and Chen, Yan and Czerwinski, Steven and Eaton, Patrick and Geels, Dennis and Gummadi, Ramakrishna and Rhea, Sean and Weatherspoon, Hakim and Wells, Chris and Zhao, Ben},
 title = {OceanStore: an architecture for global-scale persistent storage},
 abstract = {OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {190--201},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379239},
 doi = {http://doi.acm.org/10.1145/378995.379239},
 acmid = {379239},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Duesterwald:2000:SPH:384264.379241,
 author = {Duesterwald, Evelyn and Bala, Vasanth},
 title = {Software profiling for hot path prediction: less is more},
 abstract = {Recently, there has been a growing interest in exploiting profile information in adaptive systems such as just-in-time compilers, dynamic optimizers and, binary translators. In this paper, we show that sophisticated software profiling schemes that provide highly accurate information in an offline setting are ill-suited for these dynamic code generation systems. We experimentally demonstrate that hot path predictions must be made early in order to control the rising cost of missed opportunity that result from the prediction delay. We also show that existing sophisticated path profiling schemes, if used in an online setting, offer no prediction advantages over simpler schemes that exhibit much lower runtime overheads.Based on these observation we developed a new low-overhead software profiling scheme for hot path prediction. Using an abstract metric we compare our scheme to path profile based prediction and show that our scheme achieves comparable prediction quality. In our second set of experiments we include runtime overhead and evaluate the performance of our scheme in a realistic application: Dynamo, a dynamic optimization system. The results show that our prediction scheme clearly outperforms path profile based prediction and thus confirm that less</i> profiling as exhibited in our scheme will actually lead to more</i> effective hot path prediction.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {202--211},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384264.379241},
 doi = {http://doi.acm.org/10.1145/384264.379241},
 acmid = {379241},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Duesterwald:2000:SPH:378993.379241,
 author = {Duesterwald, Evelyn and Bala, Vasanth},
 title = {Software profiling for hot path prediction: less is more},
 abstract = {Recently, there has been a growing interest in exploiting profile information in adaptive systems such as just-in-time compilers, dynamic optimizers and, binary translators. In this paper, we show that sophisticated software profiling schemes that provide highly accurate information in an offline setting are ill-suited for these dynamic code generation systems. We experimentally demonstrate that hot path predictions must be made early in order to control the rising cost of missed opportunity that result from the prediction delay. We also show that existing sophisticated path profiling schemes, if used in an online setting, offer no prediction advantages over simpler schemes that exhibit much lower runtime overheads.Based on these observation we developed a new low-overhead software profiling scheme for hot path prediction. Using an abstract metric we compare our scheme to path profile based prediction and show that our scheme achieves comparable prediction quality. In our second set of experiments we include runtime overhead and evaluate the performance of our scheme in a realistic application: Dynamo, a dynamic optimization system. The results show that our prediction scheme clearly outperforms path profile based prediction and thus confirm that less</i> profiling as exhibited in our scheme will actually lead to more</i> effective hot path prediction.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {202--211},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378993.379241},
 doi = {http://doi.acm.org/10.1145/378993.379241},
 acmid = {379241},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Duesterwald:2000:SPH:378995.379241,
 author = {Duesterwald, Evelyn and Bala, Vasanth},
 title = {Software profiling for hot path prediction: less is more},
 abstract = {Recently, there has been a growing interest in exploiting profile information in adaptive systems such as just-in-time compilers, dynamic optimizers and, binary translators. In this paper, we show that sophisticated software profiling schemes that provide highly accurate information in an offline setting are ill-suited for these dynamic code generation systems. We experimentally demonstrate that hot path predictions must be made early in order to control the rising cost of missed opportunity that result from the prediction delay. We also show that existing sophisticated path profiling schemes, if used in an online setting, offer no prediction advantages over simpler schemes that exhibit much lower runtime overheads.Based on these observation we developed a new low-overhead software profiling scheme for hot path prediction. Using an abstract metric we compare our scheme to path profile based prediction and show that our scheme achieves comparable prediction quality. In our second set of experiments we include runtime overhead and evaluate the performance of our scheme in a realistic application: Dynamo, a dynamic optimization system. The results show that our prediction scheme clearly outperforms path profile based prediction and thus confirm that less</i> profiling as exhibited in our scheme will actually lead to more</i> effective hot path prediction.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {202--211},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378995.379241},
 doi = {http://doi.acm.org/10.1145/378995.379241},
 acmid = {379241},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahir:2000:OCC:378995.379242,
 author = {Zahir, Rumi and Ross, Jonathan and Morris, Dale and Hess, Drew},
 title = {OS and compiler considerations in the design of the IA-64 architecture},
 abstract = {Increasing demands for processor performance have outstripped the pace of process and frequency improvements, pushing designers to find ways of increasing the amount of work that can be processed in parallel. Traditional RISC architectures use hardware approaches to obtain more instruction-level parallelism, with the compiler and the operating system (OS) having only indirect visibility into the mechanisms used.The IA-64 architecture [14] was specifically designed to enable systems which create and exploit high levels of instruction-level parallelism by explicitly encoding a program's parallelism in the instruction set [25]. This paper provides a qualitative summary of the IA-64 architecture features that support control and data speculation, and register stacking. The paper focusses on the functional synergy between these architectural elements (rather than their individual performance merits), and emphasizes how they were designed for cooperation between processor hardware, compilers and the OS.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378995.379242},
 doi = {http://doi.acm.org/10.1145/378995.379242},
 acmid = {379242},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zahir:2000:OCC:378993.379242,
 author = {Zahir, Rumi and Ross, Jonathan and Morris, Dale and Hess, Drew},
 title = {OS and compiler considerations in the design of the IA-64 architecture},
 abstract = {Increasing demands for processor performance have outstripped the pace of process and frequency improvements, pushing designers to find ways of increasing the amount of work that can be processed in parallel. Traditional RISC architectures use hardware approaches to obtain more instruction-level parallelism, with the compiler and the operating system (OS) having only indirect visibility into the mechanisms used.The IA-64 architecture [14] was specifically designed to enable systems which create and exploit high levels of instruction-level parallelism by explicitly encoding a program's parallelism in the instruction set [25]. This paper provides a qualitative summary of the IA-64 architecture features that support control and data speculation, and register stacking. The paper focusses on the functional synergy between these architectural elements (rather than their individual performance merits), and emphasizes how they were designed for cooperation between processor hardware, compilers and the OS.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378993.379242},
 doi = {http://doi.acm.org/10.1145/378993.379242},
 acmid = {379242},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahir:2000:OCC:384264.379242,
 author = {Zahir, Rumi and Ross, Jonathan and Morris, Dale and Hess, Drew},
 title = {OS and compiler considerations in the design of the IA-64 architecture},
 abstract = {Increasing demands for processor performance have outstripped the pace of process and frequency improvements, pushing designers to find ways of increasing the amount of work that can be processed in parallel. Traditional RISC architectures use hardware approaches to obtain more instruction-level parallelism, with the compiler and the operating system (OS) having only indirect visibility into the mechanisms used.The IA-64 architecture [14] was specifically designed to enable systems which create and exploit high levels of instruction-level parallelism by explicitly encoding a program's parallelism in the instruction set [25]. This paper provides a qualitative summary of the IA-64 architecture features that support control and data speculation, and register stacking. The paper focusses on the functional synergy between these architectural elements (rather than their individual performance merits), and emphasizes how they were designed for cooperation between processor hardware, compilers and the OS.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384264.379242},
 doi = {http://doi.acm.org/10.1145/384264.379242},
 acmid = {379242},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Connors:2000:HSD:378995.379243,
 author = {Connors, Daniel A. and Hunter, Hillery C. and Cheng, Ben-Chung and Hwu, Wen-mei W.},
 title = {Hardware support for dynamic activation of compiler-directed computation reuse},
 abstract = {Compiler-directed Computation Reuse (CCR) enhances program execution speed and efficiency by eliminating dynamic computation redundancy. In this approach, the compiler designates large program regions for potential reuse. During run time, the execution results of these reusable regions are recorded into hardware buffers for future reuse. Previous work shows that CCR can result in significant performance enhancements in general applications. A major limitation of the work is that the compiler relies on value profiling to identify reusable regions, making it difficult to deploy the scheme in many software production environments. This paper presents a new hardware model that alleviates the need for value profiling at compile time. The compiler is allowed to designate reusable regions that may prove to be inappropriate. The hardware mechanism monitors the dynamic behavior of compiler-designated regions and selectively activates the profitable ones at run time. Experimental results show that the proposed design makes more effective utilization of hardware buffer resources, achieves rapid employment of computation regions, and improves reuse accuracy, all of which promote more flexible compiler methods of identifying reusable computation regions.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379243},
 doi = {http://doi.acm.org/10.1145/378995.379243},
 acmid = {379243},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Connors:2000:HSD:384264.379243,
 author = {Connors, Daniel A. and Hunter, Hillery C. and Cheng, Ben-Chung and Hwu, Wen-mei W.},
 title = {Hardware support for dynamic activation of compiler-directed computation reuse},
 abstract = {Compiler-directed Computation Reuse (CCR) enhances program execution speed and efficiency by eliminating dynamic computation redundancy. In this approach, the compiler designates large program regions for potential reuse. During run time, the execution results of these reusable regions are recorded into hardware buffers for future reuse. Previous work shows that CCR can result in significant performance enhancements in general applications. A major limitation of the work is that the compiler relies on value profiling to identify reusable regions, making it difficult to deploy the scheme in many software production environments. This paper presents a new hardware model that alleviates the need for value profiling at compile time. The compiler is allowed to designate reusable regions that may prove to be inappropriate. The hardware mechanism monitors the dynamic behavior of compiler-designated regions and selectively activates the profitable ones at run time. Experimental results show that the proposed design makes more effective utilization of hardware buffer resources, achieves rapid employment of computation regions, and improves reuse accuracy, all of which promote more flexible compiler methods of identifying reusable computation regions.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379243},
 doi = {http://doi.acm.org/10.1145/384264.379243},
 acmid = {379243},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Connors:2000:HSD:378993.379243,
 author = {Connors, Daniel A. and Hunter, Hillery C. and Cheng, Ben-Chung and Hwu, Wen-mei W.},
 title = {Hardware support for dynamic activation of compiler-directed computation reuse},
 abstract = {Compiler-directed Computation Reuse (CCR) enhances program execution speed and efficiency by eliminating dynamic computation redundancy. In this approach, the compiler designates large program regions for potential reuse. During run time, the execution results of these reusable regions are recorded into hardware buffers for future reuse. Previous work shows that CCR can result in significant performance enhancements in general applications. A major limitation of the work is that the compiler relies on value profiling to identify reusable regions, making it difficult to deploy the scheme in many software production environments. This paper presents a new hardware model that alleviates the need for value profiling at compile time. The compiler is allowed to designate reusable regions that may prove to be inappropriate. The hardware mechanism monitors the dynamic behavior of compiler-designated regions and selectively activates the profitable ones at run time. Experimental results show that the proposed design makes more effective utilization of hardware buffer resources, achieves rapid employment of computation regions, and improves reuse accuracy, all of which promote more flexible compiler methods of identifying reusable computation regions.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379243},
 doi = {http://doi.acm.org/10.1145/378993.379243},
 acmid = {379243},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Snavely:2000:SJS:378995.379244,
 author = {Snavely, Allan and Tullsen, Dean M.},
 title = {Symbiotic jobscheduling for a simultaneous multithreaded processor},
 abstract = {Simultaneous Multithreading machines fetch and execute instructions from multiple instruction streams to increase system utilization and speedup the execution of jobs. When there are more jobs in the system than there is hardware to support simultaneous execution, the operating system scheduler must choose the set of jobs to coscheduleThis paper demonstrates that performance on a hardware multithreaded processor is sensitive to the set of jobs that are coscheduled by the operating system jobscheduler. Thus, the full benefits of SMT hardware can only be achieved if the scheduler is aware of thread interactions. Here, a mechanism is presented that allows the scheduler to significantly raise the performance of SMT architectures. This is done without any advance knowledge of a workload's characteristics, using sampling to identify jobs which run well together.We demonstrate an SMT jobscheduler called SOS. SOS combines an overhead-free sample phase which collects information about various possible schedules, and a symbiosis phase which uses that information to predict which schedule will provide the best performance. We show that a small sample of the possible schedules is sufficient to identify a good schedule quickly. On a system with random job arrivals and departures, response time is improved as much as 17\% over a schedule which does not incorporate symbiosis.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {234--244},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378995.379244},
 doi = {http://doi.acm.org/10.1145/378995.379244},
 acmid = {379244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Snavely:2000:SJS:384264.379244,
 author = {Snavely, Allan and Tullsen, Dean M.},
 title = {Symbiotic jobscheduling for a simultaneous multithreaded processor},
 abstract = {Simultaneous Multithreading machines fetch and execute instructions from multiple instruction streams to increase system utilization and speedup the execution of jobs. When there are more jobs in the system than there is hardware to support simultaneous execution, the operating system scheduler must choose the set of jobs to coscheduleThis paper demonstrates that performance on a hardware multithreaded processor is sensitive to the set of jobs that are coscheduled by the operating system jobscheduler. Thus, the full benefits of SMT hardware can only be achieved if the scheduler is aware of thread interactions. Here, a mechanism is presented that allows the scheduler to significantly raise the performance of SMT architectures. This is done without any advance knowledge of a workload's characteristics, using sampling to identify jobs which run well together.We demonstrate an SMT jobscheduler called SOS. SOS combines an overhead-free sample phase which collects information about various possible schedules, and a symbiosis phase which uses that information to predict which schedule will provide the best performance. We show that a small sample of the possible schedules is sufficient to identify a good schedule quickly. On a system with random job arrivals and departures, response time is improved as much as 17\% over a schedule which does not incorporate symbiosis.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {234--244},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384264.379244},
 doi = {http://doi.acm.org/10.1145/384264.379244},
 acmid = {379244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Snavely:2000:SJS:378993.379244,
 author = {Snavely, Allan and Tullsen, Dean M.},
 title = {Symbiotic jobscheduling for a simultaneous multithreaded processor},
 abstract = {Simultaneous Multithreading machines fetch and execute instructions from multiple instruction streams to increase system utilization and speedup the execution of jobs. When there are more jobs in the system than there is hardware to support simultaneous execution, the operating system scheduler must choose the set of jobs to coscheduleThis paper demonstrates that performance on a hardware multithreaded processor is sensitive to the set of jobs that are coscheduled by the operating system jobscheduler. Thus, the full benefits of SMT hardware can only be achieved if the scheduler is aware of thread interactions. Here, a mechanism is presented that allows the scheduler to significantly raise the performance of SMT architectures. This is done without any advance knowledge of a workload's characteristics, using sampling to identify jobs which run well together.We demonstrate an SMT jobscheduler called SOS. SOS combines an overhead-free sample phase which collects information about various possible schedules, and a symbiosis phase which uses that information to predict which schedule will provide the best performance. We show that a small sample of the possible schedules is sufficient to identify a good schedule quickly. On a system with random job arrivals and departures, response time is improved as much as 17\% over a schedule which does not incorporate symbiosis.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {234--244},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378993.379244},
 doi = {http://doi.acm.org/10.1145/378993.379244},
 acmid = {379244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Redstone:2000:AOS:378995.379245,
 author = {Redstone, Joshua A. and Eggers, Susan J. and Levy, Henry M.},
 title = {An analysis of operating system behavior on a simultaneous multithreaded architecture},
 abstract = {This paper presents the first analysis of operating system execution on a simultaneous multithreaded (SMT) processor. While SMT has been studied extensively over the past 6 years, previous research has focused entirely on user-mode execution. However, many of the applications most amenable to multithreading technologies spend a significant fraction of their time in kernel code. A full understanding of the behavior of such workloads therefore requires execution and measurement of the operating system, as well as the application itself.To carry out this study, we (1) modified the Digital Unix 4.0d operating system to run on an SMT CPU, and (2) integrated our SMT Alpha instruction set simulator into the SimOS simulator to provide an execution environment. For an OS-intensive workload, we ran the multithreaded Apache Web server on an 8-context SMT. We compared Apache's user- and kernel-mode behavior to a standard multiprogrammed SPECInt workload, and compared the SMT processor to an out-of-order superscalar running both workloads. Overall, our results demonstrate the microarchitectural impact of an OS-intensive workload on an SMT processor and provide insight into the OS demands of the Apache Web server. The synergy between the SMT processor and Web and OS software produced a greater throughput gain over superscalar execution than seen on any previously examined workloads, including commercial databases and explicitly parallel programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {245--256},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379245},
 doi = {http://doi.acm.org/10.1145/378995.379245},
 acmid = {379245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Redstone:2000:AOS:384264.379245,
 author = {Redstone, Joshua A. and Eggers, Susan J. and Levy, Henry M.},
 title = {An analysis of operating system behavior on a simultaneous multithreaded architecture},
 abstract = {This paper presents the first analysis of operating system execution on a simultaneous multithreaded (SMT) processor. While SMT has been studied extensively over the past 6 years, previous research has focused entirely on user-mode execution. However, many of the applications most amenable to multithreading technologies spend a significant fraction of their time in kernel code. A full understanding of the behavior of such workloads therefore requires execution and measurement of the operating system, as well as the application itself.To carry out this study, we (1) modified the Digital Unix 4.0d operating system to run on an SMT CPU, and (2) integrated our SMT Alpha instruction set simulator into the SimOS simulator to provide an execution environment. For an OS-intensive workload, we ran the multithreaded Apache Web server on an 8-context SMT. We compared Apache's user- and kernel-mode behavior to a standard multiprogrammed SPECInt workload, and compared the SMT processor to an out-of-order superscalar running both workloads. Overall, our results demonstrate the microarchitectural impact of an OS-intensive workload on an SMT processor and provide insight into the OS demands of the Apache Web server. The synergy between the SMT processor and Web and OS software produced a greater throughput gain over superscalar execution than seen on any previously examined workloads, including commercial databases and explicitly parallel programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {245--256},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379245},
 doi = {http://doi.acm.org/10.1145/384264.379245},
 acmid = {379245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Redstone:2000:AOS:378993.379245,
 author = {Redstone, Joshua A. and Eggers, Susan J. and Levy, Henry M.},
 title = {An analysis of operating system behavior on a simultaneous multithreaded architecture},
 abstract = {This paper presents the first analysis of operating system execution on a simultaneous multithreaded (SMT) processor. While SMT has been studied extensively over the past 6 years, previous research has focused entirely on user-mode execution. However, many of the applications most amenable to multithreading technologies spend a significant fraction of their time in kernel code. A full understanding of the behavior of such workloads therefore requires execution and measurement of the operating system, as well as the application itself.To carry out this study, we (1) modified the Digital Unix 4.0d operating system to run on an SMT CPU, and (2) integrated our SMT Alpha instruction set simulator into the SimOS simulator to provide an execution environment. For an OS-intensive workload, we ran the multithreaded Apache Web server on an 8-context SMT. We compared Apache's user- and kernel-mode behavior to a standard multiprogrammed SPECInt workload, and compared the SMT processor to an out-of-order superscalar running both workloads. Overall, our results demonstrate the microarchitectural impact of an OS-intensive workload on an SMT processor and provide insight into the OS demands of the Apache Web server. The synergy between the SMT processor and Web and OS software produced a greater throughput gain over superscalar execution than seen on any previously examined workloads, including commercial databases and explicitly parallel programs.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {245--256},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379245},
 doi = {http://doi.acm.org/10.1145/378993.379245},
 acmid = {379245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sundaramoorthy:2000:SPI:378993.379247,
 author = {Sundaramoorthy, Karthik and Purser, Zach and Rotenburg, Eric},
 title = {Slipstream processors: improving both performance and fault tolerance},
 abstract = {Processors execute the full dynamic instruction stream to arrive at the final output of a program, yet there exist shorter instruction streams that produce the same overall effect. We propose creating a shorter but otherwise equivalent version of the original program by removing ineffectual computation and computation related to highly-predictable control flow. The shortened program is run concurrently with the full program on a chip multiprocessor simultaneous multithreaded processor, with two key advantages:1) Improved single-program performance.</i> The shorter program speculatively runs ahead of the full program and supplies the full program with control and data flow outcomes. The full program executes efficiently due to the communicated outcomes, at the same time validating the speculative, shorter program. The two programs combined run faster than the original program alone. Detailed simulations of an example implementation show an average improvement of 7\% for the SPEC95 integer benchmarks.2) Fault tolerance.</i> The shorter program is a subset of the full program and this partial-redundancy is transparently leveraged for detecting and recovering from transient hardware faults.},
 booktitle = {Proceedings of the ninth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IX},
 year = {2000},
 isbn = {1-58113-317-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {257--268},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378993.379247},
 doi = {http://doi.acm.org/10.1145/378993.379247},
 acmid = {379247},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sundaramoorthy:2000:SPI:378995.379247,
 author = {Sundaramoorthy, Karthik and Purser, Zach and Rotenburg, Eric},
 title = {Slipstream processors: improving both performance and fault tolerance},
 abstract = {Processors execute the full dynamic instruction stream to arrive at the final output of a program, yet there exist shorter instruction streams that produce the same overall effect. We propose creating a shorter but otherwise equivalent version of the original program by removing ineffectual computation and computation related to highly-predictable control flow. The shortened program is run concurrently with the full program on a chip multiprocessor simultaneous multithreaded processor, with two key advantages:1) Improved single-program performance.</i> The shorter program speculatively runs ahead of the full program and supplies the full program with control and data flow outcomes. The full program executes efficiently due to the communicated outcomes, at the same time validating the speculative, shorter program. The two programs combined run faster than the original program alone. Detailed simulations of an example implementation show an average improvement of 7\% for the SPEC95 integer benchmarks.2) Fault tolerance.</i> The shorter program is a subset of the full program and this partial-redundancy is transparently leveraged for detecting and recovering from transient hardware faults.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {28},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5964},
 pages = {257--268},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378995.379247},
 doi = {http://doi.acm.org/10.1145/378995.379247},
 acmid = {379247},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sundaramoorthy:2000:SPI:384264.379247,
 author = {Sundaramoorthy, Karthik and Purser, Zach and Rotenburg, Eric},
 title = {Slipstream processors: improving both performance and fault tolerance},
 abstract = {Processors execute the full dynamic instruction stream to arrive at the final output of a program, yet there exist shorter instruction streams that produce the same overall effect. We propose creating a shorter but otherwise equivalent version of the original program by removing ineffectual computation and computation related to highly-predictable control flow. The shortened program is run concurrently with the full program on a chip multiprocessor simultaneous multithreaded processor, with two key advantages:1) Improved single-program performance.</i> The shorter program speculatively runs ahead of the full program and supplies the full program with control and data flow outcomes. The full program executes efficiently due to the communicated outcomes, at the same time validating the speculative, shorter program. The two programs combined run faster than the original program alone. Detailed simulations of an example implementation show an average improvement of 7\% for the SPEC95 integer benchmarks.2) Fault tolerance.</i> The shorter program is a subset of the full program and this partial-redundancy is transparently leveraged for detecting and recovering from transient hardware faults.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {34},
 issue = {5},
 month = {November},
 year = {2000},
 issn = {0163-5980},
 pages = {257--268},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384264.379247},
 doi = {http://doi.acm.org/10.1145/384264.379247},
 acmid = {379247},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooper:1998:CM:291006.291010,
 author = {Cooper, Keith D. and Harvey, Timothy J.},
 title = {Compiler-controlled memory},
 abstract = {Optimizations aimed at reducing the impact of memory operations on execution speed have long concentrated on improving cache performance. These efforts achieve a. reasonable level of success. The primary limit on the compiler's ability to improve memory behavior is its imperfect knowledge about the run-time behavior of the program. The compiler cannot completely predict runtime access patterns.There is an exception to this rule. During the register allocation phase, the compiler often must insert substantial amounts of spill code</i>; that is, instructions that move values from registers to memory and back again. Because the compiler itself inserts these memory instructions, it has more knowledge about them than other memory operations in the program.Spill-code operations are disjoint from the memory manipulations required by the semantics of the program being compiled, and, indeed, the two can interfere in the cache. This paper proposes a hardware solution to the problem of increased spill costs---a small compiler-controlled memory</i> (CCM) to hold spilled values. This small random-access memory can (and should) be placed in a distinct address space from the main memory hierarchy. The compiler can target spill instructions to use the CCM, moving most compiler-inserted memory traffic out of the pathway to main memory and eliminating any impact that those spill instructions would have on the state of the main memory hierarchy. Such memories already exist on some DSP microprocessors. Our techniques can be applied directly on those chips.This paper presents two compiler-based methods to exploit such a memory, along with experimental results showing that speedups from using CCM may be sizable. It shows that using the register allocation's coloring paradigm to assign spilled values to memory can greatly reduce the amount of memory required by a program.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291010},
 doi = {http://doi.acm.org/10.1145/291006.291010},
 acmid = {291010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cooper:1998:CM:291069.291010,
 author = {Cooper, Keith D. and Harvey, Timothy J.},
 title = {Compiler-controlled memory},
 abstract = {Optimizations aimed at reducing the impact of memory operations on execution speed have long concentrated on improving cache performance. These efforts achieve a. reasonable level of success. The primary limit on the compiler's ability to improve memory behavior is its imperfect knowledge about the run-time behavior of the program. The compiler cannot completely predict runtime access patterns.There is an exception to this rule. During the register allocation phase, the compiler often must insert substantial amounts of spill code</i>; that is, instructions that move values from registers to memory and back again. Because the compiler itself inserts these memory instructions, it has more knowledge about them than other memory operations in the program.Spill-code operations are disjoint from the memory manipulations required by the semantics of the program being compiled, and, indeed, the two can interfere in the cache. This paper proposes a hardware solution to the problem of increased spill costs---a small compiler-controlled memory</i> (CCM) to hold spilled values. This small random-access memory can (and should) be placed in a distinct address space from the main memory hierarchy. The compiler can target spill instructions to use the CCM, moving most compiler-inserted memory traffic out of the pathway to main memory and eliminating any impact that those spill instructions would have on the state of the main memory hierarchy. Such memories already exist on some DSP microprocessors. Our techniques can be applied directly on those chips.This paper presents two compiler-based methods to exploit such a memory, along with experimental results showing that speedups from using CCM may be sizable. It shows that using the register allocation's coloring paradigm to assign spilled values to memory can greatly reduce the amount of memory required by a program.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291010},
 doi = {http://doi.acm.org/10.1145/291069.291010},
 acmid = {291010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cooper:1998:CM:384265.291010,
 author = {Cooper, Keith D. and Harvey, Timothy J.},
 title = {Compiler-controlled memory},
 abstract = {Optimizations aimed at reducing the impact of memory operations on execution speed have long concentrated on improving cache performance. These efforts achieve a. reasonable level of success. The primary limit on the compiler's ability to improve memory behavior is its imperfect knowledge about the run-time behavior of the program. The compiler cannot completely predict runtime access patterns.There is an exception to this rule. During the register allocation phase, the compiler often must insert substantial amounts of spill code</i>; that is, instructions that move values from registers to memory and back again. Because the compiler itself inserts these memory instructions, it has more knowledge about them than other memory operations in the program.Spill-code operations are disjoint from the memory manipulations required by the semantics of the program being compiled, and, indeed, the two can interfere in the cache. This paper proposes a hardware solution to the problem of increased spill costs---a small compiler-controlled memory</i> (CCM) to hold spilled values. This small random-access memory can (and should) be placed in a distinct address space from the main memory hierarchy. The compiler can target spill instructions to use the CCM, moving most compiler-inserted memory traffic out of the pathway to main memory and eliminating any impact that those spill instructions would have on the state of the main memory hierarchy. Such memories already exist on some DSP microprocessors. Our techniques can be applied directly on those chips.This paper presents two compiler-based methods to exploit such a memory, along with experimental results showing that speedups from using CCM may be sizable. It shows that using the register allocation's coloring paradigm to assign spilled values to memory can greatly reduce the amount of memory required by a program.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291010},
 doi = {http://doi.acm.org/10.1145/384265.291010},
 acmid = {291010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Seidl:1998:SHO:384265.291012,
 author = {Seidl, Matthew L. and Zorn, Benjamin G.},
 title = {Segregating heap objects by reference behavior and lifetime},
 abstract = {Dynamic storage allocation has become increasingly important in many applications, in part due to the use of the object-oriented paradigm. At the same time, processor speeds are increasing faster than memory speeds and programs are increasing in size faster than memories. In this paper, we investigate efforts to predict heap object reference and lifetime behavior at the time objects are allocated. Our approach uses profile-based optimization, and considers a variety of different information sources present at the time of object allocation to predict the object's reference frequency and lifetime. Our results, based on measurements of six allocation intensive programs, show that program references to heap objects are highly predictable and that our prediction methods can successfully predict the behavior of these heap objects. We show that our methods can decrease the page fault rate of the programs measured, sometimes dramatically, in cases where the physical memory available to the program is constrained.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {12--23},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291012},
 doi = {http://doi.acm.org/10.1145/384265.291012},
 acmid = {291012},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Seidl:1998:SHO:291069.291012,
 author = {Seidl, Matthew L. and Zorn, Benjamin G.},
 title = {Segregating heap objects by reference behavior and lifetime},
 abstract = {Dynamic storage allocation has become increasingly important in many applications, in part due to the use of the object-oriented paradigm. At the same time, processor speeds are increasing faster than memory speeds and programs are increasing in size faster than memories. In this paper, we investigate efforts to predict heap object reference and lifetime behavior at the time objects are allocated. Our approach uses profile-based optimization, and considers a variety of different information sources present at the time of object allocation to predict the object's reference frequency and lifetime. Our results, based on measurements of six allocation intensive programs, show that program references to heap objects are highly predictable and that our prediction methods can successfully predict the behavior of these heap objects. We show that our methods can decrease the page fault rate of the programs measured, sometimes dramatically, in cases where the physical memory available to the program is constrained.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {12--23},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291012},
 doi = {http://doi.acm.org/10.1145/291069.291012},
 acmid = {291012},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Seidl:1998:SHO:291006.291012,
 author = {Seidl, Matthew L. and Zorn, Benjamin G.},
 title = {Segregating heap objects by reference behavior and lifetime},
 abstract = {Dynamic storage allocation has become increasingly important in many applications, in part due to the use of the object-oriented paradigm. At the same time, processor speeds are increasing faster than memory speeds and programs are increasing in size faster than memories. In this paper, we investigate efforts to predict heap object reference and lifetime behavior at the time objects are allocated. Our approach uses profile-based optimization, and considers a variety of different information sources present at the time of object allocation to predict the object's reference frequency and lifetime. Our results, based on measurements of six allocation intensive programs, show that program references to heap objects are highly predictable and that our prediction methods can successfully predict the behavior of these heap objects. We show that our methods can decrease the page fault rate of the programs measured, sometimes dramatically, in cases where the physical memory available to the program is constrained.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {12--23},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291012},
 doi = {http://doi.acm.org/10.1145/291006.291012},
 acmid = {291012},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Strout:1998:SSM:291069.291015,
 author = {Strout, Michelle Mills and Carter, Larry and Ferrante, Jeanne and Simon, Beth},
 title = {Schedule-independent storage mapping for loops},
 abstract = {This paper studies the relationship between storage requirements and performance. Storage-related dependences inhibit optimizations for locality and parallelism. Techniques such as renaming and array expansion can eliminate all storage-related dependences, but do so at the expense of increased storage. This paper introduces the universal occupancy vector</i> (UOV) for loops with a regular stencil of dependences. The UOV provides a schedule-independent storage reuse pattern that introduces no further dependences (other than those implied by true flow dependences). OV-mapped code requires less storage than full array expansion and only slightly more storage than schedule-dependent minimal storage.We show that determine if a vector is a UOV is NPcomplete. However, an easily constructed but possibly nonminimal UOV can be used. We also present a branch and bound algorithm which finds the minimal UOV, while still maintaining a legal UOV at all times.Our experimental results show that the use of OV-mapped storage, coupled with tiling for locality, achieves better performance than tiling after array expansion, and accommodates larger problem sizes than untilable, storage-optimized code. F'urthermore, storage mapping based on the UOV introduces negligible runtime overhead.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {24--33},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291015},
 doi = {http://doi.acm.org/10.1145/291069.291015},
 acmid = {291015},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Strout:1998:SSM:291006.291015,
 author = {Strout, Michelle Mills and Carter, Larry and Ferrante, Jeanne and Simon, Beth},
 title = {Schedule-independent storage mapping for loops},
 abstract = {This paper studies the relationship between storage requirements and performance. Storage-related dependences inhibit optimizations for locality and parallelism. Techniques such as renaming and array expansion can eliminate all storage-related dependences, but do so at the expense of increased storage. This paper introduces the universal occupancy vector</i> (UOV) for loops with a regular stencil of dependences. The UOV provides a schedule-independent storage reuse pattern that introduces no further dependences (other than those implied by true flow dependences). OV-mapped code requires less storage than full array expansion and only slightly more storage than schedule-dependent minimal storage.We show that determine if a vector is a UOV is NPcomplete. However, an easily constructed but possibly nonminimal UOV can be used. We also present a branch and bound algorithm which finds the minimal UOV, while still maintaining a legal UOV at all times.Our experimental results show that the use of OV-mapped storage, coupled with tiling for locality, achieves better performance than tiling after array expansion, and accommodates larger problem sizes than untilable, storage-optimized code. F'urthermore, storage mapping based on the UOV introduces negligible runtime overhead.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {24--33},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291015},
 doi = {http://doi.acm.org/10.1145/291006.291015},
 acmid = {291015},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Strout:1998:SSM:384265.291015,
 author = {Strout, Michelle Mills and Carter, Larry and Ferrante, Jeanne and Simon, Beth},
 title = {Schedule-independent storage mapping for loops},
 abstract = {This paper studies the relationship between storage requirements and performance. Storage-related dependences inhibit optimizations for locality and parallelism. Techniques such as renaming and array expansion can eliminate all storage-related dependences, but do so at the expense of increased storage. This paper introduces the universal occupancy vector</i> (UOV) for loops with a regular stencil of dependences. The UOV provides a schedule-independent storage reuse pattern that introduces no further dependences (other than those implied by true flow dependences). OV-mapped code requires less storage than full array expansion and only slightly more storage than schedule-dependent minimal storage.We show that determine if a vector is a UOV is NPcomplete. However, an easily constructed but possibly nonminimal UOV can be used. We also present a branch and bound algorithm which finds the minimal UOV, while still maintaining a legal UOV at all times.Our experimental results show that the use of OV-mapped storage, coupled with tiling for locality, achieves better performance than tiling after array expansion, and accommodates larger problem sizes than untilable, storage-optimized code. F'urthermore, storage mapping based on the UOV introduces negligible runtime overhead.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {24--33},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291015},
 doi = {http://doi.acm.org/10.1145/384265.291015},
 acmid = {291015},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sodani:1998:EAI:291006.291016,
 author = {Sodani, Avinash and Sohi, Gurindar S.},
 title = {An empirical analysis of instruction repetition},
 abstract = {We study the phenomenon of instruction repetition, where the inputs and outputs of multiple dynamic instances of a static instruction are repeated. We observe that over 80\% of the dynamic instructions executed in several programs are repeated and most of the repetition is due to a small number of static instructions. We attempt understand the source of this repetitive behavior by categorizing dynamic program instructions into dynamic program slices at both a global level and a local (within function) level. We observe that repeatability is more an artifact of how computation is expressed, and less of program inputs. Function-level analysis suggests that many functions are called with repeated arguments, though almost all of them have side effects. We provide commentary on exploiting the observed phenomenon and its sources in both software and hardware.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {35--45},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291006.291016},
 doi = {http://doi.acm.org/10.1145/291006.291016},
 acmid = {291016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sodani:1998:EAI:291069.291016,
 author = {Sodani, Avinash and Sohi, Gurindar S.},
 title = {An empirical analysis of instruction repetition},
 abstract = {We study the phenomenon of instruction repetition, where the inputs and outputs of multiple dynamic instances of a static instruction are repeated. We observe that over 80\% of the dynamic instructions executed in several programs are repeated and most of the repetition is due to a small number of static instructions. We attempt understand the source of this repetitive behavior by categorizing dynamic program instructions into dynamic program slices at both a global level and a local (within function) level. We observe that repeatability is more an artifact of how computation is expressed, and less of program inputs. Function-level analysis suggests that many functions are called with repeated arguments, though almost all of them have side effects. We provide commentary on exploiting the observed phenomenon and its sources in both software and hardware.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {35--45},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291069.291016},
 doi = {http://doi.acm.org/10.1145/291069.291016},
 acmid = {291016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sodani:1998:EAI:384265.291016,
 author = {Sodani, Avinash and Sohi, Gurindar S.},
 title = {An empirical analysis of instruction repetition},
 abstract = {We study the phenomenon of instruction repetition, where the inputs and outputs of multiple dynamic instances of a static instruction are repeated. We observe that over 80\% of the dynamic instructions executed in several programs are repeated and most of the repetition is due to a small number of static instructions. We attempt understand the source of this repetitive behavior by categorizing dynamic program instructions into dynamic program slices at both a global level and a local (within function) level. We observe that repeatability is more an artifact of how computation is expressed, and less of program inputs. Function-level analysis suggests that many functions are called with repeated arguments, though almost all of them have side effects. We provide commentary on exploiting the observed phenomenon and its sources in both software and hardware.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {35--45},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384265.291016},
 doi = {http://doi.acm.org/10.1145/384265.291016},
 acmid = {291016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1998:SSI:291069.291018,
 author = {Lee, Walter and Barua, Rajeev and Frank, Matthew and Srikrishna, Devabhaktuni and Babb, Jonathan and Sarkar, Vivek and Amarasinghe, Saman},
 title = {Space-time scheduling of instruction-level parallelism on a raw machine},
 abstract = {Increasing demand for both greater parallelism and faster clocks dictate that future generation architectures will need to decentralize their resources and eliminate primitives that require single cycle global communication. A Raw microprocessor distributes all of its resources, including instruction streams, register files, memory ports, and ALUs, over a pipelined two-dimensional mesh interconnect, and exposes them fully to the compiler. Because communication in Raw machines is distributed, compiling for instruction-level parallelism (ILP) requires both spatial instruction partitioning as well as traditional temporal instruction scheduling. In addition, the compiler must explicitly manage all communication through the interconnect, including the global synchronization required at branch points. This paper describes RAWCC, the compiler we have developed for compiling general-purpose sequential programs to the distributed Raw architecture. We present performance results that demonstrate that although Raw machines provide no mechanisms for global communication the Raw compiler can schedule to achieve speedups that scale with the number of available functional units.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {46--57},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291018},
 doi = {http://doi.acm.org/10.1145/291069.291018},
 acmid = {291018},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1998:SSI:291006.291018,
 author = {Lee, Walter and Barua, Rajeev and Frank, Matthew and Srikrishna, Devabhaktuni and Babb, Jonathan and Sarkar, Vivek and Amarasinghe, Saman},
 title = {Space-time scheduling of instruction-level parallelism on a raw machine},
 abstract = {Increasing demand for both greater parallelism and faster clocks dictate that future generation architectures will need to decentralize their resources and eliminate primitives that require single cycle global communication. A Raw microprocessor distributes all of its resources, including instruction streams, register files, memory ports, and ALUs, over a pipelined two-dimensional mesh interconnect, and exposes them fully to the compiler. Because communication in Raw machines is distributed, compiling for instruction-level parallelism (ILP) requires both spatial instruction partitioning as well as traditional temporal instruction scheduling. In addition, the compiler must explicitly manage all communication through the interconnect, including the global synchronization required at branch points. This paper describes RAWCC, the compiler we have developed for compiling general-purpose sequential programs to the distributed Raw architecture. We present performance results that demonstrate that although Raw machines provide no mechanisms for global communication the Raw compiler can schedule to achieve speedups that scale with the number of available functional units.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {46--57},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291018},
 doi = {http://doi.acm.org/10.1145/291006.291018},
 acmid = {291018},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1998:SSI:384265.291018,
 author = {Lee, Walter and Barua, Rajeev and Frank, Matthew and Srikrishna, Devabhaktuni and Babb, Jonathan and Sarkar, Vivek and Amarasinghe, Saman},
 title = {Space-time scheduling of instruction-level parallelism on a raw machine},
 abstract = {Increasing demand for both greater parallelism and faster clocks dictate that future generation architectures will need to decentralize their resources and eliminate primitives that require single cycle global communication. A Raw microprocessor distributes all of its resources, including instruction streams, register files, memory ports, and ALUs, over a pipelined two-dimensional mesh interconnect, and exposes them fully to the compiler. Because communication in Raw machines is distributed, compiling for instruction-level parallelism (ILP) requires both spatial instruction partitioning as well as traditional temporal instruction scheduling. In addition, the compiler must explicitly manage all communication through the interconnect, including the global synchronization required at branch points. This paper describes RAWCC, the compiler we have developed for compiling general-purpose sequential programs to the distributed Raw architecture. We present performance results that demonstrate that although Raw machines provide no mechanisms for global communication the Raw compiler can schedule to achieve speedups that scale with the number of available functional units.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {46--57},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291018},
 doi = {http://doi.acm.org/10.1145/384265.291018},
 acmid = {291018},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hammond:1998:DSS:291006.291020,
 author = {Hammond, Lance and Willey, Mark and Olukotun, Kunle},
 title = {Data speculation support for a chip multiprocessor},
 abstract = {Thread-level speculation is a technique that enables parallel execution of sequential applications on a multiprocessor. This paper describes the complete implementation of the support for threadlevel speculation on the Hydra chip multiprocessor (CMP). The support consists of a number of software speculation control handlers and modifications to the shared secondary cache memory system of the CMP This support is evaluated using five representative integer applications. Our results show that the speculative support is only able to improve performance when there is a substantial amount of medium--grained loop-level parallelism in the application. When the granularity of parallelism is too small or there is little inherent parallelism in the application, the overhead of the software handlers overwhelms any potential performance benefits from speculative-thread parallelism. Overall, thread-level speculation still appears to be a promising approach for expanding the class of applications that can be automatically parallelized, but more hardware intensive implementations for managing speculation control are required to achieve performance improvements on a wide class of integer applications.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {58--69},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291020},
 doi = {http://doi.acm.org/10.1145/291006.291020},
 acmid = {291020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hammond:1998:DSS:291069.291020,
 author = {Hammond, Lance and Willey, Mark and Olukotun, Kunle},
 title = {Data speculation support for a chip multiprocessor},
 abstract = {Thread-level speculation is a technique that enables parallel execution of sequential applications on a multiprocessor. This paper describes the complete implementation of the support for threadlevel speculation on the Hydra chip multiprocessor (CMP). The support consists of a number of software speculation control handlers and modifications to the shared secondary cache memory system of the CMP This support is evaluated using five representative integer applications. Our results show that the speculative support is only able to improve performance when there is a substantial amount of medium--grained loop-level parallelism in the application. When the granularity of parallelism is too small or there is little inherent parallelism in the application, the overhead of the software handlers overwhelms any potential performance benefits from speculative-thread parallelism. Overall, thread-level speculation still appears to be a promising approach for expanding the class of applications that can be automatically parallelized, but more hardware intensive implementations for managing speculation control are required to achieve performance improvements on a wide class of integer applications.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {58--69},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291020},
 doi = {http://doi.acm.org/10.1145/291069.291020},
 acmid = {291020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hammond:1998:DSS:384265.291020,
 author = {Hammond, Lance and Willey, Mark and Olukotun, Kunle},
 title = {Data speculation support for a chip multiprocessor},
 abstract = {Thread-level speculation is a technique that enables parallel execution of sequential applications on a multiprocessor. This paper describes the complete implementation of the support for threadlevel speculation on the Hydra chip multiprocessor (CMP). The support consists of a number of software speculation control handlers and modifications to the shared secondary cache memory system of the CMP This support is evaluated using five representative integer applications. Our results show that the speculative support is only able to improve performance when there is a substantial amount of medium--grained loop-level parallelism in the application. When the granularity of parallelism is too small or there is little inherent parallelism in the application, the overhead of the software handlers overwhelms any potential performance benefits from speculative-thread parallelism. Overall, thread-level speculation still appears to be a promising approach for expanding the class of applications that can be automatically parallelized, but more hardware intensive implementations for managing speculation control are required to achieve performance improvements on a wide class of integer applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {58--69},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291020},
 doi = {http://doi.acm.org/10.1145/384265.291020},
 acmid = {291020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Van Meter:1998:VNV:291069.291023,
 author = {Van Meter, Rodney and Finn, Gregory G. and Hotz, Steve},
 title = {VISA: Netstation's virtual Internet SCSI adapter},
 abstract = {In this paper we describe the implementation of VISA, our Virtual Internet SCSI Adapter. VISA was built to evaluate the performance impact on the host operating system of using IP to communicate with peripherals, especially storage devices. We have built and benchmarked file systems on VISA-attached emulated disk drives using UDP/IP. By using IP, we expect to take advantage of its scaling characteristics and support for heterogeneous media to build large, long-lived systems. Detailed file system and network CPU utilization and performance data indicate that it is possible for UDP/IP to reach more than 80\% of SCSI's maximum throughput without the use of network coprocessors. We conclude that IP is a viable alternative to special-purpose storage network protocols, and presents numerous advantages.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291023},
 doi = {http://doi.acm.org/10.1145/291069.291023},
 acmid = {291023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Van Meter:1998:VNV:384265.291023,
 author = {Van Meter, Rodney and Finn, Gregory G. and Hotz, Steve},
 title = {VISA: Netstation's virtual Internet SCSI adapter},
 abstract = {In this paper we describe the implementation of VISA, our Virtual Internet SCSI Adapter. VISA was built to evaluate the performance impact on the host operating system of using IP to communicate with peripherals, especially storage devices. We have built and benchmarked file systems on VISA-attached emulated disk drives using UDP/IP. By using IP, we expect to take advantage of its scaling characteristics and support for heterogeneous media to build large, long-lived systems. Detailed file system and network CPU utilization and performance data indicate that it is possible for UDP/IP to reach more than 80\% of SCSI's maximum throughput without the use of network coprocessors. We conclude that IP is a viable alternative to special-purpose storage network protocols, and presents numerous advantages.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291023},
 doi = {http://doi.acm.org/10.1145/384265.291023},
 acmid = {291023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Van Meter:1998:VNV:291006.291023,
 author = {Van Meter, Rodney and Finn, Gregory G. and Hotz, Steve},
 title = {VISA: Netstation's virtual Internet SCSI adapter},
 abstract = {In this paper we describe the implementation of VISA, our Virtual Internet SCSI Adapter. VISA was built to evaluate the performance impact on the host operating system of using IP to communicate with peripherals, especially storage devices. We have built and benchmarked file systems on VISA-attached emulated disk drives using UDP/IP. By using IP, we expect to take advantage of its scaling characteristics and support for heterogeneous media to build large, long-lived systems. Detailed file system and network CPU utilization and performance data indicate that it is possible for UDP/IP to reach more than 80\% of SCSI's maximum throughput without the use of network coprocessors. We conclude that IP is a viable alternative to special-purpose storage network protocols, and presents numerous advantages.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291023},
 doi = {http://doi.acm.org/10.1145/291006.291023},
 acmid = {291023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Acharya:1998:ADP:291006.291026,
 author = {Acharya, Anurag and Uysal, Mustafa and Saltz, Joel},
 title = {Active disks: programming model, algorithms and evaluation},
 abstract = {Several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk</i> architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. The key idea is to offload bulk of the processing to the diskresident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed efficiently and safely. Simulation results for a suite of six algorithms from three application domains (commercial data warehouses, image processing and satellite data processing) indicate that for these algorithms, Active Disks outperform conventional-disk architectures.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {81--91},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291006.291026},
 doi = {http://doi.acm.org/10.1145/291006.291026},
 acmid = {291026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Acharya:1998:ADP:291069.291026,
 author = {Acharya, Anurag and Uysal, Mustafa and Saltz, Joel},
 title = {Active disks: programming model, algorithms and evaluation},
 abstract = {Several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk</i> architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. The key idea is to offload bulk of the processing to the diskresident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed efficiently and safely. Simulation results for a suite of six algorithms from three application domains (commercial data warehouses, image processing and satellite data processing) indicate that for these algorithms, Active Disks outperform conventional-disk architectures.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {81--91},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291069.291026},
 doi = {http://doi.acm.org/10.1145/291069.291026},
 acmid = {291026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Acharya:1998:ADP:384265.291026,
 author = {Acharya, Anurag and Uysal, Mustafa and Saltz, Joel},
 title = {Active disks: programming model, algorithms and evaluation},
 abstract = {Several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk</i> architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. The key idea is to offload bulk of the processing to the diskresident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed efficiently and safely. Simulation results for a suite of six algorithms from three application domains (commercial data warehouses, image processing and satellite data processing) indicate that for these algorithms, Active Disks outperform conventional-disk architectures.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {81--91},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384265.291026},
 doi = {http://doi.acm.org/10.1145/384265.291026},
 acmid = {291026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gibson:1998:CHS:384265.291029,
 author = {Gibson, Garth A. and Nagle, David F. and Amiri, Khalil and Butler, Jeff and Chang, Fay W. and Gobioff, Howard and Hardin, Charles and Riedel, Erik and Rochberg, David and Zelenka, Jim},
 title = {A cost-effective, high-bandwidth storage architecture},
 abstract = {This paper describes the Network-Attached Secure Disk (NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three, filesystems built on our prototype. NASD provides scalable storage bandwidth without the cost of servers used primarily, for transferring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-effectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesysterns suggest that NASD cun support conventional distributed filesystems without performance degradation. More importantly, we show scaluble bandwidth for NASD-specialized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per clientdrive pair, tested with up to eight pairs in our lab.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {92--103},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291029},
 doi = {http://doi.acm.org/10.1145/384265.291029},
 acmid = {291029},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gibson:1998:CHS:291069.291029,
 author = {Gibson, Garth A. and Nagle, David F. and Amiri, Khalil and Butler, Jeff and Chang, Fay W. and Gobioff, Howard and Hardin, Charles and Riedel, Erik and Rochberg, David and Zelenka, Jim},
 title = {A cost-effective, high-bandwidth storage architecture},
 abstract = {This paper describes the Network-Attached Secure Disk (NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three, filesystems built on our prototype. NASD provides scalable storage bandwidth without the cost of servers used primarily, for transferring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-effectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesysterns suggest that NASD cun support conventional distributed filesystems without performance degradation. More importantly, we show scaluble bandwidth for NASD-specialized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per clientdrive pair, tested with up to eight pairs in our lab.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {92--103},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291029},
 doi = {http://doi.acm.org/10.1145/291069.291029},
 acmid = {291029},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gibson:1998:CHS:291006.291029,
 author = {Gibson, Garth A. and Nagle, David F. and Amiri, Khalil and Butler, Jeff and Chang, Fay W. and Gobioff, Howard and Hardin, Charles and Riedel, Erik and Rochberg, David and Zelenka, Jim},
 title = {A cost-effective, high-bandwidth storage architecture},
 abstract = {This paper describes the Network-Attached Secure Disk (NASD) storage architecture, prototype implementations oj NASD drives, array management for our architecture, and three, filesystems built on our prototype. NASD provides scalable storage bandwidth without the cost of servers used primarily, for transferring data from peripheral networks (e.g. SCSI) to client networks (e.g. ethernet). Increasing datuset sizes, new attachment technologies, the convergence of peripheral and interprocessor switched networks, and the increased availability of on-drive transistors motivate and enable this new architecture. NASD is based on four main principles: direct transfer to clients, secure interfaces via cryptographic support, asynchronous non-critical-path oversight, and variably-sized data objects. Measurements of our prototype system show that these services can be cost-effectively integrated into a next generation disk drive ASK. End-to-end measurements of our prototype drive andfilesysterns suggest that NASD cun support conventional distributed filesystems without performance degradation. More importantly, we show scaluble bandwidth for NASD-specialized filesystems. Using a parallel data mining application, NASD drives deliver u linear scaling of 6.2 MB/s per clientdrive pair, tested with up to eight pairs in our lab.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {92--103},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291029},
 doi = {http://doi.acm.org/10.1145/291006.291029},
 acmid = {291029},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Machanick:1998:HTD:291006.291032,
 author = {Machanick, Philip and Salverda, Pierre and Pompe, Lance},
 title = {Hardware-software trade-offs in a direct Rambus implementation of the RAMpage memory hierarchy},
 abstract = {The RAMpage memory hierarchy is an alternative to the traditional division between cache and main memory: main memory is moved up a level and DRAM is used as a paging device. The idea behind RAMpage is to reduce hardware complexity, if at the cost of software complexity, with a view to allowing more flexible memory system design. This paper investigates some issues in choosing between RAMpage and a conventionalcache architecture, with a view to illustrating trade-offs which can be made in choosing whether to place complexity in the memory system in hardware or in software. Performance results in this paper are based on a simple Rambus implementation of DRAM, with performance characteristics of Direct Rambus, which should be available in 1999. This paper explores the conditions under which it becomes feasible to perform a context switch on a miss in the RAMpage model, and the conditions under which RAMpage is a win over a conventional cache architecture: as the CPU-DRAM speed gap grows, RAMpage becomes more viable.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291032},
 doi = {http://doi.acm.org/10.1145/291006.291032},
 acmid = {291032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Machanick:1998:HTD:384265.291032,
 author = {Machanick, Philip and Salverda, Pierre and Pompe, Lance},
 title = {Hardware-software trade-offs in a direct Rambus implementation of the RAMpage memory hierarchy},
 abstract = {The RAMpage memory hierarchy is an alternative to the traditional division between cache and main memory: main memory is moved up a level and DRAM is used as a paging device. The idea behind RAMpage is to reduce hardware complexity, if at the cost of software complexity, with a view to allowing more flexible memory system design. This paper investigates some issues in choosing between RAMpage and a conventionalcache architecture, with a view to illustrating trade-offs which can be made in choosing whether to place complexity in the memory system in hardware or in software. Performance results in this paper are based on a simple Rambus implementation of DRAM, with performance characteristics of Direct Rambus, which should be available in 1999. This paper explores the conditions under which it becomes feasible to perform a context switch on a miss in the RAMpage model, and the conditions under which RAMpage is a win over a conventional cache architecture: as the CPU-DRAM speed gap grows, RAMpage becomes more viable.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291032},
 doi = {http://doi.acm.org/10.1145/384265.291032},
 acmid = {291032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Machanick:1998:HTD:291069.291032,
 author = {Machanick, Philip and Salverda, Pierre and Pompe, Lance},
 title = {Hardware-software trade-offs in a direct Rambus implementation of the RAMpage memory hierarchy},
 abstract = {The RAMpage memory hierarchy is an alternative to the traditional division between cache and main memory: main memory is moved up a level and DRAM is used as a paging device. The idea behind RAMpage is to reduce hardware complexity, if at the cost of software complexity, with a view to allowing more flexible memory system design. This paper investigates some issues in choosing between RAMpage and a conventionalcache architecture, with a view to illustrating trade-offs which can be made in choosing whether to place complexity in the memory system in hardware or in software. Performance results in this paper are based on a simple Rambus implementation of DRAM, with performance characteristics of Direct Rambus, which should be available in 1999. This paper explores the conditions under which it becomes feasible to perform a context switch on a miss in the RAMpage model, and the conditions under which RAMpage is a win over a conventional cache architecture: as the CPU-DRAM speed gap grows, RAMpage becomes more viable.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291032},
 doi = {http://doi.acm.org/10.1145/291069.291032},
 acmid = {291032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Roth:1998:DBP:291069.291034,
 author = {Roth, Amir and Moshovos, Andreas and Sohi, Gurindar S.},
 title = {Dependence based prefetching for linked data structures},
 abstract = {We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25\% on a suite of pointer-intensive programs.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {115--126},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291034},
 doi = {http://doi.acm.org/10.1145/291069.291034},
 acmid = {291034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Roth:1998:DBP:291006.291034,
 author = {Roth, Amir and Moshovos, Andreas and Sohi, Gurindar S.},
 title = {Dependence based prefetching for linked data structures},
 abstract = {We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25\% on a suite of pointer-intensive programs.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {115--126},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291034},
 doi = {http://doi.acm.org/10.1145/291006.291034},
 acmid = {291034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Roth:1998:DBP:384265.291034,
 author = {Roth, Amir and Moshovos, Andreas and Sohi, Gurindar S.},
 title = {Dependence based prefetching for linked data structures},
 abstract = {We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25\% on a suite of pointer-intensive programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {115--126},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291034},
 doi = {http://doi.acm.org/10.1145/384265.291034},
 acmid = {291034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weissman:1998:PCS:291006.291035,
 author = {Weissman, Boris},
 title = {Performance counters and state sharing annotations: a unified approach to thread locality},
 abstract = {This paper describes a combined approach for improving thread locality that uses the bardware performance monitors of modem processors and program-centric code annotations to guide thread scheduling on SMPs. The approach relies on a shared state cache model to compute expected thread footprints in the cache on-line. The accuracy of the model has been analyzed by simmations involving a set of parallel applications. We demonstrate how the cache model can be used to implement several practical locality-based thread scheduling policies with little overhead. Active Threads, a portable, high-performance thread system, has been built and used to investigate the performance impact of locality scheduling for several applications.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {127--138},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291035},
 doi = {http://doi.acm.org/10.1145/291006.291035},
 acmid = {291035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weissman:1998:PCS:384265.291035,
 author = {Weissman, Boris},
 title = {Performance counters and state sharing annotations: a unified approach to thread locality},
 abstract = {This paper describes a combined approach for improving thread locality that uses the bardware performance monitors of modem processors and program-centric code annotations to guide thread scheduling on SMPs. The approach relies on a shared state cache model to compute expected thread footprints in the cache on-line. The accuracy of the model has been analyzed by simmations involving a set of parallel applications. We demonstrate how the cache model can be used to implement several practical locality-based thread scheduling policies with little overhead. Active Threads, a portable, high-performance thread system, has been built and used to investigate the performance impact of locality scheduling for several applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {127--138},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291035},
 doi = {http://doi.acm.org/10.1145/384265.291035},
 acmid = {291035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Weissman:1998:PCS:291069.291035,
 author = {Weissman, Boris},
 title = {Performance counters and state sharing annotations: a unified approach to thread locality},
 abstract = {This paper describes a combined approach for improving thread locality that uses the bardware performance monitors of modem processors and program-centric code annotations to guide thread scheduling on SMPs. The approach relies on a shared state cache model to compute expected thread footprints in the cache on-line. The accuracy of the model has been analyzed by simmations involving a set of parallel applications. We demonstrate how the cache model can be used to implement several practical locality-based thread scheduling policies with little overhead. Active Threads, a portable, high-performance thread system, has been built and used to investigate the performance impact of locality scheduling for several applications.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {127--138},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291035},
 doi = {http://doi.acm.org/10.1145/291069.291035},
 acmid = {291035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Calder:1998:CDP:291006.291036,
 author = {Calder, Brad and Krintz, Chandra and John, Simmi and Austin, Todd},
 title = {Cache-conscious data placement},
 abstract = {As the gap between memory and processor speeds continues to widen, cache eficiency is an increasingly important component of processor performance. Compiler techniques have been used to improve instruction cache pet\$ormance by mapping code with temporal locality to different cache blocks in the virtual address space eliminating cache conflicts. These code placement techniques can be applied directly to the problem of placing data for improved data cache pedormance.In this paper we present a general framework for Cache Conscious Data Placement. This is a compiler directed approach that creates an address placement for the stack (local variables), global variables, heap objects, and constants in order to reduce data cache misses. The placement of data objects is guided by a temporal relationship graph between objects generated via profiling. Our results show that profile driven data placement significantly reduces the data miss rate by 24\% on average.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291006.291036},
 doi = {http://doi.acm.org/10.1145/291006.291036},
 acmid = {291036},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Calder:1998:CDP:384265.291036,
 author = {Calder, Brad and Krintz, Chandra and John, Simmi and Austin, Todd},
 title = {Cache-conscious data placement},
 abstract = {As the gap between memory and processor speeds continues to widen, cache eficiency is an increasingly important component of processor performance. Compiler techniques have been used to improve instruction cache pet\$ormance by mapping code with temporal locality to different cache blocks in the virtual address space eliminating cache conflicts. These code placement techniques can be applied directly to the problem of placing data for improved data cache pedormance.In this paper we present a general framework for Cache Conscious Data Placement. This is a compiler directed approach that creates an address placement for the stack (local variables), global variables, heap objects, and constants in order to reduce data cache misses. The placement of data objects is guided by a temporal relationship graph between objects generated via profiling. Our results show that profile driven data placement significantly reduces the data miss rate by 24\% on average.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384265.291036},
 doi = {http://doi.acm.org/10.1145/384265.291036},
 acmid = {291036},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Calder:1998:CDP:291069.291036,
 author = {Calder, Brad and Krintz, Chandra and John, Simmi and Austin, Todd},
 title = {Cache-conscious data placement},
 abstract = {As the gap between memory and processor speeds continues to widen, cache eficiency is an increasingly important component of processor performance. Compiler techniques have been used to improve instruction cache pet\$ormance by mapping code with temporal locality to different cache blocks in the virtual address space eliminating cache conflicts. These code placement techniques can be applied directly to the problem of placing data for improved data cache pedormance.In this paper we present a general framework for Cache Conscious Data Placement. This is a compiler directed approach that creates an address placement for the stack (local variables), global variables, heap objects, and constants in order to reduce data cache misses. The placement of data objects is guided by a temporal relationship graph between objects generated via profiling. Our results show that profile driven data placement significantly reduces the data miss rate by 24\% on average.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {139--149},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291069.291036},
 doi = {http://doi.acm.org/10.1145/291069.291036},
 acmid = {291036},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Le:1998:OET:384265.291039,
 author = {Le, Bich C.},
 title = {An out-of-order execution technique for runtime binary translators},
 abstract = {A dynamic translator emulates an instruction set architccturc by translating source instructions to native code during execution. On statically-scheduled hardware, higher performance can potentially be achieved by reordering the translated instructions; however, this is a challenging transformation if the source architecture supports precise exception semantics, and the user-level program is allowed to register exception handlers. This paper presents a software technique which allows a translator to achieve the out-of-order execution of user-level programs, while preserving all sequential semantics. The design combines a translator, an interpreter, and a set of operating system services. Using the proposed techniques, a dynamic translator can optimistically reorder instructions and speculate them across branch boundaries. If a mispeculated operation causes an exception, the recovery algorithm reverts the application state to a safe point, then retranslates the faulty code without reordering to disable further exceptions.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {151--158},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/384265.291039},
 doi = {http://doi.acm.org/10.1145/384265.291039},
 acmid = {291039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Le:1998:OET:291006.291039,
 author = {Le, Bich C.},
 title = {An out-of-order execution technique for runtime binary translators},
 abstract = {A dynamic translator emulates an instruction set architccturc by translating source instructions to native code during execution. On statically-scheduled hardware, higher performance can potentially be achieved by reordering the translated instructions; however, this is a challenging transformation if the source architecture supports precise exception semantics, and the user-level program is allowed to register exception handlers. This paper presents a software technique which allows a translator to achieve the out-of-order execution of user-level programs, while preserving all sequential semantics. The design combines a translator, an interpreter, and a set of operating system services. Using the proposed techniques, a dynamic translator can optimistically reorder instructions and speculate them across branch boundaries. If a mispeculated operation causes an exception, the recovery algorithm reverts the application state to a safe point, then retranslates the faulty code without reordering to disable further exceptions.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {151--158},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/291006.291039},
 doi = {http://doi.acm.org/10.1145/291006.291039},
 acmid = {291039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Le:1998:OET:291069.291039,
 author = {Le, Bich C.},
 title = {An out-of-order execution technique for runtime binary translators},
 abstract = {A dynamic translator emulates an instruction set architccturc by translating source instructions to native code during execution. On statically-scheduled hardware, higher performance can potentially be achieved by reordering the translated instructions; however, this is a challenging transformation if the source architecture supports precise exception semantics, and the user-level program is allowed to register exception handlers. This paper presents a software technique which allows a translator to achieve the out-of-order execution of user-level programs, while preserving all sequential semantics. The design combines a translator, an interpreter, and a set of operating system services. Using the proposed techniques, a dynamic translator can optimistically reorder instructions and speculate them across branch boundaries. If a mispeculated operation causes an exception, the recovery algorithm reverts the application state to a safe point, then retranslates the faulty code without reordering to disable further exceptions.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {151--158},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/291069.291039},
 doi = {http://doi.acm.org/10.1145/291069.291039},
 acmid = {291039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krintz:1998:OET:291006.291040,
 author = {Krintz, Chandra and Calder, Brad and Lee, Han Bok and Zorn, Benjamin G.},
 title = {Overlapping execution with transfer using non-strict execution for mobile programs},
 abstract = {In order to execute a program on a remote computer, it mustfirst be transferred over a network. This transmission incurs the over-head of network latency before execution can begin. This latency can vary greatly depending upon the size of the program., where it is located (e.g., on a local network or across the Internet), and the bandwidth available to retrieve the program. Existing technologies, like Java, require that a jle be filly transferred before it can start executing. For large files and low bandwidth lines, this delay can be significant.In this paper we propose and evaluate a non-strict form of mobile program execution. A mobile program is any program that is transferred to a different machine and executed. The goal of nonstrict execution is to overlap execution with transfer; allowing the program to start executing as soon as possible. Non-strict execution allows a procedure in the program to start executing as soon as its code and data have transferred. To enable this technology, we examine several techniques for rearranging procedures and reorganizing the data inside Java classjles. Our results show that nonstrict execution decreases the initial transfer delay between 31\% and 56\% on average, with an average reduction in overall execution time between 25\% and 40\%.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291006.291040},
 doi = {http://doi.acm.org/10.1145/291006.291040},
 acmid = {291040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krintz:1998:OET:291069.291040,
 author = {Krintz, Chandra and Calder, Brad and Lee, Han Bok and Zorn, Benjamin G.},
 title = {Overlapping execution with transfer using non-strict execution for mobile programs},
 abstract = {In order to execute a program on a remote computer, it mustfirst be transferred over a network. This transmission incurs the over-head of network latency before execution can begin. This latency can vary greatly depending upon the size of the program., where it is located (e.g., on a local network or across the Internet), and the bandwidth available to retrieve the program. Existing technologies, like Java, require that a jle be filly transferred before it can start executing. For large files and low bandwidth lines, this delay can be significant.In this paper we propose and evaluate a non-strict form of mobile program execution. A mobile program is any program that is transferred to a different machine and executed. The goal of nonstrict execution is to overlap execution with transfer; allowing the program to start executing as soon as possible. Non-strict execution allows a procedure in the program to start executing as soon as its code and data have transferred. To enable this technology, we examine several techniques for rearranging procedures and reorganizing the data inside Java classjles. Our results show that nonstrict execution decreases the initial transfer delay between 31\% and 56\% on average, with an average reduction in overall execution time between 25\% and 40\%.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291069.291040},
 doi = {http://doi.acm.org/10.1145/291069.291040},
 acmid = {291040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krintz:1998:OET:384265.291040,
 author = {Krintz, Chandra and Calder, Brad and Lee, Han Bok and Zorn, Benjamin G.},
 title = {Overlapping execution with transfer using non-strict execution for mobile programs},
 abstract = {In order to execute a program on a remote computer, it mustfirst be transferred over a network. This transmission incurs the over-head of network latency before execution can begin. This latency can vary greatly depending upon the size of the program., where it is located (e.g., on a local network or across the Internet), and the bandwidth available to retrieve the program. Existing technologies, like Java, require that a jle be filly transferred before it can start executing. For large files and low bandwidth lines, this delay can be significant.In this paper we propose and evaluate a non-strict form of mobile program execution. A mobile program is any program that is transferred to a different machine and executed. The goal of nonstrict execution is to overlap execution with transfer; allowing the program to start executing as soon as possible. Non-strict execution allows a procedure in the program to start executing as soon as its code and data have transferred. To enable this technology, we examine several techniques for rearranging procedures and reorganizing the data inside Java classjles. Our results show that nonstrict execution decreases the initial transfer delay between 31\% and 56\% on average, with an average reduction in overall execution time between 25\% and 40\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384265.291040},
 doi = {http://doi.acm.org/10.1145/384265.291040},
 acmid = {291040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stark:1998:VLP:384265.291042,
 author = {Stark, Jared and Evers, Marius and Patt, Yale N.},
 title = {Variable length path branch prediction},
 abstract = {Accurate branch prediction is required to achieve high performance in deeply pipelined, wide-issue processors. Recent studies have shown that conditional and indirect (or computed) branch targets can be accuratelypredicted by recording the path, which consists of the target addresses of recent branches, leading up to the branch. In current path based branch predictors, the N</i> most recent target addresses are hashed together to form an index into a table, where N</i> is some fixed integer. The indexed table entry isused to make a prediction for the current branch.This paper introduces a new branch predictor in which the value of N</i> is allowed to vary. By constructing the index into the table using the last N</i> target addresses, and using profiling information to select the proper value of N</i> for each branch, extremely accurate branch prediction is achieved. For the SPECint95 gee benchmark, this new predictor has a conditional branch misprediction rate of 4.3\% given a 4K byte hardware budget. For comparison, the gshare predictor, a predictor known for its high accuracy, has a conditional branch misprediction rate of 8.8\% given the same hardware budget. For the indirect branches in gee, the new predictor achieves a misprediction rate of 27.7\% when given a hardware budget of 512 bytes, whereas the best competingpredictor achieves a misprediction rate of 44.2\% when given the same hardware budget.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291042},
 doi = {http://doi.acm.org/10.1145/384265.291042},
 acmid = {291042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stark:1998:VLP:291069.291042,
 author = {Stark, Jared and Evers, Marius and Patt, Yale N.},
 title = {Variable length path branch prediction},
 abstract = {Accurate branch prediction is required to achieve high performance in deeply pipelined, wide-issue processors. Recent studies have shown that conditional and indirect (or computed) branch targets can be accuratelypredicted by recording the path, which consists of the target addresses of recent branches, leading up to the branch. In current path based branch predictors, the N</i> most recent target addresses are hashed together to form an index into a table, where N</i> is some fixed integer. The indexed table entry isused to make a prediction for the current branch.This paper introduces a new branch predictor in which the value of N</i> is allowed to vary. By constructing the index into the table using the last N</i> target addresses, and using profiling information to select the proper value of N</i> for each branch, extremely accurate branch prediction is achieved. For the SPECint95 gee benchmark, this new predictor has a conditional branch misprediction rate of 4.3\% given a 4K byte hardware budget. For comparison, the gshare predictor, a predictor known for its high accuracy, has a conditional branch misprediction rate of 8.8\% given the same hardware budget. For the indirect branches in gee, the new predictor achieves a misprediction rate of 27.7\% when given a hardware budget of 512 bytes, whereas the best competingpredictor achieves a misprediction rate of 44.2\% when given the same hardware budget.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291042},
 doi = {http://doi.acm.org/10.1145/291069.291042},
 acmid = {291042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stark:1998:VLP:291006.291042,
 author = {Stark, Jared and Evers, Marius and Patt, Yale N.},
 title = {Variable length path branch prediction},
 abstract = {Accurate branch prediction is required to achieve high performance in deeply pipelined, wide-issue processors. Recent studies have shown that conditional and indirect (or computed) branch targets can be accuratelypredicted by recording the path, which consists of the target addresses of recent branches, leading up to the branch. In current path based branch predictors, the N</i> most recent target addresses are hashed together to form an index into a table, where N</i> is some fixed integer. The indexed table entry isused to make a prediction for the current branch.This paper introduces a new branch predictor in which the value of N</i> is allowed to vary. By constructing the index into the table using the last N</i> target addresses, and using profiling information to select the proper value of N</i> for each branch, extremely accurate branch prediction is achieved. For the SPECint95 gee benchmark, this new predictor has a conditional branch misprediction rate of 4.3\% given a 4K byte hardware budget. For comparison, the gshare predictor, a predictor known for its high accuracy, has a conditional branch misprediction rate of 8.8\% given the same hardware budget. For the indirect branches in gee, the new predictor achieves a misprediction rate of 27.7\% when given a hardware budget of 512 bytes, whereas the best competingpredictor achieves a misprediction rate of 44.2\% when given the same hardware budget.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291042},
 doi = {http://doi.acm.org/10.1145/291006.291042},
 acmid = {291042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Verghese:1998:PIS:384265.291044,
 author = {Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Performance isolation: sharing and isolation in shared-memory multiprocessors},
 abstract = {Shared-memory multiprocessors (SMPs) are being extensively used as general-purpose servers. The tight coupling of multiple processors, memory, and I/O provides enormous computing power in a single system, and enables the efficient sharing of these resources.The operating systems for these machines (UNIX or Windows NT) provide very few controls for sharing the resources of the system among the active tasks or users. This unconstrained sharing model is a serious limitation for a server because the load placed by one user can adversely affect other users' performance in an unpredictable manner. We show that this lack of isolation is caused by the resource allocation scheme (or lack thereof) carried over from singleuser workstations. Multi-user multiprocessor systems require more sophisticated resource management, and we show how the proposed "performance isolation" scheme can address the current weaknesses of these systems. We have implemented performance isolation in the Silicon Graphics IRIX operating system for three important system resources: CPU time, memory, and disk bandwidth. Running a number of workloads we show that our proposed scheme is successful at providing workstation-like isolation under heavy load, SMP-like latency under light load, and SMP-like throughput in all cases.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291044},
 doi = {http://doi.acm.org/10.1145/384265.291044},
 acmid = {291044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Verghese:1998:PIS:291069.291044,
 author = {Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Performance isolation: sharing and isolation in shared-memory multiprocessors},
 abstract = {Shared-memory multiprocessors (SMPs) are being extensively used as general-purpose servers. The tight coupling of multiple processors, memory, and I/O provides enormous computing power in a single system, and enables the efficient sharing of these resources.The operating systems for these machines (UNIX or Windows NT) provide very few controls for sharing the resources of the system among the active tasks or users. This unconstrained sharing model is a serious limitation for a server because the load placed by one user can adversely affect other users' performance in an unpredictable manner. We show that this lack of isolation is caused by the resource allocation scheme (or lack thereof) carried over from singleuser workstations. Multi-user multiprocessor systems require more sophisticated resource management, and we show how the proposed "performance isolation" scheme can address the current weaknesses of these systems. We have implemented performance isolation in the Silicon Graphics IRIX operating system for three important system resources: CPU time, memory, and disk bandwidth. Running a number of workloads we show that our proposed scheme is successful at providing workstation-like isolation under heavy load, SMP-like latency under light load, and SMP-like throughput in all cases.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291044},
 doi = {http://doi.acm.org/10.1145/291069.291044},
 acmid = {291044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Verghese:1998:PIS:291006.291044,
 author = {Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Performance isolation: sharing and isolation in shared-memory multiprocessors},
 abstract = {Shared-memory multiprocessors (SMPs) are being extensively used as general-purpose servers. The tight coupling of multiple processors, memory, and I/O provides enormous computing power in a single system, and enables the efficient sharing of these resources.The operating systems for these machines (UNIX or Windows NT) provide very few controls for sharing the resources of the system among the active tasks or users. This unconstrained sharing model is a serious limitation for a server because the load placed by one user can adversely affect other users' performance in an unpredictable manner. We show that this lack of isolation is caused by the resource allocation scheme (or lack thereof) carried over from singleuser workstations. Multi-user multiprocessor systems require more sophisticated resource management, and we show how the proposed "performance isolation" scheme can address the current weaknesses of these systems. We have implemented performance isolation in the Silicon Graphics IRIX operating system for three important system resources: CPU time, memory, and disk bandwidth. Running a number of workloads we show that our proposed scheme is successful at providing workstation-like isolation under heavy load, SMP-like latency under light load, and SMP-like throughput in all cases.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291044},
 doi = {http://doi.acm.org/10.1145/291006.291044},
 acmid = {291044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1998:UMA:291006.291046,
 author = {Chen, Yuqun and Bilas, Angelos and Damianakis, Stefanos N. and Dubnicki, Cezary and Li, Kai},
 title = {UTLB: a mechanism for address translation on network interfaces},
 abstract = {An important aspect of a high-speed network system is the ability to transfer data directly between the network interface and application buffers. Such a direct data path</i> requires the network interface to "know" the virtual-to-physical address translation of a user buffer, i.e</i>., the physical memory location of the buffer. This paper presents an efficient address translation architecture, User-managed TLB (UTLB), which eliminates system calls and device interrupts from the common communication path. UTLB also supports application-specific policies to pin and unpin application memory. We report micro-benchmark results for an implementation on Myrinet PC clusters. A trace-driven analysis is used to compare the UTLB approach with the interrupt-based approach. It is also used to study the effects of UTLB cache size, associativity, and prefetching. Our results show that the UTLB approach delivers robust performance with relatively small translation cache sizes.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291046},
 doi = {http://doi.acm.org/10.1145/291006.291046},
 acmid = {291046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1998:UMA:291069.291046,
 author = {Chen, Yuqun and Bilas, Angelos and Damianakis, Stefanos N. and Dubnicki, Cezary and Li, Kai},
 title = {UTLB: a mechanism for address translation on network interfaces},
 abstract = {An important aspect of a high-speed network system is the ability to transfer data directly between the network interface and application buffers. Such a direct data path</i> requires the network interface to "know" the virtual-to-physical address translation of a user buffer, i.e</i>., the physical memory location of the buffer. This paper presents an efficient address translation architecture, User-managed TLB (UTLB), which eliminates system calls and device interrupts from the common communication path. UTLB also supports application-specific policies to pin and unpin application memory. We report micro-benchmark results for an implementation on Myrinet PC clusters. A trace-driven analysis is used to compare the UTLB approach with the interrupt-based approach. It is also used to study the effects of UTLB cache size, associativity, and prefetching. Our results show that the UTLB approach delivers robust performance with relatively small translation cache sizes.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291046},
 doi = {http://doi.acm.org/10.1145/291069.291046},
 acmid = {291046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1998:UMA:384265.291046,
 author = {Chen, Yuqun and Bilas, Angelos and Damianakis, Stefanos N. and Dubnicki, Cezary and Li, Kai},
 title = {UTLB: a mechanism for address translation on network interfaces},
 abstract = {An important aspect of a high-speed network system is the ability to transfer data directly between the network interface and application buffers. Such a direct data path</i> requires the network interface to "know" the virtual-to-physical address translation of a user buffer, i.e</i>., the physical memory location of the buffer. This paper presents an efficient address translation architecture, User-managed TLB (UTLB), which eliminates system calls and device interrupts from the common communication path. UTLB also supports application-specific policies to pin and unpin application memory. We report micro-benchmark results for an implementation on Myrinet PC clusters. A trace-driven analysis is used to compare the UTLB approach with the interrupt-based approach. It is also used to study the effects of UTLB cache size, associativity, and prefetching. Our results show that the UTLB approach delivers robust performance with relatively small translation cache sizes.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291046},
 doi = {http://doi.acm.org/10.1145/384265.291046},
 acmid = {291046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pai:1998:LRD:291069.291048,
 author = {Pai, Vivek S. and Aron, Mohit and Banga, Gaurov and Svendsen, Michael and Druschel, Peter and Zwaenepoel, Willy and Nahum, Erich},
 title = {Locality-aware request distribution in cluster-based network servers},
 abstract = {We consider cluster-based network servers in which a front-end directs incoming requests to one of a number of back-ends. Specifically, we consider content-based request distribution:</i> the front-end uses the content requested, in addition to information about the load on the back-end nodes, to choose which back-end will handle this request. Content-based request distribution can improve locality in the back-ends' main memory caches, increase secondary storage scalability by partitioning the server's database, and provide the ability to employ back-end nodes that are specialized for certain types of requests.As a specific policy for content-based request distribution, we introduce a simple, practical strategy for locality-aware</i> request distribution (LARD). With LARD, the front-end distributes incoming requests in a manner that achieves high locality in the back-ends' main memory caches as well as load balancing. Locality is increased by dynamically subdividing the server's working set over the back-ends. Trace-based simulation results and measurements on a prototype implementation demonstrate substantial performance improvements over state-of-the-art approaches that use only load information to distribute requests. On workloads with working sets that do not fit in a single server node's main memory cache, the achieved throughput exceeds that of the state-of-the-art approach by a factor of two to four.With content-based distribution, incoming requests must be handed off to a back-end in a manner transparent to the client, after</i> the front-end has inspected the content of the request. To this end, we introduce an efficient TCP handoflprotocol</i> that can hand off an established TCP connection in a client-transparent manner.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291048},
 doi = {http://doi.acm.org/10.1145/291069.291048},
 acmid = {291048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pai:1998:LRD:384265.291048,
 author = {Pai, Vivek S. and Aron, Mohit and Banga, Gaurov and Svendsen, Michael and Druschel, Peter and Zwaenepoel, Willy and Nahum, Erich},
 title = {Locality-aware request distribution in cluster-based network servers},
 abstract = {We consider cluster-based network servers in which a front-end directs incoming requests to one of a number of back-ends. Specifically, we consider content-based request distribution:</i> the front-end uses the content requested, in addition to information about the load on the back-end nodes, to choose which back-end will handle this request. Content-based request distribution can improve locality in the back-ends' main memory caches, increase secondary storage scalability by partitioning the server's database, and provide the ability to employ back-end nodes that are specialized for certain types of requests.As a specific policy for content-based request distribution, we introduce a simple, practical strategy for locality-aware</i> request distribution (LARD). With LARD, the front-end distributes incoming requests in a manner that achieves high locality in the back-ends' main memory caches as well as load balancing. Locality is increased by dynamically subdividing the server's working set over the back-ends. Trace-based simulation results and measurements on a prototype implementation demonstrate substantial performance improvements over state-of-the-art approaches that use only load information to distribute requests. On workloads with working sets that do not fit in a single server node's main memory cache, the achieved throughput exceeds that of the state-of-the-art approach by a factor of two to four.With content-based distribution, incoming requests must be handed off to a back-end in a manner transparent to the client, after</i> the front-end has inspected the content of the request. To this end, we introduce an efficient TCP handoflprotocol</i> that can hand off an established TCP connection in a client-transparent manner.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291048},
 doi = {http://doi.acm.org/10.1145/384265.291048},
 acmid = {291048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pai:1998:LRD:291006.291048,
 author = {Pai, Vivek S. and Aron, Mohit and Banga, Gaurov and Svendsen, Michael and Druschel, Peter and Zwaenepoel, Willy and Nahum, Erich},
 title = {Locality-aware request distribution in cluster-based network servers},
 abstract = {We consider cluster-based network servers in which a front-end directs incoming requests to one of a number of back-ends. Specifically, we consider content-based request distribution:</i> the front-end uses the content requested, in addition to information about the load on the back-end nodes, to choose which back-end will handle this request. Content-based request distribution can improve locality in the back-ends' main memory caches, increase secondary storage scalability by partitioning the server's database, and provide the ability to employ back-end nodes that are specialized for certain types of requests.As a specific policy for content-based request distribution, we introduce a simple, practical strategy for locality-aware</i> request distribution (LARD). With LARD, the front-end distributes incoming requests in a manner that achieves high locality in the back-ends' main memory caches as well as load balancing. Locality is increased by dynamically subdividing the server's working set over the back-ends. Trace-based simulation results and measurements on a prototype implementation demonstrate substantial performance improvements over state-of-the-art approaches that use only load information to distribute requests. On workloads with working sets that do not fit in a single server node's main memory cache, the achieved throughput exceeds that of the state-of-the-art approach by a factor of two to four.With content-based distribution, incoming requests must be handed off to a back-end in a manner transparent to the client, after</i> the front-end has inspected the content of the request. To this end, we introduce an efficient TCP handoflprotocol</i> that can hand off an established TCP connection in a client-transparent manner.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291048},
 doi = {http://doi.acm.org/10.1145/291006.291048},
 acmid = {291048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Temam:1998:IOL:291006.291050,
 author = {Temam, Olivier},
 title = {Investigating optimal local memory performance},
 abstract = {Recent work has demonstrated that, cache space is often poorly utilized. However, no previous work has yet demonstrated upper bounds on what a cache or local memory could achieve when exploiting both spatial and temporal locality. Belady's MIN algorithm does yield an upper bound, but exploits only temporal locality. In this article, we present an optimal replacement algorithm for local memory that exploits temporal locality and spatial locality simultaneously. This algorithm is an extension of Belady's algorithm. We prove the optimality of this new algorithm with respect to minimizing misses, and we show experimentally that the algorithm produces nearly minimum memory traffic on the SPEC95 benchmarks. Like Belady's algorithm, our algorithm requires the entire program trace. It selects replacement victims and the number of words it fetches at once based on future accesses. Many different spatial locality strategies can be implemented with this algorithm. With an optimal strategy, the algorithm yields an upper bound that enables us to evaluate alternative implementations to today's caches. We further demonstrate the utility of this algorithm as an analysis tool by evaluating several intermediate strategies between cache and optimal to highlight the limitations of the cache line paradigm using the SPEC95 benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {218--227},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291050},
 doi = {http://doi.acm.org/10.1145/291006.291050},
 acmid = {291050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Temam:1998:IOL:384265.291050,
 author = {Temam, Olivier},
 title = {Investigating optimal local memory performance},
 abstract = {Recent work has demonstrated that, cache space is often poorly utilized. However, no previous work has yet demonstrated upper bounds on what a cache or local memory could achieve when exploiting both spatial and temporal locality. Belady's MIN algorithm does yield an upper bound, but exploits only temporal locality. In this article, we present an optimal replacement algorithm for local memory that exploits temporal locality and spatial locality simultaneously. This algorithm is an extension of Belady's algorithm. We prove the optimality of this new algorithm with respect to minimizing misses, and we show experimentally that the algorithm produces nearly minimum memory traffic on the SPEC95 benchmarks. Like Belady's algorithm, our algorithm requires the entire program trace. It selects replacement victims and the number of words it fetches at once based on future accesses. Many different spatial locality strategies can be implemented with this algorithm. With an optimal strategy, the algorithm yields an upper bound that enables us to evaluate alternative implementations to today's caches. We further demonstrate the utility of this algorithm as an analysis tool by evaluating several intermediate strategies between cache and optimal to highlight the limitations of the cache line paradigm using the SPEC95 benchmarks.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {218--227},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291050},
 doi = {http://doi.acm.org/10.1145/384265.291050},
 acmid = {291050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Temam:1998:IOL:291069.291050,
 author = {Temam, Olivier},
 title = {Investigating optimal local memory performance},
 abstract = {Recent work has demonstrated that, cache space is often poorly utilized. However, no previous work has yet demonstrated upper bounds on what a cache or local memory could achieve when exploiting both spatial and temporal locality. Belady's MIN algorithm does yield an upper bound, but exploits only temporal locality. In this article, we present an optimal replacement algorithm for local memory that exploits temporal locality and spatial locality simultaneously. This algorithm is an extension of Belady's algorithm. We prove the optimality of this new algorithm with respect to minimizing misses, and we show experimentally that the algorithm produces nearly minimum memory traffic on the SPEC95 benchmarks. Like Belady's algorithm, our algorithm requires the entire program trace. It selects replacement victims and the number of words it fetches at once based on future accesses. Many different spatial locality strategies can be implemented with this algorithm. With an optimal strategy, the algorithm yields an upper bound that enables us to evaluate alternative implementations to today's caches. We further demonstrate the utility of this algorithm as an analysis tool by evaluating several intermediate strategies between cache and optimal to highlight the limitations of the cache line paradigm using the SPEC95 benchmarks.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {218--227},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291050},
 doi = {http://doi.acm.org/10.1145/291069.291050},
 acmid = {291050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghosh:1998:PMA:384265.291051,
 author = {Ghosh, Somnath and Martonosi, Margaret and Malik, Sharad},
 title = {Precise miss analysis for program transformations with caches of arbitrary associativity},
 abstract = {Analyzing and optimizing program memory performance is a pressing problem in high-performance computer architectures. Currently, software solutions addressing the processor-memory performance gap include compiler-or programmer-applied optimizations like data structure padding, matrix blocking, and other program transformations. Compiler optimization can be effective, but the lack of precise</i> analysis and optimization frameworks makes it impossible to confidently make optimal, rather than heuristic-based, program transformations. Imprecision is most problematic in situations where hard-to-predict cache conflicts foil heuristic approaches. Furthermore, the lack of a general</i> framework for compiler memory performance analysis makes it impossible to understand the combined effects of several program transformations.The Cache Miss Equation (CME) framework discussed in this paper addresses these issues. We express memory reference and cache conflict behavior in terms of sets of equations. The mathematical precision of CMEs allows us to find true optimal solutions for transformations like blocking or padding. The generality of CMEs also allows us to reason about interactions between transformations applied in concert. Unlike our prior work, this framework applies to caches of arbitrary associativity. This paper also demonstrates the utility of CMEs by presenting precise algorithms for intra-variable padding, inter-variable padding, and selecting tile sizes. Our experiences with CMEs implemented in the SUIF system show that they are a unifying mathematical framework offering the generality and precision imperative for compiler optimizations on current high-performance architectures.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291051},
 doi = {http://doi.acm.org/10.1145/384265.291051},
 acmid = {291051},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ghosh:1998:PMA:291069.291051,
 author = {Ghosh, Somnath and Martonosi, Margaret and Malik, Sharad},
 title = {Precise miss analysis for program transformations with caches of arbitrary associativity},
 abstract = {Analyzing and optimizing program memory performance is a pressing problem in high-performance computer architectures. Currently, software solutions addressing the processor-memory performance gap include compiler-or programmer-applied optimizations like data structure padding, matrix blocking, and other program transformations. Compiler optimization can be effective, but the lack of precise</i> analysis and optimization frameworks makes it impossible to confidently make optimal, rather than heuristic-based, program transformations. Imprecision is most problematic in situations where hard-to-predict cache conflicts foil heuristic approaches. Furthermore, the lack of a general</i> framework for compiler memory performance analysis makes it impossible to understand the combined effects of several program transformations.The Cache Miss Equation (CME) framework discussed in this paper addresses these issues. We express memory reference and cache conflict behavior in terms of sets of equations. The mathematical precision of CMEs allows us to find true optimal solutions for transformations like blocking or padding. The generality of CMEs also allows us to reason about interactions between transformations applied in concert. Unlike our prior work, this framework applies to caches of arbitrary associativity. This paper also demonstrates the utility of CMEs by presenting precise algorithms for intra-variable padding, inter-variable padding, and selecting tile sizes. Our experiences with CMEs implemented in the SUIF system show that they are a unifying mathematical framework offering the generality and precision imperative for compiler optimizations on current high-performance architectures.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291051},
 doi = {http://doi.acm.org/10.1145/291069.291051},
 acmid = {291051},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghosh:1998:PMA:291006.291051,
 author = {Ghosh, Somnath and Martonosi, Margaret and Malik, Sharad},
 title = {Precise miss analysis for program transformations with caches of arbitrary associativity},
 abstract = {Analyzing and optimizing program memory performance is a pressing problem in high-performance computer architectures. Currently, software solutions addressing the processor-memory performance gap include compiler-or programmer-applied optimizations like data structure padding, matrix blocking, and other program transformations. Compiler optimization can be effective, but the lack of precise</i> analysis and optimization frameworks makes it impossible to confidently make optimal, rather than heuristic-based, program transformations. Imprecision is most problematic in situations where hard-to-predict cache conflicts foil heuristic approaches. Furthermore, the lack of a general</i> framework for compiler memory performance analysis makes it impossible to understand the combined effects of several program transformations.The Cache Miss Equation (CME) framework discussed in this paper addresses these issues. We express memory reference and cache conflict behavior in terms of sets of equations. The mathematical precision of CMEs allows us to find true optimal solutions for transformations like blocking or padding. The generality of CMEs also allows us to reason about interactions between transformations applied in concert. Unlike our prior work, this framework applies to caches of arbitrary associativity. This paper also demonstrates the utility of CMEs by presenting precise algorithms for intra-variable padding, inter-variable padding, and selecting tile sizes. Our experiences with CMEs implemented in the SUIF system show that they are a unifying mathematical framework offering the generality and precision imperative for compiler optimizations on current high-performance architectures.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291051},
 doi = {http://doi.acm.org/10.1145/291006.291051},
 acmid = {291051},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peir:1998:CDM:384265.291053,
 author = {Peir, Jih-Kwon and Lee, Yongjoon and Hsu, Windsor W.},
 title = {Capturing dynamic memory reference behavior with adaptive cache topology},
 abstract = {Memory references exhibit locality and are therefore not uniformly distributed across the sets of a cache. This skew reduces the effectiveness of a cache because it results in the caching of a considerable number of less-recently-used lines which are less likely to be re-referenced before they are replaced. In this paper, we describe a technique that dynamically identifies these less-recently-used lines and effectively utilizes the cache frames they occupy to more accurately approximate the global least-recently-used replacement policy while maintaining the fast access time of a direct-mapped cache. We also explore the idea of using these underutilized cache frames to reduce cache misses through data prefetching. In the proposed design, the possible locations that a line can reside in is not predetermined. Instead, the cache is dynamically partitioned into groups of cache lines. Because both the total number of groups and the individual group associativity adapt to the dynamic reference pattern, we call this design the adaptive group-associative cache. Performance evaluation using trace-driven simulations of the TPC-C benchmark and selected programs from the SPEC95 benchmark suite shows that the group-associative cache is able to achieve a hit ratio that is consistently better than that of a 4-way set-associative cache. For some of the workloads, the hit ratio approaches that of a fully-associative cache.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {240--250},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384265.291053},
 doi = {http://doi.acm.org/10.1145/384265.291053},
 acmid = {291053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peir:1998:CDM:291006.291053,
 author = {Peir, Jih-Kwon and Lee, Yongjoon and Hsu, Windsor W.},
 title = {Capturing dynamic memory reference behavior with adaptive cache topology},
 abstract = {Memory references exhibit locality and are therefore not uniformly distributed across the sets of a cache. This skew reduces the effectiveness of a cache because it results in the caching of a considerable number of less-recently-used lines which are less likely to be re-referenced before they are replaced. In this paper, we describe a technique that dynamically identifies these less-recently-used lines and effectively utilizes the cache frames they occupy to more accurately approximate the global least-recently-used replacement policy while maintaining the fast access time of a direct-mapped cache. We also explore the idea of using these underutilized cache frames to reduce cache misses through data prefetching. In the proposed design, the possible locations that a line can reside in is not predetermined. Instead, the cache is dynamically partitioned into groups of cache lines. Because both the total number of groups and the individual group associativity adapt to the dynamic reference pattern, we call this design the adaptive group-associative cache. Performance evaluation using trace-driven simulations of the TPC-C benchmark and selected programs from the SPEC95 benchmark suite shows that the group-associative cache is able to achieve a hit ratio that is consistently better than that of a 4-way set-associative cache. For some of the workloads, the hit ratio approaches that of a fully-associative cache.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {240--250},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291006.291053},
 doi = {http://doi.acm.org/10.1145/291006.291053},
 acmid = {291053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peir:1998:CDM:291069.291053,
 author = {Peir, Jih-Kwon and Lee, Yongjoon and Hsu, Windsor W.},
 title = {Capturing dynamic memory reference behavior with adaptive cache topology},
 abstract = {Memory references exhibit locality and are therefore not uniformly distributed across the sets of a cache. This skew reduces the effectiveness of a cache because it results in the caching of a considerable number of less-recently-used lines which are less likely to be re-referenced before they are replaced. In this paper, we describe a technique that dynamically identifies these less-recently-used lines and effectively utilizes the cache frames they occupy to more accurately approximate the global least-recently-used replacement policy while maintaining the fast access time of a direct-mapped cache. We also explore the idea of using these underutilized cache frames to reduce cache misses through data prefetching. In the proposed design, the possible locations that a line can reside in is not predetermined. Instead, the cache is dynamically partitioned into groups of cache lines. Because both the total number of groups and the individual group associativity adapt to the dynamic reference pattern, we call this design the adaptive group-associative cache. Performance evaluation using trace-driven simulations of the TPC-C benchmark and selected programs from the SPEC95 benchmark suite shows that the group-associative cache is able to achieve a hit ratio that is consistently better than that of a 4-way set-associative cache. For some of the workloads, the hit ratio approaches that of a fully-associative cache.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {240--250},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/291069.291053},
 doi = {http://doi.acm.org/10.1145/291069.291053},
 acmid = {291053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Citron:1998:AMP:384265.291056,
 author = {Citron, Daniel and Feitelson, Dror and Rudolph, Larry},
 title = {Accelerating multi-media processing by implementing memoing in multiplication and division units},
 abstract = {This paper proposes a technique that enables performing multi-cycle (multiplication, division, square-root \&amp;hellip;) computations in a single cycle. The technique is based on the notion of memoing: saving the input and output of previous calculations and using the output if the input is encountered again. This technique is especially suitable for Multi-Media (MM) processing. In MM applications the local entropy of the data tends to be low which results in repeated operations on the same datum.The inputs and outputs of assembly level operations are stored in cache-like lookup tables and accessed in parallel to the conventional computation. A successful lookup gives the result of a multi-cycle computation in a single cycle, and a failed lookup doesn't necessitate a penalty in computation time.Results of simulations have shown that on the average, for a modestly sized memo-table, about 40\% of the floating point multiplications and 50\% of the floating point divisions, in Multi-Media applications, can be avoided by using the values within the memo-table, leading to an average computational speedup of more than 20\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {252--261},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291056},
 doi = {http://doi.acm.org/10.1145/384265.291056},
 acmid = {291056},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Citron:1998:AMP:291006.291056,
 author = {Citron, Daniel and Feitelson, Dror and Rudolph, Larry},
 title = {Accelerating multi-media processing by implementing memoing in multiplication and division units},
 abstract = {This paper proposes a technique that enables performing multi-cycle (multiplication, division, square-root \&amp;hellip;) computations in a single cycle. The technique is based on the notion of memoing: saving the input and output of previous calculations and using the output if the input is encountered again. This technique is especially suitable for Multi-Media (MM) processing. In MM applications the local entropy of the data tends to be low which results in repeated operations on the same datum.The inputs and outputs of assembly level operations are stored in cache-like lookup tables and accessed in parallel to the conventional computation. A successful lookup gives the result of a multi-cycle computation in a single cycle, and a failed lookup doesn't necessitate a penalty in computation time.Results of simulations have shown that on the average, for a modestly sized memo-table, about 40\% of the floating point multiplications and 50\% of the floating point divisions, in Multi-Media applications, can be avoided by using the values within the memo-table, leading to an average computational speedup of more than 20\%.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {252--261},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291056},
 doi = {http://doi.acm.org/10.1145/291006.291056},
 acmid = {291056},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Citron:1998:AMP:291069.291056,
 author = {Citron, Daniel and Feitelson, Dror and Rudolph, Larry},
 title = {Accelerating multi-media processing by implementing memoing in multiplication and division units},
 abstract = {This paper proposes a technique that enables performing multi-cycle (multiplication, division, square-root \&amp;hellip;) computations in a single cycle. The technique is based on the notion of memoing: saving the input and output of previous calculations and using the output if the input is encountered again. This technique is especially suitable for Multi-Media (MM) processing. In MM applications the local entropy of the data tends to be low which results in repeated operations on the same datum.The inputs and outputs of assembly level operations are stored in cache-like lookup tables and accessed in parallel to the conventional computation. A successful lookup gives the result of a multi-cycle computation in a single cycle, and a failed lookup doesn't necessitate a penalty in computation time.Results of simulations have shown that on the average, for a modestly sized memo-table, about 40\% of the floating point multiplications and 50\% of the floating point divisions, in Multi-Media applications, can be avoided by using the values within the memo-table, leading to an average computational speedup of more than 20\%.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {252--261},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291056},
 doi = {http://doi.acm.org/10.1145/291069.291056},
 acmid = {291056},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fu:1998:VSS:291006.291058,
 author = {Fu, Chao-Ying and Jennings, Matthew D. and Larin, Sergei Y. and Conte, Thomas M.},
 title = {Value speculation scheduling for high performance processors},
 abstract = {Recent research in value prediction shows a surprising amount of predictability for the values produced by register-writing instructions. Several hardware based value predictor designs have been proposed to exploit this predictability by eliminating flow dependencies for highly predictable values. This paper proposed a hardware and software based scheme for value speculation scheduling (VSS). Static VLIW scheduling techniques are used to speculate value dependent instructions by scheduling them above the instructions whose results they are dependent on. Prediction hardware is used to provide value predictions for allowing the execution of speculated instructions to continue. In the case of miss-predicted values, control flow is redirected to patch-up code so that execution can proceed with the correct results. In this paper, experiments in VSS for load operations in the SPECint95 benchmarks are performed. Speedup of up to 17\% has been shown for using VSS. Empirical results on the value predictability of loads, based on value profiling data, are also provided.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {262--271},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291058},
 doi = {http://doi.acm.org/10.1145/291006.291058},
 acmid = {291058},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLIW instruction schedulings, instruction level parallelism, value prediction, value speculation},
} 

@inproceedings{Fu:1998:VSS:291069.291058,
 author = {Fu, Chao-Ying and Jennings, Matthew D. and Larin, Sergei Y. and Conte, Thomas M.},
 title = {Value speculation scheduling for high performance processors},
 abstract = {Recent research in value prediction shows a surprising amount of predictability for the values produced by register-writing instructions. Several hardware based value predictor designs have been proposed to exploit this predictability by eliminating flow dependencies for highly predictable values. This paper proposed a hardware and software based scheme for value speculation scheduling (VSS). Static VLIW scheduling techniques are used to speculate value dependent instructions by scheduling them above the instructions whose results they are dependent on. Prediction hardware is used to provide value predictions for allowing the execution of speculated instructions to continue. In the case of miss-predicted values, control flow is redirected to patch-up code so that execution can proceed with the correct results. In this paper, experiments in VSS for load operations in the SPECint95 benchmarks are performed. Speedup of up to 17\% has been shown for using VSS. Empirical results on the value predictability of loads, based on value profiling data, are also provided.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {262--271},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291058},
 doi = {http://doi.acm.org/10.1145/291069.291058},
 acmid = {291058},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLIW instruction schedulings, instruction level parallelism, value prediction, value speculation},
} 

@article{Fu:1998:VSS:384265.291058,
 author = {Fu, Chao-Ying and Jennings, Matthew D. and Larin, Sergei Y. and Conte, Thomas M.},
 title = {Value speculation scheduling for high performance processors},
 abstract = {Recent research in value prediction shows a surprising amount of predictability for the values produced by register-writing instructions. Several hardware based value predictor designs have been proposed to exploit this predictability by eliminating flow dependencies for highly predictable values. This paper proposed a hardware and software based scheme for value speculation scheduling (VSS). Static VLIW scheduling techniques are used to speculate value dependent instructions by scheduling them above the instructions whose results they are dependent on. Prediction hardware is used to provide value predictions for allowing the execution of speculated instructions to continue. In the case of miss-predicted values, control flow is redirected to patch-up code so that execution can proceed with the correct results. In this paper, experiments in VSS for load operations in the SPECint95 benchmarks are performed. Speedup of up to 17\% has been shown for using VSS. Empirical results on the value predictability of loads, based on value profiling data, are also provided.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {262--271},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291058},
 doi = {http://doi.acm.org/10.1145/384265.291058},
 acmid = {291058},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLIW instruction schedulings, instruction level parallelism, value prediction, value speculation},
} 

@article{Ranganathan:1998:ESD:384265.291061,
 author = {Ranganathan, Narayan and Franklin, Manoj},
 title = {An empirical study of decentralized ILP execution models},
 abstract = {Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {272--281},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384265.291061},
 doi = {http://doi.acm.org/10.1145/384265.291061},
 acmid = {291061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control dependence, data dependence, decentralization, dynamic scheduling, execution unit dependence, hardware window, instruction-level parallelism, speculative execution},
} 

@article{Ranganathan:1998:ESD:291006.291061,
 author = {Ranganathan, Narayan and Franklin, Manoj},
 title = {An empirical study of decentralized ILP execution models},
 abstract = {Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {272--281},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291006.291061},
 doi = {http://doi.acm.org/10.1145/291006.291061},
 acmid = {291061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control dependence, data dependence, decentralization, dynamic scheduling, execution unit dependence, hardware window, instruction-level parallelism, speculative execution},
} 

@inproceedings{Ranganathan:1998:ESD:291069.291061,
 author = {Ranganathan, Narayan and Franklin, Manoj},
 title = {An empirical study of decentralized ILP execution models},
 abstract = {Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {272--281},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/291069.291061},
 doi = {http://doi.acm.org/10.1145/291069.291061},
 acmid = {291061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control dependence, data dependence, decentralization, dynamic scheduling, execution unit dependence, hardware window, instruction-level parallelism, speculative execution},
} 

@article{Schnarr:1998:FOP:384265.291063,
 author = {Schnarr, Eric and Larus, James R.},
 title = {Fast out-of-order processor simulation using memoization},
 abstract = {Our new out-of-order processor simulatol; FastSim, uses two innovations to speed up simulation 8--15 times (vs. Wisconsin SimpleScalar) with no loss in simulation accuracy. First, FastSim uses speculative direct-execution to accelerate the functional emulation of speculatively executed program code. Second, it uses a variation on memoization---a well-known technique in programming language implementation---to cache microarchitecture states and the resulting simulator actions, and then "fast forwards" the simulation the next time a cached state is reached. Fast-forwarding accelerates simulation by an order of magnitude, while producing exactly the same, cycle-accurate result as conventional simulation.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291063},
 doi = {http://doi.acm.org/10.1145/384265.291063},
 acmid = {291063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {direct-execution, memoization, out-of-order processor simulation},
} 

@inproceedings{Schnarr:1998:FOP:291069.291063,
 author = {Schnarr, Eric and Larus, James R.},
 title = {Fast out-of-order processor simulation using memoization},
 abstract = {Our new out-of-order processor simulatol; FastSim, uses two innovations to speed up simulation 8--15 times (vs. Wisconsin SimpleScalar) with no loss in simulation accuracy. First, FastSim uses speculative direct-execution to accelerate the functional emulation of speculatively executed program code. Second, it uses a variation on memoization---a well-known technique in programming language implementation---to cache microarchitecture states and the resulting simulator actions, and then "fast forwards" the simulation the next time a cached state is reached. Fast-forwarding accelerates simulation by an order of magnitude, while producing exactly the same, cycle-accurate result as conventional simulation.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291063},
 doi = {http://doi.acm.org/10.1145/291069.291063},
 acmid = {291063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {direct-execution, memoization, out-of-order processor simulation},
} 

@article{Schnarr:1998:FOP:291006.291063,
 author = {Schnarr, Eric and Larus, James R.},
 title = {Fast out-of-order processor simulation using memoization},
 abstract = {Our new out-of-order processor simulatol; FastSim, uses two innovations to speed up simulation 8--15 times (vs. Wisconsin SimpleScalar) with no loss in simulation accuracy. First, FastSim uses speculative direct-execution to accelerate the functional emulation of speculatively executed program code. Second, it uses a variation on memoization---a well-known technique in programming language implementation---to cache microarchitecture states and the resulting simulator actions, and then "fast forwards" the simulation the next time a cached state is reached. Fast-forwarding accelerates simulation by an order of magnitude, while producing exactly the same, cycle-accurate result as conventional simulation.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291063},
 doi = {http://doi.acm.org/10.1145/291006.291063},
 acmid = {291063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {direct-execution, memoization, out-of-order processor simulation},
} 

@article{Jacob:1998:LSM:291006.291065,
 author = {Jacob, Bruce L. and Mudge, Trevor N.},
 title = {A look at several memory management units, TLB-refill mechanisms, and page table organizations},
 abstract = {Virtual memory is a staple in modem systems, though there is little agreement on how its functionality is to be implemented on either the hardware or software side of the interface. The myriad of design choices and incompatible hardware mechanisms suggests potential performance problems, especially since increasing numbers of systems (even embedded systems) are using memory management. A comparative study of the implementation choices in virtual memory should therefore aid system-level designers.This paper compares several virtual memory designs, including combinations of hierarchical and inverted page tables on hardware-managed and software-managed translation lookaside buffers (TLBs). The simulations show that systems are fairly sensitive to TLB size; that interrupts already account for a large portion of memory-management overhead and can become a significant factor as processors execute more concurrent instructions; and that if one includes the cache misses inflicted on applications by the VM system, the total VM overhead is roughly twice what was thought (10--20\% rather than 5--10\%).},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {295--306},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291065},
 doi = {http://doi.acm.org/10.1145/291006.291065},
 acmid = {291065},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jacob:1998:LSM:291069.291065,
 author = {Jacob, Bruce L. and Mudge, Trevor N.},
 title = {A look at several memory management units, TLB-refill mechanisms, and page table organizations},
 abstract = {Virtual memory is a staple in modem systems, though there is little agreement on how its functionality is to be implemented on either the hardware or software side of the interface. The myriad of design choices and incompatible hardware mechanisms suggests potential performance problems, especially since increasing numbers of systems (even embedded systems) are using memory management. A comparative study of the implementation choices in virtual memory should therefore aid system-level designers.This paper compares several virtual memory designs, including combinations of hierarchical and inverted page tables on hardware-managed and software-managed translation lookaside buffers (TLBs). The simulations show that systems are fairly sensitive to TLB size; that interrupts already account for a large portion of memory-management overhead and can become a significant factor as processors execute more concurrent instructions; and that if one includes the cache misses inflicted on applications by the VM system, the total VM overhead is roughly twice what was thought (10--20\% rather than 5--10\%).},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {295--306},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291065},
 doi = {http://doi.acm.org/10.1145/291069.291065},
 acmid = {291065},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jacob:1998:LSM:384265.291065,
 author = {Jacob, Bruce L. and Mudge, Trevor N.},
 title = {A look at several memory management units, TLB-refill mechanisms, and page table organizations},
 abstract = {Virtual memory is a staple in modem systems, though there is little agreement on how its functionality is to be implemented on either the hardware or software side of the interface. The myriad of design choices and incompatible hardware mechanisms suggests potential performance problems, especially since increasing numbers of systems (even embedded systems) are using memory management. A comparative study of the implementation choices in virtual memory should therefore aid system-level designers.This paper compares several virtual memory designs, including combinations of hierarchical and inverted page tables on hardware-managed and software-managed translation lookaside buffers (TLBs). The simulations show that systems are fairly sensitive to TLB size; that interrupts already account for a large portion of memory-management overhead and can become a significant factor as processors execute more concurrent instructions; and that if one includes the cache misses inflicted on applications by the VM system, the total VM overhead is roughly twice what was thought (10--20\% rather than 5--10\%).},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {295--306},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291065},
 doi = {http://doi.acm.org/10.1145/384265.291065},
 acmid = {291065},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ranganathan:1998:PDW:384265.291067,
 author = {Ranganathan, Parthasarathy and Gharachorloo, Kourosh and Adve, Sarita V. and Barroso, Luiz Andr\'{e}},
 title = {Performance of database workloads on shared-memory systems with out-of-order processors},
 abstract = {Database applications such as online transaction processing (OLTP) and decision support systems (DSS) constitute the largest and fastest-growing segment of the market for multiprocessor servers. However, most current system designs have been optimized to perform well on scientific and engineering workloads. Given the radically different behavior of database workloads (especially OLTP), it is important to re-evaluate key system design decisions in the context of this important class of applications.This paper examines the behavior of database workloads on shared-memory multiprocessors with aggressive out-of-order processors, and considers simple optimizations that can provide further performance improvements. Our study is based on detailed simulations of the Oracle commercial database engine. The results show that the combination of out-of-order execution and multiple instruction issue is indeed effective in improving performance of database workloads, providing gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP and DSS, respectively. In addition, speculative techniques enable optimized implementations of memory consistency models that significantly improve the performance of stricter consistency models, bringing the performance to within 10--15\% of the performance of more relaxed models.The second part of our study focuses on the more challenging OLTP workload. We show that an instruction stream buffer is effective in reducing the remaining instruction stalls in OLTP, providing a 17\% reduction in execution time (approaching a perfect instruction cache to within 15\%). Furthermore, our characterization shows that a large fraction of the data communication misses in OLTP exhibit migratory behavior; our preliminary results show that software prefetch and writeback/flush hints can be used for this data to further reduce execution time by 12\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {32},
 issue = {5},
 month = {October},
 year = {1998},
 issn = {0163-5980},
 pages = {307--318},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384265.291067},
 doi = {http://doi.acm.org/10.1145/384265.291067},
 acmid = {291067},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ranganathan:1998:PDW:291069.291067,
 author = {Ranganathan, Parthasarathy and Gharachorloo, Kourosh and Adve, Sarita V. and Barroso, Luiz Andr\'{e}},
 title = {Performance of database workloads on shared-memory systems with out-of-order processors},
 abstract = {Database applications such as online transaction processing (OLTP) and decision support systems (DSS) constitute the largest and fastest-growing segment of the market for multiprocessor servers. However, most current system designs have been optimized to perform well on scientific and engineering workloads. Given the radically different behavior of database workloads (especially OLTP), it is important to re-evaluate key system design decisions in the context of this important class of applications.This paper examines the behavior of database workloads on shared-memory multiprocessors with aggressive out-of-order processors, and considers simple optimizations that can provide further performance improvements. Our study is based on detailed simulations of the Oracle commercial database engine. The results show that the combination of out-of-order execution and multiple instruction issue is indeed effective in improving performance of database workloads, providing gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP and DSS, respectively. In addition, speculative techniques enable optimized implementations of memory consistency models that significantly improve the performance of stricter consistency models, bringing the performance to within 10--15\% of the performance of more relaxed models.The second part of our study focuses on the more challenging OLTP workload. We show that an instruction stream buffer is effective in reducing the remaining instruction stalls in OLTP, providing a 17\% reduction in execution time (approaching a perfect instruction cache to within 15\%). Furthermore, our characterization shows that a large fraction of the data communication misses in OLTP exhibit migratory behavior; our preliminary results show that software prefetch and writeback/flush hints can be used for this data to further reduce execution time by 12\%.},
 booktitle = {Proceedings of the eighth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 location = {San Jose, California, United States},
 pages = {307--318},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291067},
 doi = {http://doi.acm.org/10.1145/291069.291067},
 acmid = {291067},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ranganathan:1998:PDW:291006.291067,
 author = {Ranganathan, Parthasarathy and Gharachorloo, Kourosh and Adve, Sarita V. and Barroso, Luiz Andr\'{e}},
 title = {Performance of database workloads on shared-memory systems with out-of-order processors},
 abstract = {Database applications such as online transaction processing (OLTP) and decision support systems (DSS) constitute the largest and fastest-growing segment of the market for multiprocessor servers. However, most current system designs have been optimized to perform well on scientific and engineering workloads. Given the radically different behavior of database workloads (especially OLTP), it is important to re-evaluate key system design decisions in the context of this important class of applications.This paper examines the behavior of database workloads on shared-memory multiprocessors with aggressive out-of-order processors, and considers simple optimizations that can provide further performance improvements. Our study is based on detailed simulations of the Oracle commercial database engine. The results show that the combination of out-of-order execution and multiple instruction issue is indeed effective in improving performance of database workloads, providing gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP and DSS, respectively. In addition, speculative techniques enable optimized implementations of memory consistency models that significantly improve the performance of stricter consistency models, bringing the performance to within 10--15\% of the performance of more relaxed models.The second part of our study focuses on the more challenging OLTP workload. We show that an instruction stream buffer is effective in reducing the remaining instruction stalls in OLTP, providing a 17\% reduction in execution time (approaching a perfect instruction cache to within 15\%). Furthermore, our characterization shows that a large fraction of the data communication misses in OLTP exhibit migratory behavior; our preliminary results show that software prefetch and writeback/flush hints can be used for this data to further reduce execution time by 12\%.},
 journal = {SIGPLAN Not.},
 volume = {33},
 issue = {11},
 month = {October},
 year = {1998},
 issn = {0362-1340},
 pages = {307--318},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291006.291067},
 doi = {http://doi.acm.org/10.1145/291006.291067},
 acmid = {291067},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Olukotun:1996:CSM:248209.237140,
 author = {Olukotun, Kunle and Nayfeh, Basem A. and Hammond, Lance and Wilson, Ken and Chang, Kunyung},
 title = {The case for a single-chip multiprocessor},
 abstract = {Advances in IC processing allow for more microprocessor design options. The increasing gate density and cost of wires in advanced integrated circuit technologies require that we look for new ways to use their capabilities effectively. This paper shows that in advanced technologies it is possible to implement a single-chip multiprocessor in the same area as a wide issue superscalar processor. We find that for applications with little parallelism the performance of the two microarchitectures is comparable. For applications with large amounts of parallelism at both the fine and coarse grained levels, the multiprocessor microarchitecture outperforms the superscalar architecture by a significant margin. Single-chip multiprocessor architectures have the advantage in that they offer localized implementation of a high-clock rate processor for inherently sequential applications and low latency interprocessor communication for parallel applications.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237140},
 doi = {http://doi.acm.org/10.1145/248209.237140},
 acmid = {237140},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Olukotun:1996:CSM:248208.237140,
 author = {Olukotun, Kunle and Nayfeh, Basem A. and Hammond, Lance and Wilson, Ken and Chang, Kunyung},
 title = {The case for a single-chip multiprocessor},
 abstract = {Advances in IC processing allow for more microprocessor design options. The increasing gate density and cost of wires in advanced integrated circuit technologies require that we look for new ways to use their capabilities effectively. This paper shows that in advanced technologies it is possible to implement a single-chip multiprocessor in the same area as a wide issue superscalar processor. We find that for applications with little parallelism the performance of the two microarchitectures is comparable. For applications with large amounts of parallelism at both the fine and coarse grained levels, the multiprocessor microarchitecture outperforms the superscalar architecture by a significant margin. Single-chip multiprocessor architectures have the advantage in that they offer localized implementation of a high-clock rate processor for inherently sequential applications and low latency interprocessor communication for parallel applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237140},
 doi = {http://doi.acm.org/10.1145/248208.237140},
 acmid = {237140},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Olukotun:1996:CSM:237090.237140,
 author = {Olukotun, Kunle and Nayfeh, Basem A. and Hammond, Lance and Wilson, Ken and Chang, Kunyung},
 title = {The case for a single-chip multiprocessor},
 abstract = {Advances in IC processing allow for more microprocessor design options. The increasing gate density and cost of wires in advanced integrated circuit technologies require that we look for new ways to use their capabilities effectively. This paper shows that in advanced technologies it is possible to implement a single-chip multiprocessor in the same area as a wide issue superscalar processor. We find that for applications with little parallelism the performance of the two microarchitectures is comparable. For applications with large amounts of parallelism at both the fine and coarse grained levels, the multiprocessor microarchitecture outperforms the superscalar architecture by a significant margin. Single-chip multiprocessor architectures have the advantage in that they offer localized implementation of a high-clock rate processor for inherently sequential applications and low latency interprocessor communication for parallel applications.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237140},
 doi = {http://doi.acm.org/10.1145/237090.237140},
 acmid = {237140},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pai:1996:EMC:248209.237142,
 author = {Pai, Vijay S. and Ranganathan, Parthasarathy and Adve, Sarita V. and Harton, Tracy},
 title = {An evaluation of memory consistency models for shared-memory systems with ILP processors},
 abstract = {Relaxed consistency models have been shown to significantly outperform sequential consistency for single-issue, statically scheduled processors with blocking reads. However, current microprocessors aggressively exploit instruction-level parallelism (ILP) using methods such as multiple issue, dynamic scheduling, and non-blocking reads. Researchers have conjectured that two techniques, hardware-controlled non-binding prefetching and speculative loads, have the potential to equalize the hardware performance of memory consistency models on such processors.This paper performs the first detailed quantitative comparison of several implementations of sequential consistency and release consistency optimized for aggressive ILP processors. Our results indicate that hardware prefetching and speculative loads dramatically improve the performance of sequential consistency. However, the gap between sequential consistency and release consistency depends on the cache write policy and the complexity of the cache-coherence protocol implementation. In most cases, release consistency significantly outperforms sequential consistency, but for two applications, the use of a write-back primary cache and a more complex cache-coherence protocol nearly equalizes the performance of the two models.We also observe that the existing techniques, which require on-chip hardware modifications, enhance the performance of release consistency only to a small extent. We propose two new software techniques --- fuzzy acquires</i> and selective acquires</i> --- to achieve more overlap than allowed by the previous implementations of release consistency. To enhance methods for overlapping acquires, we also propose a technique to eliminate control dependences caused by an acquire loop, using a small amount of off-chip hardware called the synchronization buffer.</i>},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {12--23},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237142},
 doi = {http://doi.acm.org/10.1145/248209.237142},
 acmid = {237142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pai:1996:EMC:237090.237142,
 author = {Pai, Vijay S. and Ranganathan, Parthasarathy and Adve, Sarita V. and Harton, Tracy},
 title = {An evaluation of memory consistency models for shared-memory systems with ILP processors},
 abstract = {Relaxed consistency models have been shown to significantly outperform sequential consistency for single-issue, statically scheduled processors with blocking reads. However, current microprocessors aggressively exploit instruction-level parallelism (ILP) using methods such as multiple issue, dynamic scheduling, and non-blocking reads. Researchers have conjectured that two techniques, hardware-controlled non-binding prefetching and speculative loads, have the potential to equalize the hardware performance of memory consistency models on such processors.This paper performs the first detailed quantitative comparison of several implementations of sequential consistency and release consistency optimized for aggressive ILP processors. Our results indicate that hardware prefetching and speculative loads dramatically improve the performance of sequential consistency. However, the gap between sequential consistency and release consistency depends on the cache write policy and the complexity of the cache-coherence protocol implementation. In most cases, release consistency significantly outperforms sequential consistency, but for two applications, the use of a write-back primary cache and a more complex cache-coherence protocol nearly equalizes the performance of the two models.We also observe that the existing techniques, which require on-chip hardware modifications, enhance the performance of release consistency only to a small extent. We propose two new software techniques --- fuzzy acquires</i> and selective acquires</i> --- to achieve more overlap than allowed by the previous implementations of release consistency. To enhance methods for overlapping acquires, we also propose a technique to eliminate control dependences caused by an acquire loop, using a small amount of off-chip hardware called the synchronization buffer.</i>},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {12--23},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237142},
 doi = {http://doi.acm.org/10.1145/237090.237142},
 acmid = {237142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pai:1996:EMC:248208.237142,
 author = {Pai, Vijay S. and Ranganathan, Parthasarathy and Adve, Sarita V. and Harton, Tracy},
 title = {An evaluation of memory consistency models for shared-memory systems with ILP processors},
 abstract = {Relaxed consistency models have been shown to significantly outperform sequential consistency for single-issue, statically scheduled processors with blocking reads. However, current microprocessors aggressively exploit instruction-level parallelism (ILP) using methods such as multiple issue, dynamic scheduling, and non-blocking reads. Researchers have conjectured that two techniques, hardware-controlled non-binding prefetching and speculative loads, have the potential to equalize the hardware performance of memory consistency models on such processors.This paper performs the first detailed quantitative comparison of several implementations of sequential consistency and release consistency optimized for aggressive ILP processors. Our results indicate that hardware prefetching and speculative loads dramatically improve the performance of sequential consistency. However, the gap between sequential consistency and release consistency depends on the cache write policy and the complexity of the cache-coherence protocol implementation. In most cases, release consistency significantly outperforms sequential consistency, but for two applications, the use of a write-back primary cache and a more complex cache-coherence protocol nearly equalizes the performance of the two models.We also observe that the existing techniques, which require on-chip hardware modifications, enhance the performance of release consistency only to a small extent. We propose two new software techniques --- fuzzy acquires</i> and selective acquires</i> --- to achieve more overlap than allowed by the previous implementations of release consistency. To enhance methods for overlapping acquires, we also propose a technique to eliminate control dependences caused by an acquire loop, using a small amount of off-chip hardware called the synchronization buffer.</i>},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {12--23},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237142},
 doi = {http://doi.acm.org/10.1145/248208.237142},
 acmid = {237142},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Scott:1996:SCT:248208.237144,
 author = {Scott, Steven L.},
 title = {Synchronization and communication in the T3E multiprocessor},
 abstract = {This paper describes the synchronization and communication primitives of the Cray T3E multiprocessor, a shared memory system scalable to 2048 processors. We discuss what we have learned from the T3D project (the predecessor to the T3E) and the rationale behind changes made for the T3E. We include performance measurements for various aspects of communication and synchronization.The T3E augments the memory interface of the DEC 21164 microprocessor with a large set of explicitly-managed, external registers (E-registers). E-registers are used as the source or target for all remote communication. They provide a highly pipelined interface to global memory that allows dozens of requests per processor to be outstanding. Through E-registers, the T3E provides a rich set of atomic memory operations and a flexible, user-level messaging facility. The T3E also provides a set of virtual hardware barrier/eureka networks that can be arbitrarily embedded into the 3D torus interconnect.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {26--36},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248208.237144},
 doi = {http://doi.acm.org/10.1145/248208.237144},
 acmid = {237144},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Scott:1996:SCT:237090.237144,
 author = {Scott, Steven L.},
 title = {Synchronization and communication in the T3E multiprocessor},
 abstract = {This paper describes the synchronization and communication primitives of the Cray T3E multiprocessor, a shared memory system scalable to 2048 processors. We discuss what we have learned from the T3D project (the predecessor to the T3E) and the rationale behind changes made for the T3E. We include performance measurements for various aspects of communication and synchronization.The T3E augments the memory interface of the DEC 21164 microprocessor with a large set of explicitly-managed, external registers (E-registers). E-registers are used as the source or target for all remote communication. They provide a highly pipelined interface to global memory that allows dozens of requests per processor to be outstanding. Through E-registers, the T3E provides a rich set of atomic memory operations and a flexible, user-level messaging facility. The T3E also provides a set of virtual hardware barrier/eureka networks that can be arbitrarily embedded into the 3D torus interconnect.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {26--36},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237144},
 doi = {http://doi.acm.org/10.1145/237090.237144},
 acmid = {237144},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Scott:1996:SCT:248209.237144,
 author = {Scott, Steven L.},
 title = {Synchronization and communication in the T3E multiprocessor},
 abstract = {This paper describes the synchronization and communication primitives of the Cray T3E multiprocessor, a shared memory system scalable to 2048 processors. We discuss what we have learned from the T3D project (the predecessor to the T3E) and the rationale behind changes made for the T3E. We include performance measurements for various aspects of communication and synchronization.The T3E augments the memory interface of the DEC 21164 microprocessor with a large set of explicitly-managed, external registers (E-registers). E-registers are used as the source or target for all remote communication. They provide a highly pipelined interface to global memory that allows dozens of requests per processor to be outstanding. Through E-registers, the T3E provides a rich set of atomic memory operations and a flexible, user-level messaging facility. The T3E also provides a set of virtual hardware barrier/eureka networks that can be arbitrarily embedded into the 3D torus interconnect.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {26--36},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248209.237144},
 doi = {http://doi.acm.org/10.1145/248209.237144},
 acmid = {237144},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krishnamurthy:1996:EAS:248208.237147,
 author = {Krishnamurthy, Arvind and Schauser, Klaus E. and Scheiman, Chris J. and Wang, Randolph Y. and Culler, David E. and Yelick, Katherine},
 title = {Evaluation of architectural support for global address-based communication in large-scale parallel machines},
 abstract = {Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic evaluation of a broad spectrum of current design alternatives based on our implementations of a global address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes a range of compilation strategies that make varying use of the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family of interacting issues that determine the performance trade-offs in each implementation, quantify the resulting latency, overhead, and bandwidth of the global access operations, and demonstrate the effects on application performance.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237147},
 doi = {http://doi.acm.org/10.1145/248208.237147},
 acmid = {237147},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krishnamurthy:1996:EAS:248209.237147,
 author = {Krishnamurthy, Arvind and Schauser, Klaus E. and Scheiman, Chris J. and Wang, Randolph Y. and Culler, David E. and Yelick, Katherine},
 title = {Evaluation of architectural support for global address-based communication in large-scale parallel machines},
 abstract = {Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic evaluation of a broad spectrum of current design alternatives based on our implementations of a global address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes a range of compilation strategies that make varying use of the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family of interacting issues that determine the performance trade-offs in each implementation, quantify the resulting latency, overhead, and bandwidth of the global access operations, and demonstrate the effects on application performance.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237147},
 doi = {http://doi.acm.org/10.1145/248209.237147},
 acmid = {237147},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krishnamurthy:1996:EAS:237090.237147,
 author = {Krishnamurthy, Arvind and Schauser, Klaus E. and Scheiman, Chris J. and Wang, Randolph Y. and Culler, David E. and Yelick, Katherine},
 title = {Evaluation of architectural support for global address-based communication in large-scale parallel machines},
 abstract = {Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic evaluation of a broad spectrum of current design alternatives based on our implementations of a global address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes a range of compilation strategies that make varying use of the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family of interacting issues that determine the performance trade-offs in each implementation, quantify the resulting latency, overhead, and bandwidth of the global access operations, and demonstrate the effects on application performance.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237147},
 doi = {http://doi.acm.org/10.1145/237090.237147},
 acmid = {237147},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Grunwald:1996:WOT:248209.237149,
 author = {Grunwald, Dirk and Neves, Richard},
 title = {Whole-program optimization for time and space efficient threads},
 abstract = {Modern languages and operating systems often encourage programmers to use threads,</i> or independent control streams, to mask the overhead of some operations and simplify program structure. Multitasking operating systems use threads to mask communication latency, either with hardwares devices or users. Client-server applications typically use threads to simplify the complex control-flow that arises when multiple clients are used. Recently, the scientific computing community has started using threads to mask network communication latency in massively parallel architectures, allowing computation and communication to be overlapped. Lastly, some architectures implement threads in hardware, using those threads to tolerate memory latency.In general, it would be desirable if threaded programs could be written to expose the largest degree of parallelism possible, or to simplify the program design. However, threads incur time and space overheads, and programmers often compromise simple designs for performance. In this paper, we show how to reduce time and space thread overhead using control flow and register liveness information inferred after</i> compilation. Our techniques work on binaries, are not specific to a particular compiler or thread library and reduce the the overall execution time of fine-grain threaded programs by \&amp;asymp; 15-30\%. We use execution-driven analysis and an instrumented operating system to show why the execution time is reduced and to indicate areas for future work.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237149},
 doi = {http://doi.acm.org/10.1145/248209.237149},
 acmid = {237149},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Grunwald:1996:WOT:248208.237149,
 author = {Grunwald, Dirk and Neves, Richard},
 title = {Whole-program optimization for time and space efficient threads},
 abstract = {Modern languages and operating systems often encourage programmers to use threads,</i> or independent control streams, to mask the overhead of some operations and simplify program structure. Multitasking operating systems use threads to mask communication latency, either with hardwares devices or users. Client-server applications typically use threads to simplify the complex control-flow that arises when multiple clients are used. Recently, the scientific computing community has started using threads to mask network communication latency in massively parallel architectures, allowing computation and communication to be overlapped. Lastly, some architectures implement threads in hardware, using those threads to tolerate memory latency.In general, it would be desirable if threaded programs could be written to expose the largest degree of parallelism possible, or to simplify the program design. However, threads incur time and space overheads, and programmers often compromise simple designs for performance. In this paper, we show how to reduce time and space thread overhead using control flow and register liveness information inferred after</i> compilation. Our techniques work on binaries, are not specific to a particular compiler or thread library and reduce the the overall execution time of fine-grain threaded programs by \&amp;asymp; 15-30\%. We use execution-driven analysis and an instrumented operating system to show why the execution time is reduced and to indicate areas for future work.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237149},
 doi = {http://doi.acm.org/10.1145/248208.237149},
 acmid = {237149},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Grunwald:1996:WOT:237090.237149,
 author = {Grunwald, Dirk and Neves, Richard},
 title = {Whole-program optimization for time and space efficient threads},
 abstract = {Modern languages and operating systems often encourage programmers to use threads,</i> or independent control streams, to mask the overhead of some operations and simplify program structure. Multitasking operating systems use threads to mask communication latency, either with hardwares devices or users. Client-server applications typically use threads to simplify the complex control-flow that arises when multiple clients are used. Recently, the scientific computing community has started using threads to mask network communication latency in massively parallel architectures, allowing computation and communication to be overlapped. Lastly, some architectures implement threads in hardware, using those threads to tolerate memory latency.In general, it would be desirable if threaded programs could be written to expose the largest degree of parallelism possible, or to simplify the program design. However, threads incur time and space overheads, and programmers often compromise simple designs for performance. In this paper, we show how to reduce time and space thread overhead using control flow and register liveness information inferred after</i> compilation. Our techniques work on binaries, are not specific to a particular compiler or thread library and reduce the the overall execution time of fine-grain threaded programs by \&amp;asymp; 15-30\%. We use execution-driven analysis and an instrumented operating system to show why the execution time is reduced and to indicate areas for future work.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237149},
 doi = {http://doi.acm.org/10.1145/237090.237149},
 acmid = {237149},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Philbin:1996:TSC:248209.237151,
 author = {Philbin, James and Edler, Jan and Anshus, Otto J. and Douglas, Craig C. and Li, Kai},
 title = {Thread scheduling for cache locality},
 abstract = {This paper describes a method to improve the cache locality of sequential programs by scheduling fine-grained threads. The algorithm relies upon hints provided at the time of thread creation to determine a thread execution order likely to reduce cache misses. This technique may be particularly valuable when compiler-directed tiling is not feasible. Experiments with several application programs, on two systems with different cache structures, show that our thread scheduling method can improve program performance by reducing second-level cache misses.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {60--71},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237151},
 doi = {http://doi.acm.org/10.1145/248209.237151},
 acmid = {237151},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Philbin:1996:TSC:237090.237151,
 author = {Philbin, James and Edler, Jan and Anshus, Otto J. and Douglas, Craig C. and Li, Kai},
 title = {Thread scheduling for cache locality},
 abstract = {This paper describes a method to improve the cache locality of sequential programs by scheduling fine-grained threads. The algorithm relies upon hints provided at the time of thread creation to determine a thread execution order likely to reduce cache misses. This technique may be particularly valuable when compiler-directed tiling is not feasible. Experiments with several application programs, on two systems with different cache structures, show that our thread scheduling method can improve program performance by reducing second-level cache misses.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {60--71},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237151},
 doi = {http://doi.acm.org/10.1145/237090.237151},
 acmid = {237151},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Philbin:1996:TSC:248208.237151,
 author = {Philbin, James and Edler, Jan and Anshus, Otto J. and Douglas, Craig C. and Li, Kai},
 title = {Thread scheduling for cache locality},
 abstract = {This paper describes a method to improve the cache locality of sequential programs by scheduling fine-grained threads. The algorithm relies upon hints provided at the time of thread creation to determine a thread execution order likely to reduce cache misses. This technique may be particularly valuable when compiler-directed tiling is not feasible. Experiments with several application programs, on two systems with different cache structures, show that our thread scheduling method can improve program performance by reducing second-level cache misses.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {60--71},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237151},
 doi = {http://doi.acm.org/10.1145/248208.237151},
 acmid = {237151},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1996:RFC:248209.237154,
 author = {Chen, Peter M. and Ng, Wee Teck and Chandra, Subhachandra and Aycock, Christopher and Rajamani, Gurushankar and Lowell, David},
 title = {The Rio file cache: surviving operating system crashes},
 abstract = {One of the fundamental limits to high-performance, high-reliability file systems is memory's vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve the best of both worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To achieve reliability, we protect memory during a crash and restore it during a reboot (a "warm" reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system. Adding protection makes memory even safer</i> than a write-through file system while adding essentially no overhead. By eliminating reliability-induced disk writes, Rio performs 4-22 times as fast as a write-through file system, 2-14 times as fast as a standard Unix file system, and 1-3 times as fast as an optimized system that risks losing 30 seconds of data and metadata.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {74--83},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237154},
 doi = {http://doi.acm.org/10.1145/248209.237154},
 acmid = {237154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1996:RFC:237090.237154,
 author = {Chen, Peter M. and Ng, Wee Teck and Chandra, Subhachandra and Aycock, Christopher and Rajamani, Gurushankar and Lowell, David},
 title = {The Rio file cache: surviving operating system crashes},
 abstract = {One of the fundamental limits to high-performance, high-reliability file systems is memory's vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve the best of both worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To achieve reliability, we protect memory during a crash and restore it during a reboot (a "warm" reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system. Adding protection makes memory even safer</i> than a write-through file system while adding essentially no overhead. By eliminating reliability-induced disk writes, Rio performs 4-22 times as fast as a write-through file system, 2-14 times as fast as a standard Unix file system, and 1-3 times as fast as an optimized system that risks losing 30 seconds of data and metadata.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {74--83},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237154},
 doi = {http://doi.acm.org/10.1145/237090.237154},
 acmid = {237154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1996:RFC:248208.237154,
 author = {Chen, Peter M. and Ng, Wee Teck and Chandra, Subhachandra and Aycock, Christopher and Rajamani, Gurushankar and Lowell, David},
 title = {The Rio file cache: surviving operating system crashes},
 abstract = {One of the fundamental limits to high-performance, high-reliability file systems is memory's vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve the best of both worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To achieve reliability, we protect memory during a crash and restore it during a reboot (a "warm" reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system. Adding protection makes memory even safer</i> than a write-through file system while adding essentially no overhead. By eliminating reliability-induced disk writes, Rio performs 4-22 times as fast as a write-through file system, 2-14 times as fast as a standard Unix file system, and 1-3 times as fast as an optimized system that risks losing 30 seconds of data and metadata.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {74--83},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237154},
 doi = {http://doi.acm.org/10.1145/248208.237154},
 acmid = {237154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1996:PDV:237090.237157,
 author = {Lee, Edward K. and Thekkath, Chandramohan A.},
 title = {Petal: distributed virtual disks},
 abstract = {The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of network-connected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks.</i> A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal.We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 workstations running Digital Unix and connected by a 155 Mbit/s ATM network. The prototype provides clients with virtual disks that tolerate and recover from disk, server, and network failures. Latency is comparable to a locally attached disk, and throughput scales with the number of servers. The prototype can achieve I/O rates of up to 3150 requests/sec and bandwidth up to 43.1 Mbytes/sec.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {84--92},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/237090.237157},
 doi = {http://doi.acm.org/10.1145/237090.237157},
 acmid = {237157},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1996:PDV:248208.237157,
 author = {Lee, Edward K. and Thekkath, Chandramohan A.},
 title = {Petal: distributed virtual disks},
 abstract = {The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of network-connected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks.</i> A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal.We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 workstations running Digital Unix and connected by a 155 Mbit/s ATM network. The prototype provides clients with virtual disks that tolerate and recover from disk, server, and network failures. Latency is comparable to a locally attached disk, and throughput scales with the number of servers. The prototype can achieve I/O rates of up to 3150 requests/sec and bandwidth up to 43.1 Mbytes/sec.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {84--92},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/248208.237157},
 doi = {http://doi.acm.org/10.1145/248208.237157},
 acmid = {237157},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1996:PDV:248209.237157,
 author = {Lee, Edward K. and Thekkath, Chandramohan A.},
 title = {Petal: distributed virtual disks},
 abstract = {The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of network-connected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks.</i> A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal.We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 workstations running Digital Unix and connected by a 155 Mbit/s ATM network. The prototype provides clients with virtual disks that tolerate and recover from disk, server, and network failures. Latency is comparable to a locally attached disk, and throughput scales with the number of servers. The prototype can achieve I/O rates of up to 3150 requests/sec and bandwidth up to 43.1 Mbytes/sec.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {84--92},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/248209.237157},
 doi = {http://doi.acm.org/10.1145/248209.237157},
 acmid = {237157},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McKinley:1996:QAL:248209.237161,
 author = {McKinley, Kathryn S. and Temam, Olivier},
 title = {A quantitative analysis of loop nest locality},
 abstract = {This paper analyzes and quantifies the locality characteristics of numerical loop nests in order to suggest future directions for architecture and software cache optimizations. Since most programs spend the majority of their time in nests, the vast majority of cache optimization techniques target loop nests. In contrast, the locality characteristics that drive these optimizations are usually collected across the entire application rather than the nest level. Indeed, researchers have studied numerical codes for so long that a number of commonly held assertions have emerged on their locality characteristics. In light of these assertions, we use the Perfect Benchmarks to take a new look at measuring locality on numerical codes based on references, loop nests, and program locality properties. Our results show that several popular assertions are at best overstatements. For example, we find that temporal and spatial reuse have balanced roles within a loop nest and most reuse across nests and the entire program is temporal. These results are consistent with high hit rates, but go against the commonly held assumption that spatial reuse dominates. Another result contrary to popular assumption is that misses within a nest are overwhelmingly conflict misses rather than capacity misses. Capacity misses are a significant source of misses for the entire program, but mostly correspond to potential reuse between different loop nests. Our locality measurements reveal important differences between loop nests and programs; refute some popular assertions; and provide new insights for the compiler writer and the architect.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {94--104},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248209.237161},
 doi = {http://doi.acm.org/10.1145/248209.237161},
 acmid = {237161},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McKinley:1996:QAL:237090.237161,
 author = {McKinley, Kathryn S. and Temam, Olivier},
 title = {A quantitative analysis of loop nest locality},
 abstract = {This paper analyzes and quantifies the locality characteristics of numerical loop nests in order to suggest future directions for architecture and software cache optimizations. Since most programs spend the majority of their time in nests, the vast majority of cache optimization techniques target loop nests. In contrast, the locality characteristics that drive these optimizations are usually collected across the entire application rather than the nest level. Indeed, researchers have studied numerical codes for so long that a number of commonly held assertions have emerged on their locality characteristics. In light of these assertions, we use the Perfect Benchmarks to take a new look at measuring locality on numerical codes based on references, loop nests, and program locality properties. Our results show that several popular assertions are at best overstatements. For example, we find that temporal and spatial reuse have balanced roles within a loop nest and most reuse across nests and the entire program is temporal. These results are consistent with high hit rates, but go against the commonly held assumption that spatial reuse dominates. Another result contrary to popular assumption is that misses within a nest are overwhelmingly conflict misses rather than capacity misses. Capacity misses are a significant source of misses for the entire program, but mostly correspond to potential reuse between different loop nests. Our locality measurements reveal important differences between loop nests and programs; refute some popular assertions; and provide new insights for the compiler writer and the architect.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {94--104},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237161},
 doi = {http://doi.acm.org/10.1145/237090.237161},
 acmid = {237161},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McKinley:1996:QAL:248208.237161,
 author = {McKinley, Kathryn S. and Temam, Olivier},
 title = {A quantitative analysis of loop nest locality},
 abstract = {This paper analyzes and quantifies the locality characteristics of numerical loop nests in order to suggest future directions for architecture and software cache optimizations. Since most programs spend the majority of their time in nests, the vast majority of cache optimization techniques target loop nests. In contrast, the locality characteristics that drive these optimizations are usually collected across the entire application rather than the nest level. Indeed, researchers have studied numerical codes for so long that a number of commonly held assertions have emerged on their locality characteristics. In light of these assertions, we use the Perfect Benchmarks to take a new look at measuring locality on numerical codes based on references, loop nests, and program locality properties. Our results show that several popular assertions are at best overstatements. For example, we find that temporal and spatial reuse have balanced roles within a loop nest and most reuse across nests and the entire program is temporal. These results are consistent with high hit rates, but go against the commonly held assumption that spatial reuse dominates. Another result contrary to popular assumption is that misses within a nest are overwhelmingly conflict misses rather than capacity misses. Capacity misses are a significant source of misses for the entire program, but mostly correspond to potential reuse between different loop nests. Our locality measurements reveal important differences between loop nests and programs; refute some popular assertions; and provide new insights for the compiler writer and the architect.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {94--104},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248208.237161},
 doi = {http://doi.acm.org/10.1145/248208.237161},
 acmid = {237161},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Huang:1996:IBR:237090.237163,
 author = {Huang, Andrew S. and Shen, John Paul},
 title = {The intrinsic bandwidth requirements of ordinary programs},
 abstract = {While there has been an abundance of recent papers on hardware and software approaches to improving the performance of memory accesses, few papers have addressed the problem from the program's point of view. There is a general notion that certain programs have larger working sets than others. However, there is no quantitative method for evaluating and comparing the memory requirements of programs.This paper introduces the bandwidth spectrum</i> for characterizing the memory requirements of a program's instruction and data stream. The bandwidth spectrum measures the average bandwidth requirement of a program as a function of available local memory. These measurements are performed under the most idealized conditions of perfect knowledge and perfect memory management. As such, they represent the lower bounds on the memory requirements of programs. We present the bandwidth spectrums for a set of 22 benchmarks and show how they can be used in the comparison of memory requirements and I/O requirement. The bandwidth spectrums also offer a convenient method to weigh the trade-off amongst instruction issue rate, local memory capacity and bandwidth into local memory.Using the bandwidth spectrum, we show that at issue rates of four or less, bandwidth usually scales linearly with the issue rate. At higher issue rates, bandwidth can often scale superlinearly with respect to issue rate. Finally, we also investigate the effects of varying the input sets on the bandwidth spectrums.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237163},
 doi = {http://doi.acm.org/10.1145/237090.237163},
 acmid = {237163},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Huang:1996:IBR:248209.237163,
 author = {Huang, Andrew S. and Shen, John Paul},
 title = {The intrinsic bandwidth requirements of ordinary programs},
 abstract = {While there has been an abundance of recent papers on hardware and software approaches to improving the performance of memory accesses, few papers have addressed the problem from the program's point of view. There is a general notion that certain programs have larger working sets than others. However, there is no quantitative method for evaluating and comparing the memory requirements of programs.This paper introduces the bandwidth spectrum</i> for characterizing the memory requirements of a program's instruction and data stream. The bandwidth spectrum measures the average bandwidth requirement of a program as a function of available local memory. These measurements are performed under the most idealized conditions of perfect knowledge and perfect memory management. As such, they represent the lower bounds on the memory requirements of programs. We present the bandwidth spectrums for a set of 22 benchmarks and show how they can be used in the comparison of memory requirements and I/O requirement. The bandwidth spectrums also offer a convenient method to weigh the trade-off amongst instruction issue rate, local memory capacity and bandwidth into local memory.Using the bandwidth spectrum, we show that at issue rates of four or less, bandwidth usually scales linearly with the issue rate. At higher issue rates, bandwidth can often scale superlinearly with respect to issue rate. Finally, we also investigate the effects of varying the input sets on the bandwidth spectrums.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237163},
 doi = {http://doi.acm.org/10.1145/248209.237163},
 acmid = {237163},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Huang:1996:IBR:248208.237163,
 author = {Huang, Andrew S. and Shen, John Paul},
 title = {The intrinsic bandwidth requirements of ordinary programs},
 abstract = {While there has been an abundance of recent papers on hardware and software approaches to improving the performance of memory accesses, few papers have addressed the problem from the program's point of view. There is a general notion that certain programs have larger working sets than others. However, there is no quantitative method for evaluating and comparing the memory requirements of programs.This paper introduces the bandwidth spectrum</i> for characterizing the memory requirements of a program's instruction and data stream. The bandwidth spectrum measures the average bandwidth requirement of a program as a function of available local memory. These measurements are performed under the most idealized conditions of perfect knowledge and perfect memory management. As such, they represent the lower bounds on the memory requirements of programs. We present the bandwidth spectrums for a set of 22 benchmarks and show how they can be used in the comparison of memory requirements and I/O requirement. The bandwidth spectrums also offer a convenient method to weigh the trade-off amongst instruction issue rate, local memory capacity and bandwidth into local memory.Using the bandwidth spectrum, we show that at issue rates of four or less, bandwidth usually scales linearly with the issue rate. At higher issue rates, bandwidth can often scale superlinearly with respect to issue rate. Finally, we also investigate the effects of varying the input sets on the bandwidth spectrums.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237163},
 doi = {http://doi.acm.org/10.1145/248208.237163},
 acmid = {237163},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Seznec:1996:MAB:237090.237169,
 author = {Seznec, Andr\'{e} and Jourdan, St\'{e}phan and Sainrat, Pascal and Michaud, Pierre},
 title = {Multiple-block ahead branch predictors},
 abstract = {A basic rule in computer architecture is that a processor cannot execute an application faster than it fetches its instructions. This paper presents a novel cost-effective mechanism called the two-block ahead branch predictor. Information from the current instruction block is not used for predicting the address of the next instruction block, but rather for predicting the block following the next instruction block.This approach overcomes the instruction fetch bottle-neck exhibited by wide-dispatch "brainiac" processors by enabling them to efficiently predict addresses of two instruction blocks in a single cycle. Furthermore, pipelining the branch prediction process can also be done by means of our predictor for "speed demon" processors to achieve higher clock rate or to improve the prediction accuracy by means of bigger prediction structures.Moreover, and unlike the previously-proposed multiple predictor schemes, multiple-block ahead branch predictors can use any of the branch prediction schemes to perform the very accurate predictions required to achieve high-performance on superscalar processors.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237169},
 doi = {http://doi.acm.org/10.1145/237090.237169},
 acmid = {237169},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Seznec:1996:MAB:248208.237169,
 author = {Seznec, Andr\'{e} and Jourdan, St\'{e}phan and Sainrat, Pascal and Michaud, Pierre},
 title = {Multiple-block ahead branch predictors},
 abstract = {A basic rule in computer architecture is that a processor cannot execute an application faster than it fetches its instructions. This paper presents a novel cost-effective mechanism called the two-block ahead branch predictor. Information from the current instruction block is not used for predicting the address of the next instruction block, but rather for predicting the block following the next instruction block.This approach overcomes the instruction fetch bottle-neck exhibited by wide-dispatch "brainiac" processors by enabling them to efficiently predict addresses of two instruction blocks in a single cycle. Furthermore, pipelining the branch prediction process can also be done by means of our predictor for "speed demon" processors to achieve higher clock rate or to improve the prediction accuracy by means of bigger prediction structures.Moreover, and unlike the previously-proposed multiple predictor schemes, multiple-block ahead branch predictors can use any of the branch prediction schemes to perform the very accurate predictions required to achieve high-performance on superscalar processors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237169},
 doi = {http://doi.acm.org/10.1145/248208.237169},
 acmid = {237169},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Seznec:1996:MAB:248209.237169,
 author = {Seznec, Andr\'{e} and Jourdan, St\'{e}phan and Sainrat, Pascal and Michaud, Pierre},
 title = {Multiple-block ahead branch predictors},
 abstract = {A basic rule in computer architecture is that a processor cannot execute an application faster than it fetches its instructions. This paper presents a novel cost-effective mechanism called the two-block ahead branch predictor. Information from the current instruction block is not used for predicting the address of the next instruction block, but rather for predicting the block following the next instruction block.This approach overcomes the instruction fetch bottle-neck exhibited by wide-dispatch "brainiac" processors by enabling them to efficiently predict addresses of two instruction blocks in a single cycle. Furthermore, pipelining the branch prediction process can also be done by means of our predictor for "speed demon" processors to achieve higher clock rate or to improve the prediction accuracy by means of bigger prediction structures.Moreover, and unlike the previously-proposed multiple predictor schemes, multiple-block ahead branch predictors can use any of the branch prediction schemes to perform the very accurate predictions required to achieve high-performance on superscalar processors.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237169},
 doi = {http://doi.acm.org/10.1145/248209.237169},
 acmid = {237169},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1996:ABP:237090.237171,
 author = {Chen, I-Cheng K. and Coffey, John T. and Mudge, Trevor N.},
 title = {Analysis of branch prediction via data compression},
 abstract = {Branch prediction is an important mechanism in modern microprocessor design. The focus of research in this area has been on designing new branch prediction schemes. In contrast, very few studies address the theoretical basis behind these prediction schemes. Knowing this theoretical basis helps us to evaluate how good a prediction scheme is and how much we can expect to improve its accuracy.In this paper, we apply techniques from data compression to establish a theoretical basis for branch prediction, and to illustrate alternatives for further improvement. To establish a theoretical basis, we first introduce a conceptual model to characterize each component in a branch prediction process. Then we show that current "two-level" or correlation based predictors are, in fact, simplifications of an optimal predictor in data compression, Prediction by Partial Matching (PPM).If the information provided to the predictor remains the same, it is unlikely that significant improvements can be expected (asymptotically) from two-level predictors, since PPM is optimal. However, there are a rich set of predictors available from data compression, several of which can still yield some improvement in cases where resources are limited. To illustrate this, we conduct trace-driven simulation running the Instruction Benchmark Suite and the SPEC CINT95 benchmarks. The results show that PPM can outperform a two-level predictor for modest sized branch target buffers.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {128--137},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237171},
 doi = {http://doi.acm.org/10.1145/237090.237171},
 acmid = {237171},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1996:ABP:248208.237171,
 author = {Chen, I-Cheng K. and Coffey, John T. and Mudge, Trevor N.},
 title = {Analysis of branch prediction via data compression},
 abstract = {Branch prediction is an important mechanism in modern microprocessor design. The focus of research in this area has been on designing new branch prediction schemes. In contrast, very few studies address the theoretical basis behind these prediction schemes. Knowing this theoretical basis helps us to evaluate how good a prediction scheme is and how much we can expect to improve its accuracy.In this paper, we apply techniques from data compression to establish a theoretical basis for branch prediction, and to illustrate alternatives for further improvement. To establish a theoretical basis, we first introduce a conceptual model to characterize each component in a branch prediction process. Then we show that current "two-level" or correlation based predictors are, in fact, simplifications of an optimal predictor in data compression, Prediction by Partial Matching (PPM).If the information provided to the predictor remains the same, it is unlikely that significant improvements can be expected (asymptotically) from two-level predictors, since PPM is optimal. However, there are a rich set of predictors available from data compression, several of which can still yield some improvement in cases where resources are limited. To illustrate this, we conduct trace-driven simulation running the Instruction Benchmark Suite and the SPEC CINT95 benchmarks. The results show that PPM can outperform a two-level predictor for modest sized branch target buffers.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {128--137},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237171},
 doi = {http://doi.acm.org/10.1145/248208.237171},
 acmid = {237171},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1996:ABP:248209.237171,
 author = {Chen, I-Cheng K. and Coffey, John T. and Mudge, Trevor N.},
 title = {Analysis of branch prediction via data compression},
 abstract = {Branch prediction is an important mechanism in modern microprocessor design. The focus of research in this area has been on designing new branch prediction schemes. In contrast, very few studies address the theoretical basis behind these prediction schemes. Knowing this theoretical basis helps us to evaluate how good a prediction scheme is and how much we can expect to improve its accuracy.In this paper, we apply techniques from data compression to establish a theoretical basis for branch prediction, and to illustrate alternatives for further improvement. To establish a theoretical basis, we first introduce a conceptual model to characterize each component in a branch prediction process. Then we show that current "two-level" or correlation based predictors are, in fact, simplifications of an optimal predictor in data compression, Prediction by Partial Matching (PPM).If the information provided to the predictor remains the same, it is unlikely that significant improvements can be expected (asymptotically) from two-level predictors, since PPM is optimal. However, there are a rich set of predictors available from data compression, several of which can still yield some improvement in cases where resources are limited. To illustrate this, we conduct trace-driven simulation running the Instruction Benchmark Suite and the SPEC CINT95 benchmarks. The results show that PPM can outperform a two-level predictor for modest sized branch target buffers.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {128--137},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237171},
 doi = {http://doi.acm.org/10.1145/248209.237171},
 acmid = {237171},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lipasti:1996:VLL:237090.237173,
 author = {Lipasti, Mikko H. and Wilkerson, Christopher B. and Shen, John Paul},
 title = {Value locality and load value prediction},
 abstract = {Since the introduction of virtual memory demand-paging and cache memories, computer systems have been exploiting spatial and temporal locality to reduce the average latency of a memory reference. In this paper, we introduce the notion of value locality,</i> a third facet of locality that is frequently present in real-world programs, and describe how to effectively capture and exploit it in order to perform load value prediction.</i> Temporal and spatial locality are attributes of storage locations, and describe the future likelihood of references to those locations or their close neighbors. In a similar vein, value locality</i> describes the likelihood of the recurrence of a previously-seen value within a storage location. Modern processors already exploit value locality</i> in a very restricted sense through the use of control speculation (i.e. branch prediction), which seeks to predict the future value of a single condition bit based on previously-seen values. Our work extends this to predict entire 32- and 64-bit register values based on previously-seen values. We find that, just as condition bits are fairly predictable on a per-static-branch basis, full register values being loaded from memory are frequently predictable as well. Furthermore, we show that simple microarchitectural enhancements to two modern microprocessor implementations (based on the PowerPC 620 and Alpha 21164) that enable load value prediction</i> can effectively exploit value locality</i> to collapse true dependencies, reduce average memory latency and bandwidth requirements, and provide measurable performance gains.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237173},
 doi = {http://doi.acm.org/10.1145/237090.237173},
 acmid = {237173},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lipasti:1996:VLL:248209.237173,
 author = {Lipasti, Mikko H. and Wilkerson, Christopher B. and Shen, John Paul},
 title = {Value locality and load value prediction},
 abstract = {Since the introduction of virtual memory demand-paging and cache memories, computer systems have been exploiting spatial and temporal locality to reduce the average latency of a memory reference. In this paper, we introduce the notion of value locality,</i> a third facet of locality that is frequently present in real-world programs, and describe how to effectively capture and exploit it in order to perform load value prediction.</i> Temporal and spatial locality are attributes of storage locations, and describe the future likelihood of references to those locations or their close neighbors. In a similar vein, value locality</i> describes the likelihood of the recurrence of a previously-seen value within a storage location. Modern processors already exploit value locality</i> in a very restricted sense through the use of control speculation (i.e. branch prediction), which seeks to predict the future value of a single condition bit based on previously-seen values. Our work extends this to predict entire 32- and 64-bit register values based on previously-seen values. We find that, just as condition bits are fairly predictable on a per-static-branch basis, full register values being loaded from memory are frequently predictable as well. Furthermore, we show that simple microarchitectural enhancements to two modern microprocessor implementations (based on the PowerPC 620 and Alpha 21164) that enable load value prediction</i> can effectively exploit value locality</i> to collapse true dependencies, reduce average memory latency and bandwidth requirements, and provide measurable performance gains.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237173},
 doi = {http://doi.acm.org/10.1145/248209.237173},
 acmid = {237173},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lipasti:1996:VLL:248208.237173,
 author = {Lipasti, Mikko H. and Wilkerson, Christopher B. and Shen, John Paul},
 title = {Value locality and load value prediction},
 abstract = {Since the introduction of virtual memory demand-paging and cache memories, computer systems have been exploiting spatial and temporal locality to reduce the average latency of a memory reference. In this paper, we introduce the notion of value locality,</i> a third facet of locality that is frequently present in real-world programs, and describe how to effectively capture and exploit it in order to perform load value prediction.</i> Temporal and spatial locality are attributes of storage locations, and describe the future likelihood of references to those locations or their close neighbors. In a similar vein, value locality</i> describes the likelihood of the recurrence of a previously-seen value within a storage location. Modern processors already exploit value locality</i> in a very restricted sense through the use of control speculation (i.e. branch prediction), which seeks to predict the future value of a single condition bit based on previously-seen values. Our work extends this to predict entire 32- and 64-bit register values based on previously-seen values. We find that, just as condition bits are fairly predictable on a per-static-branch basis, full register values being loaded from memory are frequently predictable as well. Furthermore, we show that simple microarchitectural enhancements to two modern microprocessor implementations (based on the PowerPC 620 and Alpha 21164) that enable load value prediction</i> can effectively exploit value locality</i> to collapse true dependencies, reduce average memory latency and bandwidth requirements, and provide measurable performance gains.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237173},
 doi = {http://doi.acm.org/10.1145/248208.237173},
 acmid = {237173},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Romer:1996:SPI:248209.237175,
 author = {Romer, Theodore H. and Lee, Dennis and Voelker, Geoffrey M. and Wolman, Alec and Wong, Wayne A. and Baer, Jean-Loup and Bershad, Brian N. and Levy, Henry M.},
 title = {The structure and performance of interpreters},
 abstract = {Interpreted languages have become increasingly popular due to demands for rapid program development, ease of use, portability, and safety. Beyond the general impression that they are "slow," however, little has been documented about the performance of interpreters as a class of applications.This paper examines interpreter performance by measuring and analyzing interpreters from both software and hardware perspectives. As examples, we measure the MIPSI, Java, Perl, and Tcl interpreters running an array of micro and macro benchmarks on a DEC Alpha platform. Our measurements of these interpreters relate performance to the complexity of the interpreter's virtual machine and demonstrate that native runtime libraries can play a key role in providing good performance. From an architectural perspective, we show that interpreter performance is primarily a function of the interpreter itself and is relatively independent</i> of the application being interpreted. We also demonstrate that high-level interpreters' demands on processor resources are comparable to those of other complex compiled programs, such as gcc. We conclude that interpreters, as a class of applications, do not currently motivate special hardware support for increased performance.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {150--159},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237175},
 doi = {http://doi.acm.org/10.1145/248209.237175},
 acmid = {237175},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Romer:1996:SPI:237090.237175,
 author = {Romer, Theodore H. and Lee, Dennis and Voelker, Geoffrey M. and Wolman, Alec and Wong, Wayne A. and Baer, Jean-Loup and Bershad, Brian N. and Levy, Henry M.},
 title = {The structure and performance of interpreters},
 abstract = {Interpreted languages have become increasingly popular due to demands for rapid program development, ease of use, portability, and safety. Beyond the general impression that they are "slow," however, little has been documented about the performance of interpreters as a class of applications.This paper examines interpreter performance by measuring and analyzing interpreters from both software and hardware perspectives. As examples, we measure the MIPSI, Java, Perl, and Tcl interpreters running an array of micro and macro benchmarks on a DEC Alpha platform. Our measurements of these interpreters relate performance to the complexity of the interpreter's virtual machine and demonstrate that native runtime libraries can play a key role in providing good performance. From an architectural perspective, we show that interpreter performance is primarily a function of the interpreter itself and is relatively independent</i> of the application being interpreted. We also demonstrate that high-level interpreters' demands on processor resources are comparable to those of other complex compiled programs, such as gcc. We conclude that interpreters, as a class of applications, do not currently motivate special hardware support for increased performance.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {150--159},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237175},
 doi = {http://doi.acm.org/10.1145/237090.237175},
 acmid = {237175},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Romer:1996:SPI:248208.237175,
 author = {Romer, Theodore H. and Lee, Dennis and Voelker, Geoffrey M. and Wolman, Alec and Wong, Wayne A. and Baer, Jean-Loup and Bershad, Brian N. and Levy, Henry M.},
 title = {The structure and performance of interpreters},
 abstract = {Interpreted languages have become increasingly popular due to demands for rapid program development, ease of use, portability, and safety. Beyond the general impression that they are "slow," however, little has been documented about the performance of interpreters as a class of applications.This paper examines interpreter performance by measuring and analyzing interpreters from both software and hardware perspectives. As examples, we measure the MIPSI, Java, Perl, and Tcl interpreters running an array of micro and macro benchmarks on a DEC Alpha platform. Our measurements of these interpreters relate performance to the complexity of the interpreter's virtual machine and demonstrate that native runtime libraries can play a key role in providing good performance. From an architectural perspective, we show that interpreter performance is primarily a function of the interpreter itself and is relatively independent</i> of the application being interpreted. We also demonstrate that high-level interpreters' demands on processor resources are comparable to those of other complex compiled programs, such as gcc. We conclude that interpreters, as a class of applications, do not currently motivate special hardware support for increased performance.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {150--159},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237175},
 doi = {http://doi.acm.org/10.1145/248208.237175},
 acmid = {237175},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fox:1996:ANC:248208.237177,
 author = {Fox, Armando and Gribble, Steven D. and Brewer, Eric A. and Amir, Elan},
 title = {Adapting to network and client variability via on-demand dynamic distillation},
 abstract = {The explosive growth of the Internet and the proliferation of smart cellular phones and handheld wireless devices is widening an already large gap between Internet clients. Clients vary in their hardware resources, software sophistication, and quality of connectivity, yet server support for client variation ranges from relatively poor to none at all. In this paper we introduce some design principles that we believe are fundamental to providing "meaningful" Internet access for the entire range of clients. In particular, we show how to perform on-demand datatype-specific lossy compression on semantically typed data, tailoring content to the specific constraints of the client. We instantiate our design principles in a proxy architecture that further exploits typed data to enable application-level management of scarce network resources. Our proxy architecture generalizes previous work addressing all three aspects of client variation by applying well-understood techniques in a novel way, resulting in quantitatively better end-to-end performance, higher quality display output, and new capabilities for low-end clients.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248208.237177},
 doi = {http://doi.acm.org/10.1145/248208.237177},
 acmid = {237177},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fox:1996:ANC:237090.237177,
 author = {Fox, Armando and Gribble, Steven D. and Brewer, Eric A. and Amir, Elan},
 title = {Adapting to network and client variability via on-demand dynamic distillation},
 abstract = {The explosive growth of the Internet and the proliferation of smart cellular phones and handheld wireless devices is widening an already large gap between Internet clients. Clients vary in their hardware resources, software sophistication, and quality of connectivity, yet server support for client variation ranges from relatively poor to none at all. In this paper we introduce some design principles that we believe are fundamental to providing "meaningful" Internet access for the entire range of clients. In particular, we show how to perform on-demand datatype-specific lossy compression on semantically typed data, tailoring content to the specific constraints of the client. We instantiate our design principles in a proxy architecture that further exploits typed data to enable application-level management of scarce network resources. Our proxy architecture generalizes previous work addressing all three aspects of client variation by applying well-understood techniques in a novel way, resulting in quantitatively better end-to-end performance, higher quality display output, and new capabilities for low-end clients.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237177},
 doi = {http://doi.acm.org/10.1145/237090.237177},
 acmid = {237177},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fox:1996:ANC:248209.237177,
 author = {Fox, Armando and Gribble, Steven D. and Brewer, Eric A. and Amir, Elan},
 title = {Adapting to network and client variability via on-demand dynamic distillation},
 abstract = {The explosive growth of the Internet and the proliferation of smart cellular phones and handheld wireless devices is widening an already large gap between Internet clients. Clients vary in their hardware resources, software sophistication, and quality of connectivity, yet server support for client variation ranges from relatively poor to none at all. In this paper we introduce some design principles that we believe are fundamental to providing "meaningful" Internet access for the entire range of clients. In particular, we show how to perform on-demand datatype-specific lossy compression on semantically typed data, tailoring content to the specific constraints of the client. We instantiate our design principles in a proxy architecture that further exploits typed data to enable application-level management of scarce network resources. Our proxy architecture generalizes previous work addressing all three aspects of client variation by applying well-understood techniques in a novel way, resulting in quantitatively better end-to-end performance, higher quality display output, and new capabilities for low-end clients.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {160--170},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248209.237177},
 doi = {http://doi.acm.org/10.1145/248209.237177},
 acmid = {237177},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Scales:1996:SLO:237090.237179,
 author = {Scales, Daniel J. and Gharachorloo, Kourosh and Thekkath, Chandramohan A.},
 title = {Shasta: a low overhead, software-only approach for supporting fine-grain shared memory},
 abstract = {This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software, it also provides tremendous flexibility in supporting different types of cache coherence protocols. We have implemented an efficient cache coherence protocol that incorporates a number of optimizations, including support for multiple communication granularities and use of relaxed memory models. This system is fully functional and runs on a cluster of Alpha workstations.The primary focus of this paper is to describe the techniques used in Shasta to reduce the checking overhead for supporting fine granularity sharing in software. These techniques include careful layout of the shared address space, scheduling the checking code for efficient execution on modern processors, using a simple method that checks loads using only the value loaded, reducing the extra cache misses caused by the checking code, and combining the checks for multiple loads and stores. To characterize the effect of these techniques, we present detailed performance results for the SPLASH-2 applications running on an Alpha processor. Without our optimizations, the checking overheads are excessively high, exceeding 100\% for several applications. However, our techniques are effective in reducing these overheads to a range of 5\% to 35\% for almost all of the applications. We also describe our coherence protocol and present some preliminary results on the parallel performance of several applications running on our workstation cluster. Our experience so far indicates that once the cost of checking memory accesses is reduced using our techniques, the Shasta approach is an attractive software solution for supporting a shared address space with fine-grain access to data.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237179},
 doi = {http://doi.acm.org/10.1145/237090.237179},
 acmid = {237179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Scales:1996:SLO:248209.237179,
 author = {Scales, Daniel J. and Gharachorloo, Kourosh and Thekkath, Chandramohan A.},
 title = {Shasta: a low overhead, software-only approach for supporting fine-grain shared memory},
 abstract = {This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software, it also provides tremendous flexibility in supporting different types of cache coherence protocols. We have implemented an efficient cache coherence protocol that incorporates a number of optimizations, including support for multiple communication granularities and use of relaxed memory models. This system is fully functional and runs on a cluster of Alpha workstations.The primary focus of this paper is to describe the techniques used in Shasta to reduce the checking overhead for supporting fine granularity sharing in software. These techniques include careful layout of the shared address space, scheduling the checking code for efficient execution on modern processors, using a simple method that checks loads using only the value loaded, reducing the extra cache misses caused by the checking code, and combining the checks for multiple loads and stores. To characterize the effect of these techniques, we present detailed performance results for the SPLASH-2 applications running on an Alpha processor. Without our optimizations, the checking overheads are excessively high, exceeding 100\% for several applications. However, our techniques are effective in reducing these overheads to a range of 5\% to 35\% for almost all of the applications. We also describe our coherence protocol and present some preliminary results on the parallel performance of several applications running on our workstation cluster. Our experience so far indicates that once the cost of checking memory accesses is reduced using our techniques, the Shasta approach is an attractive software solution for supporting a shared address space with fine-grain access to data.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237179},
 doi = {http://doi.acm.org/10.1145/248209.237179},
 acmid = {237179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Scales:1996:SLO:248208.237179,
 author = {Scales, Daniel J. and Gharachorloo, Kourosh and Thekkath, Chandramohan A.},
 title = {Shasta: a low overhead, software-only approach for supporting fine-grain shared memory},
 abstract = {This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software, it also provides tremendous flexibility in supporting different types of cache coherence protocols. We have implemented an efficient cache coherence protocol that incorporates a number of optimizations, including support for multiple communication granularities and use of relaxed memory models. This system is fully functional and runs on a cluster of Alpha workstations.The primary focus of this paper is to describe the techniques used in Shasta to reduce the checking overhead for supporting fine granularity sharing in software. These techniques include careful layout of the shared address space, scheduling the checking code for efficient execution on modern processors, using a simple method that checks loads using only the value loaded, reducing the extra cache misses caused by the checking code, and combining the checks for multiple loads and stores. To characterize the effect of these techniques, we present detailed performance results for the SPLASH-2 applications running on an Alpha processor. Without our optimizations, the checking overheads are excessively high, exceeding 100\% for several applications. However, our techniques are effective in reducing these overheads to a range of 5\% to 35\% for almost all of the applications. We also describe our coherence protocol and present some preliminary results on the parallel performance of several applications running on our workstation cluster. Our experience so far indicates that once the cost of checking memory accesses is reduced using our techniques, the Shasta approach is an attractive software solution for supporting a shared address space with fine-grain access to data.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {174--185},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237179},
 doi = {http://doi.acm.org/10.1145/248208.237179},
 acmid = {237179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dwarkadas:1996:ICS:248208.237181,
 author = {Dwarkadas, Sandhya and Cox, Alan L. and Zwaenepoel, Willy},
 title = {An integrated compile-time/run-time software distributed shared memory system},
 abstract = {On a distributed memory machine, hand-coded message passing leads to the most efficient execution, but it is difficult to use. Parallelizing compilers can approach the performance of hand-coded message passing by translating data-parallel programs into message passing programs, but efficient execution is limited to those programs for which precise analysis can be carried out. Shared memory is easier to program than message passing and its domain is not constrained by the limitations of parallelizing compilers, but it lags in performance. Our goal is to close that performance gap while retaining the benefits of shared memory. In other words, our goal is (1) to make shared memory as efficient as message passing, whether hand-coded or compiler-generated, (2) to retain its ease of programming, and (3) to retain the broader class of applications it supports.To this end we have designed and implemented an integrated compile-time and run-time software DSM system. The programming model remains identical to the original pure run-time DSM system. No user intervention is required to obtain the benefits of our system. The compiler computes data access patterns for the individual processors. It then performs a source-to-source transformation, inserting in the program calls to inform the run-time system of the computed data access patterns. The run-time system uses this information to aggregate communication, to aggregate data and synchronization into a single message, to eliminate consistency overhead, and to replace global synchronization with point-to-point synchronization wherever possible.We extended the Parascope programming environment to perform the required analysis, and we augmented the TreadMarks run-time DSM library to take advantage of the analysis. We used six Fortran programs to assess the performance benefits: Jacobi, 3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, each with two different data set sizes. The experiments were run on an 8-node IBM SP/2 using user-space communication. Compiler optimization in conjunction with the augmented run-time system achieves substantial execution time improvements in comparison to the base TreadMarks, ranging from 4\% to 59\% on 8 processors. Relative to message passing implementations of the same applications, the compile-time run-time system is 0-29\% slower than message passing, while the base run-time system is 5-212\% slower. For the five programs that XHPF could parallelize (all except IS), the execution times achieved by the compiler optimized shared memory programs are within 9\% of XHPF.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {186--197},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237181},
 doi = {http://doi.acm.org/10.1145/248208.237181},
 acmid = {237181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dwarkadas:1996:ICS:237090.237181,
 author = {Dwarkadas, Sandhya and Cox, Alan L. and Zwaenepoel, Willy},
 title = {An integrated compile-time/run-time software distributed shared memory system},
 abstract = {On a distributed memory machine, hand-coded message passing leads to the most efficient execution, but it is difficult to use. Parallelizing compilers can approach the performance of hand-coded message passing by translating data-parallel programs into message passing programs, but efficient execution is limited to those programs for which precise analysis can be carried out. Shared memory is easier to program than message passing and its domain is not constrained by the limitations of parallelizing compilers, but it lags in performance. Our goal is to close that performance gap while retaining the benefits of shared memory. In other words, our goal is (1) to make shared memory as efficient as message passing, whether hand-coded or compiler-generated, (2) to retain its ease of programming, and (3) to retain the broader class of applications it supports.To this end we have designed and implemented an integrated compile-time and run-time software DSM system. The programming model remains identical to the original pure run-time DSM system. No user intervention is required to obtain the benefits of our system. The compiler computes data access patterns for the individual processors. It then performs a source-to-source transformation, inserting in the program calls to inform the run-time system of the computed data access patterns. The run-time system uses this information to aggregate communication, to aggregate data and synchronization into a single message, to eliminate consistency overhead, and to replace global synchronization with point-to-point synchronization wherever possible.We extended the Parascope programming environment to perform the required analysis, and we augmented the TreadMarks run-time DSM library to take advantage of the analysis. We used six Fortran programs to assess the performance benefits: Jacobi, 3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, each with two different data set sizes. The experiments were run on an 8-node IBM SP/2 using user-space communication. Compiler optimization in conjunction with the augmented run-time system achieves substantial execution time improvements in comparison to the base TreadMarks, ranging from 4\% to 59\% on 8 processors. Relative to message passing implementations of the same applications, the compile-time run-time system is 0-29\% slower than message passing, while the base run-time system is 5-212\% slower. For the five programs that XHPF could parallelize (all except IS), the execution times achieved by the compiler optimized shared memory programs are within 9\% of XHPF.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {186--197},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237181},
 doi = {http://doi.acm.org/10.1145/237090.237181},
 acmid = {237181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dwarkadas:1996:ICS:248209.237181,
 author = {Dwarkadas, Sandhya and Cox, Alan L. and Zwaenepoel, Willy},
 title = {An integrated compile-time/run-time software distributed shared memory system},
 abstract = {On a distributed memory machine, hand-coded message passing leads to the most efficient execution, but it is difficult to use. Parallelizing compilers can approach the performance of hand-coded message passing by translating data-parallel programs into message passing programs, but efficient execution is limited to those programs for which precise analysis can be carried out. Shared memory is easier to program than message passing and its domain is not constrained by the limitations of parallelizing compilers, but it lags in performance. Our goal is to close that performance gap while retaining the benefits of shared memory. In other words, our goal is (1) to make shared memory as efficient as message passing, whether hand-coded or compiler-generated, (2) to retain its ease of programming, and (3) to retain the broader class of applications it supports.To this end we have designed and implemented an integrated compile-time and run-time software DSM system. The programming model remains identical to the original pure run-time DSM system. No user intervention is required to obtain the benefits of our system. The compiler computes data access patterns for the individual processors. It then performs a source-to-source transformation, inserting in the program calls to inform the run-time system of the computed data access patterns. The run-time system uses this information to aggregate communication, to aggregate data and synchronization into a single message, to eliminate consistency overhead, and to replace global synchronization with point-to-point synchronization wherever possible.We extended the Parascope programming environment to perform the required analysis, and we augmented the TreadMarks run-time DSM library to take advantage of the analysis. We used six Fortran programs to assess the performance benefits: Jacobi, 3D-FFT, Integer Sort, Shallow, Gauss, and Modified Gramm-Schmidt, each with two different data set sizes. The experiments were run on an 8-node IBM SP/2 using user-space communication. Compiler optimization in conjunction with the augmented run-time system achieves substantial execution time improvements in comparison to the base TreadMarks, ranging from 4\% to 59\% on 8 processors. Relative to message passing implementations of the same applications, the compile-time run-time system is 0-29\% slower than message passing, while the base run-time system is 5-212\% slower. For the five programs that XHPF could parallelize (all except IS), the execution times achieved by the compiler optimized shared memory programs are within 9\% of XHPF.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {186--197},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237181},
 doi = {http://doi.acm.org/10.1145/248209.237181},
 acmid = {237181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bianchini:1996:HCL:248208.237185,
 author = {Bianchini, R. and Kontothanassis, L. I. and Pinto, R. and De Maria, M. and Abud, M. and Amorim, C. L.},
 title = {Hiding communication latency and coherence overhead in software DSMs},
 abstract = {In this paper we propose the use of a PCI-based programmable protocol controller</i> for hiding communication and coherence overheads in software DSMs. Our protocol controller provides three different types of overhead tolerance: a) moving basic communication and coherence tasks away from computation processors; b) prefetching of diffs; and c) generating and applying diffs with hardware assistance. We evaluate the isolated and combined impact of these features on the performance of TreadMarks. We also compare performance against two versions of the Shrimp-based AURC protocol. Using detailed execution-driven simulations of a 16-node network of workstations, we show that the greatest performance benefits provided by our protocol controller come from our hardware-supported diffs. Reducing the burden of communication and coherence transactions on the computation processor is also beneficial but to a smaller extent. Prefetching is not always profitable. Our results show that our protocol controller can improve running time performance by up to 50\% for TreadMarks, which means that it can double the TreadMarks speedups. The overlapping implementation of TreadMarks performs as well or better than AURC for 5 of our 6 applications. We conclude that the simple hardware support we propose allows for the implementation of high-performance software DSMs at low cost. Based on this conclusion, we are building the NCP<inf>2</inf> parallel system at COPPE/UFRJ.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {198--209},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237185},
 doi = {http://doi.acm.org/10.1145/248208.237185},
 acmid = {237185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bianchini:1996:HCL:248209.237185,
 author = {Bianchini, R. and Kontothanassis, L. I. and Pinto, R. and De Maria, M. and Abud, M. and Amorim, C. L.},
 title = {Hiding communication latency and coherence overhead in software DSMs},
 abstract = {In this paper we propose the use of a PCI-based programmable protocol controller</i> for hiding communication and coherence overheads in software DSMs. Our protocol controller provides three different types of overhead tolerance: a) moving basic communication and coherence tasks away from computation processors; b) prefetching of diffs; and c) generating and applying diffs with hardware assistance. We evaluate the isolated and combined impact of these features on the performance of TreadMarks. We also compare performance against two versions of the Shrimp-based AURC protocol. Using detailed execution-driven simulations of a 16-node network of workstations, we show that the greatest performance benefits provided by our protocol controller come from our hardware-supported diffs. Reducing the burden of communication and coherence transactions on the computation processor is also beneficial but to a smaller extent. Prefetching is not always profitable. Our results show that our protocol controller can improve running time performance by up to 50\% for TreadMarks, which means that it can double the TreadMarks speedups. The overlapping implementation of TreadMarks performs as well or better than AURC for 5 of our 6 applications. We conclude that the simple hardware support we propose allows for the implementation of high-performance software DSMs at low cost. Based on this conclusion, we are building the NCP<inf>2</inf> parallel system at COPPE/UFRJ.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {198--209},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237185},
 doi = {http://doi.acm.org/10.1145/248209.237185},
 acmid = {237185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bianchini:1996:HCL:237090.237185,
 author = {Bianchini, R. and Kontothanassis, L. I. and Pinto, R. and De Maria, M. and Abud, M. and Amorim, C. L.},
 title = {Hiding communication latency and coherence overhead in software DSMs},
 abstract = {In this paper we propose the use of a PCI-based programmable protocol controller</i> for hiding communication and coherence overheads in software DSMs. Our protocol controller provides three different types of overhead tolerance: a) moving basic communication and coherence tasks away from computation processors; b) prefetching of diffs; and c) generating and applying diffs with hardware assistance. We evaluate the isolated and combined impact of these features on the performance of TreadMarks. We also compare performance against two versions of the Shrimp-based AURC protocol. Using detailed execution-driven simulations of a 16-node network of workstations, we show that the greatest performance benefits provided by our protocol controller come from our hardware-supported diffs. Reducing the burden of communication and coherence transactions on the computation processor is also beneficial but to a smaller extent. Prefetching is not always profitable. Our results show that our protocol controller can improve running time performance by up to 50\% for TreadMarks, which means that it can double the TreadMarks speedups. The overlapping implementation of TreadMarks performs as well or better than AURC for 5 of our 6 applications. We conclude that the simple hardware support we propose allows for the implementation of high-performance software DSMs at low cost. Based on this conclusion, we are building the NCP<inf>2</inf> parallel system at COPPE/UFRJ.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {198--209},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237185},
 doi = {http://doi.acm.org/10.1145/237090.237185},
 acmid = {237185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Erlichson:1996:SAP:248209.237187,
 author = {Erlichson, Andrew and Nuckolls, Neal and Chesson, Greg and Hennessy, John},
 title = {SoftFLASH: analyzing the performance of clustered distributed virtual shared memory},
 abstract = {One potentially attractive way to build large-scale shared-memory machines is to use small-scale to medium-scale shared-memory machines as clusters that are interconnected with an off-the-shelf network. To create a shared-memory programming environment across the clusters, it is possible to use a virtual shared-memory software layer. Because of the low latency and high bandwidth of the interconnect available within each cluster, there are clear advantages in making the clusters as large as possible. The critical question then becomes whether the latency and bandwidth of the top-level network and the software system are sufficient to support the communication demands generated by the clusters.To explore these questions, we have built an aggressive kernel implementation of a virtual shared-memory system using SGI multiprocessors and 100Mbyte/sec HIPPI interconnects. The system obtains speedups on 32 processors (four nodes, eight processors per node plus additional reserved protocol processors) that range from 6.9 on the communication-intensive FFT program to 21.6 on Ocean (both from the SPLASH 2 suite). In general, clustering is effective in reducing internode miss rates, but as the cluster size increases, increases in the remote latency, mostly due to increased TLB synchronization cost, offset the advantages. For communication-intensive applications, such as FFT, the overhead of sending out network requests, the limited network bandwidth, and the long network latency prevent the achievement of good performance. Overall, this approach still appears promising, but our results indicate that large low latency networks may be needed to make cluster-based virtual shared-memory machines broadly useful as large-scale shared-memory multiprocessors.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248209.237187},
 doi = {http://doi.acm.org/10.1145/248209.237187},
 acmid = {237187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Erlichson:1996:SAP:237090.237187,
 author = {Erlichson, Andrew and Nuckolls, Neal and Chesson, Greg and Hennessy, John},
 title = {SoftFLASH: analyzing the performance of clustered distributed virtual shared memory},
 abstract = {One potentially attractive way to build large-scale shared-memory machines is to use small-scale to medium-scale shared-memory machines as clusters that are interconnected with an off-the-shelf network. To create a shared-memory programming environment across the clusters, it is possible to use a virtual shared-memory software layer. Because of the low latency and high bandwidth of the interconnect available within each cluster, there are clear advantages in making the clusters as large as possible. The critical question then becomes whether the latency and bandwidth of the top-level network and the software system are sufficient to support the communication demands generated by the clusters.To explore these questions, we have built an aggressive kernel implementation of a virtual shared-memory system using SGI multiprocessors and 100Mbyte/sec HIPPI interconnects. The system obtains speedups on 32 processors (four nodes, eight processors per node plus additional reserved protocol processors) that range from 6.9 on the communication-intensive FFT program to 21.6 on Ocean (both from the SPLASH 2 suite). In general, clustering is effective in reducing internode miss rates, but as the cluster size increases, increases in the remote latency, mostly due to increased TLB synchronization cost, offset the advantages. For communication-intensive applications, such as FFT, the overhead of sending out network requests, the limited network bandwidth, and the long network latency prevent the achievement of good performance. Overall, this approach still appears promising, but our results indicate that large low latency networks may be needed to make cluster-based virtual shared-memory machines broadly useful as large-scale shared-memory multiprocessors.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237187},
 doi = {http://doi.acm.org/10.1145/237090.237187},
 acmid = {237187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Erlichson:1996:SAP:248208.237187,
 author = {Erlichson, Andrew and Nuckolls, Neal and Chesson, Greg and Hennessy, John},
 title = {SoftFLASH: analyzing the performance of clustered distributed virtual shared memory},
 abstract = {One potentially attractive way to build large-scale shared-memory machines is to use small-scale to medium-scale shared-memory machines as clusters that are interconnected with an off-the-shelf network. To create a shared-memory programming environment across the clusters, it is possible to use a virtual shared-memory software layer. Because of the low latency and high bandwidth of the interconnect available within each cluster, there are clear advantages in making the clusters as large as possible. The critical question then becomes whether the latency and bandwidth of the top-level network and the software system are sufficient to support the communication demands generated by the clusters.To explore these questions, we have built an aggressive kernel implementation of a virtual shared-memory system using SGI multiprocessors and 100Mbyte/sec HIPPI interconnects. The system obtains speedups on 32 processors (four nodes, eight processors per node plus additional reserved protocol processors) that range from 6.9 on the communication-intensive FFT program to 21.6 on Ocean (both from the SPLASH 2 suite). In general, clustering is effective in reducing internode miss rates, but as the cluster size increases, increases in the remote latency, mostly due to increased TLB synchronization cost, offset the advantages. For communication-intensive applications, such as FFT, the overhead of sending out network requests, the limited network bandwidth, and the long network latency prevent the achievement of good performance. Overall, this approach still appears promising, but our results indicate that large low latency networks may be needed to make cluster-based virtual shared-memory machines broadly useful as large-scale shared-memory multiprocessors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {210--220},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248208.237187},
 doi = {http://doi.acm.org/10.1145/248208.237187},
 acmid = {237187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Luk:1996:CPR:237090.237190,
 author = {Luk, Chi-Keung and Mowry, Todd C.},
 title = {Compiler-based prefetching for recursive data structures},
 abstract = {Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. While prefetching has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the fundamental problem in prefetching pointer-based data structures and propose a guideline for devising successful prefetching schemes. Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme (greedy prefetching</i>) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000. Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45\% for the applications we study. In addition, the more sophisticated algorithms (which we currently perform by hand, but which might be implemented in future compilers) can improve performance by as much as twofold. Compared with the only other compiler-based pointer prefetching scheme in the literature, our algorithms offer substantially better performance by avoiding unnecessary overhead and hiding more latency.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237190},
 doi = {http://doi.acm.org/10.1145/237090.237190},
 acmid = {237190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Luk:1996:CPR:248208.237190,
 author = {Luk, Chi-Keung and Mowry, Todd C.},
 title = {Compiler-based prefetching for recursive data structures},
 abstract = {Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. While prefetching has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the fundamental problem in prefetching pointer-based data structures and propose a guideline for devising successful prefetching schemes. Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme (greedy prefetching</i>) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000. Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45\% for the applications we study. In addition, the more sophisticated algorithms (which we currently perform by hand, but which might be implemented in future compilers) can improve performance by as much as twofold. Compared with the only other compiler-based pointer prefetching scheme in the literature, our algorithms offer substantially better performance by avoiding unnecessary overhead and hiding more latency.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237190},
 doi = {http://doi.acm.org/10.1145/248208.237190},
 acmid = {237190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Luk:1996:CPR:248209.237190,
 author = {Luk, Chi-Keung and Mowry, Todd C.},
 title = {Compiler-based prefetching for recursive data structures},
 abstract = {Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. While prefetching has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the fundamental problem in prefetching pointer-based data structures and propose a guideline for devising successful prefetching schemes. Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme (greedy prefetching</i>) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000. Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45\% for the applications we study. In addition, the more sophisticated algorithms (which we currently perform by hand, but which might be implemented in future compilers) can improve performance by as much as twofold. Compared with the only other compiler-based pointer prefetching scheme in the literature, our algorithms offer substantially better performance by avoiding unnecessary overhead and hiding more latency.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {222--233},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237190},
 doi = {http://doi.acm.org/10.1145/248209.237190},
 acmid = {237190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Saghir:1996:EDD:237090.237193,
 author = {Saghir, Mazen A. R. and Chow, Paul and Lee, Corinna G.},
 title = {Exploiting dual data-memory banks in digital signal processors},
 abstract = {Over the past decade, digital signal processors (DSPs) have emerged as the processors of choice for implementing embedded applications in high-volume consumer products. Through their use of specialized hardware features and small chip areas, DSPs provide the high performance necessary for embedded applications at the low costs demanded by the high-volume consumer market. One feature commonly found in DSPs is the use of dual data-memory banks to double the memory system's bandwidth. When coupled with high-order data interleaving, dual memory banks provide the same bandwidth as more costly memory organizations such as a dual-ported memory. However, making effective use of dual memory banks remains difficult, especially for high-level language (HLL) DSP compilers.In this paper, we describe two algorithms --- compaction-based (CB) data partitioning and partial data duplication --- that we developed as part of our research into the effective exploitation of dual data-memory banks in HLL DSP compilers. We show that CB partitioning is an effective technique for exploiting dual data-memory banks, and that partial data duplication can augment CB partitioning in improving execution performance. Our results show that CB partitioning improves the performance of our kernel benchmarks by 13\%-40\% and the performance of our application benchmarks by 3\%-15\%. For one of the application benchmarks, partial data duplication boosts performance from 3\% to 34\%.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {234--243},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237193},
 doi = {http://doi.acm.org/10.1145/237090.237193},
 acmid = {237193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Saghir:1996:EDD:248208.237193,
 author = {Saghir, Mazen A. R. and Chow, Paul and Lee, Corinna G.},
 title = {Exploiting dual data-memory banks in digital signal processors},
 abstract = {Over the past decade, digital signal processors (DSPs) have emerged as the processors of choice for implementing embedded applications in high-volume consumer products. Through their use of specialized hardware features and small chip areas, DSPs provide the high performance necessary for embedded applications at the low costs demanded by the high-volume consumer market. One feature commonly found in DSPs is the use of dual data-memory banks to double the memory system's bandwidth. When coupled with high-order data interleaving, dual memory banks provide the same bandwidth as more costly memory organizations such as a dual-ported memory. However, making effective use of dual memory banks remains difficult, especially for high-level language (HLL) DSP compilers.In this paper, we describe two algorithms --- compaction-based (CB) data partitioning and partial data duplication --- that we developed as part of our research into the effective exploitation of dual data-memory banks in HLL DSP compilers. We show that CB partitioning is an effective technique for exploiting dual data-memory banks, and that partial data duplication can augment CB partitioning in improving execution performance. Our results show that CB partitioning improves the performance of our kernel benchmarks by 13\%-40\% and the performance of our application benchmarks by 3\%-15\%. For one of the application benchmarks, partial data duplication boosts performance from 3\% to 34\%.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {234--243},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237193},
 doi = {http://doi.acm.org/10.1145/248208.237193},
 acmid = {237193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Saghir:1996:EDD:248209.237193,
 author = {Saghir, Mazen A. R. and Chow, Paul and Lee, Corinna G.},
 title = {Exploiting dual data-memory banks in digital signal processors},
 abstract = {Over the past decade, digital signal processors (DSPs) have emerged as the processors of choice for implementing embedded applications in high-volume consumer products. Through their use of specialized hardware features and small chip areas, DSPs provide the high performance necessary for embedded applications at the low costs demanded by the high-volume consumer market. One feature commonly found in DSPs is the use of dual data-memory banks to double the memory system's bandwidth. When coupled with high-order data interleaving, dual memory banks provide the same bandwidth as more costly memory organizations such as a dual-ported memory. However, making effective use of dual memory banks remains difficult, especially for high-level language (HLL) DSP compilers.In this paper, we describe two algorithms --- compaction-based (CB) data partitioning and partial data duplication --- that we developed as part of our research into the effective exploitation of dual data-memory banks in HLL DSP compilers. We show that CB partitioning is an effective technique for exploiting dual data-memory banks, and that partial data duplication can augment CB partitioning in improving execution performance. Our results show that CB partitioning improves the performance of our kernel benchmarks by 13\%-40\% and the performance of our application benchmarks by 3\%-15\%. For one of the application benchmarks, partial data duplication boosts performance from 3\% to 34\%.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {234--243},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237193},
 doi = {http://doi.acm.org/10.1145/248209.237193},
 acmid = {237193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bugnion:1996:CPC:248209.237195,
 author = {Bugnion, Edouard and Anderson, Jennifer M. and Mowry, Todd C. and Rosenblum, Mendel and Lam, Monica S.},
 title = {Compiler-directed page coloring for multiprocessors},
 abstract = {This paper presents a new technique, compiler-directed page coloring,</i> that eliminates conflict misses in multiprocessor applications. It enables applications to make better use of the increased aggregate cache size available in a multiprocessor. This technique uses the compiler's knowledge of the access patterns of the parallelized applications to direct the operating system's virtual memory page mapping strategy. We demonstrate that this technique can lead to significant performance improvements over two commonly used page mapping strategies for machines with either direct-mapped or two-way set-associative caches. We also show that it is complementary to latency-hiding techniques such as prefetching.We implemented compiler-directed page coloring in the SUIF parallelizing compiler and on two commercial operating systems. We applied the technique to the SPEC95fp benchmark suite, a representative set of numeric programs. We used the SimOS machine simulator to analyze the applications and isolate their performance bottlenecks. We also validated these results on a real machine, an eight-processor 350MHz Digital AlphaServer. Compiler-directed page coloring leads to significant performance improvements for several applications. Overall, our technique improves the SPEC95fp rating for eight processors by 8\% over Digital UNIX's page mapping policy and by 20\% over a page coloring, a standard page mapping policy. The SUIF compiler achieves a SPEC95fp ratio of 57.4, the highest ratio to date.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {244--255},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248209.237195},
 doi = {http://doi.acm.org/10.1145/248209.237195},
 acmid = {237195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bugnion:1996:CPC:248208.237195,
 author = {Bugnion, Edouard and Anderson, Jennifer M. and Mowry, Todd C. and Rosenblum, Mendel and Lam, Monica S.},
 title = {Compiler-directed page coloring for multiprocessors},
 abstract = {This paper presents a new technique, compiler-directed page coloring,</i> that eliminates conflict misses in multiprocessor applications. It enables applications to make better use of the increased aggregate cache size available in a multiprocessor. This technique uses the compiler's knowledge of the access patterns of the parallelized applications to direct the operating system's virtual memory page mapping strategy. We demonstrate that this technique can lead to significant performance improvements over two commonly used page mapping strategies for machines with either direct-mapped or two-way set-associative caches. We also show that it is complementary to latency-hiding techniques such as prefetching.We implemented compiler-directed page coloring in the SUIF parallelizing compiler and on two commercial operating systems. We applied the technique to the SPEC95fp benchmark suite, a representative set of numeric programs. We used the SimOS machine simulator to analyze the applications and isolate their performance bottlenecks. We also validated these results on a real machine, an eight-processor 350MHz Digital AlphaServer. Compiler-directed page coloring leads to significant performance improvements for several applications. Overall, our technique improves the SPEC95fp rating for eight processors by 8\% over Digital UNIX's page mapping policy and by 20\% over a page coloring, a standard page mapping policy. The SUIF compiler achieves a SPEC95fp ratio of 57.4, the highest ratio to date.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {244--255},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/248208.237195},
 doi = {http://doi.acm.org/10.1145/248208.237195},
 acmid = {237195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bugnion:1996:CPC:237090.237195,
 author = {Bugnion, Edouard and Anderson, Jennifer M. and Mowry, Todd C. and Rosenblum, Mendel and Lam, Monica S.},
 title = {Compiler-directed page coloring for multiprocessors},
 abstract = {This paper presents a new technique, compiler-directed page coloring,</i> that eliminates conflict misses in multiprocessor applications. It enables applications to make better use of the increased aggregate cache size available in a multiprocessor. This technique uses the compiler's knowledge of the access patterns of the parallelized applications to direct the operating system's virtual memory page mapping strategy. We demonstrate that this technique can lead to significant performance improvements over two commonly used page mapping strategies for machines with either direct-mapped or two-way set-associative caches. We also show that it is complementary to latency-hiding techniques such as prefetching.We implemented compiler-directed page coloring in the SUIF parallelizing compiler and on two commercial operating systems. We applied the technique to the SPEC95fp benchmark suite, a representative set of numeric programs. We used the SimOS machine simulator to analyze the applications and isolate their performance bottlenecks. We also validated these results on a real machine, an eight-processor 350MHz Digital AlphaServer. Compiler-directed page coloring leads to significant performance improvements for several applications. Overall, our technique improves the SPEC95fp rating for eight processors by 8\% over Digital UNIX's page mapping policy and by 20\% over a page coloring, a standard page mapping policy. The SUIF compiler achieves a SPEC95fp ratio of 57.4, the highest ratio to date.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {244--255},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/237090.237195},
 doi = {http://doi.acm.org/10.1145/237090.237195},
 acmid = {237195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jamrozik:1996:RNL:248208.237198,
 author = {Jamrozik, Herv\'{e} A. and Feeley, Michael J. and Voelker, Geoffrey M. and Evans,II, James and Karlin, Anna R. and Levy, Henry M. and Vernon, Mary K.},
 title = {Reducing network latency using subpages in a global memory environment},
 abstract = {New high-speed networks greatly encourage the use of network memory as a cache for virtual memory and file pages, thereby reducing the need for disk access. Because pages are the fundamental transfer and access units in remote memory systems, page size is a key performance factor. Recently, page sizes of modern processors have been increasing in order to provide more TLB coverage and amortize disk access costs. Unfortunately, for high-speed networks, small</i> transfers are needed to provide low latency. This trend in page size is thus at odds with the use of network memory on high-speed networks.This paper studies the use of subpages</i> as a means of reducing transfer size and latency in a remote-memory environment. Using trace-driven simulation, we show how and why subpages reduce latency and improve performance of programs using network memory. Our results show that memory-intensive applications execute up to 1.8 times faster when executing with 1K-byte subpages, when compared to the same applications using full 8K-byte pages in the global memory system. Those same applications using 1K-byte subpages execute up to 4 times faster than they would using the disk for backing store. Using a prototype implementation on the DEC Alpha and AN2 network, we demonstrate how subpages can reduce remote-memory fault time; e.g., our prototype is able to satisfy a fault on a 1K subpage stored in remote memory in 0.5 milliseconds, one third the time of a full page.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {258--267},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248208.237198},
 doi = {http://doi.acm.org/10.1145/248208.237198},
 acmid = {237198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jamrozik:1996:RNL:237090.237198,
 author = {Jamrozik, Herv\'{e} A. and Feeley, Michael J. and Voelker, Geoffrey M. and Evans,II, James and Karlin, Anna R. and Levy, Henry M. and Vernon, Mary K.},
 title = {Reducing network latency using subpages in a global memory environment},
 abstract = {New high-speed networks greatly encourage the use of network memory as a cache for virtual memory and file pages, thereby reducing the need for disk access. Because pages are the fundamental transfer and access units in remote memory systems, page size is a key performance factor. Recently, page sizes of modern processors have been increasing in order to provide more TLB coverage and amortize disk access costs. Unfortunately, for high-speed networks, small</i> transfers are needed to provide low latency. This trend in page size is thus at odds with the use of network memory on high-speed networks.This paper studies the use of subpages</i> as a means of reducing transfer size and latency in a remote-memory environment. Using trace-driven simulation, we show how and why subpages reduce latency and improve performance of programs using network memory. Our results show that memory-intensive applications execute up to 1.8 times faster when executing with 1K-byte subpages, when compared to the same applications using full 8K-byte pages in the global memory system. Those same applications using 1K-byte subpages execute up to 4 times faster than they would using the disk for backing store. Using a prototype implementation on the DEC Alpha and AN2 network, we demonstrate how subpages can reduce remote-memory fault time; e.g., our prototype is able to satisfy a fault on a 1K subpage stored in remote memory in 0.5 milliseconds, one third the time of a full page.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {258--267},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/237090.237198},
 doi = {http://doi.acm.org/10.1145/237090.237198},
 acmid = {237198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jamrozik:1996:RNL:248209.237198,
 author = {Jamrozik, Herv\'{e} A. and Feeley, Michael J. and Voelker, Geoffrey M. and Evans,II, James and Karlin, Anna R. and Levy, Henry M. and Vernon, Mary K.},
 title = {Reducing network latency using subpages in a global memory environment},
 abstract = {New high-speed networks greatly encourage the use of network memory as a cache for virtual memory and file pages, thereby reducing the need for disk access. Because pages are the fundamental transfer and access units in remote memory systems, page size is a key performance factor. Recently, page sizes of modern processors have been increasing in order to provide more TLB coverage and amortize disk access costs. Unfortunately, for high-speed networks, small</i> transfers are needed to provide low latency. This trend in page size is thus at odds with the use of network memory on high-speed networks.This paper studies the use of subpages</i> as a means of reducing transfer size and latency in a remote-memory environment. Using trace-driven simulation, we show how and why subpages reduce latency and improve performance of programs using network memory. Our results show that memory-intensive applications execute up to 1.8 times faster when executing with 1K-byte subpages, when compared to the same applications using full 8K-byte pages in the global memory system. Those same applications using 1K-byte subpages execute up to 4 times faster than they would using the disk for backing store. Using a prototype implementation on the DEC Alpha and AN2 network, we demonstrate how subpages can reduce remote-memory fault time; e.g., our prototype is able to satisfy a fault on a 1K subpage stored in remote memory in 0.5 milliseconds, one third the time of a full page.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {258--267},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/248209.237198},
 doi = {http://doi.acm.org/10.1145/248209.237198},
 acmid = {237198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peir:1996:ICP:248208.237202,
 author = {Peir, Jih-Kwon and Hsu, Windsor W. and Young, Honesty and Ong, Shauchi},
 title = {Improving cache performance with balanced tag and data paths},
 abstract = {There are two concurrent paths in a typical cache access --- one through the data array and the other through the tag array. The path through the data array drives the selected set out of the array. The path through the tag array determines cache hit/miss and, for set-associative caches, selects the appropriate line from within the selected set. In both direct-mapped and set-associative caches, the path through the tag array is significantly longer than that through the data array. In this paper, we propose a path balancing technique help match the delays of the tag and data paths. The basic idea behind this technique is to employ a separate subset of the tag array to decouple the one-to-one relationship between address tags and cache lines so as to achieve a design that provides higher performance. Performance evaluation using both TPC-C and SPEC92 benchmarks shows that this path balancing technique offers impressive improvements in overall system performance over conventional cache designs. For TPC-C, improvements in the range of 6\% to 28\% are possible.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {268--278},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248208.237202},
 doi = {http://doi.acm.org/10.1145/248208.237202},
 acmid = {237202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peir:1996:ICP:237090.237202,
 author = {Peir, Jih-Kwon and Hsu, Windsor W. and Young, Honesty and Ong, Shauchi},
 title = {Improving cache performance with balanced tag and data paths},
 abstract = {There are two concurrent paths in a typical cache access --- one through the data array and the other through the tag array. The path through the data array drives the selected set out of the array. The path through the tag array determines cache hit/miss and, for set-associative caches, selects the appropriate line from within the selected set. In both direct-mapped and set-associative caches, the path through the tag array is significantly longer than that through the data array. In this paper, we propose a path balancing technique help match the delays of the tag and data paths. The basic idea behind this technique is to employ a separate subset of the tag array to decouple the one-to-one relationship between address tags and cache lines so as to achieve a design that provides higher performance. Performance evaluation using both TPC-C and SPEC92 benchmarks shows that this path balancing technique offers impressive improvements in overall system performance over conventional cache designs. For TPC-C, improvements in the range of 6\% to 28\% are possible.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {268--278},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237202},
 doi = {http://doi.acm.org/10.1145/237090.237202},
 acmid = {237202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peir:1996:ICP:248209.237202,
 author = {Peir, Jih-Kwon and Hsu, Windsor W. and Young, Honesty and Ong, Shauchi},
 title = {Improving cache performance with balanced tag and data paths},
 abstract = {There are two concurrent paths in a typical cache access --- one through the data array and the other through the tag array. The path through the data array drives the selected set out of the array. The path through the tag array determines cache hit/miss and, for set-associative caches, selects the appropriate line from within the selected set. In both direct-mapped and set-associative caches, the path through the tag array is significantly longer than that through the data array. In this paper, we propose a path balancing technique help match the delays of the tag and data paths. The basic idea behind this technique is to employ a separate subset of the tag array to decouple the one-to-one relationship between address tags and cache lines so as to achieve a design that provides higher performance. Performance evaluation using both TPC-C and SPEC92 benchmarks shows that this path balancing technique offers impressive improvements in overall system performance over conventional cache designs. For TPC-C, improvements in the range of 6\% to 28\% are possible.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {268--278},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248209.237202},
 doi = {http://doi.acm.org/10.1145/248209.237202},
 acmid = {237202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Verghese:1996:OSS:237090.237205,
 author = {Verghese, Ben and Devine, Scott and Gupta, Anoop and Rosenblum, Mendel},
 title = {Operating system support for improving data locality on CC-NUMA compute servers},
 abstract = {The dominant architecture for the next generation of shared-memory multiprocessors is CC-NUMA (cache-coherent non-uniform memory architecture). These machines are attractive as compute servers because they provide transparent access to local and remote memory. However, the access latency to remote memory is 3 to 5 times the latency to local memory. CC-NOW machines provide the benefits of cache coherence to networks of workstations, at the cost of even higher remote access latency. Given the large remote access latencies of these architectures, data locality is potentially the most important performance issue. Using realistic workloads, we study the performance improvements provided by OS supported dynamic page migration and replication. Analyzing our kernel-based implementation, we provide a detailed breakdown of the costs. We show that sampling of cache misses can be used to reduce cost without compromising performance, and that TLB misses may not be a consistent approximation for cache misses. Finally, our experiments show that dynamic page migration and replication can substantially increase application performance, as much as 30\%, and reduce contention for resources in the NUMA memory system.},
 booktitle = {Proceedings of the seventh international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, United States},
 pages = {279--289},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237205},
 doi = {http://doi.acm.org/10.1145/237090.237205},
 acmid = {237205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Verghese:1996:OSS:248208.237205,
 author = {Verghese, Ben and Devine, Scott and Gupta, Anoop and Rosenblum, Mendel},
 title = {Operating system support for improving data locality on CC-NUMA compute servers},
 abstract = {The dominant architecture for the next generation of shared-memory multiprocessors is CC-NUMA (cache-coherent non-uniform memory architecture). These machines are attractive as compute servers because they provide transparent access to local and remote memory. However, the access latency to remote memory is 3 to 5 times the latency to local memory. CC-NOW machines provide the benefits of cache coherence to networks of workstations, at the cost of even higher remote access latency. Given the large remote access latencies of these architectures, data locality is potentially the most important performance issue. Using realistic workloads, we study the performance improvements provided by OS supported dynamic page migration and replication. Analyzing our kernel-based implementation, we provide a detailed breakdown of the costs. We show that sampling of cache misses can be used to reduce cost without compromising performance, and that TLB misses may not be a consistent approximation for cache misses. Finally, our experiments show that dynamic page migration and replication can substantially increase application performance, as much as 30\%, and reduce contention for resources in the NUMA memory system.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {30},
 issue = {5},
 month = {September},
 year = {1996},
 issn = {0163-5980},
 pages = {279--289},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248208.237205},
 doi = {http://doi.acm.org/10.1145/248208.237205},
 acmid = {237205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Verghese:1996:OSS:248209.237205,
 author = {Verghese, Ben and Devine, Scott and Gupta, Anoop and Rosenblum, Mendel},
 title = {Operating system support for improving data locality on CC-NUMA compute servers},
 abstract = {The dominant architecture for the next generation of shared-memory multiprocessors is CC-NUMA (cache-coherent non-uniform memory architecture). These machines are attractive as compute servers because they provide transparent access to local and remote memory. However, the access latency to remote memory is 3 to 5 times the latency to local memory. CC-NOW machines provide the benefits of cache coherence to networks of workstations, at the cost of even higher remote access latency. Given the large remote access latencies of these architectures, data locality is potentially the most important performance issue. Using realistic workloads, we study the performance improvements provided by OS supported dynamic page migration and replication. Analyzing our kernel-based implementation, we provide a detailed breakdown of the costs. We show that sampling of cache misses can be used to reduce cost without compromising performance, and that TLB misses may not be a consistent approximation for cache misses. Finally, our experiments show that dynamic page migration and replication can substantially increase application performance, as much as 30\%, and reduce contention for resources in the NUMA memory system.},
 journal = {SIGPLAN Not.},
 volume = {31},
 issue = {9},
 month = {September},
 year = {1996},
 issn = {0362-1340},
 pages = {279--289},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/248209.237205},
 doi = {http://doi.acm.org/10.1145/248209.237205},
 acmid = {237205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:1994:SDC:195470.195481,
 author = {Thekkath, Chandramohan A. and Levy, Henry M. and Lazowska, Edward D.},
 title = {Separating data and control transfer in distributed operating systems},
 abstract = {Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet.We believe that these new network and processor technologies will permit tighter coupling of distributed systems at the hardware level, and that distributed systems software should be designed to benefit from that tighter coupling. In this paper, we propose an alternative way of structuring distributed systems that takes advantage of a communication model based on remote network access (reads and writes) to protected memory segments.A key feature of the new structure, directly supported by the communication model, is the separation of data transfer and control transfer. This is in contrast to the structure of traditional distributed systems, which are typically organized using message passing or remote procedure call (RPC). In RPC-style systems, data and control are inextricably linked\&mdash;all RPCs must transfer both data and control, even if the control transfer is unnecessary.We have implemented our model on DECstation hardware connected by an ATM network. We demonstrate how separating data transfer and control transfer can eliminate unnecessary control transfers and facilitate tighter coupling of the client and server. This has the potential to increase performance and reduce server load, which supports scaling in the face of an increasing number of clients. For example, for a small set of file server operations, our analysis shows a 50\% decrease in server load when we switched from a communications mechanism requiring both control transfer and data transfer, to an alternative structure based on pure data transfer.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195481},
 doi = {http://doi.acm.org/10.1145/195470.195481},
 acmid = {195481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thekkath:1994:SDC:195473.195481,
 author = {Thekkath, Chandramohan A. and Levy, Henry M. and Lazowska, Edward D.},
 title = {Separating data and control transfer in distributed operating systems},
 abstract = {Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet.We believe that these new network and processor technologies will permit tighter coupling of distributed systems at the hardware level, and that distributed systems software should be designed to benefit from that tighter coupling. In this paper, we propose an alternative way of structuring distributed systems that takes advantage of a communication model based on remote network access (reads and writes) to protected memory segments.A key feature of the new structure, directly supported by the communication model, is the separation of data transfer and control transfer. This is in contrast to the structure of traditional distributed systems, which are typically organized using message passing or remote procedure call (RPC). In RPC-style systems, data and control are inextricably linked\&mdash;all RPCs must transfer both data and control, even if the control transfer is unnecessary.We have implemented our model on DECstation hardware connected by an ATM network. We demonstrate how separating data transfer and control transfer can eliminate unnecessary control transfers and facilitate tighter coupling of the client and server. This has the potential to increase performance and reduce server load, which supports scaling in the face of an increasing number of clients. For example, for a small set of file server operations, our analysis shows a 50\% decrease in server load when we switched from a communications mechanism requiring both control transfer and data transfer, to an alternative structure based on pure data transfer.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195481},
 doi = {http://doi.acm.org/10.1145/195473.195481},
 acmid = {195481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:1994:SDC:381792.195481,
 author = {Thekkath, Chandramohan A. and Levy, Henry M. and Lazowska, Edward D.},
 title = {Separating data and control transfer in distributed operating systems},
 abstract = {Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet.We believe that these new network and processor technologies will permit tighter coupling of distributed systems at the hardware level, and that distributed systems software should be designed to benefit from that tighter coupling. In this paper, we propose an alternative way of structuring distributed systems that takes advantage of a communication model based on remote network access (reads and writes) to protected memory segments.A key feature of the new structure, directly supported by the communication model, is the separation of data transfer and control transfer. This is in contrast to the structure of traditional distributed systems, which are typically organized using message passing or remote procedure call (RPC). In RPC-style systems, data and control are inextricably linked\&mdash;all RPCs must transfer both data and control, even if the control transfer is unnecessary.We have implemented our model on DECstation hardware connected by an ATM network. We demonstrate how separating data transfer and control transfer can eliminate unnecessary control transfers and facilitate tighter coupling of the client and server. This has the potential to increase performance and reduce server load, which supports scaling in the face of an increasing number of clients. For example, for a small set of file server operations, our analysis shows a 50\% decrease in server load when we switched from a communications mechanism requiring both control transfer and data transfer, to an alternative structure based on pure data transfer.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195481},
 doi = {http://doi.acm.org/10.1145/381792.195481},
 acmid = {195481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chandra:1994:SPM:195473.195485,
 author = {Chandra, Rohit and Devine, Scott and Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Scheduling and page migration for multiprocessor compute servers},
 abstract = {Several cache-coherent shared-memory multiprocessors have been developed that are scalable and offer a very tight coupling between the processing resources. They are therefore quite attractive for use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management, however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of OS scheduling and page migration policies on the performance of such compute servers. Our experiments are done on the Stanford DASH, a distributed-memory cache-coherent multiprocessor. We show that for our multiprogramming workloads consisting of sequential jobs, the traditional Unix scheduling policy does very poorly. In contrast, a policy  incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to two-fold performance improvement. For our workloads consisting of multiple parallel applications, we compare space-sharing policies that divide the processors among the applications to time-slicing policies such as standard Unix or gang scheduling. We show that space-sharing policies can achieve better processor utilization due to the operating point effect, but time-slicing policies benefit strongly from user-level data distribution. Our initial experience with automatic page migration suggests that policies based only on TLB miss information can be quite effective, and useful for addressing the data distribution problems of space-sharing schedulers.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {12--24},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195473.195485},
 doi = {http://doi.acm.org/10.1145/195473.195485},
 acmid = {195485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandra:1994:SPM:195470.195485,
 author = {Chandra, Rohit and Devine, Scott and Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Scheduling and page migration for multiprocessor compute servers},
 abstract = {Several cache-coherent shared-memory multiprocessors have been developed that are scalable and offer a very tight coupling between the processing resources. They are therefore quite attractive for use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management, however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of OS scheduling and page migration policies on the performance of such compute servers. Our experiments are done on the Stanford DASH, a distributed-memory cache-coherent multiprocessor. We show that for our multiprogramming workloads consisting of sequential jobs, the traditional Unix scheduling policy does very poorly. In contrast, a policy  incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to two-fold performance improvement. For our workloads consisting of multiple parallel applications, we compare space-sharing policies that divide the processors among the applications to time-slicing policies such as standard Unix or gang scheduling. We show that space-sharing policies can achieve better processor utilization due to the operating point effect, but time-slicing policies benefit strongly from user-level data distribution. Our initial experience with automatic page migration suggests that policies based only on TLB miss information can be quite effective, and useful for addressing the data distribution problems of space-sharing schedulers.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {12--24},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195470.195485},
 doi = {http://doi.acm.org/10.1145/195470.195485},
 acmid = {195485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandra:1994:SPM:381792.195485,
 author = {Chandra, Rohit and Devine, Scott and Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Scheduling and page migration for multiprocessor compute servers},
 abstract = {Several cache-coherent shared-memory multiprocessors have been developed that are scalable and offer a very tight coupling between the processing resources. They are therefore quite attractive for use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management, however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of OS scheduling and page migration policies on the performance of such compute servers. Our experiments are done on the Stanford DASH, a distributed-memory cache-coherent multiprocessor. We show that for our multiprogramming workloads consisting of sequential jobs, the traditional Unix scheduling policy does very poorly. In contrast, a policy  incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to two-fold performance improvement. For our workloads consisting of multiple parallel applications, we compare space-sharing policies that divide the processors among the applications to time-slicing policies such as standard Unix or gang scheduling. We show that space-sharing policies can achieve better processor utilization due to the operating point effect, but time-slicing policies benefit strongly from user-level data distribution. Our initial experience with automatic page migration suggests that policies based only on TLB miss information can be quite effective, and useful for addressing the data distribution problems of space-sharing schedulers.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {12--24},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/381792.195485},
 doi = {http://doi.acm.org/10.1145/381792.195485},
 acmid = {195485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lim:1994:RSA:195473.195490,
 author = {Lim, Beng-Hong and Agarwal, Anant},
 title = {Reactive synchronization algorithms for multiprocessors},
 abstract = {Synchronization algorithms that are efficient across a wide range of applications and operating conditions are hard to design because their performance depends on unpredictable run-time factors. The designer of a synchronization algorithm has a choice of protocols to use for implementing the synchronization operation. For example, candidate protocols for locks include test-and-set protocols and queueing protocols. Frequently, the best choice of protocols depends on the level of contention: previous research has shown that test-and-set protocols for locks outperform queueing protocols at low contention, while the opposite is true at high contention.This paper investigates reactive synchronization algorithms that dynamically choose protocols in response to  the level of contention. We describe reactive algorithms for spin locks and fetch-and-op that choose among several shared-memory and message-passing protocols. Dynamically choosing protocols presents a challenge: a reactive algorithm needs to select and change protocols efficiently, and has to allow for the possibility that multiple processes may be executing different protocols at the same time. We describe the notion of consensus objects that the reactive algorithms use to preserve correctness in the face of dynamic protocol changes.Experimental measurements demonstrate that reactive algorithms perform close to the best static choice of protocols at all levels of contention. Furthermore, with mixed levels of contention, reactive algorithms  outperform passive algorithms with fixed protocols, provided that contention levels do not change too frequently. Measurements of several parallel applications show that reactive algorithms result in modest performance gains for spin locks and significant gains for fetch-and-op.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {25--35},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195490},
 doi = {http://doi.acm.org/10.1145/195473.195490},
 acmid = {195490},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lim:1994:RSA:381792.195490,
 author = {Lim, Beng-Hong and Agarwal, Anant},
 title = {Reactive synchronization algorithms for multiprocessors},
 abstract = {Synchronization algorithms that are efficient across a wide range of applications and operating conditions are hard to design because their performance depends on unpredictable run-time factors. The designer of a synchronization algorithm has a choice of protocols to use for implementing the synchronization operation. For example, candidate protocols for locks include test-and-set protocols and queueing protocols. Frequently, the best choice of protocols depends on the level of contention: previous research has shown that test-and-set protocols for locks outperform queueing protocols at low contention, while the opposite is true at high contention.This paper investigates reactive synchronization algorithms that dynamically choose protocols in response to  the level of contention. We describe reactive algorithms for spin locks and fetch-and-op that choose among several shared-memory and message-passing protocols. Dynamically choosing protocols presents a challenge: a reactive algorithm needs to select and change protocols efficiently, and has to allow for the possibility that multiple processes may be executing different protocols at the same time. We describe the notion of consensus objects that the reactive algorithms use to preserve correctness in the face of dynamic protocol changes.Experimental measurements demonstrate that reactive algorithms perform close to the best static choice of protocols at all levels of contention. Furthermore, with mixed levels of contention, reactive algorithms  outperform passive algorithms with fixed protocols, provided that contention levels do not change too frequently. Measurements of several parallel applications show that reactive algorithms result in modest performance gains for spin locks and significant gains for fetch-and-op.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {25--35},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195490},
 doi = {http://doi.acm.org/10.1145/381792.195490},
 acmid = {195490},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lim:1994:RSA:195470.195490,
 author = {Lim, Beng-Hong and Agarwal, Anant},
 title = {Reactive synchronization algorithms for multiprocessors},
 abstract = {Synchronization algorithms that are efficient across a wide range of applications and operating conditions are hard to design because their performance depends on unpredictable run-time factors. The designer of a synchronization algorithm has a choice of protocols to use for implementing the synchronization operation. For example, candidate protocols for locks include test-and-set protocols and queueing protocols. Frequently, the best choice of protocols depends on the level of contention: previous research has shown that test-and-set protocols for locks outperform queueing protocols at low contention, while the opposite is true at high contention.This paper investigates reactive synchronization algorithms that dynamically choose protocols in response to  the level of contention. We describe reactive algorithms for spin locks and fetch-and-op that choose among several shared-memory and message-passing protocols. Dynamically choosing protocols presents a challenge: a reactive algorithm needs to select and change protocols efficiently, and has to allow for the possibility that multiple processes may be executing different protocols at the same time. We describe the notion of consensus objects that the reactive algorithms use to preserve correctness in the face of dynamic protocol changes.Experimental measurements demonstrate that reactive algorithms perform close to the best static choice of protocols at all levels of contention. Furthermore, with mixed levels of contention, reactive algorithms  outperform passive algorithms with fixed protocols, provided that contention levels do not change too frequently. Measurements of several parallel applications show that reactive algorithms result in modest performance gains for spin locks and significant gains for fetch-and-op.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {25--35},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195490},
 doi = {http://doi.acm.org/10.1145/195470.195490},
 acmid = {195490},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heinlein:1994:IMP:195470.195494,
 author = {Heinlein, John and Gharachorloo, Kourosh and Dresser, Scott and Gupta, Anoop},
 title = {Integration of message passing and shared memory in the Stanford FLASH multiprocessor},
 abstract = {The advantages of using message passing over shared memory for certain types of communication and synchronization have provided an incentive to integrate both models within a single architecture. A key goal of the FLASH (FLexible Architecture for SHared memory) project at Stanford is to achieve this integration while maintaining a simple and efficient design. This paper presents the hardware and software mechanisms in FLASH to support various message passing protocols. We achieve low overhead message passing by delegating protocol functionality to the programmable node controllers in FLASH and by providing direct user-level access to this messaging subsystem. In contrast to most earlier work, we provide an integrated solution that handles the interaction of the messaging protocols with virtual memory, protected multiprogramming, and cache coherence. Detailed simulation studies indicate that this system can sustain message-transfers rates of several hundred megabytes per second, effectively utilizing projected network bandwidths for next generation multiprocessors.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {38--50},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195470.195494},
 doi = {http://doi.acm.org/10.1145/195470.195494},
 acmid = {195494},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heinlein:1994:IMP:381792.195494,
 author = {Heinlein, John and Gharachorloo, Kourosh and Dresser, Scott and Gupta, Anoop},
 title = {Integration of message passing and shared memory in the Stanford FLASH multiprocessor},
 abstract = {The advantages of using message passing over shared memory for certain types of communication and synchronization have provided an incentive to integrate both models within a single architecture. A key goal of the FLASH (FLexible Architecture for SHared memory) project at Stanford is to achieve this integration while maintaining a simple and efficient design. This paper presents the hardware and software mechanisms in FLASH to support various message passing protocols. We achieve low overhead message passing by delegating protocol functionality to the programmable node controllers in FLASH and by providing direct user-level access to this messaging subsystem. In contrast to most earlier work, we provide an integrated solution that handles the interaction of the messaging protocols with virtual memory, protected multiprogramming, and cache coherence. Detailed simulation studies indicate that this system can sustain message-transfers rates of several hundred megabytes per second, effectively utilizing projected network bandwidths for next generation multiprocessors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {38--50},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/381792.195494},
 doi = {http://doi.acm.org/10.1145/381792.195494},
 acmid = {195494},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heinlein:1994:IMP:195473.195494,
 author = {Heinlein, John and Gharachorloo, Kourosh and Dresser, Scott and Gupta, Anoop},
 title = {Integration of message passing and shared memory in the Stanford FLASH multiprocessor},
 abstract = {The advantages of using message passing over shared memory for certain types of communication and synchronization have provided an incentive to integrate both models within a single architecture. A key goal of the FLASH (FLexible Architecture for SHared memory) project at Stanford is to achieve this integration while maintaining a simple and efficient design. This paper presents the hardware and software mechanisms in FLASH to support various message passing protocols. We achieve low overhead message passing by delegating protocol functionality to the programmable node controllers in FLASH and by providing direct user-level access to this messaging subsystem. In contrast to most earlier work, we provide an integrated solution that handles the interaction of the messaging protocols with virtual memory, protected multiprogramming, and cache coherence. Detailed simulation studies indicate that this system can sustain message-transfers rates of several hundred megabytes per second, effectively utilizing projected network bandwidths for next generation multiprocessors.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {38--50},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195473.195494},
 doi = {http://doi.acm.org/10.1145/195473.195494},
 acmid = {195494},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Karamcheti:1994:SOM:381792.195499,
 author = {Karamcheti, Vijay and Chien, Andrew A.},
 title = {Software overhead in messaging layers: where does the time go?},
 abstract = {Despite improvements in network interfaces and software messaging layers, software communication overhead still dominates the hardware routing cost in most systems. In this study, we identify the sources of this overhead by analyzing software costs of typical communication protocols built atop the active messages layer on the CM-5. We show that up to 50\&ndash;70\% of the software messaging costs are a direct consequence of the gap between specific network features such as arbitrary delivery order, finite buffering, and limited fault-handling, and the user communication requirements of in-order delivery, end-to-end flow control, and reliable transmission. However, virtually all of these costs can be eliminated if routing networks provide higher-level services such as in-order delivery, end-to-end flow control, and packet-level fault-tolerance. We conclude that significant cost reductions require changing the constraints on messaging layers: we propose designing networks and network interfaces which simplify or replace software for implementing user communication requirements.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195499},
 doi = {http://doi.acm.org/10.1145/381792.195499},
 acmid = {195499},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Karamcheti:1994:SOM:195470.195499,
 author = {Karamcheti, Vijay and Chien, Andrew A.},
 title = {Software overhead in messaging layers: where does the time go?},
 abstract = {Despite improvements in network interfaces and software messaging layers, software communication overhead still dominates the hardware routing cost in most systems. In this study, we identify the sources of this overhead by analyzing software costs of typical communication protocols built atop the active messages layer on the CM-5. We show that up to 50\&ndash;70\% of the software messaging costs are a direct consequence of the gap between specific network features such as arbitrary delivery order, finite buffering, and limited fault-handling, and the user communication requirements of in-order delivery, end-to-end flow control, and reliable transmission. However, virtually all of these costs can be eliminated if routing networks provide higher-level services such as in-order delivery, end-to-end flow control, and packet-level fault-tolerance. We conclude that significant cost reductions require changing the constraints on messaging layers: we propose designing networks and network interfaces which simplify or replace software for implementing user communication requirements.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195499},
 doi = {http://doi.acm.org/10.1145/195470.195499},
 acmid = {195499},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Karamcheti:1994:SOM:195473.195499,
 author = {Karamcheti, Vijay and Chien, Andrew A.},
 title = {Software overhead in messaging layers: where does the time go?},
 abstract = {Despite improvements in network interfaces and software messaging layers, software communication overhead still dominates the hardware routing cost in most systems. In this study, we identify the sources of this overhead by analyzing software costs of typical communication protocols built atop the active messages layer on the CM-5. We show that up to 50\&ndash;70\% of the software messaging costs are a direct consequence of the gap between specific network features such as arbitrary delivery order, finite buffering, and limited fault-handling, and the user communication requirements of in-order delivery, end-to-end flow control, and reliable transmission. However, virtually all of these costs can be eliminated if routing networks provide higher-level services such as in-order delivery, end-to-end flow control, and packet-level fault-tolerance. We conclude that significant cost reductions require changing the constraints on messaging layers: we propose designing networks and network interfaces which simplify or replace software for implementing user communication requirements.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195499},
 doi = {http://doi.acm.org/10.1145/195473.195499},
 acmid = {195499},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chandra:1994:TSM:195473.195501,
 author = {Chandra, Satish and Larus, James R. and Rogers, Anne},
 title = {Where is time spent in message-passing and shared-memory programs?},
 abstract = {Message passing and shared memory are two techniques parallel programs use for coordination and communication. This paper studies the strengths and weaknesses of these two mechanisms by comparing equivalent, well-written message-passing and shared-memory programs running on similar hardware. To ensure that our measurements are comparable, we produced two carefully tuned versions of each program and measured them on closely-related simulators of a message-passing and a shared-memory machine, both of which are based on same underlying hardware assumptions.We examined the behavior and performance of each program carefully. Although the cost of computation in each pair of programs was similar, synchronization and communication differed greatly. We found that message-passing's  advantage over shared-memory is not clear-cut. Three of the four shared-memory programs ran at roughly the same speed as their message-passing equivalent, even though their communication patterns were different.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {61--73},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195473.195501},
 doi = {http://doi.acm.org/10.1145/195473.195501},
 acmid = {195501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandra:1994:TSM:381792.195501,
 author = {Chandra, Satish and Larus, James R. and Rogers, Anne},
 title = {Where is time spent in message-passing and shared-memory programs?},
 abstract = {Message passing and shared memory are two techniques parallel programs use for coordination and communication. This paper studies the strengths and weaknesses of these two mechanisms by comparing equivalent, well-written message-passing and shared-memory programs running on similar hardware. To ensure that our measurements are comparable, we produced two carefully tuned versions of each program and measured them on closely-related simulators of a message-passing and a shared-memory machine, both of which are based on same underlying hardware assumptions.We examined the behavior and performance of each program carefully. Although the cost of computation in each pair of programs was similar, synchronization and communication differed greatly. We found that message-passing's  advantage over shared-memory is not clear-cut. Three of the four shared-memory programs ran at roughly the same speed as their message-passing equivalent, even though their communication patterns were different.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {61--73},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/381792.195501},
 doi = {http://doi.acm.org/10.1145/381792.195501},
 acmid = {195501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandra:1994:TSM:195470.195501,
 author = {Chandra, Satish and Larus, James R. and Rogers, Anne},
 title = {Where is time spent in message-passing and shared-memory programs?},
 abstract = {Message passing and shared memory are two techniques parallel programs use for coordination and communication. This paper studies the strengths and weaknesses of these two mechanisms by comparing equivalent, well-written message-passing and shared-memory programs running on similar hardware. To ensure that our measurements are comparable, we produced two carefully tuned versions of each program and measured them on closely-related simulators of a message-passing and a shared-memory machine, both of which are based on same underlying hardware assumptions.We examined the behavior and performance of each program carefully. Although the cost of computation in each pair of programs was similar, synchronization and communication differed greatly. We found that message-passing's  advantage over shared-memory is not clear-cut. Three of the four shared-memory programs ran at roughly the same speed as their message-passing equivalent, even though their communication patterns were different.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {61--73},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195470.195501},
 doi = {http://doi.acm.org/10.1145/195470.195501},
 acmid = {195501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schmidt:1994:PHR:381792.195504,
 author = {Schmidt, William J. and Nilsen, Kelvin D.},
 title = {Performance of a hardware-assisted real-time garbage collector},
 abstract = {Hardware-assisted real-time garbage collection offers high throughput and small worst-case bounds on the times required to allocate dynamic objects and to access the memory contained within previously allocated objects. Whether the proposed technology is cost effective depends on various choices between configuration alternatives. This paper reports the performance of several different configurations of the hardware-assisted real-time garbage collection system subjected to several different workloads. Reported measurements demonstrate that hardware-assisted real-time garbage collection is a viable alternative to traditional explicit memory management techniques, even for low-level languages like C++.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {76--85},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195504},
 doi = {http://doi.acm.org/10.1145/381792.195504},
 acmid = {195504},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schmidt:1994:PHR:195470.195504,
 author = {Schmidt, William J. and Nilsen, Kelvin D.},
 title = {Performance of a hardware-assisted real-time garbage collector},
 abstract = {Hardware-assisted real-time garbage collection offers high throughput and small worst-case bounds on the times required to allocate dynamic objects and to access the memory contained within previously allocated objects. Whether the proposed technology is cost effective depends on various choices between configuration alternatives. This paper reports the performance of several different configurations of the hardware-assisted real-time garbage collection system subjected to several different workloads. Reported measurements demonstrate that hardware-assisted real-time garbage collection is a viable alternative to traditional explicit memory management techniques, even for low-level languages like C++.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {76--85},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195504},
 doi = {http://doi.acm.org/10.1145/195470.195504},
 acmid = {195504},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schmidt:1994:PHR:195473.195504,
 author = {Schmidt, William J. and Nilsen, Kelvin D.},
 title = {Performance of a hardware-assisted real-time garbage collector},
 abstract = {Hardware-assisted real-time garbage collection offers high throughput and small worst-case bounds on the times required to allocate dynamic objects and to access the memory contained within previously allocated objects. Whether the proposed technology is cost effective depends on various choices between configuration alternatives. This paper reports the performance of several different configurations of the hardware-assisted real-time garbage collection system subjected to several different workloads. Reported measurements demonstrate that hardware-assisted real-time garbage collection is a viable alternative to traditional explicit memory management techniques, even for low-level languages like C++.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {76--85},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195504},
 doi = {http://doi.acm.org/10.1145/195473.195504},
 acmid = {195504},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wu:1994:ENM:381792.195506,
 author = {Wu, Michael and Zwaenepoel, Willy},
 title = {eNVy: a non-volatile, main memory storage system},
 abstract = {This paper describes the architecture of eNVy, a large non-volatile main memory storage system built primarily with Flash memory. eNVy presents its storage space as a linear, memory mapped array rather than as an emulated disk in order to provide an efficient and easy to use software interface.Flash memories provide persistent storage with solid-state memory access times at a lower cost than other solid-state technologies. However, they have a number of drawbacks. Flash chips are write-once, bulk-erase devices whose contents cannot be updated in-place. They also suffer from slow program times and a limit on the number of program/erase cycles. eNVy uses a copy-on-write scheme, page remapping, a small amount of battery backed SRAM, and high bandwidth parallel data transfers to provide low latency, in-place update semantics. A cleaning algorithm optimized for large Flash arrays is used to reclaim space. The algorithm is designed to evenly wear the array, thereby extending its lifetime.Software simulations of a 2 gigabyte eNVy system show that it can support I/O rates corresponding to approximately 30,000 transactions per second on the TPC-A database benchmark. Despite the added work done to overcome the deficiencies associated with Flash memories, average latencies to the storage system are as low as 180ns for reads and 200ns for writes. The estimated lifetime of this type of storage system is in the 10 year range when exposed to a workload of 10,000 transactions per second.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381792.195506},
 doi = {http://doi.acm.org/10.1145/381792.195506},
 acmid = {195506},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wu:1994:ENM:195473.195506,
 author = {Wu, Michael and Zwaenepoel, Willy},
 title = {eNVy: a non-volatile, main memory storage system},
 abstract = {This paper describes the architecture of eNVy, a large non-volatile main memory storage system built primarily with Flash memory. eNVy presents its storage space as a linear, memory mapped array rather than as an emulated disk in order to provide an efficient and easy to use software interface.Flash memories provide persistent storage with solid-state memory access times at a lower cost than other solid-state technologies. However, they have a number of drawbacks. Flash chips are write-once, bulk-erase devices whose contents cannot be updated in-place. They also suffer from slow program times and a limit on the number of program/erase cycles. eNVy uses a copy-on-write scheme, page remapping, a small amount of battery backed SRAM, and high bandwidth parallel data transfers to provide low latency, in-place update semantics. A cleaning algorithm optimized for large Flash arrays is used to reclaim space. The algorithm is designed to evenly wear the array, thereby extending its lifetime.Software simulations of a 2 gigabyte eNVy system show that it can support I/O rates corresponding to approximately 30,000 transactions per second on the TPC-A database benchmark. Despite the added work done to overcome the deficiencies associated with Flash memories, average latencies to the storage system are as low as 180ns for reads and 200ns for writes. The estimated lifetime of this type of storage system is in the 10 year range when exposed to a workload of 10,000 transactions per second.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195473.195506},
 doi = {http://doi.acm.org/10.1145/195473.195506},
 acmid = {195506},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wu:1994:ENM:195470.195506,
 author = {Wu, Michael and Zwaenepoel, Willy},
 title = {eNVy: a non-volatile, main memory storage system},
 abstract = {This paper describes the architecture of eNVy, a large non-volatile main memory storage system built primarily with Flash memory. eNVy presents its storage space as a linear, memory mapped array rather than as an emulated disk in order to provide an efficient and easy to use software interface.Flash memories provide persistent storage with solid-state memory access times at a lower cost than other solid-state technologies. However, they have a number of drawbacks. Flash chips are write-once, bulk-erase devices whose contents cannot be updated in-place. They also suffer from slow program times and a limit on the number of program/erase cycles. eNVy uses a copy-on-write scheme, page remapping, a small amount of battery backed SRAM, and high bandwidth parallel data transfers to provide low latency, in-place update semantics. A cleaning algorithm optimized for large Flash arrays is used to reclaim space. The algorithm is designed to evenly wear the array, thereby extending its lifetime.Software simulations of a 2 gigabyte eNVy system show that it can support I/O rates corresponding to approximately 30,000 transactions per second on the TPC-A database benchmark. Despite the added work done to overcome the deficiencies associated with Flash memories, average latencies to the storage system are as low as 180ns for reads and 200ns for writes. The estimated lifetime of this type of storage system is in the 10 year range when exposed to a workload of 10,000 transactions per second.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195470.195506},
 doi = {http://doi.acm.org/10.1145/195470.195506},
 acmid = {195506},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Upton:1994:RAH:195470.195510,
 author = {Upton, Michael and Huff, Thomas and Mudge, Trevor and Brown, Richard},
 title = {Resource allocation in a high clock rate microprocessor},
 abstract = {This paper discusses the design of a high clock rate (300MHz) processor. The architecture is described, and the goals for the design are explained. The performance of three processor models is evaluated using trace-driven simulation. A cost model is used to estimate the resources required to build processors with varying sizes of on-chip memories, in both single and dual issue models. Recommendations are then made to increase the effectiveness of each of the models.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195470.195510},
 doi = {http://doi.acm.org/10.1145/195470.195510},
 acmid = {195510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decoupled architecture, floating point latencies, nonblocking cache, pipelining, prefetching, resource allocation, superscalar},
} 

@inproceedings{Upton:1994:RAH:195473.195510,
 author = {Upton, Michael and Huff, Thomas and Mudge, Trevor and Brown, Richard},
 title = {Resource allocation in a high clock rate microprocessor},
 abstract = {This paper discusses the design of a high clock rate (300MHz) processor. The architecture is described, and the goals for the design are explained. The performance of three processor models is evaluated using trace-driven simulation. A cost model is used to estimate the resources required to build processors with varying sizes of on-chip memories, in both single and dual issue models. Recommendations are then made to increase the effectiveness of each of the models.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195473.195510},
 doi = {http://doi.acm.org/10.1145/195473.195510},
 acmid = {195510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decoupled architecture, floating point latencies, nonblocking cache, pipelining, prefetching, resource allocation, superscalar},
} 

@article{Upton:1994:RAH:381792.195510,
 author = {Upton, Michael and Huff, Thomas and Mudge, Trevor and Brown, Richard},
 title = {Resource allocation in a high clock rate microprocessor},
 abstract = {This paper discusses the design of a high clock rate (300MHz) processor. The architecture is described, and the goals for the design are explained. The performance of three processor models is evaluated using trace-driven simulation. A cost model is used to estimate the resources required to build processors with varying sizes of on-chip memories, in both single and dual issue models. Recommendations are then made to increase the effectiveness of each of the models.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381792.195510},
 doi = {http://doi.acm.org/10.1145/381792.195510},
 acmid = {195510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decoupled architecture, floating point latencies, nonblocking cache, pipelining, prefetching, resource allocation, superscalar},
} 

@article{Thekkath:1994:HSS:195470.195515,
 author = {Thekkath, Chandramohan A. and Levy, Henry M.},
 title = {Hardware and software support for efficient exception handling},
 abstract = {Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms.We propose both hardware  and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exception-delivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits of that mechanism for several example applications.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {110--119},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195515},
 doi = {http://doi.acm.org/10.1145/195470.195515},
 acmid = {195515},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thekkath:1994:HSS:195473.195515,
 author = {Thekkath, Chandramohan A. and Levy, Henry M.},
 title = {Hardware and software support for efficient exception handling},
 abstract = {Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms.We propose both hardware  and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exception-delivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits of that mechanism for several example applications.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {110--119},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195515},
 doi = {http://doi.acm.org/10.1145/195473.195515},
 acmid = {195515},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:1994:HSS:381792.195515,
 author = {Thekkath, Chandramohan A. and Levy, Henry M.},
 title = {Hardware and software support for efficient exception handling},
 abstract = {Program-synchronous exceptions, for example, breakpoints, watchpoints, illegal opcodes, and memory access violations, provide information about exceptional conditions, interrupting the program and vectoring to an operating system handler. Over the last decade, however, programs and run-time systems have increasingly employed these mechanisms as a performance optimization to detect normal and expected conditions. Unfortunately, current architecture and operating system structures are designed for exceptional or erroneous conditions, where performance is of secondary importance, rather than normal conditions. Consequently, this has limited the practicality of such hardware-based detection mechanisms.We propose both hardware  and software structures that permit efficient handling of synchronous exceptions by user-level code. We demonstrate a software implementation that reduces exception-delivery cost by an order-of-magnitude on current RISC processors, and show the performance benefits of that mechanism for several example applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {110--119},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195515},
 doi = {http://doi.acm.org/10.1145/381792.195515},
 acmid = {195515},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Argade:1994:TMR:195470.195518,
 author = {Argade, Pramod V. and Charles, David K. and Taylor, Craig},
 title = {A technique for monitoring run-time dynamics of an operating system and a microprocessor executing user applications},
 abstract = {In this paper, we present a non-invasive and efficient technique for simulating applications complete with their operating system interaction. The technique involves booting and initiating an application on a hardware development system, capturing the entire state of the application and the microprocessor at a well defined point in execution and then simulating the application on microprocessor simulators. Extensive statistics generated from the simulators on run-time dynamics of the application, the operating system as well as the microprocessor enabled us to tune the operating system and the microprocessor architecture and implementation. The results also enabled us to optimize system level design choices by anticipating/predicting the performance of the target system. Lastly, the results were used to adjust and refocus the evolution of the architecture of both the operating system and the microprocessor.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195518},
 doi = {http://doi.acm.org/10.1145/195470.195518},
 acmid = {195518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Argade:1994:TMR:195473.195518,
 author = {Argade, Pramod V. and Charles, David K. and Taylor, Craig},
 title = {A technique for monitoring run-time dynamics of an operating system and a microprocessor executing user applications},
 abstract = {In this paper, we present a non-invasive and efficient technique for simulating applications complete with their operating system interaction. The technique involves booting and initiating an application on a hardware development system, capturing the entire state of the application and the microprocessor at a well defined point in execution and then simulating the application on microprocessor simulators. Extensive statistics generated from the simulators on run-time dynamics of the application, the operating system as well as the microprocessor enabled us to tune the operating system and the microprocessor architecture and implementation. The results also enabled us to optimize system level design choices by anticipating/predicting the performance of the target system. Lastly, the results were used to adjust and refocus the evolution of the architecture of both the operating system and the microprocessor.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195518},
 doi = {http://doi.acm.org/10.1145/195473.195518},
 acmid = {195518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Argade:1994:TMR:381792.195518,
 author = {Argade, Pramod V. and Charles, David K. and Taylor, Craig},
 title = {A technique for monitoring run-time dynamics of an operating system and a microprocessor executing user applications},
 abstract = {In this paper, we present a non-invasive and efficient technique for simulating applications complete with their operating system interaction. The technique involves booting and initiating an application on a hardware development system, capturing the entire state of the application and the microprocessor at a well defined point in execution and then simulating the application on microprocessor simulators. Extensive statistics generated from the simulators on run-time dynamics of the application, the operating system as well as the microprocessor enabled us to tune the operating system and the microprocessor architecture and implementation. The results also enabled us to optimize system level design choices by anticipating/predicting the performance of the target system. Lastly, the results were used to adjust and refocus the evolution of the architecture of both the operating system and the microprocessor.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195518},
 doi = {http://doi.acm.org/10.1145/381792.195518},
 acmid = {195518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Uhlig:1994:TST:195470.195521,
 author = {Uhlig, Richard and Nagle, David and Mudge, Trevor and Sechrest, Stuart},
 title = {Trap-driven simulation with Tapeworm II},
 abstract = {Tapeworm II is a software-based simulation tool that evaluates the cache and TLB performance of multiple-task and operating system intensive workloads. Tapeworm resides in an OS kernel and causes a host machine's hardware to drive simulations with kernel traps instead of with address traces, as is conventionally done. This allows Tapeworm to quickly and accurately capture complete memory referencing behavior with a limited degradation in overall system performance. This paper compares trap-driven simulation, as implemented in Tapeworm, with the more common technique of trace-driven memory simulation with respect to speed, accuracy, portability and flexibility.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {132--144},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195470.195521},
 doi = {http://doi.acm.org/10.1145/195470.195521},
 acmid = {195521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TLB, cache, memory system, trace-driven simulation, trap-driven simulation},
} 

@article{Uhlig:1994:TST:381792.195521,
 author = {Uhlig, Richard and Nagle, David and Mudge, Trevor and Sechrest, Stuart},
 title = {Trap-driven simulation with Tapeworm II},
 abstract = {Tapeworm II is a software-based simulation tool that evaluates the cache and TLB performance of multiple-task and operating system intensive workloads. Tapeworm resides in an OS kernel and causes a host machine's hardware to drive simulations with kernel traps instead of with address traces, as is conventionally done. This allows Tapeworm to quickly and accurately capture complete memory referencing behavior with a limited degradation in overall system performance. This paper compares trap-driven simulation, as implemented in Tapeworm, with the more common technique of trace-driven memory simulation with respect to speed, accuracy, portability and flexibility.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {132--144},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/381792.195521},
 doi = {http://doi.acm.org/10.1145/381792.195521},
 acmid = {195521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TLB, cache, memory system, trace-driven simulation, trap-driven simulation},
} 

@inproceedings{Uhlig:1994:TST:195473.195521,
 author = {Uhlig, Richard and Nagle, David and Mudge, Trevor and Sechrest, Stuart},
 title = {Trap-driven simulation with Tapeworm II},
 abstract = {Tapeworm II is a software-based simulation tool that evaluates the cache and TLB performance of multiple-task and operating system intensive workloads. Tapeworm resides in an OS kernel and causes a host machine's hardware to drive simulations with kernel traps instead of with address traces, as is conventionally done. This allows Tapeworm to quickly and accurately capture complete memory referencing behavior with a limited degradation in overall system performance. This paper compares trap-driven simulation, as implemented in Tapeworm, with the more common technique of trace-driven memory simulation with respect to speed, accuracy, portability and flexibility.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {132--144},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195473.195521},
 doi = {http://doi.acm.org/10.1145/195473.195521},
 acmid = {195521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TLB, cache, memory system, trace-driven simulation, trap-driven simulation},
} 

@article{Maynard:1994:CCC:381792.195524,
 author = {Maynard, Ann Marie Grizzaffi and Donnelly, Colette M. and Olszewski, Bret R.},
 title = {Contrasting characteristics and cache performance of technical and multi-user commercial workloads},
 abstract = {Experience has shown that many widely used benchmarks are poor predictors of the performance of systems running commercial applications. Research into this anomaly has long been hampered by a lack of address traces from representative multi-user commercial workloads. This paper presents research, using traces of industry-standard commercial benchmarks, which examines the characteristic differences between technical and commercial workloads and illustrates how those differences affect cache performance.Commercial and technical environments differ in their respective branch behavior, operating system activity, I/O, and dispatching characteristics. A wide range of uniprocessor instruction and data cache geometries were studied. The instruction cache results for commercial  workloads demonstrate that instruction cache performance can no longer be neglected because these workloads have much larger code working sets than technical applications. For database workloads, a breakdown of kernel and user behavior reveals that the application component can exhibit behavior similar to the operating system and therefore, can experience miss rates equally high. This paper also indicates that ``dispatching" or process switching characteristics must be considered when designing level-two caches. The data presented shows that increasing the associativity of second-level caches can reduce miss rates significantly. Overall, the results of this research should help system designers choose a cache configuration that will perform well in commercial markets.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381792.195524},
 doi = {http://doi.acm.org/10.1145/381792.195524},
 acmid = {195524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache performance, commercial workloads, memory subsystems, operating system activity, technical applications},
} 

@inproceedings{Maynard:1994:CCC:195473.195524,
 author = {Maynard, Ann Marie Grizzaffi and Donnelly, Colette M. and Olszewski, Bret R.},
 title = {Contrasting characteristics and cache performance of technical and multi-user commercial workloads},
 abstract = {Experience has shown that many widely used benchmarks are poor predictors of the performance of systems running commercial applications. Research into this anomaly has long been hampered by a lack of address traces from representative multi-user commercial workloads. This paper presents research, using traces of industry-standard commercial benchmarks, which examines the characteristic differences between technical and commercial workloads and illustrates how those differences affect cache performance.Commercial and technical environments differ in their respective branch behavior, operating system activity, I/O, and dispatching characteristics. A wide range of uniprocessor instruction and data cache geometries were studied. The instruction cache results for commercial  workloads demonstrate that instruction cache performance can no longer be neglected because these workloads have much larger code working sets than technical applications. For database workloads, a breakdown of kernel and user behavior reveals that the application component can exhibit behavior similar to the operating system and therefore, can experience miss rates equally high. This paper also indicates that ``dispatching" or process switching characteristics must be considered when designing level-two caches. The data presented shows that increasing the associativity of second-level caches can reduce miss rates significantly. Overall, the results of this research should help system designers choose a cache configuration that will perform well in commercial markets.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195473.195524},
 doi = {http://doi.acm.org/10.1145/195473.195524},
 acmid = {195524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache performance, commercial workloads, memory subsystems, operating system activity, technical applications},
} 

@article{Maynard:1994:CCC:195470.195524,
 author = {Maynard, Ann Marie Grizzaffi and Donnelly, Colette M. and Olszewski, Bret R.},
 title = {Contrasting characteristics and cache performance of technical and multi-user commercial workloads},
 abstract = {Experience has shown that many widely used benchmarks are poor predictors of the performance of systems running commercial applications. Research into this anomaly has long been hampered by a lack of address traces from representative multi-user commercial workloads. This paper presents research, using traces of industry-standard commercial benchmarks, which examines the characteristic differences between technical and commercial workloads and illustrates how those differences affect cache performance.Commercial and technical environments differ in their respective branch behavior, operating system activity, I/O, and dispatching characteristics. A wide range of uniprocessor instruction and data cache geometries were studied. The instruction cache results for commercial  workloads demonstrate that instruction cache performance can no longer be neglected because these workloads have much larger code working sets than technical applications. For database workloads, a breakdown of kernel and user behavior reveals that the application component can exhibit behavior similar to the operating system and therefore, can experience miss rates equally high. This paper also indicates that ``dispatching" or process switching characteristics must be considered when designing level-two caches. The data presented shows that increasing the associativity of second-level caches can reduce miss rates significantly. Overall, the results of this research should help system designers choose a cache configuration that will perform well in commercial markets.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195470.195524},
 doi = {http://doi.acm.org/10.1145/195470.195524},
 acmid = {195524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache performance, commercial workloads, memory subsystems, operating system activity, technical applications},
} 

@inproceedings{Bershad:1994:ACM:195473.195527,
 author = {Bershad, Brian N. and Lee, Dennis and Romer, Theodore H. and Chen, J. Bradley},
 title = {Avoiding conflict misses dynamically in large direct-mapped caches},
 abstract = {This paper describes a method for improving the performance of a large direct-mapped cache by reducing the number of conflict misses. Our solution consists of two components: an inexpensive hardware device called a Cache Miss Lookaside (CML) buffer that detects conflicts by recording and summarizing a history of cache misses, and a software policy within the operating system's virtual memory system that removes conflicts by dynamically remapping pages whenever large numbers of conflict misses are detected. Using trace-driven simulation of applications and the operating system, we show that a CML buffer enables a large direct-mapped cache to perform nearly as well as a two-way set associative cache of equivalent size and speed, although with lower hardware cost and complexity.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {158--170},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195473.195527},
 doi = {http://doi.acm.org/10.1145/195473.195527},
 acmid = {195527},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bershad:1994:ACM:381792.195527,
 author = {Bershad, Brian N. and Lee, Dennis and Romer, Theodore H. and Chen, J. Bradley},
 title = {Avoiding conflict misses dynamically in large direct-mapped caches},
 abstract = {This paper describes a method for improving the performance of a large direct-mapped cache by reducing the number of conflict misses. Our solution consists of two components: an inexpensive hardware device called a Cache Miss Lookaside (CML) buffer that detects conflicts by recording and summarizing a history of cache misses, and a software policy within the operating system's virtual memory system that removes conflicts by dynamically remapping pages whenever large numbers of conflict misses are detected. Using trace-driven simulation of applications and the operating system, we show that a CML buffer enables a large direct-mapped cache to perform nearly as well as a two-way set associative cache of equivalent size and speed, although with lower hardware cost and complexity.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {158--170},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/381792.195527},
 doi = {http://doi.acm.org/10.1145/381792.195527},
 acmid = {195527},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bershad:1994:ACM:195470.195527,
 author = {Bershad, Brian N. and Lee, Dennis and Romer, Theodore H. and Chen, J. Bradley},
 title = {Avoiding conflict misses dynamically in large direct-mapped caches},
 abstract = {This paper describes a method for improving the performance of a large direct-mapped cache by reducing the number of conflict misses. Our solution consists of two components: an inexpensive hardware device called a Cache Miss Lookaside (CML) buffer that detects conflicts by recording and summarizing a history of cache misses, and a software policy within the operating system's virtual memory system that removes conflicts by dynamically remapping pages whenever large numbers of conflict misses are detected. Using trace-driven simulation of applications and the operating system, we show that a CML buffer enables a large direct-mapped cache to perform nearly as well as a two-way set associative cache of equivalent size and speed, although with lower hardware cost and complexity.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {158--170},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/195470.195527},
 doi = {http://doi.acm.org/10.1145/195470.195527},
 acmid = {195527},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Talluri:1994:STP:381792.195531,
 author = {Talluri, Madhusudhan and Hill, Mark D.},
 title = {Surpassing the TLB performance of superpages with less operating system support},
 abstract = {Many commercial microprocessor architectures have added translation lookaside buffer (TLB) support for superpages. Superpages differ from segments because their size must be a power of two multiple of the base page size and they must be aligned in both virtual and physical address spaces. Very large superpages (e.g., 1MB) are clearly useful for mapping special structures, such as kernel data or frame buffers. This paper considers the architectural and operating system support required to exploit medium-sized superpages (e.g., 64KB, i.e., sixteen times a 4KB base page size). First, we show that superpages improve TLB performance only after invasive operating system modifications that introduce considerable overhead.We then propose two  subblock TLB designs as alternate ways to improve TLB performance. Analogous to a subblock cache, a complete-subblock TLB associates a tag with a superpage-sized region but has valid bits, physical page number, attributes, etc., for each possible base page mapping. A partial-subblock TLB entry is much smaller than a complete-subblock TLB entry, because it shares physical page number and attribute fields across base page mappings. A drawback of a partial-subblock TLB is that base page mappings can share a TLB entry only if they map to consecutive physical pages and have the same attributes. We propose a physical memory allocation algorithm, page reservation, that makes this sharing more likely. When page reservation is used, experimental  results show partial-subblock TLBs perform better than superpage TLBs, while requiring simpler operating system changes. If operating system changes are inappropriate, however, complete-subblock TLBs perform best.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381792.195531},
 doi = {http://doi.acm.org/10.1145/381792.195531},
 acmid = {195531},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Talluri:1994:STP:195473.195531,
 author = {Talluri, Madhusudhan and Hill, Mark D.},
 title = {Surpassing the TLB performance of superpages with less operating system support},
 abstract = {Many commercial microprocessor architectures have added translation lookaside buffer (TLB) support for superpages. Superpages differ from segments because their size must be a power of two multiple of the base page size and they must be aligned in both virtual and physical address spaces. Very large superpages (e.g., 1MB) are clearly useful for mapping special structures, such as kernel data or frame buffers. This paper considers the architectural and operating system support required to exploit medium-sized superpages (e.g., 64KB, i.e., sixteen times a 4KB base page size). First, we show that superpages improve TLB performance only after invasive operating system modifications that introduce considerable overhead.We then propose two  subblock TLB designs as alternate ways to improve TLB performance. Analogous to a subblock cache, a complete-subblock TLB associates a tag with a superpage-sized region but has valid bits, physical page number, attributes, etc., for each possible base page mapping. A partial-subblock TLB entry is much smaller than a complete-subblock TLB entry, because it shares physical page number and attribute fields across base page mappings. A drawback of a partial-subblock TLB is that base page mappings can share a TLB entry only if they map to consecutive physical pages and have the same attributes. We propose a physical memory allocation algorithm, page reservation, that makes this sharing more likely. When page reservation is used, experimental  results show partial-subblock TLBs perform better than superpage TLBs, while requiring simpler operating system changes. If operating system changes are inappropriate, however, complete-subblock TLBs perform best.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195473.195531},
 doi = {http://doi.acm.org/10.1145/195473.195531},
 acmid = {195531},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Talluri:1994:STP:195470.195531,
 author = {Talluri, Madhusudhan and Hill, Mark D.},
 title = {Surpassing the TLB performance of superpages with less operating system support},
 abstract = {Many commercial microprocessor architectures have added translation lookaside buffer (TLB) support for superpages. Superpages differ from segments because their size must be a power of two multiple of the base page size and they must be aligned in both virtual and physical address spaces. Very large superpages (e.g., 1MB) are clearly useful for mapping special structures, such as kernel data or frame buffers. This paper considers the architectural and operating system support required to exploit medium-sized superpages (e.g., 64KB, i.e., sixteen times a 4KB base page size). First, we show that superpages improve TLB performance only after invasive operating system modifications that introduce considerable overhead.We then propose two  subblock TLB designs as alternate ways to improve TLB performance. Analogous to a subblock cache, a complete-subblock TLB associates a tag with a superpage-sized region but has valid bits, physical page number, attributes, etc., for each possible base page mapping. A partial-subblock TLB entry is much smaller than a complete-subblock TLB entry, because it shares physical page number and attribute fields across base page mappings. A drawback of a partial-subblock TLB is that base page mappings can share a TLB entry only if they map to consecutive physical pages and have the same attributes. We propose a physical memory allocation algorithm, page reservation, that makes this sharing more likely. When page reservation is used, experimental  results show partial-subblock TLBs perform better than superpage TLBs, while requiring simpler operating system changes. If operating system changes are inappropriate, however, complete-subblock TLBs perform best.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195470.195531},
 doi = {http://doi.acm.org/10.1145/195470.195531},
 acmid = {195531},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gallagher:1994:DMD:195470.195534,
 author = {Gallagher, David M. and Chen, William Y. and Mahlke, Scott A. and Gyllenhaal, John C. and Hwu, Wen-mei W.},
 title = {Dynamic memory disambiguation using the memory conflict buffer},
 abstract = {To exploit instruction level parallelism, compilers for VLIW and superscalar processors often employ static code scheduling. However, the available code reordering may be severely restricted due to ambiguous dependences between memory instructions. This paper introduces a simple hardware mechanism, referred to as the memory conflict buffer, which facilitates static code scheduling in the presence of memory store/load dependences. Correct program execution is ensured by the memory conflict buffer and repair code provided by the compiler. With this addition, significant speedup over an aggressive code scheduling model can be achieved for both non-numerical and numerical programs.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {183--193},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195534},
 doi = {http://doi.acm.org/10.1145/195470.195534},
 acmid = {195534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gallagher:1994:DMD:381792.195534,
 author = {Gallagher, David M. and Chen, William Y. and Mahlke, Scott A. and Gyllenhaal, John C. and Hwu, Wen-mei W.},
 title = {Dynamic memory disambiguation using the memory conflict buffer},
 abstract = {To exploit instruction level parallelism, compilers for VLIW and superscalar processors often employ static code scheduling. However, the available code reordering may be severely restricted due to ambiguous dependences between memory instructions. This paper introduces a simple hardware mechanism, referred to as the memory conflict buffer, which facilitates static code scheduling in the presence of memory store/load dependences. Correct program execution is ensured by the memory conflict buffer and repair code provided by the compiler. With this addition, significant speedup over an aggressive code scheduling model can be achieved for both non-numerical and numerical programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {183--193},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195534},
 doi = {http://doi.acm.org/10.1145/381792.195534},
 acmid = {195534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gallagher:1994:DMD:195473.195534,
 author = {Gallagher, David M. and Chen, William Y. and Mahlke, Scott A. and Gyllenhaal, John C. and Hwu, Wen-mei W.},
 title = {Dynamic memory disambiguation using the memory conflict buffer},
 abstract = {To exploit instruction level parallelism, compilers for VLIW and superscalar processors often employ static code scheduling. However, the available code reordering may be severely restricted due to ambiguous dependences between memory instructions. This paper introduces a simple hardware mechanism, referred to as the memory conflict buffer, which facilitates static code scheduling in the presence of memory store/load dependences. Correct program execution is ensured by the memory conflict buffer and repair code provided by the compiler. With this addition, significant speedup over an aggressive code scheduling model can be achieved for both non-numerical and numerical programs.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {183--193},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195534},
 doi = {http://doi.acm.org/10.1145/195473.195534},
 acmid = {195534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hayashi:1994:AAS:195470.195538,
 author = {Hayashi, Kenichi and Doi, Tsunehisa and Horie, Takeshi and Koyanagi, Yoichi and Shiraki, Osamu and Imamura, Nobutaka and Shimizu, Toshiyuki and Ishihata, Hiroaki and Shindo, Tatsuya},
 title = {AP1000+: architectural support of PUT/GET interface for parallelizing compiler},
 abstract = {The scalability of distributed-memory parallel computers makes them attractive candidates for solving large-scale problems. New languages, such as HPF, FortranD, and VPP Fortran, have been developed to enable existing software to be easily ported to such machines. Many distributed-memory parallel computers have been built, but none of them support the mechanisms required by such languages. We studied the mechanisms required by parallelizing compilers and proposed a new architecture to support them. Based on this proposed architecture, we developed a new distributed-memory parallel computer, the AP1000+, which is an enhanced version of the AP1000. Using scientific applications in VPP Fortran and C, such as NAS parallel benchmarks, we simulated the performance of the AP1000+.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195470.195538},
 doi = {http://doi.acm.org/10.1145/195470.195538},
 acmid = {195538},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hayashi:1994:AAS:381792.195538,
 author = {Hayashi, Kenichi and Doi, Tsunehisa and Horie, Takeshi and Koyanagi, Yoichi and Shiraki, Osamu and Imamura, Nobutaka and Shimizu, Toshiyuki and Ishihata, Hiroaki and Shindo, Tatsuya},
 title = {AP1000+: architectural support of PUT/GET interface for parallelizing compiler},
 abstract = {The scalability of distributed-memory parallel computers makes them attractive candidates for solving large-scale problems. New languages, such as HPF, FortranD, and VPP Fortran, have been developed to enable existing software to be easily ported to such machines. Many distributed-memory parallel computers have been built, but none of them support the mechanisms required by such languages. We studied the mechanisms required by parallelizing compilers and proposed a new architecture to support them. Based on this proposed architecture, we developed a new distributed-memory parallel computer, the AP1000+, which is an enhanced version of the AP1000. Using scientific applications in VPP Fortran and C, such as NAS parallel benchmarks, we simulated the performance of the AP1000+.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381792.195538},
 doi = {http://doi.acm.org/10.1145/381792.195538},
 acmid = {195538},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hayashi:1994:AAS:195473.195538,
 author = {Hayashi, Kenichi and Doi, Tsunehisa and Horie, Takeshi and Koyanagi, Yoichi and Shiraki, Osamu and Imamura, Nobutaka and Shimizu, Toshiyuki and Ishihata, Hiroaki and Shindo, Tatsuya},
 title = {AP1000+: architectural support of PUT/GET interface for parallelizing compiler},
 abstract = {The scalability of distributed-memory parallel computers makes them attractive candidates for solving large-scale problems. New languages, such as HPF, FortranD, and VPP Fortran, have been developed to enable existing software to be easily ported to such machines. Many distributed-memory parallel computers have been built, but none of them support the mechanisms required by such languages. We studied the mechanisms required by parallelizing compilers and proposed a new architecture to support them. Based on this proposed architecture, we developed a new distributed-memory parallel computer, the AP1000+, which is an enhanced version of the AP1000. Using scientific applications in VPP Fortran and C, such as NAS parallel benchmarks, we simulated the performance of the AP1000+.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {196--207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195473.195538},
 doi = {http://doi.acm.org/10.1145/195473.195538},
 acmid = {195538},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Larus:1994:LMS:195470.195545,
 author = {Larus, James R. and Richards, Brad and Viswanathan, Guhan},
 title = {LCM: memory system support for parallel language implementation},
 abstract = {Higher-level parallel programming languages can be difficult to implement efficiently on parallel machines. This paper shows how a flexible, compiler-controlled memory system can help achieve good performance for language constructs that previously appeared too costly to be practical.Our compiler-controlled memory system is called Loosely Coherent Memory (LCM). It is an example of a larger class of Reconcilable Shared Memory (RSM) systems, which generalize the replication and merge policies of cache-coherent shared-memory. RSM protocols differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location. LCM memory becomes temporarily inconsistent to implement the semantics of C** parallel functions efficiently. RSM provides a compiler with control over memory-system policies, which it can use to implement a language's semantics, improve performance, or detect errors. We illustrate the first two points with LCM and our compiler for the data-parallel language C**.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {208--218},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195545},
 doi = {http://doi.acm.org/10.1145/195470.195545},
 acmid = {195545},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Larus:1994:LMS:195473.195545,
 author = {Larus, James R. and Richards, Brad and Viswanathan, Guhan},
 title = {LCM: memory system support for parallel language implementation},
 abstract = {Higher-level parallel programming languages can be difficult to implement efficiently on parallel machines. This paper shows how a flexible, compiler-controlled memory system can help achieve good performance for language constructs that previously appeared too costly to be practical.Our compiler-controlled memory system is called Loosely Coherent Memory (LCM). It is an example of a larger class of Reconcilable Shared Memory (RSM) systems, which generalize the replication and merge policies of cache-coherent shared-memory. RSM protocols differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location. LCM memory becomes temporarily inconsistent to implement the semantics of C** parallel functions efficiently. RSM provides a compiler with control over memory-system policies, which it can use to implement a language's semantics, improve performance, or detect errors. We illustrate the first two points with LCM and our compiler for the data-parallel language C**.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {208--218},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195545},
 doi = {http://doi.acm.org/10.1145/195473.195545},
 acmid = {195545},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Larus:1994:LMS:381792.195545,
 author = {Larus, James R. and Richards, Brad and Viswanathan, Guhan},
 title = {LCM: memory system support for parallel language implementation},
 abstract = {Higher-level parallel programming languages can be difficult to implement efficiently on parallel machines. This paper shows how a flexible, compiler-controlled memory system can help achieve good performance for language constructs that previously appeared too costly to be practical.Our compiler-controlled memory system is called Loosely Coherent Memory (LCM). It is an example of a larger class of Reconcilable Shared Memory (RSM) systems, which generalize the replication and merge policies of cache-coherent shared-memory. RSM protocols differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location. LCM memory becomes temporarily inconsistent to implement the semantics of C** parallel functions efficiently. RSM provides a compiler with control over memory-system policies, which it can use to implement a language's semantics, improve performance, or detect errors. We illustrate the first two points with LCM and our compiler for the data-parallel language C**.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {208--218},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195545},
 doi = {http://doi.acm.org/10.1145/381792.195545},
 acmid = {195545},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Woo:1994:PAI:381792.195547,
 author = {Woo, Steven Cameron and Singh, Jaswinder Pal and Hennessy, John L.},
 title = {The performance advantages of integrating block data transfer in cache-coherent multiprocessors},
 abstract = {Integrating support for block data transfer has become an important emphasis in recent cache-coherent shared address space multiprocessors. This paper examines the potential performance benefits of adding this support. A set of ambitious hardware mechanisms is used to study performance gains in five important scientific computations that appear to be good candidates for using block transfer. Our conclusion is that the benefits of block transfer are not substantial for hardware cache-coherent multiprocessors. The main reasons for this are (i) the relatively modest fraction of time applications spend in communication amenable to block transfer, (ii) the difficulty of finding enough independent computation to overlap with the communication latency that remains after block transfer, and  (iii) long cache lines often capture many of the benefits of block transfer in efficient cache-coherent machines. In the cases where block transfer improves performance, prefetching can often provide comparable, if not superior, performance benefits. We also examine the impact of varying important communication parameters and processor speed on the effectiveness of block transfer, and comment on useful features that a block transfer facility should support for real applications.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {219--229},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195547},
 doi = {http://doi.acm.org/10.1145/381792.195547},
 acmid = {195547},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Woo:1994:PAI:195470.195547,
 author = {Woo, Steven Cameron and Singh, Jaswinder Pal and Hennessy, John L.},
 title = {The performance advantages of integrating block data transfer in cache-coherent multiprocessors},
 abstract = {Integrating support for block data transfer has become an important emphasis in recent cache-coherent shared address space multiprocessors. This paper examines the potential performance benefits of adding this support. A set of ambitious hardware mechanisms is used to study performance gains in five important scientific computations that appear to be good candidates for using block transfer. Our conclusion is that the benefits of block transfer are not substantial for hardware cache-coherent multiprocessors. The main reasons for this are (i) the relatively modest fraction of time applications spend in communication amenable to block transfer, (ii) the difficulty of finding enough independent computation to overlap with the communication latency that remains after block transfer, and  (iii) long cache lines often capture many of the benefits of block transfer in efficient cache-coherent machines. In the cases where block transfer improves performance, prefetching can often provide comparable, if not superior, performance benefits. We also examine the impact of varying important communication parameters and processor speed on the effectiveness of block transfer, and comment on useful features that a block transfer facility should support for real applications.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {219--229},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195547},
 doi = {http://doi.acm.org/10.1145/195470.195547},
 acmid = {195547},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Woo:1994:PAI:195473.195547,
 author = {Woo, Steven Cameron and Singh, Jaswinder Pal and Hennessy, John L.},
 title = {The performance advantages of integrating block data transfer in cache-coherent multiprocessors},
 abstract = {Integrating support for block data transfer has become an important emphasis in recent cache-coherent shared address space multiprocessors. This paper examines the potential performance benefits of adding this support. A set of ambitious hardware mechanisms is used to study performance gains in five important scientific computations that appear to be good candidates for using block transfer. Our conclusion is that the benefits of block transfer are not substantial for hardware cache-coherent multiprocessors. The main reasons for this are (i) the relatively modest fraction of time applications spend in communication amenable to block transfer, (ii) the difficulty of finding enough independent computation to overlap with the communication latency that remains after block transfer, and  (iii) long cache lines often capture many of the benefits of block transfer in efficient cache-coherent machines. In the cases where block transfer improves performance, prefetching can often provide comparable, if not superior, performance benefits. We also examine the impact of varying important communication parameters and processor speed on the effectiveness of block transfer, and comment on useful features that a block transfer facility should support for real applications.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {219--229},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195547},
 doi = {http://doi.acm.org/10.1145/195473.195547},
 acmid = {195547},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Young:1994:IAS:195473.195549,
 author = {Young, Cliff and Smith, Michael D.},
 title = {Improving the accuracy of static branch prediction using branch correlation},
 abstract = {Recent work in history-based branch prediction uses novel hardware structures to capture branch correlation and increase branch prediction accuracy. We present a profile-based code transformation that exploits branch correlation to improve the accuracy of static branch prediction schemes. Our general method encodes branch history information in the program counter through the duplication and placement of program basic blocks. For correlation histories of eight branches, our experimental results achieve up to a 14.7\% improvement in prediction accuracy over conventional profile-based prediction without any increase in the dynamic instruction count of our benchmark applications. In the majority of these applications, code duplication increases code size by less than 30\%. For the few applications with code segments that exhibit exponential branching paths and no branch correlation, simple compile-time heuristics can eliminate these branches as code-transformation candidates.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {232--241},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195549},
 doi = {http://doi.acm.org/10.1145/195473.195549},
 acmid = {195549},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Young:1994:IAS:195470.195549,
 author = {Young, Cliff and Smith, Michael D.},
 title = {Improving the accuracy of static branch prediction using branch correlation},
 abstract = {Recent work in history-based branch prediction uses novel hardware structures to capture branch correlation and increase branch prediction accuracy. We present a profile-based code transformation that exploits branch correlation to improve the accuracy of static branch prediction schemes. Our general method encodes branch history information in the program counter through the duplication and placement of program basic blocks. For correlation histories of eight branches, our experimental results achieve up to a 14.7\% improvement in prediction accuracy over conventional profile-based prediction without any increase in the dynamic instruction count of our benchmark applications. In the majority of these applications, code duplication increases code size by less than 30\%. For the few applications with code segments that exhibit exponential branching paths and no branch correlation, simple compile-time heuristics can eliminate these branches as code-transformation candidates.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {232--241},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195549},
 doi = {http://doi.acm.org/10.1145/195470.195549},
 acmid = {195549},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Young:1994:IAS:381792.195549,
 author = {Young, Cliff and Smith, Michael D.},
 title = {Improving the accuracy of static branch prediction using branch correlation},
 abstract = {Recent work in history-based branch prediction uses novel hardware structures to capture branch correlation and increase branch prediction accuracy. We present a profile-based code transformation that exploits branch correlation to improve the accuracy of static branch prediction schemes. Our general method encodes branch history information in the program counter through the duplication and placement of program basic blocks. For correlation histories of eight branches, our experimental results achieve up to a 14.7\% improvement in prediction accuracy over conventional profile-based prediction without any increase in the dynamic instruction count of our benchmark applications. In the majority of these applications, code duplication increases code size by less than 30\%. For the few applications with code segments that exhibit exponential branching paths and no branch correlation, simple compile-time heuristics can eliminate these branches as code-transformation candidates.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {232--241},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195549},
 doi = {http://doi.acm.org/10.1145/381792.195549},
 acmid = {195549},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Calder:1994:RBC:195470.195553,
 author = {Calder, Brad and Grunwald, Dirk},
 title = {Reducing branch costs via branch alignment},
 abstract = {Several researchers have proposed algorithms for basic block reordering. We call these branch alignment algorithms. The primary emphasis of these algorithms has been on improving instruction cache locality, and the few studies concerned with branch prediction reported small or minimal improvements. As wide-issue architectures become increasingly popular the importance of reducing branch costs will increase, and branch alignment is one mechanism which can effectively reduce these costs.In this paper, we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and the branch prediction architecture when performing the basic block reordering. We show that branch alignment algorithms can improve a broad range  of static and dynamic branch prediction architectures. We also show that a program performance can be improved by approximately 5\% even when using recently proposed, highly accurate branch prediction architectures. The programs are compiled by any existing compiler and then transformed via binary transformations. When implementing these algorithms on a Alpha AXP 21604 up to a 16\% reduction in total execution time is achieved.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {242--251},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195553},
 doi = {http://doi.acm.org/10.1145/195470.195553},
 acmid = {195553},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, branch target buffers, profile-based optimization, trace scheduling},
} 

@article{Calder:1994:RBC:381792.195553,
 author = {Calder, Brad and Grunwald, Dirk},
 title = {Reducing branch costs via branch alignment},
 abstract = {Several researchers have proposed algorithms for basic block reordering. We call these branch alignment algorithms. The primary emphasis of these algorithms has been on improving instruction cache locality, and the few studies concerned with branch prediction reported small or minimal improvements. As wide-issue architectures become increasingly popular the importance of reducing branch costs will increase, and branch alignment is one mechanism which can effectively reduce these costs.In this paper, we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and the branch prediction architecture when performing the basic block reordering. We show that branch alignment algorithms can improve a broad range  of static and dynamic branch prediction architectures. We also show that a program performance can be improved by approximately 5\% even when using recently proposed, highly accurate branch prediction architectures. The programs are compiled by any existing compiler and then transformed via binary transformations. When implementing these algorithms on a Alpha AXP 21604 up to a 16\% reduction in total execution time is achieved.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {242--251},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195553},
 doi = {http://doi.acm.org/10.1145/381792.195553},
 acmid = {195553},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, branch target buffers, profile-based optimization, trace scheduling},
} 

@inproceedings{Calder:1994:RBC:195473.195553,
 author = {Calder, Brad and Grunwald, Dirk},
 title = {Reducing branch costs via branch alignment},
 abstract = {Several researchers have proposed algorithms for basic block reordering. We call these branch alignment algorithms. The primary emphasis of these algorithms has been on improving instruction cache locality, and the few studies concerned with branch prediction reported small or minimal improvements. As wide-issue architectures become increasingly popular the importance of reducing branch costs will increase, and branch alignment is one mechanism which can effectively reduce these costs.In this paper, we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and the branch prediction architecture when performing the basic block reordering. We show that branch alignment algorithms can improve a broad range  of static and dynamic branch prediction architectures. We also show that a program performance can be improved by approximately 5\% even when using recently proposed, highly accurate branch prediction architectures. The programs are compiled by any existing compiler and then transformed via binary transformations. When implementing these algorithms on a Alpha AXP 21604 up to a 16\% reduction in total execution time is achieved.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {242--251},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195553},
 doi = {http://doi.acm.org/10.1145/195473.195553},
 acmid = {195553},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, branch target buffers, profile-based optimization, trace scheduling},
} 

@article{Carr:1994:COI:381792.195557,
 author = {Carr, Steve and McKinley, Kathryn S. and Tseng, Chau-Wen},
 title = {Compiler optimizations for improving data locality},
 abstract = {In the past decade, processor speed has become significantly faster than memory speed. Small, fast cache memories are designed to overcome this discrepancy, but they are only effective when programs exhibit data locality. In this paper, we present compiler optimizations to improve data locality based on a simple yet accurate cost model. The model computes both temporal and spatial reuse of cache lines to find desirable loop organizations. The cost model drives the application of compound transformations consisting of loop permutation, loop fusion, loop distribution, and loop reversal. We demonstrate that these program transformations are useful for optimizing many programs.To validate our optimization strategy, we  implemented our algorithms and ran experiments on a large collection of scientific programs and kernels. Experiments with kernels illustrate that our model and algorithm can select and achieve the best performance. For over thirty complete applications, we executed the original and transformed versions and simulated cache hit rates. We collected statistics about the inherent characteristics of these programs and our ability to improve their data locality. To our knowledge, these studies are the first of such breadth and depth. We found performance improvements were difficult to achieve because benchmark programs typically have high hit rates even for small data caches; however, our optimizations significantly improved several programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195557},
 doi = {http://doi.acm.org/10.1145/381792.195557},
 acmid = {195557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Carr:1994:COI:195473.195557,
 author = {Carr, Steve and McKinley, Kathryn S. and Tseng, Chau-Wen},
 title = {Compiler optimizations for improving data locality},
 abstract = {In the past decade, processor speed has become significantly faster than memory speed. Small, fast cache memories are designed to overcome this discrepancy, but they are only effective when programs exhibit data locality. In this paper, we present compiler optimizations to improve data locality based on a simple yet accurate cost model. The model computes both temporal and spatial reuse of cache lines to find desirable loop organizations. The cost model drives the application of compound transformations consisting of loop permutation, loop fusion, loop distribution, and loop reversal. We demonstrate that these program transformations are useful for optimizing many programs.To validate our optimization strategy, we  implemented our algorithms and ran experiments on a large collection of scientific programs and kernels. Experiments with kernels illustrate that our model and algorithm can select and achieve the best performance. For over thirty complete applications, we executed the original and transformed versions and simulated cache hit rates. We collected statistics about the inherent characteristics of these programs and our ability to improve their data locality. To our knowledge, these studies are the first of such breadth and depth. We found performance improvements were difficult to achieve because benchmark programs typically have high hit rates even for small data caches; however, our optimizations significantly improved several programs.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195557},
 doi = {http://doi.acm.org/10.1145/195473.195557},
 acmid = {195557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carr:1994:COI:195470.195557,
 author = {Carr, Steve and McKinley, Kathryn S. and Tseng, Chau-Wen},
 title = {Compiler optimizations for improving data locality},
 abstract = {In the past decade, processor speed has become significantly faster than memory speed. Small, fast cache memories are designed to overcome this discrepancy, but they are only effective when programs exhibit data locality. In this paper, we present compiler optimizations to improve data locality based on a simple yet accurate cost model. The model computes both temporal and spatial reuse of cache lines to find desirable loop organizations. The cost model drives the application of compound transformations consisting of loop permutation, loop fusion, loop distribution, and loop reversal. We demonstrate that these program transformations are useful for optimizing many programs.To validate our optimization strategy, we  implemented our algorithms and ran experiments on a large collection of scientific programs and kernels. Experiments with kernels illustrate that our model and algorithm can select and achieve the best performance. For over thirty complete applications, we executed the original and transformed versions and simulated cache hit rates. We collected statistics about the inherent characteristics of these programs and our ability to improve their data locality. To our knowledge, these studies are the first of such breadth and depth. We found performance improvements were difficult to achieve because benchmark programs typically have high hit rates even for small data caches; however, our optimizations significantly improved several programs.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {252--262},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195557},
 doi = {http://doi.acm.org/10.1145/195470.195557},
 acmid = {195557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Engler:1994:DER:195470.195567,
 author = {Engler, Dawson R. and Proebsting, Todd A.},
 title = {DCG: an efficient, retargetable dynamic code generation system},
 abstract = {Dynamic code generation allows aggressive optimization through the use of runtime information. Previous systems typically relied on ad hoc code generators that were not designed for retargetability, and did not shield the client from machine-specific details. We present a system, dcg, that allows clients to specify dynamically generated code in a machine-independent manner. Our one-pass code generator is easily retargeted and extremely efficient (code generation costs approximately 350 instructions per generated instruction). Experiments show that dynamic code generation increases some application speeds by over an order of magnitude.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {263--272},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195567},
 doi = {http://doi.acm.org/10.1145/195470.195567},
 acmid = {195567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Engler:1994:DER:381792.195567,
 author = {Engler, Dawson R. and Proebsting, Todd A.},
 title = {DCG: an efficient, retargetable dynamic code generation system},
 abstract = {Dynamic code generation allows aggressive optimization through the use of runtime information. Previous systems typically relied on ad hoc code generators that were not designed for retargetability, and did not shield the client from machine-specific details. We present a system, dcg, that allows clients to specify dynamically generated code in a machine-independent manner. Our one-pass code generator is easily retargeted and extremely efficient (code generation costs approximately 350 instructions per generated instruction). Experiments show that dynamic code generation increases some application speeds by over an order of magnitude.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {263--272},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195567},
 doi = {http://doi.acm.org/10.1145/381792.195567},
 acmid = {195567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Engler:1994:DER:195473.195567,
 author = {Engler, Dawson R. and Proebsting, Todd A.},
 title = {DCG: an efficient, retargetable dynamic code generation system},
 abstract = {Dynamic code generation allows aggressive optimization through the use of runtime information. Previous systems typically relied on ad hoc code generators that were not designed for retargetability, and did not shield the client from machine-specific details. We present a system, dcg, that allows clients to specify dynamically generated code in a machine-independent manner. Our one-pass code generator is easily retargeted and extremely efficient (code generation costs approximately 350 instructions per generated instruction). Experiments show that dynamic code generation increases some application speeds by over an order of magnitude.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {263--272},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195567},
 doi = {http://doi.acm.org/10.1145/195473.195567},
 acmid = {195567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heinrich:1994:PIF:381792.195569,
 author = {Heinrich, Mark and Kuskin, Jeffrey and Ofelt, David and Heinlein, John and Baxter, Joel and Singh, Jaswinder Pal and Simoni, Richard and Gharachorloo, Kourosh and Nakahira, David and Horowitz, Mark and Gupta, Anoop and Rosenblum, Mendel and Hennessy, John},
 title = {The performance impact of flexibility in the Stanford FLASH multiprocessor},
 abstract = {A flexible communication mechanism is a desirable feature in multiprocessors because it allows support for multiple communication protocols, expands performance monitoring capabilities, and leads to a simpler design and debug process. In the Stanford FLASH multiprocessor, flexibility is obtained by requiring all transactions in a node to pass through a programmable node controller, called MAGIC. In this paper, we evaluate the performance costs of flexibility by comparing the performance of FLASH to that of an idealized hardwired machine on representative parallel applications and a multiprogramming workload. To measure the performance of FLASH, we use a detailed simulator of the FLASH and MAGIC designs, together with the code sequences that implement the cache-coherence protocol. We find that for a range of optimized parallel applications the performance differences between the idealized machine and FLASH are small. For these programs, either the miss rates are small or the latency of the programmable protocol can be hidden behind the memory access time. For applications that incur a large number of remote misses or exhibit substantial hot-spotting, performance is poor for both machines, though the increased remote access latencies or the occupancy of MAGIC lead to lower performance for the flexible design. In most cases, however, FLASH is only 2\%\&ndash;12\% slower than the idealized machine.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {274--285},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/381792.195569},
 doi = {http://doi.acm.org/10.1145/381792.195569},
 acmid = {195569},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heinrich:1994:PIF:195473.195569,
 author = {Heinrich, Mark and Kuskin, Jeffrey and Ofelt, David and Heinlein, John and Baxter, Joel and Singh, Jaswinder Pal and Simoni, Richard and Gharachorloo, Kourosh and Nakahira, David and Horowitz, Mark and Gupta, Anoop and Rosenblum, Mendel and Hennessy, John},
 title = {The performance impact of flexibility in the Stanford FLASH multiprocessor},
 abstract = {A flexible communication mechanism is a desirable feature in multiprocessors because it allows support for multiple communication protocols, expands performance monitoring capabilities, and leads to a simpler design and debug process. In the Stanford FLASH multiprocessor, flexibility is obtained by requiring all transactions in a node to pass through a programmable node controller, called MAGIC. In this paper, we evaluate the performance costs of flexibility by comparing the performance of FLASH to that of an idealized hardwired machine on representative parallel applications and a multiprogramming workload. To measure the performance of FLASH, we use a detailed simulator of the FLASH and MAGIC designs, together with the code sequences that implement the cache-coherence protocol. We find that for a range of optimized parallel applications the performance differences between the idealized machine and FLASH are small. For these programs, either the miss rates are small or the latency of the programmable protocol can be hidden behind the memory access time. For applications that incur a large number of remote misses or exhibit substantial hot-spotting, performance is poor for both machines, though the increased remote access latencies or the occupancy of MAGIC lead to lower performance for the flexible design. In most cases, however, FLASH is only 2\%\&ndash;12\% slower than the idealized machine.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {274--285},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195473.195569},
 doi = {http://doi.acm.org/10.1145/195473.195569},
 acmid = {195569},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heinrich:1994:PIF:195470.195569,
 author = {Heinrich, Mark and Kuskin, Jeffrey and Ofelt, David and Heinlein, John and Baxter, Joel and Singh, Jaswinder Pal and Simoni, Richard and Gharachorloo, Kourosh and Nakahira, David and Horowitz, Mark and Gupta, Anoop and Rosenblum, Mendel and Hennessy, John},
 title = {The performance impact of flexibility in the Stanford FLASH multiprocessor},
 abstract = {A flexible communication mechanism is a desirable feature in multiprocessors because it allows support for multiple communication protocols, expands performance monitoring capabilities, and leads to a simpler design and debug process. In the Stanford FLASH multiprocessor, flexibility is obtained by requiring all transactions in a node to pass through a programmable node controller, called MAGIC. In this paper, we evaluate the performance costs of flexibility by comparing the performance of FLASH to that of an idealized hardwired machine on representative parallel applications and a multiprogramming workload. To measure the performance of FLASH, we use a detailed simulator of the FLASH and MAGIC designs, together with the code sequences that implement the cache-coherence protocol. We find that for a range of optimized parallel applications the performance differences between the idealized machine and FLASH are small. For these programs, either the miss rates are small or the latency of the programmable protocol can be hidden behind the memory access time. For applications that incur a large number of remote misses or exhibit substantial hot-spotting, performance is poor for both machines, though the increased remote access latencies or the occupancy of MAGIC lead to lower performance for the flexible design. In most cases, however, FLASH is only 2\%\&ndash;12\% slower than the idealized machine.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {274--285},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/195470.195569},
 doi = {http://doi.acm.org/10.1145/195470.195569},
 acmid = {195569},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Skeppstedt:1994:SCA:195473.195572,
 author = {Skeppstedt, Jonas and Stenstr\"{o}m, Per},
 title = {Simple compiler algorithms to reduce ownership overhead in cache coherence protocols},
 abstract = {We study in this paper the design and efficiency of compiler algorithms that remove ownership overhead in shared-memory multiprocessors with write-invalidate protocols. These algorithms detect loads followed by stores to the same address. Such loads are marked and constitute a hint to the cache to obtain an exclusive copy of the block. We consider three algorithms where the first one focuses on load-store sequences within each basic block of code and the other two analyse the existence of load-store sequences across basic blocks at the intra-procedural level. Since the dataflow analysis we adopt is a trivial variation of live-variable analysis, the algorithms are easily incorporated into a compiler.Through detailed simulations of a cache-coherent NUMA architecture using five scientific parallel benchmark programs, we find that the algorithms are capable of removing over 95\% of the separate ownership acquisitions. Moreover, we also find that even the simplest algorithm is comparable in efficiency with previously proposed hardware-based adaptive cache coherence protocols to attack the same problem.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {286--296},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195572},
 doi = {http://doi.acm.org/10.1145/195473.195572},
 acmid = {195572},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Skeppstedt:1994:SCA:381792.195572,
 author = {Skeppstedt, Jonas and Stenstr\"{o}m, Per},
 title = {Simple compiler algorithms to reduce ownership overhead in cache coherence protocols},
 abstract = {We study in this paper the design and efficiency of compiler algorithms that remove ownership overhead in shared-memory multiprocessors with write-invalidate protocols. These algorithms detect loads followed by stores to the same address. Such loads are marked and constitute a hint to the cache to obtain an exclusive copy of the block. We consider three algorithms where the first one focuses on load-store sequences within each basic block of code and the other two analyse the existence of load-store sequences across basic blocks at the intra-procedural level. Since the dataflow analysis we adopt is a trivial variation of live-variable analysis, the algorithms are easily incorporated into a compiler.Through detailed simulations of a cache-coherent NUMA architecture using five scientific parallel benchmark programs, we find that the algorithms are capable of removing over 95\% of the separate ownership acquisitions. Moreover, we also find that even the simplest algorithm is comparable in efficiency with previously proposed hardware-based adaptive cache coherence protocols to attack the same problem.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {286--296},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195572},
 doi = {http://doi.acm.org/10.1145/381792.195572},
 acmid = {195572},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Skeppstedt:1994:SCA:195470.195572,
 author = {Skeppstedt, Jonas and Stenstr\"{o}m, Per},
 title = {Simple compiler algorithms to reduce ownership overhead in cache coherence protocols},
 abstract = {We study in this paper the design and efficiency of compiler algorithms that remove ownership overhead in shared-memory multiprocessors with write-invalidate protocols. These algorithms detect loads followed by stores to the same address. Such loads are marked and constitute a hint to the cache to obtain an exclusive copy of the block. We consider three algorithms where the first one focuses on load-store sequences within each basic block of code and the other two analyse the existence of load-store sequences across basic blocks at the intra-procedural level. Since the dataflow analysis we adopt is a trivial variation of live-variable analysis, the algorithms are easily incorporated into a compiler.Through detailed simulations of a cache-coherent NUMA architecture using five scientific parallel benchmark programs, we find that the algorithms are capable of removing over 95\% of the separate ownership acquisitions. Moreover, we also find that even the simplest algorithm is comparable in efficiency with previously proposed hardware-based adaptive cache coherence protocols to attack the same problem.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {286--296},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195572},
 doi = {http://doi.acm.org/10.1145/195470.195572},
 acmid = {195572},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schoinas:1994:FAC:195473.195575,
 author = {Schoinas, Ioannis and Falsafi, Babak and Lebeck, Alvin R. and Reinhardt, Steven K. and Larus, James R. and Wood, David A.},
 title = {Fine-grain access control for distributed shared memory},
 abstract = {This paper discusses implementations of fine-grain memory access control, which selectively restricts reads and writes to cache-block-sized memory regions. Fine-grain access control forms the basis of efficient cache-coherent shared memory. This paper focuses on low-cost implementations that require little or no additional hardware. These techniques permit efficient implementation of shared memory on a wide range of parallel systems, thereby providing shared-memory codes with a portability previously limited to message passing.This paper categorizes techniques based on where access control is enforced and where access conflicts are handled. We incorporated three techniques that require no additional hardware into Blizzard, a system that supports distributed shared memory on the CM-5. The first adds a software lookup before each shared-memory reference by modifying the program's executable. The second uses the memory's error correcting code (ECC) as cache-block valid bits. The third is a hybrid. The software technique ranged from slightly faster to two times slower than the ECC approach. Blizzard's performance is roughly comparable to a hardware shared-memory machine. These results argue that clusters of workstations or personal computers with networks comparable to the CM-5's will be able to support the same shared-memory interfaces as supercomputers.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {297--306},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195575},
 doi = {http://doi.acm.org/10.1145/195473.195575},
 acmid = {195575},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schoinas:1994:FAC:381792.195575,
 author = {Schoinas, Ioannis and Falsafi, Babak and Lebeck, Alvin R. and Reinhardt, Steven K. and Larus, James R. and Wood, David A.},
 title = {Fine-grain access control for distributed shared memory},
 abstract = {This paper discusses implementations of fine-grain memory access control, which selectively restricts reads and writes to cache-block-sized memory regions. Fine-grain access control forms the basis of efficient cache-coherent shared memory. This paper focuses on low-cost implementations that require little or no additional hardware. These techniques permit efficient implementation of shared memory on a wide range of parallel systems, thereby providing shared-memory codes with a portability previously limited to message passing.This paper categorizes techniques based on where access control is enforced and where access conflicts are handled. We incorporated three techniques that require no additional hardware into Blizzard, a system that supports distributed shared memory on the CM-5. The first adds a software lookup before each shared-memory reference by modifying the program's executable. The second uses the memory's error correcting code (ECC) as cache-block valid bits. The third is a hybrid. The software technique ranged from slightly faster to two times slower than the ECC approach. Blizzard's performance is roughly comparable to a hardware shared-memory machine. These results argue that clusters of workstations or personal computers with networks comparable to the CM-5's will be able to support the same shared-memory interfaces as supercomputers.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {297--306},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195575},
 doi = {http://doi.acm.org/10.1145/381792.195575},
 acmid = {195575},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schoinas:1994:FAC:195470.195575,
 author = {Schoinas, Ioannis and Falsafi, Babak and Lebeck, Alvin R. and Reinhardt, Steven K. and Larus, James R. and Wood, David A.},
 title = {Fine-grain access control for distributed shared memory},
 abstract = {This paper discusses implementations of fine-grain memory access control, which selectively restricts reads and writes to cache-block-sized memory regions. Fine-grain access control forms the basis of efficient cache-coherent shared memory. This paper focuses on low-cost implementations that require little or no additional hardware. These techniques permit efficient implementation of shared memory on a wide range of parallel systems, thereby providing shared-memory codes with a portability previously limited to message passing.This paper categorizes techniques based on where access control is enforced and where access conflicts are handled. We incorporated three techniques that require no additional hardware into Blizzard, a system that supports distributed shared memory on the CM-5. The first adds a software lookup before each shared-memory reference by modifying the program's executable. The second uses the memory's error correcting code (ECC) as cache-block valid bits. The third is a hybrid. The software technique ranged from slightly faster to two times slower than the ECC approach. Blizzard's performance is roughly comparable to a hardware shared-memory machine. These results argue that clusters of workstations or personal computers with networks comparable to the CM-5's will be able to support the same shared-memory interfaces as supercomputers.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {297--306},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195575},
 doi = {http://doi.acm.org/10.1145/195470.195575},
 acmid = {195575},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Laudon:1994:IMT:195473.195576,
 author = {Laudon, James and Gupta, Anoop and Horowitz, Mark},
 title = {Interleaving: a multithreading technique targeting multiprocessors and workstations},
 abstract = {There is an increasing trend to use commodity microprocessors as the compute engines in large-scale multiprocessors. However, given that the majority of the microprocessors are sold in the workstation market, not in the multiprocessor market, it is only natural that architectural features that benefit only multiprocessors are less likely to be adopted in commodity microprocessors. In this paper, we explore multiple-context processors, an architectural technique proposed to hide the large memory latency in multiprocessors. We show that while current multiple-context designs work reasonably well for multiprocessors, they are ineffective in hiding the much shorter uniprocessor latencies using the limited parallelism found in workstation environments. We propose an alternative design that combines the best features of two existing approaches, and present simulation results that show it yields better performance for both multiprogrammed workloads on a workstation and parallel applications on a multiprocessor. By addressing the needs of the workstation environment, our proposal makes multiple contexts more attractive for commodity microprocessors.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195473.195576},
 doi = {http://doi.acm.org/10.1145/195473.195576},
 acmid = {195576},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Laudon:1994:IMT:381792.195576,
 author = {Laudon, James and Gupta, Anoop and Horowitz, Mark},
 title = {Interleaving: a multithreading technique targeting multiprocessors and workstations},
 abstract = {There is an increasing trend to use commodity microprocessors as the compute engines in large-scale multiprocessors. However, given that the majority of the microprocessors are sold in the workstation market, not in the multiprocessor market, it is only natural that architectural features that benefit only multiprocessors are less likely to be adopted in commodity microprocessors. In this paper, we explore multiple-context processors, an architectural technique proposed to hide the large memory latency in multiprocessors. We show that while current multiple-context designs work reasonably well for multiprocessors, they are ineffective in hiding the much shorter uniprocessor latencies using the limited parallelism found in workstation environments. We propose an alternative design that combines the best features of two existing approaches, and present simulation results that show it yields better performance for both multiprogrammed workloads on a workstation and parallel applications on a multiprocessor. By addressing the needs of the workstation environment, our proposal makes multiple contexts more attractive for commodity microprocessors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/381792.195576},
 doi = {http://doi.acm.org/10.1145/381792.195576},
 acmid = {195576},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Laudon:1994:IMT:195470.195576,
 author = {Laudon, James and Gupta, Anoop and Horowitz, Mark},
 title = {Interleaving: a multithreading technique targeting multiprocessors and workstations},
 abstract = {There is an increasing trend to use commodity microprocessors as the compute engines in large-scale multiprocessors. However, given that the majority of the microprocessors are sold in the workstation market, not in the multiprocessor market, it is only natural that architectural features that benefit only multiprocessors are less likely to be adopted in commodity microprocessors. In this paper, we explore multiple-context processors, an architectural technique proposed to hide the large memory latency in multiprocessors. We show that while current multiple-context designs work reasonably well for multiprocessors, they are ineffective in hiding the much shorter uniprocessor latencies using the limited parallelism found in workstation environments. We propose an alternative design that combines the best features of two existing approaches, and present simulation results that show it yields better performance for both multiprogrammed workloads on a workstation and parallel applications on a multiprocessor. By addressing the needs of the workstation environment, our proposal makes multiple contexts more attractive for commodity microprocessors.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {308--318},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/195470.195576},
 doi = {http://doi.acm.org/10.1145/195470.195576},
 acmid = {195576},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carter:1994:HSF:195470.195579,
 author = {Carter, Nicholas P. and Keckler, Stephen W. and Dally, William J.},
 title = {Hardware support for fast capability-based addressing},
 abstract = {Traditional methods of providing protection in memory systems do so at the cost of increased context switch time and/or increased storage to record access permissions for processes. With the advent of computers that supported cycle-by-cycle multithreading, protection schemes that increase the time to perform a context switch are unacceptable, but protecting unrelated processes from each other is still necessary if such machines are to be used in non-trusting environments.This paper examines guarded pointers, a hardware technique which uses tagged 64-bit pointer objects to implement capability-based addressing. Guarded pointers encode a segment descriptor into the upper bits of every pointer, eliminating the indirection and related performance penalties  associated with traditional implementations of capabilities. All processes share a single 54-bit virtual address space, and access is limited to the data that can be referenced through the pointers that a process has been issued. Only one level of address translation is required to perform a memory reference. Sharing data between processes is efficient, and protection states are defined to allow fast protected subsystem calls and create unforgeable data keys.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {319--327},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/195470.195579},
 doi = {http://doi.acm.org/10.1145/195470.195579},
 acmid = {195579},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Carter:1994:HSF:195473.195579,
 author = {Carter, Nicholas P. and Keckler, Stephen W. and Dally, William J.},
 title = {Hardware support for fast capability-based addressing},
 abstract = {Traditional methods of providing protection in memory systems do so at the cost of increased context switch time and/or increased storage to record access permissions for processes. With the advent of computers that supported cycle-by-cycle multithreading, protection schemes that increase the time to perform a context switch are unacceptable, but protecting unrelated processes from each other is still necessary if such machines are to be used in non-trusting environments.This paper examines guarded pointers, a hardware technique which uses tagged 64-bit pointer objects to implement capability-based addressing. Guarded pointers encode a segment descriptor into the upper bits of every pointer, eliminating the indirection and related performance penalties  associated with traditional implementations of capabilities. All processes share a single 54-bit virtual address space, and access is limited to the data that can be referenced through the pointers that a process has been issued. Only one level of address translation is required to perform a memory reference. Sharing data between processes is efficient, and protection states are defined to allow fast protected subsystem calls and create unforgeable data keys.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {319--327},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/195473.195579},
 doi = {http://doi.acm.org/10.1145/195473.195579},
 acmid = {195579},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carter:1994:HSF:381792.195579,
 author = {Carter, Nicholas P. and Keckler, Stephen W. and Dally, William J.},
 title = {Hardware support for fast capability-based addressing},
 abstract = {Traditional methods of providing protection in memory systems do so at the cost of increased context switch time and/or increased storage to record access permissions for processes. With the advent of computers that supported cycle-by-cycle multithreading, protection schemes that increase the time to perform a context switch are unacceptable, but protecting unrelated processes from each other is still necessary if such machines are to be used in non-trusting environments.This paper examines guarded pointers, a hardware technique which uses tagged 64-bit pointer objects to implement capability-based addressing. Guarded pointers encode a segment descriptor into the upper bits of every pointer, eliminating the indirection and related performance penalties  associated with traditional implementations of capabilities. All processes share a single 54-bit virtual address space, and access is limited to the data that can be referenced through the pointers that a process has been issued. Only one level of address translation is required to perform a memory reference. Sharing data between processes is efficient, and protection states are defined to allow fast protected subsystem calls and create unforgeable data keys.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {319--327},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/381792.195579},
 doi = {http://doi.acm.org/10.1145/381792.195579},
 acmid = {195579},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:1994:EMH:381792.195583,
 author = {Thekkath, Radhika and Eggers, Susan J.},
 title = {The effectiveness of multiple hardware contexts},
 abstract = {Multithreaded processors are used to tolerate long memory latencies. By executing threads loaded in multiple hardware contexts, an otherwise idle processor can keep busy, thus increasing its utilization. However, the larger size of a multi-thread working set can have a negative effect on cache conflict misses. In this paper we evaluate the two phenomena together, examining their combined effect on execution time.The usefulness of multiple hardware contexts depends on: program data locality, cache organization and degree of multiprocessing. Multiple hardware contexts are most effective on programs that have been optimized for data locality. For these programs, execution time dropped with increasing contexts, over widely varying architectures. With unoptimized applications, multiple contexts had limited value. The best performance was seen with only two contexts, and only on uniprocessors and small multiprocessors. The behavior of the unoptimized applications changed more noticeably with variations in cache associativity and cache hierarchy, unlike the optimized programs.As a mechanism for exploiting program parallelism, an additional processor is clearly better than another context. However, there were many configurations for which the addition of a few hardware contexts brought as much or greater performance than a larger multiprocessor with fewer than the optimal number of contexts.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {28},
 issue = {5},
 month = {November},
 year = {1994},
 issn = {0163-5980},
 pages = {328--337},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/381792.195583},
 doi = {http://doi.acm.org/10.1145/381792.195583},
 acmid = {195583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thekkath:1994:EMH:195473.195583,
 author = {Thekkath, Radhika and Eggers, Susan J.},
 title = {The effectiveness of multiple hardware contexts},
 abstract = {Multithreaded processors are used to tolerate long memory latencies. By executing threads loaded in multiple hardware contexts, an otherwise idle processor can keep busy, thus increasing its utilization. However, the larger size of a multi-thread working set can have a negative effect on cache conflict misses. In this paper we evaluate the two phenomena together, examining their combined effect on execution time.The usefulness of multiple hardware contexts depends on: program data locality, cache organization and degree of multiprocessing. Multiple hardware contexts are most effective on programs that have been optimized for data locality. For these programs, execution time dropped with increasing contexts, over widely varying architectures. With unoptimized applications, multiple contexts had limited value. The best performance was seen with only two contexts, and only on uniprocessors and small multiprocessors. The behavior of the unoptimized applications changed more noticeably with variations in cache associativity and cache hierarchy, unlike the optimized programs.As a mechanism for exploiting program parallelism, an additional processor is clearly better than another context. However, there were many configurations for which the addition of a few hardware contexts brought as much or greater performance than a larger multiprocessor with fewer than the optimal number of contexts.},
 booktitle = {Proceedings of the sixth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-VI},
 year = {1994},
 isbn = {0-89791-660-3},
 location = {San Jose, California, United States},
 pages = {328--337},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195473.195583},
 doi = {http://doi.acm.org/10.1145/195473.195583},
 acmid = {195583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thekkath:1994:EMH:195470.195583,
 author = {Thekkath, Radhika and Eggers, Susan J.},
 title = {The effectiveness of multiple hardware contexts},
 abstract = {Multithreaded processors are used to tolerate long memory latencies. By executing threads loaded in multiple hardware contexts, an otherwise idle processor can keep busy, thus increasing its utilization. However, the larger size of a multi-thread working set can have a negative effect on cache conflict misses. In this paper we evaluate the two phenomena together, examining their combined effect on execution time.The usefulness of multiple hardware contexts depends on: program data locality, cache organization and degree of multiprocessing. Multiple hardware contexts are most effective on programs that have been optimized for data locality. For these programs, execution time dropped with increasing contexts, over widely varying architectures. With unoptimized applications, multiple contexts had limited value. The best performance was seen with only two contexts, and only on uniprocessors and small multiprocessors. The behavior of the unoptimized applications changed more noticeably with variations in cache associativity and cache hierarchy, unlike the optimized programs.As a mechanism for exploiting program parallelism, an additional processor is clearly better than another context. However, there were many configurations for which the addition of a few hardware contexts brought as much or greater performance than a larger multiprocessor with fewer than the optimal number of contexts.},
 journal = {SIGPLAN Not.},
 volume = {29},
 issue = {11},
 month = {November},
 year = {1994},
 issn = {0362-1340},
 pages = {328--337},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/195470.195583},
 doi = {http://doi.acm.org/10.1145/195470.195583},
 acmid = {195583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burrows:1992:ODC:143371.143376,
 author = {Burrows, Michael and Jerian, Charles and Lampson, Butler and Mann, Timothy},
 title = {On-line data compression in a log-structured file system},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {2--9},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/143371.143376},
 doi = {http://doi.acm.org/10.1145/143371.143376},
 acmid = {143376},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Burrows:1992:ODC:143365.143376,
 author = {Burrows, Michael and Jerian, Charles and Lampson, Butler and Mann, Timothy},
 title = {On-line data compression in a log-structured file system},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {2--9},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/143365.143376},
 doi = {http://doi.acm.org/10.1145/143365.143376},
 acmid = {143376},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Baker:1992:NMF:143371.143380,
 author = {Baker, Mary and Asami, Satoshi and Deprit, Etienne and Ouseterhout, John and Seltzer, Margo},
 title = {Non-volatile memory for fast, reliable file systems},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {10--22},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143380},
 doi = {http://doi.acm.org/10.1145/143371.143380},
 acmid = {143380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Baker:1992:NMF:143365.143380,
 author = {Baker, Mary and Asami, Satoshi and Deprit, Etienne and Ouseterhout, John and Seltzer, Margo},
 title = {Non-volatile memory for fast, reliable file systems},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {10--22},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143380},
 doi = {http://doi.acm.org/10.1145/143365.143380},
 acmid = {143380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Holland:1992:PDC:143365.143383,
 author = {Holland, Mark and Gibson, Garth A.},
 title = {Parity declustering for continuous operation in redundant disk arrays},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {23--35},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143383},
 doi = {http://doi.acm.org/10.1145/143365.143383},
 acmid = {143383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Holland:1992:PDC:143371.143383,
 author = {Holland, Mark and Gibson, Garth A.},
 title = {Parity declustering for continuous operation in redundant disk arrays},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {23--35},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143383},
 doi = {http://doi.acm.org/10.1145/143371.143383},
 acmid = {143383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rogers:1992:SSS:143371.143484,
 author = {Rogers, Anne and Li, Kai},
 title = {Software support for speculative loads},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {38--50},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143484},
 doi = {http://doi.acm.org/10.1145/143371.143484},
 acmid = {143484},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rogers:1992:SSS:143365.143484,
 author = {Rogers, Anne and Li, Kai},
 title = {Software support for speculative loads},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {38--50},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143484},
 doi = {http://doi.acm.org/10.1145/143365.143484},
 acmid = {143484},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1992:RML:143365.143486,
 author = {Chen, Tien-Fu and Baer, Jean-Loup},
 title = {Reducing memory latency via non-blocking and prefetching caches},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {51--61},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143365.143486},
 doi = {http://doi.acm.org/10.1145/143365.143486},
 acmid = {143486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1992:RML:143371.143486,
 author = {Chen, Tien-Fu and Baer, Jean-Loup},
 title = {Reducing memory latency via non-blocking and prefetching caches},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {51--61},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143371.143486},
 doi = {http://doi.acm.org/10.1145/143371.143486},
 acmid = {143486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mowry:1992:DEC:143365.143488,
 author = {Mowry, Todd C. and Lam, Monica S. and Gupta, Anoop},
 title = {Design and evaluation of a compiler algorithm for prefetching},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {62--73},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143488},
 doi = {http://doi.acm.org/10.1145/143365.143488},
 acmid = {143488},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mowry:1992:DEC:143371.143488,
 author = {Mowry, Todd C. and Lam, Monica S. and Gupta, Anoop},
 title = {Design and evaluation of a compiler algorithm for prefetching},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {62--73},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143488},
 doi = {http://doi.acm.org/10.1145/143371.143488},
 acmid = {143488},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pan:1992:IAD:143371.143490,
 author = {Pan, Shien-Tai and So, Kimming and Rahmeh, Joseph T.},
 title = {Improving the accuracy of dynamic branch prediction using branch correlation},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {76--84},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/143371.143490},
 doi = {http://doi.acm.org/10.1145/143371.143490},
 acmid = {143490},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pan:1992:IAD:143365.143490,
 author = {Pan, Shien-Tai and So, Kimming and Rahmeh, Joseph T.},
 title = {Improving the accuracy of dynamic branch prediction using branch correlation},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {76--84},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/143365.143490},
 doi = {http://doi.acm.org/10.1145/143365.143490},
 acmid = {143490},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fisher:1992:PCB:143371.143493,
 author = {Fisher, Joseph A. and Freudenberger, Stefan M.},
 title = {Predicting conditional branch directions from previous runs of a program},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143371.143493},
 doi = {http://doi.acm.org/10.1145/143371.143493},
 acmid = {143493},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fisher:1992:PCB:143365.143493,
 author = {Fisher, Joseph A. and Freudenberger, Stefan M.},
 title = {Predicting conditional branch directions from previous runs of a program},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {85--95},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143365.143493},
 doi = {http://doi.acm.org/10.1145/143365.143493},
 acmid = {143493},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1992:HSS:143365.143495,
 author = {Anderson, Thomas E. and Owicki, Susan S. and Saxe, James B. and Thacker, Charles P.},
 title = {High speed switch scheduling for local area networks},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {98--110},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143495},
 doi = {http://doi.acm.org/10.1145/143365.143495},
 acmid = {143495},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1992:HSS:143371.143495,
 author = {Anderson, Thomas E. and Owicki, Susan S. and Saxe, James B. and Thacker, Charles P.},
 title = {High speed switch scheduling for local area networks},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {98--110},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143495},
 doi = {http://doi.acm.org/10.1145/143371.143495},
 acmid = {143495},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Henry:1992:TPI:143365.143497,
 author = {Henry, Dana S. and Joerg, Christopher F.},
 title = {A tightly-coupled processor-network interface},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {111--122},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143497},
 doi = {http://doi.acm.org/10.1145/143365.143497},
 acmid = {143497},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Henry:1992:TPI:143371.143497,
 author = {Henry, Dana S. and Joerg, Christopher F.},
 title = {A tightly-coupled processor-network interface},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {111--122},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143497},
 doi = {http://doi.acm.org/10.1145/143371.143497},
 acmid = {143497},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wheeler:1992:CMV:143365.143499,
 author = {Wheeler, Bob and Bershad, Brian N.},
 title = {Consistency management for virtually indexed caches},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {124--136},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143499},
 doi = {http://doi.acm.org/10.1145/143365.143499},
 acmid = {143499},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wheeler:1992:CMV:143371.143499,
 author = {Wheeler, Bob and Bershad, Brian N.},
 title = {Consistency management for virtually indexed caches},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {124--136},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143499},
 doi = {http://doi.acm.org/10.1145/143371.143499},
 acmid = {143499},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiueh:1992:EAT:143365.143501,
 author = {Chiueh, Tzi-cker and Katz, Randy H.},
 title = {Eliminating the address translation bottleneck for physical address cache},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {137--148},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143501},
 doi = {http://doi.acm.org/10.1145/143365.143501},
 acmid = {143501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chiueh:1992:EAT:143371.143501,
 author = {Chiueh, Tzi-cker and Katz, Randy H.},
 title = {Eliminating the address translation bottleneck for physical address cache},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {137--148},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143501},
 doi = {http://doi.acm.org/10.1145/143371.143501},
 acmid = {143501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Veenstra:1992:PEO:143371.143503,
 author = {Veenstra, Jack E. and Fowler, Robert J.},
 title = {A performance evaluation of optimal hybrid cache coherency protocols},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {149--160},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143503},
 doi = {http://doi.acm.org/10.1145/143371.143503},
 acmid = {143503},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Veenstra:1992:PEO:143365.143503,
 author = {Veenstra, Jack E. and Fowler, Robert J.},
 title = {A performance evaluation of optimal hybrid cache coherency protocols},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {149--160},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143503},
 doi = {http://doi.acm.org/10.1145/143365.143503},
 acmid = {143503},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Torrellas:1992:CCS:143371.143506,
 author = {Torrellas, Josep and Gupta, Anoop and Hennessy, John},
 title = {Characterizing the caching and synchronization performance of a multiprocessor operating system},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {162--174},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143506},
 doi = {http://doi.acm.org/10.1145/143371.143506},
 acmid = {143506},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Torrellas:1992:CCS:143365.143506,
 author = {Torrellas, Josep and Gupta, Anoop and Hennessy, John},
 title = {Characterizing the caching and synchronization performance of a multiprocessor operating system},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {162--174},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143506},
 doi = {http://doi.acm.org/10.1145/143365.143506},
 acmid = {143506},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Koldinger:1992:ASS:143371.143508,
 author = {Koldinger, Eric J. and Chase, Jeffrey S. and Eggers, Susan J.},
 title = {Architecture support for single address space operating systems},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {175--186},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143508},
 doi = {http://doi.acm.org/10.1145/143371.143508},
 acmid = {143508},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Koldinger:1992:ASS:143365.143508,
 author = {Koldinger, Eric J. and Chase, Jeffrey S. and Eggers, Susan J.},
 title = {Architecture support for single address space operating systems},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {175--186},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143508},
 doi = {http://doi.acm.org/10.1145/143365.143508},
 acmid = {143508},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harty:1992:APM:143365.143511,
 author = {Harty, Kieran and Cheriton, David R.},
 title = {Application-controlled physical memory using external page-cache management},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {187--197},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143365.143511},
 doi = {http://doi.acm.org/10.1145/143365.143511},
 acmid = {143511},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harty:1992:APM:143371.143511,
 author = {Harty, Kieran and Cheriton, David R.},
 title = {Application-controlled physical memory using external page-cache management},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {187--197},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143371.143511},
 doi = {http://doi.acm.org/10.1145/143371.143511},
 acmid = {143511},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wahbe:1992:EDB:143365.143518,
 author = {Wahbe, Robert},
 title = {Efficient data breakpoints},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {200--212},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143365.143518},
 doi = {http://doi.acm.org/10.1145/143365.143518},
 acmid = {143518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wahbe:1992:EDB:143371.143518,
 author = {Wahbe, Robert},
 title = {Efficient data breakpoints},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {200--212},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/143371.143518},
 doi = {http://doi.acm.org/10.1145/143371.143518},
 acmid = {143518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Andrews:1992:MCC:143365.143520,
 author = {Andrews, Kristy and Sand, Duane},
 title = {Migrating a CISC computer family onto RISC via object code translation},
 abstract = {},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {213--222},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143365.143520},
 doi = {http://doi.acm.org/10.1145/143365.143520},
 acmid = {143520},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Andrews:1992:MCC:143371.143520,
 author = {Andrews, Kristy and Sand, Duane},
 title = {Migrating a CISC computer family onto RISC via object code translation},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {213--222},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143371.143520},
 doi = {http://doi.acm.org/10.1145/143371.143520},
 acmid = {143520},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bershad:1992:FME:143371.143523,
 author = {Bershad, Brian N. and Redell, David D. and Ellis, John R.},
 title = {Fast mutual exclusion for uniprocessors},
 abstract = {In this paper we describe restartable atomic sequences, an optimistic mechanism for implementing simple atomic operations (such as Test-And-Set) on a uniprocessor. A thread that is suspended within a restartable atomic sequence is resumed by the operating system at the beginning of the sequence, rather than at the point of suspension. This guarantees that the thread eventually executes the sequence atomically. A restartable atomic sequence has significantly less overhead than other software-based synchronization mechanisms, such as kernel emulation or software reservation. Consequently, it is an attractive alternative for use on uniprocessors that do no support atomic operations. Even on processors that do support atomic operations in  hardware, restartable atomic sequences can have lower overhead.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {223--233},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143371.143523},
 doi = {http://doi.acm.org/10.1145/143371.143523},
 acmid = {143523},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bershad:1992:FME:143365.143523,
 author = {Bershad, Brian N. and Redell, David D. and Ellis, John R.},
 title = {Fast mutual exclusion for uniprocessors},
 abstract = {In this paper we describe restartable atomic sequences, an optimistic mechanism for implementing simple atomic operations (such as Test-And-Set) on a uniprocessor. A thread that is suspended within a restartable atomic sequence is resumed by the operating system at the beginning of the sequence, rather than at the point of suspension. This guarantees that the thread eventually executes the sequence atomically. A restartable atomic sequence has significantly less overhead than other software-based synchronization mechanisms, such as kernel emulation or software reservation. Consequently, it is an attractive alternative for use on uniprocessors that do no support atomic operations. Even on processors that do support atomic operations in  hardware, restartable atomic sequences can have lower overhead.
},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {223--233},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143365.143523},
 doi = {http://doi.acm.org/10.1145/143365.143523},
 acmid = {143523},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mahlke:1992:SSV:143371.143529,
 author = {Mahlke, Scott A. and Chen, William Y. and Hwu, Wen-mei W. and Rau, B. Ramakrishna and Schlansker, Michael S.},
 title = {Sentinel scheduling for VLIW and superscalar processors},
 abstract = {Speculative execution is an important source of parallelism for VLIW and superscalar processors. A serious challenge with compiler-controlled speculative execution is to accurately detect and report all program execution errors at the time of occurrence. In this paper, a set of architectural features and compile-time scheduling support referred to as sentinel scheduling is introduced. Sentinel scheduling provides an effective framework for compiler-controlled speculative execution that accurately detects and reports all exceptions. Sentinel scheduling also supports speculative execution of store instructions by providing a store buffer which allows probationary entries. Experimental results show that sentinel scheduling is highly effective for a wide range of VLIW and superscalar processors.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {238--247},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143371.143529},
 doi = {http://doi.acm.org/10.1145/143371.143529},
 acmid = {143529},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mahlke:1992:SSV:143365.143529,
 author = {Mahlke, Scott A. and Chen, William Y. and Hwu, Wen-mei W. and Rau, B. Ramakrishna and Schlansker, Michael S.},
 title = {Sentinel scheduling for VLIW and superscalar processors},
 abstract = {Speculative execution is an important source of parallelism for VLIW and superscalar processors. A serious challenge with compiler-controlled speculative execution is to accurately detect and report all program execution errors at the time of occurrence. In this paper, a set of architectural features and compile-time scheduling support referred to as sentinel scheduling is introduced. Sentinel scheduling provides an effective framework for compiler-controlled speculative execution that accurately detects and reports all exceptions. Sentinel scheduling also supports speculative execution of store instructions by providing a store buffer which allows probationary entries. Experimental results show that sentinel scheduling is highly effective for a wide range of VLIW and superscalar processors.},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {238--247},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/143365.143529},
 doi = {http://doi.acm.org/10.1145/143365.143529},
 acmid = {143529},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:1992:ESP:143371.143534,
 author = {Smith, Michael D. and Horowitz, Mark and Lam, Monica S.},
 title = {Efficient superscalar performance through boosting},
 abstract = {The foremost goal of superscalar processor design is to increase performance through the exploitation of instruction-level parallelism (ILP). Previous studies have shown that speculative execution is required for high instruction per cycle (IPC) rates in non-numerical applications. The general trend has been toward supporting speculative execution in complicated, dynamically-scheduled processors. Performance, though, is more than just a high IPC rate; it also depends upon instruction count and cycle time. Boosting is an architectural technique that supports general speculative execution in simpler, statically-scheduled processors. Boosting labels speculative instructions with their control dependence information. This labelling eliminates control dependence constraints on instruction  scheduling while still providng full dependence information to the hardware. We have incorporated boosting into a trace-based, global scheduling algorithm that exploits ILP without adversely affecting the instruction count of a program. We use this algorithm and estimates of the boosting hardware involved to evaluate how much speculative execution support is really necessary to achieve good performance. We find that a statically-scheduled superscalar processor using a minimal implementation of boosting can easily reach the performance of a much more complex dynamically-scheduled superscalar processor.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143534},
 doi = {http://doi.acm.org/10.1145/143371.143534},
 acmid = {143534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smith:1992:ESP:143365.143534,
 author = {Smith, Michael D. and Horowitz, Mark and Lam, Monica S.},
 title = {Efficient superscalar performance through boosting},
 abstract = {The foremost goal of superscalar processor design is to increase performance through the exploitation of instruction-level parallelism (ILP). Previous studies have shown that speculative execution is required for high instruction per cycle (IPC) rates in non-numerical applications. The general trend has been toward supporting speculative execution in complicated, dynamically-scheduled processors. Performance, though, is more than just a high IPC rate; it also depends upon instruction count and cycle time. Boosting is an architectural technique that supports general speculative execution in simpler, statically-scheduled processors. Boosting labels speculative instructions with their control dependence information. This labelling eliminates control dependence constraints on instruction  scheduling while still providng full dependence information to the hardware. We have incorporated boosting into a trace-based, global scheduling algorithm that exploits ILP without adversely affecting the instruction count of a program. We use this algorithm and estimates of the boosting hardware involved to evaluate how much speculative execution support is really necessary to achieve good performance. We find that a statically-scheduled superscalar processor using a minimal implementation of boosting can easily reach the performance of a much more complex dynamically-scheduled superscalar processor.},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143534},
 doi = {http://doi.acm.org/10.1145/143365.143534},
 acmid = {143534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hill:1992:CSM:143371.143537,
 author = {Hill, Mark D. and Larus, James R. and Reinhardt, Steven K. and Wood, David A.},
 title = {Cooperative shared memory: software and hardware for scalable multiprocessor},
 abstract = {We believe the absence of massively-parallel, shared-memory machines follows from the lack of a shared-memory programming performance model that can inform programmers of the cost of operations (so they can avoid expensive ones) and can tell hardware designers which cases are common (so they can build simple hardware to optimize them). Cooperative shared memory, our approach to shared-memory design, addresses this problem.
},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {262--273},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143371.143537},
 doi = {http://doi.acm.org/10.1145/143371.143537},
 acmid = {143537},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hill:1992:CSM:143365.143537,
 author = {Hill, Mark D. and Larus, James R. and Reinhardt, Steven K. and Wood, David A.},
 title = {Cooperative shared memory: software and hardware for scalable multiprocessor},
 abstract = {We believe the absence of massively-parallel, shared-memory machines follows from the lack of a shared-memory programming performance model that can inform programmers of the cost of operations (so they can avoid expensive ones) and can tell hardware designers which cases are common (so they can build simple hardware to optimize them). Cooperative shared memory, our approach to shared-memory design, addresses this problem.
},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {262--273},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/143365.143537},
 doi = {http://doi.acm.org/10.1145/143365.143537},
 acmid = {143537},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kubiatowicz:1992:CWV:143365.143540,
 author = {Kubiatowicz, John and Chaiken, David and Agarwal, Anant},
 title = {Closing the window of vulnerability in multiphase memory transactions},
 abstract = {Multiprocessor architects have begun to explore several mechanisms such as prefetching, context-switching and software-assisted dynamic cache-coherence, which transform single-phase memory transactions in conventional memory systems into multiphase operations. Multiphase operations introduce a window of vulnerability in which data can be invalidated before it is used. Losing data due to invalidations introduces damaging livelock situations. This paper discusses the origins of the window of vulnerability and proposes an architectural framework that closes it. The framework is implemented in Alewife, a large-scale multi-processor being built at MIT.},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {274--284},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143365.143540},
 doi = {http://doi.acm.org/10.1145/143365.143540},
 acmid = {143540},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kubiatowicz:1992:CWV:143371.143540,
 author = {Kubiatowicz, John and Chaiken, David and Agarwal, Anant},
 title = {Closing the window of vulnerability in multiphase memory transactions},
 abstract = {Multiprocessor architects have begun to explore several mechanisms such as prefetching, context-switching and software-assisted dynamic cache-coherence, which transform single-phase memory transactions in conventional memory systems into multiphase operations. Multiphase operations introduce a window of vulnerability in which data can be invalidated before it is used. Losing data due to invalidations introduces damaging livelock situations. This paper discusses the origins of the window of vulnerability and proposes an architectural framework that closes it. The framework is implemented in Alewife, a large-scale multi-processor being built at MIT.},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {274--284},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143371.143540},
 doi = {http://doi.acm.org/10.1145/143371.143540},
 acmid = {143540},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Li:1992:ANL:143371.143541,
 author = {Li, Wei and Pingali, Keshav},
 title = {Access normalization: loop restructuring for NUMA compilers},
 abstract = {In scalable parallel machines, processors can make local memory accesses much faster than they can make remote memory accesses. In addition, when a number of remote accesses must be made, it is usually more efficient to use block transfers of data rather than to use many small messages. To run well on such machines, software must exploit these features. We believe it is too onerous for a programmer to do this by hand, so we have been exploring the use of restructuring compiler tecnology for this purpose. In this paper, we start with a language like FORTRAN-D with user-specified data distribution and develop a systematic loop transformation strategy called access normalization that restructures loop nests to exploit locality and block transfers. We demonstrate the  power of our techniques using routines from the BLAS (Basic Linear Algebra Subprograms) library. An important feature of our approach is that we model loop transformations using invertible matrices and integer lattice theory, thereby generalizing Banerjee's framework of unimodular matrices [5].},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {285--295},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143371.143541},
 doi = {http://doi.acm.org/10.1145/143371.143541},
 acmid = {143541},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Li:1992:ANL:143365.143541,
 author = {Li, Wei and Pingali, Keshav},
 title = {Access normalization: loop restructuring for NUMA compilers},
 abstract = {In scalable parallel machines, processors can make local memory accesses much faster than they can make remote memory accesses. In addition, when a number of remote accesses must be made, it is usually more efficient to use block transfers of data rather than to use many small messages. To run well on such machines, software must exploit these features. We believe it is too onerous for a programmer to do this by hand, so we have been exploring the use of restructuring compiler tecnology for this purpose. In this paper, we start with a language like FORTRAN-D with user-specified data distribution and develop a systematic loop transformation strategy called access normalization that restructures loop nests to exploit locality and block transfers. We demonstrate the  power of our techniques using routines from the BLAS (Basic Linear Algebra Subprograms) library. An important feature of our approach is that we model loop transformations using invertible matrices and integer lattice theory, thereby generalizing Banerjee's framework of unimodular matrices [5].},
 booktitle = {Proceedings of the fifth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-V},
 year = {1992},
 isbn = {0-89791-534-8},
 location = {Boston, Massachusetts, United States},
 pages = {285--295},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/143365.143541},
 doi = {http://doi.acm.org/10.1145/143365.143541},
 acmid = {143541},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolfe:1991:VIS:106974.106976,
 author = {Wolfe, Andrew and Shen, John P.},
 title = {A variable instruction stream extension to the VLIW architecture},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {2--14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106976},
 doi = {http://doi.acm.org/10.1145/106974.106976},
 acmid = {106976},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolfe:1991:VIS:106975.106976,
 author = {Wolfe, Andrew and Shen, John P.},
 title = {A variable instruction stream extension to the VLIW architecture},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {2--14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.106976},
 doi = {http://doi.acm.org/10.1145/106975.106976},
 acmid = {106976},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolfe:1991:VIS:106972.106976,
 author = {Wolfe, Andrew and Shen, John P.},
 title = {A variable instruction stream extension to the VLIW architecture},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {2--14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.106976},
 doi = {http://doi.acm.org/10.1145/106972.106976},
 acmid = {106976},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolfe:1991:VIS:106973.106976,
 author = {Wolfe, Andrew and Shen, John P.},
 title = {A variable instruction stream extension to the VLIW architecture},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {2--14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.106976},
 doi = {http://doi.acm.org/10.1145/106973.106976},
 acmid = {106976},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Katevenis:1991:RBP:106973.106977,
 author = {Katevenis, Manolis and Tzartzanis, Nestoras},
 title = {Reducing the branch penalty by rearranging instructions in a double-width memory},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {15--27},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.106977},
 doi = {http://doi.acm.org/10.1145/106973.106977},
 acmid = {106977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Katevenis:1991:RBP:106975.106977,
 author = {Katevenis, Manolis and Tzartzanis, Nestoras},
 title = {Reducing the branch penalty by rearranging instructions in a double-width memory},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {15--27},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.106977},
 doi = {http://doi.acm.org/10.1145/106975.106977},
 acmid = {106977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Katevenis:1991:RBP:106974.106977,
 author = {Katevenis, Manolis and Tzartzanis, Nestoras},
 title = {Reducing the branch penalty by rearranging instructions in a double-width memory},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {15--27},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106977},
 doi = {http://doi.acm.org/10.1145/106974.106977},
 acmid = {106977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Katevenis:1991:RBP:106972.106977,
 author = {Katevenis, Manolis and Tzartzanis, Nestoras},
 title = {Reducing the branch penalty by rearranging instructions in a double-width memory},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {15--27},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.106977},
 doi = {http://doi.acm.org/10.1145/106972.106977},
 acmid = {106977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1991:FPP:106974.106978,
 author = {Lee, Roland L. and Kwok, Alex Y. and Briggs, Fay\'{e} A.},
 title = {The floating point performance of a superscalar SPARC processor},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {28--37},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106978},
 doi = {http://doi.acm.org/10.1145/106974.106978},
 acmid = {106978},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1991:FPP:106973.106978,
 author = {Lee, Roland L. and Kwok, Alex Y. and Briggs, Fay\'{e} A.},
 title = {The floating point performance of a superscalar SPARC processor},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {28--37},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106978},
 doi = {http://doi.acm.org/10.1145/106973.106978},
 acmid = {106978},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1991:FPP:106972.106978,
 author = {Lee, Roland L. and Kwok, Alex Y. and Briggs, Fay\'{e} A.},
 title = {The floating point performance of a superscalar SPARC processor},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {28--37},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106978},
 doi = {http://doi.acm.org/10.1145/106972.106978},
 acmid = {106978},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1991:FPP:106975.106978,
 author = {Lee, Roland L. and Kwok, Alex Y. and Briggs, Fay\'{e} A.},
 title = {The floating point performance of a superscalar SPARC processor},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {28--37},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106978},
 doi = {http://doi.acm.org/10.1145/106975.106978},
 acmid = {106978},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1991:SP:106973.106979,
 author = {Callahan, David and Kennedy, Ken and Porterfield, Allan},
 title = {Software prefetching},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {40--52},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.106979},
 doi = {http://doi.acm.org/10.1145/106973.106979},
 acmid = {106979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Callahan:1991:SP:106972.106979,
 author = {Callahan, David and Kennedy, Ken and Porterfield, Allan},
 title = {Software prefetching},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {40--52},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.106979},
 doi = {http://doi.acm.org/10.1145/106972.106979},
 acmid = {106979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1991:SP:106975.106979,
 author = {Callahan, David and Kennedy, Ken and Porterfield, Allan},
 title = {Software prefetching},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {40--52},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.106979},
 doi = {http://doi.acm.org/10.1145/106975.106979},
 acmid = {106979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Callahan:1991:SP:106974.106979,
 author = {Callahan, David and Kennedy, Ken and Porterfield, Allan},
 title = {Software prefetching},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {40--52},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106979},
 doi = {http://doi.acm.org/10.1145/106974.106979},
 acmid = {106979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sohi:1991:HDM:106972.106980,
 author = {Sohi, Gurindar S. and Franklin, Manoj},
 title = {High-bandwidth data memory systems for superscalar processors},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106980},
 doi = {http://doi.acm.org/10.1145/106972.106980},
 acmid = {106980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sohi:1991:HDM:106975.106980,
 author = {Sohi, Gurindar S. and Franklin, Manoj},
 title = {High-bandwidth data memory systems for superscalar processors},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106980},
 doi = {http://doi.acm.org/10.1145/106975.106980},
 acmid = {106980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sohi:1991:HDM:106974.106980,
 author = {Sohi, Gurindar S. and Franklin, Manoj},
 title = {High-bandwidth data memory systems for superscalar processors},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106980},
 doi = {http://doi.acm.org/10.1145/106974.106980},
 acmid = {106980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sohi:1991:HDM:106973.106980,
 author = {Sohi, Gurindar S. and Franklin, Manoj},
 title = {High-bandwidth data memory systems for superscalar processors},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106980},
 doi = {http://doi.acm.org/10.1145/106973.106980},
 acmid = {106980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lam:1991:CPO:106974.106981,
 author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
 title = {The cache performance and optimizations of blocked algorithms},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106974.106981},
 doi = {http://doi.acm.org/10.1145/106974.106981},
 acmid = {106981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lam:1991:CPO:106973.106981,
 author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
 title = {The cache performance and optimizations of blocked algorithms},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106973.106981},
 doi = {http://doi.acm.org/10.1145/106973.106981},
 acmid = {106981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lam:1991:CPO:106975.106981,
 author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
 title = {The cache performance and optimizations of blocked algorithms},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106975.106981},
 doi = {http://doi.acm.org/10.1145/106975.106981},
 acmid = {106981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lam:1991:CPO:106972.106981,
 author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
 title = {The cache performance and optimizations of blocked algorithms},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106972.106981},
 doi = {http://doi.acm.org/10.1145/106972.106981},
 acmid = {106981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mogul:1991:ECS:106973.106982,
 author = {Mogul, Jeffrey C. and Borg, Anita},
 title = {The effect of context switches on cache performance},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106982},
 doi = {http://doi.acm.org/10.1145/106973.106982},
 acmid = {106982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mogul:1991:ECS:106974.106982,
 author = {Mogul, Jeffrey C. and Borg, Anita},
 title = {The effect of context switches on cache performance},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106982},
 doi = {http://doi.acm.org/10.1145/106974.106982},
 acmid = {106982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mogul:1991:ECS:106972.106982,
 author = {Mogul, Jeffrey C. and Borg, Anita},
 title = {The effect of context switches on cache performance},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106982},
 doi = {http://doi.acm.org/10.1145/106972.106982},
 acmid = {106982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mogul:1991:ECS:106975.106982,
 author = {Mogul, Jeffrey C. and Borg, Anita},
 title = {The effect of context switches on cache performance},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106982},
 doi = {http://doi.acm.org/10.1145/106975.106982},
 acmid = {106982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Keppel:1991:PIO:106975.106983,
 author = {Keppel, David},
 title = {A portable interface for on-the-fly instruction space modification},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106983},
 doi = {http://doi.acm.org/10.1145/106975.106983},
 acmid = {106983},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Keppel:1991:PIO:106974.106983,
 author = {Keppel, David},
 title = {A portable interface for on-the-fly instruction space modification},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106983},
 doi = {http://doi.acm.org/10.1145/106974.106983},
 acmid = {106983},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Keppel:1991:PIO:106972.106983,
 author = {Keppel, David},
 title = {A portable interface for on-the-fly instruction space modification},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106983},
 doi = {http://doi.acm.org/10.1145/106972.106983},
 acmid = {106983},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Keppel:1991:PIO:106973.106983,
 author = {Keppel, David},
 title = {A portable interface for on-the-fly instruction space modification},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106983},
 doi = {http://doi.acm.org/10.1145/106973.106983},
 acmid = {106983},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Appel:1991:VMP:106974.106984,
 author = {Appel, Andrew W. and Li, Kai},
 title = {Virtual memory primitives for user programs},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106974.106984},
 doi = {http://doi.acm.org/10.1145/106974.106984},
 acmid = {106984},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Appel:1991:VMP:106972.106984,
 author = {Appel, Andrew W. and Li, Kai},
 title = {Virtual memory primitives for user programs},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106972.106984},
 doi = {http://doi.acm.org/10.1145/106972.106984},
 acmid = {106984},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Appel:1991:VMP:106975.106984,
 author = {Appel, Andrew W. and Li, Kai},
 title = {Virtual memory primitives for user programs},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106975.106984},
 doi = {http://doi.acm.org/10.1145/106975.106984},
 acmid = {106984},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Appel:1991:VMP:106973.106984,
 author = {Appel, Andrew W. and Li, Kai},
 title = {Virtual memory primitives for user programs},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {96--107},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106973.106984},
 doi = {http://doi.acm.org/10.1145/106973.106984},
 acmid = {106984},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1991:IAO:106973.106985,
 author = {Anderson, Thomas E. and Levy, Henry M. and Bershad, Brian N. and Lazowska, Edward D.},
 title = {The interaction of architecture and operating system design},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.106985},
 doi = {http://doi.acm.org/10.1145/106973.106985},
 acmid = {106985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1991:IAO:106972.106985,
 author = {Anderson, Thomas E. and Levy, Henry M. and Bershad, Brian N. and Lazowska, Edward D.},
 title = {The interaction of architecture and operating system design},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.106985},
 doi = {http://doi.acm.org/10.1145/106972.106985},
 acmid = {106985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1991:IAO:106975.106985,
 author = {Anderson, Thomas E. and Levy, Henry M. and Bershad, Brian N. and Lazowska, Edward D.},
 title = {The interaction of architecture and operating system design},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.106985},
 doi = {http://doi.acm.org/10.1145/106975.106985},
 acmid = {106985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1991:IAO:106974.106985,
 author = {Anderson, Thomas E. and Levy, Henry M. and Bershad, Brian N. and Lazowska, Edward D.},
 title = {The interaction of architecture and operating system design},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {108--120},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106985},
 doi = {http://doi.acm.org/10.1145/106974.106985},
 acmid = {106985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bradlee:1991:IRA:106973.106986,
 author = {Bradlee, David G. and Eggers, Susan J. and Henry, Robert R.},
 title = {Integrating register allocation and instruction scheduling for RISCs},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106986},
 doi = {http://doi.acm.org/10.1145/106973.106986},
 acmid = {106986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bradlee:1991:IRA:106975.106986,
 author = {Bradlee, David G. and Eggers, Susan J. and Henry, Robert R.},
 title = {Integrating register allocation and instruction scheduling for RISCs},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106986},
 doi = {http://doi.acm.org/10.1145/106975.106986},
 acmid = {106986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bradlee:1991:IRA:106972.106986,
 author = {Bradlee, David G. and Eggers, Susan J. and Henry, Robert R.},
 title = {Integrating register allocation and instruction scheduling for RISCs},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106986},
 doi = {http://doi.acm.org/10.1145/106972.106986},
 acmid = {106986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bradlee:1991:IRA:106974.106986,
 author = {Bradlee, David G. and Eggers, Susan J. and Henry, Robert R.},
 title = {Integrating register allocation and instruction scheduling for RISCs},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106986},
 doi = {http://doi.acm.org/10.1145/106974.106986},
 acmid = {106986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Benitez:1991:CGS:106974.106987,
 author = {Benitez, Manuel E. and Davidson, Jack W.},
 title = {Code generation for streaming: an access/execute mechanism},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {132--141},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106987},
 doi = {http://doi.acm.org/10.1145/106974.106987},
 acmid = {106987},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Benitez:1991:CGS:106973.106987,
 author = {Benitez, Manuel E. and Davidson, Jack W.},
 title = {Code generation for streaming: an access/execute mechanism},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {132--141},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106987},
 doi = {http://doi.acm.org/10.1145/106973.106987},
 acmid = {106987},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Benitez:1991:CGS:106975.106987,
 author = {Benitez, Manuel E. and Davidson, Jack W.},
 title = {Code generation for streaming: an access/execute mechanism},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {132--141},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106987},
 doi = {http://doi.acm.org/10.1145/106975.106987},
 acmid = {106987},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Benitez:1991:CGS:106972.106987,
 author = {Benitez, Manuel E. and Davidson, Jack W.},
 title = {Code generation for streaming: an access/execute mechanism},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {132--141},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106987},
 doi = {http://doi.acm.org/10.1145/106972.106987},
 acmid = {106987},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bagrodia:1991:EIH:106975.376053,
 author = {Bagrodia, Rajive and Mathur, Sharad},
 title = {Efficient Implementation of high-level parallel programs},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {142--151},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.376053},
 doi = {http://doi.acm.org/10.1145/106975.376053},
 acmid = {376053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bagrodia:1991:EIH:106974.376053,
 author = {Bagrodia, Rajive and Mathur, Sharad},
 title = {Efficient Implementation of high-level parallel programs},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {142--151},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.376053},
 doi = {http://doi.acm.org/10.1145/106974.376053},
 acmid = {376053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bagrodia:1991:EIH:106972.376053,
 author = {Bagrodia, Rajive and Mathur, Sharad},
 title = {Efficient Implementation of high-level parallel programs},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {142--151},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.376053},
 doi = {http://doi.acm.org/10.1145/106972.376053},
 acmid = {376053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bagrodia:1991:EIH:106973.376053,
 author = {Bagrodia, Rajive and Mathur, Sharad},
 title = {Efficient Implementation of high-level parallel programs},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {142--151},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.376053},
 doi = {http://doi.acm.org/10.1145/106973.376053},
 acmid = {376053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mangione-Smith:1991:VRD:106975.328664,
 author = {Mangione-Smith, William and Abraham, Santosh G. and Davidson, Edward S.},
 title = {Vector register design for polycyclic vector scheduling},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.328664},
 doi = {http://doi.acm.org/10.1145/106975.328664},
 acmid = {328664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mangione-Smith:1991:VRD:106972.328664,
 author = {Mangione-Smith, William and Abraham, Santosh G. and Davidson, Edward S.},
 title = {Vector register design for polycyclic vector scheduling},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.328664},
 doi = {http://doi.acm.org/10.1145/106972.328664},
 acmid = {328664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mangione-Smith:1991:VRD:106973.328664,
 author = {Mangione-Smith, William and Abraham, Santosh G. and Davidson, Edward S.},
 title = {Vector register design for polycyclic vector scheduling},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.328664},
 doi = {http://doi.acm.org/10.1145/106973.328664},
 acmid = {328664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mangione-Smith:1991:VRD:106974.328664,
 author = {Mangione-Smith, William and Abraham, Santosh G. and Davidson, Edward S.},
 title = {Vector register design for polycyclic vector scheduling},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.328664},
 doi = {http://doi.acm.org/10.1145/106974.328664},
 acmid = {328664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Culler:1991:FPM:106972.106990,
 author = {Culler, David E. and Sah, Anurag and Schauser, Klaus E. and von Eicken, Thorsten and Wawrzynek, John},
 title = {Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {164--175},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106972.106990},
 doi = {http://doi.acm.org/10.1145/106972.106990},
 acmid = {106990},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Culler:1991:FPM:106975.106990,
 author = {Culler, David E. and Sah, Anurag and Schauser, Klaus E. and von Eicken, Thorsten and Wawrzynek, John},
 title = {Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {164--175},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106975.106990},
 doi = {http://doi.acm.org/10.1145/106975.106990},
 acmid = {106990},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Culler:1991:FPM:106973.106990,
 author = {Culler, David E. and Sah, Anurag and Schauser, Klaus E. and von Eicken, Thorsten and Wawrzynek, John},
 title = {Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {164--175},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106973.106990},
 doi = {http://doi.acm.org/10.1145/106973.106990},
 acmid = {106990},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Culler:1991:FPM:106974.106990,
 author = {Culler, David E. and Sah, Anurag and Schauser, Klaus E. and von Eicken, Thorsten and Wawrzynek, John},
 title = {Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {164--175},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106974.106990},
 doi = {http://doi.acm.org/10.1145/106974.106990},
 acmid = {106990},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1991:LIP:106973.106991,
 author = {Wall, David W.},
 title = {Limits of instruction-level parallelism},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {176--188},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.106991},
 doi = {http://doi.acm.org/10.1145/106973.106991},
 acmid = {106991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1991:LIP:106975.106991,
 author = {Wall, David W.},
 title = {Limits of instruction-level parallelism},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {176--188},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.106991},
 doi = {http://doi.acm.org/10.1145/106975.106991},
 acmid = {106991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1991:LIP:106974.106991,
 author = {Wall, David W.},
 title = {Limits of instruction-level parallelism},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {176--188},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106991},
 doi = {http://doi.acm.org/10.1145/106974.106991},
 acmid = {106991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wall:1991:LIP:106972.106991,
 author = {Wall, David W.},
 title = {Limits of instruction-level parallelism},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {176--188},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.106991},
 doi = {http://doi.acm.org/10.1145/106972.106991},
 acmid = {106991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1991:PCP:106974.106992,
 author = {Lee, Edward K. and Katz, Randy H.},
 title = {Performance consequences of parity placement in disk arrays},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106992},
 doi = {http://doi.acm.org/10.1145/106974.106992},
 acmid = {106992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1991:PCP:106973.106992,
 author = {Lee, Edward K. and Katz, Randy H.},
 title = {Performance consequences of parity placement in disk arrays},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106992},
 doi = {http://doi.acm.org/10.1145/106973.106992},
 acmid = {106992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1991:PCP:106972.106992,
 author = {Lee, Edward K. and Katz, Randy H.},
 title = {Performance consequences of parity placement in disk arrays},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106992},
 doi = {http://doi.acm.org/10.1145/106972.106992},
 acmid = {106992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1991:PCP:106975.106992,
 author = {Lee, Edward K. and Katz, Randy H.},
 title = {Performance consequences of parity placement in disk arrays},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106992},
 doi = {http://doi.acm.org/10.1145/106975.106992},
 acmid = {106992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cate:1991:CCC:106972.106993,
 author = {Cate, Vincent and Gross, Thomas},
 title = {Combining the concepts of compression and caching for a two-level filesystem},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {200--211},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106972.106993},
 doi = {http://doi.acm.org/10.1145/106972.106993},
 acmid = {106993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cate:1991:CCC:106975.106993,
 author = {Cate, Vincent and Gross, Thomas},
 title = {Combining the concepts of compression and caching for a two-level filesystem},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {200--211},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106975.106993},
 doi = {http://doi.acm.org/10.1145/106975.106993},
 acmid = {106993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cate:1991:CCC:106973.106993,
 author = {Cate, Vincent and Gross, Thomas},
 title = {Combining the concepts of compression and caching for a two-level filesystem},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {200--211},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106973.106993},
 doi = {http://doi.acm.org/10.1145/106973.106993},
 acmid = {106993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cate:1991:CCC:106974.106993,
 author = {Cate, Vincent and Gross, Thomas},
 title = {Combining the concepts of compression and caching for a two-level filesystem},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {200--211},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/106974.106993},
 doi = {http://doi.acm.org/10.1145/106974.106993},
 acmid = {106993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bolosky:1991:NPR:106973.106994,
 author = {Bolosky, William J. and Scott, Michael L. and Fitzgerald, Robert P. and Fowler, Robert J. and Cox, Alan L.},
 title = {NUMA policies and their relation to memory architecture},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106994},
 doi = {http://doi.acm.org/10.1145/106973.106994},
 acmid = {106994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bolosky:1991:NPR:106972.106994,
 author = {Bolosky, William J. and Scott, Michael L. and Fitzgerald, Robert P. and Fowler, Robert J. and Cox, Alan L.},
 title = {NUMA policies and their relation to memory architecture},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106994},
 doi = {http://doi.acm.org/10.1145/106972.106994},
 acmid = {106994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bolosky:1991:NPR:106974.106994,
 author = {Bolosky, William J. and Scott, Michael L. and Fitzgerald, Robert P. and Fowler, Robert J. and Cox, Alan L.},
 title = {NUMA policies and their relation to memory architecture},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106994},
 doi = {http://doi.acm.org/10.1145/106974.106994},
 acmid = {106994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bolosky:1991:NPR:106975.106994,
 author = {Bolosky, William J. and Scott, Michael L. and Fitzgerald, Robert P. and Fowler, Robert J. and Cox, Alan L.},
 title = {NUMA policies and their relation to memory architecture},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106994},
 doi = {http://doi.acm.org/10.1145/106975.106994},
 acmid = {106994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chaiken:1991:LDS:106973.106995,
 author = {Chaiken, David and Kubiatowicz, John and Agarwal, Anant},
 title = {LimitLESS directories: A scalable cache coherence scheme},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/106973.106995},
 doi = {http://doi.acm.org/10.1145/106973.106995},
 acmid = {106995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chaiken:1991:LDS:106975.106995,
 author = {Chaiken, David and Kubiatowicz, John and Agarwal, Anant},
 title = {LimitLESS directories: A scalable cache coherence scheme},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/106975.106995},
 doi = {http://doi.acm.org/10.1145/106975.106995},
 acmid = {106995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chaiken:1991:LDS:106972.106995,
 author = {Chaiken, David and Kubiatowicz, John and Agarwal, Anant},
 title = {LimitLESS directories: A scalable cache coherence scheme},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/106972.106995},
 doi = {http://doi.acm.org/10.1145/106972.106995},
 acmid = {106995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chaiken:1991:LDS:106974.106995,
 author = {Chaiken, David and Kubiatowicz, John and Agarwal, Anant},
 title = {LimitLESS directories: A scalable cache coherence scheme},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {224--234},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/106974.106995},
 doi = {http://doi.acm.org/10.1145/106974.106995},
 acmid = {106995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Min:1991:ECA:106973.106996,
 author = {Min, Sang L. and Choi, Jong-Deok},
 title = {An efficient cache-based access anomaly detection scheme},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106996},
 doi = {http://doi.acm.org/10.1145/106973.106996},
 acmid = {106996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Min:1991:ECA:106974.106996,
 author = {Min, Sang L. and Choi, Jong-Deok},
 title = {An efficient cache-based access anomaly detection scheme},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106996},
 doi = {http://doi.acm.org/10.1145/106974.106996},
 acmid = {106996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Min:1991:ECA:106975.106996,
 author = {Min, Sang L. and Choi, Jong-Deok},
 title = {An efficient cache-based access anomaly detection scheme},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106996},
 doi = {http://doi.acm.org/10.1145/106975.106996},
 acmid = {106996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Min:1991:ECA:106972.106996,
 author = {Min, Sang L. and Choi, Jong-Deok},
 title = {An efficient cache-based access anomaly detection scheme},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {235--244},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106996},
 doi = {http://doi.acm.org/10.1145/106972.106996},
 acmid = {106996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gharachorloo:1991:PEM:106974.106997,
 author = {Gharachorloo, Kourosh and Gupta, Anoop and Hennessy, John},
 title = {Performance evaluation of memory consistency models for shared-memory multiprocessors},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {245--257},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.106997},
 doi = {http://doi.acm.org/10.1145/106974.106997},
 acmid = {106997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gharachorloo:1991:PEM:106972.106997,
 author = {Gharachorloo, Kourosh and Gupta, Anoop and Hennessy, John},
 title = {Performance evaluation of memory consistency models for shared-memory multiprocessors},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {245--257},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.106997},
 doi = {http://doi.acm.org/10.1145/106972.106997},
 acmid = {106997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gharachorloo:1991:PEM:106975.106997,
 author = {Gharachorloo, Kourosh and Gupta, Anoop and Hennessy, John},
 title = {Performance evaluation of memory consistency models for shared-memory multiprocessors},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {245--257},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.106997},
 doi = {http://doi.acm.org/10.1145/106975.106997},
 acmid = {106997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gharachorloo:1991:PEM:106973.106997,
 author = {Gharachorloo, Kourosh and Gupta, Anoop and Hennessy, John},
 title = {Performance evaluation of memory consistency models for shared-memory multiprocessors},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {245--257},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.106997},
 doi = {http://doi.acm.org/10.1145/106973.106997},
 acmid = {106997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Freudenthal:1991:PCF:106972.106998,
 author = {Freudenthal, Eric and Gottlieb, Allan},
 title = {Process coordination with fetch-and-increment},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {260--268},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106972.106998},
 doi = {http://doi.acm.org/10.1145/106972.106998},
 acmid = {106998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Freudenthal:1991:PCF:106975.106998,
 author = {Freudenthal, Eric and Gottlieb, Allan},
 title = {Process coordination with fetch-and-increment},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {260--268},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106975.106998},
 doi = {http://doi.acm.org/10.1145/106975.106998},
 acmid = {106998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Freudenthal:1991:PCF:106973.106998,
 author = {Freudenthal, Eric and Gottlieb, Allan},
 title = {Process coordination with fetch-and-increment},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {260--268},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106973.106998},
 doi = {http://doi.acm.org/10.1145/106973.106998},
 acmid = {106998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Freudenthal:1991:PCF:106974.106998,
 author = {Freudenthal, Eric and Gottlieb, Allan},
 title = {Process coordination with fetch-and-increment},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {260--268},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106974.106998},
 doi = {http://doi.acm.org/10.1145/106974.106998},
 acmid = {106998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mellor-Crummey:1991:SWC:106975.106999,
 author = {Mellor-Crummey, John M. and Scott, Michael L.},
 title = {Synchronization without contention},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.106999},
 doi = {http://doi.acm.org/10.1145/106975.106999},
 acmid = {106999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mellor-Crummey:1991:SWC:106973.106999,
 author = {Mellor-Crummey, John M. and Scott, Michael L.},
 title = {Synchronization without contention},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.106999},
 doi = {http://doi.acm.org/10.1145/106973.106999},
 acmid = {106999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mellor-Crummey:1991:SWC:106972.106999,
 author = {Mellor-Crummey, John M. and Scott, Michael L.},
 title = {Synchronization without contention},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.106999},
 doi = {http://doi.acm.org/10.1145/106972.106999},
 acmid = {106999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mellor-Crummey:1991:SWC:106974.106999,
 author = {Mellor-Crummey, John M. and Scott, Michael L.},
 title = {Synchronization without contention},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.106999},
 doi = {http://doi.acm.org/10.1145/106974.106999},
 acmid = {106999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1991:CRB:106973.107000,
 author = {Johnson, Douglas},
 title = {The case for a read barrier},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {279--287},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106973.107000},
 doi = {http://doi.acm.org/10.1145/106973.107000},
 acmid = {107000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Johnson:1991:CRB:106972.107000,
 author = {Johnson, Douglas},
 title = {The case for a read barrier},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {279--287},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106972.107000},
 doi = {http://doi.acm.org/10.1145/106972.107000},
 acmid = {107000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1991:CRB:106974.107000,
 author = {Johnson, Douglas},
 title = {The case for a read barrier},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {279--287},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106974.107000},
 doi = {http://doi.acm.org/10.1145/106974.107000},
 acmid = {107000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1991:CRB:106975.107000,
 author = {Johnson, Douglas},
 title = {The case for a read barrier},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {279--287},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/106975.107000},
 doi = {http://doi.acm.org/10.1145/106975.107000},
 acmid = {107000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cmelik:1991:AMS:106975.107001,
 author = {Cmelik, Robert F. and Kong, Shing I. and Ditzel, David R. and Kelly, Edmund J.},
 title = {An analysis of MIPS and SPARC instruction set utilization on the SPEC benchmarks},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {290--302},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106975.107001},
 doi = {http://doi.acm.org/10.1145/106975.107001},
 acmid = {107001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cmelik:1991:AMS:106974.107001,
 author = {Cmelik, Robert F. and Kong, Shing I. and Ditzel, David R. and Kelly, Edmund J.},
 title = {An analysis of MIPS and SPARC instruction set utilization on the SPEC benchmarks},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {290--302},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106974.107001},
 doi = {http://doi.acm.org/10.1145/106974.107001},
 acmid = {107001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cmelik:1991:AMS:106973.107001,
 author = {Cmelik, Robert F. and Kong, Shing I. and Ditzel, David R. and Kelly, Edmund J.},
 title = {An analysis of MIPS and SPARC instruction set utilization on the SPEC benchmarks},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {290--302},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106973.107001},
 doi = {http://doi.acm.org/10.1145/106973.107001},
 acmid = {107001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cmelik:1991:AMS:106972.107001,
 author = {Cmelik, Robert F. and Kong, Shing I. and Ditzel, David R. and Kelly, Edmund J.},
 title = {An analysis of MIPS and SPARC instruction set utilization on the SPEC benchmarks},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {290--302},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/106972.107001},
 doi = {http://doi.acm.org/10.1145/106972.107001},
 acmid = {107001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hall:1991:PCA:106973.107002,
 author = {Hall, C. Brian and O'Brien, Kevin},
 title = {Performance characteristics of architectural features of the IBM RISC System/6000},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {303--309},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/106973.107002},
 doi = {http://doi.acm.org/10.1145/106973.107002},
 acmid = {107002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hall:1991:PCA:106974.107002,
 author = {Hall, C. Brian and O'Brien, Kevin},
 title = {Performance characteristics of architectural features of the IBM RISC System/6000},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {303--309},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/106974.107002},
 doi = {http://doi.acm.org/10.1145/106974.107002},
 acmid = {107002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hall:1991:PCA:106975.107002,
 author = {Hall, C. Brian and O'Brien, Kevin},
 title = {Performance characteristics of architectural features of the IBM RISC System/6000},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {303--309},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/106975.107002},
 doi = {http://doi.acm.org/10.1145/106975.107002},
 acmid = {107002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hall:1991:PCA:106972.107002,
 author = {Hall, C. Brian and O'Brien, Kevin},
 title = {Performance characteristics of architectural features of the IBM RISC System/6000},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {303--309},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/106972.107002},
 doi = {http://doi.acm.org/10.1145/106972.107002},
 acmid = {107002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhandarkar:1991:PAC:106975.107003,
 author = {Bhandarkar, Dileep and Clark, Douglas W.},
 title = {Performance from architecture: comparing a RISC and a CISC with similar hardware organization},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {19},
 issue = {2},
 month = {April},
 year = {1991},
 issn = {0163-5964},
 pages = {310--319},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106975.107003},
 doi = {http://doi.acm.org/10.1145/106975.107003},
 acmid = {107003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bhandarkar:1991:PAC:106972.107003,
 author = {Bhandarkar, Dileep and Clark, Douglas W.},
 title = {Performance from architecture: comparing a RISC and a CISC with similar hardware organization},
 abstract = {},
 booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-IV},
 year = {1991},
 isbn = {0-89791-380-9},
 location = {Santa Clara, California, United States},
 pages = {310--319},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106972.107003},
 doi = {http://doi.acm.org/10.1145/106972.107003},
 acmid = {107003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhandarkar:1991:PAC:106974.107003,
 author = {Bhandarkar, Dileep and Clark, Douglas W.},
 title = {Performance from architecture: comparing a RISC and a CISC with similar hardware organization},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {25},
 issue = {Special Issue},
 month = {April},
 year = {1991},
 issn = {0163-5980},
 pages = {310--319},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106974.107003},
 doi = {http://doi.acm.org/10.1145/106974.107003},
 acmid = {107003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhandarkar:1991:PAC:106973.107003,
 author = {Bhandarkar, Dileep and Clark, Douglas W.},
 title = {Performance from architecture: comparing a RISC and a CISC with similar hardware organization},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {26},
 issue = {4},
 month = {April},
 year = {1991},
 issn = {0362-1340},
 pages = {310--319},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/106973.107003},
 doi = {http://doi.acm.org/10.1145/106973.107003},
 acmid = {107003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cohn:1989:ACT:70082.68183,
 author = {Cohn, Robert and Gross, Thomas and Lam, Monica},
 title = {Architecture and compiler tradeoffs for a long instruction wordprocessor},
 abstract = {A very long instruction word (VLIW) processor exploits parallelism by controlling multiple operations in a single instruction word. This paper describes the architecture and compiler tradeoffs in the design of iWarp, a VLIW single-chip microprocessor developed in a joint project with Intel Corp. The iWarp processor is capable of specifying up to nine operations in an instruction word and has a peak performance of 20 million floating-point operations and 20 million integer operations per second. An optimizing compiler has been constructed and used as a tool to evaluate the different architectural proposals in the development of iWarp. We present here the analysis and compiler optimizations for those architectural features that address two key issues in the design of a VLIW microprocessor: code density and a streamlined execution cycle. We support the results of our analysis with performance data for the Livermore Loops and a selection of programs from the LINPACK library.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {2--14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/70082.68183},
 doi = {http://doi.acm.org/10.1145/70082.68183},
 acmid = {68183},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cohn:1989:ACT:68182.68183,
 author = {Cohn, Robert and Gross, Thomas and Lam, Monica},
 title = {Architecture and compiler tradeoffs for a long instruction wordprocessor},
 abstract = {A very long instruction word (VLIW) processor exploits parallelism by controlling multiple operations in a single instruction word. This paper describes the architecture and compiler tradeoffs in the design of iWarp, a VLIW single-chip microprocessor developed in a joint project with Intel Corp. The iWarp processor is capable of specifying up to nine operations in an instruction word and has a peak performance of 20 million floating-point operations and 20 million integer operations per second. An optimizing compiler has been constructed and used as a tool to evaluate the different architectural proposals in the development of iWarp. We present here the analysis and compiler optimizations for those architectural features that address two key issues in the design of a VLIW microprocessor: code density and a streamlined execution cycle. We support the results of our analysis with performance data for the Livermore Loops and a selection of programs from the LINPACK library.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {2--14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/68182.68183},
 doi = {http://doi.acm.org/10.1145/68182.68183},
 acmid = {68183},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sohi:1989:TIF:70082.68184,
 author = {Sohi, Gurindar S. and Vajapeyam, Sriram},
 title = {Tradeoffs in instruction format design for horizontal architectures},
 abstract = {With recent improvements in software techniques and the enhanced level of fine grain parallelism made available by such techniques, there has been an increased interest in horizontal architectures and large instruction words that are capable of issuing more that one operation per instruction. This paper investigates some issues in the design of such instruction formats. We study how the choice of an instruction format is influenced by factors such as the degree of pipelining and the instruction's view of the register file. Our results suggest that very large instruction words capable of issuing one operation to each functional unit resource in a horizontal architecture may be overkill. Restricted instruction formats with limited operation issuing capabilities are capable of providing similar performance (measured by the total number of time steps) with significantly less hardware in many cases.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {15--25},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/70082.68184},
 doi = {http://doi.acm.org/10.1145/70082.68184},
 acmid = {68184},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sohi:1989:TIF:68182.68184,
 author = {Sohi, Gurindar S. and Vajapeyam, Sriram},
 title = {Tradeoffs in instruction format design for horizontal architectures},
 abstract = {With recent improvements in software techniques and the enhanced level of fine grain parallelism made available by such techniques, there has been an increased interest in horizontal architectures and large instruction words that are capable of issuing more that one operation per instruction. This paper investigates some issues in the design of such instruction formats. We study how the choice of an instruction format is influenced by factors such as the degree of pipelining and the instruction's view of the register file. Our results suggest that very large instruction words capable of issuing one operation to each functional unit resource in a horizontal architecture may be overkill. Restricted instruction formats with limited operation issuing capabilities are capable of providing similar performance (measured by the total number of time steps) with significantly less hardware in many cases.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {15--25},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/68182.68184},
 doi = {http://doi.acm.org/10.1145/68182.68184},
 acmid = {68184},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dehnert:1989:OLS:70082.68185,
 author = {Dehnert, James C. and Hsu, Peter Y.-T. and Bratt, Joseph P.},
 title = {Overlapped loop support in the Cydra 5},
 abstract = {The Cydra<supscrpt>TM</supscrpt> 5 architecture adds unique support for overlapping successive iterations of a loop to a very long instruction word (VLIW) base. This architecture allows highly parallel loop execution for a much larger class of loops than can be vectorized, without requiring the unrolling of loops usually used by compilers for VLIW machines. This paper discusses the Cydra 5 loop scheduling model, the special architectural features which support it, and the loop compilation techniques used to take full advantage of the architecture.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {26--38},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/70082.68185},
 doi = {http://doi.acm.org/10.1145/70082.68185},
 acmid = {68185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dehnert:1989:OLS:68182.68185,
 author = {Dehnert, James C. and Hsu, Peter Y.-T. and Bratt, Joseph P.},
 title = {Overlapped loop support in the Cydra 5},
 abstract = {The Cydra<supscrpt>TM</supscrpt> 5 architecture adds unique support for overlapping successive iterations of a loop to a very long instruction word (VLIW) base. This architecture allows highly parallel loop execution for a much larger class of loops than can be vectorized, without requiring the unrolling of loops usually used by compilers for VLIW machines. This paper discusses the Cydra 5 loop scheduling model, the special architectural features which support it, and the loop compilation techniques used to take full advantage of the architecture.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {26--38},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/68182.68185},
 doi = {http://doi.acm.org/10.1145/68182.68185},
 acmid = {68185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Burkowski:1989:ASS:70082.68186,
 author = {Burkowski, F. J. and Cormack, G. V. and Dueck, G. D. P.},
 title = {Architectural support for synchronous task communication},
 abstract = {This paper describes the motivation for a set of intertask communication primitives, the hardware support of these primitives, the architecture used in the Sylvan project which studies these issues, and the experience gained from various experiments conducted in this area. We start by describing how these facilities have been implemented in a multiprocessor configuration that utilizes a shared backplane. This configuration represents a single node in the system. The latter part of the paper discusses a distributed multiple node system and the extension of the primitives that are used in this expanded environment.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {40--53},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/70082.68186},
 doi = {http://doi.acm.org/10.1145/70082.68186},
 acmid = {68186},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burkowski:1989:ASS:68182.68186,
 author = {Burkowski, F. J. and Cormack, G. V. and Dueck, G. D. P.},
 title = {Architectural support for synchronous task communication},
 abstract = {This paper describes the motivation for a set of intertask communication primitives, the hardware support of these primitives, the architecture used in the Sylvan project which studies these issues, and the experience gained from various experiments conducted in this area. We start by describing how these facilities have been implemented in a multiprocessor configuration that utilizes a shared backplane. This configuration represents a single node in the system. The latter part of the paper discusses a distributed multiple node system and the extension of the primitives that are used in this expanded environment.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {40--53},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/68182.68186},
 doi = {http://doi.acm.org/10.1145/68182.68186},
 acmid = {68186},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gupta:1989:FBM:70082.68187,
 author = {Gupta, Rajiv},
 title = {The fuzzy barrier: a mechanism for high speed synchronization of processors},
 abstract = {Parallel programs are commonly written using barriers to synchronize parallel processes. Upon reaching a barrier, a processor must stall until all participating processors reach the barrier. A software implementation of the barrier mechanism using shared variables has two major drawbacks. Firstly, the execution of the barrier may be slow as it may not only require execution of several instructions and but also result in hot-spot accesses. Secondly, processors that are stalled waiting for other processors to reach the barrier are essentially idling and cannot do any useful work. In this paper, the notion of the fuzzy barrier is presented, that avoids the above drawbacks. The first problem is avoided by implementing the mechanism in hardware. The second problem is solved by extending the barrier concept to include a region of statements that can be executed by a processor while it awaits synchronization. The barrier regions are constructed by a compiler and consist of several instructions such that a processor is ready to synchronize upon reaching the first instruction in this region and must synchronize before exiting the region. When synchronization does occur, the processors could be executing at any point in their respective barrier regions. The larger the barrier region, the more likely it is that none of the processors will have to stall. Preliminary investigations show that barrier regions can be large and the use of program transformations can significantly increase their size. Examples of situations where such a mechanism can result in improved performance are presented. Results based on a software implementation of the fuzzy barrier on the Encore multiprocessor indicate that the synchronization overhead can be greatly reduced using the mechanism.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {54--63},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/70082.68187},
 doi = {http://doi.acm.org/10.1145/70082.68187},
 acmid = {68187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gupta:1989:FBM:68182.68187,
 author = {Gupta, Rajiv},
 title = {The fuzzy barrier: a mechanism for high speed synchronization of processors},
 abstract = {Parallel programs are commonly written using barriers to synchronize parallel processes. Upon reaching a barrier, a processor must stall until all participating processors reach the barrier. A software implementation of the barrier mechanism using shared variables has two major drawbacks. Firstly, the execution of the barrier may be slow as it may not only require execution of several instructions and but also result in hot-spot accesses. Secondly, processors that are stalled waiting for other processors to reach the barrier are essentially idling and cannot do any useful work. In this paper, the notion of the fuzzy barrier is presented, that avoids the above drawbacks. The first problem is avoided by implementing the mechanism in hardware. The second problem is solved by extending the barrier concept to include a region of statements that can be executed by a processor while it awaits synchronization. The barrier regions are constructed by a compiler and consist of several instructions such that a processor is ready to synchronize upon reaching the first instruction in this region and must synchronize before exiting the region. When synchronization does occur, the processors could be executing at any point in their respective barrier regions. The larger the barrier region, the more likely it is that none of the processors will have to stall. Preliminary investigations show that barrier regions can be large and the use of program transformations can significantly increase their size. Examples of situations where such a mechanism can result in improved performance are presented. Results based on a software implementation of the fuzzy barrier on the Encore multiprocessor indicate that the synchronization overhead can be greatly reduced using the mechanism.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {54--63},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/68182.68187},
 doi = {http://doi.acm.org/10.1145/68182.68187},
 acmid = {68187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goodman:1989:ESP:68182.68188,
 author = {Goodman, James R. and Vernon, Mary K. and Woest, Philip J.},
 title = {Efficient synchronization primitives for large-scale cache-coherent multiprocessors},
 abstract = {This paper proposes a set of efficient primitives for process synchronization in multiprocessors. The only assumptions made in developing the set of primitives are that hardware combining is not implemented in the inter-connect, and (in one case) that the interconnect supports broadcast.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {64--75},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/68182.68188},
 doi = {http://doi.acm.org/10.1145/68182.68188},
 acmid = {68188},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goodman:1989:ESP:70082.68188,
 author = {Goodman, James R. and Vernon, Mary K. and Woest, Philip J.},
 title = {Efficient synchronization primitives for large-scale cache-coherent multiprocessors},
 abstract = {This paper proposes a set of efficient primitives for process synchronization in multiprocessors. The only assumptions made in developing the set of primitives are that hardware combining is not implemented in the inter-connect, and (in one case) that the interconnect supports broadcast.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {64--75},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/70082.68188},
 doi = {http://doi.acm.org/10.1145/70082.68188},
 acmid = {68188},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mellor-Crummey:1989:SIC:68182.68189,
 author = {Mellor-Crummey, J. M. and LeBlanc, T. J.},
 title = {A software instruction counter},
 abstract = {Although several recent papers have proposed architectural support for program debugging and profiling, most processors do not yet provide even basic facilities, such as an instruction counter. As a result, system developers have been forced to invent software solutions. This paper describes our implementation of a software instruction counter for program debugging. We show that an instruction counter can be reasonably implemented in software, often with less than 10\% execution overhead. Our experience suggests that a hardware instruction counter is not necessary for a practical implementation of watch-points and reverse execution, however it will make program instrumentation much easier for the system developer.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {78--86},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/68182.68189},
 doi = {http://doi.acm.org/10.1145/68182.68189},
 acmid = {68189},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mellor-Crummey:1989:SIC:70082.68189,
 author = {Mellor-Crummey, J. M. and LeBlanc, T. J.},
 title = {A software instruction counter},
 abstract = {Although several recent papers have proposed architectural support for program debugging and profiling, most processors do not yet provide even basic facilities, such as an instruction counter. As a result, system developers have been forced to invent software solutions. This paper describes our implementation of a software instruction counter for program debugging. We show that an instruction counter can be reasonably implemented in software, often with less than 10\% execution overhead. Our experience suggests that a hardware instruction counter is not necessary for a practical implementation of watch-points and reverse execution, however it will make program instrumentation much easier for the system developer.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {78--86},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/70082.68189},
 doi = {http://doi.acm.org/10.1145/70082.68189},
 acmid = {68189},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Aral:1989:EDP:70082.68190,
 author = {Aral, Z. and Gerther, I. and Schaffer, G.},
 title = {Efficient debugging primitives for multiprocessors},
 abstract = {Existing kernel-level debugging primitives are inappropriate for instrumenting complex sequential or parallel programs. These functions incur a heavy overhead in their use of system calls and process switches. Context switches are used to alternately invoke the debugger and the target programs. System calls are used to communicate data between the target and debugger.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {87--95},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/70082.68190},
 doi = {http://doi.acm.org/10.1145/70082.68190},
 acmid = {68190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aral:1989:EDP:68182.68190,
 author = {Aral, Z. and Gerther, I. and Schaffer, G.},
 title = {Efficient debugging primitives for multiprocessors},
 abstract = {Existing kernel-level debugging primitives are inappropriate for instrumenting complex sequential or parallel programs. These functions incur a heavy overhead in their use of system calls and process switches. Context switches are used to alternately invoke the debugger and the target programs. System calls are used to communicate data between the target and debugger.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {87--95},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/68182.68190},
 doi = {http://doi.acm.org/10.1145/68182.68190},
 acmid = {68190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Staknis:1989:SMA:70082.68191,
 author = {Staknis, M. E.},
 title = {Sheaved memory: architectural support for state saving and restoration in pages systems},
 abstract = {The concept of read-one/write-many paged memory is introduced and given the name sheaved memory. It is shown that sheaved memory is useful for efficiently maintaining checkpoints in main memory and for providing state saving and state restoration for software that includes recovery blocks or similar control structures. The organization of sheaved memory is described in detail, and a design is presented for a prototype sheaved-memory module that can be built easily from inexpensive, off-the-shelf components. The module can be incorporated within many available computers without altering the computers' hardware design. The concept of sheaved memory is simple and appealing, and its potential for use in a number of software contexts is foreseen.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {96--102},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/70082.68191},
 doi = {http://doi.acm.org/10.1145/70082.68191},
 acmid = {68191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Staknis:1989:SMA:68182.68191,
 author = {Staknis, M. E.},
 title = {Sheaved memory: architectural support for state saving and restoration in pages systems},
 abstract = {The concept of read-one/write-many paged memory is introduced and given the name sheaved memory. It is shown that sheaved memory is useful for efficiently maintaining checkpoints in main memory and for providing state saving and state restoration for software that includes recovery blocks or similar control structures. The organization of sheaved memory is described in detail, and a design is presented for a prototype sheaved-memory module that can be built easily from inexpensive, off-the-shelf components. The module can be incorporated within many available computers without altering the computers' hardware design. The concept of sheaved memory is simple and appealing, and its potential for use in a number of software contexts is foreseen.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {96--102},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/68182.68191},
 doi = {http://doi.acm.org/10.1145/68182.68191},
 acmid = {68191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Holliday:1989:RHP:68182.68192,
 author = {Holliday, M. A.},
 title = {Reference history, page size, and migration daemons in local/remote architectures},
 abstract = {We address the problem of paged main memory management in the local/remote architecture subclass of shared memory multiprocessors. We consider the case where the operating system has primary responsibility and uses page migration as its main tool. We identify some of the key issues with respect to architectural support (reference history maintenance, and page size), and operating system mechanism (duration between daemon passes, and number of migration daemons).
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {104--112},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/68182.68192},
 doi = {http://doi.acm.org/10.1145/68182.68192},
 acmid = {68192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Holliday:1989:RHP:70082.68192,
 author = {Holliday, M. A.},
 title = {Reference history, page size, and migration daemons in local/remote architectures},
 abstract = {We address the problem of paged main memory management in the local/remote architecture subclass of shared memory multiprocessors. We consider the case where the operating system has primary responsibility and uses page migration as its main tool. We identify some of the key issues with respect to architectural support (reference history maintenance, and page size), and operating system mechanism (duration between daemon passes, and number of migration daemons).
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {104--112},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/70082.68192},
 doi = {http://doi.acm.org/10.1145/70082.68192},
 acmid = {68192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Black:1989:TLB:68182.68193,
 author = {Black, D. L. and Rashid, R. F. and Golub, D. B. and Hill, C. R.},
 title = {Translation lookaside buffer consistency: a software approach},
 abstract = {We discuss the translation lookaside buffer (TLB) consistency problem for multiprocessors, and introduce the Mach shootdown algorithm for maintaining TLB consistency in software. This algorithm has been implemented on several multiprocessors, and is in regular production use. Performance evaluations establish the basic costs of the algorithm and show that it has minimal impact on application performance. As a result, TLB consistency does not pose an insurmountable obstacle to multiprocessors with several hundred processors. We also discuss hardware support options for TLB consistency ranging from a minor interrupt structure modification to complete hardware implementations. Features are identified in current hardware that compound the TLB consistency problem; removal or correction of these features can simplify and/or reduce the overhead of maintaining TLB consistency in software.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {113--122},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/68182.68193},
 doi = {http://doi.acm.org/10.1145/68182.68193},
 acmid = {68193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Black:1989:TLB:70082.68193,
 author = {Black, D. L. and Rashid, R. F. and Golub, D. B. and Hill, C. R.},
 title = {Translation lookaside buffer consistency: a software approach},
 abstract = {We discuss the translation lookaside buffer (TLB) consistency problem for multiprocessors, and introduce the Mach shootdown algorithm for maintaining TLB consistency in software. This algorithm has been implemented on several multiprocessors, and is in regular production use. Performance evaluations establish the basic costs of the algorithm and show that it has minimal impact on application performance. As a result, TLB consistency does not pose an insurmountable obstacle to multiprocessors with several hundred processors. We also discuss hardware support options for TLB consistency ranging from a minor interrupt structure modification to complete hardware implementations. Features are identified in current hardware that compound the TLB consistency problem; removal or correction of these features can simplify and/or reduce the overhead of maintaining TLB consistency in software.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {113--122},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/70082.68193},
 doi = {http://doi.acm.org/10.1145/70082.68193},
 acmid = {68193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gibson:1989:FCT:68182.68194,
 author = {Gibson, G. A. and Hellerstein, L. and Karp, R. M. and Patterson, D. A.},
 title = {Failure correction techniques for large disk arrays},
 abstract = {The ever increasing need for I/O bandwidth will be met with ever larger arrays of disks. These arrays require redundancy to protect against data loss. This paper examines alternative choices for encodings, or codes, that reliably store information in disk arrays. Codes are selected to maximize mean time to data loss or minimize disks containing redundant data, but are all constrained to minimize performance penalties associated with updating information or recovering from catastrophic disk failures. We also codes that give highly reliable data storage with low redundant data overhead for arrays of 1000 information disks.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/68182.68194},
 doi = {http://doi.acm.org/10.1145/68182.68194},
 acmid = {68194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gibson:1989:FCT:70082.68194,
 author = {Gibson, G. A. and Hellerstein, L. and Karp, R. M. and Patterson, D. A.},
 title = {Failure correction techniques for large disk arrays},
 abstract = {The ever increasing need for I/O bandwidth will be met with ever larger arrays of disks. These arrays require redundancy to protect against data loss. This paper examines alternative choices for encodings, or codes, that reliably store information in disk arrays. Codes are selected to maximize mean time to data loss or minimize disks containing redundant data, but are all constrained to minimize performance penalties associated with updating information or recovering from catastrophic disk failures. We also codes that give highly reliable data storage with low redundant data overhead for arrays of 1000 information disks.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/70082.68194},
 doi = {http://doi.acm.org/10.1145/70082.68194},
 acmid = {68194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jouppi:1989:UVF:70082.68195,
 author = {Jouppi, N. P. and Bertoni, J. and Wall, D. W.},
 title = {A unified vector/scalar floating-point architecture},
 abstract = {In this paper we present a unified approach to vector and scalar computation, using a single register file for both scalar operands and vector elements. The goal of this architecture is to yield improved scalar performance while broadening the range of vectorizable applications. For example, reduction operations and recurrences can be expressed in vector form in this architecture. This approach results in greater overall performance for most applications than does the approach of emphasizing peak vector performance. The hardware required to support the enhanced vector capability is insignificant, but allows the execution of two operations per cycle for vectorized code. Moreover, the size of the unified vector/scalar register file required for peak performance is an order of magnitude smaller than traditional vector register files, allowing efficient on-chip VLSI implementation. The results of simulations of the Livermore Loops and Linpack using this architecture are presented.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/70082.68195},
 doi = {http://doi.acm.org/10.1145/70082.68195},
 acmid = {68195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jouppi:1989:UVF:68182.68195,
 author = {Jouppi, N. P. and Bertoni, J. and Wall, D. W.},
 title = {A unified vector/scalar floating-point architecture},
 abstract = {In this paper we present a unified approach to vector and scalar computation, using a single register file for both scalar operands and vector elements. The goal of this architecture is to yield improved scalar performance while broadening the range of vectorizable applications. For example, reduction operations and recurrences can be expressed in vector form in this architecture. This approach results in greater overall performance for most applications than does the approach of emphasizing peak vector performance. The hardware required to support the enhanced vector capability is insignificant, but allows the execution of two operations per cycle for vectorized code. Moreover, the size of the unified vector/scalar register file required for peak performance is an order of magnitude smaller than traditional vector register files, allowing efficient on-chip VLSI implementation. The results of simulations of the Livermore Loops and Linpack using this architecture are presented.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/68182.68195},
 doi = {http://doi.acm.org/10.1145/68182.68195},
 acmid = {68195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mulder:1989:DBR:68182.68196,
 author = {Mulder, H.},
 title = {Data buffering: run-time versus compile-time support},
 abstract = {Data-dependency, branch, and memory-access penalties are main constraints on the performance of high-speed microprocessors. The memory-access penalties concern both penalties imposed by external memory (e.g. cache) or by under utilization of the local processor memory (e.g. registers). This paper focuses solely on methods of increasing the utilization of data memory, local to the processor (registers or register-oriented buffers).
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {144--151},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/68182.68196},
 doi = {http://doi.acm.org/10.1145/68182.68196},
 acmid = {68196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mulder:1989:DBR:70082.68196,
 author = {Mulder, H.},
 title = {Data buffering: run-time versus compile-time support},
 abstract = {Data-dependency, branch, and memory-access penalties are main constraints on the performance of high-speed microprocessors. The memory-access penalties concern both penalties imposed by external memory (e.g. cache) or by under utilization of the local processor memory (e.g. registers). This paper focuses solely on methods of increasing the utilization of data memory, local to the processor (registers or register-oriented buffers).
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {144--151},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/70082.68196},
 doi = {http://doi.acm.org/10.1145/70082.68196},
 acmid = {68196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adams:1989:AIS:70082.68197,
 author = {Adams, T. L. and Zimmerman, R. E.},
 title = {An analysis of 8086 instruction set usage in MS DOS programs},
 abstract = {},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {152--160},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/70082.68197},
 doi = {http://doi.acm.org/10.1145/70082.68197},
 acmid = {68197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adams:1989:AIS:68182.68197,
 author = {Adams, T. L. and Zimmerman, R. E.},
 title = {An analysis of 8086 instruction set usage in MS DOS programs},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {152--160},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/68182.68197},
 doi = {http://doi.acm.org/10.1145/68182.68197},
 acmid = {68197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Roos:1989:RSP:68182.68198,
 author = {Roos, J.},
 title = {A real-time support processor for ada tasking},
 abstract = {Task synchronization in Ada causes excessive run-time overhead due to the complex semantics of the rendezvous. To demonstrate that the speed can be increased by two orders of magnitude by using special purpose hardware, a single chip VLSI support processor has been designed. By providing predictable and uniformly low overhead for the entire semantics of a rendezvous, the powerful real-time constructs of Ada can be used freely without performance degradation.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {162--171},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/68182.68198},
 doi = {http://doi.acm.org/10.1145/68182.68198},
 acmid = {68198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Roos:1989:RSP:70082.68198,
 author = {Roos, J.},
 title = {A real-time support processor for ada tasking},
 abstract = {Task synchronization in Ada causes excessive run-time overhead due to the complex semantics of the rendezvous. To demonstrate that the speed can be increased by two orders of magnitude by using special purpose hardware, a single chip VLSI support processor has been designed. By providing predictable and uniformly low overhead for the entire semantics of a rendezvous, the powerful real-time constructs of Ada can be used freely without performance degradation.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {162--171},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/70082.68198},
 doi = {http://doi.acm.org/10.1145/70082.68198},
 acmid = {68198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vegdahl:1989:RES:70082.68199,
 author = {Vegdahl, Steven R. and Pleban, Uwe F.},
 title = {The runtime environment for Sche a Scheme implementation on the 88000},
 abstract = {We are implementing a Scheme development system for the Motorola 88000. The core of the implementation is an optimizing native code compiler, together with a carefully designed runtime system. This paper describes our experiences with the 88000 as a target architecture. We focus on the design decisions concerning the runtime system, particularly with respect to data type representations, tag checking, procedure calling protocol, generic arithmetic, and the handling of continuations. We also discuss rejected design alternatives, and evaluate the strengths and weaknesses of the instruction set with respect to our constraints.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {172--182},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/70082.68199},
 doi = {http://doi.acm.org/10.1145/70082.68199},
 acmid = {68199},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vegdahl:1989:RES:68182.68199,
 author = {Vegdahl, Steven R. and Pleban, Uwe F.},
 title = {The runtime environment for Sche a Scheme implementation on the 88000},
 abstract = {We are implementing a Scheme development system for the Motorola 88000. The core of the implementation is an optimizing native code compiler, together with a carefully designed runtime system. This paper describes our experiences with the 88000 as a target architecture. We focus on the design decisions concerning the runtime system, particularly with respect to data type representations, tag checking, procedure calling protocol, generic arithmetic, and the handling of continuations. We also discuss rejected design alternatives, and evaluate the strengths and weaknesses of the instruction set with respect to our constraints.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {172--182},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/68182.68199},
 doi = {http://doi.acm.org/10.1145/68182.68199},
 acmid = {68199},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McFarling:1989:POI:68182.68200,
 author = {McFarling, S.},
 title = {Program optimization for instruction caches},
 abstract = {This paper presents an optimization algorithm for reducing instruction cache misses. The algorithm uses profile information to reposition programs in memory so that a direct-mapped cache behaves much like an optimal cache with full associativity and full knowledge of the future. For best results, the cache should have a mechanism for excluding certain instructions designated by the compiler. This paper first presents a reduced form of the algorithm. This form is shown to produce an optimal miss rate for programs without conditionals and with a tree call graph, assuming basic blocks can be reordered at will. If conditionals are allowed, but there are no loops within conditionals, the algorithm does as well as an optimal cache for the worst case execution of the program consistent with the profile information. Next, the algorithm is extended with heuristics for general programs. The effectiveness of these heuristics are demonstrated with empirical results for a set of 10 programs for various cache sizes. The improvement depends on cache size. For a 512 word cache, miss rates for a direct-mapped instruction cache are halved. For an 8K word cache, miss rates fall by over 75\%. Over a wide range of cache sizes the algorithm is as effective as increasing the cache size by a factor of 3 times. For 512 words, the algorithm generates only 32\% more misses than an optimal cache. Optimized programs on a direct-mapped cache have lower miss rates than unoptimized programs on set-associative caches of the same size.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {183--191},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/68182.68200},
 doi = {http://doi.acm.org/10.1145/68182.68200},
 acmid = {68200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McFarling:1989:POI:70082.68200,
 author = {McFarling, S.},
 title = {Program optimization for instruction caches},
 abstract = {This paper presents an optimization algorithm for reducing instruction cache misses. The algorithm uses profile information to reposition programs in memory so that a direct-mapped cache behaves much like an optimal cache with full associativity and full knowledge of the future. For best results, the cache should have a mechanism for excluding certain instructions designated by the compiler. This paper first presents a reduced form of the algorithm. This form is shown to produce an optimal miss rate for programs without conditionals and with a tree call graph, assuming basic blocks can be reordered at will. If conditionals are allowed, but there are no loops within conditionals, the algorithm does as well as an optimal cache for the worst case execution of the program consistent with the profile information. Next, the algorithm is extended with heuristics for general programs. The effectiveness of these heuristics are demonstrated with empirical results for a set of 10 programs for various cache sizes. The improvement depends on cache size. For a 512 word cache, miss rates for a direct-mapped instruction cache are halved. For an 8K word cache, miss rates fall by over 75\%. Over a wide range of cache sizes the algorithm is as effective as increasing the cache size by a factor of 3 times. For 512 words, the algorithm generates only 32\% more misses than an optimal cache. Optimized programs on a direct-mapped cache have lower miss rates than unoptimized programs on set-associative caches of the same size.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {183--191},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/70082.68200},
 doi = {http://doi.acm.org/10.1145/70082.68200},
 acmid = {68200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Karger:1989:URO:68182.68201,
 author = {Karger, Paul A.},
 title = {Using registers to optimize cross-domain call performance},
 abstract = {This paper describes a new technique to improve the performance of cross-domain calls and returns in a capability-based computer system. Using register optimization information obtained from the compiler, a trusted linker can minimize the number of registers that must be saved, restored, or cleared when changing from one protection domain to another. The size of the performance gain depends on the level of trust between the calling and called protection domains. The paper presents alternate implementations for an extended VAX architecture and for a RISC architecture and reports performance measurements done on a re-microprogrammed VAX-11/730 processor.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {194--204},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/68182.68201},
 doi = {http://doi.acm.org/10.1145/68182.68201},
 acmid = {68201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Karger:1989:URO:70082.68201,
 author = {Karger, Paul A.},
 title = {Using registers to optimize cross-domain call performance},
 abstract = {This paper describes a new technique to improve the performance of cross-domain calls and returns in a capability-based computer system. Using register optimization information obtained from the compiler, a trusted linker can minimize the number of registers that must be saved, restored, or cleared when changing from one protection domain to another. The size of the performance gain depends on the level of trust between the calling and called protection domains. The paper presents alternate implementations for an extended VAX architecture and for a RISC architecture and reports performance measurements done on a re-microprogrammed VAX-11/730 processor.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {194--204},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/70082.68201},
 doi = {http://doi.acm.org/10.1145/70082.68201},
 acmid = {68201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arnould:1989:DNN:70082.68202,
 author = {Arnould, Emmanuel and Kung, H. T. and Bitz, Francois and Sansom, Robert D. and Cooperm, Eric C.},
 title = {The design of nectar: a network backplane for heterogeneous multicomputers},
 abstract = {Nectar is a ``network backplane" for use in heterogeneous multicomputers. The initial system consists of a star-shaped fiber-optic network with an aggregate bandwidth of 1.6 gigabits/second and a switching latency of 700 nanoseconds. The system can be scaled up by connecting hundreds of these networks together.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/70082.68202},
 doi = {http://doi.acm.org/10.1145/70082.68202},
 acmid = {68202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arnould:1989:DNN:68182.68202,
 author = {Arnould, Emmanuel and Kung, H. T. and Bitz, Francois and Sansom, Robert D. and Cooperm, Eric C.},
 title = {The design of nectar: a network backplane for heterogeneous multicomputers},
 abstract = {Nectar is a ``network backplane" for use in heterogeneous multicomputers. The initial system consists of a star-shaped fiber-optic network with an aggregate bandwidth of 1.6 gigabits/second and a switching latency of 700 nanoseconds. The system can be scaled up by connecting hundreds of these networks together.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/68182.68202},
 doi = {http://doi.acm.org/10.1145/68182.68202},
 acmid = {68202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Delgado-Rannauro:1989:MDO:68182.68203,
 author = {Delgado-Rannauro, S. A. and Reynolds, T. J.},
 title = {A message driven OR-parallel machine},
 abstract = {A message driven architecture for the execution of OR-parallel logic languages is proposed. The computational model is based on well known compilation techniques for Logic Languages. We present first the multiple binding mechanism for the OR-parallel Prolog architecture and the corresponding OR-parallel abstract machine is described. A scheduling algorithm which does not rely upon the availability of global data structures to direct the search for work is discussed. The message driven processor, the processing node of the parallel machine, is designed to interact with a shared global address space and to efficiently process messages from other processing nodes. We discuss some of the results obtained from a high level functional simulator of the message driven machine.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/68182.68203},
 doi = {http://doi.acm.org/10.1145/68182.68203},
 acmid = {68203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Delgado-Rannauro:1989:MDO:70082.68203,
 author = {Delgado-Rannauro, S. A. and Reynolds, T. J.},
 title = {A message driven OR-parallel machine},
 abstract = {A message driven architecture for the execution of OR-parallel logic languages is proposed. The computational model is based on well known compilation techniques for Logic Languages. We present first the multiple binding mechanism for the OR-parallel Prolog architecture and the corresponding OR-parallel abstract machine is described. A scheduling algorithm which does not rely upon the availability of global data structures to direct the search for work is discussed. The message driven processor, the processing node of the parallel machine, is designed to interact with a shared global address space and to efficiently process messages from other processing nodes. We discuss some of the results obtained from a high level functional simulator of the message driven machine.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/70082.68203},
 doi = {http://doi.acm.org/10.1145/70082.68203},
 acmid = {68203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Owicki:1989:EPS:70082.68204,
 author = {Owicki, S. and Agarwal, A.},
 title = {Evaluating the performance of software cache coherence},
 abstract = {In a shared-memory multiprocessor with private caches, cached copies of a data item must be kept consistent. This is called cache coherence. Both hardware and software coherence schemes have been proposed. Software techniques are attractive because they avoid hardware complexity and can be used with any processor-memory interconnection. This paper presents an analytical model of the performance of two software coherence schemes and, for comparison, snoopy-cache hardware. The model is validated against address traces from a bus-based multiprocessor. The behavior of the coherence schemes under various workloads is compared, and their sensitivity to variations in workload parameters is assessed. The analysis shows that the performance of software schemes is critically determined by certain parameters of the workload: the proportion of data accesses, the fraction of shared references, and the number of times a shared block is accessed before it is purged from the cache. Snoopy caches are more resilient to variations in these parameters. Thus when evaluating a software scheme as a design alternative, it is essential to consider the characteristics of the expected workload. The performance of the two software schemes with a multistage interconnection network is also evaluated, and it is determined that both scale well.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {230--242},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/70082.68204},
 doi = {http://doi.acm.org/10.1145/70082.68204},
 acmid = {68204},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Owicki:1989:EPS:68182.68204,
 author = {Owicki, S. and Agarwal, A.},
 title = {Evaluating the performance of software cache coherence},
 abstract = {In a shared-memory multiprocessor with private caches, cached copies of a data item must be kept consistent. This is called cache coherence. Both hardware and software coherence schemes have been proposed. Software techniques are attractive because they avoid hardware complexity and can be used with any processor-memory interconnection. This paper presents an analytical model of the performance of two software coherence schemes and, for comparison, snoopy-cache hardware. The model is validated against address traces from a bus-based multiprocessor. The behavior of the coherence schemes under various workloads is compared, and their sensitivity to variations in workload parameters is assessed. The analysis shows that the performance of software schemes is critically determined by certain parameters of the workload: the proportion of data accesses, the fraction of shared references, and the number of times a shared block is accessed before it is purged from the cache. Snoopy caches are more resilient to variations in these parameters. Thus when evaluating a software scheme as a design alternative, it is essential to consider the characteristics of the expected workload. The performance of the two software schemes with a multistage interconnection network is also evaluated, and it is determined that both scale well.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {230--242},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/68182.68204},
 doi = {http://doi.acm.org/10.1145/68182.68204},
 acmid = {68204},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weber:1989:ACI:68182.68205,
 author = {Weber, W. and Gupta, A.},
 title = {Analysis of cache invalidation patterns in multiprocessors},
 abstract = {To make shared-memory multiprocessors scalable, researchers are now exploring cache coherence protocols that do not rely on broadcast, but instead send invalidation messages to individual caches that contain stale data. The feasibility of such directory-based protocols is highly sensitive to the cache invalidation patterns that parallel programs exhibit. In this paper, we analyze the cache invalidation patterns caused by several parallel applications and investigate the effect of these patterns on a directory-based protocol. Our results are based on multiprocessor traces with 4, 8 and 16 processors. To gain insight into what the invalidation patterns would look like beyond 16 processors, we propose a classification scheme for data objects found in parallel applications and link the invalidation traffic patterns observed in the traces back to these high-level objects. Our results show that synchronization objects have very different invalidation patterns from those of other data objects. A write reference to a synchronization object usually causes invalidations in many more caches. We point out situations where restructuring the application seems appropriate to reduce the invalidation traffic, and others where hardware support is more appropriate. Our results also show that it should be possible to scale ``well-written" parallel programs to a large number of processors without an explosion in invalidation traffic.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {243--256},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/68182.68205},
 doi = {http://doi.acm.org/10.1145/68182.68205},
 acmid = {68205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Weber:1989:ACI:70082.68205,
 author = {Weber, W. and Gupta, A.},
 title = {Analysis of cache invalidation patterns in multiprocessors},
 abstract = {To make shared-memory multiprocessors scalable, researchers are now exploring cache coherence protocols that do not rely on broadcast, but instead send invalidation messages to individual caches that contain stale data. The feasibility of such directory-based protocols is highly sensitive to the cache invalidation patterns that parallel programs exhibit. In this paper, we analyze the cache invalidation patterns caused by several parallel applications and investigate the effect of these patterns on a directory-based protocol. Our results are based on multiprocessor traces with 4, 8 and 16 processors. To gain insight into what the invalidation patterns would look like beyond 16 processors, we propose a classification scheme for data objects found in parallel applications and link the invalidation traffic patterns observed in the traces back to these high-level objects. Our results show that synchronization objects have very different invalidation patterns from those of other data objects. A write reference to a synchronization object usually causes invalidations in many more caches. We point out situations where restructuring the application seems appropriate to reduce the invalidation traffic, and others where hardware support is more appropriate. Our results also show that it should be possible to scale ``well-written" parallel programs to a large number of processors without an explosion in invalidation traffic.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {243--256},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/70082.68205},
 doi = {http://doi.acm.org/10.1145/70082.68205},
 acmid = {68205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eggers:1989:ESC:68182.68206,
 author = {Eggers, S. J. and Katz, R. H.},
 title = {The effect of sharing on the cache and bus performance of parallel programs},
 abstract = {Bus bandwidth ultimately limits the performance, and therefore the scale, of bus-based, shared memory multiprocessors. Previous studies have extrapolated from uniprocessor measurements and simulations to estimate the performance of these machines. In this study, we use traces of parallel programs to evaluate the cache and bus performance of shared memory multiprocessors, in which coherency is maintained by a write-invalidate protocol. In particular, we analyze the effect of sharing overhead on cache miss ratio and bus utilization.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {257--270},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/68182.68206},
 doi = {http://doi.acm.org/10.1145/68182.68206},
 acmid = {68206},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eggers:1989:ESC:70082.68206,
 author = {Eggers, S. J. and Katz, R. H.},
 title = {The effect of sharing on the cache and bus performance of parallel programs},
 abstract = {Bus bandwidth ultimately limits the performance, and therefore the scale, of bus-based, shared memory multiprocessors. Previous studies have extrapolated from uniprocessor measurements and simulations to estimate the performance of these machines. In this study, we use traces of parallel programs to evaluate the cache and bus performance of shared memory multiprocessors, in which coherency is maintained by a write-invalidate protocol. In particular, we analyze the effect of sharing overhead on cache miss ratio and bus utilization.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {257--270},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/70082.68206},
 doi = {http://doi.acm.org/10.1145/70082.68206},
 acmid = {68206},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jouppi:1989:AIP:70082.68207,
 author = {Jouppi, N. P. and Wall, D. W.},
 title = {Available instruction-level parallelism for superscalar and superpipelined machines},
 abstract = {Superscalar machines can issue several instructions per cycle. Superpipelined machines can issue only one instruction per cycle, but they have cycle times shorter than the latency of any functional unit. In this paper these two techniques are shown to be roughly equivalent ways of exploiting instruction-level parallelism. A parameterizable code reorganization and simulation system was developed and used to measure instruction-level parallelism for a series of benchmarks. Results of these simulations in the presence of various compiler optimizations are presented. The average degree of superpipelining metric is introduced. Our simulations suggest that this metric is already high for many machines. These machines already exploit all of the instruction-level parallelism available in many non-numeric applications, even without parallel instruction issue or higher degrees of pipelining.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/70082.68207},
 doi = {http://doi.acm.org/10.1145/70082.68207},
 acmid = {68207},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jouppi:1989:AIP:68182.68207,
 author = {Jouppi, N. P. and Wall, D. W.},
 title = {Available instruction-level parallelism for superscalar and superpipelined machines},
 abstract = {Superscalar machines can issue several instructions per cycle. Superpipelined machines can issue only one instruction per cycle, but they have cycle times shorter than the latency of any functional unit. In this paper these two techniques are shown to be roughly equivalent ways of exploiting instruction-level parallelism. A parameterizable code reorganization and simulation system was developed and used to measure instruction-level parallelism for a series of benchmarks. Results of these simulations in the presence of various compiler optimizations are presented. The average degree of superpipelining metric is introduced. Our simulations suggest that this metric is already high for many machines. These machines already exploit all of the instruction-level parallelism available in many non-numeric applications, even without parallel instruction issue or higher degrees of pipelining.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/68182.68207},
 doi = {http://doi.acm.org/10.1145/68182.68207},
 acmid = {68207},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dally:1989:MFO:68182.68208,
 author = {Dally, W. J.},
 title = {Micro-optimization of floating-point operations},
 abstract = {This paper describes micro-optimization, a technique for reducing the operation count and time required to perform floating-point calculations. Micro-optimization involves breaking floating-point operations into their constituent micro-operations and optimizing the resulting code. Exposing micro-operations to the compiler creates many opportunities for optimization. Redundant normalization operations can be eliminated or combined. Also, scheduling micro-operations separately allows dependent operations to be partially overlapped. A prototype expression compiler has been written to evaluate a number of micro-optimizations. On a set of benchmark expressions operation count is reduced by 33\% and execution time is reduced by 40\%.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {283--289},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/68182.68208},
 doi = {http://doi.acm.org/10.1145/68182.68208},
 acmid = {68208},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dally:1989:MFO:70082.68208,
 author = {Dally, W. J.},
 title = {Micro-optimization of floating-point operations},
 abstract = {This paper describes micro-optimization, a technique for reducing the operation count and time required to perform floating-point calculations. Micro-optimization involves breaking floating-point operations into their constituent micro-operations and optimizing the resulting code. Exposing micro-operations to the compiler creates many opportunities for optimization. Redundant normalization operations can be eliminated or combined. Also, scheduling micro-operations separately allows dependent operations to be partially overlapped. A prototype expression compiler has been written to evaluate a number of micro-optimizations. On a set of benchmark expressions operation count is reduced by 33\% and execution time is reduced by 40\%.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {283--289},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/70082.68208},
 doi = {http://doi.acm.org/10.1145/70082.68208},
 acmid = {68208},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:1989:LMI:68182.68209,
 author = {Smith, M. D. and Johnson, M. and Horowitz, M. A.},
 title = {Limits on multiple instruction issue},
 abstract = {This paper investigates the limitations on designing a processor which can sustain an execution rate of greater than one instruction per cycle on highly-optimized, non-scientific applications. We have used trace-driven simulations to determine that these applications contain enough instruction independence to sustain an instruction rate of about two instructions per cycle. In a straightforward implementation, cost considerations argue strongly against decoding more than two instructions in one cycle. Given this constraint, the efficiency in instruction fetching rather than the complexity of the execution hardware limits the concurrency attainable at the instruction level.
},
 journal = {SIGARCH Comput. Archit. News},
 volume = {17},
 issue = {2},
 month = {April},
 year = {1989},
 issn = {0163-5964},
 pages = {290--302},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/68182.68209},
 doi = {http://doi.acm.org/10.1145/68182.68209},
 acmid = {68209},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smith:1989:LMI:70082.68209,
 author = {Smith, M. D. and Johnson, M. and Horowitz, M. A.},
 title = {Limits on multiple instruction issue},
 abstract = {This paper investigates the limitations on designing a processor which can sustain an execution rate of greater than one instruction per cycle on highly-optimized, non-scientific applications. We have used trace-driven simulations to determine that these applications contain enough instruction independence to sustain an instruction rate of about two instructions per cycle. In a straightforward implementation, cost considerations argue strongly against decoding more than two instructions in one cycle. Given this constraint, the efficiency in instruction fetching rather than the complexity of the execution hardware limits the concurrency attainable at the instruction level.
},
 booktitle = {Proceedings of the third international conference on Architectural support for programming languages and operating systems},
 series = {ASPLOS-III},
 year = {1989},
 isbn = {0-89791-300-0},
 location = {Boston, Massachusetts, United States},
 pages = {290--302},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/70082.68209},
 doi = {http://doi.acm.org/10.1145/70082.68209},
 acmid = {68209},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wirth:1987:HAP:36205.36178,
 author = {Wirth, Nicklaus},
 title = {Hardware architectures for programming languages and programming languages for hardware architectures},
 abstract = {Programming Languages and Operating Systems introduce abstractions which allow the programmer to ignore details of an implementation. Support of an abstraction must not only concentrate on promoting the efficiency of an implementation, but also on providing the necessary guards against violations of the abstractions. In the frantic drive for efficiency the second goal has been neglected. There are indications that recent designs which are claimed to be both simple and powerful, achieve efficiency by shifting the complex issues of code generation and of appropriate guards onto compilers.Complexity has become the common hallmark of software as well as hardware designs. It cannot be mastered by the common practices of testing and simulation. Hardware design may profit from developments in programming methodology by adopting proof techniques similar to those used in programming.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {2--8},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/36205.36178},
 doi = {http://doi.acm.org/10.1145/36205.36178},
 acmid = {36178},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wirth:1987:HAP:36204.36178,
 author = {Wirth, Nicklaus},
 title = {Hardware architectures for programming languages and programming languages for hardware architectures},
 abstract = {Programming Languages and Operating Systems introduce abstractions which allow the programmer to ignore details of an implementation. Support of an abstraction must not only concentrate on promoting the efficiency of an implementation, but also on providing the necessary guards against violations of the abstractions. In the frantic drive for efficiency the second goal has been neglected. There are indications that recent designs which are claimed to be both simple and powerful, achieve efficiency by shifting the complex issues of code generation and of appropriate guards onto compilers.Complexity has become the common hallmark of software as well as hardware designs. It cannot be mastered by the common practices of testing and simulation. Hardware design may profit from developments in programming methodology by adopting proof techniques similar to those used in programming.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {2--8},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/36204.36178},
 doi = {http://doi.acm.org/10.1145/36204.36178},
 acmid = {36178},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wirth:1987:HAP:36177.36178,
 author = {Wirth, Nicklaus},
 title = {Hardware architectures for programming languages and programming languages for hardware architectures},
 abstract = {Programming Languages and Operating Systems introduce abstractions which allow the programmer to ignore details of an implementation. Support of an abstraction must not only concentrate on promoting the efficiency of an implementation, but also on providing the necessary guards against violations of the abstractions. In the frantic drive for efficiency the second goal has been neglected. There are indications that recent designs which are claimed to be both simple and powerful, achieve efficiency by shifting the complex issues of code generation and of appropriate guards onto compilers.Complexity has become the common hallmark of software as well as hardware designs. It cannot be mastered by the common practices of testing and simulation. Hardware design may profit from developments in programming methodology by adopting proof techniques similar to those used in programming.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {2--8},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/36177.36178},
 doi = {http://doi.acm.org/10.1145/36177.36178},
 acmid = {36178},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wirth:1987:HAP:36206.36178,
 author = {Wirth, Nicklaus},
 title = {Hardware architectures for programming languages and programming languages for hardware architectures},
 abstract = {Programming Languages and Operating Systems introduce abstractions which allow the programmer to ignore details of an implementation. Support of an abstraction must not only concentrate on promoting the efficiency of an implementation, but also on providing the necessary guards against violations of the abstractions. In the frantic drive for efficiency the second goal has been neglected. There are indications that recent designs which are claimed to be both simple and powerful, achieve efficiency by shifting the complex issues of code generation and of appropriate guards onto compilers.Complexity has become the common hallmark of software as well as hardware designs. It cannot be mastered by the common practices of testing and simulation. Hardware design may profit from developments in programming methodology by adopting proof techniques similar to those used in programming.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {2--8},
 numpages = {7},
 url = {http://dx.doi.org/10.1145/36206.36178},
 doi = {http://dx.doi.org/10.1145/36206.36178},
 acmid = {36178},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Beck:1987:VAM:36204.36179,
 author = {Beck, Bob and Kasten, Bob and Thakkar, Shreekant},
 title = {VLSI assist for a multiprocessor},
 abstract = {Multiprocessors have long been of interest to computer community. They provide the potential for accelerating applications through parallelism and increased throughput for large multi-user system. Three factors have limited the commercial success of multiprocessor systems; entry cost, range of performance, and ease of application. Advances in very large scale integration (VLSI) and in computer aided design (CAD) have removed these limitations, making possible a new class of multiprocessor systems based on VLSI components. A set of requirements for building an efficient shared multiprocessor system are discussed, including: low-level mutual exclusion, interrupt distribution, inter-processor signaling, process dispatching, caching, and system configuration. A system that meets these requirements is described and evaluated.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {10--20},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/36204.36179},
 doi = {http://doi.acm.org/10.1145/36204.36179},
 acmid = {36179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Beck:1987:VAM:36205.36179,
 author = {Beck, Bob and Kasten, Bob and Thakkar, Shreekant},
 title = {VLSI assist for a multiprocessor},
 abstract = {Multiprocessors have long been of interest to computer community. They provide the potential for accelerating applications through parallelism and increased throughput for large multi-user system. Three factors have limited the commercial success of multiprocessor systems; entry cost, range of performance, and ease of application. Advances in very large scale integration (VLSI) and in computer aided design (CAD) have removed these limitations, making possible a new class of multiprocessor systems based on VLSI components. A set of requirements for building an efficient shared multiprocessor system are discussed, including: low-level mutual exclusion, interrupt distribution, inter-processor signaling, process dispatching, caching, and system configuration. A system that meets these requirements is described and evaluated.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {10--20},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/36205.36179},
 doi = {http://doi.acm.org/10.1145/36205.36179},
 acmid = {36179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Beck:1987:VAM:36177.36179,
 author = {Beck, Bob and Kasten, Bob and Thakkar, Shreekant},
 title = {VLSI assist for a multiprocessor},
 abstract = {Multiprocessors have long been of interest to computer community. They provide the potential for accelerating applications through parallelism and increased throughput for large multi-user system. Three factors have limited the commercial success of multiprocessor systems; entry cost, range of performance, and ease of application. Advances in very large scale integration (VLSI) and in computer aided design (CAD) have removed these limitations, making possible a new class of multiprocessor systems based on VLSI components. A set of requirements for building an efficient shared multiprocessor system are discussed, including: low-level mutual exclusion, interrupt distribution, inter-processor signaling, process dispatching, caching, and system configuration. A system that meets these requirements is described and evaluated.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {10--20},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/36177.36179},
 doi = {http://doi.acm.org/10.1145/36177.36179},
 acmid = {36179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Beck:1987:VAM:36206.36179,
 author = {Beck, Bob and Kasten, Bob and Thakkar, Shreekant},
 title = {VLSI assist for a multiprocessor},
 abstract = {Multiprocessors have long been of interest to computer community. They provide the potential for accelerating applications through parallelism and increased throughput for large multi-user system. Three factors have limited the commercial success of multiprocessor systems; entry cost, range of performance, and ease of application. Advances in very large scale integration (VLSI) and in computer aided design (CAD) have removed these limitations, making possible a new class of multiprocessor systems based on VLSI components. A set of requirements for building an efficient shared multiprocessor system are discussed, including: low-level mutual exclusion, interrupt distribution, inter-processor signaling, process dispatching, caching, and system configuration. A system that meets these requirements is described and evaluated.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {10--20},
 numpages = {11},
 url = {http://dx.doi.org/10.1145/36206.36179},
 doi = {http://dx.doi.org/10.1145/36206.36179},
 acmid = {36179},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Bisiani:1987:ASM:36205.36180,
 author = {Bisiani, Roberto and Forin, Alessandro},
 title = {Architectural support for multilanguage parallel programming on heterogeneous systems},
 abstract = {We have designed and implemented a software facility, called Agora, that supports the development of parallel applications written in multiple languages. At the core of Agora there is a mechanism that allows concurrent computations to share data structures independently of the computer architecture they are executed on. Concurrent computations exchange control information by using a pattern-directed technique. This paper describes the Agora shared memory and its software implementation on both tightly and loosely-coupled architectures.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36205.36180},
 doi = {http://doi.acm.org/10.1145/36205.36180},
 acmid = {36180},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bisiani:1987:ASM:36206.36180,
 author = {Bisiani, Roberto and Forin, Alessandro},
 title = {Architectural support for multilanguage parallel programming on heterogeneous systems},
 abstract = {We have designed and implemented a software facility, called Agora, that supports the development of parallel applications written in multiple languages. At the core of Agora there is a mechanism that allows concurrent computations to share data structures independently of the computer architecture they are executed on. Concurrent computations exchange control information by using a pattern-directed technique. This paper describes the Agora shared memory and its software implementation on both tightly and loosely-coupled architectures.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {21--30},
 numpages = {10},
 url = {http://dx.doi.org/10.1145/36206.36180},
 doi = {http://dx.doi.org/10.1145/36206.36180},
 acmid = {36180},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Bisiani:1987:ASM:36177.36180,
 author = {Bisiani, Roberto and Forin, Alessandro},
 title = {Architectural support for multilanguage parallel programming on heterogeneous systems},
 abstract = {We have designed and implemented a software facility, called Agora, that supports the development of parallel applications written in multiple languages. At the core of Agora there is a mechanism that allows concurrent computations to share data structures independently of the computer architecture they are executed on. Concurrent computations exchange control information by using a pattern-directed technique. This paper describes the Agora shared memory and its software implementation on both tightly and loosely-coupled architectures.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36177.36180},
 doi = {http://doi.acm.org/10.1145/36177.36180},
 acmid = {36180},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bisiani:1987:ASM:36204.36180,
 author = {Bisiani, Roberto and Forin, Alessandro},
 title = {Architectural support for multilanguage parallel programming on heterogeneous systems},
 abstract = {We have designed and implemented a software facility, called Agora, that supports the development of parallel applications written in multiple languages. At the core of Agora there is a mechanism that allows concurrent computations to share data structures independently of the computer architecture they are executed on. Concurrent computations exchange control information by using a pattern-directed technique. This paper describes the Agora shared memory and its software implementation on both tightly and loosely-coupled architectures.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36204.36180},
 doi = {http://doi.acm.org/10.1145/36204.36180},
 acmid = {36180},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rashid:1987:MVM:36204.36181,
 author = {Rashid, Richard and Tevanian, Avadis and Young, Michael and Golub, David and Baron, Robert and Black, David and Bolosky, William and Chew, Jonathan},
 title = {Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures},
 abstract = {This paper describes the design and implementation of virtual memory management within the CMU Mach Operating System and the experiences gained by the Mach kernel group in porting that system to a variety of architectures. As of this writing, Mach runs on more than half a dozen uniprocessors and multiprocessors including the VAX family of uniprocessors and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the Sequent Balance 21000 and several experimental computers. Although these systems vary considerably in the kind of hardware support for memory management they provide, the machine-dependent portion of Mach virtual memory consists of a single code module and its related header file. This separation of software memory management from hardware support has been accomplished without sacrificing system performance. In addition to improving portability, it makes possible a relatively unbiased examination of the pros and cons of various hardware memory management schemes, especially as they apply to the support of multiprocessors.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {31--39},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/36204.36181},
 doi = {http://doi.acm.org/10.1145/36204.36181},
 acmid = {36181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rashid:1987:MVM:36206.36181,
 author = {Rashid, Richard and Tevanian, Avadis and Young, Michael and Golub, David and Baron, Robert and Black, David and Bolosky, William and Chew, Jonathan},
 title = {Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures},
 abstract = {This paper describes the design and implementation of virtual memory management within the CMU Mach Operating System and the experiences gained by the Mach kernel group in porting that system to a variety of architectures. As of this writing, Mach runs on more than half a dozen uniprocessors and multiprocessors including the VAX family of uniprocessors and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the Sequent Balance 21000 and several experimental computers. Although these systems vary considerably in the kind of hardware support for memory management they provide, the machine-dependent portion of Mach virtual memory consists of a single code module and its related header file. This separation of software memory management from hardware support has been accomplished without sacrificing system performance. In addition to improving portability, it makes possible a relatively unbiased examination of the pros and cons of various hardware memory management schemes, especially as they apply to the support of multiprocessors.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {31--39},
 numpages = {9},
 url = {http://dx.doi.org/10.1145/36206.36181},
 doi = {http://dx.doi.org/10.1145/36206.36181},
 acmid = {36181},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Rashid:1987:MVM:36177.36181,
 author = {Rashid, Richard and Tevanian, Avadis and Young, Michael and Golub, David and Baron, Robert and Black, David and Bolosky, William and Chew, Jonathan},
 title = {Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures},
 abstract = {This paper describes the design and implementation of virtual memory management within the CMU Mach Operating System and the experiences gained by the Mach kernel group in porting that system to a variety of architectures. As of this writing, Mach runs on more than half a dozen uniprocessors and multiprocessors including the VAX family of uniprocessors and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the Sequent Balance 21000 and several experimental computers. Although these systems vary considerably in the kind of hardware support for memory management they provide, the machine-dependent portion of Mach virtual memory consists of a single code module and its related header file. This separation of software memory management from hardware support has been accomplished without sacrificing system performance. In addition to improving portability, it makes possible a relatively unbiased examination of the pros and cons of various hardware memory management schemes, especially as they apply to the support of multiprocessors.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {31--39},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/36177.36181},
 doi = {http://doi.acm.org/10.1145/36177.36181},
 acmid = {36181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rashid:1987:MVM:36205.36181,
 author = {Rashid, Richard and Tevanian, Avadis and Young, Michael and Golub, David and Baron, Robert and Black, David and Bolosky, William and Chew, Jonathan},
 title = {Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures},
 abstract = {This paper describes the design and implementation of virtual memory management within the CMU Mach Operating System and the experiences gained by the Mach kernel group in porting that system to a variety of architectures. As of this writing, Mach runs on more than half a dozen uniprocessors and multiprocessors including the VAX family of uniprocessors and multiprocessors, the IBM RT PC, the SUN 3, the Encore MultiMax, the Sequent Balance 21000 and several experimental computers. Although these systems vary considerably in the kind of hardware support for memory management they provide, the machine-dependent portion of Mach virtual memory consists of a single code module and its related header file. This separation of software memory management from hardware support has been accomplished without sacrificing system performance. In addition to improving portability, it makes possible a relatively unbiased examination of the pros and cons of various hardware memory management schemes, especially as they apply to the support of multiprocessors.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {31--39},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/36205.36181},
 doi = {http://doi.acm.org/10.1145/36205.36181},
 acmid = {36181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hayes:1987:ADE:36205.36182,
 author = {Hayes, John R. and Fraeman, Martin E. and Williams, Robert L. and Zaremba, Thomas},
 title = {An architecture for the direct execution of the Forth programming language},
 abstract = {We have developed a simple direct execution architecture for a 32 bit Forth microprocessor. The processor can directly access a linear address space of over 4 gigawords. Two instruction types are defined; a subroutine call, and a user defined microcode instruction. On-chip stack caches allow most Forth primitives to execute in a single cycle.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {42--49},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/36205.36182},
 doi = {http://doi.acm.org/10.1145/36205.36182},
 acmid = {36182},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hayes:1987:ADE:36177.36182,
 author = {Hayes, John R. and Fraeman, Martin E. and Williams, Robert L. and Zaremba, Thomas},
 title = {An architecture for the direct execution of the Forth programming language},
 abstract = {We have developed a simple direct execution architecture for a 32 bit Forth microprocessor. The processor can directly access a linear address space of over 4 gigawords. Two instruction types are defined; a subroutine call, and a user defined microcode instruction. On-chip stack caches allow most Forth primitives to execute in a single cycle.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {42--49},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/36177.36182},
 doi = {http://doi.acm.org/10.1145/36177.36182},
 acmid = {36182},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hayes:1987:ADE:36204.36182,
 author = {Hayes, John R. and Fraeman, Martin E. and Williams, Robert L. and Zaremba, Thomas},
 title = {An architecture for the direct execution of the Forth programming language},
 abstract = {We have developed a simple direct execution architecture for a 32 bit Forth microprocessor. The processor can directly access a linear address space of over 4 gigawords. Two instruction types are defined; a subroutine call, and a user defined microcode instruction. On-chip stack caches allow most Forth primitives to execute in a single cycle.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {42--49},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/36204.36182},
 doi = {http://doi.acm.org/10.1145/36204.36182},
 acmid = {36182},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hayes:1987:ADE:36206.36182,
 author = {Hayes, John R. and Fraeman, Martin E. and Williams, Robert L. and Zaremba, Thomas},
 title = {An architecture for the direct execution of the Forth programming language},
 abstract = {We have developed a simple direct execution architecture for a 32 bit Forth microprocessor. The processor can directly access a linear address space of over 4 gigawords. Two instruction types are defined; a subroutine call, and a user defined microcode instruction. On-chip stack caches allow most Forth primitives to execute in a single cycle.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {42--49},
 numpages = {8},
 url = {http://dx.doi.org/10.1145/36206.36182},
 doi = {http://dx.doi.org/10.1145/36206.36182},
 acmid = {36182},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Steenkiste:1987:TTC:36204.36183,
 author = {Steenkiste, Peter and Hennessy, John},
 title = {Tags and type checking in LISP: hardware and software approaches},
 abstract = {One of the major factors that distinguishes LISP from many other languages (Pascal, C, Fortran, etc.) is the need for run-time type checking. Run-time type checking is implemented by adding to each data object a tag that encodes type information. Tags must be compared for type compatibility, removed when using the data, and inserted when new data items are created. This tag manipulation, together with other work related to dynamic type checking and generic operations, constitutes a significant component of the execution time of LISP programs. This has led both to the development of LISP machines that support tag checking in hardware and to the avoidance of type checking by users running on stock hardware. To understand the role and necessity of special-purpose hardware for tag handling, we first measure the cost of type checking operations for a group of LISP programs. We then examine hardware and software implementations of tag operations and estimate the cost of tag handling with the different tag implementation schemes. The data shows that minimal levels of support provide most of the benefits, and that tag operations can be relatively inexpensive, even when no special hardware support is present.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36204.36183},
 doi = {http://doi.acm.org/10.1145/36204.36183},
 acmid = {36183},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Steenkiste:1987:TTC:36177.36183,
 author = {Steenkiste, Peter and Hennessy, John},
 title = {Tags and type checking in LISP: hardware and software approaches},
 abstract = {One of the major factors that distinguishes LISP from many other languages (Pascal, C, Fortran, etc.) is the need for run-time type checking. Run-time type checking is implemented by adding to each data object a tag that encodes type information. Tags must be compared for type compatibility, removed when using the data, and inserted when new data items are created. This tag manipulation, together with other work related to dynamic type checking and generic operations, constitutes a significant component of the execution time of LISP programs. This has led both to the development of LISP machines that support tag checking in hardware and to the avoidance of type checking by users running on stock hardware. To understand the role and necessity of special-purpose hardware for tag handling, we first measure the cost of type checking operations for a group of LISP programs. We then examine hardware and software implementations of tag operations and estimate the cost of tag handling with the different tag implementation schemes. The data shows that minimal levels of support provide most of the benefits, and that tag operations can be relatively inexpensive, even when no special hardware support is present.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36177.36183},
 doi = {http://doi.acm.org/10.1145/36177.36183},
 acmid = {36183},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Steenkiste:1987:TTC:36205.36183,
 author = {Steenkiste, Peter and Hennessy, John},
 title = {Tags and type checking in LISP: hardware and software approaches},
 abstract = {One of the major factors that distinguishes LISP from many other languages (Pascal, C, Fortran, etc.) is the need for run-time type checking. Run-time type checking is implemented by adding to each data object a tag that encodes type information. Tags must be compared for type compatibility, removed when using the data, and inserted when new data items are created. This tag manipulation, together with other work related to dynamic type checking and generic operations, constitutes a significant component of the execution time of LISP programs. This has led both to the development of LISP machines that support tag checking in hardware and to the avoidance of type checking by users running on stock hardware. To understand the role and necessity of special-purpose hardware for tag handling, we first measure the cost of type checking operations for a group of LISP programs. We then examine hardware and software implementations of tag operations and estimate the cost of tag handling with the different tag implementation schemes. The data shows that minimal levels of support provide most of the benefits, and that tag operations can be relatively inexpensive, even when no special hardware support is present.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36205.36183},
 doi = {http://doi.acm.org/10.1145/36205.36183},
 acmid = {36183},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Steenkiste:1987:TTC:36206.36183,
 author = {Steenkiste, Peter and Hennessy, John},
 title = {Tags and type checking in LISP: hardware and software approaches},
 abstract = {One of the major factors that distinguishes LISP from many other languages (Pascal, C, Fortran, etc.) is the need for run-time type checking. Run-time type checking is implemented by adding to each data object a tag that encodes type information. Tags must be compared for type compatibility, removed when using the data, and inserted when new data items are created. This tag manipulation, together with other work related to dynamic type checking and generic operations, constitutes a significant component of the execution time of LISP programs. This has led both to the development of LISP machines that support tag checking in hardware and to the avoidance of type checking by users running on stock hardware. To understand the role and necessity of special-purpose hardware for tag handling, we first measure the cost of type checking operations for a group of LISP programs. We then examine hardware and software implementations of tag operations and estimate the cost of tag handling with the different tag implementation schemes. The data shows that minimal levels of support provide most of the benefits, and that tag operations can be relatively inexpensive, even when no special hardware support is present.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {50--59},
 numpages = {10},
 url = {http://dx.doi.org/10.1145/36206.36183},
 doi = {http://dx.doi.org/10.1145/36206.36183},
 acmid = {36183},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Davidson:1987:EIS:36205.36184,
 author = {Davidson, Jack W. and Vaughan, Richard A.},
 title = {The effect of instruction set complexity on program size and memory performance},
 abstract = {One potential disadvantage of a machine with a reduced instruction set is that object programs may be substantially larger than those for a machine with a richer, more complex instruction set. The main reason is that a small instruction set will require more instructions to implement the same function. In addition, the tendency of RISC machines to use fixed length instructions with a few instruction formats also increases object program size. It has been conjectured that the resulting larger programs could adversely affect memory performance and bus traffic. In this paper we report the results of a set of experiments to isolate and determine the effect of instruction set complexity on cache memory performance and bus traffic. Three high-level language compilers were constructed for machines with instruction sets of varying degrees of complexity. Using a set of benchmark programs, we evaluated the effect of instruction set complexity had on program size. Five of the programs were used to perform a set of trace-driven simulations to study each machine's cache and bus performance. While we found that the miss ratio is affected by object program size, it appears that this can be corrected by simplying increasing the size of the cache. Our measurements of bus traffic, however, show that even with large caches, machines with simple instruction sets can expect substantially more main memory reads than machines with dense object programs.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {60--64},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36184},
 doi = {http://doi.acm.org/10.1145/36205.36184},
 acmid = {36184},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Davidson:1987:EIS:36206.36184,
 author = {Davidson, Jack W. and Vaughan, Richard A.},
 title = {The effect of instruction set complexity on program size and memory performance},
 abstract = {One potential disadvantage of a machine with a reduced instruction set is that object programs may be substantially larger than those for a machine with a richer, more complex instruction set. The main reason is that a small instruction set will require more instructions to implement the same function. In addition, the tendency of RISC machines to use fixed length instructions with a few instruction formats also increases object program size. It has been conjectured that the resulting larger programs could adversely affect memory performance and bus traffic. In this paper we report the results of a set of experiments to isolate and determine the effect of instruction set complexity on cache memory performance and bus traffic. Three high-level language compilers were constructed for machines with instruction sets of varying degrees of complexity. Using a set of benchmark programs, we evaluated the effect of instruction set complexity had on program size. Five of the programs were used to perform a set of trace-driven simulations to study each machine's cache and bus performance. While we found that the miss ratio is affected by object program size, it appears that this can be corrected by simplying increasing the size of the cache. Our measurements of bus traffic, however, show that even with large caches, machines with simple instruction sets can expect substantially more main memory reads than machines with dense object programs.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {60--64},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36184},
 doi = {http://dx.doi.org/10.1145/36206.36184},
 acmid = {36184},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Davidson:1987:EIS:36204.36184,
 author = {Davidson, Jack W. and Vaughan, Richard A.},
 title = {The effect of instruction set complexity on program size and memory performance},
 abstract = {One potential disadvantage of a machine with a reduced instruction set is that object programs may be substantially larger than those for a machine with a richer, more complex instruction set. The main reason is that a small instruction set will require more instructions to implement the same function. In addition, the tendency of RISC machines to use fixed length instructions with a few instruction formats also increases object program size. It has been conjectured that the resulting larger programs could adversely affect memory performance and bus traffic. In this paper we report the results of a set of experiments to isolate and determine the effect of instruction set complexity on cache memory performance and bus traffic. Three high-level language compilers were constructed for machines with instruction sets of varying degrees of complexity. Using a set of benchmark programs, we evaluated the effect of instruction set complexity had on program size. Five of the programs were used to perform a set of trace-driven simulations to study each machine's cache and bus performance. While we found that the miss ratio is affected by object program size, it appears that this can be corrected by simplying increasing the size of the cache. Our measurements of bus traffic, however, show that even with large caches, machines with simple instruction sets can expect substantially more main memory reads than machines with dense object programs.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {60--64},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36184},
 doi = {http://doi.acm.org/10.1145/36204.36184},
 acmid = {36184},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Davidson:1987:EIS:36177.36184,
 author = {Davidson, Jack W. and Vaughan, Richard A.},
 title = {The effect of instruction set complexity on program size and memory performance},
 abstract = {One potential disadvantage of a machine with a reduced instruction set is that object programs may be substantially larger than those for a machine with a richer, more complex instruction set. The main reason is that a small instruction set will require more instructions to implement the same function. In addition, the tendency of RISC machines to use fixed length instructions with a few instruction formats also increases object program size. It has been conjectured that the resulting larger programs could adversely affect memory performance and bus traffic. In this paper we report the results of a set of experiments to isolate and determine the effect of instruction set complexity on cache memory performance and bus traffic. Three high-level language compilers were constructed for machines with instruction sets of varying degrees of complexity. Using a set of benchmark programs, we evaluated the effect of instruction set complexity had on program size. Five of the programs were used to perform a set of trace-driven simulations to study each machine's cache and bus performance. While we found that the miss ratio is affected by object program size, it appears that this can be corrected by simplying increasing the size of the cache. Our measurements of bus traffic, however, show that even with large caches, machines with simple instruction sets can expect substantially more main memory reads than machines with dense object programs.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {60--64},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36184},
 doi = {http://doi.acm.org/10.1145/36177.36184},
 acmid = {36184},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Atkinson:1987:DP:36177.36185,
 author = {Atkinson, Russell R. and McCreight, Edward M.},
 title = {The dragon processor},
 abstract = {The Xerox PARC Dragon is a VLSI research computer that uses several techniques to achieve dense code and fast procedure calls in a system that can support multiple processors on a central high bandwidth memory bus.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {65--69},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36185},
 doi = {http://doi.acm.org/10.1145/36177.36185},
 acmid = {36185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Atkinson:1987:DP:36204.36185,
 author = {Atkinson, Russell R. and McCreight, Edward M.},
 title = {The dragon processor},
 abstract = {The Xerox PARC Dragon is a VLSI research computer that uses several techniques to achieve dense code and fast procedure calls in a system that can support multiple processors on a central high bandwidth memory bus.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {65--69},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36185},
 doi = {http://doi.acm.org/10.1145/36204.36185},
 acmid = {36185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Atkinson:1987:DP:36206.36185,
 author = {Atkinson, Russell R. and McCreight, Edward M.},
 title = {The dragon processor},
 abstract = {The Xerox PARC Dragon is a VLSI research computer that uses several techniques to achieve dense code and fast procedure calls in a system that can support multiple processors on a central high bandwidth memory bus.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {65--69},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36185},
 doi = {http://dx.doi.org/10.1145/36206.36185},
 acmid = {36185},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Atkinson:1987:DP:36205.36185,
 author = {Atkinson, Russell R. and McCreight, Edward M.},
 title = {The dragon processor},
 abstract = {The Xerox PARC Dragon is a VLSI research computer that uses several techniques to achieve dense code and fast procedure calls in a system that can support multiple processors on a central high bandwidth memory bus.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {65--69},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36185},
 doi = {http://doi.acm.org/10.1145/36205.36185},
 acmid = {36185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goodman:1987:CMV:36177.36186,
 author = {Goodman, James R.},
 title = {Coherency for multiprocessor virtual address caches},
 abstract = {A multiprocessor cache memory system is described that supplies data to the processor based on virtual addresses, but maintains consistency in the main memory, both across caches and across virtual address spaces. Pages in the same or different address spaces may be mapped to share a single physical page. The same hardware is used for maintaining consistency both among caches and among virtual addresses. Three different notions of a cache "block" are defined: (1) the unit for transferring data to/from main storage, (2) the unit over which tag information is maintained, and (3) the unit over which consistency is maintained. The relation among these block sizes is explored, and it is shown that they can be optimized independently. It is shown that the use of large address blocks results in low overhead for the virtual address cache.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {72--81},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36177.36186},
 doi = {http://doi.acm.org/10.1145/36177.36186},
 acmid = {36186},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goodman:1987:CMV:36204.36186,
 author = {Goodman, James R.},
 title = {Coherency for multiprocessor virtual address caches},
 abstract = {A multiprocessor cache memory system is described that supplies data to the processor based on virtual addresses, but maintains consistency in the main memory, both across caches and across virtual address spaces. Pages in the same or different address spaces may be mapped to share a single physical page. The same hardware is used for maintaining consistency both among caches and among virtual addresses. Three different notions of a cache "block" are defined: (1) the unit for transferring data to/from main storage, (2) the unit over which tag information is maintained, and (3) the unit over which consistency is maintained. The relation among these block sizes is explored, and it is shown that they can be optimized independently. It is shown that the use of large address blocks results in low overhead for the virtual address cache.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {72--81},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36204.36186},
 doi = {http://doi.acm.org/10.1145/36204.36186},
 acmid = {36186},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goodman:1987:CMV:36206.36186,
 author = {Goodman, James R.},
 title = {Coherency for multiprocessor virtual address caches},
 abstract = {A multiprocessor cache memory system is described that supplies data to the processor based on virtual addresses, but maintains consistency in the main memory, both across caches and across virtual address spaces. Pages in the same or different address spaces may be mapped to share a single physical page. The same hardware is used for maintaining consistency both among caches and among virtual addresses. Three different notions of a cache "block" are defined: (1) the unit for transferring data to/from main storage, (2) the unit over which tag information is maintained, and (3) the unit over which consistency is maintained. The relation among these block sizes is explored, and it is shown that they can be optimized independently. It is shown that the use of large address blocks results in low overhead for the virtual address cache.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {72--81},
 numpages = {10},
 url = {http://dx.doi.org/10.1145/36206.36186},
 doi = {http://dx.doi.org/10.1145/36206.36186},
 acmid = {36186},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Goodman:1987:CMV:36205.36186,
 author = {Goodman, James R.},
 title = {Coherency for multiprocessor virtual address caches},
 abstract = {A multiprocessor cache memory system is described that supplies data to the processor based on virtual addresses, but maintains consistency in the main memory, both across caches and across virtual address spaces. Pages in the same or different address spaces may be mapped to share a single physical page. The same hardware is used for maintaining consistency both among caches and among virtual addresses. Three different notions of a cache "block" are defined: (1) the unit for transferring data to/from main storage, (2) the unit over which tag information is maintained, and (3) the unit over which consistency is maintained. The relation among these block sizes is explored, and it is shown that they can be optimized independently. It is shown that the use of large address blocks results in low overhead for the virtual address cache.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {72--81},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36205.36186},
 doi = {http://doi.acm.org/10.1145/36205.36186},
 acmid = {36186},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cargill:1987:CHS:36206.36187,
 author = {Cargill, T. A. and Locanthi, B. N.},
 title = {Cheap hardware support for software debugging and profiling},
 abstract = {We wish to determine the effectiveness of some simple hardware for debugging and profiling compiled programs on a conventional processor. The hardware cost is small -- a counter decremented on each instruction that raises an exception when its value becomes zero. With the counter a debugger can provide data watchpoints and reverse execution: a profiler can measure the total instruction cost of a code segment and sample the program counter accurately. Such a counter has been included on a single-board MC68020 workstation, for which system software is currently being written. We will report our progress at the symposium.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {82--83},
 numpages = {2},
 url = {http://dx.doi.org/10.1145/36206.36187},
 doi = {http://dx.doi.org/10.1145/36206.36187},
 acmid = {36187},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Cargill:1987:CHS:36177.36187,
 author = {Cargill, T. A. and Locanthi, B. N.},
 title = {Cheap hardware support for software debugging and profiling},
 abstract = {We wish to determine the effectiveness of some simple hardware for debugging and profiling compiled programs on a conventional processor. The hardware cost is small -- a counter decremented on each instruction that raises an exception when its value becomes zero. With the counter a debugger can provide data watchpoints and reverse execution: a profiler can measure the total instruction cost of a code segment and sample the program counter accurately. Such a counter has been included on a single-board MC68020 workstation, for which system software is currently being written. We will report our progress at the symposium.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {82--83},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/36177.36187},
 doi = {http://doi.acm.org/10.1145/36177.36187},
 acmid = {36187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cargill:1987:CHS:36205.36187,
 author = {Cargill, T. A. and Locanthi, B. N.},
 title = {Cheap hardware support for software debugging and profiling},
 abstract = {We wish to determine the effectiveness of some simple hardware for debugging and profiling compiled programs on a conventional processor. The hardware cost is small -- a counter decremented on each instruction that raises an exception when its value becomes zero. With the counter a debugger can provide data watchpoints and reverse execution: a profiler can measure the total instruction cost of a code segment and sample the program counter accurately. Such a counter has been included on a single-board MC68020 workstation, for which system software is currently being written. We will report our progress at the symposium.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {82--83},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/36205.36187},
 doi = {http://doi.acm.org/10.1145/36205.36187},
 acmid = {36187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cargill:1987:CHS:36204.36187,
 author = {Cargill, T. A. and Locanthi, B. N.},
 title = {Cheap hardware support for software debugging and profiling},
 abstract = {We wish to determine the effectiveness of some simple hardware for debugging and profiling compiled programs on a conventional processor. The hardware cost is small -- a counter decremented on each instruction that raises an exception when its value becomes zero. With the counter a debugger can provide data watchpoints and reverse execution: a profiler can measure the total instruction cost of a code segment and sample the program counter accurately. Such a counter has been included on a single-board MC68020 workstation, for which system software is currently being written. We will report our progress at the symposium.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {82--83},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/36204.36187},
 doi = {http://doi.acm.org/10.1145/36204.36187},
 acmid = {36187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Georgiou:1987:ECI:36177.36188,
 author = {Georgiou, C. J. and Palmer, S. L. and Rosenfeld, P. L.},
 title = {An experimental coprocessor for implementing persistent objects on an IBM 4381},
 abstract = {In this paper we describe an experimental coprocessor for an IBM 4381 that is designed to facilitate the exploration of persistent objects.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {84--87},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/36177.36188},
 doi = {http://doi.acm.org/10.1145/36177.36188},
 acmid = {36188},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Georgiou:1987:ECI:36204.36188,
 author = {Georgiou, C. J. and Palmer, S. L. and Rosenfeld, P. L.},
 title = {An experimental coprocessor for implementing persistent objects on an IBM 4381},
 abstract = {In this paper we describe an experimental coprocessor for an IBM 4381 that is designed to facilitate the exploration of persistent objects.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {84--87},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/36204.36188},
 doi = {http://doi.acm.org/10.1145/36204.36188},
 acmid = {36188},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Georgiou:1987:ECI:36206.36188,
 author = {Georgiou, C. J. and Palmer, S. L. and Rosenfeld, P. L.},
 title = {An experimental coprocessor for implementing persistent objects on an IBM 4381},
 abstract = {In this paper we describe an experimental coprocessor for an IBM 4381 that is designed to facilitate the exploration of persistent objects.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {84--87},
 numpages = {4},
 url = {http://dx.doi.org/10.1145/36206.36188},
 doi = {http://dx.doi.org/10.1145/36206.36188},
 acmid = {36188},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Georgiou:1987:ECI:36205.36188,
 author = {Georgiou, C. J. and Palmer, S. L. and Rosenfeld, P. L.},
 title = {An experimental coprocessor for implementing persistent objects on an IBM 4381},
 abstract = {In this paper we describe an experimental coprocessor for an IBM 4381 that is designed to facilitate the exploration of persistent objects.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {84--87},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/36205.36188},
 doi = {http://doi.acm.org/10.1145/36205.36188},
 acmid = {36188},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Magenheimer:1987:IMD:36177.36189,
 author = {Magenheimer, Daniel J. and Peters, Liz and Pettis, Karl and Zuras, Dan},
 title = {Integer multiplication and division on the HP precision architecture},
 abstract = {In recent years, many architectural design efforts have focused on maximizing performance for frequently executed, simple instructions. Although these efforts have resulted in machines with better average price/performance ratios, certain complex instructions and, thus, certain classes of programs which heavily depend on these instructions may suffer by comparison. Integer multiplication and division are one such set of complex instructions. This paper describes how a small set of primitive instructions combined with careful frequency analysis and clever programming allows the Hewlett-Packard Precision Architecture integer multiplication and division implementation to provide adequate performance at little or no hardware cost.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36177.36189},
 doi = {http://doi.acm.org/10.1145/36177.36189},
 acmid = {36189},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Magenheimer:1987:IMD:36206.36189,
 author = {Magenheimer, Daniel J. and Peters, Liz and Pettis, Karl and Zuras, Dan},
 title = {Integer multiplication and division on the HP precision architecture},
 abstract = {In recent years, many architectural design efforts have focused on maximizing performance for frequently executed, simple instructions. Although these efforts have resulted in machines with better average price/performance ratios, certain complex instructions and, thus, certain classes of programs which heavily depend on these instructions may suffer by comparison. Integer multiplication and division are one such set of complex instructions. This paper describes how a small set of primitive instructions combined with careful frequency analysis and clever programming allows the Hewlett-Packard Precision Architecture integer multiplication and division implementation to provide adequate performance at little or no hardware cost.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {90--99},
 numpages = {10},
 url = {http://dx.doi.org/10.1145/36206.36189},
 doi = {http://dx.doi.org/10.1145/36206.36189},
 acmid = {36189},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Magenheimer:1987:IMD:36205.36189,
 author = {Magenheimer, Daniel J. and Peters, Liz and Pettis, Karl and Zuras, Dan},
 title = {Integer multiplication and division on the HP precision architecture},
 abstract = {In recent years, many architectural design efforts have focused on maximizing performance for frequently executed, simple instructions. Although these efforts have resulted in machines with better average price/performance ratios, certain complex instructions and, thus, certain classes of programs which heavily depend on these instructions may suffer by comparison. Integer multiplication and division are one such set of complex instructions. This paper describes how a small set of primitive instructions combined with careful frequency analysis and clever programming allows the Hewlett-Packard Precision Architecture integer multiplication and division implementation to provide adequate performance at little or no hardware cost.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36205.36189},
 doi = {http://doi.acm.org/10.1145/36205.36189},
 acmid = {36189},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Magenheimer:1987:IMD:36204.36189,
 author = {Magenheimer, Daniel J. and Peters, Liz and Pettis, Karl and Zuras, Dan},
 title = {Integer multiplication and division on the HP precision architecture},
 abstract = {In recent years, many architectural design efforts have focused on maximizing performance for frequently executed, simple instructions. Although these efforts have resulted in machines with better average price/performance ratios, certain complex instructions and, thus, certain classes of programs which heavily depend on these instructions may suffer by comparison. Integer multiplication and division are one such set of complex instructions. This paper describes how a small set of primitive instructions combined with careful frequency analysis and clever programming allows the Hewlett-Packard Precision Architecture integer multiplication and division implementation to provide adequate performance at little or no hardware cost.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36204.36189},
 doi = {http://doi.acm.org/10.1145/36204.36189},
 acmid = {36189},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wall:1987:MEU:36206.36190,
 author = {Wall, David W. and Powell, Michael L.},
 title = {The Mahler experience: using an intermediate language as the machine description},
 abstract = {Division of a compiler into a front end and a back end that communicate via an intermediate language is a well-known technique. We go farther and use the intermediate language as the official description of a family of machines with simple instruction sets and addressing capabilities, hiding some of the inconvenient details of the real machine from the users and the front end compilers.To do this credibly, we have had to hide not only the existence of the details but also the performance consequences of hiding them. The back end that compiles and links the intermediate language tries to produce code that does not suffer a performance penalty because of the details that were hidden from the front end compiler. To accomplish this, we have used a number of link-time optimizations, including instruction scheduling and interprocedural register allocation, to hide the existence of such idiosyncracies as delayed branches and non-infinite register sets. For the most part we have been sucessful.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {100--104},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36190},
 doi = {http://dx.doi.org/10.1145/36206.36190},
 acmid = {36190},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Wall:1987:MEU:36205.36190,
 author = {Wall, David W. and Powell, Michael L.},
 title = {The Mahler experience: using an intermediate language as the machine description},
 abstract = {Division of a compiler into a front end and a back end that communicate via an intermediate language is a well-known technique. We go farther and use the intermediate language as the official description of a family of machines with simple instruction sets and addressing capabilities, hiding some of the inconvenient details of the real machine from the users and the front end compilers.To do this credibly, we have had to hide not only the existence of the details but also the performance consequences of hiding them. The back end that compiles and links the intermediate language tries to produce code that does not suffer a performance penalty because of the details that were hidden from the front end compiler. To accomplish this, we have used a number of link-time optimizations, including instruction scheduling and interprocedural register allocation, to hide the existence of such idiosyncracies as delayed branches and non-infinite register sets. For the most part we have been sucessful.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {100--104},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36190},
 doi = {http://doi.acm.org/10.1145/36205.36190},
 acmid = {36190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1987:MEU:36177.36190,
 author = {Wall, David W. and Powell, Michael L.},
 title = {The Mahler experience: using an intermediate language as the machine description},
 abstract = {Division of a compiler into a front end and a back end that communicate via an intermediate language is a well-known technique. We go farther and use the intermediate language as the official description of a family of machines with simple instruction sets and addressing capabilities, hiding some of the inconvenient details of the real machine from the users and the front end compilers.To do this credibly, we have had to hide not only the existence of the details but also the performance consequences of hiding them. The back end that compiles and links the intermediate language tries to produce code that does not suffer a performance penalty because of the details that were hidden from the front end compiler. To accomplish this, we have used a number of link-time optimizations, including instruction scheduling and interprocedural register allocation, to hide the existence of such idiosyncracies as delayed branches and non-infinite register sets. For the most part we have been sucessful.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {100--104},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36190},
 doi = {http://doi.acm.org/10.1145/36177.36190},
 acmid = {36190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wall:1987:MEU:36204.36190,
 author = {Wall, David W. and Powell, Michael L.},
 title = {The Mahler experience: using an intermediate language as the machine description},
 abstract = {Division of a compiler into a front end and a back end that communicate via an intermediate language is a well-known technique. We go farther and use the intermediate language as the official description of a family of machines with simple instruction sets and addressing capabilities, hiding some of the inconvenient details of the real machine from the users and the front end compilers.To do this credibly, we have had to hide not only the existence of the details but also the performance consequences of hiding them. The back end that compiles and links the intermediate language tries to produce code that does not suffer a performance penalty because of the details that were hidden from the front end compiler. To accomplish this, we have used a number of link-time optimizations, including instruction scheduling and interprocedural register allocation, to hide the existence of such idiosyncracies as delayed branches and non-infinite register sets. For the most part we have been sucessful.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {100--104},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36190},
 doi = {http://doi.acm.org/10.1145/36204.36190},
 acmid = {36190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weiss:1987:SSC:36205.36191,
 author = {Weiss, Shlomo and Smith, James E.},
 title = {A study of scalar compilation techniques for pipelined supercomputers},
 abstract = {This paper studies two compilation techniques for enhancing scalar performance in high-speed scientific processors: software pipelining and loop unrolling. We study the impact of the architecture (size of the register file) and of the hardware (size of instruction buffer) on the efficiency of loop unrolling. We also develop a methodology for classifying software pipelining techniques. For loop unrolling, a straightforward scheduling algorithm is shown to produce near-optimal results when not inhibited by recurrences or memory hazards. Software pipelining requires less hardware but also achieves less speedup. Finally, we show that the performance produced with a modified CRAY-1S scalar architecture and a code scheduler utilizing loop unrolling is comparable to the performance achieved by the CRAY-1S with a vector unit and the CFT vectorizing compiler.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {105--109},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36191},
 doi = {http://doi.acm.org/10.1145/36205.36191},
 acmid = {36191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Weiss:1987:SSC:36177.36191,
 author = {Weiss, Shlomo and Smith, James E.},
 title = {A study of scalar compilation techniques for pipelined supercomputers},
 abstract = {This paper studies two compilation techniques for enhancing scalar performance in high-speed scientific processors: software pipelining and loop unrolling. We study the impact of the architecture (size of the register file) and of the hardware (size of instruction buffer) on the efficiency of loop unrolling. We also develop a methodology for classifying software pipelining techniques. For loop unrolling, a straightforward scheduling algorithm is shown to produce near-optimal results when not inhibited by recurrences or memory hazards. Software pipelining requires less hardware but also achieves less speedup. Finally, we show that the performance produced with a modified CRAY-1S scalar architecture and a code scheduler utilizing loop unrolling is comparable to the performance achieved by the CRAY-1S with a vector unit and the CFT vectorizing compiler.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {105--109},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36191},
 doi = {http://doi.acm.org/10.1145/36177.36191},
 acmid = {36191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Weiss:1987:SSC:36206.36191,
 author = {Weiss, Shlomo and Smith, James E.},
 title = {A study of scalar compilation techniques for pipelined supercomputers},
 abstract = {This paper studies two compilation techniques for enhancing scalar performance in high-speed scientific processors: software pipelining and loop unrolling. We study the impact of the architecture (size of the register file) and of the hardware (size of instruction buffer) on the efficiency of loop unrolling. We also develop a methodology for classifying software pipelining techniques. For loop unrolling, a straightforward scheduling algorithm is shown to produce near-optimal results when not inhibited by recurrences or memory hazards. Software pipelining requires less hardware but also achieves less speedup. Finally, we show that the performance produced with a modified CRAY-1S scalar architecture and a code scheduler utilizing loop unrolling is comparable to the performance achieved by the CRAY-1S with a vector unit and the CFT vectorizing compiler.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {105--109},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36191},
 doi = {http://dx.doi.org/10.1145/36206.36191},
 acmid = {36191},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Weiss:1987:SSC:36204.36191,
 author = {Weiss, Shlomo and Smith, James E.},
 title = {A study of scalar compilation techniques for pipelined supercomputers},
 abstract = {This paper studies two compilation techniques for enhancing scalar performance in high-speed scientific processors: software pipelining and loop unrolling. We study the impact of the architecture (size of the register file) and of the hardware (size of instruction buffer) on the efficiency of loop unrolling. We also develop a methodology for classifying software pipelining techniques. For loop unrolling, a straightforward scheduling algorithm is shown to produce near-optimal results when not inhibited by recurrences or memory hazards. Software pipelining requires less hardware but also achieves less speedup. Finally, we show that the performance produced with a modified CRAY-1S scalar architecture and a code scheduler utilizing loop unrolling is comparable to the performance achieved by the CRAY-1S with a vector unit and the CFT vectorizing compiler.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {105--109},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36191},
 doi = {http://doi.acm.org/10.1145/36204.36191},
 acmid = {36191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bush:1987:CSR:36205.36192,
 author = {Bush, William R. and Samples, A. Dain and Ungar, David and Hilfinger, Paul N.},
 title = {Compiling Smalltalk-80 to a RISC},
 abstract = {The Smalltalk On A RISC project at U. C. Berkeley proves that a high-level object-oriented language can attain high performance on a modified reduced instruction set architecture. The single most important optimization is the removal of a layer of interpretation, compiling the bytecoded virtual machine instructions into low-level, register-based, hardware instructions. This paper describes the compiler and how it was affected by SOAR architectural features. The compiler generates code of reasonable density and speed. Because of Smalltalk-80's semantics, relatively few optimizations are possible, but hardware and software mechanisms at runtime offset these limitations. Register allocation for an architecture with register windows comprises the major task of the compiler. Performance analysis suggests that SOAR is not simple enough; several hardware features could be efficiently replaced by instruction sequences constructed by the compiler.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {112--116},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36192},
 doi = {http://doi.acm.org/10.1145/36205.36192},
 acmid = {36192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bush:1987:CSR:36204.36192,
 author = {Bush, William R. and Samples, A. Dain and Ungar, David and Hilfinger, Paul N.},
 title = {Compiling Smalltalk-80 to a RISC},
 abstract = {The Smalltalk On A RISC project at U. C. Berkeley proves that a high-level object-oriented language can attain high performance on a modified reduced instruction set architecture. The single most important optimization is the removal of a layer of interpretation, compiling the bytecoded virtual machine instructions into low-level, register-based, hardware instructions. This paper describes the compiler and how it was affected by SOAR architectural features. The compiler generates code of reasonable density and speed. Because of Smalltalk-80's semantics, relatively few optimizations are possible, but hardware and software mechanisms at runtime offset these limitations. Register allocation for an architecture with register windows comprises the major task of the compiler. Performance analysis suggests that SOAR is not simple enough; several hardware features could be efficiently replaced by instruction sequences constructed by the compiler.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {112--116},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36192},
 doi = {http://doi.acm.org/10.1145/36204.36192},
 acmid = {36192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bush:1987:CSR:36206.36192,
 author = {Bush, William R. and Samples, A. Dain and Ungar, David and Hilfinger, Paul N.},
 title = {Compiling Smalltalk-80 to a RISC},
 abstract = {The Smalltalk On A RISC project at U. C. Berkeley proves that a high-level object-oriented language can attain high performance on a modified reduced instruction set architecture. The single most important optimization is the removal of a layer of interpretation, compiling the bytecoded virtual machine instructions into low-level, register-based, hardware instructions. This paper describes the compiler and how it was affected by SOAR architectural features. The compiler generates code of reasonable density and speed. Because of Smalltalk-80's semantics, relatively few optimizations are possible, but hardware and software mechanisms at runtime offset these limitations. Register allocation for an architecture with register windows comprises the major task of the compiler. Performance analysis suggests that SOAR is not simple enough; several hardware features could be efficiently replaced by instruction sequences constructed by the compiler.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {112--116},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36192},
 doi = {http://dx.doi.org/10.1145/36206.36192},
 acmid = {36192},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Bush:1987:CSR:36177.36192,
 author = {Bush, William R. and Samples, A. Dain and Ungar, David and Hilfinger, Paul N.},
 title = {Compiling Smalltalk-80 to a RISC},
 abstract = {The Smalltalk On A RISC project at U. C. Berkeley proves that a high-level object-oriented language can attain high performance on a modified reduced instruction set architecture. The single most important optimization is the removal of a layer of interpretation, compiling the bytecoded virtual machine instructions into low-level, register-based, hardware instructions. This paper describes the compiler and how it was affected by SOAR architectural features. The compiler generates code of reasonable density and speed. Because of Smalltalk-80's semantics, relatively few optimizations are possible, but hardware and software mechanisms at runtime offset these limitations. Register allocation for an architecture with register windows comprises the major task of the compiler. Performance analysis suggests that SOAR is not simple enough; several hardware features could be efficiently replaced by instruction sequences constructed by the compiler.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {112--116},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36192},
 doi = {http://doi.acm.org/10.1145/36177.36192},
 acmid = {36192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chow:1987:MAM:36177.36193,
 author = {Chow, F. and Correll, S. and Himelstein, M. and Killian, E. and Weber, L.},
 title = {How many addressing modes are enough?},
 abstract = {Programs naturally require a variety of memory-addressing modes. It isn't necessary to provide them in hardware, however, if a compiler can synthesize them from a few primitive modes. This not only simplifies the hardware, but also permits the compiler to use its understanding of the program to economize on the modes which it uses. We present some compilation techniques that allow the compiler to deal effectively with a single addressing mode in a target RISC processor. We also give measurements to show the benefits of such techniques, and to support our assertion that a single addressing mode is adequate for a general purpose processor, provided that mode incorporates both a pointer and an offset.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {117--121},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36193},
 doi = {http://doi.acm.org/10.1145/36177.36193},
 acmid = {36193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chow:1987:MAM:36206.36193,
 author = {Chow, F. and Correll, S. and Himelstein, M. and Killian, E. and Weber, L.},
 title = {How many addressing modes are enough?},
 abstract = {Programs naturally require a variety of memory-addressing modes. It isn't necessary to provide them in hardware, however, if a compiler can synthesize them from a few primitive modes. This not only simplifies the hardware, but also permits the compiler to use its understanding of the program to economize on the modes which it uses. We present some compilation techniques that allow the compiler to deal effectively with a single addressing mode in a target RISC processor. We also give measurements to show the benefits of such techniques, and to support our assertion that a single addressing mode is adequate for a general purpose processor, provided that mode incorporates both a pointer and an offset.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {117--121},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36193},
 doi = {http://dx.doi.org/10.1145/36206.36193},
 acmid = {36193},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Chow:1987:MAM:36205.36193,
 author = {Chow, F. and Correll, S. and Himelstein, M. and Killian, E. and Weber, L.},
 title = {How many addressing modes are enough?},
 abstract = {Programs naturally require a variety of memory-addressing modes. It isn't necessary to provide them in hardware, however, if a compiler can synthesize them from a few primitive modes. This not only simplifies the hardware, but also permits the compiler to use its understanding of the program to economize on the modes which it uses. We present some compilation techniques that allow the compiler to deal effectively with a single addressing mode in a target RISC processor. We also give measurements to show the benefits of such techniques, and to support our assertion that a single addressing mode is adequate for a general purpose processor, provided that mode incorporates both a pointer and an offset.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {117--121},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36193},
 doi = {http://doi.acm.org/10.1145/36205.36193},
 acmid = {36193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chow:1987:MAM:36204.36193,
 author = {Chow, F. and Correll, S. and Himelstein, M. and Killian, E. and Weber, L.},
 title = {How many addressing modes are enough?},
 abstract = {Programs naturally require a variety of memory-addressing modes. It isn't necessary to provide them in hardware, however, if a compiler can synthesize them from a few primitive modes. This not only simplifies the hardware, but also permits the compiler to use its understanding of the program to economize on the modes which it uses. We present some compilation techniques that allow the compiler to deal effectively with a single addressing mode in a target RISC processor. We also give measurements to show the benefits of such techniques, and to support our assertion that a single addressing mode is adequate for a general purpose processor, provided that mode incorporates both a pointer and an offset.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {117--121},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36193},
 doi = {http://doi.acm.org/10.1145/36204.36193},
 acmid = {36193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Massalin:1987:SLS:36205.36194,
 author = {Massalin, Henry},
 title = {Superoptimizer: a look at the smallest program},
 abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {122--126},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36194},
 doi = {http://doi.acm.org/10.1145/36205.36194},
 acmid = {36194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Massalin:1987:SLS:36204.36194,
 author = {Massalin, Henry},
 title = {Superoptimizer: a look at the smallest program},
 abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {122--126},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36194},
 doi = {http://doi.acm.org/10.1145/36204.36194},
 acmid = {36194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Massalin:1987:SLS:36206.36194,
 author = {Massalin, Henry},
 title = {Superoptimizer: a look at the smallest program},
 abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {122--126},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36194},
 doi = {http://dx.doi.org/10.1145/36206.36194},
 acmid = {36194},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Massalin:1987:SLS:36177.36194,
 author = {Massalin, Henry},
 title = {Superoptimizer: a look at the smallest program},
 abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {122--126},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36194},
 doi = {http://doi.acm.org/10.1145/36177.36194},
 acmid = {36194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Taki:1987:PAE:36205.36195,
 author = {Taki, Kazuo and Nakajima, Katzuto and Nakashima, Hiroshi and Ikeda, Morihiro},
 title = {Performance and architectural evaluation of the PSI machine},
 abstract = {We evaluated a Prolog machine PSI (Personal Sequential Inference machine) for the purpose of improving and redesigning it. In this evaluation, we measured the execution speed and the dynamic characteristics of cache memory, register file, and branching hardware introduced for high-speed execution of Prolog programs.Execution speed of the PSI firmware interpreter was found to be comparable to that of the DEC-10 Prolog compiled code on the DEC-2060. It was also found that PSI was faster than DEC for executing programs containing much unification and backtracking that require runtime processing.With the cache memory, the hit ratio for application programs was found higher than 96\%; this demonstrates that the Prolog execution has much memory access locality. The memory access frequency and the appearance ratio between Read and Write command were also investigated.Concerning the register file, use rate of each dedicated access mode was measured and effect of each mode was discussed. In the branching function we confirmed a high appearance rate of conditional branches and multi-way branches based on tag values.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {128--135},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/36205.36195},
 doi = {http://doi.acm.org/10.1145/36205.36195},
 acmid = {36195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Taki:1987:PAE:36177.36195,
 author = {Taki, Kazuo and Nakajima, Katzuto and Nakashima, Hiroshi and Ikeda, Morihiro},
 title = {Performance and architectural evaluation of the PSI machine},
 abstract = {We evaluated a Prolog machine PSI (Personal Sequential Inference machine) for the purpose of improving and redesigning it. In this evaluation, we measured the execution speed and the dynamic characteristics of cache memory, register file, and branching hardware introduced for high-speed execution of Prolog programs.Execution speed of the PSI firmware interpreter was found to be comparable to that of the DEC-10 Prolog compiled code on the DEC-2060. It was also found that PSI was faster than DEC for executing programs containing much unification and backtracking that require runtime processing.With the cache memory, the hit ratio for application programs was found higher than 96\%; this demonstrates that the Prolog execution has much memory access locality. The memory access frequency and the appearance ratio between Read and Write command were also investigated.Concerning the register file, use rate of each dedicated access mode was measured and effect of each mode was discussed. In the branching function we confirmed a high appearance rate of conditional branches and multi-way branches based on tag values.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {128--135},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/36177.36195},
 doi = {http://doi.acm.org/10.1145/36177.36195},
 acmid = {36195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Taki:1987:PAE:36206.36195,
 author = {Taki, Kazuo and Nakajima, Katzuto and Nakashima, Hiroshi and Ikeda, Morihiro},
 title = {Performance and architectural evaluation of the PSI machine},
 abstract = {We evaluated a Prolog machine PSI (Personal Sequential Inference machine) for the purpose of improving and redesigning it. In this evaluation, we measured the execution speed and the dynamic characteristics of cache memory, register file, and branching hardware introduced for high-speed execution of Prolog programs.Execution speed of the PSI firmware interpreter was found to be comparable to that of the DEC-10 Prolog compiled code on the DEC-2060. It was also found that PSI was faster than DEC for executing programs containing much unification and backtracking that require runtime processing.With the cache memory, the hit ratio for application programs was found higher than 96\%; this demonstrates that the Prolog execution has much memory access locality. The memory access frequency and the appearance ratio between Read and Write command were also investigated.Concerning the register file, use rate of each dedicated access mode was measured and effect of each mode was discussed. In the branching function we confirmed a high appearance rate of conditional branches and multi-way branches based on tag values.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {128--135},
 numpages = {8},
 url = {http://dx.doi.org/10.1145/36206.36195},
 doi = {http://dx.doi.org/10.1145/36206.36195},
 acmid = {36195},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Taki:1987:PAE:36204.36195,
 author = {Taki, Kazuo and Nakajima, Katzuto and Nakashima, Hiroshi and Ikeda, Morihiro},
 title = {Performance and architectural evaluation of the PSI machine},
 abstract = {We evaluated a Prolog machine PSI (Personal Sequential Inference machine) for the purpose of improving and redesigning it. In this evaluation, we measured the execution speed and the dynamic characteristics of cache memory, register file, and branching hardware introduced for high-speed execution of Prolog programs.Execution speed of the PSI firmware interpreter was found to be comparable to that of the DEC-10 Prolog compiled code on the DEC-2060. It was also found that PSI was faster than DEC for executing programs containing much unification and backtracking that require runtime processing.With the cache memory, the hit ratio for application programs was found higher than 96\%; this demonstrates that the Prolog execution has much memory access locality. The memory access frequency and the appearance ratio between Read and Write command were also investigated.Concerning the register file, use rate of each dedicated access mode was measured and effect of each mode was discussed. In the branching function we confirmed a high appearance rate of conditional branches and multi-way branches based on tag values.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {128--135},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/36204.36195},
 doi = {http://doi.acm.org/10.1145/36204.36195},
 acmid = {36195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Borriello:1987:RVC:36205.36196,
 author = {Borriello, Gaetano and Cherenson, Andrew R. and Danzig, Peter B. and Nelson, Michael N.},
 title = {RISCs vs. CISCs for Prolog: a case study},
 abstract = {This paper compares the performance of executing compiled Prolog code on two different architectures under development at U. C. Berkeley. The first is the PLM, a special-purpose CISC architecture intended as a coprocessor for a host machine. The second is SPUR, a general-purpose RISC architecture that supports tagged data. Fourteen standard benchmark programs were run on both the PLM and SPUR simulators. The compiled code for SPUR was obtained by simple macro-expansion of PLM code generated by the PLM Prolog compiler. The two implementations are compared with regard to static and dynamic program size, execution speed, and memory system performance. On average, the macrocoded SPUR implementation has a static code size 14 times larger than the PLM, executes 16 times more instructions, yet requires only 2.3 times the number of machine cycles (or has the performance of 0.43 PLMs). When memory system performance is taken into account, SPUR is equivalent to 0.29 PLMs. Optimizations of the macro-expanded code and minor architectural changes to SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks. Thus a tagged RISC architecture can execute Prolog at least half as fast as a special-purpose CISC architecture for Prolog.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36205.36196},
 doi = {http://doi.acm.org/10.1145/36205.36196},
 acmid = {36196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Borriello:1987:RVC:36204.36196,
 author = {Borriello, Gaetano and Cherenson, Andrew R. and Danzig, Peter B. and Nelson, Michael N.},
 title = {RISCs vs. CISCs for Prolog: a case study},
 abstract = {This paper compares the performance of executing compiled Prolog code on two different architectures under development at U. C. Berkeley. The first is the PLM, a special-purpose CISC architecture intended as a coprocessor for a host machine. The second is SPUR, a general-purpose RISC architecture that supports tagged data. Fourteen standard benchmark programs were run on both the PLM and SPUR simulators. The compiled code for SPUR was obtained by simple macro-expansion of PLM code generated by the PLM Prolog compiler. The two implementations are compared with regard to static and dynamic program size, execution speed, and memory system performance. On average, the macrocoded SPUR implementation has a static code size 14 times larger than the PLM, executes 16 times more instructions, yet requires only 2.3 times the number of machine cycles (or has the performance of 0.43 PLMs). When memory system performance is taken into account, SPUR is equivalent to 0.29 PLMs. Optimizations of the macro-expanded code and minor architectural changes to SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks. Thus a tagged RISC architecture can execute Prolog at least half as fast as a special-purpose CISC architecture for Prolog.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36204.36196},
 doi = {http://doi.acm.org/10.1145/36204.36196},
 acmid = {36196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Borriello:1987:RVC:36206.36196,
 author = {Borriello, Gaetano and Cherenson, Andrew R. and Danzig, Peter B. and Nelson, Michael N.},
 title = {RISCs vs. CISCs for Prolog: a case study},
 abstract = {This paper compares the performance of executing compiled Prolog code on two different architectures under development at U. C. Berkeley. The first is the PLM, a special-purpose CISC architecture intended as a coprocessor for a host machine. The second is SPUR, a general-purpose RISC architecture that supports tagged data. Fourteen standard benchmark programs were run on both the PLM and SPUR simulators. The compiled code for SPUR was obtained by simple macro-expansion of PLM code generated by the PLM Prolog compiler. The two implementations are compared with regard to static and dynamic program size, execution speed, and memory system performance. On average, the macrocoded SPUR implementation has a static code size 14 times larger than the PLM, executes 16 times more instructions, yet requires only 2.3 times the number of machine cycles (or has the performance of 0.43 PLMs). When memory system performance is taken into account, SPUR is equivalent to 0.29 PLMs. Optimizations of the macro-expanded code and minor architectural changes to SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks. Thus a tagged RISC architecture can execute Prolog at least half as fast as a special-purpose CISC architecture for Prolog.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {136--145},
 numpages = {10},
 url = {http://dx.doi.org/10.1145/36206.36196},
 doi = {http://dx.doi.org/10.1145/36206.36196},
 acmid = {36196},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Borriello:1987:RVC:36177.36196,
 author = {Borriello, Gaetano and Cherenson, Andrew R. and Danzig, Peter B. and Nelson, Michael N.},
 title = {RISCs vs. CISCs for Prolog: a case study},
 abstract = {This paper compares the performance of executing compiled Prolog code on two different architectures under development at U. C. Berkeley. The first is the PLM, a special-purpose CISC architecture intended as a coprocessor for a host machine. The second is SPUR, a general-purpose RISC architecture that supports tagged data. Fourteen standard benchmark programs were run on both the PLM and SPUR simulators. The compiled code for SPUR was obtained by simple macro-expansion of PLM code generated by the PLM Prolog compiler. The two implementations are compared with regard to static and dynamic program size, execution speed, and memory system performance. On average, the macrocoded SPUR implementation has a static code size 14 times larger than the PLM, executes 16 times more instructions, yet requires only 2.3 times the number of machine cycles (or has the performance of 0.43 PLMs). When memory system performance is taken into account, SPUR is equivalent to 0.29 PLMs. Optimizations of the macro-expanded code and minor architectural changes to SPUR would increase this ratio to 0.53, or 0.60 for the largest benchmarks. Thus a tagged RISC architecture can execute Prolog at least half as fast as a special-purpose CISC architecture for Prolog.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36177.36196},
 doi = {http://doi.acm.org/10.1145/36177.36196},
 acmid = {36196},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kieburtz:1987:RAS:36204.36197,
 author = {Kieburtz, Richard B.},
 title = {A RISC architecture for symbolic computation},
 abstract = {The G-machine is a language-directed processor architecture designed to support graph reduction as a model of computation. It can carry out lazy evaluation of functional language programs and can evaluate programs in which logical variables are used. To support these language features, the abstract machine requires tagged memory and executes some rather complex instructions, such as to evaluate a function application.This paper explores an implementation of the G-machine as a high performance RISC architecture. Complex instructions can be represented by RISC code without experiencing a large expansion of code volume. The instruction pipeline is discussed in some detail. The processor is intended to be integrated into a standard, 32-bit memory architecture. Tagged memory is supported by aggregating data with tags in a cache.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36204.36197},
 doi = {http://doi.acm.org/10.1145/36204.36197},
 acmid = {36197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kieburtz:1987:RAS:36206.36197,
 author = {Kieburtz, Richard B.},
 title = {A RISC architecture for symbolic computation},
 abstract = {The G-machine is a language-directed processor architecture designed to support graph reduction as a model of computation. It can carry out lazy evaluation of functional language programs and can evaluate programs in which logical variables are used. To support these language features, the abstract machine requires tagged memory and executes some rather complex instructions, such as to evaluate a function application.This paper explores an implementation of the G-machine as a high performance RISC architecture. Complex instructions can be represented by RISC code without experiencing a large expansion of code volume. The instruction pipeline is discussed in some detail. The processor is intended to be integrated into a standard, 32-bit memory architecture. Tagged memory is supported by aggregating data with tags in a cache.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {146--155},
 numpages = {10},
 url = {http://dx.doi.org/10.1145/36206.36197},
 doi = {http://dx.doi.org/10.1145/36206.36197},
 acmid = {36197},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Kieburtz:1987:RAS:36177.36197,
 author = {Kieburtz, Richard B.},
 title = {A RISC architecture for symbolic computation},
 abstract = {The G-machine is a language-directed processor architecture designed to support graph reduction as a model of computation. It can carry out lazy evaluation of functional language programs and can evaluate programs in which logical variables are used. To support these language features, the abstract machine requires tagged memory and executes some rather complex instructions, such as to evaluate a function application.This paper explores an implementation of the G-machine as a high performance RISC architecture. Complex instructions can be represented by RISC code without experiencing a large expansion of code volume. The instruction pipeline is discussed in some detail. The processor is intended to be integrated into a standard, 32-bit memory architecture. Tagged memory is supported by aggregating data with tags in a cache.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36177.36197},
 doi = {http://doi.acm.org/10.1145/36177.36197},
 acmid = {36197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kieburtz:1987:RAS:36205.36197,
 author = {Kieburtz, Richard B.},
 title = {A RISC architecture for symbolic computation},
 abstract = {The G-machine is a language-directed processor architecture designed to support graph reduction as a model of computation. It can carry out lazy evaluation of functional language programs and can evaluate programs in which logical variables are used. To support these language features, the abstract machine requires tagged memory and executes some rather complex instructions, such as to evaluate a function application.This paper explores an implementation of the G-machine as a high performance RISC architecture. Complex instructions can be represented by RISC code without experiencing a large expansion of code volume. The instruction pipeline is discussed in some detail. The processor is intended to be integrated into a standard, 32-bit memory architecture. Tagged memory is supported by aggregating data with tags in a cache.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/36205.36197},
 doi = {http://doi.acm.org/10.1145/36205.36197},
 acmid = {36197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ditzel:1987:DTS:36206.36198,
 author = {Ditzel, David R. and McLellan, Hubert R. and Berenbaum, Alan D.},
 title = {Design tradeoffs to support the C programming language in the CRISP microprocessor},
 abstract = {},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {158--163},
 numpages = {6},
 url = {http://dx.doi.org/10.1145/36206.36198},
 doi = {http://dx.doi.org/10.1145/36206.36198},
 acmid = {36198},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Ditzel:1987:DTS:36205.36198,
 author = {Ditzel, David R. and McLellan, Hubert R. and Berenbaum, Alan D.},
 title = {Design tradeoffs to support the C programming language in the CRISP microprocessor},
 abstract = {},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {158--163},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36205.36198},
 doi = {http://doi.acm.org/10.1145/36205.36198},
 acmid = {36198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ditzel:1987:DTS:36177.36198,
 author = {Ditzel, David R. and McLellan, Hubert R. and Berenbaum, Alan D.},
 title = {Design tradeoffs to support the C programming language in the CRISP microprocessor},
 abstract = {},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {158--163},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36177.36198},
 doi = {http://doi.acm.org/10.1145/36177.36198},
 acmid = {36198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ditzel:1987:DTS:36204.36198,
 author = {Ditzel, David R. and McLellan, Hubert R. and Berenbaum, Alan D.},
 title = {Design tradeoffs to support the C programming language in the CRISP microprocessor},
 abstract = {},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {158--163},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36204.36198},
 doi = {http://doi.acm.org/10.1145/36204.36198},
 acmid = {36198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thacker:1987:FMW:36205.36199,
 author = {Thacker, Charles P. and Stewart, Lawrence C.},
 title = {Firefly: a multiprocessor workstation},
 abstract = {Firefly is a shared-memory multiprocessor workstation that contains from one to seven MicroVAX 78032 processors, each with a floating point unit and a sixteen kilobyte cache. The caches are coherent, so that all processors see a consistent view of main memory. A system may contain from four to sixteen megabytes of storage. Input-output is done via a standard DEC QBus. Input-output devices are an Ethernet controller, fixed disks, and a monochrome 1024 x 768 display with keyboard and mouse. Optional hardware includes a high resolution color display and a controller for high capacity disks. Figure 1 is a system block diagram.The Firefly runs a software system that emulates the Ultrix system call interface. It also supports medium- and coarse-grained multiprocessing through multiple threads of control in a single address space. Communications are implemented uniformly through the use of remote procedure calls.This paper describes the goals, architecture, implementation and performance analysis of the Firefly. It then presents some measurements of hardware performance, and discusses the degree to which SRC has been successful in producing software to take advantage of multiprocessing.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {164--172},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/36205.36199},
 doi = {http://doi.acm.org/10.1145/36205.36199},
 acmid = {36199},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thacker:1987:FMW:36177.36199,
 author = {Thacker, Charles P. and Stewart, Lawrence C.},
 title = {Firefly: a multiprocessor workstation},
 abstract = {Firefly is a shared-memory multiprocessor workstation that contains from one to seven MicroVAX 78032 processors, each with a floating point unit and a sixteen kilobyte cache. The caches are coherent, so that all processors see a consistent view of main memory. A system may contain from four to sixteen megabytes of storage. Input-output is done via a standard DEC QBus. Input-output devices are an Ethernet controller, fixed disks, and a monochrome 1024 x 768 display with keyboard and mouse. Optional hardware includes a high resolution color display and a controller for high capacity disks. Figure 1 is a system block diagram.The Firefly runs a software system that emulates the Ultrix system call interface. It also supports medium- and coarse-grained multiprocessing through multiple threads of control in a single address space. Communications are implemented uniformly through the use of remote procedure calls.This paper describes the goals, architecture, implementation and performance analysis of the Firefly. It then presents some measurements of hardware performance, and discusses the degree to which SRC has been successful in producing software to take advantage of multiprocessing.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {164--172},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/36177.36199},
 doi = {http://doi.acm.org/10.1145/36177.36199},
 acmid = {36199},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thacker:1987:FMW:36206.36199,
 author = {Thacker, Charles P. and Stewart, Lawrence C.},
 title = {Firefly: a multiprocessor workstation},
 abstract = {Firefly is a shared-memory multiprocessor workstation that contains from one to seven MicroVAX 78032 processors, each with a floating point unit and a sixteen kilobyte cache. The caches are coherent, so that all processors see a consistent view of main memory. A system may contain from four to sixteen megabytes of storage. Input-output is done via a standard DEC QBus. Input-output devices are an Ethernet controller, fixed disks, and a monochrome 1024 x 768 display with keyboard and mouse. Optional hardware includes a high resolution color display and a controller for high capacity disks. Figure 1 is a system block diagram.The Firefly runs a software system that emulates the Ultrix system call interface. It also supports medium- and coarse-grained multiprocessing through multiple threads of control in a single address space. Communications are implemented uniformly through the use of remote procedure calls.This paper describes the goals, architecture, implementation and performance analysis of the Firefly. It then presents some measurements of hardware performance, and discusses the degree to which SRC has been successful in producing software to take advantage of multiprocessing.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {164--172},
 numpages = {9},
 url = {http://dx.doi.org/10.1145/36206.36199},
 doi = {http://dx.doi.org/10.1145/36206.36199},
 acmid = {36199},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Thacker:1987:FMW:36204.36199,
 author = {Thacker, Charles P. and Stewart, Lawrence C.},
 title = {Firefly: a multiprocessor workstation},
 abstract = {Firefly is a shared-memory multiprocessor workstation that contains from one to seven MicroVAX 78032 processors, each with a floating point unit and a sixteen kilobyte cache. The caches are coherent, so that all processors see a consistent view of main memory. A system may contain from four to sixteen megabytes of storage. Input-output is done via a standard DEC QBus. Input-output devices are an Ethernet controller, fixed disks, and a monochrome 1024 x 768 display with keyboard and mouse. Optional hardware includes a high resolution color display and a controller for high capacity disks. Figure 1 is a system block diagram.The Firefly runs a software system that emulates the Ultrix system call interface. It also supports medium- and coarse-grained multiprocessing through multiple threads of control in a single address space. Communications are implemented uniformly through the use of remote procedure calls.This paper describes the goals, architecture, implementation and performance analysis of the Firefly. It then presents some measurements of hardware performance, and discusses the degree to which SRC has been successful in producing software to take advantage of multiprocessing.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {164--172},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/36204.36199},
 doi = {http://doi.acm.org/10.1145/36204.36199},
 acmid = {36199},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Clark:1987:PPV:36177.36200,
 author = {Clark, Douglas W.},
 title = {Pipelining and performance in the VAX 8800 processor},
 abstract = {The VAX 8800 family (models 8800, 8700, 8550), currently the fastest computers in the VAX product line, achieve their speed through a combination of fast cycle time and deep pipelining. Rather than pipeline highly variable VAX instructions as such, the 8800 design pipelines uniform microinstructions whose addresses are generated by instruction unit hardware. This design approach helps achieve a fast cycle time, which is the prime determinan of performance. Some preliminary measurements of cycles per average instruction are reported.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {173--177},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36177.36200},
 doi = {http://doi.acm.org/10.1145/36177.36200},
 acmid = {36200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Clark:1987:PPV:36205.36200,
 author = {Clark, Douglas W.},
 title = {Pipelining and performance in the VAX 8800 processor},
 abstract = {The VAX 8800 family (models 8800, 8700, 8550), currently the fastest computers in the VAX product line, achieve their speed through a combination of fast cycle time and deep pipelining. Rather than pipeline highly variable VAX instructions as such, the 8800 design pipelines uniform microinstructions whose addresses are generated by instruction unit hardware. This design approach helps achieve a fast cycle time, which is the prime determinan of performance. Some preliminary measurements of cycles per average instruction are reported.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {173--177},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36205.36200},
 doi = {http://doi.acm.org/10.1145/36205.36200},
 acmid = {36200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Clark:1987:PPV:36204.36200,
 author = {Clark, Douglas W.},
 title = {Pipelining and performance in the VAX 8800 processor},
 abstract = {The VAX 8800 family (models 8800, 8700, 8550), currently the fastest computers in the VAX product line, achieve their speed through a combination of fast cycle time and deep pipelining. Rather than pipeline highly variable VAX instructions as such, the 8800 design pipelines uniform microinstructions whose addresses are generated by instruction unit hardware. This design approach helps achieve a fast cycle time, which is the prime determinan of performance. Some preliminary measurements of cycles per average instruction are reported.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {173--177},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/36204.36200},
 doi = {http://doi.acm.org/10.1145/36204.36200},
 acmid = {36200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Clark:1987:PPV:36206.36200,
 author = {Clark, Douglas W.},
 title = {Pipelining and performance in the VAX 8800 processor},
 abstract = {The VAX 8800 family (models 8800, 8700, 8550), currently the fastest computers in the VAX product line, achieve their speed through a combination of fast cycle time and deep pipelining. Rather than pipeline highly variable VAX instructions as such, the 8800 design pipelines uniform microinstructions whose addresses are generated by instruction unit hardware. This design approach helps achieve a fast cycle time, which is the prime determinan of performance. Some preliminary measurements of cycles per average instruction are reported.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {173--177},
 numpages = {5},
 url = {http://dx.doi.org/10.1145/36206.36200},
 doi = {http://dx.doi.org/10.1145/36206.36200},
 acmid = {36200},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Colwell:1987:VAT:36205.36201,
 author = {Colwell, Robert P. and Nix, Robert P. and O'Donnell, John J. and Papworth, David B. and Rodman, Paul K.},
 title = {A VLIW architecture for a trace scheduling compiler},
 abstract = {Very Long Instruction Word (VLIW) architectures were promised to deliver far more than the factor of two or three that current architectures achieve from overlapped execution. Using a new type of compiler which compacts ordinary sequential code into long instruction words, a VLIW machine was expected to provide from ten to thirty times the performance of a more conventional machine built of the same implementation technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE<sup>TM</sup> along with its companion Trace Scheduling<sup>TM</sup> compacting compiler. This new machine has fulfilled the performance promises that were made. Using many fast functional units in parallel, this machine extends some of the basic Reduced-Instruction-Set precepts: the architecture is load/store, the microarchitecture is exposed to the compiler, there is no microcode, and there is almost no hardware devoted to synchronization, arbitration, or interlocking of any kind (the compiler has sole responsibility for runtime resource usage).This paper discusses the design of this machine and presents some initial performance results.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {180--192},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/36205.36201},
 doi = {http://doi.acm.org/10.1145/36205.36201},
 acmid = {36201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Colwell:1987:VAT:36206.36201,
 author = {Colwell, Robert P. and Nix, Robert P. and O'Donnell, John J. and Papworth, David B. and Rodman, Paul K.},
 title = {A VLIW architecture for a trace scheduling compiler},
 abstract = {Very Long Instruction Word (VLIW) architectures were promised to deliver far more than the factor of two or three that current architectures achieve from overlapped execution. Using a new type of compiler which compacts ordinary sequential code into long instruction words, a VLIW machine was expected to provide from ten to thirty times the performance of a more conventional machine built of the same implementation technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE<sup>TM</sup> along with its companion Trace Scheduling<sup>TM</sup> compacting compiler. This new machine has fulfilled the performance promises that were made. Using many fast functional units in parallel, this machine extends some of the basic Reduced-Instruction-Set precepts: the architecture is load/store, the microarchitecture is exposed to the compiler, there is no microcode, and there is almost no hardware devoted to synchronization, arbitration, or interlocking of any kind (the compiler has sole responsibility for runtime resource usage).This paper discusses the design of this machine and presents some initial performance results.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {180--192},
 numpages = {13},
 url = {http://dx.doi.org/10.1145/36206.36201},
 doi = {http://dx.doi.org/10.1145/36206.36201},
 acmid = {36201},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Colwell:1987:VAT:36177.36201,
 author = {Colwell, Robert P. and Nix, Robert P. and O'Donnell, John J. and Papworth, David B. and Rodman, Paul K.},
 title = {A VLIW architecture for a trace scheduling compiler},
 abstract = {Very Long Instruction Word (VLIW) architectures were promised to deliver far more than the factor of two or three that current architectures achieve from overlapped execution. Using a new type of compiler which compacts ordinary sequential code into long instruction words, a VLIW machine was expected to provide from ten to thirty times the performance of a more conventional machine built of the same implementation technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE<sup>TM</sup> along with its companion Trace Scheduling<sup>TM</sup> compacting compiler. This new machine has fulfilled the performance promises that were made. Using many fast functional units in parallel, this machine extends some of the basic Reduced-Instruction-Set precepts: the architecture is load/store, the microarchitecture is exposed to the compiler, there is no microcode, and there is almost no hardware devoted to synchronization, arbitration, or interlocking of any kind (the compiler has sole responsibility for runtime resource usage).This paper discusses the design of this machine and presents some initial performance results.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {180--192},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/36177.36201},
 doi = {http://doi.acm.org/10.1145/36177.36201},
 acmid = {36201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Colwell:1987:VAT:36204.36201,
 author = {Colwell, Robert P. and Nix, Robert P. and O'Donnell, John J. and Papworth, David B. and Rodman, Paul K.},
 title = {A VLIW architecture for a trace scheduling compiler},
 abstract = {Very Long Instruction Word (VLIW) architectures were promised to deliver far more than the factor of two or three that current architectures achieve from overlapped execution. Using a new type of compiler which compacts ordinary sequential code into long instruction words, a VLIW machine was expected to provide from ten to thirty times the performance of a more conventional machine built of the same implementation technology.Multiflow Computer, Inc., has now built a VLIW called the TRACE<sup>TM</sup> along with its companion Trace Scheduling<sup>TM</sup> compacting compiler. This new machine has fulfilled the performance promises that were made. Using many fast functional units in parallel, this machine extends some of the basic Reduced-Instruction-Set precepts: the architecture is load/store, the microarchitecture is exposed to the compiler, there is no microcode, and there is almost no hardware devoted to synchronization, arbitration, or interlocking of any kind (the compiler has sole responsibility for runtime resource usage).This paper discusses the design of this machine and presents some initial performance results.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {180--192},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/36204.36201},
 doi = {http://doi.acm.org/10.1145/36204.36201},
 acmid = {36201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levinthal:1987:PCG:36205.36202,
 author = {Levinthal, Adam and Hanrahan, Pat and Paquette, Mike and Lawson, Jim},
 title = {Parallel computers for graphics applications},
 abstract = {Specialized computer architectures can provide better price/performance for executing image processing and graphics applications than general purpose designs. Two processors are presented that use parallel SIMD data paths to support common graphics data structures as primitive operands in arithmetic expressions. A variant of the C language has been implemented to allow high level language coding of user applications on these processors. High level programming support is designed into the processor architecture that implements parallel object data typing and parallel conditional evaluation in hardware.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {193--198},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36205.36202},
 doi = {http://doi.acm.org/10.1145/36205.36202},
 acmid = {36202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levinthal:1987:PCG:36177.36202,
 author = {Levinthal, Adam and Hanrahan, Pat and Paquette, Mike and Lawson, Jim},
 title = {Parallel computers for graphics applications},
 abstract = {Specialized computer architectures can provide better price/performance for executing image processing and graphics applications than general purpose designs. Two processors are presented that use parallel SIMD data paths to support common graphics data structures as primitive operands in arithmetic expressions. A variant of the C language has been implemented to allow high level language coding of user applications on these processors. High level programming support is designed into the processor architecture that implements parallel object data typing and parallel conditional evaluation in hardware.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {193--198},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36177.36202},
 doi = {http://doi.acm.org/10.1145/36177.36202},
 acmid = {36202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Levinthal:1987:PCG:36204.36202,
 author = {Levinthal, Adam and Hanrahan, Pat and Paquette, Mike and Lawson, Jim},
 title = {Parallel computers for graphics applications},
 abstract = {Specialized computer architectures can provide better price/performance for executing image processing and graphics applications than general purpose designs. Two processors are presented that use parallel SIMD data paths to support common graphics data structures as primitive operands in arithmetic expressions. A variant of the C language has been implemented to allow high level language coding of user applications on these processors. High level programming support is designed into the processor architecture that implements parallel object data typing and parallel conditional evaluation in hardware.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {193--198},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36204.36202},
 doi = {http://doi.acm.org/10.1145/36204.36202},
 acmid = {36202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Levinthal:1987:PCG:36206.36202,
 author = {Levinthal, Adam and Hanrahan, Pat and Paquette, Mike and Lawson, Jim},
 title = {Parallel computers for graphics applications},
 abstract = {Specialized computer architectures can provide better price/performance for executing image processing and graphics applications than general purpose designs. Two processors are presented that use parallel SIMD data paths to support common graphics data structures as primitive operands in arithmetic expressions. A variant of the C language has been implemented to allow high level language coding of user applications on these processors. High level programming support is designed into the processor architecture that implements parallel object data typing and parallel conditional evaluation in hardware.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {193--198},
 numpages = {6},
 url = {http://dx.doi.org/10.1145/36206.36202},
 doi = {http://dx.doi.org/10.1145/36206.36202},
 acmid = {36202},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@article{Smith:1987:ZCP:36204.36203,
 author = {Smith, J. E. and Dermer, G. E. and Vanderwarn, B. D. and Klinger, S. D. and Rozewski, C. M.},
 title = {The ZS-1 central processor},
 abstract = {The Astronautics ZS-1 is a high speed, 64-bit computer system designed for scientific and engineering applications. The ZS-1 central processor uses a decoupled architecture, which splits instructions into two streams---one for fixed point/memory address computation and the other for floating point operations. The two instruction streams are then processed in parallel. Pipelining is also used extensively throughout the ZS-1.This paper describes the architecture and implementation of the ZS-1 central processor, beginning with some of the basic design objectives. Descriptions of the instruction set, pipeline structure, and virtual memory implementation demonstrate the methods used to satisfy the objectives. High performance is achieved through a combination of static (compile-time) instruction scheduling and dynamic (run-time) scheduling. Both types of scheduling are illustrated with examples.},
 journal = {SIGOPS Oper. Syst. Rev.},
 volume = {21},
 issue = {4},
 month = {October},
 year = {1987},
 issn = {0163-5980},
 pages = {199--204},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36204.36203},
 doi = {http://doi.acm.org/10.1145/36204.36203},
 acmid = {36203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:1987:ZCP:36177.36203,
 author = {Smith, J. E. and Dermer, G. E. and Vanderwarn, B. D. and Klinger, S. D. and Rozewski, C. M.},
 title = {The ZS-1 central processor},
 abstract = {The Astronautics ZS-1 is a high speed, 64-bit computer system designed for scientific and engineering applications. The ZS-1 central processor uses a decoupled architecture, which splits instructions into two streams---one for fixed point/memory address computation and the other for floating point operations. The two instruction streams are then processed in parallel. Pipelining is also used extensively throughout the ZS-1.This paper describes the architecture and implementation of the ZS-1 central processor, beginning with some of the basic design objectives. Descriptions of the instruction set, pipeline structure, and virtual memory implementation demonstrate the methods used to satisfy the objectives. High performance is achieved through a combination of static (compile-time) instruction scheduling and dynamic (run-time) scheduling. Both types of scheduling are illustrated with examples.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {15},
 issue = {5},
 month = {October},
 year = {1987},
 issn = {0163-5964},
 pages = {199--204},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36177.36203},
 doi = {http://doi.acm.org/10.1145/36177.36203},
 acmid = {36203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:1987:ZCP:36205.36203,
 author = {Smith, J. E. and Dermer, G. E. and Vanderwarn, B. D. and Klinger, S. D. and Rozewski, C. M.},
 title = {The ZS-1 central processor},
 abstract = {The Astronautics ZS-1 is a high speed, 64-bit computer system designed for scientific and engineering applications. The ZS-1 central processor uses a decoupled architecture, which splits instructions into two streams---one for fixed point/memory address computation and the other for floating point operations. The two instruction streams are then processed in parallel. Pipelining is also used extensively throughout the ZS-1.This paper describes the architecture and implementation of the ZS-1 central processor, beginning with some of the basic design objectives. Descriptions of the instruction set, pipeline structure, and virtual memory implementation demonstrate the methods used to satisfy the objectives. High performance is achieved through a combination of static (compile-time) instruction scheduling and dynamic (run-time) scheduling. Both types of scheduling are illustrated with examples.},
 journal = {SIGPLAN Not.},
 volume = {22},
 issue = {10},
 month = {October},
 year = {1987},
 issn = {0362-1340},
 pages = {199--204},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/36205.36203},
 doi = {http://doi.acm.org/10.1145/36205.36203},
 acmid = {36203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smith:1987:ZCP:36206.36203,
 author = {Smith, J. E. and Dermer, G. E. and Vanderwarn, B. D. and Klinger, S. D. and Rozewski, C. M.},
 title = {The ZS-1 central processor},
 abstract = {The Astronautics ZS-1 is a high speed, 64-bit computer system designed for scientific and engineering applications. The ZS-1 central processor uses a decoupled architecture, which splits instructions into two streams---one for fixed point/memory address computation and the other for floating point operations. The two instruction streams are then processed in parallel. Pipelining is also used extensively throughout the ZS-1.This paper describes the architecture and implementation of the ZS-1 central processor, beginning with some of the basic design objectives. Descriptions of the instruction set, pipeline structure, and virtual memory implementation demonstrate the methods used to satisfy the objectives. High performance is achieved through a combination of static (compile-time) instruction scheduling and dynamic (run-time) scheduling. Both types of scheduling are illustrated with examples.},
 booktitle = {Proceedings of the second international conference on Architectual support for programming languages and operating systems},
 series = {ASPLOS-II},
 year = {1987},
 isbn = {0-8186-0805-6},
 location = {Palo Alto, California, United States},
 pages = {199--204},
 numpages = {6},
 url = {http://dx.doi.org/10.1145/36206.36203},
 doi = {http://dx.doi.org/10.1145/36206.36203},
 acmid = {36203},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@inproceedings{Rattner:1982:HCI:800050.801819,
 author = {Rattner, Justin},
 title = {Hardware/software cooperation in the iAPX-432},
 abstract = {The Intel iAPX-432 is an object-based microcomputer system with a unified approach to the design and use of its architecture, operating system, and primary programming language. The concrete architecture of the 432 incorporates hardware support for data abstraction, small protection domains, and language-oriented run-time environments. It also uses its object-orientation to provide hardware support for dynamic heap storage management, interprocess communication, and processor dispatching. We begin with an overview of the 432 architecture so readers unfamiliar with its basic concepts will be able to follow the succeeding discussion without the need to consult the references. Following that, we introduce the various forms of hardware/software cooperation and the criteria by which a function or service is selected for migration. This is followed by several of the more interesting examples of hardware/software cooperation in the 432. A comparison of cooperation in the 432 with several contemporary machines and discussions of development issues, past and future, complete the paper.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/800050.801819},
 doi = {http://doi.acm.org/10.1145/800050.801819},
 acmid = {801819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rattner:1982:HCI:964750.801819,
 author = {Rattner, Justin},
 title = {Hardware/software cooperation in the iAPX-432},
 abstract = {The Intel iAPX-432 is an object-based microcomputer system with a unified approach to the design and use of its architecture, operating system, and primary programming language. The concrete architecture of the 432 incorporates hardware support for data abstraction, small protection domains, and language-oriented run-time environments. It also uses its object-orientation to provide hardware support for dynamic heap storage management, interprocess communication, and processor dispatching. We begin with an overview of the 432 architecture so readers unfamiliar with its basic concepts will be able to follow the succeeding discussion without the need to consult the references. Following that, we introduce the various forms of hardware/software cooperation and the criteria by which a function or service is selected for migration. This is followed by several of the more interesting examples of hardware/software cooperation in the 432. A comparison of cooperation in the 432 with several contemporary machines and discussions of development issues, past and future, complete the paper.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/964750.801819},
 doi = {http://doi.acm.org/10.1145/964750.801819},
 acmid = {801819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rattner:1982:HCI:960120.801819,
 author = {Rattner, Justin},
 title = {Hardware/software cooperation in the iAPX-432},
 abstract = {The Intel iAPX-432 is an object-based microcomputer system with a unified approach to the design and use of its architecture, operating system, and primary programming language. The concrete architecture of the 432 incorporates hardware support for data abstraction, small protection domains, and language-oriented run-time environments. It also uses its object-orientation to provide hardware support for dynamic heap storage management, interprocess communication, and processor dispatching. We begin with an overview of the 432 architecture so readers unfamiliar with its basic concepts will be able to follow the succeeding discussion without the need to consult the references. Following that, we introduce the various forms of hardware/software cooperation and the criteria by which a function or service is selected for migration. This is followed by several of the more interesting examples of hardware/software cooperation in the 432. A comparison of cooperation in the 432 with several contemporary machines and discussions of development issues, past and future, complete the paper.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/960120.801819},
 doi = {http://doi.acm.org/10.1145/960120.801819},
 acmid = {801819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hennessy:1982:HTI:960120.801820,
 author = {Hennessy, John and Jouppi, Norman and Baskett, Forest and Gross, Thomas and Gill, John},
 title = {Hardware/software tradeoffs for increased performance},
 abstract = {Most new computer architectures are concerned with maximizing performance by providing suitable instruction sets for compiled code and providing support for systems functions. We argue that the most effective design methodology must make simultaneous tradeoffs across all three areas: hardware, software support, and systems support. Recent trends lean towards extensive hardware support for both the compiler and operating systems software. However, consideration of all possible design tradeoffs may often lead to less hardware support. Several examples of this approach are presented, including: omission of condition codes, word-addressed machines, and imposing pipeline interlocks in software. The specifics and performance of these approaches are examined with respect to the MIPS processor.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960120.801820},
 doi = {http://doi.acm.org/10.1145/960120.801820},
 acmid = {801820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hennessy:1982:HTI:800050.801820,
 author = {Hennessy, John and Jouppi, Norman and Baskett, Forest and Gross, Thomas and Gill, John},
 title = {Hardware/software tradeoffs for increased performance},
 abstract = {Most new computer architectures are concerned with maximizing performance by providing suitable instruction sets for compiled code and providing support for systems functions. We argue that the most effective design methodology must make simultaneous tradeoffs across all three areas: hardware, software support, and systems support. Recent trends lean towards extensive hardware support for both the compiler and operating systems software. However, consideration of all possible design tradeoffs may often lead to less hardware support. Several examples of this approach are presented, including: omission of condition codes, word-addressed machines, and imposing pipeline interlocks in software. The specifics and performance of these approaches are examined with respect to the MIPS processor.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800050.801820},
 doi = {http://doi.acm.org/10.1145/800050.801820},
 acmid = {801820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hennessy:1982:HTI:964750.801820,
 author = {Hennessy, John and Jouppi, Norman and Baskett, Forest and Gross, Thomas and Gill, John},
 title = {Hardware/software tradeoffs for increased performance},
 abstract = {Most new computer architectures are concerned with maximizing performance by providing suitable instruction sets for compiled code and providing support for systems functions. We argue that the most effective design methodology must make simultaneous tradeoffs across all three areas: hardware, software support, and systems support. Recent trends lean towards extensive hardware support for both the compiler and operating systems software. However, consideration of all possible design tradeoffs may often lead to less hardware support. Several examples of this approach are presented, including: omission of condition codes, word-addressed machines, and imposing pipeline interlocks in software. The specifics and performance of these approaches are examined with respect to the MIPS processor.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/964750.801820},
 doi = {http://doi.acm.org/10.1145/964750.801820},
 acmid = {801820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rymarczyk:1982:CGP:960120.801821,
 author = {Rymarczyk, James W.},
 title = {Coding guidelines for pipelined processors},
 abstract = {This paper is a tutorial for assembly language programmers of pipelined processors. It describes the general characteristics of pipelined processors and presents a collection of coding guidelines for them. These guidelines are particularly significant to compiler developers who determine object code patterns.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {12--19},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/960120.801821},
 doi = {http://doi.acm.org/10.1145/960120.801821},
 acmid = {801821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rymarczyk:1982:CGP:964750.801821,
 author = {Rymarczyk, James W.},
 title = {Coding guidelines for pipelined processors},
 abstract = {This paper is a tutorial for assembly language programmers of pipelined processors. It describes the general characteristics of pipelined processors and presents a collection of coding guidelines for them. These guidelines are particularly significant to compiler developers who determine object code patterns.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {12--19},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/964750.801821},
 doi = {http://doi.acm.org/10.1145/964750.801821},
 acmid = {801821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rymarczyk:1982:CGP:800050.801821,
 author = {Rymarczyk, James W.},
 title = {Coding guidelines for pipelined processors},
 abstract = {This paper is a tutorial for assembly language programmers of pipelined processors. It describes the general characteristics of pipelined processors and presents a collection of coding guidelines for them. These guidelines are particularly significant to compiler developers who determine object code patterns.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {12--19},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800050.801821},
 doi = {http://doi.acm.org/10.1145/800050.801821},
 acmid = {801821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Johnsson:1982:OMP:800050.801822,
 author = {Johnsson, Richard K. and Wick, John D.},
 title = {An overview of the mesa processor architecture},
 abstract = {This paper provides an overview of the architecture of the Mesa processor, an architecture which was designed to support the Mesa programming system [4]. Mesa is a high level systems programming language and associated tools designed to support the development of large information processing applications (on the order of one million source lines). Since the start of development in 1971, the processor architecture, the programming language, and the operating system have been designed as a unit, so that proper tradeoffs among these components could be made.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {20--29},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800050.801822},
 doi = {http://doi.acm.org/10.1145/800050.801822},
 acmid = {801822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnsson:1982:OMP:960120.801822,
 author = {Johnsson, Richard K. and Wick, John D.},
 title = {An overview of the mesa processor architecture},
 abstract = {This paper provides an overview of the architecture of the Mesa processor, an architecture which was designed to support the Mesa programming system [4]. Mesa is a high level systems programming language and associated tools designed to support the development of large information processing applications (on the order of one million source lines). Since the start of development in 1971, the processor architecture, the programming language, and the operating system have been designed as a unit, so that proper tradeoffs among these components could be made.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {20--29},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960120.801822},
 doi = {http://doi.acm.org/10.1145/960120.801822},
 acmid = {801822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnsson:1982:OMP:964750.801822,
 author = {Johnsson, Richard K. and Wick, John D.},
 title = {An overview of the mesa processor architecture},
 abstract = {This paper provides an overview of the architecture of the Mesa processor, an architecture which was designed to support the Mesa programming system [4]. Mesa is a high level systems programming language and associated tools designed to support the development of large information processing applications (on the order of one million source lines). Since the start of development in 1971, the processor architecture, the programming language, and the operating system have been designed as a unit, so that proper tradeoffs among these components could be made.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {20--29},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/964750.801822},
 doi = {http://doi.acm.org/10.1145/964750.801822},
 acmid = {801822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berenbaum:1982:OSL:960120.801823,
 author = {Berenbaum, Alan D. and Condry, Michael W. and Lu, Priscilla M.},
 title = {The operating system and language support features of the BELLMAC<supscrpt>TM</supscrpt>-32 microprocessor.},
 abstract = {The BELLMAC-32 microprocessor is a 32-bit microprocessor, implemented with CMOS technology, designed to support operating system functions and high level languages efficiently. The architecture was designed with the following objectives in mind: \&bull; High performance. \&bull; Enhanced operating system support capabilities. \&bull; High level language support. \&bull; High reliability, availability and maintainability.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {30--38},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801823},
 doi = {http://doi.acm.org/10.1145/960120.801823},
 acmid = {801823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Berenbaum:1982:OSL:800050.801823,
 author = {Berenbaum, Alan D. and Condry, Michael W. and Lu, Priscilla M.},
 title = {The operating system and language support features of the BELLMAC<supscrpt>TM</supscrpt>-32 microprocessor.},
 abstract = {The BELLMAC-32 microprocessor is a 32-bit microprocessor, implemented with CMOS technology, designed to support operating system functions and high level languages efficiently. The architecture was designed with the following objectives in mind: \&bull; High performance. \&bull; Enhanced operating system support capabilities. \&bull; High level language support. \&bull; High reliability, availability and maintainability.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {30--38},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801823},
 doi = {http://doi.acm.org/10.1145/800050.801823},
 acmid = {801823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berenbaum:1982:OSL:964750.801823,
 author = {Berenbaum, Alan D. and Condry, Michael W. and Lu, Priscilla M.},
 title = {The operating system and language support features of the BELLMAC<supscrpt>TM</supscrpt>-32 microprocessor.},
 abstract = {The BELLMAC-32 microprocessor is a 32-bit microprocessor, implemented with CMOS technology, designed to support operating system functions and high level languages efficiently. The architecture was designed with the following objectives in mind: \&bull; High performance. \&bull; Enhanced operating system support capabilities. \&bull; High level language support. \&bull; High reliability, availability and maintainability.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {30--38},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801823},
 doi = {http://doi.acm.org/10.1145/964750.801823},
 acmid = {801823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Radin:1982:MIN:964750.801824,
 author = {Radin, George},
 title = {The 801 minicomputer},
 abstract = {This paper provides an overview of an experimental system developed at the IBM T. J. Watson Research Center. It consists of a running hardware prototype, a control program and an optimizing compiler. The basic concepts underlying the system are discussed as are the performance characteristics of the prototype. In particular, three principles are examined: system orientation towards the pervasive use of high level language programming and a sophisticated compiler, a primitive instruction set which can be completely hard-wired, storage hierarchy and I/O organization to enable the CPU to execute an instruction at almost every cycle.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {39--47},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801824},
 doi = {http://doi.acm.org/10.1145/964750.801824},
 acmid = {801824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Radin:1982:MIN:960120.801824,
 author = {Radin, George},
 title = {The 801 minicomputer},
 abstract = {This paper provides an overview of an experimental system developed at the IBM T. J. Watson Research Center. It consists of a running hardware prototype, a control program and an optimizing compiler. The basic concepts underlying the system are discussed as are the performance characteristics of the prototype. In particular, three principles are examined: system orientation towards the pervasive use of high level language programming and a sophisticated compiler, a primitive instruction set which can be completely hard-wired, storage hierarchy and I/O organization to enable the CPU to execute an instruction at almost every cycle.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {39--47},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801824},
 doi = {http://doi.acm.org/10.1145/960120.801824},
 acmid = {801824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Radin:1982:MIN:800050.801824,
 author = {Radin, George},
 title = {The 801 minicomputer},
 abstract = {This paper provides an overview of an experimental system developed at the IBM T. J. Watson Research Center. It consists of a running hardware prototype, a control program and an optimizing compiler. The basic concepts underlying the system are discussed as are the performance characteristics of the prototype. In particular, three principles are examined: system orientation towards the pervasive use of high level language programming and a sophisticated compiler, a primitive instruction set which can be completely hard-wired, storage hierarchy and I/O organization to enable the CPU to execute an instruction at almost every cycle.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {39--47},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801824},
 doi = {http://doi.acm.org/10.1145/800050.801824},
 acmid = {801824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ditzel:1982:RAF:800050.801825,
 author = {Ditzel, David R. and McLellan, H. R.},
 title = {Register allocation for free: The C machine stack cache},
 abstract = {The Bell Labs C Machine project is investigating computer architectures to support the C programming language.<supscrpt>1</supscrpt> One of the goals is to match an efficient architecture to the language and the compiler technology available. Measurements of different C programs show that roughly one out of every twenty instructions executed is either a procedure call or return.<supscrpt>2</supscrpt> Procedure call overhead is therefore a very important consideration in the overall machine design. A second and related area of primary concern in overall machine efficiency is the register allocation strategy. While use of additional registers can offer considerable improvement in execution times, adding registers usually has the adverse effects of increasing the procedure call overhead due to register saving and creating an undue burden on the compiler. In this paper we describe a piece of the C Machine architecture which effectively eliminates the register allocation problem, and improves procedure calling by drastically reducing storage references required by traditional register saving. The technique can be generalized for other languages and architectures, though we will only directly address those issues involving the C language.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {48--56},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801825},
 doi = {http://doi.acm.org/10.1145/800050.801825},
 acmid = {801825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ditzel:1982:RAF:960120.801825,
 author = {Ditzel, David R. and McLellan, H. R.},
 title = {Register allocation for free: The C machine stack cache},
 abstract = {The Bell Labs C Machine project is investigating computer architectures to support the C programming language.<supscrpt>1</supscrpt> One of the goals is to match an efficient architecture to the language and the compiler technology available. Measurements of different C programs show that roughly one out of every twenty instructions executed is either a procedure call or return.<supscrpt>2</supscrpt> Procedure call overhead is therefore a very important consideration in the overall machine design. A second and related area of primary concern in overall machine efficiency is the register allocation strategy. While use of additional registers can offer considerable improvement in execution times, adding registers usually has the adverse effects of increasing the procedure call overhead due to register saving and creating an undue burden on the compiler. In this paper we describe a piece of the C Machine architecture which effectively eliminates the register allocation problem, and improves procedure calling by drastically reducing storage references required by traditional register saving. The technique can be generalized for other languages and architectures, though we will only directly address those issues involving the C language.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {48--56},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801825},
 doi = {http://doi.acm.org/10.1145/960120.801825},
 acmid = {801825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ditzel:1982:RAF:964750.801825,
 author = {Ditzel, David R. and McLellan, H. R.},
 title = {Register allocation for free: The C machine stack cache},
 abstract = {The Bell Labs C Machine project is investigating computer architectures to support the C programming language.<supscrpt>1</supscrpt> One of the goals is to match an efficient architecture to the language and the compiler technology available. Measurements of different C programs show that roughly one out of every twenty instructions executed is either a procedure call or return.<supscrpt>2</supscrpt> Procedure call overhead is therefore a very important consideration in the overall machine design. A second and related area of primary concern in overall machine efficiency is the register allocation strategy. While use of additional registers can offer considerable improvement in execution times, adding registers usually has the adverse effects of increasing the procedure call overhead due to register saving and creating an undue burden on the compiler. In this paper we describe a piece of the C Machine architecture which effectively eliminates the register allocation problem, and improves procedure calling by drastically reducing storage references required by traditional register saving. The technique can be generalized for other languages and architectures, though we will only directly address those issues involving the C language.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {48--56},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801825},
 doi = {http://doi.acm.org/10.1145/964750.801825},
 acmid = {801825},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harbison:1982:AAO:960120.801826,
 author = {Harbison, Samuel P.},
 title = {An architectural alternative to optimizing compilers},
 abstract = {Programming languages are designed to make programming productive. Computer architectures are designed to make program execution efficient. Although architectures should be designed with programming languages in mind, it may be as inappropriate to make the computer execute the programming language directly it is to make the programmer use machine language. It is the compiler's job to match the programming language and the computer architectures, and therefore making compiler's efficient and easy to write are important design goals of a complete hardware/software system. This paper summerizes research completed in 1980 [5] on a computer architecture, TM, that takes over some of the more burdensome tasks of optimizing compilers for high-level-languages (HLL's), performing these tasks dynamically during the execution of the object program. This is a different approach to making compilers efficient than is commonly taken; more common approaches include devising more efficient optimization algorithms[I], being clever about when to do optimizations [4], and building the compilers semiautomatically [6].},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {57--65},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801826},
 doi = {http://doi.acm.org/10.1145/960120.801826},
 acmid = {801826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harbison:1982:AAO:964750.801826,
 author = {Harbison, Samuel P.},
 title = {An architectural alternative to optimizing compilers},
 abstract = {Programming languages are designed to make programming productive. Computer architectures are designed to make program execution efficient. Although architectures should be designed with programming languages in mind, it may be as inappropriate to make the computer execute the programming language directly it is to make the programmer use machine language. It is the compiler's job to match the programming language and the computer architectures, and therefore making compiler's efficient and easy to write are important design goals of a complete hardware/software system. This paper summerizes research completed in 1980 [5] on a computer architecture, TM, that takes over some of the more burdensome tasks of optimizing compilers for high-level-languages (HLL's), performing these tasks dynamically during the execution of the object program. This is a different approach to making compilers efficient than is commonly taken; more common approaches include devising more efficient optimization algorithms[I], being clever about when to do optimizations [4], and building the compilers semiautomatically [6].},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {57--65},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801826},
 doi = {http://doi.acm.org/10.1145/964750.801826},
 acmid = {801826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harbison:1982:AAO:800050.801826,
 author = {Harbison, Samuel P.},
 title = {An architectural alternative to optimizing compilers},
 abstract = {Programming languages are designed to make programming productive. Computer architectures are designed to make program execution efficient. Although architectures should be designed with programming languages in mind, it may be as inappropriate to make the computer execute the programming language directly it is to make the programmer use machine language. It is the compiler's job to match the programming language and the computer architectures, and therefore making compiler's efficient and easy to write are important design goals of a complete hardware/software system. This paper summerizes research completed in 1980 [5] on a computer architecture, TM, that takes over some of the more burdensome tasks of optimizing compilers for high-level-languages (HLL's), performing these tasks dynamically during the execution of the object program. This is a different approach to making compilers efficient than is commonly taken; more common approaches include devising more efficient optimization algorithms[I], being clever about when to do optimizations [4], and building the compilers semiautomatically [6].},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {57--65},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801826},
 doi = {http://doi.acm.org/10.1145/800050.801826},
 acmid = {801826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lampson:1982:FPC:964750.801827,
 author = {Lampson, Butler W.},
 title = {Fast procedure calls},
 abstract = {A mechanism for control transfers should handle a variety of applications (e.g., procedure calls and returns, coroutine transfers, exceptions, process switches) in a uniform way. It should also allow an implementation in which the common cases of procedure call and return are extremely fast, preferably as fast as unconditional jumps in the normal case. This paper describes such a mechanism and methods for its efficient implementation.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {66--76},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/964750.801827},
 doi = {http://doi.acm.org/10.1145/964750.801827},
 acmid = {801827},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Architecture, Call, Frame, Procedure, Registers, Stack, Transfer},
} 

@inproceedings{Lampson:1982:FPC:800050.801827,
 author = {Lampson, Butler W.},
 title = {Fast procedure calls},
 abstract = {A mechanism for control transfers should handle a variety of applications (e.g., procedure calls and returns, coroutine transfers, exceptions, process switches) in a uniform way. It should also allow an implementation in which the common cases of procedure call and return are extremely fast, preferably as fast as unconditional jumps in the normal case. This paper describes such a mechanism and methods for its efficient implementation.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {66--76},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800050.801827},
 doi = {http://doi.acm.org/10.1145/800050.801827},
 acmid = {801827},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Architecture, Call, Frame, Procedure, Registers, Stack, Transfer},
} 

@article{Lampson:1982:FPC:960120.801827,
 author = {Lampson, Butler W.},
 title = {Fast procedure calls},
 abstract = {A mechanism for control transfers should handle a variety of applications (e.g., procedure calls and returns, coroutine transfers, exceptions, process switches) in a uniform way. It should also allow an implementation in which the common cases of procedure call and return are extremely fast, preferably as fast as unconditional jumps in the normal case. This paper describes such a mechanism and methods for its efficient implementation.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {66--76},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/960120.801827},
 doi = {http://doi.acm.org/10.1145/960120.801827},
 acmid = {801827},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Architecture, Call, Frame, Procedure, Registers, Stack, Transfer},
} 

@article{Jones:1982:SPM:960120.801828,
 author = {Jones, Douglas W.},
 title = {Systematic protection mechanism design},
 abstract = {This work describes an attempt to systematically design a hardware resource protection mechanism when given the requirements of a particular language as a target. The design process is formalized as a structured walk through the multidimensional computer design space towards a hypothetical class of optimal machines. Each step in this walk involves a change in the distribution of work between the compiler and run-time system but no change in the source language semantics. The starting point for this walk is the result of a semantic analysis of the language to be implemented; typically, this produces a very high level machine where the compiler, if any, is trivial. The walk ends when no changes result in a net improvement. This does not guarantee that the result is even locally optimal, since the changes tried depend on the ingenuity and persistence of the designer. This design approach has been used to arrive at a practical, general purpose protection mechanism oriented towards the needs of the Ada language (preliminary version). This architecture was evaluated by comparing it with the PDP-11/45. For the purpose of this comparison, the protection mechanism was incorporated into a partially specified PDP-11 like instruction set. The number of bits making up the processor state and the number of operations involved in address computation were evaluated. On this basis, the result appears to be competitive and worth further investigation.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {77--80},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/960120.801828},
 doi = {http://doi.acm.org/10.1145/960120.801828},
 acmid = {801828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jones:1982:SPM:800050.801828,
 author = {Jones, Douglas W.},
 title = {Systematic protection mechanism design},
 abstract = {This work describes an attempt to systematically design a hardware resource protection mechanism when given the requirements of a particular language as a target. The design process is formalized as a structured walk through the multidimensional computer design space towards a hypothetical class of optimal machines. Each step in this walk involves a change in the distribution of work between the compiler and run-time system but no change in the source language semantics. The starting point for this walk is the result of a semantic analysis of the language to be implemented; typically, this produces a very high level machine where the compiler, if any, is trivial. The walk ends when no changes result in a net improvement. This does not guarantee that the result is even locally optimal, since the changes tried depend on the ingenuity and persistence of the designer. This design approach has been used to arrive at a practical, general purpose protection mechanism oriented towards the needs of the Ada language (preliminary version). This architecture was evaluated by comparing it with the PDP-11/45. For the purpose of this comparison, the protection mechanism was incorporated into a partially specified PDP-11 like instruction set. The number of bits making up the processor state and the number of operations involved in address computation were evaluated. On this basis, the result appears to be competitive and worth further investigation.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {77--80},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800050.801828},
 doi = {http://doi.acm.org/10.1145/800050.801828},
 acmid = {801828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jones:1982:SPM:964750.801828,
 author = {Jones, Douglas W.},
 title = {Systematic protection mechanism design},
 abstract = {This work describes an attempt to systematically design a hardware resource protection mechanism when given the requirements of a particular language as a target. The design process is formalized as a structured walk through the multidimensional computer design space towards a hypothetical class of optimal machines. Each step in this walk involves a change in the distribution of work between the compiler and run-time system but no change in the source language semantics. The starting point for this walk is the result of a semantic analysis of the language to be implemented; typically, this produces a very high level machine where the compiler, if any, is trivial. The walk ends when no changes result in a net improvement. This does not guarantee that the result is even locally optimal, since the changes tried depend on the ingenuity and persistence of the designer. This design approach has been used to arrive at a practical, general purpose protection mechanism oriented towards the needs of the Ada language (preliminary version). This architecture was evaluated by comparing it with the PDP-11/45. For the purpose of this comparison, the protection mechanism was incorporated into a partially specified PDP-11 like instruction set. The number of bits making up the processor state and the number of operations involved in address computation were evaluated. On this basis, the result appears to be competitive and worth further investigation.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {77--80},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/964750.801828},
 doi = {http://doi.acm.org/10.1145/964750.801828},
 acmid = {801828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reed:1982:GPM:960120.801829,
 author = {Reed, Karl},
 title = {On a general property of memory mapping tables},
 abstract = {The paper shows that memory mapping tables can be used to implement the display registers used in providing architectural support for block-structured languages such as Algol 60. This allows full lexical level addressing to be implemented on so-called von-Neuman machines. The problems of fragmentation of the paged address space are explored, and machines with memory mapping schemes capable of supporting the proposals identified. Attention is drawn to the similarity between segmented and paged schemes, and it is suggested that the latter may be used to support the former.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {81--86},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/960120.801829},
 doi = {http://doi.acm.org/10.1145/960120.801829},
 acmid = {801829},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Display, Memory mapping, Page tables, Segmentation, Virtual memory},
} 

@article{Reed:1982:GPM:964750.801829,
 author = {Reed, Karl},
 title = {On a general property of memory mapping tables},
 abstract = {The paper shows that memory mapping tables can be used to implement the display registers used in providing architectural support for block-structured languages such as Algol 60. This allows full lexical level addressing to be implemented on so-called von-Neuman machines. The problems of fragmentation of the paged address space are explored, and machines with memory mapping schemes capable of supporting the proposals identified. Attention is drawn to the similarity between segmented and paged schemes, and it is suggested that the latter may be used to support the former.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {81--86},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/964750.801829},
 doi = {http://doi.acm.org/10.1145/964750.801829},
 acmid = {801829},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Display, Memory mapping, Page tables, Segmentation, Virtual memory},
} 

@inproceedings{Reed:1982:GPM:800050.801829,
 author = {Reed, Karl},
 title = {On a general property of memory mapping tables},
 abstract = {The paper shows that memory mapping tables can be used to implement the display registers used in providing architectural support for block-structured languages such as Algol 60. This allows full lexical level addressing to be implemented on so-called von-Neuman machines. The problems of fragmentation of the paged address space are explored, and machines with memory mapping schemes capable of supporting the proposals identified. Attention is drawn to the similarity between segmented and paged schemes, and it is suggested that the latter may be used to support the former.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {81--86},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800050.801829},
 doi = {http://doi.acm.org/10.1145/800050.801829},
 acmid = {801829},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Display, Memory mapping, Page tables, Segmentation, Virtual memory},
} 

@article{Cook:1982:EIO:964750.801830,
 author = {Cook, Robert P. and Donde, Nitin},
 title = {An experiment to improve operand addressing},
 abstract = {MCODE is a high-level language, stack machine designed to support strongly-typed, Pascal-based languages with a variety of data types in a modular programming environment. The instruction set, constructed for efficiency and extensibility, is based on an analysis of 120,000 lines of Pascal programs. The design was compared for efficiency with the instruction sets of the Digital Equipment PDP-11 and VAX by examining the generated code from the same compiler for all three machines. In addition, the original design choices were tested by analyzing the generated code from 19,000 lines of StarMod programs. As a result of this iterative experiment, we have summarized our observations in an efficient reorganization of the VAX's addressing modes.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {87--91},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/964750.801830},
 doi = {http://doi.acm.org/10.1145/964750.801830},
 acmid = {801830},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Addressing modes, Computer architecture, Stack machine},
} 

@inproceedings{Cook:1982:EIO:800050.801830,
 author = {Cook, Robert P. and Donde, Nitin},
 title = {An experiment to improve operand addressing},
 abstract = {MCODE is a high-level language, stack machine designed to support strongly-typed, Pascal-based languages with a variety of data types in a modular programming environment. The instruction set, constructed for efficiency and extensibility, is based on an analysis of 120,000 lines of Pascal programs. The design was compared for efficiency with the instruction sets of the Digital Equipment PDP-11 and VAX by examining the generated code from the same compiler for all three machines. In addition, the original design choices were tested by analyzing the generated code from 19,000 lines of StarMod programs. As a result of this iterative experiment, we have summarized our observations in an efficient reorganization of the VAX's addressing modes.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {87--91},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/800050.801830},
 doi = {http://doi.acm.org/10.1145/800050.801830},
 acmid = {801830},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Addressing modes, Computer architecture, Stack machine},
} 

@article{Cook:1982:EIO:960120.801830,
 author = {Cook, Robert P. and Donde, Nitin},
 title = {An experiment to improve operand addressing},
 abstract = {MCODE is a high-level language, stack machine designed to support strongly-typed, Pascal-based languages with a variety of data types in a modular programming environment. The instruction set, constructed for efficiency and extensibility, is based on an analysis of 120,000 lines of Pascal programs. The design was compared for efficiency with the instruction sets of the Digital Equipment PDP-11 and VAX by examining the generated code from the same compiler for all three machines. In addition, the original design choices were tested by analyzing the generated code from 19,000 lines of StarMod programs. As a result of this iterative experiment, we have summarized our observations in an efficient reorganization of the VAX's addressing modes.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {87--91},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/960120.801830},
 doi = {http://doi.acm.org/10.1145/960120.801830},
 acmid = {801830},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Addressing modes, Computer architecture, Stack machine},
} 

@article{Fusaoka:1982:CCH:964750.801831,
 author = {Fusaoka, Akira and Hirayama, Masaharu},
 title = {Compiler chip: A hardware implementation of compiler},
 abstract = {In this paper we discuss about another approch: <underline>Compiler Chip</underline>, which is a VLSI implementation of a compiler. Constructing a compiler by a few VLSI chip, the computer manufacturer can deliver compilers by sets of VLSI chips, and these chips are installed in a intelligent terminal in order to remove the compilation from the tasks which are processed in the mainframe.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {92--95},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/964750.801831},
 doi = {http://doi.acm.org/10.1145/964750.801831},
 acmid = {801831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fusaoka:1982:CCH:960120.801831,
 author = {Fusaoka, Akira and Hirayama, Masaharu},
 title = {Compiler chip: A hardware implementation of compiler},
 abstract = {In this paper we discuss about another approch: <underline>Compiler Chip</underline>, which is a VLSI implementation of a compiler. Constructing a compiler by a few VLSI chip, the computer manufacturer can deliver compilers by sets of VLSI chips, and these chips are installed in a intelligent terminal in order to remove the compilation from the tasks which are processed in the mainframe.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {92--95},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/960120.801831},
 doi = {http://doi.acm.org/10.1145/960120.801831},
 acmid = {801831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fusaoka:1982:CCH:800050.801831,
 author = {Fusaoka, Akira and Hirayama, Masaharu},
 title = {Compiler chip: A hardware implementation of compiler},
 abstract = {In this paper we discuss about another approch: <underline>Compiler Chip</underline>, which is a VLSI implementation of a compiler. Constructing a compiler by a few VLSI chip, the computer manufacturer can deliver compilers by sets of VLSI chips, and these chips are installed in a intelligent terminal in order to remove the compilation from the tasks which are processed in the mainframe.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {92--95},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800050.801831},
 doi = {http://doi.acm.org/10.1145/800050.801831},
 acmid = {801831},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rau:1982:ASE:964750.801832,
 author = {Rau, B. R. and Glaeser, C. D. and Greenawalt, E. M.},
 title = {Architectural support for the efficient generation of code for horizontal architectures},
 abstract = {Horizontal architectures, such as the CDC Advanced Flexible Processor [I] and the FPS APi20-B [2, consist of a number of resources that can operate in parallel, each of which is controlled by a field in the wide instruction word. Such architectures have been developed to perform high speed scientific computations at a modest cost: Figure 1 displays those characteristics of horizontal architectures that are germane to the issues discussed in this paper. The simultaneous requirements of high performance and low cost lead to an architecture consisting of multiple pipelined processing elements (PEs) such as adders and multipliers, a memory (which for scheduling purposes may be viewed as yet another PE with two operations: a READ and a WRITE), and an interconnect which ties them all together. The interconnect allows the result of one operation to be directly routed to another PE as one of the inputs for an operation that is to be performed there. The required memory bandwidth is reduced since temporary values need not be written to and read from the memory. The final aspect of horizontal processors that is of interest is that their program memories emit wide instructions which synchronously specify the actions of the multiple and possibly dissimilar PEs. The program memory is sequenced by a conventional sequencer that assumes sequential flow of control unless a branch is explicitly specified. As a consequence of the simplicity of such an architecture, it is inexpensive relative to the potential performance of the multiple pipelined PEs. However, if this potential performance is to be realized, the multiple resources of a horizontal processor must be scheduled effectively. The scheduling task for conventional horizontal processors is quite complex and the construction of highly optimizing compilers for them is a difficult and expensiw3 project. The polycyclic architecture [3- 6] is a horizontal architecture with architectural support for the scheduling task. The cause of the complexity involved in scheduling conventional horizontal processors and the manner in which the polycyclic architecture addresses this issue are outlined in this paper.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {96--99},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/964750.801832},
 doi = {http://doi.acm.org/10.1145/964750.801832},
 acmid = {801832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rau:1982:ASE:960120.801832,
 author = {Rau, B. R. and Glaeser, C. D. and Greenawalt, E. M.},
 title = {Architectural support for the efficient generation of code for horizontal architectures},
 abstract = {Horizontal architectures, such as the CDC Advanced Flexible Processor [I] and the FPS APi20-B [2, consist of a number of resources that can operate in parallel, each of which is controlled by a field in the wide instruction word. Such architectures have been developed to perform high speed scientific computations at a modest cost: Figure 1 displays those characteristics of horizontal architectures that are germane to the issues discussed in this paper. The simultaneous requirements of high performance and low cost lead to an architecture consisting of multiple pipelined processing elements (PEs) such as adders and multipliers, a memory (which for scheduling purposes may be viewed as yet another PE with two operations: a READ and a WRITE), and an interconnect which ties them all together. The interconnect allows the result of one operation to be directly routed to another PE as one of the inputs for an operation that is to be performed there. The required memory bandwidth is reduced since temporary values need not be written to and read from the memory. The final aspect of horizontal processors that is of interest is that their program memories emit wide instructions which synchronously specify the actions of the multiple and possibly dissimilar PEs. The program memory is sequenced by a conventional sequencer that assumes sequential flow of control unless a branch is explicitly specified. As a consequence of the simplicity of such an architecture, it is inexpensive relative to the potential performance of the multiple pipelined PEs. However, if this potential performance is to be realized, the multiple resources of a horizontal processor must be scheduled effectively. The scheduling task for conventional horizontal processors is quite complex and the construction of highly optimizing compilers for them is a difficult and expensiw3 project. The polycyclic architecture [3- 6] is a horizontal architecture with architectural support for the scheduling task. The cause of the complexity involved in scheduling conventional horizontal processors and the manner in which the polycyclic architecture addresses this issue are outlined in this paper.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {96--99},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/960120.801832},
 doi = {http://doi.acm.org/10.1145/960120.801832},
 acmid = {801832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rau:1982:ASE:800050.801832,
 author = {Rau, B. R. and Glaeser, C. D. and Greenawalt, E. M.},
 title = {Architectural support for the efficient generation of code for horizontal architectures},
 abstract = {Horizontal architectures, such as the CDC Advanced Flexible Processor [I] and the FPS APi20-B [2, consist of a number of resources that can operate in parallel, each of which is controlled by a field in the wide instruction word. Such architectures have been developed to perform high speed scientific computations at a modest cost: Figure 1 displays those characteristics of horizontal architectures that are germane to the issues discussed in this paper. The simultaneous requirements of high performance and low cost lead to an architecture consisting of multiple pipelined processing elements (PEs) such as adders and multipliers, a memory (which for scheduling purposes may be viewed as yet another PE with two operations: a READ and a WRITE), and an interconnect which ties them all together. The interconnect allows the result of one operation to be directly routed to another PE as one of the inputs for an operation that is to be performed there. The required memory bandwidth is reduced since temporary values need not be written to and read from the memory. The final aspect of horizontal processors that is of interest is that their program memories emit wide instructions which synchronously specify the actions of the multiple and possibly dissimilar PEs. The program memory is sequenced by a conventional sequencer that assumes sequential flow of control unless a branch is explicitly specified. As a consequence of the simplicity of such an architecture, it is inexpensive relative to the potential performance of the multiple pipelined PEs. However, if this potential performance is to be realized, the multiple resources of a horizontal processor must be scheduled effectively. The scheduling task for conventional horizontal processors is quite complex and the construction of highly optimizing compilers for them is a difficult and expensiw3 project. The polycyclic architecture [3- 6] is a horizontal architecture with architectural support for the scheduling task. The cause of the complexity involved in scheduling conventional horizontal processors and the manner in which the polycyclic architecture addresses this issue are outlined in this paper.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {96--99},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800050.801832},
 doi = {http://doi.acm.org/10.1145/800050.801832},
 acmid = {801832},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McLear:1982:GCD:960120.801833,
 author = {McLear, R. E. and Scheibelhut, D. M. and Tammaru, E.},
 title = {Guidelines for creating a debuggable processor},
 abstract = {Hardware without software is of little use. Systems that ease the task of debugging software reduce cost and speed development. This paper presents guidelines for designing processors that ease debugging for real-time computer systems. Special hardware can aid the debugging process by tracing execution and accesses to memory. Such hardware requires access to signals that may not be readily available. Other, less exotic hardware provides an interface to the programmer and other processors. The hardware and software of the debugging system should not alter the real-time characteristics of the system under test and should be able to operate on a field-grade processor. It is undesirable to require special versions of processor hardware and software for the debugging system.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {100--106},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/960120.801833},
 doi = {http://doi.acm.org/10.1145/960120.801833},
 acmid = {801833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McLear:1982:GCD:800050.801833,
 author = {McLear, R. E. and Scheibelhut, D. M. and Tammaru, E.},
 title = {Guidelines for creating a debuggable processor},
 abstract = {Hardware without software is of little use. Systems that ease the task of debugging software reduce cost and speed development. This paper presents guidelines for designing processors that ease debugging for real-time computer systems. Special hardware can aid the debugging process by tracing execution and accesses to memory. Such hardware requires access to signals that may not be readily available. Other, less exotic hardware provides an interface to the programmer and other processors. The hardware and software of the debugging system should not alter the real-time characteristics of the system under test and should be able to operate on a field-grade processor. It is undesirable to require special versions of processor hardware and software for the debugging system.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {100--106},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800050.801833},
 doi = {http://doi.acm.org/10.1145/800050.801833},
 acmid = {801833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McLear:1982:GCD:964750.801833,
 author = {McLear, R. E. and Scheibelhut, D. M. and Tammaru, E.},
 title = {Guidelines for creating a debuggable processor},
 abstract = {Hardware without software is of little use. Systems that ease the task of debugging software reduce cost and speed development. This paper presents guidelines for designing processors that ease debugging for real-time computer systems. Special hardware can aid the debugging process by tracing execution and accesses to memory. Such hardware requires access to signals that may not be readily available. Other, less exotic hardware provides an interface to the programmer and other processors. The hardware and software of the debugging system should not alter the real-time characteristics of the system under test and should be able to operate on a field-grade processor. It is undesirable to require special versions of processor hardware and software for the debugging system.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {100--106},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/964750.801833},
 doi = {http://doi.acm.org/10.1145/964750.801833},
 acmid = {801833},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wilkes:1982:HSM:800050.801834,
 author = {Wilkes, M. V.},
 title = {Hardware support for memory protection: Capability implementations},
 abstract = {This paper is intended to stimulate discussion on the present state of hardware supported capability systems. Interest in such systems grew up in the mid-1960's and since that time information has been published on several different versions. In the opinion of some observers, the software complexity of these systems outweighs the advantage gained. The paper surveys the situation, and endeavors to set out the general features that a hardware supported capability system should possess. An attempt is made to identify the causes of the complexity and to make recommendations for removing them. The arguments for and against the tagging of capabilities are discussed and attention is drawn to a system of semi-tagging previously proposed by the author.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800050.801834},
 doi = {http://doi.acm.org/10.1145/800050.801834},
 acmid = {801834},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wilkes:1982:HSM:960120.801834,
 author = {Wilkes, M. V.},
 title = {Hardware support for memory protection: Capability implementations},
 abstract = {This paper is intended to stimulate discussion on the present state of hardware supported capability systems. Interest in such systems grew up in the mid-1960's and since that time information has been published on several different versions. In the opinion of some observers, the software complexity of these systems outweighs the advantage gained. The paper surveys the situation, and endeavors to set out the general features that a hardware supported capability system should possess. An attempt is made to identify the causes of the complexity and to make recommendations for removing them. The arguments for and against the tagging of capabilities are discussed and attention is drawn to a system of semi-tagging previously proposed by the author.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960120.801834},
 doi = {http://doi.acm.org/10.1145/960120.801834},
 acmid = {801834},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wilkes:1982:HSM:964750.801834,
 author = {Wilkes, M. V.},
 title = {Hardware support for memory protection: Capability implementations},
 abstract = {This paper is intended to stimulate discussion on the present state of hardware supported capability systems. Interest in such systems grew up in the mid-1960's and since that time information has been published on several different versions. In the opinion of some observers, the software complexity of these systems outweighs the advantage gained. The paper surveys the situation, and endeavors to set out the general features that a hardware supported capability system should possess. An attempt is made to identify the causes of the complexity and to make recommendations for removing them. The arguments for and against the tagging of capabilities are discussed and attention is drawn to a system of semi-tagging previously proposed by the author.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/964750.801834},
 doi = {http://doi.acm.org/10.1145/964750.801834},
 acmid = {801834},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pollack:1982:SAM:964750.801835,
 author = {Pollack, Fred J. and Cox, George W. and Hammerstrom, Dan W. and Kahn, Kevin C. and Lai, Konrad K. and Rattner, Justin R.},
 title = {Supporting ada memory management in the iAPX-432},
 abstract = {In this paper, we describe how the memory management mechanisms of the Intel iAPX-432 are used to implement the visibility rules of Ada. At any point in the execution of an Ada\&reg; program on the 432, the program has a protected address space that corresponds exactly to the program's accessibility at the corresponding point in the program's source. This close match of architecture and language did not occur because the 432 was designed to execute Ada\&mdash;it was not. Rather, both Ada and the 432 are the result of very similar design goals. To illustrate this point, we compare, in their support for Ada, the memory management mechanisms of the 432 to those of traditional computers. The most notable differences occur in heap-space management and multitasking. With respect to the former, we describe a degree of hardware/software cooperation that is not typical of other systems. In the latter area, we show how Ada's view of sharing is the same as the 432, but differs totally from the sharing permitted by traditional systems. A description of these differences provide some insight into the problems of implementing an Ada compiler for a traditional architecture.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {117--131},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/964750.801835},
 doi = {http://doi.acm.org/10.1145/964750.801835},
 acmid = {801835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pollack:1982:SAM:800050.801835,
 author = {Pollack, Fred J. and Cox, George W. and Hammerstrom, Dan W. and Kahn, Kevin C. and Lai, Konrad K. and Rattner, Justin R.},
 title = {Supporting ada memory management in the iAPX-432},
 abstract = {In this paper, we describe how the memory management mechanisms of the Intel iAPX-432 are used to implement the visibility rules of Ada. At any point in the execution of an Ada\&reg; program on the 432, the program has a protected address space that corresponds exactly to the program's accessibility at the corresponding point in the program's source. This close match of architecture and language did not occur because the 432 was designed to execute Ada\&mdash;it was not. Rather, both Ada and the 432 are the result of very similar design goals. To illustrate this point, we compare, in their support for Ada, the memory management mechanisms of the 432 to those of traditional computers. The most notable differences occur in heap-space management and multitasking. With respect to the former, we describe a degree of hardware/software cooperation that is not typical of other systems. In the latter area, we show how Ada's view of sharing is the same as the 432, but differs totally from the sharing permitted by traditional systems. A description of these differences provide some insight into the problems of implementing an Ada compiler for a traditional architecture.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {117--131},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800050.801835},
 doi = {http://doi.acm.org/10.1145/800050.801835},
 acmid = {801835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pollack:1982:SAM:960120.801835,
 author = {Pollack, Fred J. and Cox, George W. and Hammerstrom, Dan W. and Kahn, Kevin C. and Lai, Konrad K. and Rattner, Justin R.},
 title = {Supporting ada memory management in the iAPX-432},
 abstract = {In this paper, we describe how the memory management mechanisms of the Intel iAPX-432 are used to implement the visibility rules of Ada. At any point in the execution of an Ada\&reg; program on the 432, the program has a protected address space that corresponds exactly to the program's accessibility at the corresponding point in the program's source. This close match of architecture and language did not occur because the 432 was designed to execute Ada\&mdash;it was not. Rather, both Ada and the 432 are the result of very similar design goals. To illustrate this point, we compare, in their support for Ada, the memory management mechanisms of the 432 to those of traditional computers. The most notable differences occur in heap-space management and multitasking. With respect to the former, we describe a degree of hardware/software cooperation that is not typical of other systems. In the latter area, we show how Ada's view of sharing is the same as the 432, but differs totally from the sharing permitted by traditional systems. A description of these differences provide some insight into the problems of implementing an Ada compiler for a traditional architecture.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {117--131},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/960120.801835},
 doi = {http://doi.acm.org/10.1145/960120.801835},
 acmid = {801835},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sansonnet:1982:DEL:960120.801836,
 author = {Sansonnet, J. P. and Castan, M. and Percebois, C. and Botella, D. and Perez, J.},
 title = {Direct execution of lisp on a list_directed architecture},
 abstract = { We have defined a direct-execution model dedicated to non-numerical processing which is based upon an internal representation of source programs derived from LISP. This model provides good support for both sophisticated editing (syntactical parsing, tree manipulation, pretty-printing, ...) of conventional languages and artificial intelligence languages. A high level microprogramming language (LEM) was designed to write the interpreters and the editors. A hardware processor was built and a LISP interpreter, microprogrammed in LEM, has been operational since September 1980.   First, the influence of LISP on the LEM language and the architecture is discussed. At the LEM level, we will see that LISP has prompted the control constructs and the access functions to the tree-structured internal form. As for the architecture, we present the hardware implementation of a special garbage collector based upon reference counters.   In turn, the machine has influenced the implementation of LISP. We present here the structure of our LISP interpreter and we give evaluation measures dealing with size, development effort, speed; they prove that programming in LEM is easy, short to debug and very concise. Moreover, the speed of our LISP interpreter confirms that the architecture is very efficient for symbolic processing. },
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {132--139},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/960120.801836},
 doi = {http://doi.acm.org/10.1145/960120.801836},
 acmid = {801836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sansonnet:1982:DEL:800050.801836,
 author = {Sansonnet, J. P. and Castan, M. and Percebois, C. and Botella, D. and Perez, J.},
 title = {Direct execution of lisp on a list_directed architecture},
 abstract = { We have defined a direct-execution model dedicated to non-numerical processing which is based upon an internal representation of source programs derived from LISP. This model provides good support for both sophisticated editing (syntactical parsing, tree manipulation, pretty-printing, ...) of conventional languages and artificial intelligence languages. A high level microprogramming language (LEM) was designed to write the interpreters and the editors. A hardware processor was built and a LISP interpreter, microprogrammed in LEM, has been operational since September 1980.   First, the influence of LISP on the LEM language and the architecture is discussed. At the LEM level, we will see that LISP has prompted the control constructs and the access functions to the tree-structured internal form. As for the architecture, we present the hardware implementation of a special garbage collector based upon reference counters.   In turn, the machine has influenced the implementation of LISP. We present here the structure of our LISP interpreter and we give evaluation measures dealing with size, development effort, speed; they prove that programming in LEM is easy, short to debug and very concise. Moreover, the speed of our LISP interpreter confirms that the architecture is very efficient for symbolic processing. },
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {132--139},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800050.801836},
 doi = {http://doi.acm.org/10.1145/800050.801836},
 acmid = {801836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sansonnet:1982:DEL:964750.801836,
 author = {Sansonnet, J. P. and Castan, M. and Percebois, C. and Botella, D. and Perez, J.},
 title = {Direct execution of lisp on a list_directed architecture},
 abstract = { We have defined a direct-execution model dedicated to non-numerical processing which is based upon an internal representation of source programs derived from LISP. This model provides good support for both sophisticated editing (syntactical parsing, tree manipulation, pretty-printing, ...) of conventional languages and artificial intelligence languages. A high level microprogramming language (LEM) was designed to write the interpreters and the editors. A hardware processor was built and a LISP interpreter, microprogrammed in LEM, has been operational since September 1980.   First, the influence of LISP on the LEM language and the architecture is discussed. At the LEM level, we will see that LISP has prompted the control constructs and the access functions to the tree-structured internal form. As for the architecture, we present the hardware implementation of a special garbage collector based upon reference counters.   In turn, the machine has influenced the implementation of LISP. We present here the structure of our LISP interpreter and we give evaluation measures dealing with size, development effort, speed; they prove that programming in LEM is easy, short to debug and very concise. Moreover, the speed of our LISP interpreter confirms that the architecture is very efficient for symbolic processing. },
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {132--139},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/964750.801836},
 doi = {http://doi.acm.org/10.1145/964750.801836},
 acmid = {801836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1982:RAS:964750.801837,
 author = {Johnson, Mark Scott},
 title = {Some requirements for architectural support of software debugging},
 abstract = {Architectural support of high-level, symbolic debugging is described at three levels of abstraction: the user's view of desired debugging functionality, the debugger implementor's view of architectural requirements that support the functionality, and the computer architect's view of architectural features or attributes that implement the requirements. References are made where possible to computing systems that meet the requirements. The paper is written from the viewpoint of debugger implementors, and is addressed primarily to computer architects.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {140--148},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801837},
 doi = {http://doi.acm.org/10.1145/964750.801837},
 acmid = {801837},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Architectural debugging support, Breakpoints, Debugging, Debugging-oriented architecture, Interactive debugging, Profiles, Symbolic debugging, Traces},
} 

@article{Johnson:1982:RAS:960120.801837,
 author = {Johnson, Mark Scott},
 title = {Some requirements for architectural support of software debugging},
 abstract = {Architectural support of high-level, symbolic debugging is described at three levels of abstraction: the user's view of desired debugging functionality, the debugger implementor's view of architectural requirements that support the functionality, and the computer architect's view of architectural features or attributes that implement the requirements. References are made where possible to computing systems that meet the requirements. The paper is written from the viewpoint of debugger implementors, and is addressed primarily to computer architects.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {140--148},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801837},
 doi = {http://doi.acm.org/10.1145/960120.801837},
 acmid = {801837},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Architectural debugging support, Breakpoints, Debugging, Debugging-oriented architecture, Interactive debugging, Profiles, Symbolic debugging, Traces},
} 

@inproceedings{Johnson:1982:RAS:800050.801837,
 author = {Johnson, Mark Scott},
 title = {Some requirements for architectural support of software debugging},
 abstract = {Architectural support of high-level, symbolic debugging is described at three levels of abstraction: the user's view of desired debugging functionality, the debugger implementor's view of architectural requirements that support the functionality, and the computer architect's view of architectural features or attributes that implement the requirements. References are made where possible to computing systems that meet the requirements. The paper is written from the viewpoint of debugger implementors, and is addressed primarily to computer architects.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {140--148},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801837},
 doi = {http://doi.acm.org/10.1145/800050.801837},
 acmid = {801837},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Architectural debugging support, Breakpoints, Debugging, Debugging-oriented architecture, Interactive debugging, Profiles, Symbolic debugging, Traces},
} 

@article{Middelburg:1982:EPA:960120.801838,
 author = {Middelburg, C. A.},
 title = {The effect of the PDP-11 architecture on code generation for chill},
 abstract = {This paper outlines the implementation of the CCITT*) high level programming language CHILL on PDP-11 computers in the CHILL compiler constructed at the Dr. Neher Laboratories. The characteristics and structure of the compiler are briefly described. The relationship between the PDP-11 architecture and the implementation of CHILL is outlined in more detail.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {149--157},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801838},
 doi = {http://doi.acm.org/10.1145/960120.801838},
 acmid = {801838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Middelburg:1982:EPA:964750.801838,
 author = {Middelburg, C. A.},
 title = {The effect of the PDP-11 architecture on code generation for chill},
 abstract = {This paper outlines the implementation of the CCITT*) high level programming language CHILL on PDP-11 computers in the CHILL compiler constructed at the Dr. Neher Laboratories. The characteristics and structure of the compiler are briefly described. The relationship between the PDP-11 architecture and the implementation of CHILL is outlined in more detail.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {149--157},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801838},
 doi = {http://doi.acm.org/10.1145/964750.801838},
 acmid = {801838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Middelburg:1982:EPA:800050.801838,
 author = {Middelburg, C. A.},
 title = {The effect of the PDP-11 architecture on code generation for chill},
 abstract = {This paper outlines the implementation of the CCITT*) high level programming language CHILL on PDP-11 computers in the CHILL compiler constructed at the Dr. Neher Laboratories. The characteristics and structure of the compiler are briefly described. The relationship between the PDP-11 architecture and the implementation of CHILL is outlined in more detail.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {149--157},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801838},
 doi = {http://doi.acm.org/10.1145/800050.801838},
 acmid = {801838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sweet:1982:EAM:960120.801839,
 author = {Sweet, Richard E. and Sandman,Jr., James G.},
 title = {Empirical analysis of the mesa instruction set},
 abstract = {This paper describes recent work to refine the instruction set of the Mesa processor. Mesa [8] is a high level systems implementation language developed at Xerox PARC during the middle 1970's. Typical systems written in Mesa are large collections of programs running on single-user machines. For this reason, a major design goal of the project has been to generate compact object programs. The computers that execute Mesa programs are implementations of a stack architecture [5]. The instructions of an object program are organized into a stream of eight bit bytes. The exact complement into of instructions in the architecture has changed as the language and machine micro architecture have evolved. In Sections 3 and 4, we give a short history of the Mesa instruction set and discuss the motivation for our most recent analysis of it. In Section 5, we discuss the tools and techniques used in this analysis. Section 6 shows the results of this analysis as applied to a large sample of approximately 2.5 million instruction bytes. Sections 7 and 8 give advice to others who might be contemplating similar analyses.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {158--166},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/960120.801839},
 doi = {http://doi.acm.org/10.1145/960120.801839},
 acmid = {801839},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sweet:1982:EAM:964750.801839,
 author = {Sweet, Richard E. and Sandman,Jr., James G.},
 title = {Empirical analysis of the mesa instruction set},
 abstract = {This paper describes recent work to refine the instruction set of the Mesa processor. Mesa [8] is a high level systems implementation language developed at Xerox PARC during the middle 1970's. Typical systems written in Mesa are large collections of programs running on single-user machines. For this reason, a major design goal of the project has been to generate compact object programs. The computers that execute Mesa programs are implementations of a stack architecture [5]. The instructions of an object program are organized into a stream of eight bit bytes. The exact complement into of instructions in the architecture has changed as the language and machine micro architecture have evolved. In Sections 3 and 4, we give a short history of the Mesa instruction set and discuss the motivation for our most recent analysis of it. In Section 5, we discuss the tools and techniques used in this analysis. Section 6 shows the results of this analysis as applied to a large sample of approximately 2.5 million instruction bytes. Sections 7 and 8 give advice to others who might be contemplating similar analyses.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {158--166},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/964750.801839},
 doi = {http://doi.acm.org/10.1145/964750.801839},
 acmid = {801839},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sweet:1982:EAM:800050.801839,
 author = {Sweet, Richard E. and Sandman,Jr., James G.},
 title = {Empirical analysis of the mesa instruction set},
 abstract = {This paper describes recent work to refine the instruction set of the Mesa processor. Mesa [8] is a high level systems implementation language developed at Xerox PARC during the middle 1970's. Typical systems written in Mesa are large collections of programs running on single-user machines. For this reason, a major design goal of the project has been to generate compact object programs. The computers that execute Mesa programs are implementations of a stack architecture [5]. The instructions of an object program are organized into a stream of eight bit bytes. The exact complement into of instructions in the architecture has changed as the language and machine micro architecture have evolved. In Sections 3 and 4, we give a short history of the Mesa instruction set and discuss the motivation for our most recent analysis of it. In Section 5, we discuss the tools and techniques used in this analysis. Section 6 shows the results of this analysis as applied to a large sample of approximately 2.5 million instruction bytes. Sections 7 and 8 give advice to others who might be contemplating similar analyses.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {158--166},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800050.801839},
 doi = {http://doi.acm.org/10.1145/800050.801839},
 acmid = {801839},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McDaniel:1982:AMI:960120.801840,
 author = {McDaniel, Gene},
 title = {An analysis of a mesa instruction set using dynamic instruction frequencies},
 abstract = {The Mesa architecture is implemented on a variety of processors, and dynamic instruction frequency data for two programs is used to analyze the architecture in an implementation independent fashion. The Mesa compiler allocates variables in an order based upon their static frequency of use, and measurements are provided that show that these static predictions predict run time usage as well. We provide an evaluation of the advantages and costs of Mesa's compact byte encoding, its reliance upon an evaluation stack, and its use of memory. The Mesa language has evolved over time in a hardware environment oriented around 16-bit quantities with growing use of and accommodations to 32-bit quantities. The cost of emulating 32-bit data paths on a 16-bit machine is identified for a program that heavily exploits longer values. Several potential areas for improving the execution speed of a Mesa processor with special purpose hardware are identified.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960120.801840},
 doi = {http://doi.acm.org/10.1145/960120.801840},
 acmid = {801840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McDaniel:1982:AMI:800050.801840,
 author = {McDaniel, Gene},
 title = {An analysis of a mesa instruction set using dynamic instruction frequencies},
 abstract = {The Mesa architecture is implemented on a variety of processors, and dynamic instruction frequency data for two programs is used to analyze the architecture in an implementation independent fashion. The Mesa compiler allocates variables in an order based upon their static frequency of use, and measurements are provided that show that these static predictions predict run time usage as well. We provide an evaluation of the advantages and costs of Mesa's compact byte encoding, its reliance upon an evaluation stack, and its use of memory. The Mesa language has evolved over time in a hardware environment oriented around 16-bit quantities with growing use of and accommodations to 32-bit quantities. The cost of emulating 32-bit data paths on a 16-bit machine is identified for a program that heavily exploits longer values. Several potential areas for improving the execution speed of a Mesa processor with special purpose hardware are identified.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800050.801840},
 doi = {http://doi.acm.org/10.1145/800050.801840},
 acmid = {801840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McDaniel:1982:AMI:964750.801840,
 author = {McDaniel, Gene},
 title = {An analysis of a mesa instruction set using dynamic instruction frequencies},
 abstract = {The Mesa architecture is implemented on a variety of processors, and dynamic instruction frequency data for two programs is used to analyze the architecture in an implementation independent fashion. The Mesa compiler allocates variables in an order based upon their static frequency of use, and measurements are provided that show that these static predictions predict run time usage as well. We provide an evaluation of the advantages and costs of Mesa's compact byte encoding, its reliance upon an evaluation stack, and its use of memory. The Mesa language has evolved over time in a hardware environment oriented around 16-bit quantities with growing use of and accommodations to 32-bit quantities. The cost of emulating 32-bit data paths on a 16-bit machine is identified for a program that heavily exploits longer values. Several potential areas for improving the execution speed of a Mesa processor with special purpose hardware are identified.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/964750.801840},
 doi = {http://doi.acm.org/10.1145/964750.801840},
 acmid = {801840},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wiecek:1982:CSV:960120.801841,
 author = {Wiecek, Cheryl A.},
 title = {A case study of VAX-11 instruction set usage for compiler execution},
 abstract = {Analysis of an instruction set as large and varied as the one specified for the VAX-11 architecture is important for aiding processor design evaluation. This paper looks at dynamic VAX-11 instruction set usage by one class of programs, and discusses the methodology and tools which have been developed to provide that information. Six VAX/VMS native mode compilers from Digital Equipment Corporation were used: BASIC, BLISS, COBOL, FORTRAN, PASCAL, and PL/I. A summary of results generated by analyzing executions of these six compilers is presented. Information is included for instruction and class frequency, general instruction features, operand specifiers, the memory data stream, register utilization, instruction sequencing, and branch displacements.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {177--184},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/960120.801841},
 doi = {http://doi.acm.org/10.1145/960120.801841},
 acmid = {801841},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wiecek:1982:CSV:964750.801841,
 author = {Wiecek, Cheryl A.},
 title = {A case study of VAX-11 instruction set usage for compiler execution},
 abstract = {Analysis of an instruction set as large and varied as the one specified for the VAX-11 architecture is important for aiding processor design evaluation. This paper looks at dynamic VAX-11 instruction set usage by one class of programs, and discusses the methodology and tools which have been developed to provide that information. Six VAX/VMS native mode compilers from Digital Equipment Corporation were used: BASIC, BLISS, COBOL, FORTRAN, PASCAL, and PL/I. A summary of results generated by analyzing executions of these six compilers is presented. Information is included for instruction and class frequency, general instruction features, operand specifiers, the memory data stream, register utilization, instruction sequencing, and branch displacements.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {177--184},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/964750.801841},
 doi = {http://doi.acm.org/10.1145/964750.801841},
 acmid = {801841},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wiecek:1982:CSV:800050.801841,
 author = {Wiecek, Cheryl A.},
 title = {A case study of VAX-11 instruction set usage for compiler execution},
 abstract = {Analysis of an instruction set as large and varied as the one specified for the VAX-11 architecture is important for aiding processor design evaluation. This paper looks at dynamic VAX-11 instruction set usage by one class of programs, and discusses the methodology and tools which have been developed to provide that information. Six VAX/VMS native mode compilers from Digital Equipment Corporation were used: BASIC, BLISS, COBOL, FORTRAN, PASCAL, and PL/I. A summary of results generated by analyzing executions of these six compilers is presented. Information is included for instruction and class frequency, general instruction features, operand specifiers, the memory data stream, register utilization, instruction sequencing, and branch displacements.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {177--184},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800050.801841},
 doi = {http://doi.acm.org/10.1145/800050.801841},
 acmid = {801841},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Maekawa:1982:FSA:964750.801842,
 author = {Maekawa, Mamoru and Sakamura, Ken and Ishikawa, Chiaki},
 title = {Firmware structure and architectural support for monitors, vertical migration and user microprogramming},
 abstract = {This paper describes firmware and hardware support necessary for constructing easy-to-understand and high performance operating systems including language translators and interpreters. Basic principles are one-to-one correspondence between logical hierarchy and physical hierarchy, and vertical migration. Implementation of monitors in firmware and architectural support for it are discussed, and a sample system is shown. Architectural support for user microprogramming is then discussed and an example is shown. After a total system firmware structure is discussed, an experiment of vertical migration is described. It is shown that a proper selection of modules for migration is extremely important. It is suggested that the direction shown in this paper is one of future directions of computer systems.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/964750.801842},
 doi = {http://doi.acm.org/10.1145/964750.801842},
 acmid = {801842},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Maekawa:1982:FSA:800050.801842,
 author = {Maekawa, Mamoru and Sakamura, Ken and Ishikawa, Chiaki},
 title = {Firmware structure and architectural support for monitors, vertical migration and user microprogramming},
 abstract = {This paper describes firmware and hardware support necessary for constructing easy-to-understand and high performance operating systems including language translators and interpreters. Basic principles are one-to-one correspondence between logical hierarchy and physical hierarchy, and vertical migration. Implementation of monitors in firmware and architectural support for it are discussed, and a sample system is shown. Architectural support for user microprogramming is then discussed and an example is shown. After a total system firmware structure is discussed, an experiment of vertical migration is described. It is shown that a proper selection of modules for migration is extremely important. It is suggested that the direction shown in this paper is one of future directions of computer systems.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800050.801842},
 doi = {http://doi.acm.org/10.1145/800050.801842},
 acmid = {801842},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Maekawa:1982:FSA:960120.801842,
 author = {Maekawa, Mamoru and Sakamura, Ken and Ishikawa, Chiaki},
 title = {Firmware structure and architectural support for monitors, vertical migration and user microprogramming},
 abstract = {This paper describes firmware and hardware support necessary for constructing easy-to-understand and high performance operating systems including language translators and interpreters. Basic principles are one-to-one correspondence between logical hierarchy and physical hierarchy, and vertical migration. Implementation of monitors in firmware and architectural support for it are discussed, and a sample system is shown. Architectural support for user microprogramming is then discussed and an example is shown. After a total system firmware structure is discussed, an experiment of vertical migration is described. It is shown that a proper selection of modules for migration is extremely important. It is suggested that the direction shown in this paper is one of future directions of computer systems.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960120.801842},
 doi = {http://doi.acm.org/10.1145/960120.801842},
 acmid = {801842},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kamibayashi:1982:HOS:960120.801843,
 author = {Kamibayashi, N. and Ogawana, H. and Nagayama, K. and Aiso, H.},
 title = {Heart: An operating system nucleus machine implemented by firmware},
 abstract = {This paper discusses the role of microprogramming in operating system design and shows several things: (1) advantages of the efficiency which may be gained from microcoded operating system primitives, (2) selecting the most appropriste primitives for implementation, and (3) an analysis of the tradeoffs among software, firmware, and hardware. The authors propose a practical approach of enhancing computer architecture level, from a view point of functional hierarchy of operating systems. In order to prove the advantages of this approach, we have designed and implemented an experimental abstract machine for an operating system nucleus. This research is an experimental design, and evaluation on its operating system nucleus machine,called HEART. HEART is a set of primitive and universal functions, and works as a nucleus of a multiprogrammed operating system. The research results of our approach are the followings: First, to clarify the properties of operating system nucleus, taking functional hierarchy of operating system into consideration. Second, to show the design of operating system nucleus based on novel concepts. Third, to confirm the possibility of implimenting operating system nucleus machine. Finally, we give a performance evaluation on microcoded HEART and the effectiveness of enhancing computer architecture level based on the properties of operating systems.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {195--204},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/960120.801843},
 doi = {http://doi.acm.org/10.1145/960120.801843},
 acmid = {801843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kamibayashi:1982:HOS:964750.801843,
 author = {Kamibayashi, N. and Ogawana, H. and Nagayama, K. and Aiso, H.},
 title = {Heart: An operating system nucleus machine implemented by firmware},
 abstract = {This paper discusses the role of microprogramming in operating system design and shows several things: (1) advantages of the efficiency which may be gained from microcoded operating system primitives, (2) selecting the most appropriste primitives for implementation, and (3) an analysis of the tradeoffs among software, firmware, and hardware. The authors propose a practical approach of enhancing computer architecture level, from a view point of functional hierarchy of operating systems. In order to prove the advantages of this approach, we have designed and implemented an experimental abstract machine for an operating system nucleus. This research is an experimental design, and evaluation on its operating system nucleus machine,called HEART. HEART is a set of primitive and universal functions, and works as a nucleus of a multiprogrammed operating system. The research results of our approach are the followings: First, to clarify the properties of operating system nucleus, taking functional hierarchy of operating system into consideration. Second, to show the design of operating system nucleus based on novel concepts. Third, to confirm the possibility of implimenting operating system nucleus machine. Finally, we give a performance evaluation on microcoded HEART and the effectiveness of enhancing computer architecture level based on the properties of operating systems.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {195--204},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/964750.801843},
 doi = {http://doi.acm.org/10.1145/964750.801843},
 acmid = {801843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kamibayashi:1982:HOS:800050.801843,
 author = {Kamibayashi, N. and Ogawana, H. and Nagayama, K. and Aiso, H.},
 title = {Heart: An operating system nucleus machine implemented by firmware},
 abstract = {This paper discusses the role of microprogramming in operating system design and shows several things: (1) advantages of the efficiency which may be gained from microcoded operating system primitives, (2) selecting the most appropriste primitives for implementation, and (3) an analysis of the tradeoffs among software, firmware, and hardware. The authors propose a practical approach of enhancing computer architecture level, from a view point of functional hierarchy of operating systems. In order to prove the advantages of this approach, we have designed and implemented an experimental abstract machine for an operating system nucleus. This research is an experimental design, and evaluation on its operating system nucleus machine,called HEART. HEART is a set of primitive and universal functions, and works as a nucleus of a multiprogrammed operating system. The research results of our approach are the followings: First, to clarify the properties of operating system nucleus, taking functional hierarchy of operating system into consideration. Second, to show the design of operating system nucleus based on novel concepts. Third, to confirm the possibility of implimenting operating system nucleus machine. Finally, we give a performance evaluation on microcoded HEART and the effectiveness of enhancing computer architecture level based on the properties of operating systems.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {195--204},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800050.801843},
 doi = {http://doi.acm.org/10.1145/800050.801843},
 acmid = {801843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ahuja:1982:MAH:960120.801844,
 author = {Ahuja, Sudhir R. and Asthana, Abhaya},
 title = {A multi-microprocessor architecture with hardware support for communication and scheduling},
 abstract = {We describe a multiprocessor system that attempts to enhance the system performance by incorporating into its architecture a number of key operating system concepts. In particular: \&mdash; the scheduling and synchronization of concurrent activities are built in at the hardware level, \&mdash; the interprocess communication functions are performed in hardware, and, \&mdash; a coupling between the scheduling and communication functions is provided which allows efficient implementation of parallel systems that is precluded when the scheduling and communication functions are realized in software.},
 journal = {SIGPLAN Not.},
 volume = {17},
 issue = {4},
 month = {March},
 year = {1982},
 issn = {0362-1340},
 pages = {205--209},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/960120.801844},
 doi = {http://doi.acm.org/10.1145/960120.801844},
 acmid = {801844},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ahuja:1982:MAH:964750.801844,
 author = {Ahuja, Sudhir R. and Asthana, Abhaya},
 title = {A multi-microprocessor architecture with hardware support for communication and scheduling},
 abstract = {We describe a multiprocessor system that attempts to enhance the system performance by incorporating into its architecture a number of key operating system concepts. In particular: \&mdash; the scheduling and synchronization of concurrent activities are built in at the hardware level, \&mdash; the interprocess communication functions are performed in hardware, and, \&mdash; a coupling between the scheduling and communication functions is provided which allows efficient implementation of parallel systems that is precluded when the scheduling and communication functions are realized in software.},
 journal = {SIGARCH Comput. Archit. News},
 volume = {10},
 issue = {2},
 month = {March},
 year = {1982},
 issn = {0163-5964},
 pages = {205--209},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/964750.801844},
 doi = {http://doi.acm.org/10.1145/964750.801844},
 acmid = {801844},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ahuja:1982:MAH:800050.801844,
 author = {Ahuja, Sudhir R. and Asthana, Abhaya},
 title = {A multi-microprocessor architecture with hardware support for communication and scheduling},
 abstract = {We describe a multiprocessor system that attempts to enhance the system performance by incorporating into its architecture a number of key operating system concepts. In particular: \&mdash; the scheduling and synchronization of concurrent activities are built in at the hardware level, \&mdash; the interprocess communication functions are performed in hardware, and, \&mdash; a coupling between the scheduling and communication functions is provided which allows efficient implementation of parallel systems that is precluded when the scheduling and communication functions are realized in software.},
 booktitle = {Proceedings of the first international symposium on Architectural support for programming languages and operating systems},
 series = {ASPLOS-I},
 year = {1982},
 isbn = {0-89791-066-4},
 location = {Palo Alto, California, United States},
 pages = {205--209},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/800050.801844},
 doi = {http://doi.acm.org/10.1145/800050.801844},
 acmid = {801844},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

