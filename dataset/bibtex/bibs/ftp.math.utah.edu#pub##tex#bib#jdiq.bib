%%% -*-BibTeX-*-
%%% ====================================================================
%%% BibTeX-file{
%%%     author          = "Nelson H. F. Beebe",
%%%     version         = "1.04",
%%%     date            = "28 March 2011",
%%%     time            = "12:05:10 MST",
%%%     filename        = "jdiq.bib",
%%%     address         = "University of Utah
%%%                        Department of Mathematics, 110 LCB
%%%                        155 S 1400 E RM 233
%%%                        Salt Lake City, UT 84112-0090
%%%                        USA",
%%%     telephone       = "+1 801 581 5254",
%%%     FAX             = "+1 801 581 4148",
%%%     URL             = "http://www.math.utah.edu/~beebe",
%%%     checksum        = "56774 1036 5281 48921",
%%%     email           = "beebe at math.utah.edu, beebe at acm.org,
%%%                        beebe at computer.org (Internet)",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "Journal of Data and Information Quality
%%%                       (JDIQ); bibliography",
%%%     license         = "public domain",
%%%     supported       = "yes",
%%%     docstring       = "This is a COMPLETE BibTeX bibliography for
%%%                        the ACM Journal of Data and Information
%%%                        Quality (JDIQ) (CODEN ????, ISSN 1936-1955),
%%%                        covering all journal issues from 2009 --
%%%                        date.
%%%
%%%                        At version 1.04, the COMPLETE journal
%%%                        coverage looked like this:
%%%
%%%                             2009 (  17)    2010 (   6)    2011 (   5)
%%%
%%%                             Article:         28
%%%
%%%                             Total entries:   28
%%%
%%%                        The journal table of contents page is at:
%%%
%%%                            http://www.acm.org/jdiq/
%%%                            http://portal.acm.org/browse_dl.cfm?idx=J1191
%%%
%%%                        Qualified subscribers can retrieve the full
%%%                        text of recent articles in PDF form.
%%%
%%%                        The initial draft was extracted from the ACM
%%%                        Web pages.
%%%
%%%                        ACM copyrights explicitly permit abstracting
%%%                        with credit, so article abstracts, keywords,
%%%                        and subject classifications have been
%%%                        included in this bibliography wherever
%%%                        available.  Article reviews have been
%%%                        omitted, until their copyright status has
%%%                        been clarified.
%%%
%%%                        bibsource keys in the bibliography entries
%%%                        below indicate the entry originally came
%%%                        from the computer science bibliography
%%%                        archive, even though it has likely since
%%%                        been corrected and updated.
%%%
%%%                        URL keys in the bibliography point to
%%%                        World Wide Web locations of additional
%%%                        information about the entry.
%%%
%%%                        BibTeX citation tags are uniformly chosen
%%%                        as name:year:abbrev, where name is the
%%%                        family name of the first author or editor,
%%%                        year is a 4-digit number, and abbrev is a
%%%                        3-letter condensation of important title
%%%                        words. Citation tags were automatically
%%%                        generated by software developed for the
%%%                        BibNet Project.
%%%
%%%                        In this bibliography, entries are sorted in
%%%                        publication order, using ``bibsort -byvolume.''
%%%
%%%                        The checksum field above contains a CRC-16
%%%                        checksum as the first value, followed by the
%%%                        equivalent of the standard UNIX wc (word
%%%                        count) utility output of lines, words, and
%%%                        characters.  This is produced by Robert
%%%                        Solovay's checksum utility."
%%%     }
%%% ====================================================================

@Preamble{"\input bibnames.sty" #
    "\def \TM {${}^{\sc TM}$}"
}

%%% ====================================================================
%%% Acknowledgement abbreviations:

@String{ack-nhfb = "Nelson H. F. Beebe,
                    University of Utah,
                    Department of Mathematics, 110 LCB,
                    155 S 1400 E RM 233,
                    Salt Lake City, UT 84112-0090, USA,
                    Tel: +1 801 581 5254,
                    FAX: +1 801 581 4148,
                    e-mail: \path|beebe@math.utah.edu|,
                            \path|beebe@acm.org|,
                            \path|beebe@computer.org| (Internet),
                    URL: \path|http://www.math.utah.edu/~beebe/|"}

%%% ====================================================================
%%% Journal abbreviations:

@String{j-JDIQ                  = "Journal of Data and Information
                                  Quality (JDIQ)"}

%%% ====================================================================
%%% Bibliography entries:

@Article{Madnick:2009:EII,
  author =       "Stuart E. Madnick and Yang W. Lee",
  title =        "Editorial for the Inaugural Issue of the {ACM Journal
                 of Data and Information Quality (JDIQ)}",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "1",
  pages =        "1:1--1:??",
  month =        jun,
  year =         "2009",
  CODEN =        "????",
  ISSN =         "1936-1955",
  bibdate =      "Fri Sep 18 15:11:35 MDT 2009",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "1",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Madnick:2009:OFD,
  author =       "Stuart E. Madnick and Richard Y. Wang and Yang W. Lee
                 and Hongwei Zhu",
  title =        "Overview and Framework for Data and Information
                 Quality Research",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "1",
  pages =        "2:1--2:??",
  month =        jun,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1515693.1516680",
  ISSN =         "1936-1955",
  bibdate =      "Fri Sep 18 15:11:35 MDT 2009",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Awareness of data and information quality issues has
                 grown rapidly in light of the critical role played by
                 the quality of information in our data-intensive,
                 knowledge-based economy. Research in the past two
                 decades has produced a large body of data quality
                 knowledge and has expanded our ability to solve many
                 data and information quality problems. In this article,
                 we present an overview of the evolution and current
                 landscape of data and information quality research. We
                 introduce a framework to characterize the research
                 along two dimensions: topics and methods.
                 Representative papers are cited for purposes of
                 illustrating the issues addressed and the methods used.
                 We also identify and discuss challenges to be addressed
                 in future research.",
  acknowledgement = ack-nhfb,
  articleno =    "2",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Li:2009:BAE,
  author =       "Xiao-Bai Li",
  title =        "A {Bayesian} Approach for Estimating and Replacing
                 Missing Categorical Data",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "1",
  pages =        "3:1--3:??",
  month =        jun,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1515693.1515695",
  ISSN =         "1936-1955",
  bibdate =      "Fri Sep 18 15:11:35 MDT 2009",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "We propose a new approach for estimating and replacing
                 missing categorical data. With this approach, the
                 posterior probabilities of a missing attribute value
                 belonging to a certain category are estimated using the
                 simple Bayes method. Two alternative methods for
                 replacing the missing value are proposed: The first
                 replaces the missing value with the value having the
                 estimated maximum probability; the second uses a value
                 that is selected with probability proportional to the
                 estimated posterior distribution. The effectiveness of
                 the proposed approach is evaluated based on some
                 important data quality measures for data warehousing
                 and data mining. The results of the experimental study
                 demonstrate the effectiveness of the proposed approach.",
  acknowledgement = ack-nhfb,
  articleno =    "3",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Weber:2009:OSD,
  author =       "Kristin Weber and Boris Otto and Hubert {\"O}sterle",
  title =        "One Size Does Not Fit All---{A} Contingency Approach
                 to Data Governance",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "1",
  pages =        "4:1--4:??",
  month =        jun,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1515693.1515696",
  ISSN =         "1936-1955",
  bibdate =      "Fri Sep 18 15:11:35 MDT 2009",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Enterprizes need Data Quality Management (DQM) to
                 respond to strategic and operational challenges
                 demanding high-quality corporate data. Hitherto,
                 companies have mostly assigned accountabilities for DQM
                 to Information Technology (IT) departments. They have
                 thereby neglected the organizational issues critical to
                 successful DQM. With data governance, however,
                 companies may implement corporate-wide accountabilities
                 for DQM that encompass professionals from business and
                 IT departments. This research aims at starting a
                 scientific discussion on data governance by
                 transferring concepts from IT governance and
                 organizational theory to the previously largely ignored
                 field of data governance. The article presents the
                 first results of a community action research project on
                 data governance comprising six international companies
                 from various industries. It outlines a data governance
                 model that consists of three components (data quality
                 roles, decision areas, and responsibilities), which
                 together form a responsibility assignment matrix. The
                 data governance model documents data quality roles and
                 their type of interaction with DQM activities. In
                 addition, the article describes a data governance
                 contingency model and demonstrates the influence of
                 performance strategy, diversification breadth,
                 organization structure, competitive strategy, degree of
                 process harmonization, degree of market regulation, and
                 decision-making style on data governance. Based on
                 these findings, companies can structure their specific
                 data governance model.",
  acknowledgement = ack-nhfb,
  articleno =    "4",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Heinrich:2009:PDM,
  author =       "B. Heinrich and M. Klier and M. Kaiser",
  title =        "A Procedure to Develop Metrics for Currency and its
                 Application in {CRM}",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "1",
  pages =        "5:1--5:??",
  month =        jun,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1515693.1515697",
  ISSN =         "1936-1955",
  bibdate =      "Fri Sep 18 15:11:35 MDT 2009",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Due to the importance of using up-to-date data in
                 information systems, this article analyzes how the
                 data-quality dimension currency can be
                 quantified. Based on several requirements (e.g.,
                 normalization and interpretability) and a literature
                 review, we design a procedure to develop
                 probability-based metrics for currency which can be
                 adjusted to the specific characteristics of data
                 attribute values. We evaluate the presented procedure
                 with regard to the requirements and illustrate the
                 applicability as well as its practical benefit. In
                 cooperation with a major German mobile services
                 provider, the procedure was applied in the field of
                 campaign management in order to improve both success
                 rates and profits.",
  acknowledgement = ack-nhfb,
  articleno =    "5",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Madnick:2009:ELS,
  author =       "Stuart E. Madnick and Yang W. Lee",
  title =        "Editorial Letter for the Special Issue on Data Quality
                 in Databases and Information Systems",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "6:1--6:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577841",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "6",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Naumann:2009:GES,
  author =       "Felix Naumann and Louiqa Raschid",
  title =        "Guest Editorial for the Special Issue on Data Quality
                 in Databases",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "7:1--7:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577842",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "7",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Dash:2009:MLN,
  author =       "Manoranjan Dash and Ayush Singhania",
  title =        "Mining in Large Noisy Domains",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "8:1--8:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577843",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "In this article we address the issue of how to mine
                 efficiently in large and noisy data. We propose an
                 efficient sampling algorithm ({\em Concise\/}) as a
                 solution for large and noisy data. Concise is far more
                 superior than the Simple Random Sampling ({\em SRS\/})
                 in selecting a representative sample. Particularly when
                 the data is very large and noisy, Concise achieves the
                 maximum gain over SRS. The comparison is in terms of
                 their impact on subsequent data mining tasks,
                 specifically, classification, clustering, and
                 association rule mining. We compared Concise with a few
                 existing noise removal algorithms followed by SRS.
                 Although the accuracy of mining results are similar,
                 Concise spends very little time compared to the
                 existing algorithms because Concise has linear time
                 complexity.",
  acknowledgement = ack-nhfb,
  articleno =    "8",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "association rule mining; classification; clustering;
                 data mining; Information filtering; sampling; selection
                 process",
}

@Article{Moustakides:2009:OSR,
  author =       "George V. Moustakides and Vassilios S. Verykios",
  title =        "Optimal Stopping: {A} Record-Linkage Approach",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "9:1--9:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577844",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Record-linkage is the process of identifying whether
                 two separate records refer to the same real-world
                 entity when some elements of the record's identifying
                 information (attributes) agree and others disagree.
                 Existing record-linkage decision methodologies use the
                 outcomes from the comparisons of the whole set of
                 attributes. Here, we propose an alternative scheme that
                 assesses the attributes sequentially, allowing for a
                 decision to made at any attribute's comparison stage,
                 and thus before exhausting all available attributes.
                 The scheme we develop is optimum in that it minimizes a
                 well-defined average cost criterion while the
                 corresponding optimum solution can be easily mapped
                 into a decision tree to facilitate the record-linkage
                 decision process. Experimental results performed in
                 real datasets indicate the superiority of our
                 methodology compared to existing approaches.",
  acknowledgement = ack-nhfb,
  articleno =    "9",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "duplicate detection; optimal stopping;
                 Record-linkage",
}

@Article{Klein:2009:RDQ,
  author =       "A. Klein and W. Lehner",
  title =        "Representing Data Quality in Sensor Data Streaming
                 Environments",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "10:1--10:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577845",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Sensors in smart-item environments capture data about
                 product conditions and usage to support business
                 decisions as well as production automation processes. A
                 challenging issue in this application area is the
                 restricted quality of sensor data due to limited sensor
                 precision and sensor failures. Moreover, data stream
                 processing to meet resource constraints in streaming
                 environments introduces additional noise and decreases
                 the data quality. In order to avoid wrong business
                 decisions due to dirty data, quality characteristics
                 have to be captured, processed, and provided to the
                 respective business task. However, the issue of how to
                 efficiently provide applications with information about
                 data quality is still an open research problem.\par

                 In this article, we address this problem by presenting
                 a flexible model for the propagation and processing of
                 data quality. The comprehensive analysis of common data
                 stream processing operators and their impact on data
                 quality allows a fruitful data evaluation and
                 diminishes incorrect business decisions. Further, we
                 propose the data quality model control to adapt the
                 data quality granularity to the data stream
                 interestingness.",
  acknowledgement = ack-nhfb,
  articleno =    "10",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "data quality; Data stream processing; smart items",
}

@Article{Embury:2009:IDS,
  author =       "Suzanne M. Embury and Paolo Missier and Sandra Sampaio
                 and R. Mark Greenwood and Alun D. Preece",
  title =        "Incorporating Domain-Specific Information Quality
                 Constraints into Database Queries",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "11:1--11:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577846",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "The range of information now available in queryable
                 repositories opens up a host of possibilities for new
                 and valuable forms of data analysis. Database query
                 languages such as SQL and XQuery offer a concise and
                 high-level means by which such analyses can be
                 implemented, facilitating the extraction of relevant
                 data subsets into either generic or bespoke data
                 analysis environments. Unfortunately, the quality of
                 data in these repositories is often highly variable.
                 The data is still useful, but only if the consumer is
                 aware of the data quality problems and can work around
                 them. Standard query languages offer little support for
                 this aspect of data management. In principle, however,
                 it should be possible to embed constraints describing
                 the consumer's data quality requirements into the query
                 directly, so that the query evaluator can take over
                 responsibility for enforcing them during query
                 processing.\par

                 Most previous attempts to incorporate information
                 quality constraints into database queries have been
                 based around a small number of highly generic quality
                 measures, which are defined and computed by the
                 information provider. This is a useful approach in some
                 application areas but, in practice, quality criteria
                 are more commonly determined by the user of the
                 information not by the provider. In this article, we
                 explore an approach to incorporating quality
                 constraints into database queries where the definition
                 of quality is set by the user and not the provider of
                 the information. Our approach is based around the
                 concept of a {\em quality view}, a configurable quality
                 assessment component into which domain-specific notions
                 of quality can be embedded. We examine how quality
                 views can be incorporated into XQuery, and draw from
                 this the language features that are required in general
                 to embed quality views into any query language. We also
                 propose some syntactic sugar on top of XQuery to
                 simplify the process of querying with quality
                 constraints.",
  acknowledgement = ack-nhfb,
  articleno =    "11",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "database query languages; Information quality; views;
                 XQuery",
}

@Article{Madnick:2009:CPS,
  author =       "Stuart E. Madnick and Yang W. Lee",
  title =        "Call for Papers Special Issue on Healthcare
                 Information Quality: the Challenges and Opportunities
                 in Healthcare Systems and Services",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "2",
  pages =        "12:1--12:??",
  month =        sep,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1577840.1577847",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:40 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "12",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Madnick:2009:ECW,
  author =       "Stuart E. Madnick and Yang W. Lee",
  title =        "Editors' Comments: Where the {JDIQ} Articles Come
                 From: Incubating Research in an Emerging Field",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "3",
  pages =        "13:1--13:??",
  month =        dec,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1659225.1659226",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:55 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "13",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Sessions:2009:TMD,
  author =       "V. Sessions and M. Valtorta",
  title =        "Towards a Method for Data Accuracy Assessment
                 Utilizing a {Bayesian} Network Learning Algorithm",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "3",
  pages =        "14:1--14:??",
  month =        dec,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1659225.1659227",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:55 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "This research develops a data quality algorithm
                 entitled the Accuracy Assessment Algorithm (AAA). This
                 is an extension of research in developing an
                 enhancement to a Bayesian Network (BN) learning
                 algorithm called the Data Quality (DQ) algorithm. This
                 new algorithm is concerned with estimating the accuracy
                 levels of a dataset by assessing the quality of the
                 data with no prior knowledge of the dataset. The AAA
                 and associated metrics were tested using two canonical
                 BNs and one large-scale medical network. The article
                 presents the results regarding the efficacy of the
                 algorithm and the implications for future research and
                 practice.",
  acknowledgement = ack-nhfb,
  articleno =    "14",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "accuracy levels; Bayesian networks; data quality
                 assessment; PC algorithm",
}

@Article{Even:2009:DAD,
  author =       "Adir Even and G. Shankaranarayanan",
  title =        "Dual Assessment of Data Quality in Customer
                 Databases",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "3",
  pages =        "15:1--15:??",
  month =        dec,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1659225.1659228",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:55 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Quantitative assessment of data quality is critical
                 for identifying the presence of data defects and the
                 extent of the damage due to these defects. Quantitative
                 assessment can help define realistic quality
                 improvement targets, track progress, evaluate the
                 impacts of different solutions, and prioritize
                 improvement efforts accordingly. This study describes a
                 methodology for quantitatively assessing both impartial
                 {\em and\/} contextual data quality in large datasets.
                 Impartial assessment measures the extent to which a
                 dataset is defective, independent of the context in
                 which that dataset is used. Contextual assessment, as
                 defined in this study, measures the extent to which the
                 presence of defects reduces a dataset's utility, the
                 benefits gained by using that dataset in a specific
                 context. The dual assessment methodology is
                 demonstrated in the context of Customer Relationship
                 Management (CRM), using large data samples from
                 real-world datasets. The results from comparing the two
                 assessments offer important insights for directing
                 quality maintenance efforts and prioritizing quality
                 improvement solutions for this dataset. The study
                 describes the steps and the computation involved in the
                 dual-assessment methodology and discusses the
                 implications for applying the methodology in other
                 business contexts and data environments.",
  acknowledgement = ack-nhfb,
  articleno =    "15",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "CRM; customer relationship management; databases; Data
                 quality; information value; total data quality
                 management",
}

@Article{Fisher:2009:AMP,
  author =       "Craig W. Fisher and Eitel J. M. Lauria and Carolyn C.
                 Matheus",
  title =        "An Accuracy Metric: Percentages, Randomness, and
                 Probabilities",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "3",
  pages =        "16:1--16:??",
  month =        dec,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1659225.1659229",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:55 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "Practitioners and researchers regularly refer to error
                 rates or accuracy percentages of databases. The former
                 is the number of cells in error divided by the total
                 number of cells; the latter is the number of correct
                 cells divided by the total number of cells. However,
                 databases may have similar error rates (or accuracy
                 percentages) but differ drastically in the complexity
                 of their accuracy problems. A simple percent does not
                 provide information as to whether the errors are
                 systematic or randomly distributed throughout the
                 database. We expand the accuracy metric to include a
                 randomness measure and include a probability
                 distribution value. The proposed randomness check is
                 based on the Lempel--Ziv (LZ) complexity measure.
                 Through two simulation studies we show that the LZ
                 complexity measure can clearly differentiate as to
                 whether the errors are random or systematic. This
                 determination is a significant first step and is a
                 major departure from the percentage-alone technique.
                 Once it is determined that the errors are random, a
                 probability distribution, Poisson, is used to help
                 address various managerial questions.",
  acknowledgement = ack-nhfb,
  articleno =    "16",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "complexity; Data and information quality; randomness",
}

@Article{Ababneh:2009:CSE,
  author =       "Sufyan Ababneh and Rashid Ansari and Ashfaq Khokhar",
  title =        "Compensated Signature Embedding for Multimedia Content
                 Authentication",
  journal =      j-JDIQ,
  volume =       "1",
  number =       "3",
  pages =        "17:1--17:??",
  month =        dec,
  year =         "2009",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1659225.1659230",
  ISSN =         "1936-1955",
  bibdate =      "Wed Mar 17 14:47:55 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "One of the main goals of digital content
                 authentication and preservation techniques is to
                 guarantee the originality and quality of the
                 information. In this article, robust watermarking is
                 used to embed content-based fragile signatures in
                 multimedia signals to achieve efficient authentication
                 without requiring any third-party reference or side
                 information. To overcome the signature alteration
                 caused by the embedding perturbation and other possible
                 encoding operations, a closed-form compensation
                 technique is proposed for ensuring signature
                 consistency by employing a Lagrangian-based approach. A
                 minimum distortion criterion is used to ensure signal
                 quality. The effectiveness of the proposed approach is
                 investigated with simulations of examples of image
                 authentication in which signatures are designed to
                 reveal tamper localization. Results using quantitative
                 performance criteria show successful authentication
                 over a range of robustness in embedding watermarks
                 using both QIM-DM and spread-spectrum techniques. A
                 comparison with two iterative compensation schemes is
                 also presented.",
  acknowledgement = ack-nhfb,
  articleno =    "17",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "compensated signature embedding; Content
                 authentication; watermarking",
}

@Article{Madnick:2010:ECA,
  author =       "Stuart E. Madnick and Yang W. Lee",
  title =        "{Editors}' Comments: {ACM Journal of Data and
                 Information Quality (JDIQ)} is alive and well!",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "1",
  pages =        "1:1--1:??",
  month =        jul,
  year =         "2010",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1805286.1805287",
  ISSN =         "1936-1955",
  bibdate =      "Tue Sep 7 08:41:54 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "1",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Tremblay:2010:UDM,
  author =       "Monica Chiarini Tremblay and Kaushik Dutta and Debra
                 Vandermeer",
  title =        "Using Data Mining Techniques to Discover Bias Patterns
                 in Missing Data",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "1",
  pages =        "2:1--2:??",
  month =        jul,
  year =         "2010",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1805286.1805288",
  ISSN =         "1936-1955",
  bibdate =      "Tue Sep 7 08:41:54 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "In today's data-rich environment, decision makers draw
                 conclusions from data repositories that may contain
                 data quality problems. In this context, missing data is
                 an important and known problem, since it can seriously
                 affect the accuracy of conclusions drawn. Researchers
                 have described several approaches for dealing with
                 missing data, primarily attempting to infer values or
                 estimate the impact of missing data on conclusions.
                 However, few have considered approaches to characterize
                 patterns of bias in missing data, that is, to determine
                 the specific attributes that predict the missingness of
                 data values. Knowledge of the specific systematic bias
                 patterns in the incidence of missing data can help
                 analysts more accurately assess the quality of
                 conclusions drawn from data sets with missing data.
                 This research proposes a methodology to combine a
                 number of Knowledge Discovery and Data Mining
                 techniques, including association rule mining, to
                 discover patterns in related attribute values that help
                 characterize these bias patterns. We demonstrate the
                 efficacy of our proposed approach by applying it on a
                 demo census dataset seeded with biased missing data.
                 The experimental results show that our approach was
                 able to find seeded biases and filter out most seeded
                 noise.",
  acknowledgement = ack-nhfb,
  articleno =    "2",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "Data quality; missing data; pattern discovery",
}

@Article{Jensen:2010:JCI,
  author =       "Matthew L. Jensen and Judee K. Burgoon and Jay F.
                 Nunamaker and Jr",
  title =        "Judging the Credibility of Information Gathered from
                 Face-to-Face Interactions",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "1",
  pages =        "3:1--3:??",
  month =        jul,
  year =         "2010",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1805286.1805289",
  ISSN =         "1936-1955",
  bibdate =      "Tue Sep 7 08:41:54 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "One of the most pernicious threats to information
                 quality comes through perpetration of deception by
                 information suppliers. Deception undermines many
                 critical dimensions of information quality, such as
                 accuracy, completeness, and believability. Despite this
                 threat, information gatherers are ill equipped to
                 assess the credibility of information suppliers. This
                 work presents a prototype system that examines messages
                 gathered during direct, face-to-face information
                 gathering. The system unobtrusively identifies kinesic
                 and linguistic features that may indicate deception in
                 information suppliers' messages. System use was found
                 to significantly improve assessment ability in
                 between-subjects and within-subjects tests. The
                 improved ability to accurately assess credibility
                 during face-to-face interactions should yield higher
                 information quality.",
  acknowledgement = ack-nhfb,
  articleno =    "3",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "Credibility assessment; deception detection;
                 decision-aids; human-computer interaction; information
                 veracity; kinesics; linguistics",
}

@Article{Meda:2010:DDF,
  author =       "Hema S. Meda and Anup Kumar Sen and Amitava Bagchi",
  title =        "On Detecting Data Flow Errors in Workflows",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "1",
  pages =        "4:1--4:??",
  month =        jul,
  year =         "2010",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1805286.1805290",
  ISSN =         "1936-1955",
  bibdate =      "Tue Sep 7 08:41:54 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "When designing a business workflow, it is customary
                 practice to create the control flow structure first and
                 to ensure its correctness. Information about the flow
                 of data is introduced subsequently into the workflow
                 and its correctness is independently verified. Improper
                 specification of data requirements of tasks and XOR
                 splits can cause problems such as wrong branching at
                 XOR splits and the failure of tasks to execute. Here we
                 present a graph traversal algorithm called GTforDF for
                 detecting data flow errors in both nested and
                 unstructured workflows, and illustrate its operation on
                 realistic examples. Two of these have interconnected
                 loops and are free of control flow errors, and the
                 third one is an unstructured loop-free workflow. Our
                 approach extends and generalizes data flow verification
                 methods that have been recently proposed. It also makes
                 use of the concept of corresponding pairs lately
                 introduced in control flow verification. It thus has
                 the potential for development into a unified
                 algorithmic procedure for the concurrent detection of
                 control flow and data flow errors. The correctness of
                 the algorithm has been proved theoretically. It has
                 also been tested experimentally on many examples.",
  acknowledgement = ack-nhfb,
  articleno =    "4",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "Corresponding pair; Data flow errors; Workflow
                 management",
}

@Article{Magnani:2010:SUM,
  author =       "Matteo Magnani and Danilo Montesi",
  title =        "A Survey on Uncertainty Management in Data
                 Integration",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "1",
  pages =        "5:1--5:??",
  month =        jul,
  year =         "2010",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1805286.1805291",
  ISSN =         "1936-1955",
  bibdate =      "Tue Sep 7 08:41:54 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  abstract =     "In the last few years, uncertainty management has come
                 to be recognized as a fundamental aspect of data
                 integration. It is now accepted that it may not be
                 possible to remove uncertainty generated during data
                 integration processes and that uncertainty in itself
                 may represent a source of relevant information. Several
                 issues, such as the aggregation of uncertain mappings
                 and the querying of uncertain mediated schemata, have
                 been addressed by applying well-known uncertainty
                 management theories. However, several problems lie
                 unresolved. This article sketches an initial picture of
                 this highly active research area; it details existing
                 works in the light of a homogeneous framework, and
                 identifies and discusses the leading issues awaiting
                 solutions.",
  acknowledgement = ack-nhfb,
  articleno =    "5",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
  keywords =     "Data integration; uncertainty",
}

@Article{Talburt:2010:CPS,
  author =       "John R. Talburt and Stuart E. Madnick and Yang W.
                 Lee",
  title =        "Call for Papers: Special Issue on Entity Resolution",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "1",
  pages =        "6:1--6:??",
  month =        jul,
  year =         "2010",
  CODEN =        "????",
  DOI =          "http://doi.acm.org/10.1145/1805286.1805292",
  ISSN =         "1936-1955",
  bibdate =      "Tue Sep 7 08:41:54 MDT 2010",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "6",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Madnick:2011:ESN,
  author =       "Stuart E. Madnick and Yang W. Lee",
  title =        "Editorial: In Search of Novel Ideas and Solutions with
                 a Broader Context of Data Quality in Mind",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "2",
  pages =        "7:1--7:??",
  month =        feb,
  year =         "2011",
  CODEN =        "????",
  DOI =          "http://dx.doi.org/10.1145/1891879.1891880",
  ISSN =         "1936-1955",
  bibdate =      "Mon Mar 28 12:03:59 MDT 2011",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "7",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Blake:2011:EID,
  author =       "Roger Blake and Paul Mangiameli",
  title =        "The Effects and Interactions of Data Quality and
                 Problem Complexity on Classification",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "2",
  pages =        "8:1--8:??",
  month =        feb,
  year =         "2011",
  CODEN =        "????",
  DOI =          "http://dx.doi.org/10.1145/1891879.1891881",
  ISSN =         "1936-1955",
  bibdate =      "Mon Mar 28 12:03:59 MDT 2011",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "8",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Gelman:2011:GGA,
  author =       "Irit Askira Gelman",
  title =        "{GIGO} or not {GIGO}: The Accuracy of Multi-Criteria
                 Satisficing Decisions",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "2",
  pages =        "9:1--9:??",
  month =        feb,
  year =         "2011",
  CODEN =        "????",
  DOI =          "http://dx.doi.org/10.1145/1891879.1891882",
  ISSN =         "1936-1955",
  bibdate =      "Mon Mar 28 12:03:59 MDT 2011",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "9",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Fan:2011:GBN,
  author =       "Xiaoming Fan and Jianyong Wang and Xu Pu and Lizhu
                 Zhou and Bing Lv",
  title =        "On Graph-Based Name Disambiguation",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "2",
  pages =        "10:1--10:??",
  month =        feb,
  year =         "2011",
  CODEN =        "????",
  DOI =          "http://dx.doi.org/10.1145/1891879.1891883",
  ISSN =         "1936-1955",
  bibdate =      "Mon Mar 28 12:03:59 MDT 2011",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "10",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}

@Article{Ngugi:2011:TBI,
  author =       "Benjamin Ngugi and Beverly K. Kahn and Marilyn
                 Tremaine",
  title =        "Typing Biometrics: Impact of Human Learning on
                 Performance Quality",
  journal =      j-JDIQ,
  volume =       "2",
  number =       "2",
  pages =        "11:1--11:??",
  month =        feb,
  year =         "2011",
  CODEN =        "????",
  DOI =          "http://dx.doi.org/10.1145/1891879.1891884",
  ISSN =         "1936-1955",
  bibdate =      "Mon Mar 28 12:03:59 MDT 2011",
  bibsource =    "http://www.acm.org/pubs/contents/journals/jdqi/",
  acknowledgement = ack-nhfb,
  articleno =    "11",
  fjournal =     "Journal of Data and Information Quality (JDIQ)",
}
