@inproceedings{4086144,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Petoumenos, P. and Keramidas, G. and Zeffer, H. and Kaxiras, S. and Hagersten, E.},
 year = {2006},
 pages = {160--171},
 publisher = {IEEE},
 title = {Modeling Cache Sharing on Chip Multiprocessor Architectures},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302740},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086144},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086144.pdf?arnumber=4086144},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, CMP cache sharing, Computer architecture, Dynamic scheduling, Information technology, Predictive models, Resource management, Size measurement, StatShare, Statistics, Time measurement, Yarn, cache decay, cache management, cache partitioning, cache storage, cache-line granularity, chip multiprocessor architecture, memory architecture, microprocessor chips, multi-threading, multiprocessing systems, statistical analysis, statistical model, thread behavior, },
 abstract = {As CMPs are emerging as the dominant architecture for a wide range of platforms (from embedded systems and game consoles, to PCs, and to servers) the need to manage on-chip resources, such as shared caches, becomes a necessity. In this paper we propose a new statistical model of a CMP shared cache which not only describes cache sharing but also its management via a novel fine-grain mechanism. Our model, called StatShare, accurately describes the behavior of the sharing threads using run-time information (reuse-distance information for memory accesses) and helps us understand how effectively each thread uses its space. The mechanism to manage the cache at the cache-line granularity is inspired by cache decay, but contains important differences. Decayed cache-lines are not turned-off to save leakage but are rather "available for replacement." Decay modifies the underlying replacement policy (random, LRU) to control sharing but in a very flexible and non-strict way which makes it superior to strict cache partitioning schemes (both fine and coarse grained). The statistical model allows us to assess a thread's cache behavior under decay. Detailed CMP simulations show that: i) StatShare accurately predicts the thread behavior in a shared cache, ii) managing sharing via decay (in combination with the StatShare run time information) can be used to enforce external QoS requirements or various high-level fairness policies },
}

@inproceedings{4086145,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Murphy, R. and Berry, J. and McLendon, W. and Hendrickson, B. and Gregor, D. and Lumsdaine, A.},
 year = {2006},
 pages = {175--177},
 publisher = {IEEE},
 title = {DFS: A Simple to Write Yet Difficult to Execute Benchmark},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302741},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086145},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086145.pdf?arnumber=4086145},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, Data mining, Data structures, Graph theory, Informatics, Laboratories, Libraries, Social network services, Supercomputers, US Department of Energy, boost graph library, depth first search, graph theory, graph theory, large power-law graph, memory architecture, memory architecture, random memory access patterns, tree searching, },
 abstract = {Many emerging applications are built upon large, unstructured datasets that exhibit highly irregular (or even nearly random) memory access patterns. Examples include informatics applications, and other problems that are often represented by unstructured graph-based data structures. It is well known that these applications are challenging for conventional architectures to execute (either serially or in parallel). The depth first search (DFS) benchmark proposed in this work uses the boost graph library to perform a depth-first search on a large power-law graph, representing "small world" phenomena. The graph in question exhibits a small average distance between any two vertices, a small diameter, and has a few high-degree vertices with a large number of low-degree vertices. Graphs such as this appear in many fields, including networking, biology, social networks, and data mining. Many of these applications are of critical importance to researchers, and the challenge of executing them on conventional machines increases as the graph size grows. The benchmark proposed in this work is used as the basis for many fundamental algorithms in graph theory, is critical to several emerging applications, is memory intensive, and exhibits poor performance on conventional machines. Section 2 quantitatively demonstrates the memory characteristics of the benchmark in an architecture independent fashion, showing that it is extremely memory intensive. Section 3 describes the execution phases of the benchmark. And section 4 presents the conclusions },
}

@inproceedings{4086146,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Altun, O. and Dursunoglu, N. and Amasyali, M.F.},
 year = {2006},
 pages = {178--181},
 publisher = {IEEE},
 title = {Clustering Application Benchmark},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302742},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086146},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086146.pdf?arnumber=4086146},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {ANSI C specifications, Animals, Benchmark testing, C language, Cities and towns, Clustering Methods, Clustering algorithms, Computer languages, Credit cards, Earthquakes, K-means batch, Plants (biology), SOM-2 dimension, Self-organizing feature maps, Spatial databases, clustering algorithm, clustering application benchmark, compiler, data handling, hierarchical K-means online, hierarchical SOM-1 dimension, pattern clustering, program compilers, self-organising feature maps, self-organizing feature maps, },
 abstract = {An application benchmark based on a set of clustering algorithms is described in this paper. The details of algorithms (K-means online, K-means batch, SOM-1 dimension, SOM-2 dimension, hierarchical K-means online and hierarchical SOM-1 dimension) are given. The code provided complies with ANSI C specifications, as a result is highly portable. The benchmark has been tested on various platforms using different compilers },
}

@inproceedings{4086147,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Narayanan, R. and Ozisikyilmaz, B. and Zambreno, J. and Memik, G. and Choudhary, A.},
 year = {2006},
 pages = {182--188},
 publisher = {IEEE},
 title = {MineBench: A Benchmark Suite for Data Mining Workloads},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302743},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086147},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086147.pdf?arnumber=4086147},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, Association rules, Classification tree analysis, Clustering algorithms, Computer architecture, DNA, Data mining, Databases, Fuzzy logic, MineBench, Sequences, computer architecture, computer architecture, data extraction, data mining, data mining, },
 abstract = {Data mining constitutes an important class of scientific and commercial applications. Recent advances in data extraction techniques have created vast data sets, which require increasingly complex data mining algorithms to sift through them to generate meaningful information. The disproportionately slower rate of growth of computer systems has led to a sizeable performance gap between data mining systems and algorithms. The first step in closing this gap is to analyze these algorithms and understand their bottlenecks. With this knowledge, current computer architectures can be optimized for data mining applications. In this paper, we present MineBench, a publicly available benchmark suite containing fifteen representative data mining applications belonging to various categories such as clustering, classification, and association rule mining. We believe that MineBench will be of use to those looking to characterize and accelerate data mining workloads },
}

@inproceedings{4086140,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Dong Ye and Ray, J. and Harle, C. and Kaeli, D.},
 year = {2006},
 pages = {120--127},
 publisher = {IEEE},
 title = {Performance Characterization of SPEC CPU2006 Integer Benchmarks on x86-64 Architecture},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302736},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086140},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086140.pdf?arnumber=4086140},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {32 bit, 64 bit, Application software, Arithmetic, CPU2006 integer programs, Computer architecture, Hardware, Microcomputers, Multimedia databases, Operating systems, Performance analysis, Programming profession, Registers, SPEC CPU2006 integer benchmarks, instruction sets, memory address space, microprocessor chips, performance evaluation, x86-64 architecture, x86-64 processors, },
 abstract = {As x86-64 processors become the CPU of choice for the personal computer market, it becomes increasingly important to understand the performance we can expect by migrating applications from a 32-bit environment to a 64-bit environment. For applications that can effectively exploit a larger memory address space (e.g., commercial databases and digital content authoring tools), it is not surprising that x86-64 can provide a performance boost. However, for less-demanding desktop applications that can fit in a 32-bit address space, we would like to know if we can expect any performance benefits by moving to this platform. In this paper, we report on a range of performance characteristics for programs compiled for both 32 bits and 64 bits and run directly (32-bit binaries are run in compatibility mode; 64-bit binaries are run in 64-bit mode) on a single x86-64 based system. In this study we utilize the integer benchmarks from the newly released SPEC CPU2006 suite. We have observed that for the SPEC CPU2006 integer benchmarks, 64-bit mode offers a sizable performance advantage over 32-bit mode (7\% on average). However, the advantages vary from benchmark to benchmark, and for a handful of programs, 64-bit mode is significantly slower than 32-bit mode (in this subset of benchmarks, performance is reduced by more than 16\% when running in 64-bit mode.) We further analyze 5 benchmarks that exhibit significant differences in performance between these two modes. For this set of CPU2006 integer programs, we present a range of performance characteristics that illustrate the impact of moving to a 64-bit environment. Our results and analysis can be used by performance engineers and developers to better understand how to exploit the capabilities of the x86-64 architecture },
}

@inproceedings{5649419,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Fortuna, E. and Anderson, O. and Ceze, L. and Eggers, S.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {A limit study of JavaScript parallelism},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649419},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649419},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649419.pdf?arnumber=5649419},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Benchmark testing, Cryptography, Facebook, Internet, Java, JavaScript engine, JavaScript parallelisation, Switches, Visualization, Web pages, YouTube, authoring languages, dependency types, looping behavior, optimisation, optimizations, search engines, ubiquitous computing, ubiquitous computing, virtual registers, },
 abstract = {JavaScript is ubiquitous on the web. At the same time, the language's dynamic behavior makes optimizations challenging, leading to poor performance. In this paper we conduct a limit study on the potential parallelism of JavaScript applications, including popular web pages and standard JavaScript benchmarks. We examine dependency types and looping behavior to better understand the potential for JavaScript parallelization. Our results show that the potential speedup is very encouraging- averaging 8.9x and as high as 45.5x. Parallelizing functions themselves, rather than just loop bodies proves to be more fruitful in increasing JavaScript execution speed. The results also indicate in our JavaScript engine, most of the dependencies manifest via virtual registers rather than hash table lookups. },
}

@inproceedings{4086142,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Thaker, D.D. and Franklin, D. and Oliver, J. and Biswas, S. and Lockhart, D. and Metodi, T. and Chong, F.T.},
 year = {2006},
 pages = {142--149},
 publisher = {IEEE},
 title = {Characterization of Error-Tolerant Applications when Protecting Control Data},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302738},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086142},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086142.pdf?arnumber=4086142},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Circuit faults, Control systems, Costs, Decoding, Error correction, Image coding, Latches, Microarchitecture, Protection, Time measurement, architectural vulnerability factor, control data, control operation, control protection, fault tolerant computing, program control structures, program diagnostics, soft-error tolerance, static analysis, system behavior, },
 abstract = {Soft errors have become a significant concern and recent studies have measured the "architectural vulnerability factor" of systems to such errors, or conversely, the potential that a soft error is masked by latches or other system behavior. We take soft-error tolerance one step further and examine when an application can tolerate errors that are not masked. For example, a video decoder or approximation algorithm can tolerate errors if the user is willing to accept degraded output. The key observation is that while the decoder can tolerate error in its data, it can not tolerate error in its control. We first present static analysis that protects most control operations. We examine several SPEC CPU2000 and MiBench benchmarks for error tolerance, develop fidelity measures for each, and quantify the effect of errors on fidelity. We show that protecting control is crucial to producing error-tolerance, for without this protection, many applications experience catastrophic errors (infinite execution time or crashing). Overall, our results indicate that with simple control protection, the error tolerance of many applications can provide designers with considerable added flexibility when considering future challenges posed by soft errors },
}

@inproceedings{4086143,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Yoo, R.M. and Han Lee and Kingsum Chow and Lee, H.-H.S.},
 year = {2006},
 pages = {150--159},
 publisher = {IEEE},
 title = {Constructing a Non-Linear Model with Neural Networks for Workload Characterization},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302739},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086143},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086143.pdf?arnumber=4086143},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Accuracy, Application software, Artificial neural networks, Computer networks, Delay, Neural networks, Performance analysis, Predictive models, Runtime, Software performance, artificial neural network, complex multitiered workloads, neural nets, nonlinear model, performance characteristics, performance evaluation, workload behavior complexity, workload characterization, workload configuration, },
 abstract = {Workload characterization involves the understanding of the relationship between workload configurations and performance characteristics. To better assess the complexity of workload behavior, a model based approach is needed. Nevertheless, several configuration parameters and performance characteristics exhibit non-linear relationships that prohibit the development of an accurate application behavior model. In this paper, we propose a non-linear model based on an artificial neural network to explore such complex relationship. We achieved high accuracy and good predictability between configurations and performance characteristics when applying such a model to a 3-tier setup with response time restrictions. As shown by our work, a non-linear model and neural networks can increase the understandings of complex multi-tiered workloads, which further provide useful insights for performance engineers to tune their workloads for improving performance },
}

@inproceedings{809353,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {},
 year = {1999},
 publisher = {IEEE},
 title = {Workload Characterization: Methodology and Case Studies. Based on the First Workshop on Workload Characterization},
 date = {1999},
 doi = {10.1109/WWC.1998.809353},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809353},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809353.pdf?arnumber=809353},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = { I/O,  Java,  Java,  Web server workloads,  data mining,  data mining,  desktop workloads,  file servers,  graphics workloads,  information resources,  memory architecture,  memory characterization,  performance evaluation,  program diagnostics,  storage management,  workload characterization, Data engineering, Data mining, Graphics, Java, Web server, },
 abstract = {The following topics are dealt with: workload characterization; Java and graphics workloads; data mining and Web server workloads; I/O and memory characterization; and scientific, engineering and desktop workloads },
}

@inproceedings{1437403,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Ali, R. and Radhakrishnan, R. and Kochhar, G. and Hsieh, J. and Celebioglu, O. and Chadalavada, K. and Rajagopalan, R.},
 year = {2004},
 pages = { 81-- 88},
 publisher = {IEEE},
 title = {Evaluating performance of BLAST on Intel Xeon and Itanium2 processors},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437403},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437403},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437403.pdf?arnumber=1437403},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { BLAST,  Basic Local Alignment Search Tool,  Dell PowerEdge servers,  IA-32 based Xeon system,  Intel Itanium 2 processor,  Intel Xeon processor,  Intel architecture-based servers,  PE1750,  PE1850,  PE3250,  biological research,  biology computing,  file servers,  high-performance computing,  memory technology,  microprocessor chips,  multiprocessing systems,  performance evaluation,  performance evaluation, Bandwidth, Computer aided instruction, Computer architecture, Computer industry, Frequency, Linux, Random access memory, Supercomputers, Technological innovation, Web pages, },
 abstract = {High-performance computing has increasingly adopted the use of clustered Intel architecture-based servers. This increase in adoption has been largely fueled by a number of technological enhancements in the Intel architecture-based servers, primarily due to substantial improvement in the Intel processor and memory technology over the past few years. This paper compares the performance characteristics of three Dell PowerEdge (PE) servers that are based on three different Intel processor technologies. They are the PE1750 which is an IA-32 based Xeon system, PE1850 which uses the new 90nm technology Xeon processor at faster frequencies and the PE3250 which is an Itanium2 based system. BLAST (Basic Local Alignment Search Tool), a high performance computing application used in the field of biological research, is used as the workload for this study. The aim is to understand the performance impact of the different features associated with each processor/platform technology when running the BLAST workload. },
}

@inproceedings{4086148,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Iyer, R. and Bhat, M. and Zhao, L. and Illikkal, R. and Makineni, S. and Jones, M. and Shiv, K. and Newell, D.},
 year = {2006},
 pages = {191--200},
 publisher = {IEEE},
 title = {Exploring Small-Scale and Large-Scale CMP Architectures for Commercial Java Servers},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302744},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086148},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086148.pdf?arnumber=4086148},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Bandwidth, Computer architecture, Emulation, Enterprise Java workload, Java, Java, Laboratories, Large-scale systems, Out of order, Performance analysis, Random access memory, SPECjbb2005, Throughput, chip multiprocessor architecture, commercial Java servers, large-scale CMP architecture, mainstream server workloads, multiprocessing systems, parallel architectures, small-scale CMP architecture, },
 abstract = {As we enter the era of chip multiprocessor (CMP) architectures, it is important that we explore the scaling characteristics of mainstream server workloads on these platforms. In this paper, we analyze the performance of an Enterprise Java workload (SPECjbb2005) on two important classes of CMP architectures. One class of CMP platforms comprise of "small-scale" CMP (SCMP) processors with a few large out-of order cores on the die. Another class of CMP platforms comprise of "large-scale" CMP (LCMP) processors) with several small in-order cores on the die. For these classes of CMP architectures to succeed, it is important that there are sufficient resources (cache, memory and interconnect) to allow for a balanced scalable platform. In this paper, we focus on evaluating the resource scaling characteristics (cores, caches and memory) of SPECjbb2005 on these two architectures and understanding architectural trade-offs that may be required in future CMP offerings. The overall evaluation is uniquely conducted using four different methodologies (measurements on latest platforms, trace-based cache simulation, trace-based platform simulation and execution-driven emulation). Based on our findings, we summarize the architectural recommendations for future CMP server platforms (e.g. the need for large DRAM caches) },
}

@inproceedings{4086149,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Binder, W. and Hulaas, J. and Moret, P.},
 year = {2006},
 pages = {201--209},
 publisher = {IEEE},
 title = {A Quantitative Evaluation of the Contribution of Native Code to Java Workloads},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302745},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086149},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086149.pdf?arnumber=4086149},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Hardware, Informatics, Instruments, JVM Tool Interface, Java, Java, Java Native Interface, Java workloads, Libraries, Performance analysis, Runtime, Software performance, Time measurement, Virtual machining, bytecode instrumentation, hardware performance counters, native code libraries, software libraries, },
 abstract = {Many performance analysis tools for Java focus on tracking executed bytecodes, but provide little support in determining the specific contribution of native code libraries. This paper introduces and assesses a portable approach for characterizing the amount of native code executed by Java applications. A profiling agent based on the JVM Tool Interface (JVMTI) accurately keeps track of all runtime transitions between bytecode and native code. It relies on a combination of JVMTI events, Java Native Interface (JNI) function interception, bytecode instrumentation, and hardware performance counters },
}

@inproceedings{809357,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Poursepanj, A. and Christie, D.},
 year = {1999},
 pages = {36--45},
 publisher = {IEEE},
 title = {Generation of 3D graphics workload for system performance analysis },
 date = {1999},
 doi = {10.1109/WWC.1998.809357},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809357},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809357.pdf?arnumber=809357},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {3D graphics workload generation, Advanced Graphics Port, CPU instruction trace, Character generation, Delay, Graphics, Logic devices, PC system architecture, PCI bus, Performance analysis, Protocols, Research and development, System performance, Telecommunication traffic, Timing, bus mastering devices, bus protocols, bus traces, computer graphic equipment, computer graphics, contiguous trace length, logic analyser buffer size, logic analysers, memory access, performance evaluation, performance model, processor performance models, synthetic graphics traces, system behaviour characterization, system buses, system configuration, system performance analysis, timing, trace-driven models, },
 abstract = {Generation of representative workloads for system performance models has been a challenge for PC system architects who are using trace-driven models. Unlike processor performance models that typically only use a single CPU instruction trace, system models in most cases require traces of CPU, Advanced Graphics Port (AGP), PCI and other bus mastering devices that can access memory. A common approach is to collect bus traces with a logic analyzer. Although this allows the generation of realistic traces, typical analyzer buffer sizes seriously limit the length of contiguous traces. Another problem is that traces collected in a specific system configuration may not be representative of other systems, especially future systems with different timings and/or bus protocols. This paper presents an overview of an approach that can be used to generate long bus traces for performance model stimulus. We describe methods for the characterization of system behavior and for the generation of accurate synthetic graphics traces based on real traces, and give examples of correlated CPU and AGP traces that are synthetic but reflect the characteristics of real CPU/AGP traces },
}

@inproceedings{809356,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Conte, M.T. and Trick, A.R. and Gyllenhaal, J.C. and Hwu, W.W.},
 year = {1999},
 pages = {27--35},
 publisher = {IEEE},
 title = {A study of code reuse and sharing characteristics of Java applications},
 date = {1999},
 doi = {10.1109/WWC.1998.809356},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809356},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809356.pdf?arnumber=809356},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Application software, Content management, Independent component analysis, Internet, Internet searches, Java, Java, Java applets, Java applications, Joining processes, Read only memory, Runtime, SPECjvm98 benchmarks, Virtual machining, Virtual manufacturing, class level, code equivalence, code optimization, code reuse, code sharing, constant pool index differences, distributed programming, enhanced Web crawler, information resources, method level, minor code changes, object-oriented programming, optimising compilers, program level, software performance evaluation, software reusability, workload characterization, },
 abstract = {Presents a detailed characterization of Java application and applet workloads in terms of reuse and sharing of Java code at the program, class and method level. In order to expose more sharing opportunities, techniques for detecting code equivalence (even in the presence of minor code changes or constant pool index differences) are also proposed and examined. The analyzed application workload consists of the recently released SPECjvm98 benchmarks, and the applet workload is derived from three extensive searches of the Internet between May 1997 and May 1998 using an enhanced Web crawler. Analysis of these workloads reveals several new code sharing and optimization opportunities },
}

@inproceedings{5650280,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Ueda, Y. and Nakatani, T.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Performance variations of two open-source cloud platforms},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650280},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650280},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650280.pdf?arnumber=5650280},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Apache day trader benchmark, Eucalyptus, Internet, NFS configuration, OpenNebula, Web transaction, Wikipedia data, Wikipedia software, batch processing, batch processing (computers), eager-allocation, lazy-allocation, open source cloud platform, public domain software, software metrics, transaction processing, virtual machine disk image, virtual machines, },
 abstract = {The performance of workloads running on cloud platforms varies significantly depending on the cloud platform configurations. We evaluated the performance variations using two open-source cloud platforms, OpenNebula and Eucalyptus. To assess the performance variations on the cloud platforms, we created a representative workload from Wikipedia software and data. The performance with this workload was quite sensitive to two key configuration choices, the physical location of the virtual machine disk images (local disk or NFS), and eager or lazy allocation of the virtual machine disk images. Our performance metrics included (1) the provisioning times for the virtual machines, (2) the elapsed times for two types of batch processing, and (3) the throughputs of two types of Web transactions. The local-disk configuration was 75\% slower for provisioning, 2.9 times faster for batch possessing, and 50\% faster for Web transactions compared to the NFS configuration. Relative to lazy-allocation, eager-allocation took 2.7 times longer for provisioning and was 43\% faster for batch processing, but was only 1.5\% faster for Web transactions. Our results indicate that no configuration offers the best performance for all three of the metrics at the same time. If batch processing is more important than provisioning, the local-disk configuration with eager disk allocation should be used. Otherwise, local-disk allocation with lazy allocation should be used. We also evaluated a multi-tenancy scenario using the Apache Day Trader benchmark on Eucalyptus. The results show that VM provisioning significantly affected the throughputs of Day Trader due to the lack of any disk I/O throttling mechanism. },
}

@inproceedings{1525990,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {John, L.K.},
 year = {2005},
 pages = { iii-- iii},
 publisher = {IEEE},
 title = {Message from the General Chair},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525990},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525990},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525990.pdf?arnumber=1525990},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{5648864,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Ren-Shuo Liu and Yun-Cheng Tsai and Chia-Lin Yang},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Parallelization and characterization of GARCH option pricing on GPUs},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5648864},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5648864},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05648864.pdf?arnumber=5648864},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {4-core CPU, Arrays, CUDA, CUDA, Computational modeling, GARCH, GARCH, GPU, GPU, Graphics processing unit, Instruction sets, Mean Tracking, Optimization, Option Pricing, Pricing, Tree, Tree data structures, autoregressive processes, computer graphic equipment, coprocessors, floating point arithmetic, floating point arithmetic, fusion optimization, multi-threaded program, optimisation, option pricing, parallel architectures, pricing, stochastic volatility, tree based simulations, tree data structure, tree data structures, },
 abstract = {Option pricing is an important problem in computational finance due to the fast-growing market and increasing complexity of options. For option pricing, a model is required to describe the price process of the underlying asset. The GARCH model is one of the prominent option pricing models since it can model stochastic volatility of the underlying asset. To derive expected profit based on the GARCH model, tree-based simulations are one of the commonly used approaches. Tree-based GARCH option pricing is computing intensive since the tree grows exponentially, and it requires enormous floating point arithmetic operations. In this paper, we present the first work on accelerating the tree-based GARCH option pricing on GPUs with CUDA. As the conventional tree data structure is not memory access friendly to GPUs, we propose a new family of tree data structures which position concurrently accessed nodes in contiguous and aligned memory locations. Moreover, to reduce memory bandwidth requirement, we apply fusion optimization, which combines two threads into one to keep data with temporal locality in register files. Our results show 50\&#x00D7; speedup compared to a multi-threaded program on a 4-core CPU. },
}

@inproceedings{5649499,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Yaozu Dong and Xudong Zheng and Xiantao Zhang and Jinquan Dai and Jianhui Li and Xin Li and Gang Zhai and Haibing Guan},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Improving virtualization performance and scalability with advanced hardware accelerations},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649499},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649499},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649499.pdf?arnumber=5649499},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Acceleration, Artificial intelligence, Driver circuits, EPT, Hardware, Iron, Optimization, PLE, SR-IOV, Scalability, Virtual Machine, Virtualization, Xen, advanced hardware acceleration, benchmark testing, microbenchmark, multicore system, multiprocessing systems, optimisation, optimization, overcommitted system, scalability, server consolidation benchmark, vConsolidate, virtual machine, virtual machines, virtualization performance, },
 abstract = {Many advanced hardware accelerations for virtualization, such as Pause Loop Exit (PLE), Extended Page Table (EPT), and Single Root I/O Virtualization (SR-IOV), have been introduced recently to improve the virtualization performance and scalability. In this paper, we share our experience with the performance and scalability issues of virtualization, especially those brought by the modern, multi-core and/or overcommitted systems. We then describe our work on the implementation and optimizations of the advanced hardware acceleration support in the latest version of Xen. Finally, we present performance evaluations and characterizations of these hardware accelerations, using both micro-benchmarks and a server consolidation benchmark (vConsolidate). The experimental results demonstrate an up to 77\% improvement with these hardware accelerations, 49\% of which is due to EPT and another 28\% due to SR-IOV. },
}

@inproceedings{4086141,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Tsafrir, D. and Feitelson, D.G.},
 year = {2006},
 pages = {131--141},
 publisher = {IEEE},
 title = {The Dynamics of Backfilling: Solving the Mystery of Why Increased Inaccuracy May Help},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302737},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086141},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086141.pdf?arnumber=4086141},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Clustering algorithms, Computer science, Dynamic scheduling, Performance evaluation, Planing, Processor scheduling, Runtime, Scheduling algorithm, Supercomputers, Yield estimation, backfilling, heel and toe dynamics, parallel job scheduling, parallel processing, processor scheduling, shortest-job first scheduling, },
 abstract = {Parallel job scheduling with backfilling requires users to provide runtime estimates, used by the scheduler to better pack the jobs. Studies of the impact of such estimates on performance have modeled them using a "badness factor" f ges 0 in an attempt to capture their inaccuracy (given a runtime r, the estimate is uniformly distributed in [r, (f + 1) middot r]). Surprisingly, inaccurate estimates (f \&gt; 0) yielded better performance than accurate ones (f = 0). We explain this by a "heel and toe" dynamics that, with f \&gt; 0, cause backfilling to approximate shortest-job first scheduling. We further find the effect of systematically increasing f is V-shaped: average wait time and slowdown initially drop, only to rise later on. This happens because higher fs create bigger "holes" in the schedule (longer jobs can backfill) and increase the randomness (more long jobs appear as short), thus overshadowing the initial heel-and-toe preference for shorter jobs. The bottom line is that artificial inaccuracy generated by multiplying (real or perfect) estimates by a factor is (1) just a scheduling technique that trades off fairness for performance, and is (2) ill-suited for studying the effect of real inaccuracy. Real estimates are modal (90\% of the jobs use the same 20 estimates) and bounded by a maximum (usually the most popular estimate). Therefore, when performing an evaluation, "increased inaccuracy" should translate to increased modality. Unlike multiplying, this indeed worsens performance as one would intuitively expect },
}

@inproceedings{4636097,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Kavalanekar, S. and Worthington, B. and Qi Zhang and Sharda, V.},
 year = {2008},
 pages = {119--128},
 publisher = {IEEE},
 title = {Characterization of storage workload traces from production Windows Servers},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636097},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636097},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636097.pdf?arnumber=4636097},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Authentication, Data security, Displays, Instruments, Internet, MSN storage, Microsoft Corporation production servers, Production, Secure storage, Software tools, Statistical analysis, Statistical distributions, Visualization, Windows Performance Tools kit, Windows server 2008, block-level statistics, data visualisation, display advertisement platform servers, event tracing for windows instrumentation, file access frequency, file servers, live maps, message authentication, multiparameter distributions, operating systems (computers), production windows servers, security authentication, statistical analysis, storage industry, storage management, storage workload traces, trace capture, trace data visualizations, visualization tools, },
 abstract = {The scarcity of publicly available storage workload traces of production servers impairs characterization, modeling research, and development efforts across the storage industry. Twelve sets of storage traces from a diverse set of Microsoft Corporation production servers were captured using ETW (event tracing for windows) instrumentation. Windows server 2008 dramatically increases the breadth and depth of ETW instrumentation, and new trace capture and visualization tools are available in the Windows Performance Tools kit. Additional analytical tools were developed to analyze and visualize traces captured from Exchange, software build and release, Live Maps, MSN storage, security authentication, and display advertisement platform servers. This paper contains a first set of characterizations for these traces, including simple block-level statistics, multi-parameter distributions, rankings of file access frequencies, and more complex analyses such as temporal and spatial self-similarity measurements. Trace data visualizations enable the examination of workload parameters, subcomponents, phases, and deviations from predicted behavior. },
}

@inproceedings{4636096,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Nagpurkar, P. and Horn, W. and Gopalakrishnan, U. and Dubey, N. and Jann, J. and Pattnaik, P.},
 year = {2008},
 pages = {109--118},
 publisher = {IEEE},
 title = {Workload characterization of selected JEE-based Web 2.0 applications},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636096},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636096},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636096.pdf?arnumber=4636096},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Blogs, Collaboration, Disaster management, File servers, IP networks, Information resources, Internet, JEE-based Web 2.0 applications, Java, Mashups, Network servers, Social network services, Web 2.0 development, Web server, Web-based communities, content aggregation, portal-based technologies, rich Internet applications, social networking, workload characterization, },
 abstract = {Web 2.0 represents the evolution of the web from a source of information to a platform. Network advances have permitted users to migrate from desktop applications to so-called Rich Internet Applications (RIAs) characterized by thin clients, which are browser-based and store their state on managed servers. Other Web 2.0 technologies have enabled users to more easily participate, collaborate, and share in web-based communities. With the emergence of wikis, blogs, and social networking, users are no longer only consumers, they become contributors to the collective knowledge accessible on the web. In another Web 2.0 development, content aggregation is moving from portal-based technologies to more sophisticated so-called mashups where aggregation capabilities are greatly expanded. While Web 2.0 has generated a great deal of interest and discussion, there has not been much work on analyzing these emerging workloads. This paper presents a detailed characterization of several applications that exploit Web 2.0 technologies, running on an IBM Power5 system, with the goal of establishing, whether the server-side workloads generated by Web 2.0 applications are significantly different from traditional web workloads, and whether they present new challenges to underlying systems. In this paper, we present a detailed characterization of three Web 2.0 workloads, and a synthetic benchmark representing commercial workloads that do not exploit Web 2.0, for comparison. },
}

@inproceedings{4636095,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Wenisch, T.F. and Ferdman, M. and Ailamaki, A. and Falsafi, B. and Moshovos, A.},
 year = {2008},
 pages = {99--108},
 publisher = {IEEE},
 title = {Temporal streams in commercial server applications},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636095},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636095},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636095.pdf?arnumber=4636095},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Analytical models, Application software, Computer architecture, Information analysis, Kernel, Operating systems, Pattern analysis, Prefetching, Solar power generation, Traffic control, Web services, Web serving, commercial server, complex nonstrided access patterns, decision support workloads, file servers, frequent sharing, information analysis, memory accesses, memory system performance, multichip multiprocessors, multiprocessor systems, online transaction processing, recurring miss sequences, single-chip multiprocessors, temporal streams, },
 abstract = {Commercial server applications remain memory bound on modern multiprocessor systems because of their large data footprints, frequent sharing, complex non-strided access patterns, and long chains of dependant misses. To improve memory system performance despite these challenging access patterns, researchers have proposed prefetchers that exploit temporal streams-recurring sequences of memory accesses. Although prior studies show substantial performance improvement from such schemes, they fail to explain why temporal streams arise; that is, they treat commercial applications as a black box and do not identify the specific behaviors that lead to recurring miss sequences. In this paper, we perform an information-theoretic analysis of miss traces from single-chip and multi-chip multiprocessors to identify recurring temporal streams in web serving, online transaction processing, and decision support workloads. Then, using function names embedded in the application binaries and Solaris kernel, we identify the code modules and behaviors that give rise to temporal streams. },
}

@inproceedings{4636094,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Stewart, C. and Leventi, M. and Kai Shen},
 year = {2008},
 pages = {90--96},
 publisher = {IEEE},
 title = {Empirical examination of a collaborative web application},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636094},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636094},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636094.pdf?arnumber=4636094},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Application software, Collaborative work, Computer science, Differential equations, Internet, Online Communities/Technical Collaboration, Physics, Predictive models, Productivity, RUBiS, Resource management, SPECweb, Social network services, TPC-C, WeBWorK, Web content, Web sites, Wiki-based Web sites, collaborative Web application, computer aided instruction, empirical examination, groupware, math problems online, online benchmarks, online instructional applications, social networking sites, type-based resource usage prediction, },
 abstract = {Online instructional applications, social networking sites, Wiki-based Web sites, and other emerging Web applications that rely on end users for the generation of web content are increasingly popular. However, these collaborative Web applications are still absent from the benchmark suites commonly used in the evaluation of online systems. This paper argues that collaborative Web applications are unlike traditional online benchmarks, and therefore warrant a new class of benchmarks. Specifically, request behaviors in collaborative Web applications are determined by contributions from end users, which leads to qualitatively more diverse server-side resource requirements and execution patterns compared to traditional online benchmarks. Our arguments stem from an empirical examination of WeBWorK-a widely-used collaborative Web application that allows teachers to post math or physics problems for their students to solve online. Compared to traditional online benchmarks (like TPC-C, SPECweb, and RUBiS), WeBWorK requests are harder to cluster according to their resource consumption, and they follow less regular patterns. Further, we demonstrate that the use of a WeBWorK-style benchmark would probably have led to different results in some recent research studies concerning request classification from event chains and type-based resource usage prediction. },
}

@inproceedings{4636093,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Becchi, M. and Franklin, M. and Crowley, P.},
 year = {2008},
 pages = {79--89},
 publisher = {IEEE},
 title = {A workload for evaluating deep packet inspection architectures},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636093},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636093},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636093.pdf?arnumber=4636093},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Algorithm design and analysis, Character generation, Hardware, Inspection, Intrusion detection, Pattern matching, Proposals, Scalability, Telecommunication traffic, Traffic control, communication complexity, communication complexity, compressed deterministic finite automata, compressed nondeterministic finite automata, computer networks, data compression, deep packet inspection architecture, deterministic automata, finite automata, formal languages, high-performance regular expression pattern matching, high-speed content inspection, malicious network activity, network intrusion detection system, network traffic, programmable networking system, string matching, telecommunication security, telecommunication traffic, },
 abstract = {High-speed content inspection of network traffic is an important new application area for programmable networking systems, and has recently led to several proposals for high-performance regular expression matching. At the same time, the number and complexity of the patterns present in well-known network intrusion detection systems has been rapidly increasing. This increase is important since both the practicality and the performance of specific pattern matching designs are strictly dependent upon characteristics of the underlying regular expression set. However, a commonly agreed upon workload for the evaluation of deep packet inspection architectures is still missing, leading to frequent unfair comparisons, and to designs lacking in generality or scalability. In this paper, we propose a workload for the evaluation of regular expression matching architectures. The workload includes a regular expression model and a traffic generator, with the former characterizing different levels of expressiveness within rule-sets and the latter characterizing varying degrees of malicious network activity. The proposed workload is used here to evaluate designs (e.g., different memory layouts and hardware organizations) where the matching algorithm is based on compressed deterministic and non deterministic finite automata (DFAs and NFAs). },
}

@inproceedings{4636092,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Dixon, R. and Sherwood, T.},
 year = {2008},
 pages = {69--78},
 publisher = {IEEE},
 title = {Whiteboards that compute: A workload analysis},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636092},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636092},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636092.pdf?arnumber=4636092},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Application software, Character recognition, Circuit analysis computing, Circuit simulation, Computer science, Displays, Economic forecasting, Environmental economics, Equations, Image analysis, circuit simulations, mathematical analysis, mathematical equations, systems architects, user interfaces, whiteboard-sized devices, workload analysis, },
 abstract = {A whiteboard that automatically identifies drawn strokes, interprets them in context, and augments drawn images with computational results, such as solutions to mathematical equations or results of circuit simulations, is a surprisingly realistic goal for systems architects. In this paper we describe the state of this emerging domain and argue that technical trends will make this a particularly attractive workload in the future. We provide a preliminary characterization of the critical loops that exist within one state of the art system, currently undergoing development, and we attempt to quantify the workloads that whiteboard-sized devices are likely to face in the future. While this work is by no means a typical workload characterization paper, given the shift in programming models that we are about to endure, it is now more important than ever before to identify and understand those applications that have the potential to drive our industry forward. },
}

@inproceedings{4636091,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Contreras, G. and Martonosi, M.},
 year = {2008},
 pages = {57--66},
 publisher = {IEEE},
 title = {Characterizing and improving the performance of Intel Threading Building Blocks},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636091},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636091},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636091.pdf?arnumber=4636091},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {C++ language, C++ parallelization environment, Concurrent computing, Costs, Intel threading building block runtime library, Parallel processing, Parallel programming, Productivity, Programming profession, Robustness, Runtime library, Scalability, Yarn, chip multiprocessor, concurrency control, concurrency identification, dynamic parallel task redistribution, multi-threading, multiprocessing systems, parallel programming, random stealing, software libraries, system monitoring, },
 abstract = {The Intel threading building blocks (TBB) runtime library is a popular C++ parallelization environment (D. Bolton, 2007) that offers a set of methods and templates for creating parallel applications. Through support of parallel tasks rather than parallel threads, the TBB runtime library offers improved performance scalability by dynamically redistributing parallel tasks across available processors. This not only creates more scalable, portable parallel applications, but also increases programming productivity by allowing programmers to focus their efforts on identifying concurrency rather than worrying about its management. While many applications benefit from dynamic management of parallelism, dynamic management carries parallelization overhead that increases with increasing core counts and decreasing task sizes. Understanding the sources of these overheads and their implications on application performance can help programmers make more efficient use of available parallelism. Clearly understanding the behavior of these overheads is the first step in creating efficient, scalable parallelization environments targeted at future CMP systems. In this paper we study and characterize some of the overheads of the Intel Threading Building Blocks through the use of real-hardware and simulation performance measurements. Our results show that synchronization overheads within TBB can have a significant and detrimental effect on parallelism performance. Random stealing, while simple and effective at low core counts, becomes less effective as application heterogeneity and core counts increase. Overall, our study provides valuable insights that can be used to create more robust, scalable runtime libraries. },
}

@inproceedings{4636090,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Bienia, C. and Kumar, S. and Kai Li},
 year = {2008},
 pages = {47--56},
 publisher = {IEEE},
 title = {PARSEC vs. SPLASH-2: A quantitative comparison of two multithreaded benchmark suites on Chip-Multiprocessors},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636090},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636090},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636090.pdf?arnumber=4636090},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Acceleration, Application software, Computer science, Machine learning, Microprocessors, Multithreading, PARSEC, Parallel machines, SPLASH, Software measurement, Software performance, Statistical analysis, benchmark suite, chip-multiprocessors, microprocessor chips, multi-threading, multithreaded benchmark, multithreading, performance measurement, quantitative comparison, shared memory systems, shared-memory computers, statistical analysis, },
 abstract = {The PARSEC benchmark suite was recently released and has been adopted by a significant number of users within a short amount of time. This new collection of workloads is not yet fully understood by researchers. In this study we compare the SPLASH-2 and PARSEC benchmark suites with each other to gain insights into differences and similarities between the two program collections. We use standard statistical methods and machine learning to analyze the suites for redundancy and overlap on chip-multiprocessors (CMPs). Our analysis shows that PARSEC workloads are fundamentally different from SPLASH-2 benchmarks. The observed differences can be explained with two technology trends, the proliferation of CMPs and the accelerating growth of world data. },
}

@inproceedings{1226497,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Zhen Fang and McKee, S.A.},
 year = {2002},
 pages = { 91-- 97},
 publisher = {IEEE},
 title = {MEPEG-4: fallacies and paradoxes},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226497},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226497},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226497.pdf?arnumber=1226497},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { IEC standards,  ISO standards,  ISO/IEC 14496,  MPEG-4,  coding,  computational models,  decoding,  encoding,  international standard,  memory-system optimizations,  storage management,  video coding, },
 abstract = {MPEG-4 is an important international standard with wide applicability. This paper focuses on MPEG-4's main profile, video, whose approach allows more efficiency in coding and more flexibility in managing heterogeneous media objects than previous MPEG standards. This brief study presents evidence to support the assertion that for non-SIMD architectures and computational models, most memory-system optimizations will have little effect on MPEG-4 performance. This paper makes two contributions. First, it serves as an independent confirmation that for current, general-purpose architectures, MPEG-4 video is computation bound (just like most other media processing applications). Second, our findings should prove useful to other researchers and practitioners considering how to (or how not to) optimize MPEG-4 performance. },
}

@inproceedings{1226496,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Rajan, A.S. and Shiwen Hu and Rubio, J.},
 year = {2002},
 pages = { 81-- 90},
 publisher = {IEEE},
 title = {Cache performance in Java virtual machines: a study of constituent phases},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226496},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226496},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226496.pdf?arnumber=1226496},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { JIT execution mode,  Java,  Java programs,  Latte Java virtual machine,  SPECjvm98 applications,  cache configurations,  cache performance,  cache storage,  code installation,  data cache write miss rates,  data write miss rate,  garbage collection,  garbage collector,  instruction cache,  memory reference traces,  performance evaluation,  pointer chasing,  storage management,  virtual machines, },
 abstract = {This paper studies the level 1 cache performance of Java programs by analyzing memory reference traces of the SPECjvm98 applications executed by the Latte Java virtual machine. We study in detail Java programs' cache performance of different access types in three JVM phases, under two execution modes, using three cache configurations and two application data sets. We observe that the poor data cache performance in the JIT execution mode is caused by code installation, when the data write miss rate in the execution engine can be as high as 70\%. In addition, code installation also deteriorates instruction cache performance during execution of translated code. High cache miss rate in garbage collection is mainly caused by large working set and pointer chasing of the garbage collector. A larger data cache works better on eliminating data cache read misses than write misses, and is more efficient on improving cache performance in the execution engine than in the garbage collection. As application data set increases in the JIT execution mode, instruction cache and data cache write miss rates of the execution engine decrease, while data cache read miss rate of the execution engine increases. On the other hand, impact of varying data set on cache performance is not as pronounced in the interpreted mode as in the JIT mode. },
}

@inproceedings{1226495,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Chen, G. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M.J.},
 year = {2002},
 pages = { 71-- 80},
 publisher = {IEEE},
 title = {PennBench: a benchmark suite for embedded Java},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226495},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226495},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226495.pdf?arnumber=1226495},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Java,  Java-enabled handsets,  PDA devices,  PennBench benchmark suite,  cell phone like devices,  embedded Java,  embedded systems,  heap footprints,  machine-independent characterization,  memory characteristics,  memory size constraints,  mobile computing,  notebook computers,  performance,  power budgets,  software performance evaluation, },
 abstract = {Currently, there are 23 million Java-enabled handsets with more than 50 different models from 17-plus suppliers. With the growing popularity of such devices, there is a need in the embedded industry for a set of applications for accurate indications of the performance of embedded Java solutions. To address this problem, we gather a set of 12 Java applications running on PDA and cell phone like devices. We present a machine-independent characterization of the applications, specifically focusing on their memory characteristics. Our focus is motivated by the criticality of memory size constraints and heap footprints on both performance and power budgets. },
}

@inproceedings{1226494,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Mehis, A. and Radhakrishnan, R.},
 year = {2002},
 pages = { 59-- 67},
 publisher = {IEEE},
 title = {Optimizing applications for performance on the pentium 4 architecture},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226494},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226494},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226494.pdf?arnumber=1226494},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Intel Pentium 4 architecture,  Pentium III architectures,  benchmarks,  compiler optimizations,  microprocessor chips,  optimising compilers,  performance enhancing design features,  performance evaluation,  performance evaluation, },
 abstract = {In this paper we characterize the performance impact of using advanced compiler optimizations on the Intel Pentium 4 (P4) processor. Using the Intel C++/FORTRAN compilers we show that on a variety of benchmarks, advanced compiler optimizations are required to improve performance on the P4 processor. For applications developed using advanced optimizations targeting the earlier PentiumPro through Pentium III architectures, recompilation is likely required to obtain and/or maximize performance improvements on the P4. The performance enhancing design features of the P4 although dynamic in nature, require that applications be recompiled using P4 architecture aware compilers to obtain performance improvements. },
}

@inproceedings{1226493,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Rupley, J., II and Annavaram, M. and De Vale, J. and Diep, T. and Black, B.},
 year = {2002},
 pages = { 49-- 58},
 publisher = {IEEE},
 title = {Comparing and contrasting a commercial OLTP workload with CPU2000 on IPF},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226493},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226493},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226493.pdf?arnumber=1226493},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { CPU2000,  Itanium Processor Family microprocessors,  Oracle Database Benchmark,  SPEC CPU2000,  bundle constraints,  cache misses,  code mix,  code stream,  commercial OLTP workload,  control flow predictability,  data supply,  enterprise servers,  instruction supply,  large active branch footprint,  large memory footprint,  memory operations,  microcomputers,  online transaction processing workload,  program compilers,  register stack engine,  relational databases,  server class applications,  transaction processing,  value locality,  virtual machines, },
 abstract = {With the recent introduction of Itanium Processor Family (IPF) microprocessors for enterprise servers it is imperative to understand the behavior of server class applications. This paper analyzes the behavior of the Oracle Database Benchmark (ODB), an online transaction processing (OLTP) workload, and compares it with SPEC CPU2000. This study examines code mix, instruction and data supply, and value locality. The results show that while IPF's bundle constraints cause a large injection of NOPs into the code stream, IPFs register stack engine successfully reduces the number of memory operations by nearly 50\%. The control-flow predictability of ODB is better than CPU2000, in spite of ODB's large active branch footprint. Due to ODB's large memory footprint, cache misses (particularly instruction cache misses) are a much more serious problem than in CPU2000. },
}

@inproceedings{1226492,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Stets, R. and Gharachorloo, K. and Barroso, L.A.},
 year = {2002},
 pages = { 37-- 48},
 publisher = {IEEE},
 title = {A detailed comparison of two transaction processing workloads},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226492},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226492},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226492.pdf?arnumber=1226492},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Alpha multiprocessors,  Oracle commercial database engine,  TPC-B benchmark,  TPC-C benchmark,  Web servers,  architectural metrics,  computer architecture,  cycles per instruction,  databases,  debit-credit transaction processing workloads,  dirty miss frequency,  file servers,  full system simulations,  high-performance servers,  inefficient executions,  memory stall component,  online transaction processing workloads,  order-entry transaction processing workloads,  relational databases,  transaction processing, },
 abstract = {Commercial applications such as databases and Web servers constitute the most important market segment for high-performance servers. Among these applications, online transaction processing (OLTP) workloads provide a challenging set of requirements for system designs since they often exhibit inefficient executions dominated by a large memory stall component. A number of recent studies have characterized the behavior of transaction processing workloads and proposed architectural features to improve their performance. These studies have typically used a workload based on either the TPC-B or the TPC-C benchmark, with many of them opting for the simpler TPC-B benchmark. Given that the TPC-B and TPC-C workloads exhibit dramatically different characteristics on certain architectural metrics (such as cycles-per-instruction), it becomes important to find out whether the results or conclusions of these previous studies are heavily biased due to their choice of workload. This paper presents a detailed comparison of the debit-credit (modeled after TPC-B) and order-entry (modeled after TPC-C) transaction processing workloads in the context of various architectural choices. Our experiments use the Oracle commercial database engine for running the workloads, with results generated using both full system simulations and actual runs on Alpha multiprocessors. Our results confirm that certain characteristics of these workloads, such as cycles-per-instruction (CPI) and dirty miss frequency, are indeed quite different. Nonetheless, it turns out that the overall impact of most architectural choices (e.g., out-of-order execution, on-chip integration of system modules, chip multiprocessing) are surprisingly similar for the two workloads. Furthermore, the above similarity between the two workloads is sometimes due to non-intuitive effects that would be difficult to predict without conducting the experiment with both workloads. The findings in this paper make it easier to compare results from studies that use one or the other workload. Overall, we observe that for a wide range of architectural decisions that we considered, using the simpler TPC-B workload leads to virtually the same conclusions as using the more complex TPC-C workload. Finally, we show that these same conclusions hold- across two generations of the Oracle database engine. },
}

@inproceedings{4636099,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Weaver, V.M. and McKee, S.A.},
 year = {2008},
 pages = {141--150},
 publisher = {IEEE},
 title = {Can hardware performance counters be trusted?},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636099},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636099},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636099.pdf?arnumber=4636099},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Aggregates, Counting circuits, Hardware, Instruments, Laboratories, SPEC CPU 2000, SPEC CPU 2006, SPEC benchmarks, Sampling methods, Semiconductor device measurement, Statistics, Systolic arrays, Time measurement, architectural tools, computer architecture, dynamic binary instrumentation tools, hardware performance counters, },
 abstract = {When creating architectural tools, it is essential to know whether the generated results make sense. Comparing a toolpsilas outputs against hardware performance counters on an actual machine is a common means of executing a quick sanity check. If the results do not match, this can indicate problems with the tool, unknown interactions with the benchmarks being investigated, or even unexpected behavior of the real hardware. To make future analyses of this type easier, we explore the behavior of the SPEC benchmarks with both dynamic binary instrumentation (DBI) tools and hardware counters. We collect retired instruction performance counter data from the full SPEC CPU 2000 and 2006 benchmark suites on nine different implementations of the times86 architecture. When run with no special preparation, hardware counters have a coefficient of variation of up to 1.07\%. After analyzing results in depth, we find that minor changes to the experimental setup reduce observed errors to less than 0.002\% for all benchmarks. The fact that subtle changes in how experiments are conducted can largely impact observed results is unexpected, and it is important that researchers using these counters be aware of the issues involved. },
}

@inproceedings{4636098,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Ruiz-Alvarez, A. and Hazelwood, K.},
 year = {2008},
 pages = {131--140},
 publisher = {IEEE},
 title = {Evaluating the impact of dynamic binary translation systems on hardware cache performance},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636098},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636098},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636098.pdf?arnumber=4636098},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Analytical models, Application software, Counting circuits, Degradation, DynamoRIO, Hardware, Instruments, Microarchitecture, Performance analysis, Performance evaluation, Pin, Security, cache storage, dynamic binary translation systems, hardware cache performance, program instrumentation, program interpreters, software code cache, software performance evaluation, },
 abstract = {Dynamic binary translation systems enable a wide range of applications such as program instrumentation, optimization, and security. DBTs use a software code cache to store previously translated instructions. The code layout in the code cache greatly differs from the code layout of the original program. This paper provides an exhaustive analysis of the performance of the instruction/trace cache and other structures of the micro-architecture while executing DBTs that focus on program instrumentation, such as DynamoRIO and Pin. We performed our evaluation along two axes. First, we directly accessed the hardware performance counters to determine actual cache miss counts. Second, we used simulation to analyze the spatial locality of the translated application. Our results show that when executing an application under the control of Pin or DynamoRIO, the icache miss counts actually increase over 2X. Surprisingly, the L2 cache and the L1 data cache show a much lower performance degradation or even break even with the native application. We also found that overall performance degradations are due to the instructions added by the DBT itself, and that these extra instructions outweigh any possible spatial locality benefits exhibited in the code cache. Our observations held regardless of the trace length, code cache size, or the presence of a hardware trace cache. These results provide a better understanding of the efficiency of current instrumentation tools and their effects on instruction/trace cache performance and other structures of the microarchitecture. },
}

@inproceedings{5650208,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Van Ertvelde, L. and Eeckhout, L.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Benchmark synthesis for architecture and compiler exploration},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650208},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650208},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650208.pdf?arnumber=5650208},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Benchmark testing, C language, Cloning, Computer architecture, Computer languages, Hardware, MiBench benchmark, Optimization, Program processors, benchmark synthesis, compiler exploration, compiler space, high level programming language, instruction set architecture, instruction sets, microarchitecture, optimization level, program compilers, proprietary information, software architecture, software plagiarism detection tool, synthetic benchmark, synthetic proxy, target system, three key feature, },
 abstract = {This paper presents a novel benchmark synthesis framework with three key features. First, it generates synthetic benchmarks in a high-level programming language (C in our case), in contrast to prior work in benchmark synthesis which generates synthetic benchmarks in assembly. Second, the synthetic benchmarks hide proprietary information from the original workloads they are built after. Hence, companies may want to distribute synthetic benchmark clones to third parties as proxies for their proprietary codes; third parties can then optimize the target system without having access to the original codes. Third, the synthetic benchmarks are shorter running than the original workloads they are modeled after, yet they are representative. In summary, the proposed framework generates small (thus quick to simulate) and representative benchmarks that can serve as proxies for other workloads without revealing proprietary information; and because the benchmarks are generated in a high-level programming language, they can be used to explore both the architecture and compiler spaces. The results obtained with our initial framework are promising. We demonstrate that we can generate synthetic proxy benchmarks for the MiBench benchmarks, and we show that they are representative across a range of machines with different instruction-set architectures, microarchitectures, and compilers and optimization levels, while being 30 times shorter running on average. We also verify using software plagiarism detection tools that the synthetic benchmark clones hide proprietary information from the original workloads. },
}

@inproceedings{1525998,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Alvarez, M. and Salami, E. and Ramirez, A. and Valero, M.},
 year = {2005},
 pages = { 24-- 33},
 publisher = {IEEE},
 title = {A performance characterization of high definition digital video decoding using H.264/AVC},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525998},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525998},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525998.pdf?arnumber=1525998},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { H.264 decoder,  H.264/AVC video coding standard,  SIMD optimized decoder,  computational complexity,  decoding,  hardware performance monitoring,  high definition digital video decoding,  multiprocessor architecture,  performance characterization,  prefetching,  video coding, Automatic voltage control, Bandwidth, Computational complexity, Decoding, Hardware, High definition video, MPEG 4 Standard, Monitoring, Performance analysis, Video coding, },
 abstract = {H.264/AVC is a new international video coding standard that provides higher coding efficiency with respect to previous standards at the expense of a higher computational complexity. The complexity is even higher when H.264/AVC is used in applications with high bandwidth and high quality like high definition (HD) video decoding. In this paper, we analyze the computational requirements of H.264 decoder with a special emphasis in HD video and we compare it with previous standards and lower resolutions. The analysis was done with a SIMD optimized decoder using hardware performance monitoring. The main objective is to identify the application bottlenecks and to suggest the necessary support in the architecture for processing HD video efficiently. We have found that H.264/AVC decoding of HD video perform many more operations per frame than MPEG-4 and MPEG-2, has new kernels with more demanding memory access patterns and has a lot data dependent branches that are difficult to predict. In order to improve the H.264/AVC decoding process at HD it is necessary to explore a better support in media instructions, specialized prefetching techniques and possibly, the use of some kind of multiprocessor architecture. },
}

@inproceedings{4086134,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Ozisikyilmaz, B. and Narayanan, R. and Zambreno, J. and Memik, G. and Choudhary, A.},
 year = {2006},
 pages = {61--70},
 publisher = {IEEE},
 title = {An Architectural Characterization Study of Data Mining and Bioinformatics Workloads},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302730},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086134},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086134.pdf?arnumber=4086134},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Algorithm design and analysis, Application software, Association rules, Bioinformatics, Computer science, Data engineering, Data mining, L1 cache miss rates, L2 cache miss rates, MineBench, Multimedia databases, Performance analysis, Streaming media, architectural characterization, association rule mining, benchmark testing, bioinformatics workloads, biology computing, branch misprediction rates, cache storage, data extraction, data mining, data mining, performance analysis, shared memory machine, shared memory systems, },
 abstract = {Data mining is the process of automatically finding implicit, previously unknown, and potentially useful information from large volumes of data. Advances in data extraction techniques have resulted in tremendous increase in the input data size of data mining applications. Data mining systems, on the other hand, have been unable to maintain the same rate of growth. Therefore, there is an increasing need to understand the bottlenecks associated with the execution of these applications in modern architectures. In this paper, we present MineBench, a publicly available benchmark suite containing fifteen representative data mining applications belonging to various categories: classification, clustering, association rule mining and optimization. First, we highlight the uniqueness of data mining applications. Subsequently, we evaluate the MineBench applications on an 8-way shared memory (SMP) machine and analyze important performance characteristics such as L1 and L2 cache miss rates, branch misprediction rates },
}

@inproceedings{1525999,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Man-Lap Li and Sasanka, R. and Adve, S.V. and Yen-Kuang Chen and Debes, E.},
 year = {2005},
 pages = { 34-- 45},
 publisher = {IEEE},
 title = {The ALPBench benchmark suite for complex multimedia applications},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525999},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525999},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525999.pdf?arnumber=1525999},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { ALPBench benchmark suite,  CMU Sphinx 3,  Inters SSE2 instructions,  MPEG-2 decode,  MPEG-2 encode,  POSIX threads,  Tachyon,  benchmark testing,  compiler optimizations,  complex multimedia applications,  data-level parallelism,  decoding,  face recognition,  face recognition,  instruction-level parallelism,  multi-threading,  multimedia computing,  ray tracing,  ray tracing,  speech recognition,  speech recognition,  sub-word SIMD,  thread-level parallelism,  video coding, Application software, Computer architecture, Decoding, Face recognition, Kernel, Parallel processing, Ray tracing, Speech recognition, Videoconference, Yarn, },
 abstract = {Multimedia applications are becoming increasingly important for a large class of general-purpose processors. Contemporary media applications are highly complex and demand high performance. A distinctive feature of these applications is that they have significant parallelism, including thread- , data-, and instruction-level parallelism, that is potentially well-aligned with the increasing parallelism supported by emerging multi-core architectures. Designing systems to meet the demands of these applications therefore requires a benchmark suite comprising these complex applications and that exposes the parallelism present in them. This paper makes two contributions. First, it presents ALPBench, a publicly available benchmark suite that pulls together five complex media applications from various sources: speech recognition (CMU Sphinx 3), face recognition (CSU), ray tracing (Tachyon), MPEG-2 encode (MSSG), and MPEG-2 decode (MSSG). We have modified the original applications to expose thread-level and data-level parallelism using POSIX threads and sub-word SIMD (Inters SSE2) instructions respectively. Second, the paper provides a performance characterization of the ALPBench benchmarks, with a focus on parallelism. Such a characterization is useful for architects and compiler writers for designing systems and compiler optimizations for these applications. },
}

@inproceedings{5650369,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Nohhyun Park and Lilja, D.J.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Characterizing datasets for data deduplication in backup applications},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650369},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650369},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650369.pdf?arnumber=5650369},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Dictionaries, Generators, Indexes, Measurement, Redundancy, Servers, Throughput, back-up procedures, backup application, cache storage, data compression, data compression, data deduplication, evaluation metric, index caching, redundancy, software metrics, storage system, },
 abstract = {The compression and throughput performance of data deduplication system is directly affected by the input dataset. We propose two sets of evaluation metrics, and the means to extract those metrics, for deduplication systems. The First set of metrics represents how the composition of segments changes within the deduplication system over five full backups. This in turn allows more insights into how the compression ratio will change as data accumulate. The second set of metrics represents index table fragmentation caused by duplicate elimination and the arrival rate at the underlying storage system. We show that, while shorter sequences of unique data may be bad for index caching, they provide a more uniform arrival rate which improves the overall throughput. Finally, we compute the metrics derived from the datasets under evaluation and show how the datasets perform with different metrics. Our evaluation shows that backup datasets typically exhibit patterns in how they change over time and that these patterns are quantifiable in terms of how they affect the deduplication process. This quantification allows us to: 1) decide whether deduplication is applicable, 2) provision resources, 3) tune the data deduplication parameters and 4) potentially decide which portion of the dataset is best suited for deduplication. },
}

@inproceedings{809359,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Jin-Soo Kim and Xiaohan Qin and Yarsun Hsu},
 year = {1999},
 pages = {60--68},
 publisher = {IEEE},
 title = {Memory characterization of a parallel data mining workload},
 date = {1999},
 doi = {10.1109/WWC.1998.809359},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809359},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809359.pdf?arnumber=809359},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Application software, Data mining, Ear, IBM Intelligent Miner, IBM computers, Insurance, Microprocessors, Organizing, Performance evaluation, Prototypes, Read only memory, Warehousing, batch processing (computers), batch update scheme, buffer storage, communication characteristics, conflict misses, data mining, input record size, memory block utilization, memory characterization, multiprocessor environment, neural network model, parallel data mining workload, parallel machines, parallel processing, performance evaluation, prototype array size, scalability, self-organising feature maps, self-organizing map, similar record groups, skewed TLB performance, sparse data sets, spatial locality, special purpose computers, temporal locality, translation lookaside buffer performance, two-way set-associative TLB, working set hierarchy, working set size, },
 abstract = {Studies a representative of an important class of emerging applications: a parallel data mining workload. The application, extracted from the IBM Intelligent Miner, identifies groups of records that are mathematically similar, based on a neural network model called a self-organizing map. We examine and compare, in detail, two implementations of the application: (1) temporal locality or working set size; (2) spatial locality and memory block utilization; (3) communication characteristics and scalability; and (4) translation lookaside buffer (TLB) performance. First, we find that the working set hierarchy of the application is governed by two parameters, namely the size of an input record and the size of prototype array; it is independent of the number of input records. Second, the application shows good spatial locality, with the implementation optimized for sparse data sets having slightly worse spatial locality. Third, due to the batch update scheme, the application bears very low communication. Finally, a two-way set-associative TLB may result in severely skewed TLB performance in a multiprocessor environment, caused by the large discrepancy in the number of conflict misses. Increasing the set associativity is more effective in mitigating the problem than increasing the TLB size },
}

@inproceedings{4362194,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Skarie, James and Debnath, Biplob K. and Lilja, David J. and Mokbel, Mohamed F.},
 year = {2007},
 pages = {183--192},
 publisher = {IEEE},
 title = {SCRAP: A Statistical Approach for Creating a Database Query Workload Based on Performance Bottlenecks},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362194},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362194},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362194.pdf?arnumber=4362194},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Buildings, Computer science, Costs, Data engineering, Database systems, Indexes, Internet, Runtime, System performance, },
 abstract = {With the tremendous growth in stored data, the role of database systems has become more significant than ever before. Standard query workloads, such as the TPC-C and TPC-H benchmark suites, are used to evaluate and tune the functionality and performance of database systems. Running and configuring benchmarks is a time consuming task. It requires substantial statistical expertise due to the enormous data size and large number of queries in the workload. Subsetting can be used to reduce the number of queries in a workload. An existing workload subsetting technique selected queries based on similarities of the ranks of the queries for low-level characteristics, such as cache miss rates, or based on the execution time required in different computer systems. However, many low-level characteristics are correlated, produce similar behaviors. Also, raw execution time as a metric is too diffuse to capture important performance bottlenecks. Our goal is to select a subset of queries that can reproduce the same bottlenecks in the system as the original workload. In this paper, we propose a statistical approach for creating a database query workload based on performance bottlenecks (SCRAP). Our methodology takes a query workload and a set of system configuration parameters as inputs, and selects a subset of the queries from the workload based on the similarity of performance bottlenecks. Experimental results using the TPC-H benchmark and the PostgreSQL database system, show that the reduced workload and the original workload produce similar performance bottlenecks, and the subset accurately estimates the total execution time. },
}

@inproceedings{809355,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Crowley, P. and Baer, J.-L.},
 year = {1999},
 pages = {15--24},
 publisher = {IEEE},
 title = {On the use of trace sampling for architectural studies of desktop applications},
 date = {1999},
 doi = {10.1109/WWC.1998.809355},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809355},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809355.pdf?arnumber=809355},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {0 to 64 KB, Application software, Computational efficiency, Computer science, INITMR technique, Microsoft Windows NT, Parameter estimation, Sampling methods, Testing, accuracy, analytical cache model, architectural studies, branch prediction architecture, cache miss rates, cache miss ratio determination, cache storage, desktop applications, memory architecture, microcomputer applications, performance evaluation, program diagnostics, reliability, sampling methods, stitch technique, trace sampling, victim cache architecture, workload, },
 abstract = {Examines the feasibility of performing architectural studies with trace sampling for a suite of desktop application traces on Windows NT. This paper makes three contributions: we compare the accuracy of several sampling techniques to determine cache miss rates for these workloads, we present victim cache and branch prediction architecture studies that demonstrate that sampling can be used to drive such studies, and we show how sampling may be used to accurately and efficiently derive the parameters for A. Agarwal et al.'s (1988) analytical cache model. Of the sampling techniques used for the cache miss ratio determinations, the stitch technique, which assumes that the state of the cache at the beginning of a sample is the same as the state at the end of the previous sample, narrowly outperforms the more complex INITMR technique of D.A. Wood et al. (1991) for these workloads. These two techniques are more accurate than the others and are reliable for caches up to 64 KB in size },
}

@inproceedings{809354,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {John, L.K. and Vasudevan, P. and Sabarinathan, J.},
 year = {1999},
 pages = {3--14},
 publisher = {IEEE},
 title = {Workload characterization: motivation, goals and methodology},
 date = {1999},
 doi = {10.1109/WWC.1998.809354},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809354},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809354.pdf?arnumber=809354},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Application software, Computer aided instruction, Computer architecture, Electrical capacitance tomography, Engineering profession, Measurement, Memory architecture, Optimizing compilers, Pipelines, Software performance, analytical performance modeling, application intrinsic properties, architecture-independent metrics, computer architecture, computer architecture design, control flow behavior, instruction-level parallelism, memory access behavior, memory hierarchy tuning, memory reference locality, performance evaluation, performance evaluation, performance measurements, processor micro-architecture tuning, processor model, program behavior model, program features, simulation results, system architecture tuning, tuning, workload characterization, },
 abstract = {Understanding the characteristics of workloads is extremely important in the design of efficient computer architectures. Accurate characterization of workload behavior leads to the design of improved architectures. The characterization of applications allows one to tune the processor micro-architecture, memory hierarchy and system architecture to suit particular features in programs. Workload characterization also has a significant impact on performance evaluation. Understanding the nature of the workload and its intrinsic features can help to interpret performance measurements and simulation results. Identifying and characterizing the intrinsic properties of an application in terms of its memory access behavior, locality, control flow behavior, instruction-level parallelism, etc. can eventually lead to a program behavior model, which can be used in conjunction with a processor model to do analytical performance modeling of computer systems. In this paper, we describe the objectives of workload characterization and emphasize the importance of obtaining architecture-independent metrics for workloads. A study of memory reference locality using some generic metrics is presented as an example },
}

@inproceedings{5654398,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Hui Lv and Xudong Zheng and Zhiteng Huang and Jiangang Duan},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Tackling the challenges of server consolidation on multi-core systems},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5654398},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5654398},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05654398.pdf?arnumber=5654398},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Electronic mail, I/O virtualization, Java, Operating systems, Semiconductor optical amplifiers, Servers, VM scheduling, consolidation workload, input-output programs, multi-core systems, multiple servers, optimisation, optimization, scheduling, server consolidation, virtual reality, virtualization technologies, },
 abstract = {With increasing demand to reduce system operation cost amidst growing adoption of virtualization technologies, multiple servers on a single physical chip is fast seeing typical usage in modern data centers. This trend has become more obvious with advances in system design that puts more and more multi-core CPUs into one system. As a result, it is interesting to investigate the challenges of virtualization on top of a multi-core system and the scalability of the consolidation workload on top of it. With this objective, we conduct a comprehensive analysis of an industry consolidation workload on top of a state of the art Intel system that consists of 64 physical threads. Our findings reveal that careful considerations must be taken in I/O virtualization and VM scheduling to achieve high scalability under the consolidation environment in a multi-core system. Against this backdrop, we carried out some preliminary work to demonstrate optimization opportunities. In particular, applying interrupt balance method brings in 16\% performance boost whereas a finer-grained scheduler prototype added another 6\% performance lift. Based on our analyses, existing VMMs should be enhanced with a) a well-designed multi-threaded I/O virtualization implementation and b) a scheduler with finer grained control on priorities. },
}

@inproceedings{5648812,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Sungpack Hong and Oguntebi, T. and Casper, J. and Bronson, N. and Kozyrakis, C. and Olukotun, K.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Eigenbench: A simple exploration tool for orthogonal TM characteristics},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5648812},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5648812},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05648812.pdf?arnumber=5648812},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Benchmark testing, Concurrent computing, EigenBench, Equations, History, Indexes, Pathology, Pollution, STAMP, TM system evaluation, benchmark testing, evaluation framework, exploration tool, microbenchmark, orthogonal TM characteristic, storage management, transaction processing, transactional behavior, transactional benchmark, transactional memory, },
 abstract = {There are a significant number of Transactional Memory(TM) proposals, varying in almost all aspects of the design space. Although several transactional benchmarks have been suggested, a simple, yet thorough, evaluation framework is still needed to completely characterize a TM system and allow for comparison among the various proposals. Unfortunately, TM system evaluation is difficult because the application characteristics which affect performance are often difficult to isolate from each other. We propose a set of orthogonal application characteristics that form a basis for transactional behavior and are useful in fully understanding the performance of a TM system. In this paper, we present EigenBench, a lightweight yet powerful microbenchmark for fully evaluating a transactional memory system. We show that EigenBench is useful for thoroughly exploring the orthogonal space of TM application characteristics. Because of its flexibility, our microbenchmark is also capable of reproducing a representative set of TM performance pathologies. In this paper, we use Eigenbench to evaluate two well-known TM systems and provide significant insight about their strengths and weaknesses. We also demonstrate how EigenBench can be used to mimic the evaluation coverage of a popular TM benchmark suite called STAMP. },
}

@inproceedings{5654431,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Nakaike, T. and Odaira, R. and Nakatani, T. and Michael, M.M.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Real Java applications in software transactional memory},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5654431},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5654431},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05654431.pdf?arnumber=5654431},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Databases, Geronimo application server, GlassFish application server, HSQLDB, Java, Java, Runtime library, Servers, lock contentions, parallel programming, real Java applications, software transactional memory, source code, storage management, },
 abstract = {Transactional Memory (TM) shows promise as a new concurrency control mechanism to replace lock-based synchronization. However, there have been few studies of TM systems with real applications, and the real-world benefits and barriers of TM remain unknown. In this paper, we present a detailed analysis of the behavior of real applications on a software transactional memory system. Based on this analysis, we aim to clarify what programming work is required to achieve reasonable performance in TM-based applications. We selected three existing Java applications: (1) HSQLDB, (2) the Geronimo application server, and (3) the GlassFish application server, because each application has a scalability problem caused by lock contentions. We identified the critical sections where lock contentions frequently occur, and modified the source code so that the critical sections are executed transactionally. However, this simple modification proved insufficient to achieve reasonable performance because of excessive data conflicts. We found that most of the data conflicts were caused by application-level optimizations such as reusing objects to reduce the memory usage. After modifying the source code to disable those optimizations, the TM-based applications showed higher or competitive performance compared to lock-based applications. Another finding is that the number of variables that actually cause data conflicts is much smaller than the number of variables that can be accessed in critical sections. This implies that the performance tuning of TM-based applications may be easier than that of lock-based applications where we need to take care of all of the variables that can be accessed in the critical sections. },
}

@inproceedings{4636081,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {},
 year = {2008},
 pages = {viii--viii},
 publisher = {IEEE},
 title = {IISWC-2008 conference committee},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636081},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636081},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636081.pdf?arnumber=4636081},
 isbn = {978-1-4244-2777-2},
 language = {English},
 abstract = {},
}

@inproceedings{4636084,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Sweeney, T.},
 year = {2008},
 pages = {1--1},
 publisher = {IEEE},
 title = {Wild speculation on consumer workloads in 2010&#x2013;2020},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636084},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636084},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636084.pdf?arnumber=4636084},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Application software, Concurrent computing, Engines, Functional programming, Graphics, Large-scale systems, Research and development, Shape, Throughput, Vector processors, computer games, consumer computing performance, consumer workloads, graphics processors, large-scale multi-core CPU, performance-intensive consumer applications, wild speculation, },
 abstract = {Summary form only given. Games are among the most performance-intensive consumer applications, and often lead the way in bringing research technologies into practice. This occasionally leads to non-evolutionary leaps in performance and workload characteristics, such as the 1000-fold increase in 3D throughput enabled by consumer graphics accelerators beginning in 1998. The speaker will argue that another revolution in consumer computing performance is on the horizon, driven by large-scale multi-core CPUs with vector-processing extensions inspired by todaypsilas graphics processors (GPUs). He will present a view of the key problems and solutions facing consumer software developers in 2010-2020, and speculate on the shape and scale of workloads in that timeframe. The essential questions to cover are: What portions of an application can scale effectively to many cores and vector processors? How and when can concurrency research bring techniques like functional programming, software transactional memory, and vectorization into mainstream practice? },
}

@inproceedings{4636085,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Diwan, A.},
 year = {2008},
 pages = {2--2},
 publisher = {IEEE},
 title = {We have it easy, but do we have it right?},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636085},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636085},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636085.pdf?arnumber=4636085},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Instruments, Measurement standards, Memory management, Particle measurements, Performance analysis, Productivity, Program processors, Programming profession, Technological innovation, Time measurement, computer systems, data collection, natural sciences, performance analysts, program diagnostics, social sciences, statistical techniques, },
 abstract = {Summary form only given. To evaluate an innovation in computer systems, performance analysts measure execution time or other metrics using one or more standard workloads. The performance analyst may carefully minimize the amount of measurement instrumentation, control the environment in which measurement takes place, and repeat each measurement multiple times. Finally, the performance analyst may use statistical techniques to characterize the data. Unfortunately, even with such a responsible approach, the collected data may be misleading due to measurement bias and observer effect. Measurement bias occurs when the experimental setup inadvertently favors a particular outcome. Observer effect occurs if data collection alters the behavior of the system being measured. This talk demonstrates that observer effect and measurement bias are (i) large enough to mislead performance analysts; and (ii) common enough that they cannot be ignored. While these phenomenon are well known to the natural and social sciences this talk will demonstrate that research in computer systems typically does not take adequate measures to guard against measurement bias and observer effect. },
}

@inproceedings{4636086,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Jian Chen and John, L.K.},
 year = {2008},
 pages = {5--13},
 publisher = {IEEE},
 title = {Energy-aware application scheduling on a heterogeneous multi-core system},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636086},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636086},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636086.pdf?arnumber=4636086},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Algorithm design and analysis, Application software, Energy consumption, Fuzzy logic, Hardware, Multicore processing, Power engineering and energy, Power engineering computing, Processor scheduling, Scheduling algorithm, branch transition rate, energy-aware application scheduling mechanism, fuzzy logic, fuzzy logic, heterogeneous multicore processor, instruction dependency distance, power aware computing, power efficient computing, processor scheduling, program execution, random scheduling approach, resource allocation, resource requirement, suitability-guided program scheduling mechanism, workload balancing, },
 abstract = {Heterogeneous multi-core processors are attractive for power efficient computing because of their ability to meet varied resource requirements of diverse applications in a workload. However, one of the challenges of using a heterogeneous multi-core processor is to schedule different programs in a workload to matching cores that can deliver the most efficient program execution. This paper presents an energy-aware scheduling mechanism that employs fuzzy logic to calculate the suitability between programs and cores by analyzing important inherent program characteristics such as instruction dependency distance and branch transition rate. The obtained suitability is then used to guide the program scheduling in the heterogeneous multi-core system. The experimental results show that the proposed suitability-guided program scheduling mechanism achieves up to 15.0\% average reduction in energy-delay product compared with that of the random scheduling approach. To the best of our knowledge, this study is the first to apply fuzzy logic to schedule programs in heterogeneous multi-core systems. },
}

@inproceedings{4636087,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Hao Feng and Eric Li and Yurong Chen and Yimin Zhang},
 year = {2008},
 pages = {14--23},
 publisher = {IEEE},
 title = {Parallelization and characterization of SIFT on multi-core systems},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636087},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636087},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636087.pdf?arnumber=4636087},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Acceleration, Application software, CMP simulator, Computational modeling, Computer simulation, Computer vision, Concurrent computing, Feature extraction, HDTV, Large-scale systems, Performance gain, cache storage, cache-conscious optimization, computer vision, computer vision application, distinctive invariant feature extraction, feature extraction, large scale chip multiprocessor, multicore system, multiprocessing systems, parallel algorithm, parallel algorithms, scale invariant feature transform, single instruction multiple data, symmetric multiprocessor, transforms, },
 abstract = {This paper parallelizes and characterizes an important computer vision application -Scale Invariant Feature Transform (SIFT) both on a Symmetric Multiprocessor (SMP) platform and a large scale Chip Multiprocessor (CMP) simulator. SIFT is an approach for extracting distinctive invariant features from images and has been widely applied. In many computer vision problems, a real-time or even super-real-time processing capability of SIFT is required. To meet the computation demand, we optimize and parallelize SIFT to accelerate its execution on multi-core systems. Our study shows that SIFT can achieve a 9.7x ~ llx speedup on a 16 -core SMP system. Furthermore, Single Instruction Multiple Data (SIMD) and cache-conscious optimization bring another 85\% performance gain at most. But it is still three times slower than the real-time requirement for High-Definition Television (HDTV) image. Then we study the performance of SIFT on a 64 -core CMP simulator. The results show that for HDTV image, SIFT can achieve an excellent speedup of 52 x and run in real-time finally. Besides the parallelization and optimization work, we also conduct a detailed performance analysis for SIFT on those two platforms. We find that load imbalance significantly limits the scalability and SIFT suffers from intensive burst memory bandwidth requirement on the 16 -core SMP system. However, on the 64 -core CMP simulator the memory pressure is not high due to the shared last-level cache (LLC) which accommodates tremendous read-write sharing in SIFT. Thus it does not affect the scaling performance. In short, understanding the characterization of SIFT can help identify the program bottlenecks and give us further insights into designing better systems. },
}

@inproceedings{809358,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Bradford, J.P. and Fortes, J.},
 year = {1999},
 pages = {49--59},
 publisher = {IEEE},
 title = {Performance and memory-access characterization of data mining applications},
 date = {1999},
 doi = {10.1109/WWC.1998.809358},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809358},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809358.pdf?arnumber=809358},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Application software, CPU performance, Data engineering, Data mining, Databases, Decision trees, Humidity, Knowledge engineering, L1 data cache, L1 instruction cache, L2 data cache, Out of order, RSIM, Rain, Weather forecasting, cache size scaling, cache storage, data mining, data mining applications, data reference locality, databases, decision tree induction program, decision trees, execution-driven simulator, instruction miss rate, instruction-level parallelism, inter-processor communication, knowledge discovery, memory access characterization, multiple issue, out-of-order dispatch, parallel programming, performance characterization, performance improvement, scaled-down data sets, software performance evaluation, uniprocessor models, virtual machines, },
 abstract = {Characterizes the performance and memory-access behavior of a decision tree induction program, a previously unstudied application used in data mining and knowledge discovery in databases. Performance is studied via RSIM, an execution-driven simulator, for three uniprocessor models that exploit instruction-level parallelism to varying degrees. Several properties of the program are noted. Out-of-order dispatch and multiple issue provide a significant performance advantage: 50\%-250\% improvement in inter-processor communication (IPC) for out-of-order dispatch vs. in-order dispatch, and 5\%-120\% improvement in IPC for four-way issue vs. single issue. Multiple issue provides a greater performance improvement for larger L2 cache sizes, when the program is limited by CPU performance; out-of-order dispatch provides a greater performance improvement for smaller L2 cache sizes. The program has a very small instruction footprint: for an 8-kB L1 instruction cache, the instruction miss rate is below 0.1\%. A small (8 kB) L1 data cache is sufficient to capture most of the locality of the data references, resulting in L1 miss rates between 10\%-20\%. Increasing the size of the L2 data cache does not significantly improve performance until a significant fraction (over 1/4) of the data set fits into the L2 cache. Lastly, a procedure is developed for scaling the cache sizes when using scaled-down data sets, allowing the results for smaller data sets to be used to predict the performance of full-sized data sets },
}

@inproceedings{1226489,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Amza, C. and Chanda, A. and Cox, A.L. and Elnikety, S. and Gil, R. and Rajamani, K. and Zwaenepoel, W. and Cecchet, E. and Marguerite, J.},
 year = {2002},
 pages = { 3-- 13},
 publisher = {IEEE},
 title = {Specification and implementation of dynamic Web site benchmarks},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226489},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226489},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226489.pdf?arnumber=1226489},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Enterprise Java Beans,  Internet,  Java,  Java servlets,  PHP,  TPC-W specification,  Web server,  Web sites,  auction site,  bottlenecks,  bulletin board,  caching,  client emulator,  client-server systems,  clustering,  distributed object management,  dynamic Web site benchmarks,  dynamic content,  file servers,  online bookstore,  open-source software,  workloads, },
 abstract = {The absence of benchmarks for Web sites with dynamic content has been a major impediment to research in this area. We describe three benchmarks for evaluating the performance of Web sites with dynamic content. The benchmarks model three common types of dynamic content Web sites with widely varying application characteristics: an online bookstore, an auction site, and a bulletin board. For the online bookstore, we use the TPCW specification. For the auction site and the bulletin board, we provide our own specification, modeled after ebay.com and slahdot.org, respectively. For each benchmark we describe the design of the database and the interactions provided by the Web server. We have implemented these three benchmarks with a variety of methods for building dynamic-content applications, including PHP, Java servlets and EJB (Enterprise Java Beans). In all cases, we use commonly used open-source software. We also provide a client emulator that allows a dynamic content Web server to be driven with various workloads. Our implementations are available freely from our Web site for other researchers to use. These benchmarks can be used for research in dynamic Web and application server design. In this paper, we provide one example of such possible use, namely discovering the bottlenecks for applications in a particular server configuration. Other possible uses include studies of clustering and caching for dynamic content, comparison of different application implementation methods, and studying the effect of different workload characteristics on the performance of servers. With these benchmarks we hope to provide a common reference point for studies in these areas. },
}

@inproceedings{4636082,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {},
 year = {2008},
 pages = {x--x},
 publisher = {IEEE},
 title = {Sponsors},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636082},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636082},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636082.pdf?arnumber=4636082},
 isbn = {978-1-4244-2777-2},
 language = {English},
 abstract = {},
}

@inproceedings{4636083,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {},
 year = {2008},
 pages = {xi--xii},
 publisher = {IEEE},
 title = {Table of contents},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636083},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636083},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636083.pdf?arnumber=4636083},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {commercial workloads, emerging workloads, multicore systems, multiprocessing systems, parallel processing, software architecture, software architecture, thread parallelism, workload fidelity, },
 abstract = {The following topics are dealt with: multicore systems; thread parallelism; emerging workloads; commercial workloads; architecture issues and workload fidelity. },
}

@inproceedings{4086139,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Reilly, J.},
 year = {2006},
 pages = {119--119},
 publisher = {IEEE},
 title = {Evolve or Die: Making SPEC's CPU Suite Relevant Today and Tomorrow},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302735},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086139},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086139.pdf?arnumber=4086139},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Biographies, Computer industry, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04086139.png" border="0"> },
}

@inproceedings{4636088,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Apparao, P. and Iyer, R. and Newell, D.},
 year = {2008},
 pages = {24--32},
 publisher = {IEEE},
 title = {Implications of cache asymmetry on server consolidation performance},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636088},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636088},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636088.pdf?arnumber=4636088},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Enterprise resource planning, Hardware, Java, Laboratories, Multicore processing, Performance analysis, Potential well, Processor scheduling, Sockets, Virtual machining, cache storage, chip multiprocessor platform, file servers, logical cache asymmetry, microprocessor chips, multi workload scenario, physical asymmetric cache, server consolidation performance, shared cache resource, virtual asymmetric cache, virtual machine performance, virtual machines, },
 abstract = {Todaypsilas CMP platforms are designed to be symmetric in terms of platform resources such as shared caches. However, it is becoming increasingly important to understand the performance implications of asymmetric caches for two key reasons: (a) multi-workload scenarios such as server consolidation are a growing trend and contention for shared cache resources between workloads causes logical cache asymmetry, (b) future CMP platforms may be designed to be physically asymmetric in hardware due to die area pressure, process variability or power/performance efficiency. Our focus in this paper is to understand the performance implications of both logical as well as physical asymmetric caches on server consolidation. Based on real measurements of a state-of-the-art CMP processor running a server consolidation benchmark (vConsolidate) we compare the performance implications as a function of (a) symmetric caches, (b) virtually asymmetric caches, (c) physically asymmetric caches and (d) a combination of logically and physically asymmetric caches. We analyze the performance behavior in terms of (i) performance of each of the individual workloads being consolidated and (ii) architectural components such as CPI and MPI. We believe that this asymmetric cache study is the first of its kind and provides useful data/insights on cache characteristics of server consolidation. We also present inferences on future optimizations in the VMM scheduler as well as potential hardware techniques for future CMPs with cache asymmetry. },
}

@inproceedings{4636089,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Chi Cao Minh and JaeWoong Chung and Kozyrakis, C. and Olukotun, K.},
 year = {2008},
 pages = {35--46},
 publisher = {IEEE},
 title = {STAMP: Stanford Transactional Applications for Multi-Processing},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636089},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636089},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636089.pdf?arnumber=4636089},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Application software, Concurrent computing, Hardware, Laboratories, Parallel programming, Programming profession, Proposals, STAMP, Software systems, Stanford transactional application, Stress, Yarn, microbenchmark, multiprocessing, multiprocessing systems, parallel programming, parallel programming, transaction processing, transactional execution, transactional memory, },
 abstract = {Transactional Memory (TM) is emerging as a promising technology to simplify parallel programming. While several TM systems have been proposed in the research literature, we are still missing the tools and workloads necessary to analyze and compare the proposals. Most TM systems have been evaluated using microbenchmarks, which may not be representative of any real-world behavior, or individual applications, which do not stress a wide range of execution scenarios. We introduce the Stanford Transactional Application for Multi-Processing (STAMP), a comprehensive benchmark suite for evaluating TM systems. STAMP includes eight applications and thirty variants of input parameters and data sets in order to represent several application domains and cover a wide range of transactional execution cases (frequent or rare use of transactions, large or small transactions, high or low contention, etc.). Moreover, STAMP is portable across many types of TM systems, including hardware, software, and hybrid systems. In this paper, we provide descriptions and a detailed characterization of the applications in STAMP. We also use the suite to evaluate six different TM systems, identify their shortcomings, and motivate further research on their performance characteristics. },
}

@inproceedings{4086138,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Joshi, A. and Eeckhout, L. and Bell, R.H. and John, L.},
 year = {2006},
 pages = {105--115},
 publisher = {IEEE},
 title = {Performance Cloning: A Technique for Disseminating Proprietary Applications as Benchmarks},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302734},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086138},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086138.pdf?arnumber=4086138},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, Cloning, Costs, Embedded computing, Embedded system, Intellectual property, Microarchitecture, Microprocessors, Predictive models, Workstations, benchmark testing, code functional meaning, embedded benchmark, embedded systems, industrial property, microarchitecture configuration, microarchitecture-independent performance attribute, microprocessor chips, performance attribute extraction, performance characteristics, performance cloning, performance evaluation, proprietary application dissemination, synthetic benchmark clone, workload synthesis, },
 abstract = {Many embedded real world applications are intellectual property, and vendors hesitate to share these proprietary applications with computer architects and designers. This poses a serious problem for embedded microprocessor designers - how do they customize the design of their microprocessor to provide optimal performance for a class of target customer applications? In this paper, we explore a technique that can automatically extract key performance attributes of a real world application and clone them into a synthetic benchmark. The advantage of the synthetic benchmark clone is that it hides functional meaning of the code but exhibits similar performance characteristics as the target application. Unlike previously proposed workload synthesis techniques, we only model microarchitecture-independent performance attributes into the synthetic clone. By using a set of embedded benchmarks from the MediaBench and MiBench suites, we demonstrate that the performance and power consumption of the synthetic clone correlates well with that of the original application across a wide range of microarchitecture configurations },
}

@inproceedings{1437378,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 publisher = {IEEE},
 title = {2004 IEEE International Workshop on Workload Characterization (IEEE Cat. No.04EX977)},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437378},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437378},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437378.pdf?arnumber=1437378},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { benchmark subsetting,  benchmark testing,  data flow analysis,  dataflow characterization,  high performance computing,  media,  memory characterization,  microprocessor chips,  multiprocessing systems,  network,  performance evaluation,  processor scheduling,  storage management,  tracing,  workload characterization,  workload generation, },
 abstract = {},
}

@inproceedings{1437379,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 0_2-- 0_2},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437379},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437379},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437379.pdf?arnumber=1437379},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{5649529,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Jaewoong Chung and Chakrabarti, D.R. and Minh, C.C.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {Analysis on semantic transactional memory footprint for hardware transactional memory},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649529},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649529},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649529.pdf?arnumber=5649529},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {32KB L1 cache, Arrays, Bioinformatics, Bloom filters, Genomics, Hardware, Instruction sets, Registers, cache storage, false positive probability, hardware transactional memory, instruction prefix, intelligent software toolchain, memory access, probability, semantic transactional memory footprint, trace analysis, transaction processing, transaction signature management, },
 abstract = {We analyze various characteristics of semantic transactional memory footprint (STMF) that consists of only the memory accesses the underlying hardware transactional memory (HTM) system has to manage for the correct execution of transactional programs. Our analysis shows that STMF can be significantly smaller than declarative transactional memory footprint (DTMF) that contains all memory accesses within transaction boundaries (i.e., only 8.3\% of DTMF in the applications examined). This result encourages processor designers and software toolchain developers to explore new design points for low-cost HTM systems and intelligent software toolchains to find and leverage STMF efficiently. We identify seven code patterns that belong to DTMF, but not to STMF, and show that they take up 91.7\% of all memory accesses in transactional boundaries, on average, for the transactional programs examined. A new instruction prefix is proposed to express STMF efficiently, and the existing compiler techniques are examined to check their applicability to deduce STMF from DTMF. Our trace analysis shows that using STMF significantly reduces the ratio of transactions overflowing a 32KB L1 cache, from 12.80\% to 2.00\%, and substantially lowers the false positive probability of Bloom filters used for transaction signature management, from 23.60\% to less than 0.001\%. The simulation result shows that the STAMP applications with the STMF expression run 40\% faster on average than those with the DTMF expression. },
}

@inproceedings{1526005,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Sanchez, F. and Salami, E. and Ramirez, A. and Valero, M.},
 year = {2005},
 pages = { 99-- 108},
 publisher = {IEEE},
 title = {Parallel processing in biological sequence comparison using general purpose processors},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526005},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526005},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526005.pdf?arnumber=1526005},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { Smith-Waterman algorithm,  biological sequence comparison,  biology computing,  computational complexity,  computational complexity,  general purpose processors,  parallel processing,  parallel processing,  single-instruction multiple-data extension,  string matching, Bioinformatics, Biology computing, Computational complexity, DNA, Data mining, Databases, Parallel processing, Proposals, Proteins, Sequences, },
 abstract = {The comparison and alignment of DNA and protein sequences are important tasks in molecular biology and bioinformatics. One of the most well known algorithms to perform the string-matching operation present in these tasks is the Smith-Waterman algorithm (SW). However, it is a computation intensive algorithm, and many researchers have developed heuristic strategies to avoid using it, specially when using large databases to perform the search. There are several efficient implementations of the SW algorithm on general purpose processors. These implementations try to extract data-level parallelism taking advantage of single-instruction multiple-data extensions (SIMD), capable of performing several operations in parallel on a set of data. In this paper, we propose a more efficient data parallel implementation of the SW algorithm. Our proposed implementation obtains a 30\% reduction in the execution time relative to the previous best data-parallel alternative. In this paper we review different alternative implementation of the SW algorithm, compare them with our proposal, and present preliminary results for some heuristic implementations. Finally, we present a detailed study of the computational complexity of the different alignment algorithms presented and their behavior on the different aspect of the CPU microarchitecture. },
}

@inproceedings{1526004,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Srinivasan, U. and Peng-Sheng Chen and Qian Diao and Chu-Cheow Lim and Li, E. and Yongjian Chen and Ju, R. and Yimin Zhang},
 year = {2005},
 pages = { 87-- 98},
 publisher = {IEEE},
 title = {Characterization and analysis of HMMER and SVM-RFE parallel bioinformatics applications},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526004},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526004},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526004.pdf?arnumber=1526004},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { HMMER,  Intel x86 based hyperthread-capable shared-memory multiprocessor systems,  SVM-RFE,  biology computing,  compiler optimization,  gene expression analysis,  hyperthreading,  load balancing optimizations,  multi-threading,  parallel bioinformatics applications,  parallel processing,  prefetching,  processor scheduling,  resource allocation,  runtime optimization,  sequence alignment,  shared memory systems,  storage management,  thread scheduling tuning,  workload analysis, Bandwidth, Bioinformatics, Gene expression, Hardware, Hidden Markov models, Load management, Multiprocessing systems, Performance analysis, Scalability, Yarn, },
 abstract = {Bioinformatics applications constitute an emerging data-intensive, high-performance computing (HPC) domain. While there is much research on algorithmic improvements, (2004), the actual performance of an application also depends on how well the program maps to the target hardware. This paper presents a performance study of two parallel bioinformatics applications HMMER (sequence alignment) and SVM-RFE (gene expression analysis), on Intel x86 based hyperthread-capable (2002) shared-memory multiprocessor systems. The performance characteristics varied according to the application and target hardware characteristics. For instance, HMMER is compute intensive and showed better scalability on a 3.0 GHz system versus a 2.2 GHz system. However, SVM-RFE is memory intensive and showed better absolute performance on the 2.2 GHz machine which has better memory bandwidth. The performance is also impacted by processor features, e.g. hyperthreading (HT) (2002) and prefetching. With HMMER we could obtain -75\% of the performance with HT enabled with respect to doubling the number of CPUs. While load balancing optimizations can provide speedup of -30\% for HMMER on a hyperthreading-enabled system, the load balancing has to adapt to the target number of processors and threads. SVM-RFE benefits differently from the same load-balancing and thread scheduling tuning. We conclude that compiler and runtime optimizations play an important role to achieve the best performance for a given bioinformatics algorithm. },
}

@inproceedings{1526007,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Bell, R.H., Jr. and John, L.K.},
 year = {2005},
 pages = { 110-- 118},
 publisher = {IEEE},
 title = {Efficient power analysis using synthetic testcases},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526007},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526007},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526007.pdf?arnumber=1526007},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { SPEC2000 benchmark,  STREAM benchmark,  benchmark testing,  circuit simulation,  low-power electronics,  microprocessor chips,  power analysis,  power dissipation assessment,  processor designs,  synthetic testcases, Analytical models, Application software, Benchmark testing, Computational modeling, Hardware, Power dissipation, Power system modeling, Power system reliability, Process design, Runtime, },
 abstract = {Power dissipation has become an important consideration for processor designs. Assessing power using simulators is problematic given the long runtimes of real applications. Researchers have responded with techniques to reduce the total number of simulated instructions while still maintaining representative simulation behavior. Synthetic testcases have been shown to reduce the number of necessary instructions significantly while still achieving accurate performance results for many workload characteristics. In this paper, we show that the synthetic testcases can rapidly and accurately assess the dynamic power dissipation of real programs. Synthetic versions of the SPEC2000 and STREAM benchmarks can predict the total power per cycle to within 6.8\% error on average, with a maximum of 15\% error, and total power per instruction to within 4.4\% error. In addition, for many design changes for which IPC and power change significantly, the synthetic testcases show small errors, many less than 5\%. We also show that simulated power dissipation for both applications and synthetics correlates well with the IPCs of the real programs, often giving a correlation coefficient greater than 0.9. },
}

@inproceedings{1526006,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Cvetanovic, Z.},
 year = {2005},
 publisher = {IEEE},
 title = {Visualization techniques for system performance characterization},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526006},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526006},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526006.pdf?arnumber=1526006},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { data visualisation,  data visualization,  software performance evaluation,  system bottleneck identification,  system monitoring,  system performance characterization, Application software, Biographies, Character generation, Large-scale systems, Performance analysis, Product development, Scalability, System performance, System software, Visualization, },
 abstract = {As computer environments increase in size and complexity, it is becoming more challenging to analyze and identify factors that limit performance and scalability. Application developers and system administrators need easy-to-use tools that enable them to quickly identify system bottlenecks and configure system for best performance. In this paper, we present techniques that allow users to view the load on all major system component simultaneously, with negligible overhead, and no changes in the application. We include several case studies where such techniques have been used to characterize and improve performance of applications, system software, and future CPU/platform designs. },
}

@inproceedings{4362169,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {},
 year = {2007},
 pages = {ii--ii},
 publisher = {IEEE},
 title = {Message From the General Chair},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362169},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362169},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362169.pdf?arnumber=4362169},
 isbn = {978-1-4244-1561-8},
 language = {English},
 abstract = {},
}

@inproceedings{4362168,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {},
 year = {2007},
 pages = {i--i},
 publisher = {IEEE},
 title = {IEEE International Symposium on Workload Characterization},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362168},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362168},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362168.pdf?arnumber=4362168},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {microarchitecture, multicore system, parallel processing, parallel processing, resource allocation, signal processing, workload analysis, },
 abstract = {The following topics are dealt with: multi-core system; workload analysis; signal processing; microarchitecture; parallel processing. },
}

@inproceedings{1526003,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Chang-Burm Cho and Chande, A.V. and Yue Li and Tao Li},
 year = {2005},
 pages = { 76-- 86},
 publisher = {IEEE},
 title = {Workload characterization of biometric applications on Pentium 4 microarchitecture},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526003},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526003},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526003.pdf?arnumber=1526003},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { Pentium 4 microarchitecture,  benchmark testing,  biometric applications,  biometrics (access control),  branch misprediction rate,  cache storage,  large L2 cache,  memory architecture,  microcomputers,  prefetching,  security of data,  storage management,  workload characterization, Biometrics, Computer architecture, Large-scale systems, Memory architecture, Microarchitecture, Microprocessors, Pattern matching, Prefetching, Privacy, Security, },
 abstract = {Biometric computing is a technique that uses physiological and behavioral characteristics of persons to identify and authenticate individuals. Due to the increasing demand on security, privacy and anti-terrorism, biometric applications represent the rapidly growing computing workloads. However, very few results on the execution characteristics of these applications on the state-of-the-art microprocessor and memory systems have been published so far. This paper proposes a suite of biometric applications and reports the results of a biometric workload characterization effort, focusing on various architecture features. To understand the impacts and implications of biometric workloads on the processor and memory architecture design, we contrast the characteristics of biometric workloads and the widely used SPEC 2000 integer benchmarks. Our experiments show that biometric applications typically show small instruction footprint that can fit in the L1 instruction cache. The loads and stores account for more than 50\% of the dynamic instructions. This indicates that biometric applications are data-centric in nature. Although biometric applications work across large-scale datasets to identify matched patterns, the active working sets of these workloads are usually small. As a result, prefetching and large L2 cache effectively handle the data footprints of a majority of the studied benchmarks. Branch misprediction rate is less than 4\% on all studied workloads. The IPC of the studied benchmarks ranges from 0.13 to 0.77 indicates that out-of-order superscalar execution is not quite efficient. The developed biometric benchmark suite (BMW) and input data sets are freely available and can be downloaded from http://www.ideal.ece.ufl.edu/BMW. },
}

@inproceedings{1526002,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Morin, R. and Kumar, A. and Ilyina, E.},
 year = {2005},
 pages = { 67-- 75},
 publisher = {IEEE},
 title = {A multi-level comparative performance characterization of SPECjbb2005 versus SPECjbb2000},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526002},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526002},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526002.pdf?arnumber=1526002},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { CPU utilization,  Intel Xeon processor-based commercial server,  JIT optimization,  JVM monitoring tools,  Java,  SPECjbb2000,  SPECjbb2005,  benchmark testing,  branch behavior,  bus utilization,  cache miss rates,  garbage collection,  low-intrusion application profiling tools,  memory utilization,  multilevel comparative performance characterization,  operating system monitoring tools,  processor performance monitoring events,  server-side Java benchmark,  software metrics,  software performance evaluation,  storage management,  system level metrics,  system monitoring, Hardware, Java, Libraries, Measurement, Monitoring, Operating systems, Optimizing compilers, Productivity, Runtime environment, Scalability, },
 abstract = {SPEC has released SPECjbb2005, a new server-side Java benchmark which supersedes SPECjbb2000. SPECjbb2005 is a substantial update to SPECjbb2000, intended to make the workload more representative based on current Java development practices. SPECjbb2000 has been in existence for about five years and it has been a valuable tool for optimizing the performance of commercial JVMs as well as supporting research activities. Since SPECjbb2005 replaces SPECjbb2000, it is important to understand the key differences between the two, as well as implications for JVM and hardware designers. In this paper, we present a comparative characterization of these two workloads based on detailed measurements on an Intel\&reg; Xeon\&trade; processor-based commercial server. First, we describe key functional changes introduced in SPECjbb2005. Using low-intrusion application profiling tools we compare application execution profiles. Through JVM monitoring tools, we compare JVM behavior including JIT optimization and garbage collection. Using operating system monitoring tools we compare key system level metrics including CPU utilization. With the aid of processor performance monitoring events, we compare key architectural characteristics such as cache miss rates, memory/bus utilization, and branch behavior. Finally, we summarize key findings, provide recommendations to JVM developers and hardware designers, and suggest areas for future work. },
}

@inproceedings{5649408,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Poggi, N. and Carrera, D. and Gavalda, R. and Torres, J. and Ayguade, E.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Characterization of workload and resource consumption for an online travel and booking site},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649408},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649408},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649408.pdf?arnumber=5649408},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {AJAX, B2B Web services, Browsers, Crawlers, Databases, Java, Javascript, Time factors, Web server, Web services, XML, XML, caching, e-commerce, electronic commerce, interactive programming, online booking site, online travel agency, online travel site, quality of service, quality of service, resource consumption, search algorithms, travel industry, workload consumption, },
 abstract = {Online travel and ticket booking is one of the top E-Commerce industries. As they present a mix of products: flights, hotels, tickets, restaurants, activities and vacational packages, they rely on a wide range of technologies to support them: Javascript, AJAX, XML, B2B Web services, Caching, Search Algorithms and Affiliation; resulting in a very rich and heterogeneous workload. Moreover, visits to travel sites present a great variability depending on time of the day, season, promotions, events, and linking; creating bursty traffic, making capacity planning a challenge. It is therefore of great importance to understand how users and crawlers interact on travel sites and their effect on server resources, for devising cost effective infrastructures and improving the Quality of Service for users. In this paper we present a detailed workload and resource consumption characterization of the web site of a top national Online Travel Agency. Characterization is performed on server logs, including both HTTP data and resource consumption of the requests, as well as the server load status during the execution. From the dataset we characterize user sessions, their patterns and how response time is affected as load on Web servers increases. We provide a fine grain analysis by performing experiments differentiating: types of request, time of the day, products, and resource requirements for each. Results show that the workload is bursty, as expected, that exhibit different properties between day and night traffic in terms of request type mix, that user session length cover a wide range of durations, which response time grows proportionally to server load, and that response time of external data providers also increase on peak hours, amongst other results. Such results can be useful for optimizing infrastructure costs, improving QoS for users, and development of realistic workload generators for similar applications. },
}

@inproceedings{1526009,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Maron, B. and Chen, T. and Vianney, D. and Olszewski, B. and Kunkel, S. and Mericas, A.},
 year = {2005},
 pages = { 129-- 136},
 publisher = {IEEE},
 title = {Workload characterization for the design of future servers},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526009},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526009},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526009.pdf?arnumber=1526009},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { POWER5 architecture,  POWER5 performance monitor facilities,  commercial workloads,  computer architecture,  cycles per instruction breakdown modeling,  memory workloads,  performance evaluation,  scientific workloads,  workload characterization, Control systems, Decoding, Delay, Design optimization, Electric breakdown, Hardware, Memory management, Pipelines, Registers, Size control, },
 abstract = {Workload characterization has become an integral part of the design of future servers since their characteristics can guide the developers to understand the workload requirements and how the underlying architecture would optimize the performance of the intended workload. In this paper, we give an overview of the POWER5 architecture. We also introduce the POWER5 performance monitor facilities and performance events that lead to the construction of a CPI (cycles per instruction) breakdown model. For our study, we characterize four different groups of workloads: commercial, HPC, memory, and scientific. Using the data obtained from the POWER5 performance counters, we breakdown the CPI stack into a base component, when the processor is completing work and a stall component when the processor is not completing instructions. The stall component can be further divided into cycles when the pipeline was empty and cycles when the pipeline was not empty but completion is stalled. With this model, we enumerate the number of processing cycles, i.e., a fraction of the CPI, a workload spent while progressing through the core resources and the incurred penalty upon encountering those resource usage inhibitors. The results show the CPI breakdown for each workload, identify where each workload spends its processing cycles and the associated CPI cost when accessing the core resources. },
}

@inproceedings{1526008,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Zhongbo Cao and Wei Huang and Chang, J.M.},
 year = {2005},
 pages = { 119-- 128},
 publisher = {IEEE},
 title = {A study of Java virtual machine scalability issues on SMP systems},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526008},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526008},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526008.pdf?arnumber=1526008},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { Java,  Java heap space,  Java virtual machine scalability,  L2 cache misses,  allocation buffer,  benchmark testing,  cache storage,  cache-to-cache transfers,  cycle-accurate simulator,  memory system latencies,  multi-threading,  multiprocessing systems,  multithreaded Java benchmarks,  performance scaling evaluation,  resource allocation,  resource contentions,  software performance evaluation,  symmetrical multiprocessing systems,  thread-local heap,  virtual machines, Computational modeling, Delay, Java, Kernel, Operating systems, Performance analysis, Processor scheduling, Scalability, Virtual machining, Yarn, },
 abstract = {This paper studies the scalability issues of Java virtual machine (JVM) on symmetrical multiprocessing (SMP) systems. Using a cycle-accurate simulator, we evaluate the performance scaling of multithreaded Java benchmarks with the number of processors and application threads. By correlating low-level hardware performance data to two high-level software constructs: thread types and memory regions, we present in detail the performance analysis and study the potential performance impacts of memory system latencies and resource contentions on scalability. Several key findings are revealed through this paper. First, among the memory access latency components, the primary portion of memory stalls are produced by L2 cache misses and cache-to-cache transfers. Second, among the regions of memory, Java heap space produces most memory stalls. Additionally, a large majority of memory stalls occur in application threads, as opposed to other JVM threads. Furthermore, we find that increasing the number of processors or application threads, independently of each other, leads to increases in L2 cache miss ratio and cache-to-cache transfer ratio. This problem can be alleviated by using a thread-local heap or allocation buffer which can improve L2 cache performance. For certain benchmarks such as Raytracer, their cache-to-cache transfers, mainly dominated by false sharing, can be significantly reduced. Our experiments also show that a thread-local allocation buffer with a size between 16KB and 256KB often leads to optimal performance. },
}

@inproceedings{809364,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Boyd, W.T. and Recio, R.J.},
 year = {1999},
 pages = {87--96},
 publisher = {IEEE},
 title = {I/O workload characteristics of modern servers},
 date = {1999},
 doi = {10.1109/WWC.1998.809364},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809364},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809364.pdf?arnumber=809364},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Arm, I/O subsystems, I/O workload characteristics, Indexes, Read-write memory, Relational databases, Throughput, application environments, bottlenecks, computing industry, data mining, microprocessor technology, network servers, performance evaluation, servers, system structure, technological forecasting, transaction processing, },
 abstract = {The design and development of future I/O subsystems needs to keep pace with the rapid rate of improvement in microprocessor technology and changes in system structure. In order to analyze the potential bottlenecks of I/O subsystems, we must first identify and characterize the various workloads that will run on these future systems. This paper has two major goals. The first is to identify and analyze the application environments that are presently being implemented throughout the computing industry. The second goal is to identify and summarize the I/O subsystem characteristics of various present-day and future workloads that typify these application environments },
}

@inproceedings{809365,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Gomez, M.E. and Santonja, V.},
 year = {1999},
 pages = {97--104},
 publisher = {IEEE},
 title = {Self-similarity in I/O workload: analysis and modeling},
 date = {1999},
 doi = {10.1109/WWC.1998.809365},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809365},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809365.pdf?arnumber=809365},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {I/O access, I/O workload, Information security, Mathematics, ON/OFF model, Poisson arrivals, Testing, computer networks, disc storage, disk activity collection, disk request generator, disk-level I/O requests, fractals, local-area network traffic, performance evaluation, performance evaluation, queueing theory, self-similarity, storage subsystem designs, storage subsystem implementations, telecommunication traffic, trace data, wide-area network traffic, writes, },
 abstract = {Recently, the notion of self-similarity has been applied to wide-area and local-area network traffic. This paper demonstrates that disk-level I/O requests are self-similar in nature. We show evidence, both visual and mathematical, that the I/O accesses are consistent with self-similarity. Moreover, we show that this property of I/O accesses is mainly due to writes. For our experiments, we use two sets of traces that collect the disk activity from two systems over a period of two months. Such behavior has serious implications for the performance evaluation of storage subsystem designs and implementations, since commonly-used simplifying assumptions about workload characteristics (e.g. Poisson arrivals) are shown to be incorrect. Using the ON/OFF model, we implement a disk request generator. The inputs of this generator are the measured properties of the available trace data. We analyze the synthesized workload and confirm that it exhibits the correct self-similar behavior },
}

@inproceedings{809366,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Brown, M. and Jenevein, R.M. and Ullah, N.},
 year = {1999},
 pages = {105--113},
 publisher = {IEEE},
 title = {Memory access pattern analysis},
 date = {1999},
 doi = {10.1109/WWC.1998.809366},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809366},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809366.pdf?arnumber=809366},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Algorithm design and analysis, Argon, Control systems, Delay, Identity-based encryption, MPAT, Memory architecture, Operating systems, Pattern analysis, Random access memory, System performance, dynamic instruction traces, hardware, memory access pattern analysis, memory architecture, memory behavior analysis, memory model, memory pattern analysis tool, memory system design, memory system performance, memory transactions, performance evaluation, program diagnostics, software changes, storage management, },
 abstract = {A methodology for analyzing memory behavior has been developed for the purpose of evaluating memory system design. MPAT, a memory pattern analysis tool, was used to profile memory transactions of dynamic instruction traces. The paper first describes the memory model and metrics gathered by MPAT. Then the metrics are evaluated in order to determine what hardware and software changes should be made to improve memory system performance },
}

@inproceedings{809367,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Grayson, B. and Chase, C.},
 year = {1999},
 pages = {114--121},
 publisher = {IEEE},
 title = {Characterizing instruction latency for speculative issue SMPs: a case study of varying memory system performance on the SPLASH-2 benchmarks},
 date = {1999},
 doi = {10.1109/WWC.1998.809367},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809367},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809367.pdf?arnumber=809367},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Armadillo simulator, Computer aided software engineering, Delay, Design engineering, Electronic switching systems, Frequency, Out of order, Read only memory, SPLASH-2 benchmark suite, SPLASH-2 benchmarks, Statistics, Switched-mode power supply, System performance, aggressive superscalar processors, bus frequency, bus width, cache latency, case study, characterization framework, critical path, instruction latency, instruction pipeline, instruction sets, key memory system parameters, memory latency, memory system performance, meta-stages, multiprocessing systems, multiprocessor systems, parallel architectures, performance evaluation, shared bus, speculative issue SMPs, speculative superscalar processors, storage management, symmetric multiprocessors, virtual machines, },
 abstract = {Out-of-order, speculative, superscalar processors are complex. The behavior of multiprocessor systems that use such processors is not well understood and very difficult to predict. We tackle this problem using a powerful simulator, Armadillo, and a novel characterization framework that breaks the instruction pipeline into five meta-stages. The Armadillo simulator models symmetric multiprocessors (SMPs) constructed from highly aggressive superscalar processors on a shared bus, and is able to provide accurate, detailed statistics on numerous aspects of the simulated system, including the amount of time each instruction spends in each of these five meta-stages. We also analyze the fraction of each instruction's lifetime, during which it remains speculative and the amount of time that an instruction spends on the critical path. To demonstrate the effectiveness of this approach, we apply the characterization to applications from the SPLASH-2 benchmark suite. We evaluated the applications' sensitivity to key memory system parameters: bus frequency, bus width, memory latency, and cache latency },
}

@inproceedings{809360,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Murta, C.D. and Almeida, V.A.F.},
 year = {1999},
 pages = {69--75},
 publisher = {IEEE},
 title = {Characterizing response time of WWW caching proxy servers},
 date = {1999},
 doi = {10.1109/WWC.1998.809360},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809360},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809360.pdf?arnumber=809360},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Bandwidth, Data analysis, Delay, Europe, Europe, Internet, Internet, Measurement, South America, South America, Telecommunication traffic, USA, Web latency, Web server, World Wide Web, World Wide Web, bandwidth, cache storage, caching proxy servers, client links, file servers, file size, information resources, international proxies, log data, network traffic, performance characterization, performance evaluation, performance modeling, response time, variability, },
 abstract = {Caching proxies have an important role in the infrastructure of the World Wide Web (WWW). They save network traffic and reduce Web latency. While they have been largely deployed in the WWW, little is known about Web proxy behavior, and in particular about international proxies. This paper presents an analysis of caching proxy response times, based on logs from real proxies located in the USA, Europe and South America. We found that high variability is an invariant in caching response times across log data of different proxies. We show that the high variability can be explained through a combination of factors, such as the high variability in the file sizes and bandwidth of the client links to the caching proxies. Finally, we discuss the implications of high variability in the proxy behavior on performance characterization and modeling },
}

@inproceedings{809362,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Radhakrishnan, R. and Rawson, F.L., III},
 year = {1999},
 pages = {76--83},
 publisher = {IEEE},
 title = {Characterizing the behavior of Windows NT Web server workloads using processor performance counters},
 date = {1999},
 doi = {10.1109/WWC.1998.809362},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809362},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809362.pdf?arnumber=809362},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Application software, Counting circuits, HTML, Hardware, Java, Microarchitecture, Microsoft Windows NT, Operating systems, Pentium Pro PCs, Pentium microprocessors, Personal communication networks, Web server, Workstations, World Wide Web server workload, branch instructions, branch prediction, cycles per instruction, graphical user interfaces, hardware environment, information resources, instruction set architecture, instruction set parallelism, micro-operation level, microcomputer applications, network servers, operating system environment, operating systems (computers), performance evaluation, processor performance counters, processor timestamp, resource usage, server application programs, },
 abstract = {Studies the behavior of modern World Wide Web servers and server application programs to understand how they interact with the underlying hardware and operating system (OS) environments. We characterize the workload placed on both Pentium and Pentium Pro PCs running Windows NT Workstations 4.0 by three simple Web serving scenarios using the processor timestamp and performance counters. We used both the Pentium and the Pentium Pro to investigate the effect on the workloads of two processors that have the same instruction set architecture, but which have rather different microarchitectures. The workload shows a high percentage of branch instructions with only fair branch prediction for both processors. The numbers from the Pentium suggest a very low level of available instruction set parallelism at the instruction set architecture level while the improvement in the cycles per instruction (CPI) on the Pentium Pro indicates that there is more parallelism at the micro-operation level, even though the code makes somewhat inefficient use of the available resources },
}

@inproceedings{1249058,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Ahmad, I. and Anderson, J.M. and Holler, A.M. and Kambo, R. and Makhija, V.},
 year = {2003},
 pages = { 65-- 76},
 publisher = {IEEE},
 title = {An analysis of disk performance in VMware ESX server virtual machines},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249058},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249058},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249058.pdf?arnumber=1249058},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { I/O behavior,  VMware ESX server,  benchmark testing,  digital storage,  disk microbenchmarks,  disk performance analysis,  file server,  file servers,  hardware resources,  mail server,  multiplexing,  multiplexing,  native systems,  performance evaluation,  server architecture,  software platform,  storage management,  storage subsystem,  storage systems,  virtual machines,  virtual machines, Application software, Computer architecture, Electronic mail, File servers, Hardware, Performance analysis, Throughput, Virtual machining, Virtual manufacturing, Voice mail, },
 abstract = {VMware ESX Server is a software platform that efficiently multiplexes the hardware resources of a server among virtual machines. This paper studies the performance of a key component of the ESX Server architecture: its storage subsystem. We characterize the performance of native systems and virtual machines using a series of disk microbenchmarks on several different storage systems. We show that the virtual machines perform well compared to native, and that the I/O behavior of virtual machines closely matches that of the native server. We then discuss how the microbenchmarks can be used to estimate virtual machine performance for disk-intensive applications by studying two workloads: a simple file server and a commercial mail server. },
}

@inproceedings{809368,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Yong Luo and Cameron, K.W.},
 year = {1999},
 pages = {125--131},
 publisher = {IEEE},
 title = {Instruction-level characterization of scientific computing applications using hardware performance counters},
 date = {1999},
 doi = {10.1109/WWC.1998.809368},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809368},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809368.pdf?arnumber=809368},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {CPI0, Counting circuits, Hardware, Kirk field collapse effect, Laboratories, Microprocessors, Pipelines, Postal services, Power system modeling, Process design, Scientific computing, architectural limitations, cache storage, characterization methods, code characterization, computer architecture, empirical performance counter measurements, empirical/analytical modeling, hardware performance counters, ideal CPI, instruction sets, instruction-level characterization, memory hierarchy, natural sciences computing, outstanding miss utilization, perfect L1 cache, performance evaluation, processor/code development, scientific computing applications, stall time, },
 abstract = {The paper provides characterization methods based on empirical performance counter measurements. In particular, we provide an instruction-level characterization derived empirically in an effort to demonstrate how architectural limitations in underlying hardware will affect the performance of existing codes. Preliminary results provide promise in code characterization, and empirical/analytical modeling. These include the ability to quantify outstanding miss utilization and stall time attributable to architectural limitations in the CPU and the memory hierarchy. This work further promises insight into quantifying bounds for CPI<sub>0</sub> or the ideal CPI with infinite, perfect L1 cache. In general, if we can characterize workloads using parameters that are independent of architecture, such as this work, then we can more appropriately compare different architectures in an effort to direct processor/code development },
}

@inproceedings{809369,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Kanjo, R.},
 year = {1999},
 pages = {132--141},
 publisher = {IEEE},
 title = {Analysis of Pro/ENGINEER V19 Bench98 benchmark on a platform based on the Pentium(R) II XeonTM processor},
 date = {1999},
 doi = {10.1109/WWC.1998.809369},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809369},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809369.pdf?arnumber=809369},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {82440GX AGPset, CAD, Counting circuits, Frequency, Graphics, IA-32 workstation platform, Microprocessors, Pentium II Xeon processor, Phased arrays, Pro/ENGINEER V19 Bench98 benchmark, Pro/ENGINEER V19 benchmark, SPEC95, Semiconductor device measurement, Size measurement, Time measurement, Velocity measurement, Workstations, computer architecture, mechanical engineering computing, memory behavior, performance evaluation, platform components, workstations, },
 abstract = {The paper characterizes the behavior of the latest Pro/ENGINEER V19 benchmark, Bench98<sup>TM</sup>, on an IA-32 workstation platform based on the recently introduced Pentium(R) II Xeon<sup>TM</sup> processor and 82440GX AGPset. The paper investigates the sensitivity of the benchmark to the size and/or speed of various platform components and analyzes the floating point, branching, caching, and memory behavior of the benchmark, comparing the results to the measurements obtained using SPEC95 },
}

@inproceedings{4362187,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Bolme, D.S. and Strout, M. and Beveridge, J.R.},
 year = {2007},
 pages = {114--119},
 publisher = {IEEE},
 title = {FacePerf: Benchmarks for Face Recognition Algorithms},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362187},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362187},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362187.pdf?arnumber=4362187},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Benchmark testing, Biometrics, C biometric performance benchmark algorithms, C++ biometric performance benchmark algorithms, Clocks, Computer science, Computer vision, Face detection, Face recognition, FacePerf benchmarks, Haar transforms, Haar-based face detection, Image databases, OpenCV cascade classifier, Principal component analysis, Space technology, biometrics (access control), computational complexity, computer vision, computer vision, cosine approximation, elastic bunch graph matching, face recognition, face recognition algorithms, graph theory, image classification, image matching, principal component analysis, principal components analysis, },
 abstract = {In this paper we present a collection of C and C++ biometric performance benchmark algorithms called FacePerf. The benchmark includes three different face recognition algorithms that are historically important to the face recognition community: Haar-based face detection, Principal Components Analysis, and Elastic Bunch Graph Matching. The algorithms are fast enough to be useful in realtime systems; however, improving performance would allow the algorithms to process more images or search larger face databases. Bottlenecks for each phase in the algorithms have been identified. A cosine approximation was able to reduce the execution time of the Elastic Bunch Graph Matching implementation by 32\%. },
}

@inproceedings{4362186,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Scott, M.L. and Spear, M.F. and Dalessandro, L. and Marathe, V.J.},
 year = {2007},
 pages = {107--113},
 publisher = {IEEE},
 title = {Delaunay Triangulation with Transactions and Barriers},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362186},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362186},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362186.pdf?arnumber=4362186},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {C++, C++ language, Computer science, Costs, Delaunay triangulation, High performance computing, Java, Multicore processing, Open source software, Partitioning algorithms, RSTM software transactional memory library, Software libraries, Time sharing computer systems, Yarn, barrier-separated phases, geometrically partitioned regions triangulate, mesh generation, multicore machines, multiprocessing systems, multiprocessor machines, multithreaded programs, nontrivial transactional programs, open-source implementation, public domain software, sequential algorithms, transactional memory, },
 abstract = {Transactional memory has been widely hailed as a simpler alternative to locks in multithreaded programs, but few nontrivial transactional programs are currently available. We describe an open-source implementation of Delaunay triangulation that uses transactions as one component of a larger parallelization strategy. The code is written in C+ +, for use with the RSTM software transactional memory library (also open source). It employs one of the fastest known sequential algorithms to triangulate geometrically partitioned regions in parallel; it then employs alternating, barrier-separated phases of transactional and partitioned work to stitch those regions together. Experiments on multiprocessor and multicore machines confirm excellent single-thread performance and good speedup with increasing thread count. Since execution time is dominated by geometrically partitioned computation, performance is largely insensitive to the overhead of transactions, but highly sensitive to any costs imposed on shamble data that are currently "privatized". },
}

@inproceedings{4362185,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Lee, G.L. and Ahn, D.H. and de Supinski, B.R. and Gyllenhaal, J. and Miller, P.},
 year = {2007},
 pages = {101--106},
 publisher = {IEEE},
 title = {Pynamic: the Python Dynamic Benchmark},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362185},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362185},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362185.pdf?arnumber=4362185},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Joining processes, Large-scale systems, Libraries, MPI, Operating systems, Productivity, Programming profession, Pynamic benchmark, Scientific computing, Stress, System software, computational steering, dynamically linked library, message passing, message passing, natural sciences computing, operating system, operating systems (computers), python dynamic benchmark, software libraries, system software, },
 abstract = {Python is widely used in scientific computing to facilitate application development and to support features such as computational steering. Making full use of some of Python's popular features, which improve programmer productivity, leads to applications that access extremely high numbers of dynamically linked libraries (DLLs). As a result, some important Python-based applications severely stress a system's dynamic linking and loading capabilities and also cause significant difficulties for most development environment tools, such as debuggers. Furthermore, using the Python paradigm for large scale MPI-based applications can create significant file IO and further stress tools and operating systems. In this paper, we present Pynamic, the first benchmark program to support configurable emulation of a wide-range of the DLL usage of Python-based applications for large scale systems. Pynamic has already accurately reproduced system software and tool issues encountered by important large Python-based scientific applications on our supercomputers. Pynamic provided insight for our system software and tool vendors, and our application developers, into the impact of several design decisions. As we describe the Pynamic benchmark, we will highlight some of the issues discovered in our large scale system software and tools using Pynamic. },
}

@inproceedings{4362184,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Sachdeva, V. and Speight, E. and Stephenson, M. and Lei Chen},
 year = {2007},
 pages = {89--97},
 publisher = {IEEE},
 title = {Characterizing and Improving the Performance of Bioinformatics Workloads on the POWER5 Architecture},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362184},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362184},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362184.pdf?arnumber=4362184},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Bioinformatics, Computational biology, Computer architecture, DNA, Databases, Genomics, Hidden Markov models, POWER5 architecture, Performance analysis, Sequences, bioinformatics workloads, biology computing, performance counter information, },
 abstract = {This paper examines several mechanisms to improve the performance of life science applications on high-performance computer architectures typically designed for more traditional supercomputing tasks. In particular, we look at the detailed performance characteristics of some of the most popular sequence alignment and homology applications on the POWERS architecture offering from IBM. Through detailed analysis of performance counter information collected from the hardware, we identify the main performance bottleneck in the current POWER5 architecture for these applications is the high branch misprediction penalty of the most time-consuming kernels of these codes. Utilizing our PowerPC full system simulation environment, we show the performance improvement afforded by adding conditional assignments to the PowerPC ISA. We also show the impact of changing the number of functional units to a more appropriate mix for the characteristics of bioinformatics applications. Finally, we examine the benefit of removing the two-cycle penalty currently in the POWERS architecture for taken branches due to the lack of a branch target buffer. Addressing these three performance-limiting aspects provides an average 64\% improvement in application performance. },
}

@inproceedings{4362183,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Qiang Wu and Yong Liao and Wolf, T. and Lixin Gao},
 year = {2007},
 pages = {79--88},
 publisher = {IEEE},
 title = {Benchmarking BGP Routers},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362183},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362183},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362183.pdf?arnumber=4362183},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Border Gateway Protocol, Communication system control, Communication system traffic control, Computer architecture, Computer networks, Data communication, Joining processes, Peer to peer computing, Routing protocols, Web and internet services, Workstations, commercial router, computer networks, computer networks, dual-core workstation, embedded network processor, forwarding traffic, protocols, telecommunication network routing, unicore workstation, },
 abstract = {Determining which routes to use when forwarding traffic is one of the major processing tasks in the control plane of computer networks. We present a novel benchmark that evaluates the performance of the most commonly used Internet-wide routing protocol, the Border Gateway Protocol (BGP). Using this benchmark, we evaluate four different systems that implement BGP, including a uni-core and a dual-core workstation, an embedded network processor, and a commercial router. We present performance results for these systems under various loads of cross-traffic and explore the tradeoffs between different system architectures. Our observations help identify bottlenecks and limitations in current systems and can lead to next-generation router architectures that are better optimized for this important workload. },
}

@inproceedings{4362182,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Shiv, K. and Iyer, R. and Bhat, M. and Illikkal, R. and Jones, M. and Makineni, S. and Domer, J. and Newell, D.},
 year = {2007},
 pages = {66--75},
 publisher = {IEEE},
 title = {Addressing Cache/Memory Overheads in Enterprise Java CMP Servers},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362182},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362182},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362182.pdf?arnumber=4362182},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Current measurement, Emulation, Frequency, Hardware, Intel core 2 Duo Xeon platform, Java, Java, Memory architecture, Microprocessors, Performance analysis, Random access memory, Yarn, architectural optimization, cache storage, cache/memory overhead, chip multiprocessor architecture, computer architecture, data-less cache line initialization, electronic engineering computing, enterprise Java CMP server, execution-driven emulation, hardware-guided thread collocation, microprocessor chips, on-socket DRAM cache, trace-driven simulation, },
 abstract = {As we enter the era of chip multiprocessor (CMP) architectures, it is important that we explore the scaling characteristics of mainstream server workloads on these platforms. In this paper, we analyze the performance of two significant enterprise Java workloads (SPECjAppServer2004 and SPECjbb2005) on CMP platforms -present and future. We start by characterizing the core, cache and memory behavior of these workloads on the newly released Intel core 2 Duo Xeon platform (dual-core, dual-socket). Our findings from these measurements indicate that these workloads have a significant performance dependence on cache and memory subsystems. In order to guide the evolution of future CMP platforms, we perform a detailed investigation of potential cache and memory architecture choices. This includes analyzing the effects of thread sharing and migration, object allocation and garbage collection. Based on the observed behavior, we propose architectural optimizations along three dimensions: (a) data-less cache line initialization (DCLI), (b) hardware-guided thread collocation (HGTC) and (c) on-socket DRAM caches (OSDC). In this paper, we will describe these optimizations in detail and validate their performance potential based on trace-driven simulations and execution-driven emulation. Overall, we expect that the findings in this paper will guide future CMP architectures for enterprise Java servers. },
}

@inproceedings{4362181,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Tseng, J.H. and Hao Yu and Nagar, S. and Dubey, N. and Franke, H. and Pattnaik, P.},
 year = {2007},
 pages = {57--65},
 publisher = {IEEE},
 title = {Performance Studies of Commercial Workloads on a Multi-core System},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362181},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362181},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362181.pdf?arnumber=4362181},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Databases, Delay, Hardware, Humans, Marketing and sales, Multicore processing, Production planning, Sun, Throughput, Yarn, commercial workload, computer architecture, latency memory operation, multi-threading, multicore system, performance scalability, thread-placement sensitivity, },
 abstract = {The multi-threaded nature of many commercial applications makes them seemingly a good fit with the increasing number of available multi-core architectures. This paper presents our performance studies of a collection of commercial workloads on a multi-core system that is designed for total throughput. The selected workloads include full operational applications such as SAP-SD and IBM Trade, and popular synthetic benchmarks such as SPECjbb2005, SPEC SDET, Dbench, and Tbench. To evaluate the performance scalability and the thread-placement sensitivity, we monitor the application throughput, processor performance, and the memory subsystem of 8, 16, 24, and 32 hardware threads with (a) increasing number of cores and (b) increasing number of threads per core. We observe that these workloads scale close to linearly (with efficiencies ranging from 86\% to 99\%) with increasing number of cores. For scaling with hardware-threads per core, the efficiencies are between 50\% and 70\%. Furthermore, among other observations, our data show that the ability of hiding long latency memory operations (i.e. L2 misses) in a multi-core system enables the performance scaling. },
}

@inproceedings{4362180,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Jerger, N.E. and Vantrease, D. and Lipasti, M.},
 year = {2007},
 pages = {47--56},
 publisher = {IEEE},
 title = {An Evaluation of Server Consolidation Workloads for Multi-Core Designs},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362180},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362180},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362180.pdf?arnumber=4362180},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Communication system control, Computer science, Costs, Design engineering, Interference, Job shop scheduling, Space technology, Sun, Yarn, cache sharing, cache storage, chip multiprocessors, file servers, logic design, microprocessor chips, multicore design, multiprocessing systems, processor scheduling, server consolidation workload scheduling, },
 abstract = {While chip multiprocessors with ten or more cores will be feasible within a few years, the search for applications that fully exploit their attributes continues. In the meantime, one sure-fire application for such machines will be to serve as consolidation platforms for sets of workloads that previously occupied multiple discrete systems. Such server consolidation scenarios will simplify system administration and lead to savings in power, cost, and physical infrastructure. This paper studies the behavior of server consolidation workloads, focusing particularly on sharing of caches across a variety of configurations. Noteworthy interactions emerge within a workload, and notably across workloads, when multiple server workloads are scheduled on the same chip. These workloads present an interesting design point and will help designers better evaluate trade-offs as we push forward into the many-core era. },
}

@inproceedings{4362189,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Moseley, T. and Grunwald, D. and Peri, R.},
 year = {2007},
 pages = {129--138},
 publisher = {IEEE},
 title = {Seekable Compressed Traces},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362189},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362189},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362189.pdf?arnumber=4362189},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Compression algorithms, Computational modeling, Computer science, Computer simulation, Instruments, Optimizing compilers, Random access memory, SPEC2000 memory address trace, Sampling methods, Scheduling, System testing, data compression, harmonic mean compression rate, processor simulation, program diagnostics, program slicing, program tracing, seekable compressed trace, value prediction, },
 abstract = {Program traces are commonly used for purposes such as profiling, processor simulation, and program slicing. Uncompressed, these traces are often too large to exist on disk. Although existing trace compression algorithms achieve high compression rates, they sacrifice the accessibility of uncompressed traces; typical compressed traces must be traversed linearly to reach a desired position in the stream. This paper describes seekable compressed traces that allow arbitrary positioning in the compressed data stream. Furthermore, we enhance existing value prediction based techniques to achieve higher compression rates, particularly for difficult-to-compress traces. Our base algorithm achieves a harmonic mean compression rate for SPEC2000 memory address traces that is 3.47 times better than existing methods. We introduce the concept of seekpoints that enable fast seeking to positions evenly distributed throughout a compressed trace. Adding seekpoints enables rapid sampling and backwards traversal of compressed traces. At a granularity of every 10 M instructions, seekpoints only increase trace sizes by an average factor of 2.65. },
}

@inproceedings{4362188,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Alvarez, M. and Salami, E. and Ramirez, A. and Valero, M.},
 year = {2007},
 pages = {120--125},
 publisher = {IEEE},
 title = {HD-VideoBench. A Benchmark for Evaluating High Definition Digital Video Applications},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362188},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362188},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362188.pdf?arnumber=4362188},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Availability, Code standards, Computer architecture, Decoding, H.264 video standards, HD-VideoBench, High definition video, Licenses, MPEG-2, MPEG-4, Optimization, Video codecs, Video coding, Video compression, code quality-portability, code standards, data compression, high definition digital video processing, high definition video, high performance optimization, video application domain, video codecs, video codecs, video coding, video encoder-decoder, },
 abstract = {HD-VideoBench is a benchmark devoted to high definition (HD) digital video processing. It includes a set of video encoders and decoders (Codecs) for the MPEG-2, MPEG-4 and H.264 video standards. The applications were carefully selected taken into account the quality and portability of the code, the representativeness of the video application domain, the availability of high performance optimizations and the distribution under a free license. Additionally, HD-VideoBench defines a set of input sequences and configuration parameters of the video Codecs which are appropriate for the HD video domain. },
}

@inproceedings{5306801,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Kerr, A. and Diamos, G. and Yalamanchili, S.},
 year = {2009},
 pages = {3--12},
 publisher = {IEEE},
 title = {A characterization and analysis of PTX kernels},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306801},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306801},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306801.pdf?arnumber=5306801},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Acceleration, Application software, C language, C-based programming environment, Computer applications, GPU, Instruction sets, Kernel, NVIDIA CUDA SDK, OpenCL, PTX kernel, Parallel processing, Program processors, Programming environments, Software tools, UIUC Parboil Benchmark Suite, Yarn, application re-structuring, branch reconvergence, compiler optimization, compute-intensive application, control flow, cost-effective approach, data flow, data flow analysis, data parallel execution, data-intensive application, full function emulator, general purpose application development, low level virtual ISA, machine model, memory behavior, microarchitecture design, multi-threading, operating system kernels, optimising compilers, parallel thread execution architecture, parallelism, program behavior analysis, programming environments, software tool, virtual machine, virtual machines, },
 abstract = {General purpose application development for GPUs (GPGPU) has recently gained momentum as a cost-effective approach for accelerating data- and compute-intensive applications. It has been driven by the introduction of C-based programming environments such as NVIDIA's CUDA, OpenCL, and Intel's Ct. While significant effort has been focused on developing and evaluating applications and software tools, comparatively little has been devoted to the analysis and characterization of applications to assist future work in compiler optimizations, application re-structuring, and micro-architecture design. This paper proposes a set of metrics for GPU workloads and uses these metrics to analyze the behavior of GPU programs. We report on an analysis of over 50 kernels and applications including the full NVIDIA CUDA SDK and UIUC's Parboil Benchmark Suite covering control flow, data flow, parallelism, and memory behavior. The analysis was performed using a full function emulator we developed that implements the NVIDIA virtual machine referred to as PTX (parallel thread execution architecture) - a machine model and low level virtual ISA that is representative of ISAs for data parallel execution. The emulator can execute compiled kernels from the CUDA compiler, currently supports the full PTX 1.4 specification, and has been validated against the full CUDA SDK. The results quantify the importance of optimizations such as those for branch reconvergence, the prevalance of sharing between threads, and highlights opportunities for additional parallelism. },
}

@inproceedings{5306800,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Ranganathan, P.},
 year = {2009},
 pages = {1--2},
 publisher = {IEEE},
 title = {Green clouds and black swans in the exascale era},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306800},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306800},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306800.pdf?arnumber=5306800},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Biographies, Clouds, Energy efficiency, Energy management, Hardware, Modeling, Project management, YouTube, black swan event, computer centres, data center, exascale era, green cloud, petascale, systems analysis, systems design, workload characterization, workload understanding, },
 abstract = {Summary form only given. The petascale milestone is behind us, and the next grand challenge is to design systems and datacenters for the exascale era (10^18 flops). this talk will speculate on challenges and opportunities in understanding and characterizing workloads in the exascale era, with particular emphasis on potential ldquoblack swan events<sup>*</sup>rdquo. },
}

@inproceedings{5306803,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {},
 year = {2009},
 pages = {vii--x},
 publisher = {IEEE},
 title = {Table of contents},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306803},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306803},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306803.pdf?arnumber=5306803},
 isbn = {978-1-4244-5156-2},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/05306803.png" border="0"> },
}

@inproceedings{5306802,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {},
 year = {2009},
 pages = {vi--vi},
 publisher = {IEEE},
 title = {IISWC 2009 reviewers},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306802},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306802},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306802.pdf?arnumber=5306802},
 isbn = {978-1-4244-5156-2},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/05306802.png" border="0"> },
}

@inproceedings{5306805,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {},
 year = {2009},
 pages = {v--v},
 publisher = {IEEE},
 title = {IISWC 2009 organizing committee},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306805},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306805},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306805.pdf?arnumber=5306805},
 isbn = {978-1-4244-5156-2},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/05306805.png" border="0"> },
}

@inproceedings{5306804,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Conte, Tom},
 year = {2009},
 pages = {iv--iv},
 publisher = {IEEE},
 title = {Message from the program chair},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306804},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306804},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306804.pdf?arnumber=5306804},
 isbn = {978-1-4244-5156-2},
 language = {English},
 abstract = {It is my pleasure to present to you the program of the 2009 fifth annual IEEE International Symposium on Workload Characterization. We received 56 papers, of which we accepted 23 to the symposium. Each paper received on average 3.5 reviews. The program committee worked tirelessly to do these reviews, largely by themselves, or with the help of colleagues in a few rare cases. The program committee then met in Austin, TX in person and via teleconference in June to do the hard work of deciding which papers made the cut. This was no easy process, and I am indebted to many on the program committee for their hard work. In particular, I am quite grateful to those who helped shepherd papers that needed a few light revisions. These include of IBM Research, of Georgia Tech, Leslie Barnes of AMD and David Kaeli of Northeastern University. },
}

@inproceedings{5306807,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Chiou, Derek},
 year = {2009},
 pages = {iii--iii},
 publisher = {IEEE},
 title = {Message from the general chair},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306807},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306807},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306807.pdf?arnumber=5306807},
 isbn = {978-1-4244-5156-2},
 language = {English},
 abstract = {Welcome to the fifth annual IEEE International Symposium on Workload Characterization being held from October 4th to October 6th, 2009 at the AT\&#x00026;T Conference Center at the edge of the University of Texas at Austin campus. IISWC started in Austin as the Workshop on Workload Characterization and has not been held in Austin since the very first IISWC held in 2005. It's good to be back. },
}

@inproceedings{4636102,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Pereira, C. and Patil, H. and Calder, B.},
 year = {2008},
 pages = {173--182},
 publisher = {IEEE},
 title = {Reproducible simulation of multi-threaded workloads for architecture design exploration},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636102},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636102},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636102.pdf?arnumber=4636102},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Computational modeling, Computer architecture, Computer science, Computer simulation, Design engineering, Instruments, Registers, Space exploration, Spinning, Yarn, architecture design exploration, binary instrumentation, computer architecture, multi-threading, multiprocessing systems, multiprocessor architecture, multithreaded workload, },
 abstract = {As multiprocessors become mainstream, techniques to address efficient simulation of multi-threaded workloads are needed. Multi-threaded simulation presents a new challenge: non-determinism across simulations for different architecture configurations. If the execution paths between two simulation runs of the same benchmark with the same input are too different, the simulation results cannot be used to compare the configurations. In this paper we focus on a simulation technique to efficiently collect simulation checkpoints for multi-threaded workloads, and to compare simulation runs addressing this non-determinism problem. We focus on user-level simulation of multi-threaded workloads for multiprocessor architectures. We present an approach, based on binary instrumentation, to collect checkpoints for simulation. Our checkpoints allow reproducible execution of the samples across different architecture configurations by controlling the sources of nondeterminism during simulation. This results in stalls that would not naturally occur in execution. We propose techniques that allow us to accurately compare performance across architecture configurations in the presence of these stalls. },
}

@inproceedings{1526012,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Rauch, F.},
 year = {2005},
 pages = { 155-- 162},
 publisher = {IEEE},
 title = {Comprehensive throughput evaluation of LANs in clusters of PCs with Switchbench - or how to bring your switch to its knees},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526012},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526012},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526012.pdf?arnumber=1526012},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { Local Area Network,  PC clusters,  Switchbench,  all-to-all personalised communication test,  benchmark testing,  comprehensive throughput evaluation,  high-speed throughput test,  microbenchmarks,  parallel processing,  performance evaluation,  workstation clusters, Benchmark testing, Clustering algorithms, Communication switching, Joining processes, Knee, Local area networks, Network interfaces, Personal communication networks, Switches, Throughput, },
 abstract = {Understanding the performance of parallel applications for prevalent clusters of commodity PCs is still not an easy task: One must understand performance characteristics of all subsystems in the cluster machine, besides the inherently required knowledge about the applications' behaviour. While there are already many benchmarks that characterise a single node's subsystems like CPU, memory and I/O. as well as a few to evaluate its network interface with point-to-point data streams, there are to the best of our knowledge no benchmarks available that characterise a cluster network or LAN as a whole. We present Switchbench (2005), a set of three microbenchmarks that thoroughly evaluate the throughput characteristics of networks for clusters. A first microbenchmark tests the basic processing limitations of the switches, by sending and receiving data concurrently at maximum throughputs on all network interfaces. A second microbenchmark tests arbitrary communication patterns by pairwise connecting nodes for high-speed throughput tests. A third and slightly more realistic microbenchmark executes an all-to-all personalised communication (AAPC) algorithm to test many different patterns and critical bisections in the network. The microbenchmarks already proved to be extremely useful in a previous study to experimentally quantify performance limitations in different networks of clusters of PCs with up to 128 nodes. We also establish the suitability of our microbenchmarks by comparing their results with two application benchmarks. The benchmarks consist of two C programs supported by shell scripts to start the programs on all nodes of the cluster with the correct execution parameters to automatically scale the workloads from a few nodes up to the full cluster size. },
}

@inproceedings{1526013,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Bader, D.A. and Yue Li and Tao Li and Sachdeva, V.},
 year = {2005},
 pages = { 163-- 173},
 publisher = {IEEE},
 title = {BioPerf: a benchmark suite to evaluate high-performance computer architecture on bioinformatics applications},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526013},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526013},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526013.pdf?arnumber=1526013},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { Alpha binaries,  Apple G5 executions,  BioPerf,  IBM Mambo simulations,  benchmark suite,  benchmark testing,  bioinformatics,  biology computing,  computational biology,  computer architecture,  high-performance computer architecture evaluation,  parallel codes, Application software, Bioinformatics, Biological system modeling, Biology computing, Computational modeling, Computer architecture, Genomics, Information analysis, Large-scale systems, Sequences, },
 abstract = {The exponential growth in the amount of genomic data has spurred growing interest in large scale analysis of genetic information. Bioinformatics applications, which explore computational methods to allow researchers to sift through the massive biological data and extract useful information, are becoming increasingly important computer workloads. This paper presents BioPerf a benchmark suite of representative bioinformatics applications to facilitate the design and evaluation of high-performance computer architectures for these emerging workloads. Currently, the BioPerf suite contains codes from 10 highly popular bioinformatics packages and covers the major fields of study in computational biology such as sequence comparison, phylogenetic reconstruction, protein structure prediction, and sequence homology \& gene finding. We demonstrate the use of BioPerf by providing simulation points of pre-compiled Alpha binaries and with a performance study on IBM Power using IBM Mambo simulations cross-compared with Apple G5 executions. The BioPerf suite (available from www.bioperf.org) includes benchmark source code, input datasets of various sizes, and information for compiling and using the benchmarks. Our benchmark suite includes parallel codes where available. },
}

@inproceedings{4362178,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Sarikaya, R. and Buyuktosunoglu, A.},
 year = {2007},
 pages = {25--34},
 publisher = {IEEE},
 title = {Predicting Program Behavior Based On Objective Function Minimization},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362178},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362178},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362178.pdf?arnumber=4362178},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Adaptive systems, Closed-form solution, Computer errors, Energy management, Least squares methods, Power system management, Power system reliability, Prediction methods, Predictive models, Temperature, accumulated squared error, autoregressive model, autoregressive processes, computer system, instruction per cycle, objective function minimization, optimal metric prediction problem, predictive least square prediction, program behavior prediction, reverse engineering, software metrics, system monitoring, total squared error, },
 abstract = {Computer systems increasingly rely on dynamic management of their operations with the goal of optimizing an individual or joint metric involving performance, power, temperature, reliability and so on. Such an adaptive system requires an accurate, reliable, and practically viable metric predictors to invoke the dynamic management actions in a timely and efficient manner. Unlike ad-hoc predictors proposed in the past, we propose a unified prediction method in which the optimal metric prediction problem is considered as that of minimizing an objective function. Choice of the objective function and the model type determines the form of the solution whether it is a closed form or one that is numerically determined through optimization. We formulate two particular realizations of the unified prediction method by using the total squared error and accumulated squared error as the objective functions in conjunction with autoregressive models. Under this scenario, the unified prediction method becomes linear prediction and the predictive least square (PLS) prediction, respectively. For both of these predictors, there is a analytical closed form solution that determines model parameters. Experimental results with prediction of instruction per cycle (IPC) and L1 cache miss rate metrics demonstrate superior performance for the proposed predictors over the last value predictor on SPECCPU 2000 benchmarks where in some cases the mean absolute prediction error is reduced by as much as 10-fold. },
}

@inproceedings{1526011,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Munson, T.S. and Hovland, P.D.},
 year = {2005},
 pages = { 150-- 154},
 publisher = {IEEE},
 title = {The FeasNewt benchmark},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526011},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526011},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526011.pdf?arnumber=1526011},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { FeasNewt mesh-quality optimization benchmark,  Hessian assembly,  Hessian evaluation,  benchmark testing,  finite volume methods,  finite-element PDE solver,  finite-volume PDE solvers,  floating-point operations,  gradient evaluation,  gradient methods,  mathematics computing,  memory access patterns,  mesh generation,  mesh smoothing,  nonlinear differential equations,  nonlinear partial differential equations,  optimisation,  partial differential equations,  runtime data-reordering,  runtime iteration-reordering,  sparse matrices,  sparse matrix-vector products, Assembly, Computer science, Finite element methods, Jacobian matrices, Laboratories, Mathematics, Optimization methods, Partial differential equations, Runtime, Sparse matrices, },
 abstract = {We describe the FeasNewt mesh-quality optimization benchmark. The performance of the code is dominated by three phases - gradient evaluation, Hessian evaluation and assembly, and sparse matrix-vector products - that have very different mixtures of floating-point operations and memory access patterns. The code includes an optional runtime data- and iteration-reordering phase, making it suitable for research on irregular memory access patterns. Mesh-quality optimization (or "mesh smoothing") is an important ingredient in the solution of nonlinear partial differential equations (PDEs) as well as an excellent surrogate for finite-element or finite-volume PDE solvers. },
}

@inproceedings{1526016,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Schaelicke, L. and Freeland, J.C.},
 year = {2005},
 pages = { 188-- 196},
 publisher = {IEEE},
 title = {Characterizing sources and remedies for packet loss in network intrusion detection systems},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526016},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526016},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526016.pdf?arnumber=1526016},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { computer networks,  full-system simulation,  interrupt aggregation,  modern workstation architecture,  open source network intrusion detection systems,  packet loss minimization,  performance evaluation,  public domain software,  rule set pruning,  security of data,  system-level optimization,  telecommunication security, Communication system control, Communication system traffic control, Computer science, High-speed networks, Intelligent networks, Intrusion detection, Telecommunication traffic, Throughput, Traffic control, Workstations, },
 abstract = {Network intrusion detection is becoming an increasingly important tool to protect critical information and infrastructure from unauthorized access. Network intrusion detection systems (NIDS) are commonly based on general-purpose workstations connected to a network tap. However, these general-purpose systems, although cost-efficient, are not able to sustain the packet rates of modern high-speed networks. The resulting packet loss degrades the system's overall effectiveness, since attackers can intentionally overload the NIDS to evade detection. This paper studies the performance requirements of a commonly used open-source NIDS on a modern workstation architecture. Using full-system simulation, this paper characterizes the impact of a number of system-level optimizations and architectural trends on packet loss, and highlights the key bottlenecks for this type of network-intensive workloads. Results suggest that interrupt aggregation combined with rule set pruning is most effective in minimizing packet loss. Surprisingly, the workload also exhibits sufficient locality to benefit from larger level-2 caches as well. On the other hand, many other common architecture and system optimizations have only a negligible impact on throughput. },
}

@inproceedings{1226499,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Zhiyuan Li and Rong Xu},
 year = {2002},
 pages = { 109-- 117},
 publisher = {IEEE},
 title = {Energy impact of secure computation on a handheld device},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226499},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226499},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226499.pdf?arnumber=1226499},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { IPsec,  communication cost,  computation offloading,  computational tasks,  cryptography,  energy consumption,  energy impact,  handheld device,  multimedia applications,  multimedia systems,  notebook computers,  performance evaluation,  performance evaluation,  secure computation,  security mechanism,  wireless LAN,  wireless LAN, },
 abstract = {Computation offloading is an important approach to save the energy consumption while improving performance for wireless networked handheld devices. With such an approach, computational tasks are offloaded from the handheld device to a server, depending on the tradeoff between the communication cost and the computation cost. Adding security to the wireless network changes the relative cost of computation and communication. In this paper, we measure the energy consumption characteristics of multimedia applications on a handheld device, supported by computation offloading though a wireless LAN which is secured with IPsec. The measurement indicates that despite the overhead of the security mechanism, offloading remains quite effective as a method to reduce program execution time and energy consumption. },
}

@inproceedings{1526014,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Daniel, S. and Faith, R.E.},
 year = {2005},
 pages = { 174-- 177},
 publisher = {IEEE},
 title = {A portable, open-source implementation of the SPC-1 workload},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526014},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526014},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526014.pdf?arnumber=1526014},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { SPC-1 workload,  Storage Performance Council SPC benchmark-1,  benchmark testing,  portable open-source implementation,  public domain software,  software performance evaluation,  storage management, Councils, Delay, Engines, Formal specifications, Home appliances, Open source software, Throughput, Trademarks, },
 abstract = {This paper describes an open-source implementation of the Storage Performance Council's SPC benchmark-1 (SPC-1). Although this implementation cannot be used to generate official SPC-1 results, the code can be used for research and other unofficial work. We begin with a brief introduction to SPC-1, concentrating on qualities necessary to understand and use our implementation. Then we discuss important features of our open-source implementation and how it was validated against the official SPC-1 benchmark implementation. },
}

@inproceedings{1526015,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Kamil, S. and Shalf, J. and Oliker, L. and Skinner, D.},
 year = {2005},
 pages = { 178-- 187},
 publisher = {IEEE},
 title = {Understanding ultra-scale application communication requirements},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526015},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526015},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526015.pdf?arnumber=1526015},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { FFT-based solvers,  communication complexity,  communication topology,  finite-difference method,  integrated performance monitoring,  interconnects,  lattice-Bolzmann method,  multiprocessor interconnection networks,  natural sciences computing,  parallel processing,  particle in cell method,  particle mesh ewald,  performance evaluation,  sparse linear algebra,  ultra-scale application communication requirements, Computer applications, Computer architecture, Costs, Finite difference methods, Linear algebra, Monitoring, Parallel processing, Scalability, Statistics, Topology, },
 abstract = {As thermal constraints reduce the pace of CPU performance improvements, the cost and scalability of future HPC architectures are increasingly dominated by the interconnect. In this paper we perform an in-depth study of the communication requirements across a broad spectrum of important scientific applications, whose computational methods include: finite-difference, lattice-Bolzmann, particle in cell, sparse linear algebra, particle mesh ewald, and FFT-based solvers. We use the IPM (integrated performance monitoring) profiling framework to collect detailed statistics on communication topology and message volume with minimal impact to code performance. By characterizing the parallelism and communication requirements of such a diverse set of applications, we hope to guide architectural choices for the design and implementation of interconnects for future HPC systems. },
}

@inproceedings{4362172,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {},
 year = {2007},
 pages = {v--v},
 publisher = {IEEE},
 title = {IISWC-2007 Reviewers},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362172},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362172},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362172.pdf?arnumber=4362172},
 isbn = {978-1-4244-1561-8},
 language = {English},
 abstract = {},
}

@inproceedings{4362173,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {},
 year = {2007},
 pages = {nil1--nil1},
 publisher = {IEEE},
 title = {Breaker pages},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362173},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362173},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362173.pdf?arnumber=4362173},
 isbn = {978-1-4244-1561-8},
 language = {English},
 abstract = {},
}

@inproceedings{1526018,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {},
 year = {2005},
 pages = { 204-- 204},
 publisher = {IEEE},
 title = {Corporate sponsors},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526018},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526018},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526018.pdf?arnumber=1526018},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{1226498,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Gheewala, A. and Peir, J.-K. and Yen-Kuang Chen and Lai, K.},
 year = {2002},
 pages = { 98-- 106},
 publisher = {IEEE},
 title = {Estimating multimedia instruction performance based on workload characterization and measurement},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226498},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226498},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226498.pdf?arnumber=1226498},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { SIMD capability,  application programs,  benchmarking,  design tradeoffs,  measurement results,  media enhancement instructions,  media instructions,  microprocessor chips,  microprocessors,  multimedia instruction performance estimation,  multimedia systems,  performance evaluation,  processor architects,  sequential segment,  vectorizable segment,  workload characterization,  workload measurement, },
 abstract = {The increasing popularity in multimedia applications provokes microprocessors to include media-enhancement instructions. In this paper, we describe a methodology to estimate performance improvement of a new set of media instructions on emerging applications based on workload characterization and measurement. Application programs are characterized into a sequential segment, a vectorizable segment, and extra data moves for utilizing the SIMD capability of new media instructions. Techniques based on benchmarking and measurement on existing systems are used to estimate the execution time of each segment. Based on the measurement results, the speedup and the additional data moves of using the new media instructions can be estimated to help processor architects and designers evaluate different design tradeoffs. },
}

@inproceedings{4362176,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Chang-Burm Cho and Wangyuan Zhang and Tao Li},
 year = {2007},
 pages = {5--14},
 publisher = {IEEE},
 title = {Characterizing the Effect of Microarchitecture Design Parameters on Workload Dynamic Behavior},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362176},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362176},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362176.pdf?arnumber=4362176},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Linear regression, Microarchitecture, Microprocessors, Out of order, Process design, Runtime, Space exploration, Wavelet analysis, Wavelet coefficients, Wavelet transforms, data flow analysis, error-bounded linear regression models, microarchitecture design parameters, microcomputers, program dynamics, program runtime behavior, regression analysis, runtime workload dynamics, wavelet coefficients, wavelet-based analysis, workload dynamic behavior, },
 abstract = {Program runtime behavior exhibits significant variations across multiple scales. The increasing design complexity and technology scaling make microprocessor performance and efficiency increasingly depend on runtime workload dynamics. Therefore understanding the effect of design parameters on workload dynamics at early, microarchitecture exploration stage is crucial for high-performance and complexity-efficient designs. In this study, we apply wavelet-based analysis to decompose workload dynamics into a series of wavelet coefficients, which represent program behavior ranging from low-resolution approximation to high-resolution detail. We then construct error-bounded linear regression models that relate microarchitecture design parameters to various wavelet coefficients that capture workload dynamics at multiresolution levels. The most significant factors affecting program dynamics at different scales are obtained. To our knowledge, this paper presents the first work on microarchitecture design space exploration focusing on workload dynamics. },
}

@inproceedings{4362177,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Zilles, C. and Rajwar, R.},
 year = {2007},
 pages = {15--24},
 publisher = {IEEE},
 title = {Implications of False Conflict Rate Trends for Robust Software Transactional Memory},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362177},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362177},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362177.pdf?arnumber=4362177},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Analytical models, Atomic measurements, Concurrent computing, Data structures, Hardware, Programming profession, Proposals, Protocols, Robustness, Scalability, concurrency, concurrency control, data footprint, false conflict rate trend, robust software transactional memory, single-thread overhead, storage management, tagged ownership table, tagless organization, transaction processing, word-based software transactional memory system, },
 abstract = {We demonstrate that a common optimization for reducing the single-thread overhead of word-based Software Transactional Memory (STM) systems can have a significant negative impact on their scalability. Specifically, we find that the use of a tagless ownership table incurs false conflicts at a rate that grows superlinearly with both the TM data footprint and concurrency, and that increasing the size of the ownership table results in only a sub-linear reduction in conflict rate. These empirically observed trends are shown to result from the same statistical priniciples responsible for the (so called) "Birthday Paradox," as we demonstrate with an analytical model based on random population of an ownership table by concurrently executing transactions. From this study, we conclude that tagless ownership tables are not a robust approach to supporting transactional memories. Even large tables (\&gt; 64K entries) are only somewhat effective at mitigating false conflicts in the presence of modestly-sized transactions (e.g., 20 cache blocks) and modest degrees of concurrency (e.g., 4 simultaneous transactions). The practical implications of these results are particularly acute for a hybrid TMs, where the small transactions are likely handled in hardware, leaving only the large ones for the STM. For reasonably-sized tables, a tagless organization will almost guarantee a maximum concurrency of 1 for these overflowed transactions. Using a tagged ownership table completely avoids these false conflict problems. },
}

@inproceedings{4362174,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Henning, John L.},
 year = {2007},
 pages = {1--1},
 publisher = {IEEE},
 title = {The SPEC Gorilla Turns One. So What?},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362174},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362174},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362174.pdf?arnumber=4362174},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Computational modeling, Feedback, Microphones, Performance analysis, Software performance, Sun, Text processing, },
 abstract = {SPEC CPU2006 is a 500 pound gorilla of benchmarking, with 1300 results published since its release one year ago (24 August 2006), despite consuming vastly more time and computational resources than its predecessor suites. What have we learned about its workloads during its first year of life? Are there surprises lurking in the code, workloads, or run rules that are difficult to simulate? What characteristics of CPU2006 have proven successful? What does SPEC need to improve in successor suites? Some proposed answers will be provided and time will be reserved for an open microphone. The presenter will also be available during breaks to listen to feedback about the suite. },
}

@inproceedings{4362175,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Herlihy, Maurice},
 year = {2007},
 pages = {2--2},
 publisher = {IEEE},
 title = {Taking Concurrency Seriously: the Multicore Challenge},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362175},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362175},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362175.pdf?arnumber=4362175},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Clocks, Computer architecture, Concurrent computing, Design engineering, Hardware, Manufacturing processes, Multicore processing, Software design, Software systems, Synchronization, },
 abstract = {Computer architecture is undergoing, if not another revolution, then a vigorous shaking-up. The major chip manufacturers have, for the time being, simply given up trying to make processors run faster. Instead, they have recently started shipping "multicore" architectures, in which multiple processors (cores) communicate directly through shared hardware caches, providing increased concurrency instead of increased clock speed. As a result, system designers and software engineers can no longer rely on increasing clock speed to hide software bloat. Instead, they must somehow learn to make effective use of increasing parallelism. This adaptation will not be easy. Conventional synchronization techniques based on locks and conditions are unlikely to be effective in such a demanding environment. Transactional memory is a computational model in which threads synchronize by transactions. This synchronization model promises to alleviate many (perhaps not all) of the problems associated with locking, and there is a growing community of researchers working on both software and hardware support for this approach. This talk will survey the area, with a focus on open research problems. },
}

@inproceedings{809372,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {},
 year = {1999},
 pages = {151--153},
 publisher = {IEEE},
 title = {Subject Index},
 date = {1999},
 doi = {10.1109/WWC.1998.809372},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809372},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809372.pdf?arnumber=809372},
 isbn = {0-7695-0450-7},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00809372.png" border="0"> },
}

@inproceedings{5650339,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Sarikaya, R. and Isci, C. and Buyuktosunoglu, A.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Runtime workload behavior prediction using statistical metric modeling with application to dynamic power management},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650339},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650339},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650339.pdf?arnumber=5650339},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Computational modeling, Data models, History, Maximum likelihood estimation, Measurement, Natural languages, Predictive models, adaptive computing systems, dynamic power management, last-value prediction, maximum likelihood estimation, maximum likelihood estimation, microprocessor chips, microprocessors, parameter estimation, performance management schemes, power aware computing, probability, probability distribution, runtime workload behavior prediction, smoothing method, smoothing methods, statistical metric modeling, table-based prediction, },
 abstract = {Adaptive computing systems rely on accurate predictions of workload behavior to understand and respond to the dynamically-varying application characteristics. In this study, we propose a Statistical Metric Model (SMM) that is system-and metric-independent for predicting workload behavior. SMM is a probability distribution over workload patterns and it attempts to model how frequently a specific behavior occurs. Maximum Likelihood Estimation (MLE) criterion is used to estimate the parameters of the SMM. The model parameters are further refined with a smoothing method to improve prediction robustness. The SMM learns the application patterns during runtime as applications run, and at the same time predicts the upcoming program phases based on what it has learned so far. An extensive and rigorous series of prediction experiments demonstrates the superior performance of the SMM predictor over existing predictors on a wide range of benchmarks. For some of the benchmarks, SMM improves prediction accuracy by 10X and 3X, compared to the existing last-value and table-based prediction approaches respectively. SMM's improved prediction accuracy results in superior power-performance trade-offs when it is applied to dynamic power management. },
}

@inproceedings{809370,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {Kalamatianos, J. and Kaeli, D. and Chaiken, R.},
 year = {1999},
 pages = {142--149},
 publisher = {IEEE},
 title = {Parameter value characterization of Windows NT-based applications },
 date = {1999},
 doi = {10.1109/WWC.1998.809370},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809370},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809370.pdf?arnumber=809370},
 isbn = {0-7695-0450-7},
 language = {English},
 keywords = {Application software, Electronic switching systems, Filters, Graphical user interfaces, MS Windows NT 4, Optimizing compilers, Program processors, Reactive power, Read only memory, SPECINT95 suite, Software engineering, Windows NT based applications, code specialization, compile time, compiler optimizations, desktop applications, dynamic nature, graphical user interfaces, identifiable invariance, interactive systems, invariant variables, network operating systems, optimising compilers, parameter value characterization, parameter values, partial evaluation, procedure parameters, program diagnostics, rich GUI, software performance evaluation, temporal locality, unpredictable behavior, value profiling, variable values, },
 abstract = {Compiler optimizations such as code specialization and partial evaluation can be used to effectively exploit identifiable invariance of variable values. To identify the invariant variables that the compiler misses at compile time, value profiling can provide valuable information. We focus on the invariance of procedure parameters for a set of desktop applications run on MS Windows NT 4.0. Most of those applications are non-scientific and execute interactively through a rich GUI. Due to the dynamic nature of this workload, one would expect that parameter values would exhibit an unpredictable behavior. Our work attempts to address this question by measuring the invariance and temporal locality of parameter values. We also measure she invariance of parameter values for four benchmarks from the SPECINT95 suite for comparison },
}

@inproceedings{5650333,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Sreeram, J. and Pande, S.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Exploiting approximate value locality for data synchronization on multi-core processors},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650333},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650333},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650333.pdf?arnumber=5650333},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Approximation methods, C++ language, C/C++ language, Hardware, Instruction sets, Open area test sites, Schedules, Synchronization, Writing, approximate value locality, data synchronization, multi-core processors, multiprocessing programs, optimistic synchronization, parallel programming, parallel soft computing programs, synchronisation, },
 abstract = {This paper shows that for a variety of parallel \&#x201C;soft computing\&#x201D; programs that use optimistic synchronization, the approximate nature of the values produced during execution can be exploited to improve performance significantly. Specifically, through mechanisms for imprecise sharing of values between threads, the amount of contention in these programs can be reduced thereby avoiding expensive aborts and improving parallel performance while keeping the results produced by the program within the bounds of an acceptable approximation. This is made possible due to our observation that for many such programs, a large fraction of the values produced during execution exhibit a substantial amount of value locality. We describe how this locality can be exploited using extensions to C/C++ language types that allow specification of limits on the precision and accuracy required and a novel value-aware conflict detection scheme that minimizes the number of conflicts while respecting these limits. Our experiments indicate that for the programs studied substantial speedups can be achieved - upto 5.7x over the original program for the same number of threads. We also present experimental evidence that for the programs studied, the amount of error introduced often grows relatively slowly. },
}

@inproceedings{4636101,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Hughes, C. and Tao Li},
 year = {2008},
 pages = {163--172},
 publisher = {IEEE},
 title = {Accelerating multi-core processor design space evaluation using automatic multi-threaded workload synthesis},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636101},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636101},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636101.pdf?arnumber=4636101},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Acceleration, Flow graphs, Machine learning, Microprocessors, Multicore processing, Process design, Sampling methods, Statistics, Stress, Yarn, automatic multi-threaded workload synthesis, computer architecture, learning (artificial intelligence), logic design, machine learning, microprocessor architectures, microprocessor chips, multi-core processor design space evaluation, multi-threading, performance evaluation, performance evaluation, sampling methods, statistical sampling methods, thread-aware data reference models, wavelet transforms, wavelet-based branching models, },
 abstract = {The design and evaluation of microprocessor architectures is a difficult and time-consuming task. Although small, hand-coded microbenchmarks can be used to accelerate performance evaluation, these programs lack the complexity to stress increasingly complex architecture designs. Larger and more complex real-world workloads should be employed to measure the performance of a given design or to evaluate the efficiency of various design alternatives. These applications can take days or weeks if run to completion on a detailed architecture simulator. In the past, researchers have applied machine learning and statistical sampling methods to reduce the average number of instructions required for detailed simulation. Others have proposed statistical simulation and workload synthesis techniques, which can produce programs that emulate the execution characteristics of the application from which they are derived but have a much shorter execution period than the original. However, these existing methods are difficult to apply to multi-threaded programs and can result in simplifications that miss the complex interactions between multiple, concurrently running threads. This study focuses on developing new techniques for accurate and effective multi-threaded workload synthesis, which can significantly accelerate architecture design evaluation of multi-core processors. We propose to construct synchronized statistical flow graphs that incorporate inter-thread synchronization and sharing behavior to capture the complex characteristics and interactions of multiple threads. Moreover, we develop thread-aware data reference models and wavelet-based branching models to generate accurate memory access and dynamic branch statistics. Experimental results show that a framework integrated with the aforementioned models can automatically generate synthetic programs that maintain characteristics of original workloads but have significantly reduced runtime. },
}

@inproceedings{990738,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {},
 year = {2001},
 publisher = {IEEE},
 title = {Proceedings of the Fourth Annual IEEE International Workshop on Workload Characterization. WWC-4 (Cat. No.01EX538)},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990738},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990738},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990738.pdf?arnumber=990738},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { Internet,  benchmarks,  contemporary applications,  data sets,  memory,  modeling,  performance evaluation,  phase characteristics,  predictability,  tools, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00990738.png" border="0"> },
}

@inproceedings{4636100,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {Isen, C. and John, L. and Jung Pil Choi and Hyo Jung Song},
 year = {2008},
 pages = {153--162},
 publisher = {IEEE},
 title = {On the representativeness of embedded Java benchmarks},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636100},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636100},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636100.pdf?arnumber=4636100},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Application software, Code standards, Current measurement, EEMBC Java Grinder Bench, Grinding machines, Java, Java, Java virtual machines, MIDPMark, Measurement standards, Mobile handsets, MorphMark, Performance analysis, Performance evaluation, Virtual machining, embedded Java benchmarks, mobile computing, virtual machines, },
 abstract = {Java has become one of the predominant languages for embedded and mobile platforms due to its architecturally neutral design, portability, and security. But Java execution in the embedded world encompasses Java virtual machines (JVMs) specially tuned for the embedded world, with stripped-down capabilities, and configurations for memory-limited environments. While there have been some studies on desktop and server Java, there have been very few studies on embedded Java. The non proliferation of embedded Java benchmarks and the lack of widespread profiling tools and simulators have only exacerbated the problem. While the industry uses some benchmarks such as MorphMark, MIDPMark, and EEMBC Java Grinder Bench, their representativeness in comparison to actual embedded Java applications has not been studied. In order to conduct such a study, we gathered an actual mobile phone application suite and characterized it in detail. We measure several properties of the various applications and benchmarks, perform similarity/dissimilarity analysis and shed light on the representativeness of current industry standard embedded benchmarks against actual mobile Java applications. It was observed that for many characteristics, the applications had a broader range, indicating that the benchmarks were under representing the range of characteristics in the real world. Furthermore, we find that the applications exhibit less code reuse/hotness compared to the benchmarks. We also draw comparisons of the embedded benchmarks against popular desktop/client Java benchmarks, such as the SPECjvm98 and DaCapo. Interestingly, embedded applications spend a significant amount of time in standard library code, on average 65\%, suggesting to the usefulness of software and hardware techniques to facilitate pre-compilation with out the real time resource overhead of JIT. },
}

@inproceedings{5649519,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Bienia, C. and Kai Li},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Fidelity and scaling of the PARSEC benchmark inputs},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649519},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649519},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649519.pdf?arnumber=5649519},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Accuracy, Approximation error, Benchmark testing, Computational modeling, PARSEC benchmark input scaling, PARSEC benchmark inputs fidelity, Parallel processing, Pixel, approximation error, approximation theory, benchmark subject, benchmark suite, benchmark testing, computer architecture, full sized input selection problem, gate level simulation, optimisation, optimization problem, real machines, register level simulation, time constraint, },
 abstract = {A good benchmark suite should provide users with inputs that have multiple levels of fidelity for different use cases such as running on real machines, register level simulations, or gate-level simulations. Although input reduction has been explored in the past, there is a lack of understanding how to systematically scale input sets for a benchmark suite. This paper presents a framework that takes the novel view that benchmark inputs should be considered approximations of their original, full-sized inputs. It formulates the input selection problem for a benchmark as an optimization problem that maximizes the accuracy of the benchmark subject to a time constraint. The paper demonstrates how to use the proposed methodology to create several simulation input sets for the PARSEC benchmarks and how to quantify and measure their approximation error. The paper also shows which parts of the inputs are more likely to distort their original characteristics. Finally, the paper provides guidelines for users to create their own customized input sets. },
}

@inproceedings{4362195,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Van Biesbrouck, Michael and Eeckhout, Lieven and Calder, Brad},
 year = {2007},
 pages = {193--203},
 publisher = {IEEE},
 title = {Representative Multiprogram Workloads for Multithreaded Processor Simulation},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362195},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362195},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362195.pdf?arnumber=4362195},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Analytical models, Computational modeling, Computer simulation, Microarchitecture, Multicore processing, Principal component analysis, Process design, Surface-mount technology, Throughput, Yarn, },
 abstract = {Almost all new consumer-grade processors are capable of executing multiple programs simultaneously. The analysis of multiprogrammed workloads for multicore and SMT processors is challenging and time-consuming because there are many possible combinations of benchmarks to execute and each combination may exhibit several different interesting behaviors. Missing particular combinations of program behaviors could hide performance problems with designs. It is thus of utmost importance to have a representative multiprogrammed workload when evaluating multithreaded processor designs. This paper presents a methodology that uses phase analysis, principal components analysis (PCA) and cluster analysis (CA) applied to microarchitecture-independent program characteristics in order to find important program interactions in multiprogrammed workloads. The end result is a small set of co-phases with associated weights that are representative for a multiprogrammed workload across multithreaded processor architectures. Applying our methodology to the SPEC CPU 2000 benchmark suite yields 50 distinct combinations for two-context multithreaded processor simulation that6 researchers and architects can use for simulation. Each combination is simulated for 50 million instructions, giving a total of 2.5 billion instructions to be simulated for the SPEC CPU2000 benchmark suite. The performance prediction error with these representative combinations is under 2.5\% of the real workload for absolute throughput prediction and can be used to make relative throughput comparisons across processor architectures. },
}

@inproceedings{4362196,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Yoo, R.M. and Lee, H.-H.S. and Han Lee and Kingsum Chow},
 year = {2007},
 pages = {204--213},
 publisher = {IEEE},
 title = {Hierarchical Means: Single Number Benchmarking with Workload Cluster Analysis},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362196},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362196},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362196.pdf?arnumber=4362196},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Corporate acquisitions, Engineering management, Java, Merging, Performance analysis, Process design, Robustness, Runtime, Software performance, Technology management, benchmark testing, hierarchical geometric mean, hypothetical Java benchmark suite, redundancy issue, resource allocation, statistical analysis, weight-based score adjustment, workload cluster analysis, },
 abstract = {Benchmark suite scores are typically calculated by averaging the performance of each individual workload. The scores are inherently affected by the distribution of workloads. Given the applications of a benchmark suite are typically contributed by many consortium members, workload redundancy becomes inevitable. Especially, the merger of the benchmarks can significantly increase artificial redundancy. Redundancy in the workloads of a benchmark suite renders the benchmark scores biased, making the score of a suite susceptible to malicious tweaks. The current standard workaround method to alleviating the redundancy issue is to weigh each individual workload during the final score calculation. Unfortunately, such a weight-based score adjustment can significantly undermine the credibility of the objectiveness of benchmark scores. In this paper, we propose a set of benchmark suite score calculation methods called the hierarchical means that incorporate cluster analysis to amortize the negative effect of workload redundancy. These methods not only improve the accuracy and robustness of the score, but also improve the objectiveness over the weight-based approach. In addition, they can also be used to analyze the inherent redundancy and cluster characteristics in a quantitative manner for evaluating a new benchmark suite. In our case study, the hierarchical geometric mean was applied to a hypothetical Java benchmark suite, which attempts to model the upcoming release of the new SPECjvm benchmark suite. In addition, we also show that benchmark suite clustering heavily depends on how the workloads are characterized. },
}

@inproceedings{990739,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Guthaus, M.R. and Ringenberg, J.S. and Ernst, D. and Austin, T.M. and Mudge, T. and Brown, R.B.},
 year = {2001},
 pages = { 3-- 14},
 publisher = {IEEE},
 title = {MiBench: A free, commercially representative embedded benchmark suite},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990739},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990739},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990739.pdf?arnumber=990739},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { ARM instruction set,  SPEC2000,  SimpleScalar,  benchmark suite,  benchmarking,  embedded programs,  embedded systems,  instruction sets,  performance,  representative embedded programs,  security of data,  software performance evaluation,  telecommunication computing, Application software, Code standards, Computer science, Digital audio players, Instruction sets, Microcontrollers, Microprocessors, Multimedia systems, Parallel processing, Process design, },
 abstract = {This paper examines a set of commercially representative embedded programs and compares them to an existing benchmark suite, SPEC2000. A new version of SimpleScalar that has been adapted to the ARM instruction set is used to characterize the performance of the benchmarks using configurations similar to current and next generation embedded processors. Several characteristics distinguish the representative embedded programs from the existing SPEC benchmarks including instruction distribution, memory behavior, and available parallelism. The embedded benchmarks, called MiBench, are freely available to all researchers. },
}

@inproceedings{4362190,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Kodakara, S.V. and Kim, J. and Lilja, D.J. and Wei-Chung Hsu and Pen-Chung Yew},
 year = {2007},
 pages = {139--148},
 publisher = {IEEE},
 title = {Analysis of Statistical Sampling in Microarchitecture Simulation: Metric, Methodology and Program Characterization},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362190},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362190},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362190.pdf?arnumber=4362190},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Analytical models, Computational modeling, Computer integrated manufacturing, Costs, Design optimization, Microarchitecture, Performance analysis, Phase estimation, SPEC CPU2000 benchmarks, Sampling methods, Size measurement, benchmark program, benchmark testing, confidence interval of estimated mean, microarchitecture simulation, performance estimation, program characterization, statistical analysis, statistical sampling, },
 abstract = {Statistical sampling, especially stratified random sampling, is a promising technique for estimating the performance of the benchmark program without executing the complete program on microarchitecture simulators or real machines. The accuracy of the performance estimate and the simulation cost depend on the three parameters, namely the interval size, the sample size, and the number of phases (or strata). Optimum values for these three parameters depends on the performance behavior of the program and the microarchitecture configuration being evaluated. In this paper, we quantify the effect of these three parameters and their interactions on the accuracy of the performance estimate and simulation cost. We use the Confidence Interval of estimated Mean (CIM), a metric derived from statistical sampling theory, to measure the accuracy of the performance estimate; we also discuss why CIM is an appropriate metric for this analysis. We use the total number of instructions simulated and the total number of samples measured as cost parameters. Finally, we characterize 21 SPEC CPU2000 benchmarks based on our analysis. },
}

@inproceedings{4362191,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Ahmad, I.},
 year = {2007},
 pages = {149--158},
 publisher = {IEEE},
 title = {Easy and Efficient Disk I/O Workload Characterization in VMware ESX Server},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362191},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362191},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362191.pdf?arnumber=4362191},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, CPU, Delay, Filters, Hardware, Histograms, Instruments, Measurement, Operating systems, Solaris, VMware ESX server, Virtual machine monitors, Virtual machining, disc storage, disk I/O workload characterization, file servers, file system, input-output programs, online histogram, operating system, peripheral interface, peripheral interfaces, statistical analysis, virtual SCSI command tracing framework, virtual machine hypervisor, virtual machines, },
 abstract = {Collection of detailed characteristics of disk I/O for workloads is the first step in tuning disk subsystem performance. This paper presents an efficient implementation of disk I/O workload characterization using online histograms in a virtual machine hypervisor VMware ESX Server. This technique allows transparent and online collection of essential workload characteristics for arbitrary, unmodified operating system instances running in virtual machines. For analysis that cannot be done efficiently online, we provide a virtual SCSI command tracing framework. Our online histograms encompass essential disk I/O performance metrics including I/O block size, latency, spatial locality, I/O interarrival period and active queue depth. We demonstrate our technique on workloads of Filebench, DBT-2 and large file copy running in virtual machines and provide an analysis of the differences between ZFS and UFS filesystems on Solaris. We show that our implementation introduces negligible overheads in CPU, memory and latency and yet is able to capture essential workload characteristics. },
}

@inproceedings{4362192,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Pu, C. and Sahai, A. and Parekh, J. and Gueyoung Jung and Ji Bae and You-Kyung Cha and Garcia, T. and Irani, D. and Jae Lee and Qifeng Lin},
 year = {2007},
 pages = {161--170},
 publisher = {IEEE},
 title = {An Observation-Based Approach to Performance Characterization of Distributed n-Tier Applications},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362192},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362192},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362192.pdf?arnumber=4362192},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Costs, Databases, Extraterrestrial measurements, Hardware, Internet, Internet, Large-scale systems, Software measurement, Software performance, Software tools, Time measurement, Web server, application server, database server, distributed n-tier application performance, environmental setting, error-prone, large-scale experimental observation, manual script, software-hardware combination, time consuming, workload setting, },
 abstract = {The characterization of distributed n-tier application performance is an important and challenging problem due to their complex structure and the significant variations in their workload. Theoretical models have difficulties with such wide range of environmental and workload settings. Experimental approaches using manual scripts are error-prone, time consuming, and expensive. We use code generation techniques and tools to create and run the scripts for large-scale experimental observation of n-tier benchmarking application performance measurements over a wide range of parameter settings and software/hardware combinations. Our experiments show the feasibility of experimental observations as a sound basis for performance characterization, by studying in detail the performance achieved by (up to 3) database servers and (up to 12) application servers in the RUBiS benchmark with a workload of up to 2700 concurrent users. },
}

@inproceedings{4362193,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Gmach, D. and Rolia, J. and Cherkasova, L. and Kemper, A.},
 year = {2007},
 pages = {171--180},
 publisher = {IEEE},
 title = {Workload Analysis and Demand Prediction of Enterprise Data Center Applications},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362193},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362193},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362193.pdf?arnumber=4362193},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Application virtualization, Capacity planning, Character generation, Costs, Humans, Laboratories, Performance analysis, Resource virtualization, Web server, capacity planning, demand prediction, enterprise data center applications, enterprise resource planning, enterprise services, resource savings, virtualization technology, workload analysis, },
 abstract = {Advances in virtualization technology are enabling the creation of resource pools of servers that permit multiple application workloads to share each server in the pool. Understanding the nature of enterprise workloads is crucial to properly designing and provisioning current and future services in such pools. This paper considers issues of workload analysis, performance modeling, and capacity planning. Our goal is to automate the efficient use of resource pools when hosting large numbers of enterprise services. We use a trace based approach for capacity management that relies on i) the characterization of workload demand patterns, ii) the generation of synthetic workloads that predict future demands based on the patterns, and m) a workload placement recommendation service. The accuracy of capacity planning predictions depends on our ability to characterize workload demand patterns, to recognize trends for expected changes in future demands, and to reflect business forecasts for otherwise unexpected changes in future demands. A workload analysis demonstrates the busrtiness and repetitive nature of enterprise workloads. Workloads are automatically classified according to their periodic behavior. The similarity among repeated occurrences of patterns is evaluated. Synthetic workloads are generated from the patterns in a manner that maintains the periodic nature, burstiness, and trending behavior of the workloads. A case study involving six months of data for 139 enterprise applications is used to apply and evaluate the enterprise workload analysis and related capacity planning methods. The results show that when consolidating to 8 processor systems, we predicted future per-server required capacity to within one processor 95\% of the time. The accuracy of predictions for required capacity suggests that such resource savings can be achieved with little risk. },
}

@inproceedings{5648828,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Gordon, B. and Sohoni, S. and Chandler, D.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Data handling inefficiencies between CUDA, 3D rendering, and system memory},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5648828},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5648828},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05648828.pdf?arnumber=5648828},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {3D rendering, API, Analytical models, CUDA, Computational modeling, GPGPU programming, Graphics processing unit, Load modeling, Pixel, Runtime, Three dimensional displays, application program interfaces, computer graphic equipment, coprocessors, data exchange, data handling, data handling, data transfer, electronic data interchange, parallel architectures, rendering (computer graphics), solid modelling, storage management, system memory, },
 abstract = {While GPGPU programming offers faster computation of highly parallelized code, the memory bandwidth between the system and the GPU can create a bottleneck that reduces the potential gains. CUDA is a prominent GPGPU API which can transfer data to and from system code, and which can also access data used by 3D rendering APIs. In an application that relies on both GPU programming APIs to accelerate 3D modeling and an easily parallelized algorithm, the hidden inefficiencies of nVidia's data handling with CUDA become apparent. First, CUDA uses the CPU's store units to copy data between the graphics card and system memory instead of using a more efficient method like DMA. Second, data exchanged between the two GPU-based APIs travels through the main processor instead of staying on the GPU. As a result, a non-GPGPU implementation of a program runs faster than the same program using GPGPU. },
}

@inproceedings{5306806,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {},
 year = {2009},
 pages = {i--ii},
 publisher = {IEEE},
 title = {Title page},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306806},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306806},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306806.pdf?arnumber=5306806},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Internet, World Wide Web, benchmark suite construction, distributed processing, many-core system, mobile system, multicore system, performance characterization, resource allocation, storage system, workload characterization, },
 abstract = {The following topics are dealt with: workload characterization; multi-core/many-core systems; benchmark suite construction; mobile and storage system; performance characterization; Web and Internet. },
}

@inproceedings{5306791,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Malkowski, S. and Hedwig, M. and Pu, C.},
 year = {2009},
 pages = {118--127},
 publisher = {IEEE},
 title = {Experimental evaluation of N-tier systems: Observation and analysis of multi-bottlenecks},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306791},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306791},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306791.pdf?arnumber=5306791},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Adaptive filters, Analytical models, Benchmark testing, Independent component analysis, Kernel, Mission critical systems, Performance analysis, RUBBoS benchmarks, RUBiS benchmarks, Resource management, Statistical analysis, Throughput, mission-critical N-tier applications, multibottlenecks, resource allocation, resource utilization, software performance evaluation, statistical analysis, statistical methods, systems analysis, },
 abstract = {In many areas such as e-commerce, mission-critical N-tier applications have grown increasingly complex. They are characterized by non-stationary workloads (e.g., peak load several times the sustained load) and complex dependencies among the component servers. We have studied N-tier applications through a large number of experiments using the RUBiS and RUBBoS benchmarks. We apply statistical methods such as kernel density estimation, adaptive filtering, and change detection through multiple-model hypothesis tests to analyze more than 200 GB of recorded data. Beyond the usual single-bottlenecks, we have observed more intricate bottleneck phenomena. For instance, in several configurations all system components show average resource utilization significantly below saturation, but overall throughput is limited despite addition of more resources. More concretely, our analysis shows experimental evidence of multi-bottleneck cases with low average resource utilization where several resources saturate alternatively, indicating a clear lack of independence in their utilization. Our data corroborates the increasing awareness of the need for more sophisticated analytical performance models to describe N-tier applications that do not rely on independent resource utilization assumptions. We also present a preliminary taxonomy of multi-bottlenecks found in our experimentally observed data. },
}

@inproceedings{5306790,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Hughes, C. and Poe, J. and Qouneh, A. and Tao Li},
 year = {2009},
 pages = {108--117},
 publisher = {IEEE},
 title = {On the (dis)similarity of transactional memory workloads},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306790},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306790},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306790.pdf?arnumber=5306790},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Algorithm design and analysis, Application software, Clustering algorithms, Computer architecture, Design engineering, Monitoring, Multicore processing, PARSEC program, Performance analysis, Process design, Runtime, SPLASH-2 program, STAMP program, Transactional memory, architecture-independent transaction, clustering algorithms, concurrency control, concurrent programming, multicore system, pattern clustering, performance analysis, principal component analysis, principle component analysis, software performance evaluation, transaction processing, transactional memory workloads, transactional microbenchmarks, workload similarity, },
 abstract = {Programming to exploit the resources in a multicore system remains a major obstacle for both computer and software engineers. Transactional memory offers an attractive alternative to traditional concurrent programming but implementations emerged before the programming model, leaving a gap in the design process. In previous research, transactional microbenchmarks have been used to evaluate designs or lock-based multithreaded workloads have been manually converted into their transactional equivalents; others have even created dedicated transactional benchmarks. Yet, throughout all of the investigations, transactional memory researchers have not settled on a way to describe the runtime characteristics that these programs exhibit; nor has there been any attempt to unify the way transactional memory implementations are evaluated. In addition, the similarity (or redundancy) of these workloads is largely unknown. Evaluating transactional memory designs using workloads that exhibit similar characteristics will unnecessarily increase the number of simulations without contributing new insight. On the other hand, arbitrarily choosing a subset of transactional memory workloads for evaluation can miss important features and lead to biased or incorrect conclusions. In this work, we propose a set of architecture-independent transaction-oriented workload characteristics that can accurately capture the behavior of transactional code. We apply principle component analysis and clustering algorithms to analyze the proposed workload characteristics collected from a set of SPLASH-2, STAMP, and PARSEC transactional memory programs. Our results show that using transactional characteristics to cluster the chosen benchmarks can reduce the number of required simulations by almost half. We also show that the methods presented in this paper can be used to identify specific feature subsets. With the increasing number of TM workloads in the future, we believe that the proposed transactional memor- y workload characterization techniques will help TM architects select a small, diverse, set of TM workloads for their design evaluation. },
}

@inproceedings{5306793,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Bhadauria, M. and Weaver, V.M. and McKee, S.A.},
 year = {2009},
 pages = {98--107},
 publisher = {IEEE},
 title = {Understanding PARSEC performance on contemporary CMPs},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306793},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306793},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306793.pdf?arnumber=5306793},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Arithmetic, Bandwidth, Counting circuits, Frequency, Hardware, Out of order, PARSEC performance, Parallel processing, Princeton Application Repository for Shared-Memory Computers, System buses, Testing, Yarn, cache-to-cache transfers, chip multiprocessor design, hardware performance counters, instruction-level parallelism, memory channels, memory hierarchy configurations, micro-architectural resources, microprocessor chips, multi-threading, off-chip memory, out-of-order cores, processor frequencies, systems-level approach, thread-level parallelism, },
 abstract = {PARSEC is a reference application suite used in industry and academia to assess new chip multiprocessor (CMP) designs. No investigation to date has profiled PARSEC on real hardware to better understand scaling properties and bottlenecks. This understanding is crucial in guiding future CMP designs for these kinds of emerging workloads. We use hardware performance counters, taking a systems-level approach and varying common architectural parameters: number of out-of-order cores, memory hierarchy configurations, number of multiple simultaneous threads, number of memory channels, and processor frequencies. We find these programs to be largely compute-bound, and thus limited by number of cores, micro-architectural resources, and cache-to-cache transfers, rather than by off-chip memory or system bus bandwidth. Half the suite fails to scale linearly with increasing number of threads, and some applications saturate performance at few threads on all platforms tested. Exploiting thread level parallelism delivers greater payoffs than exploiting instruction level parallelism. To reduce power and improve performance, we recommend increasing the number of arithmetic units per core, increasing support for TLP, and reducing support for ILP. },
}

@inproceedings{5306792,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Barrow-Williams, N. and Fensch, C. and Moore, S.},
 year = {2009},
 pages = {86--97},
 publisher = {IEEE},
 title = {A communication characterisation of Splash-2 and Parsec},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306792},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306792},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306792.pdf?arnumber=5306792},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Analytical models, Computational modeling, Computer simulation, Facial animation, Fluid dynamics, High performance computing, Linux, Linux OS, Oceans, Parsec, Protocols, Rendering (computer graphics), Splash-2, Yarn, chip-multiprocessor, coherence protocol, communication characterisation, core granularities, data sharing, microprocessor chips, multi-threading, multiprocessing systems, network-on-chip, parallelisation, program analysis, program run, thread granularities, thread mapping, },
 abstract = {Recent benchmark suite releases such as Parsec specifically utilise the tightly coupled cores available in chip-multiprocessors to allow the use of newer, high performance, models of parallelisation. However, these techniques introduce additional irregularity and complexity to data sharing and are entirely dependent on efficient communication performance between processors. This paper thoroughly examines the crucial communication and sharing behaviour of these future applications. The infrastructure used allows both accurate and comprehensive program analysis, employing a full Linux OS running on a simulated 32-core x86 machine. Experiments use full program runs, with communication classified at both core and thread granularities. Migratory, read-only and producer-consumer sharing patterns are observed and their behaviour characterised. The temporal and spatial characteristics of communication are presented for the full collection of Splash-2 and Parsec benchmarks. Our results aim to support the design of future communication systems for CMPs, encompassing coherence protocols, network-on-chip and thread mapping. },
}

@inproceedings{5306795,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Najaf-abadi, H.H. and Rotenberg, E.},
 year = {2009},
 pages = {75--85},
 publisher = {IEEE},
 title = {The importance of accurate task arrival characterization in the design of processing cores},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306795},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306795},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306795.pdf?arnumber=5306795},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Costs, Face, Process design, Robustness, Stochastic processes, Throughput, chip multiprocessor, dual-core design, harmonic mean performance, microprocessor chips, multiprocessing systems, processing cores, stochastic characterization, task arrival behavior, task arrival characterization, task arrival pattern, },
 abstract = {This paper studies the importance of accounting for a neglected facet of overall workload behavior, the pattern of task arrival. A stochastic characterization is formulated that defines regularity in the task arrival pattern. This characterization is used as the basis for a quantitative evaluation of the importance of accurately accounting for the task arrival behavior in the design of the processing cores of a Chip Multi-processor (CMP). The results of this study show that because the methodologies traditionally used for evaluating overall performance do not accurately account for task arrival behavior, they can lead to significantly suboptimal design solutions. For instance, it is found that, for an unvarying mix of benchmarks, the best dual-core design for harmonic mean performance can result in up to 21\% suboptimality depending on the task arrival pattern. In addition, it is shown that when the pattern of task arrival is prone to change, simply accounting for average task arrival behavior can result in up to 12\% inaccuracy, and suboptimality in employed design solutions. A practical conclusion that can be drawn from the results of this study is that benchmark vendors need to provide not only a representative mix of instruction level behavior (the traditional application benchmarks), but also representative task arrival patterns. In addition, robustness to variation in the task arrival pattern needs to be accounted for as an overall merit in the evaluation of potential design solutions. },
}

@inproceedings{5306794,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Venkata, S.K. and Ahn, I. and Donghwan Jeon and Gupta, A. and Louie, C. and Garcia, S. and Belongie, S. and Taylor, M.B.},
 year = {2009},
 pages = {55--64},
 publisher = {IEEE},
 title = {SD-VBS: The San Diego Vision Benchmark Suite},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306794},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306794},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306794.pdf?arnumber=5306794},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, C language, Computational modeling, Computer architecture, Computer vision, Drives, Energy efficiency, MATLAB, MATLAB, Parallel processing, Portable computers, Runtime, SD-VBS, San Diego Vision Benchmark Suite, benchmark testing, computer vision, computer vision, many-core processors, multi-core processors, },
 abstract = {In the era of multi-core, computer vision has emerged as an exciting application area which promises to continue to drive the demand for both more powerful and more energy efficient processors. Although there is still a long way to go, vision has matured significantly over the last few decades, and the list of applications that are useful to end users continues to grow. The parallelism inherent in vision applications makes them a promising workload for multi-core and many-core processors. While the vision community has focused many years on improving the accuracy of vision algorithms, a major barrier to the study of their computational properties has been the lack of a benchmark suite that simultaneously spans a wide portion of the vision space and is accessible in a portable form that the architecture community can easily use. We present the San Diego Vision Benchmark Suite (SD-VBS), a suite of diverse vision applications drawn from the vision domain. The applications are drawn from the current state-of-the-art in computer vision, in consultation with vision researchers. Each benchmark is provided in both MATLAB and C form. MATLAB is the preferred language of vision researchers, while C makes it easier to map the applications to research platforms. The C code minimizes pointer usage and employs clean constructs to make them easier for parallelization. Furthermore, we provide a spectrum of input sets that enable researchers to control simulation time, and to understand properties as inputs increase to leverage better processor performance. In this paper, we describe the benchmarks, show how their runtime is attributed to their constituent kernels, overview some of their computational properties - including parallelism - and show how they are affected by growing inputs. The benchmark suite will be made available on the Internet, and updated as new applications emerge. },
}

@inproceedings{5306797,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Shuai Che and Boyer, M. and Jiayuan Meng and Tarjan, D. and Sheaffer, J.W. and Sang-Ha Lee and Skadron, K.},
 year = {2009},
 pages = {44--54},
 publisher = {IEEE},
 title = {Rodinia: A benchmark suite for heterogeneous computing},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306797},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306797},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306797.pdf?arnumber=5306797},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, Benchmark testing, Berkeleys dwarf taxonomy, Central Processing Unit, Computer architecture, Energy consumption, Kernel, Microprocessors, Multicore processing, Parallel processing, Rodinia-a benchmark suite, Yarn, data layout, heterogeneous computing, memory-bandwidth limitation, multicore CPU platform, multicore GPU platform, parallel communication pattern, parallel program, parallel programming, power consumption, synchronization technique, },
 abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout. },
}

@inproceedings{5306796,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Qiang Xu and Subhlok, J. and Rong Zheng and Voss, S.},
 year = {2009},
 pages = {34--43},
 publisher = {IEEE},
 title = {Logicalization of communication traces from parallel execution},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306796},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306796},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306796.pdf?arnumber=5306796},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Cities and towns, Computer science, Drives, Educational institutions, Global communication, High performance computing, Matrix converters, Message passing, Performance analysis, SPMD MPI program, Topology, communication traces, logical message exchanges, message passing, parallel execution, parallel programming, parallel programs, topology identification, trace volume reduction, },
 abstract = {Communication traces are integral to performance modeling and analysis of parallel programs. However, execution on a large number of nodes results in a large trace volume that is cumbersome and expensive to analyze. This paper presents an automatic framework to convert all process traces corresponding to the parallel execution of an SPMD MPI program into a single logical trace. First, the application communication matrix is generated from process traces. Next, topology identification is performed based on the underlying communication structure and independent of the way ranks (or numbers) are assigned to processes. Finally, message exchanges between physical processes are converted into logical message exchanges that represent similar message exchanges across all processes, resulting in a trace volume reduction approximately equal to the number of processes executing the application. This logicalization framework has been implemented and the results report on its performance and effectiveness. },
}

@inproceedings{5306799,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Lugones, D. and Franco, D. and Rexachs, D. and Moure, J.C. and Luque, E. and Argollo, E. and Falcon, A. and Ortega, D. and Faraboschi, P.},
 year = {2009},
 pages = {24--33},
 publisher = {IEEE},
 title = {High-speed network modeling for full system simulation},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306799},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306799},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306799.pdf?arnumber=5306799},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Computational modeling, Computer architecture, Computer networks, Concurrent computing, Context modeling, High-speed networks, Multiprocessor interconnection networks, Stress, Telecommunication traffic, Traffic control, cluster computing systems, full system simulation, high-speed network modeling, interconnection networks, network latency times, parallel scientific application, synthetic traffic, workstation clusters, },
 abstract = {The widespread adoption of cluster computing systems has shifted the modeling focus from synthetic traffic to realistic workloads to better capture the complex interactions between applications and architecture. In this context, a full-system simulation environment also needs to model the networking component, but the simulation duration that is practically affordable is too short to appropriately stress the networking bottlenecks. In this paper, we present a methodology that overcomes this problem and enables the modeling of interconnection networks while ensuring representative results with fast simulation turnaround. We use standard network tools to extract simplified models that are statistically validated and at the same time compatible with a full system simulation environment. We propose three models with different accuracy vs. speed ratios that compute network latency times according to the estimated traffic and measure them on a real-world parallel scientific application. },
}

@inproceedings{5306798,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Scarpazza, D.P. and Braudaway, G.W.},
 year = {2009},
 pages = {13--23},
 publisher = {IEEE},
 title = {Workload characterization and optimization of high-performance text indexing on the Cell Broadband Engine&#x2122; (Cell/B.E.)},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306798},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306798},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306798.pdf?arnumber=5306798},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, Cell Broadband Engine, Computer architecture, Concurrent computing, Hardware, Home appliances, Indexing, Microprocessors, Multicore processing, Parallel processing, Search engines, cell processor, high-performance text indexing, indexing, microprocessor, microprocessor chips, multicore architecture, multicore processors, parallel software design, search engines, text analysis, workload characterization, workload optimization, },
 abstract = {In this paper we examine text indexing on the Cell Broadband Enginetrade (Cell/B.E.), an emerging workload on an emerging multicore architecture. The Cell Broadband Engine is a microprocessor jointly developed by Sony Computer Entertainment, Toshiba, and IBM (herein, we refer to it simply as the "Cell"). The importance of text indexing is growing not only because it is the core task of commercial and enterprise-level search engines, but also because it appears more and more frequently in desktop and mobile applications, and on network appliances. Text indexing is a computationally intensive task. Multi-core processors promise a multiplicative increase in compute power, but this power is fully available only if workloads exhibit the right amount and kind of parallelism. We present the challenges and the results of mapping text indexing tasks to the Cell processor. The Cell has become known as a platform capable of impressive performance, but only when algorithms have been parallelized with attention paid to its hardware peculiarities (expensive branching, wide SIMD units, small local memories). We propose a parallel software design that provides essential text indexing features at a high throughput (161 Mbyte/s per chip on Wikipedia inputs) and we present a performance analysis that details the resources absorbed by each subtask. Not only does this result affect traditional applications, but it also enables new ones such as live network traffic indexing for security forensics, until now believed to be too computationally demanding to be performed in real time. We conclude that, at the cost of a radical algorithmic redesign, our Cell-based solution delivers a 4x performance advantage over recent commodity machine like the Intel Q6600. In a per-chip comparison, ours is the fastest text indexer that we are aware of. },
}

@inproceedings{1226490,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Ballocca, G. and Politi, R. and Russo, V. and Ruffo, G.},
 year = {2002},
 pages = { 14-- 22},
 publisher = {IEEE},
 title = {Benchmarking a site with realistic workload},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226490},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226490},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226490.pdf?arnumber=1226490},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Web benchmarking tools,  Web sites,  Web sites,  capacity planning,  customer behavior model graph,  e-commerce site,  electronic commerce,  integrated Web stressing tool,  log files,  realistic workload,  workload characterization, },
 abstract = {The rapidly growing number of Web users and the consequent importance of capacity planning have lead to the development of Web benchmarking tools. One common criticism of this approach, is that synthetic workload produced by Web stressing tools is far from realistic. This paper deals with a benchmarking methodology based on workload characterization generated from log files. A customer behavior model graph (CBMG) was proposed by Mensace, et al., (1999) as workload characterization of an e-commerce site. We discuss how CBMG methodology has a wider field of application and how to use this model to efficiently improve a fully integrated Web stressing tool. We also evaluate the differences between our approach and other models based on different characterizations. },
}

@inproceedings{1525989,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {},
 year = {2005},
 pages = { ii-- ii},
 publisher = {IEEE},
 title = {IEEE International Symposium on Workload Characterization - Copyright},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525989},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525989},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525989.pdf?arnumber=1525989},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{1525988,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {},
 year = {2005},
 pages = { i-- i},
 publisher = {IEEE},
 title = {Proceedings of the 2005 IEEE International Symposium on Workload Characterization},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525988},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525988},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525988.pdf?arnumber=1525988},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{5650274,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Shuai Che and Sheaffer, J.W. and Boyer, M. and Szafaryn, L.G. and Liang Wang and Skadron, K.},
 year = {2010},
 pages = {1--11},
 publisher = {IEEE},
 title = {A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650274},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650274},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650274.pdf?arnumber=5650274},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Benchmark testing, Computational fluid dynamics, Computer architecture, GPU, Graphics processing unit, Heart, Instruction sets, Kernel, NVIDIA GeForce GTX480, Parsec, Rodinia benchmark suite, computer graphics, contemporary CMP workloads, coprocessors, graphics processors unit, principal component analysis, principal component analysis, space coverage, },
 abstract = {The recently released Rodinia benchmark suite enables users to evaluate heterogeneous systems including both accelerators, such as GPUs, and multicore CPUs. As Rodinia sees higher levels of acceptance, it becomes important that researchers understand this new set of benchmarks, especially in how they differ from previous work. In this paper, we present recent extensions to Rodinia and conduct a detailed characterization of the Rodinia benchmarks (including performance results on an NVIDIA GeForce GTX480, the first product released based on the Fermi architecture). We also compare and contrast Rodinia with Parsec to gain insights into the similarities and differences of the two benchmark collections; we apply principal component analysis to analyze the application space coverage of the two suites. Our analysis shows that many of the workloads in Rodinia and Parsec are complementary, capturing different aspects of certain performance metrics. },
}

@inproceedings{1226501,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Murat Fiskiran, A. and Lee, R.B.},
 year = {2002},
 pages = { 127-- 137},
 publisher = {IEEE},
 title = {Workload characterization of elliptic curve cryptography and other network security algorithms for constrained environments},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226501},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226501},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226501.pdf?arnumber=1226501},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Diffie-Hellman key exchange,  computing resources,  elliptic curve cryptography,  elliptic-curve versions,  hash algorithms,  mobile information appliances,  polynomials,  power availability,  public key cryptography,  public-key cryptography algorithms,  software implementations,  workload characterization, },
 abstract = {In recent years, some cryptographic algorithms have gained popularity due to properties that make them suitable for use in constrained environments like mobile information appliances, where computing resources and power availability are limited. In this paper, we select a set of public-key, symmetric-key and hash algorithms suitable for such environments and study their workload characteristics. In particular, we study elliptic-curve versions of public-key cryptography algorithms, which allow fast software implementations while reducing the key size needed for a desired level of security compared to previous integer-based public-key algorithms. We characterize the operations needed by elliptic-curve analogs of Diffie-Hellman key exchange, ElGamal and the Digital Signature Algorithm for public-key cryptography, for different key sizes and different levels of software optimization. We also include characterizations for the Advanced Encryption Standard (AES) for symmetric-key cryptography, and SHA as a hash algorithm. We show that all these algorithms can be implemented efficiently with a very simple processor. },
}

@inproceedings{1226500,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Nogueira, D. and Rocha, L. and Santos, J. and Araujo, P. and Almeida, V. and Meira, W., Jr.},
 year = {2002},
 pages = { 118-- 126},
 publisher = {IEEE},
 title = {A methodology for workload characterization of file-sharing peer-to-peer networks},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226500},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226500},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226500.pdf?arnumber=1226500},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Gnutella network,  Internet,  Internet,  computational resources,  computer networks,  content accessibility,  content coherence,  file-sharing patterns,  file-sharing peer-to-peer networks,  host availability,  performance evaluation,  response latency,  search patterns,  workload characterization, },
 abstract = {The main characteristic of peer-to-peer (P2P) networks is that the hosts in the network may act as both clients and servers at the same time, being called servents. These networks have been widely adopted for sharing idle computational resources available in the Internet, improving content accessibility while reducing costs and response latency, although host availability and content coherence is not usually guaranteed. As a consequence, traditional workload characterization strategies are not suitable for analyzing and understanding these networks, motivating the design of specific strategies for their characterization. In this article we present a novel workload characterization methodology for P2P networks, which account for the main features of these networks. We validate our methodology through the characterization of the Gnutella network, through which we are able to characterize file-sharing patterns, the availability of the servents, and the search patterns, among others. },
}

@inproceedings{1226502,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {},
 year = {2002},
 pages = { 139-- 140},
 publisher = {IEEE},
 title = {Author Index},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226502},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226502},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226502.pdf?arnumber=1226502},
 isbn = {0-7803-7681-1},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01226502.png" border="0"> },
}

@inproceedings{1437383,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { iv-- iv},
 publisher = {IEEE},
 title = {Foreword},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437383},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437383},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437383.pdf?arnumber=1437383},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = {Computer architecture, Conferences, Laboratories, Meetings, },
 abstract = {},
}

@inproceedings{1437382,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { iii-- iii},
 publisher = {IEEE},
 title = {Table of contents},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437382},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437382},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437382.pdf?arnumber=1437382},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437381,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { ii-- ii},
 publisher = {IEEE},
 title = {Copyright page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437381},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437381},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437381.pdf?arnumber=1437381},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437380,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { i-- i},
 publisher = {IEEE},
 title = {IEEE International Workshop on Workload Characterization},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437380},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437380},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437380.pdf?arnumber=1437380},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437387,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 2-- 2},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437387},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437387},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437387.pdf?arnumber=1437387},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437386,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 1-- 1},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437386},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437386},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437386.pdf?arnumber=1437386},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437385,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { vi-- vi},
 publisher = {IEEE},
 title = {Reviewers},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437385},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437385},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437385.pdf?arnumber=1437385},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = {IEEE, },
 abstract = {},
}

@inproceedings{1437384,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { v-- v},
 publisher = {IEEE},
 title = {Workshop committee},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437384},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437384},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437384.pdf?arnumber=1437384},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437389,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Sassone, P.G. and Wills, D.S.},
 year = {2004},
 pages = { 11-- 18},
 publisher = {IEEE},
 title = {On the extraction and analysis of prevalent dataflow patterns},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437389},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437389},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437389.pdf?arnumber=1437389},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { CPX,  Media-Bench benchmark,  NP-complete problem,  Spec2000int benchmark,  application binaries,  benchmark testing,  code representations,  communication complexity,  communication patterns,  complexity-effectiveness,  data flow analysis,  data flow graphs,  dataflow communication,  dataflow graphs,  dataflow idioms,  dataflow pattern analysis,  dataflow pattern extraction,  dataflow subgraph extraction,  dependence chains,  dynamic instructions,  dynamic optimization,  integer code,  message passing,  microarchitectural resources,  operand movement patterns,  pattern recognition,  wire-dominated architectures, Application software, Assembly, Broadcasting, Data analysis, Data mining, Frequency, Libraries, Microelectronics, Pattern analysis, Programming profession, },
 abstract = {The complexity-effectiveness of modern wire-dominated architectures is heavily influenced by operand movement patterns within workloads. Unfortunately, the study of these common patterns is burdensome given the NP-completeness of the problem and the size of the dataflow graphs in modern applications. In response we present CPX, a fast and memory-efficient tool for the extraction of common dataflow subgraphs from application binaries. Using this tool and a practical metric of pattern popularity, we analyze Media-Bench and Spec2000int benchmarks and present their most frequent communication patterns. Results confirm the intuition of prior research that dependence chains dominate integer code, but more importantly demonstrate that dataflow communication is restricted to a tractable set of templates. A set of only ten small patterns characterizes over 90\% of Spec2000int and over 75\% of MediaBench dynamic instructions. These common dataflow idioms are amenable to dynamic optimization, more efficient code representations, and reducing the broadcast nature of micro-architectural resources. },
}

@inproceedings{1437388,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Kotla, R. and Devgan, A. and Ghiasi, S. and Keller, T. and Rawson, F.},
 year = {2004},
 pages = { 3-- 10},
 publisher = {IEEE},
 title = {Characterizing the impact of different memory-intensity levels},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437388},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437388},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437388.pdf?arnumber=1437388},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { IPC,  SPECCPU benchmark,  SPECjbb benchmark,  TPC-W benchmark,  benchmark testing,  high-end processors,  instruction sets,  instructions per cycle,  load demands,  memory behavior monitoring,  memory intensity,  memory-intensity levels,  memory-intensive phases,  microprocessor chips,  performance counters,  performance evaluation,  performance penalties,  power consumption,  power reduction,  processor resources,  processor scheduling,  resource allocation,  storage management, Counting circuits, Energy consumption, Frequency, Laboratories, Monitoring, Operating systems, Performance gain, Predictive models, Process design, Throughput, },
 abstract = {Applications on today's high-end processors typically make varying load demands over time. A single application may have many different phases during its lifetime, and workload mixes show interleaved phases. This work examines and uses the differences between memory- and CPU-intensive phases to reduce power. Today's processors provide resources that are underutilized during memory-intensive phases, consuming power while producing little incremental gain in performance. This work examines a deployed system consisting of identical cores with a goal of running each one at a different effective frequency. The initial goal is to find the appropriate frequency at which to run each phase. This paper demonstrates that memory intensity directly affects the throughput of applications. The results indicate that simple metrics such as IPC (instructions per cycle) cannot be used to determine what frequency to run a phase. Instead, it uses performance counters which directly monitor memory behavior to identify. Memory-intensive phases can then be run on a slower core without incurring significant performance penalties. The key result of the paper is the introduction of a very simple, online model that uses the performance counter data to predict the performance of a program phase at any particular frequency setting. The information from this model allows a scheduler to decide which core to use to execute the program phase. Using a sophisticated power model for the processor family shows that this approach significantly reduces power consumption. The model was evaluated using a subset of SPECCPU and the SPECjbb and TPC-W benchmarks. It predicts performance with an average error of less than 10\%. The power modeling shows that memory-intensive benchmarks achieve up to-a 58\%, power reduction at a performance loss of less than 20\% when run at 80\% of nominal frequency. },
}

@inproceedings{4086137,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Yi, J.J. and Sendag, R. and Eeckhout, L. and Joshi, A. and Lilja, D.J. and John, L.K.},
 year = {2006},
 pages = {93--104},
 publisher = {IEEE},
 title = {Evaluating Benchmark Subsetting Approaches},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302733},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086137},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086137.pdf?arnumber=4086137},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, Computational modeling, Computer architecture, Computer networks, Computer simulation, Information systems, Microarchitecture, Plackett and Burman design, Principal component analysis, benchmark suite subsetting evaluation, benchmark testing, computer architecture, energy-delay product, memory subsystem enhancement, performance evaluation, principal component analysis, principal components analysis, processor absolute accuracy, processor relative accuracy, set theory, statistically-based subsetting approach, },
 abstract = {To reduce the simulation time to a tractable amount or due to compilation (or other related) problems, computer architects often simulate only a subset of the benchmarks in a benchmark suite. However, if the architect chooses a subset of benchmarks that is not representative, the subsequent simulation results will, at best, be misleading or, at worst, yield incorrect conclusions. To address this problem, computer architects have recently proposed several statistically-based approaches to subset a benchmark suite. While some of these approaches are well-grounded statistically, what has not yet been thoroughly evaluated is the: 1) absolute accuracy; 2) relative accuracy across a range of processor and memory subsystem enhancements; and 3) representativeness and coverage of each approach for a range of subset sizes. Specifically, this paper evaluates statistically-based subsetting approaches based on principal components analysis (PCA) and the Plackett and Burman (P\&amp;B) design, in addition to prevailing approaches such as integer vs. floating-point, core vs. memory-bound, by language, and at random. Our results show that the two statistically-based approaches, PCA and P\&amp;B, have the best absolute and relative accuracy for CPI and energy-delay product (EDP), produce subsets that are the most representative, and choose benchmark and input set pairs that are most well-distributed across the benchmark space. To achieve a 5\% absolute CPI and EDP error, across a wide range of configurations, PCA and P\&amp;B typically need about 17 benchmark and input set pairs, while the other five approaches often choose more than 30 benchmark and input set pairs },
}

@inproceedings{1526010,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Skinner, D. and Kramer, W.},
 year = {2005},
 pages = { 137-- 149},
 publisher = {IEEE},
 title = {Understanding the causes of performance variability in HPC workloads},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526010},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526010},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526010.pdf?arnumber=1526010},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { high performance computing,  multiuser production computing,  parallel architectures,  performance evaluation,  performance variability, Application software, Benchmark testing, Computer architecture, Concurrent computing, Large-scale systems, Processor scheduling, Production, Runtime, Scientific computing, Timing, },
 abstract = {While most workload characterization focuses on application and architecture performance, the variability in performance also has wide ranging impacts on the users and managers of large scale computing resources. Performance variability, though secondary to absolute performance itself can significantly detract from both the overall performance realized by parallel workloads and the suitability of a given architecture for a workload. In making choices about how to best match an HPC workload to an HPC architecture most examinations focus primarily on application performance, often in terms nominal or optimal performance. A practical concern which brackets the degree to which one can expect to see this performance in a multi-user production computing environment is the degree to which performance varies. Without an understanding of the performance variability exhibited by a computer for a given workload, in a practical sense, the effective performance that can be realized is still undetermined. In this work we examine both architectural and application causes of variability, quantify their impacts, and demonstrate performance gains realized by reducing variability. },
}

@inproceedings{1437404,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 89-- 90},
 publisher = {IEEE},
 title = {Author index},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437404},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437404},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437404.pdf?arnumber=1437404},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{4636080,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {},
 year = {2008},
 pages = {ix--ix},
 publisher = {IEEE},
 title = {IISWC-2008 reviewers},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636080},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636080},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636080.pdf?arnumber=4636080},
 isbn = {978-1-4244-2777-2},
 language = {English},
 abstract = {},
}

@inproceedings{1525994,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {},
 year = {2005},
 pages = { vii-- viii},
 publisher = {IEEE},
 title = {Table of Contents},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525994},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525994},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525994.pdf?arnumber=1525994},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{1525995,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Mashey, J.R.},
 year = {2005},
 publisher = {IEEE},
 title = {Summarizing performance is no mean feat [computer performance analysis]},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525995},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525995},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525995.pdf?arnumber=1525995},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { algebra,  computer benchmarking,  computer performance analysis,  performance evaluation,  relative performance analyses,  statistical analysis,  workload analyses, Biographies, Computer architecture, Computer performance, Digital arithmetic, Military computing, Operating systems, Performance analysis, Reduced instruction set computing, Silicon, Software development management, },
 abstract = {For decades, computer benchmarkers have fought a war of means, arguing over proper uses of arithmetic, harmonic, and geometric means, starting in the mid-1980s. One would think this basic issue of computer performance analysis would have been long resolved, but contradictions are still present in some excellent and widely-used textbooks. This paper offers a framework that resolves these issues and includes both workload analyses and relative performance analyses (such as SPEC or Livermore Loops), emphasizing differences between algebraic and statistical approaches. In some cases, the lognormal distribution is found to be quite useful, especially with appropriate forms of standard deviation and confidence interval used to augment the usual geometric mean. Results can be used to indicate the relative importance of careful workload analysis. },
}

@inproceedings{1525992,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {},
 year = {2005},
 pages = { v-- v},
 publisher = {IEEE},
 title = {IISWC-2005 people},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525992},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525992},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525992.pdf?arnumber=1525992},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = {Bonding, Finance, Instruments, Organizing, },
 abstract = {},
}

@inproceedings{1525993,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {},
 year = {2005},
 pages = { vi-- vi},
 publisher = {IEEE},
 title = {IISWC-2005 reviewers},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525993},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525993},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525993.pdf?arnumber=1525993},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{1437402,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Koka, P. and Suh, T. and Smelyanskiy, M. and Grzeszczuk, R. and Dulong, C.},
 year = {2004},
 pages = { 73-- 80},
 publisher = {IEEE},
 title = {Construction and performance characterization of parallel interior point solver on 4-way Intel Itanium 2 multiprocessor system},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437402},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437402},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437402.pdf?arnumber=1437402},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { 4-way Intel Itanium 2 multiprocessor system,  IPM workload,  communication overhead,  convex optimization problems,  interior point method,  linear programming,  load balancing,  microarchitectural analysis,  microprocessor chips,  multiprocessing systems,  multiprocessor systems,  parallel IPM,  parallel architectures,  parallel interior point solver,  parallel linear programming,  performance characterization,  performance evaluation,  resource allocation,  scalable performance, Application software, Concurrent computing, Hardware, Multiprocessing systems, Optimization methods, Parallel processing, Performance analysis, Robustness, Scalability, Telecommunication computing, VTune&trade,  performance analyzer, },
 abstract = {In recent years the interior point method (IPM) has became a dominant choice for solving large convex optimization problems for many scientific, engineering and commercial applications. Two reasons for the success of the IPM are its good scalability on existing multiprocessor systems with a small number of processors and its potential to deliver a scalable performance on systems with a large number of processors. The scalability of a parallel IPM is determined by several key issues such as exploiting parallelism due to sparsity of the problem, reducing communication overhead and proper load balancing. In this paper we present an implementation of a parallel linear programming IPM workload and characterize its scalability on a 4-way Itanium\&reg; 2 system. We show a speedup of up to 3-times for some of the datasets. We also present a detailed micro-architectural analysis of the workload using VTune\&trade; performance analyzer. Our results suggest that a good IPM implementation is latency-bound. Based on these findings, we make suggestions on how to improve the performance of the IPM workload in the future. },
}

@inproceedings{1525991,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Kaeli, D.},
 year = {2005},
 pages = { iv-- iv},
 publisher = {IEEE},
 title = {Message from the Program Chair},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525991},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525991},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525991.pdf?arnumber=1525991},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 abstract = {},
}

@inproceedings{5306786,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Sankar, S. and Vaid, K.},
 year = {2009},
 pages = {148--157},
 publisher = {IEEE},
 title = {Storage characterization for unstructured data in online services applications},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306786},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306786},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306786.pdf?arnumber=5306786},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Geography, Image storage, Large-scale systems, Logic, Memory, Network servers, Scalability, Telecommunication traffic, Web 2.0, Web server, Web service, Web services, Web services, back-end data storage, blob data, customer traffic load, disc storage, disk subsystem design, electronic mail, file servers, front-end Web server, geomapping application, image tile storage, large scale email application, middle-tier application logic, online front-ends, online services application, optimal service scalability, probabilistic model, probability, social networking (online), social networking update, state transition, storage characterization, three tiered hierarchy, time series, time-series characteristics, unstructured data, user content storage, workload attribute, },
 abstract = {Mega datacenters hosting large scale Web services have unique workload attributes that need to be taken into account for optimal service scalability. Provisioning compute and storage resources to provide a seamless user experience is challenging since customer traffic loads vary widely across time and geographies, and the servers hosting these applications have to be rightsized to provide both performance within a single server and across a scale-out cluster. Typical user-facing Web services have a three tiered hierarchy - front-end Web servers, middle-tier application logic, and back-end data storage and processing layer. In this paper, we address the challenge of disk subsystem design for back-end servers hosting large amounts of unstructured (also called blob) data. Examples of typical content hosted on such servers include user generated content such as photos, email messages, videos, and social networking updates. Specific server applications analyzed in this paper correspond to the message store of a large scale email application, image tile storage for a large scale geo-mapping application, and user content storage for Web 2.0 type applications. We analyze the storage subsystems for these Web services in a live production environment and provide an overview of the disk traffic patterns and access characteristics for each of these applications. We then explore time-series characteristics and derive probabilistic models showing state transitions between locations on the data volumes for these applications. We then explore how these probabilistic models could be extended into a framework for synthetic benchmark generation for such applications. Finally, we discuss how this framework can be used for storage subsystem rightsizing for optimal scalability of such backend storage clusters. },
}

@inproceedings{5306787,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Riska, A. and Riedel, E.},
 year = {2009},
 pages = {158--167},
 publisher = {IEEE},
 title = {Evaluation of disk-level workloads at different time-scales},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306787},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306787},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306787.pdf?arnumber=5306787},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Availability, Bandwidth, Control systems, Data mining, Disk drives, Educational institutions, Electromagnetic compatibility, Instruments, Measurement techniques, Monitoring, disc drives, disk drive, disk-level utilization, disk-level workload evaluation, hour traces, lifetime traces, millisecond traces, storage management, },
 abstract = {In this paper, we characterize three different sets of disk-level traces collected from enterprise systems. The data sets differ in the granularity of the recorded information and are called accordingly the millisecond, the hour, and the lifetime traces. We analyze the disk-level utilization, the availability of idleness, the dynamics of the read and write traffic, over time and across an entire drive family. Our evaluation confirms that disk drives operate in moderate utilization and experience long stretches of idleness. The workload arriving at the disk is bursty across all time scales evaluated. Also, there is variability across drives of the same family, with a portion of them fully utilizing the available disk bandwidth for hours at a time. },
}

@inproceedings{5306784,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Danhua Guo and Guangdeng Liao and Bhuyan, L.N.},
 year = {2009},
 pages = {168--177},
 publisher = {IEEE},
 title = {Performance characterization and cache-aware core scheduling in a virtualized multi-core server under 10GbE},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306784},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306784},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306784.pdf?arnumber=5306784},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Isolation technology, Multicore processing, Network servers, Processor scheduling, Resource management, Scalability, TCP streaming microbenchmark, Topology, Virtual machining, Virtual manufacturing, Web server, Web server, Xen scheduler, cache storage, cache topology, cache-aware core scheduling, file servers, performance characterization, processor scheduling, ubiquitous multicore processor monitor, virtual machine, virtual machines, virtualized multicore server, },
 abstract = {Virtual Machine (VM) technology is experiencing a resurgent interest as the ubiquitous multi-core processors have become the de facto configuration on modern Web servers. Multicore servers potentially provide sufficient physical resources to realize VM's benefits including performance isolation, manageability and scalability. However, the network performance of virtualized multi-core servers falls short of expectation. It is therefore important to understand the overhead implications. In this paper, we evaluate the network performance of a virtualized multi-core server using a TCP streaming microbenchmark (Iperf) and SPECweb2005. We first motivate our research by presenting the performance gap between native and virtualized environment. We then break down the overhead from an architectural viewpoint and show that the cache topology greatly influences the performance. We also profile the virtual machine monitor (VMM) at a function level to illustrate that functions in the current version of the Xen scheduler are the major contributors to the poor utilization of cache topology. Consequently, we implement a static onloading scheme to separate interrupt handling from application processes and execute them on cores with cache affinity. Based on the observed benefits, we modify the Xen scheduler to migrate virtual CPUs dynamically to exploit the cache topology. Our results show that the VM performance improves by an average of 12\% for Iperf and 15\% for SPECweb2005. },
}

@inproceedings{5306785,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Borin, E. and Youfeng Wu},
 year = {2009},
 pages = {178--187},
 publisher = {IEEE},
 title = {Characterization of DBT overhead},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306785},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306785},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306785.pdf?arnumber=5306785},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, Binary codes, Educational institutions, Hardware, ISA virtualization, Instruction sets, Instruments, Microarchitecture, Performance analysis, Runtime, Table lookup, binary codes, dynamic binary translator performance, legacy binary code, microarchitectures, transparent binary instrumentation, zero-overhead dynamic binary translation, },
 abstract = {In recent years, dynamic binary translation has emerged as an important tool with many real world applications. Besides supporting legacy binary code and ISA virtualization, it enables innovative co-designed microarchitectures and allows transparent binary instrumentation. The dynamic nature of the translation usually incurs extra execution overhead and many research works had proposed software and hardware solutions to minimize the overhead. In this paper, we analyze our dynamic binary translator performance and depict the main sources of overhead in details. We classify the translation operations and associated overhead into five major categories, and quantify their contribution to the overall overhead. Based on the analysis and detailed evaluation, we identify and point out the most promising solutions to address the overhead problem. We believe this study is an important first step toward the grand goal of zero-overhead dynamic binary translation. },
}

@inproceedings{5306782,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Charles, J. and Jassi, P. and Ananth, N.S. and Sadat, A. and Fedorova, A.},
 year = {2009},
 pages = {188--197},
 publisher = {IEEE},
 title = {Evaluation of the Intel&#x00AE; Core&#x2122; i7 Turbo Boost feature},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306782},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306782},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306782.pdf?arnumber=5306782},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Acceleration, Application software, Energy consumption, Frequency estimation, Intel Core i7, Leakage current, Multicore processing, Nehalem, Performance analysis, Temperature, Turbo Boost technology, Voltage, Yarn, energy consumption, memory accesses, microprocessor chips, parallel applications, performance evaluation, },
 abstract = {The Intelreg Coretrade i7 processor code named Nehalem has a novel feature called Turbo Boost which dynamically varies the frequencies of the processor's cores. The frequency of a core is determined by core temperature, the number of active cores, the estimated power and the estimated current consumption. We perform an extensive analysis of the Turbo Boost technology to characterize its behavior in varying workload conditions. In particular, we analyze how the activation of Turbo Boost is affected by inherent properties of applications (i.e., their rate of memory accesses) and by the overall load imposed on the processor. Furthermore, we analyze the capability of Turbo Boost to mitigate Amdahl's law by accelerating sequential phases of parallel applications. Finally, we estimate the impact of the Turbo Boost technology on the overall energy consumption. We found that Turbo Boost can provide (on average) up to a 6\% reduction in execution time but can result in an increase in energy consumption up to 16\%. Our results also indicate that Turbo Boost sets the processor to operate at maximum frequency (where it has the potential to provide the maximum gain in performance) when the mapping of threads to hardware contexts is sub-optimal. },
}

@inproceedings{5306783,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Yoo, R.M. and Romano, A. and Kozyrakis, C.},
 year = {2009},
 pages = {198--207},
 publisher = {IEEE},
 title = {Phoenix rebirth: Scalable MapReduce on a large-scale shared-memory system},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306783},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306783},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306783.pdf?arnumber=5306783},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Concurrent computing, Delay, Large-scale systems, Load management, MapReduce runtime, NUMA characteristics, Operating systems, Parallel programming, Phoenix, Programming profession, Runtime, Scalability, UltraSPARC T2+ system, Yarn, dynamic runtime, large-scale shared-memory system, multiprocessors, parallel programming, parallel programming, scalable MapReduce, shared memory systems, shared-memory multicores, },
 abstract = {Dynamic runtimes can simplify parallel programming by automatically managing concurrency and locality without further burdening the programmer. Nevertheless, implementing such runtime systems for large-scale, shared-memory systems can be challenging. This work optimizes Phoenix, a MapReduce runtime for shared-memory multi-cores and multiprocessors, on a quad-chip, 32-core, 256-thread UltraSPARC T2+ system with NUMA characteristics. We show how a multi-layered approach that comprises optimizations on the algorithm, implementation, and OS interaction leads to significant speedup improvements with 256 threads (average of 2.5times higher speedup, maximum of 19times). We also identify the roadblocks that limit the scalability of parallel runtimes on shared-memory systems, which are inherently tied to the OS scalability on large-scale systems. },
}

@inproceedings{5306780,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Shu Xu and Bo Huang and Junyong Ding and Jinquan Dai},
 year = {2009},
 pages = {208--216},
 publisher = {IEEE},
 title = {Browser workload characterization for an Ajax-based commercial online service},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306780},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306780},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306780.pdf?arnumber=5306780},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {AJAX technique, Application software, Cloud computing, Collaborative work, Internet, Java, Logic, Network servers, Search engines, User interfaces, Web 2.0, Web services, XML, Zero current switching, Zimbra, application logic, browser memory footprint, browser submodule breakdown, browser workload characterization, browser-based client, browser-independent design, cloud computing, commercial online service, disruptive trend, fancy user interface, online front-ends, platform-independent design, user interfaces, },
 abstract = {The transition to cloud computing and SaaS is a disruptive trend where users can conveniently access the services through browsers at any clients. In addition, with the prevalence of Web 2.0 and AJAX techniques, a browser-based client can have complex application logic and fancy user interface that are comparable to traditional desktop applications. This paper reports the study of workload construction and characterization for browser-based clients, using the Ajax-based Web client of Zimbra (a commercial online messaging and collaboration suite). By comparing the various workload behaviors across different Zimbra server datasets, different browsers and different client platforms, it presents the characteristics of a real-life Web application, which has significant differences from existing browser benchmarks in the literature. In addition, the platform-independent and browser-independent design of our workload makes it portable across various clients. Finally, this paper also provides valuable insights to the browser internals by analyzing the workload execution, the browser memory footprint and the breakdown of browser sub-modules. },
}

@inproceedings{5306781,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Ishizaki, K. and Nakatani, T. and Daijavad, S.},
 year = {2009},
 pages = {217--226},
 publisher = {IEEE},
 title = {Analyzing and improving performance scalability of commercial server workloads on a chip multiprocessor},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306781},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306781},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306781.pdf?arnumber=5306781},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, CMOS technology, CPI, Frequency, Hardware, Java, Java code, L2 data cache miss rates, L2 instruction cache miss rates, Parallel processing, Performance analysis, Pipelines, Process design, Scalability, Yarn, architectural characteristics, cache storage, chip multiprocessor, commercial server workloads, data TLB miss rates, lock contention, memory traffic, multiprocessing systems, network servers, performance scalability, scalability enhancements, },
 abstract = {A chip multiprocessor (CMP) with many low performance cores can achieve high performance or high performance/power for commercial server applications. The large number of hardware threads of a CMP with many low performance cores poses significant challenges to application developers in writing scalable applications. Many papers have assessed the architectural characteristics and the performance scalability, and some of them have identified lock contention as one of the scalability bottlenecks. However, there are few studies that resolved these problems, analyzed their causes, and compared the architectural characteristics before and after the scalability limitations were addressed. We analyzed and resolved some of the problems limiting the scalability of three commercial server applications with 64 hardware threads. We also did before and after comparisons of the architectural characteristics affected by the scalability enhancements, supporting the development of new processors. We addressed the lock contention with changes in the Java code. Our enhancements improved the performance scalability by up to 132\%. We show that though the causes of lock contention are in different software layers, they share certain similarities and can be organized in three categories. Our comparisons reveal that the CPI and data TLB miss rates decrease, but the L2 data cache miss rates, L2 instruction cache miss rates, and memory traffic increase. These results suggest that we need to address the performance scalability problems of an application before we can accurately measure the architectural characteristics of a CMP. },
}

@inproceedings{1525996,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Eeckhout, L. and Sampson, J. and Calder, B.},
 year = {2005},
 pages = { 2-- 12},
 publisher = {IEEE},
 title = {Exploiting program microarchitecture independent characteristics and phase behavior for reduced benchmark suite simulation},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525996},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525996},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525996.pdf?arnumber=1525996},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { architecture design space exploration,  benchmark testing,  phase behavior,  pipeline simulation,  program microarchitecture,  program simulation,  reduced benchmark suite simulation,  sampling methods,  software performance evaluation,  targetted sampling, Application software, Computational modeling, Computer architecture, Costs, Microarchitecture, Pipelines, Sampling methods, Space exploration, },
 abstract = {Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of an industry standard benchmark can take weeks to complete. Simulating the full execution of the whole benchmark suite for one architecture configuration can take months. To address this issue researchers have examined using targetted sampling based on phase behavior to significantly reduce the simulation time of each program in the benchmark suite. However, even with this sampling approach, simulating the full benchmark suite across a large range of architecture designs can take days to weeks to complete. The goal of this paper is to further reduce simulation time for architecture design space exploration. We reduce simulation time by finding similarity between benchmarks and program inputs at the level of samples (100M instructions of execution). This allows us to use a representative sample of execution from one benchmark to accurately represent a sample of execution of other benchmarks and inputs. The end result of our analysis is a small number of sample points of execution. These are selected across the whole benchmark suite in order to accurately represent the complete simulation of the whole benchmark suite for design space exploration. We show that this provides approximately the same accuracy as the SimPoint sampling approach while reducing the number of simulated instructions by a factor of 1.5. },
}

@inproceedings{5650317,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Dhanotia, A. and Grover, S. and Byrd, G.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Analyzing and scaling parallelism for network routing protocols},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650317},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650317},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650317.pdf?arnumber=5650317},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {BGP scalable multithreaded implementation, Dynamic scheduling, Internet, Internet backbone, Multicore processing, Parallel processing, Routing, Routing protocols, Software, generic architecture, legacy code, multi-threading, multicore processing, network routing protocols, parallelization methodology, routing protocols, scaling parallelism, ubiquitous computing, ubiquitous protocol, },
 abstract = {The serial nature of legacy code in routing protocol implementations has inhibited a shift to multicore processing in the control plane, even though there is much inherent parallelism. In this paper, we investigate the use of multicore as the compute platform for routing applications using BGP, the ubiquitous protocol for routing in the Internet backbone, as a representative application. We develop a scalable multithreaded implementation for BGP and evaluate its performance on several multicore configurations using a fully configurable multicore simulation environment. We implement several optimizations at the software and architecture levels, achieving a speedup of 6.5 times over the sequential implementation, which translates to a throughput of ~170K updates per second. Subsequently, we propose a generic architecture and parallelization methodology which can be applied to all routing protocol implementations to achieve significant performance improvement. },
}

@inproceedings{990760,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Kirubanandan, N. and Sivasubramaniam, A. and Vijaykrishnan, N. and Kandemir, M. and Irwin, M.J.},
 year = {2001},
 pages = { 193-- 201},
 publisher = {IEEE},
 title = {Memory energy characterization and optimization for the SPEC2000 benchmarks},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990760},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990760},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990760.pdf?arnumber=990760},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { DRAM power mode control,  PDA,  SPEC2000 benchmarks,  benchmarks,  low power computing devices,  memory access behavior,  memory devices,  memory energy characterization,  optimal energy savings,  optimization,  performance evaluation,  power mode control,  storage management, Design optimization, Energy consumption, Energy management, Hardware, Mobile computing, Pattern analysis, Performance analysis, Personal digital assistants, Power system management, Random access memory, },
 abstract = {With an increasing focus on low power computing devices such as PDA's, energy has become an important criterion for optimization. Power mode control of memory devices has the potential to significantly reduce energy consumption, for a moderate increase in execution time. Prior studies have explored the benefits of many schemes for DRAM power mode control. In this work, we present a framework for estimating the optimal energy savings that can be obtained under various constraints. We also analyze the memory energy consumption patterns of a set of benchmarks drawn from the SPEC2000 suite and establish that energy consumed during power mode transitions is a significant component of DRAM energy. We characterize the memory access behavior of these benchmarks by studying their inter access times. Based on our analysis, we revisit existing mode control schemes and fine tune their performance. In particular we propose a history based mode control policy, which performs better than the existing schemes in most cases. },
}

@inproceedings{1226491,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {Sorenson, E.S. and Flanagan, J.K.},
 year = {2002},
 pages = { 23-- 33},
 publisher = {IEEE},
 title = {Evaluating synthetic trace models using locality surfaces},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226491},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226491},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226491.pdf?arnumber=1226491},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { cache performance statistics,  cache simulation,  cache storage,  locality surfaces,  synthetic trace generation models,  virtual machines, },
 abstract = {In this paper we analyze several synthetic trace generation models using locality surfaces. The locality surfaces let us discover what elements of the real trace were accurately modeled and what features were not. None of the models examined are very good at retaining the locality of the real trace. We can see from cache simulation results that if the locality surface does not accurately reflect the locality of the real workload, the cache performance statistics will not be accurate either. },
}

@inproceedings{1437390,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Bhowmik, A. and Franklin, M.},
 year = {2004},
 pages = { 19-- 26},
 publisher = {IEEE},
 title = {Evaluation of a speculative multithreading compiler by characterizing program dependences},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437390},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437390},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437390.pdf?arnumber=1437390},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { SpMT compiler,  SpMT system,  control speculations,  data speculations,  dynamic program dependences,  interthread control predictions,  interthread data dependences,  multi-threading,  nonnumeric applications,  nonnumeric program parallelization,  parallelising compilers,  processor scheduling,  program dependence characterization,  run-time behaviors,  sequential program partitioning,  speculative multithreading compiler,  thread formation,  thread generation, Automation, Computer science, Control systems, Hardware, Multithreading, Parallel processing, Performance analysis, Program processors, Runtime, Yarn, },
 abstract = {Speculative multithreading (SpMT) promises to be an effective mechanism for parallelizing non-numeric programs. Proper thread formation is crucial for obtaining good speedup in an SpMT system. We have developed an SpMT compiler framework for partitioning sequential programs into multiple threads. Since control and data speculations are the essence of SpMT execution model, inter-thread data dependences and inter-thread control predictions at run-time play crucial roles in affecting the performance of the SpMT system. Therefore, to evaluate existing SpMT compiler or hardware systems, and to design more efficient systems it is necessary to characterize the dynamic program dependences carefully. In this paper, we have studied the run-time behaviors of inter-thread data and control dependences of the threads generated by our compiler in detail and used that for analyzing the performance. The analyses reveal that our compiler has successfully modeled the inter-thread data and control dependences of non-numeric applications and minimized them while generating the threads. },
}

@inproceedings{1437391,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 27-- 27},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437391},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437391},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437391.pdf?arnumber=1437391},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437392,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 28-- 28},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437392},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437392},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437392.pdf?arnumber=1437392},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1526017,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Nazhandali, L. and Minuth, M. and Austin, T.},
 year = {2005},
 pages = { 197-- 203},
 publisher = {IEEE},
 title = {SenseBench: toward an accurate evaluation of sensor network processors},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526017},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526017},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526017.pdf?arnumber=1526017},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { SenseBench,  composition foot print,  distributed sensors,  energy per bundle,  microprocessor chips,  performance evaluation,  real-time systems,  real-time workload,  sensor network processor evaluation,  stream applications,  times real-time, Biomedical monitoring, Computer networks, Defense industry, Inspection, Military computing, Portable computers, Power supplies, Process design, Sensor systems, Surveillance, },
 abstract = {Sensor network processors introduce an unprecedented level of compact and portable computing. These small processing systems reside in the environment which they monitor, combining sensing, computation, storage, communication, and power supplies into small form factors. Sensor processors have a wide variety of applications in medical monitoring, environmental sensing, industrial inspection, and military surveillance. Despite efforts to design suitable processors for these systems (Ekanayake et al., 2004; Hempstead et al., 2005; Nazhandali et al., 2005; Wameke and Pister, 2004), there is no well-defined method to evaluate their performance and energy consumption. The historically used MIPS (millions of instructions per second) and EPI (energy per instruction) metrics cannot provide an accurate comparison because of their dependence on the nature of instructions, which differ across instruction set architectures. On the other hand, the current well-defined benchmarks (1989; Guthaus et al., 2001; Lee et al., 1997) do not represent typical workloads of sensor network systems, and hence, are not suitable to compare sensor processors. This paper defines a set of stream applications representing the typical real-time workload of a sensor processor. Furthermore, three new metrics, EPB (energy per bundle), xRT (times real-time), and CFP (composition foot print) are introduced to evaluate and compare such systems. },
}

@inproceedings{1437394,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Illikkal, R. and Iyer, R. and Newell, D.},
 year = {2004},
 pages = { 37-- 44},
 publisher = {IEEE},
 title = {Micro-architectural anatomy of a commercial TCP/IP stack},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437394},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437394},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437394.pdf?arnumber=1437394},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { CPI,  MPI,  O/S scheduler,  TCP/IP packet processing,  TCP/IP processing overhead,  TLB misses,  application program interfaces,  commercial TCP/IP stack,  computer architecture,  computer architecture,  drivers,  instruction breakdown,  interrupt routines,  interrupts,  message passing,  microarchitectural anatomy,  microprocessor,  multiprocessing systems,  network stack,  packet switching,  performance analysis,  performance evaluation,  performance simulations,  processor scheduling,  software modules,  symbol annotation,  system simulation,  transport protocols, Anatomy, Communication system control, Communications technology, Data mining, Electric breakdown, Laboratories, Microprocessors, Scheduling, TCPIP, Transport protocols, },
 abstract = {Over the last couple of decades, computer architects and performance analysts have routinely attempted to profile the overhead of TCP/IP processing in an effort to understand where the time was spent. It is well understood that this is a rather difficult problem since the processing time is spread across various software modules such as the network stack, interrupt routines, drivers, O/S scheduler, etc. As a result, the problem of extracting the micro-architectural characteristics of TCP/IP processing is significantly more challenging. In this paper, we start by covering the previous attempts at this problem and show what existing tools can provide in terms of execution time characteristics. We then propose a detailed methodology that combines full-system simulation, cycle-accurate performance simulations and symbol annotation to provide a rich cycle-accurate view of TCP/IP packet processing execution. We discuss initial results based on our profiling methodology and discuss where the time is spent. This includes an analysis of micro-architectural characteristics (such as instruction breakdown, CPI, MPI and TLB misses on a state-of-the-art microprocessor). },
}

@inproceedings{1437395,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 45-- 45},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437395},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437395},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437395.pdf?arnumber=1437395},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437396,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 46-- 46},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437396},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437396},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437396.pdf?arnumber=1437396},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1437397,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Watson, M. and Flanagan, J.K.},
 year = {2004},
 pages = { 47-- 54},
 publisher = {IEEE},
 title = {Does halting make hardware trace collection inaccurate? A study using Pentium 4 performance counters and SPEC2000},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437397},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437397},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437397.pdf?arnumber=1437397},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { Pentium 4 performance counters,  SPEC2000,  buffers,  cache designs,  cache storage,  halting effects,  hardware trace collection,  interrupt,  memory access,  memory hierarchy testing,  microprocessor chips,  microprocessors,  multiprocessing systems,  performance evaluation,  processor address traces,  runtime statistics,  storage allocation,  system monitoring,  trace-collection systems,  workload characterization, Computer displays, Computer science, Counting circuits, Hardware, Instruments, Microprocessors, Pins, Probes, Statistics, Testing, },
 abstract = {Processor address traces are invaluable for characterizing workloads and testing proposed memory hierarchies. Long traces are needed to exercise modern cache designs and produce meaningful results, but are difficult to collect with hardware monitors because microprocessors access memory too frequently for disks or other large storage to keep up. The small, fast buffers of the monitors fill quickly; in order to obtain long contiguous traces, the processor must be stopped while the buffer is emptied. This halting may perturb the traces collected, but this cannot be measured directly, since long uninterrupted traces cannot be collected. We make the case that hardware performance counters, which collect runtime statistics without influencing execution, can be used to measure halting effects. We use the performance counters of the Pentium 4 processor to collect statistics while halting the processor as if traces were being collected. We then compare these results to the statistics obtained from unhalted runs. We present our results in terms of which counters are affected, why, and what this means for trace-collection systems. },
}

@inproceedings{1437398,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Vandierendonck, H. and De Bosschere, K.},
 year = {2004},
 pages = { 55-- 62},
 publisher = {IEEE},
 title = {Experiments with subsetting benchmark suites},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437398},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437398},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437398.pdf?arnumber=1437398},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { SPEC CPU 2000,  benchmark programs,  benchmark suite subsetting,  benchmark testing,  computing system performance,  performance evaluation,  redundancy,  redundancy,  subsetting algorithm, Computational modeling, Computer displays, Computer performance, Costs, Image analysis, Information systems, Redundancy, Time measurement, },
 abstract = {Benchmarks are one of the most popular tools to compare the performance of computing systems. Benchmark suites typically contain multiple benchmark programs with more or less the same properties. Hence the suite contains redundancy, which increases the cost of executing or simulating the benchmark suite without adding value. To limit simulation time, researchers frequently subset benchmark suites. However, correctly identifying a representative subset is of paramount importance to perform a trustworthy evaluation. This paper shows that subsetting a benchmark suite in such a way that representativeness of the suite is maintained is non-trivial. We show that a small randomly selected subset is not representative of the fill benchmark suite. We discuss algorithms to subset the SPEC CPU 2000 benchmark suite and show that they provide more representative subsets than randomly selected subsets. However, the algorithms evaluated in this paper do not always compute representative subsets: the algorithms produce bad results for some subset sizes. In this sense, these algorithms are unreliable, as it remains necessary to validate the benchmark suite subset. We find one subsetting algorithm that is reliable. It is, however, uncertain whether this algorithm is also reliable under other circumstances. },
}

@inproceedings{1437399,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Pereira, A. and Franco, G. and Silva, L. and Meira, W., Jr. and Santos, W.},
 year = {2004},
 pages = { 63-- 70},
 publisher = {IEEE},
 title = {The USAR characterization model},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437399},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437399},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437399.pdf?arnumber=1437399},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { Internet,  USAR characterization model,  Web server performance,  Web server scalability,  file servers,  hierarchical characterization,  performance evaluation,  proxy-cache server,  query processing,  request interarrival time,  request latency,  simulation model,  user behavior,  user interaction,  workload generators, Aggregates, Character generation, Delay, Information analysis, Internet, Pattern analysis, Quality of service, Scalability, Time factors, Web server, },
 abstract = {Understanding the user behavior is a need to analyze the performance and the scalability of Web servers. This knowledge is used, for instance, to build workload generators that help evaluating the performance of those servers. Current workload generators are typically memory-less, being unable to mimic actual user interaction with the system. In this work we propose a hierarchical characterization and simulation model focused on the user behavior, named USAR. We use the latency and inter-arrival time of the requests to model user actions, which are the basis of our model. We validate this model through a proxy-cache server case study, where we perform the characterization and construct a user behavior simulator. We foresee from the results the possibility to generate more realistic workloads. },
}

@inproceedings{990761,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {},
 year = {2001},
 pages = { 202-- 203},
 publisher = {IEEE},
 title = {Author Index},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990761},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990761},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990761.pdf?arnumber=990761},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00990761.png" border="0"> },
}

@inproceedings{4362170,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {},
 year = {2007},
 pages = {iii--iii},
 publisher = {IEEE},
 title = {Message from the Program Co-Chairs},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362170},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362170},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362170.pdf?arnumber=4362170},
 isbn = {978-1-4244-1561-8},
 language = {English},
 abstract = {},
}

@inproceedings{1525997,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Isci, C. and Martonosi, M.},
 year = {2005},
 pages = { 13-- 23},
 publisher = {IEEE},
 title = {Detecting recurrent phase behavior under real-system variability},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1525997},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1525997},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01525997.pdf?arnumber=1525997},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { application phase analysis,  dynamic on-the-fly system management,  real-system variability,  recurrent phase behavior detection,  software metrics,  software performance evaluation,  time-varying real system measurements,  transition-based phase detection, Analytical models, Application software, Displays, Energy management, Face detection, Operating systems, Pattern recognition, Phase detection, Power system management, Timing, },
 abstract = {As computer systems become ever more complex and power hungry, research on dynamic on-the-fly system management and adaptations receives increasing attention. Such research relies on recognizing and responding to patterns or phases in application execution, which has therefore become an important and widely-studied research area. While application phase analysis has received significant attention, much of this attention thus far has focused on simulation-based studies. In these cycle-level simulations without indeterministic operating system intervention, applications display behavior that is repeatable from phase to phase and from run to run. A natural question, therefore, concerns how these phases appear in real system runs, where interrupts and time variability can influence the timing and behavior of the program. Our paper examines the phase behavior of applications running on real systems. The key goals of our work are to reliably discern and recover phase behavior in the face of application variability stemming from real system effects and time sampling. We propose a set of new, "transition-based" phase detection techniques. Our techniques can detect repeatable workload phase information from time-varying, real system measurements with less than 5\% false alarm probabilities. In comparison to previous value-based detection methods, our transition-based techniques achieve on average 6x higher recurrent phase detection efficiency under real system variability. },
}

@inproceedings{4362171,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {},
 year = {2007},
 pages = {iv--iv},
 publisher = {IEEE},
 title = {IISWC-2007 People},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362171},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362171},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362171.pdf?arnumber=4362171},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Finance, },
 abstract = {},
}

@inproceedings{4636079,
 booktitle = {Workload Characterization, 2008. IISWC 2008. IEEE International Symposium on},
 author = {},
 year = {2008},
 pages = {vii--vii},
 publisher = {IEEE},
 title = {IISWC 2008 preface},
 date = {14-16 Sept. 2008},
 doi = {10.1109/IISWC.2008.4636079},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4636079},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4629859/4636078/04636079.pdf?arnumber=4636079},
 isbn = {978-1-4244-2777-2},
 language = {English},
 keywords = {Abstracts, Airports, Art, Cities and towns, Computer science, Cultural differences, Feedback, Internet, Organizing, Technological innovation, },
 abstract = {},
}

@inproceedings{5649169,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Ioannou, N. and Singer, J. and Khan, S. and Xekalakis, P. and Yiapanis, P. and Pocock, A. and Brown, G. and Luja&#x0301;n, M. and Watson, I. and Cintra, M.},
 year = {2010},
 pages = {1--12},
 publisher = {IEEE},
 title = {Toward a more accurate understanding of the limits of the TLS execution paradigm},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649169},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649169},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649169.pdf?arnumber=5649169},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Benchmark testing, Instruction sets, Java, Optimization, Programming, Synchronization, TLS execution paradigm, architectural constraint, checkpointing, compiler development, data dependence, data handling, load imbalance, parallel programming, parallel thread, program compilers, programming style, resource allocation, thread-level speculation, },
 abstract = {Thread-Level Speculation (TLS) facilitates the extraction of parallel threads from sequential applications. Most prior work has focused on developing the compiler and architecture for this execution paradigm. Such studies often narrowly concentrated on a specific design point. On the other hand, other studies have attempted to assess how well TLS performs if some architectural/ compiler constraint is relaxed. Unfortunately, such previous studies have failed to truly assess TLS performance potential, because they have been bound to some specific TLS architecture and have ignored one or another important TLS design choice, such as support for out-of-order task spawn or support for intermediate checkpointing. In this paper we attempt to remedy some of the shortcomings of previous TLS limit studies. To this end a characterization approach is pursued that is, as much as possible, independent of specific architecture configurations. High-level TLS architectural support is explored in one common framework. In this way, a more accurate upper-bound on the performance potential of the TLS execution paradigm is obtained (as opposed to some particular architecture design point) and, moreover, relative performance gains can be related to specific high-level architectural support. Finally, in the spirit of performing a comprehensive study, applications from a variety of domains and programming styles are evaluated. Experimental results suggest that TLS performance varies significantly depending on the features provided by the architecture. Additionally, the performance of these systems is not only hindered by data dependences, but also by load imbalance and limited coverage. },
}

@inproceedings{5306788,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Srinivasan, S. and Zhen Fang and Iyer, R. and Zhang, S. and Espig, M. and Newell, D. and Cermak, D. and Yi Wu and Kozintsev, I. and Haussecker, H.},
 year = {2009},
 pages = {128--137},
 publisher = {IEEE},
 title = {Performance characterization and optimization of mobile augmented reality on handheld platforms},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306788},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306788},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306788.pdf?arnumber=5306788},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, Augmented reality, Cameras, Cities and towns, Displays, Handheld computers, Image databases, Legged locomotion, MAR workload, Mobile computing, Smart phones, augmented reality, cache conflict avoidance, displays information recognition, handheld platform, hotspot function architectural characterization, image recognition, low power general purpose processor, low-power electronics, miscellaneous code optimization, mobile Internet device, mobile augmented reality optimization, multi-threading, notebook computers, performance characterization, performance evaluation, software optimization, vectorization, visual computing application, },
 abstract = {The introduction of low power general purpose processors (like the Intelreg Atomtrade processor) expands the capability of handheld and mobile Internet devices (MIDs) to include compelling visual computing applications. One rapidly emerging visual computing usage model is known as mobile augmented reality (MAR). In the MAR usage model, the user is able to point the handheld camera to an object (like a wine bottle) or a set of objects (like an outdoor scene of buildings or monuments) and the device automatically recognizes and displays information regarding the object(s). Achieving this on the handheld requires significant compute processing resulting in a response time in the order of several seconds. In this paper, we analyze a MAR workload and identify the primary hotspot functions that incur a large fraction of the overall response time. We also present a detailed architectural characterization of the hotspot functions in terms of CPI, MPI, etc. We then implement and analyze the benefits of several software optimizations: (a) vectorization, (b) multi-threading, (c) cache conflict avoidance and (d) miscellaneous code optimizations that reduce the number of computations. We show that a 3X performance improvement in execution time can be achieved by implementing these optimizations. Overall, we believe our analysis provides a detailed understanding of the processing for a new domain of visual computing workloads (i.e. MAR) running on low power handheld compute platforms. },
}

@inproceedings{1249059,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Ramanathan, S. and Srinivasan, R. and Cook, J.},
 year = {2003},
 pages = { 77-- 85},
 publisher = {IEEE},
 title = {Intrinsic data locality of modern scientific workloads},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249059},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249059},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249059.pdf?arnumber=1249059},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { SPEC2000 workloads,  analytic model,  cache configurations,  cache hit ratios,  cache performance,  cache sizes,  cache storage,  intrinsic data locality,  memory architecture,  memory performance prediction,  microarchitecture-dependent metric,  microarchitecture-independent characterization,  microarchitecture-independent manner,  modern scientific workloads,  performance evaluation,  quantitative statistics,  statistical analysis, Data engineering, Degradation, Delay effects, Microarchitecture, Modems, Particle measurements, Performance analysis, Size measurement, Statistics, },
 abstract = {Understanding the intrinsic data locality of a workload is essential to understanding and predicting cache performance. The intrinsic data locality of a particular application or workload can be measured in a microarchitecture-independent manner. The data resulting from these measurements ideally can be used to develop an analytic model for predicting memory performance on different cache sizes and configurations. Many studies on data locality use cache hit ratios, a microarchitecture-dependent metric, to examine locality. In this paper, we present a microarchitecture-dependent and a microarchitecture-independent characterization of the SPEC2000 workloads. We present quantitative statistics on the different types of data locality (e.g. spatial and temporal) exhibited by these workloads and we show that the composite intrinsic locality can be correlated to locality measured by cache hit ratio. },
}

@inproceedings{1249054,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Black, J.E. and Wright, D.F. and Salgueiro, E.M.},
 year = {2003},
 pages = { 21-- 29},
 publisher = {IEEE},
 title = {Improving the performance of OLTP workloads on SMP computer systems by limiting modified cache lines},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249054},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249054},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249054.pdf?arnumber=1249054},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { CPU,  OLTP workloads,  SMP systems,  Unisys NX6830 series,  average memory latency,  cache storage,  cache-to-cache migration,  modified cache lines,  multiprocessing systems,  online transaction processing,  original limiting queue,  performance evaluation,  performance improvement,  queueing theory,  second level cache,  symmetric multiprocessor systems,  system performance,  transaction processing, Bandwidth, Degradation, Delay, Information retrieval, Monitoring, Multiprocessing systems, Protocols, Topology, Upper bound, },
 abstract = {Symmetric multiprocessor (SMP) computer systems with more than four CPUs often exhibit significantly lower overall performance than would be expected from the sum of the performance of the individual CPUs. One of the causes of this degradation is the increased average memory latency due to cache to cache migration of modified cache lines. Such transfers often incur significantly longer latencies than a simple cache miss, which can be satisfied from main memory. By setting an upper bound on the number of modified cache lines that are allowed to exist in a main memory when this limit is exceeded, the average memory latency and overall system performance on an online transaction processing (OLTP) workload can be improved. This paper presents an investigation of this concept, called original limiting, on a commercial SMP system. The Original Limiting concept was implemented in the second level cache (SLC) of the Unisys NX6830 series of SMP systems, which support up to eight CPUs. An original limiting queue (OLQ) was added to limit the number of exclusive or modified lines in a 5\% improvement in the number of transactions processed per minute, by reducing the average memory latency. A variety of experiments indicate that the OLQ is a simple, but effective, mechanism to enhance the performance of OLTP applications on SMP systems. },
}

@inproceedings{1249055,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Makineni, S. and Iyer, R.},
 year = {2003},
 pages = { 33-- 41},
 publisher = {IEEE},
 title = {Performance characterization of TCP/IP packet processing in commercial server workloads},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249055},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249055},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249055.pdf?arnumber=1249055},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { Intel,  Jumbo frames,  Large Segment Offload,  Microsoft Windows* Server 2003 operating system,  NIC features,  Pentium M microprocessor,  SPECweb99,  TCP/IP packet processing,  TPC-C,  TPC-W,  Web services,  communication protocol,  electronic commerce,  file servers,  memory storage,  microprocessor chips,  operating systems (computers),  packet switching,  performance characterization,  performance evaluation,  server applications,  server workloads,  termination nodes,  transport protocols, Engines, Intelligent networks, Laboratories, Microprocessors, Network servers, Operating systems, Performance analysis, Protocols, Service oriented architecture, TCPIP, },
 abstract = {TCP/IP is the communication protocol of choice for many current and next generation server applications (Web services, e-commerce, storage, etc.). As a result, the performance of these applications can be heavily dependent on the efficient TCP/IP packet processing within the termination nodes. Motivated by this, our work presented in this paper focuses on analyzing the underlying architectural characteristics of TCP/IP packet processing component within server workloads. Our analysis and characterization methodology is based on in-depth measurement experiments of TCP/IP packet processing performance on Intel's state-of-the-art low-power Pentium\&reg; M microprocessor running the Microsoft Windows* Server 2003 operating system. We start by analyzing the impact of NIC features such as Large Segment Offload and the use of Jumbo frames on TCP/IP packet processing performance. We then show that the architectural characteristics of transmit-side processing (largely compute-bound) are significantly different than receive-side processing (mostly memory-bound). Finally we quantify the computational requirements for sending/receiving packets within commercial workloads (SPECweb99, TPC-C and TPC-W) and show that they can form a substantial component. },
}

@inproceedings{1249056,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Ramaswamy, R. and Wolf, T.},
 year = {2003},
 pages = { 42-- 50},
 publisher = {IEEE},
 title = {PacketBench: a tool for workload characterization of network processing},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249056},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249056},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249056.pdf?arnumber=1249056},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { Internet,  Internet,  PacketBench,  application optimization,  computer networks,  microarchitectural metrics,  network performance,  network processing,  network processor design,  network routers,  network security,  networking related metrics,  packet forwarding,  packet memory management,  packet payload encryption,  packet switching,  packet traces,  resource allocation,  software tools,  statistics collection,  storage management,  workload characteristics,  workload characterization, Computer architecture, Cryptography, IP networks, Memory management, Network servers, Payloads, Search engines, Space exploration, Statistics, Workstations, },
 abstract = {Network processing is becoming an increasingly important paradigm as the Internet moves towards an architecture with more complex functionality inside the network. Modern routers not only forward packets, but also process headers and payloads to implement a variety of functions related to security, performance, and customization. It is important to get a detailed understanding of the workloads associated with this processing in order to be able to develop efficient network processing engines. We present a tool called PacketBench, which provides a framework for implementing network processing applications and obtaining an extensive set of workload characteristics. PacketBench provides the support functions to handle various packet traces and manage packet memory. For statistics collection, PacketBench provides the ability to derive a number of microarchitectural and networking related metrics. We present the results of such measurements for four different networking applications ranging from simple packet forwarding to complex packet payload encryption. The results show that such workload analysis has a range of uses from network processor design to application optimization. },
}

@inproceedings{1249057,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Gast, J. and Barford, P.},
 year = {2003},
 pages = { 51-- 61},
 publisher = {IEEE},
 title = {Evaluating and modeling window synchronization in highly multiplexed flows},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249057},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249057},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249057.pdf?arnumber=1249057},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { Internet,  Internet,  Surveyor infrastructure,  active probe data,  aggregated flows,  cWnd values,  computer networks,  discrete congestion events,  fast links,  multiplexed flows,  multiplexing,  neighboring links,  packet switching,  performance evaluation,  queue buildup,  queue draining phases,  queueing theory,  spaced loss events,  synchronisation,  telecommunication congestion control,  user interfaces,  window synchronization, Bit rate, Computer science, Fractals, Internet, Modems, Predictive models, Spine, Telecommunication traffic, Traffic control, Transport protocols, },
 abstract = {In this paper, we investigate issues of synchronization in highly aggregated flows such as would be found in the Internet backbone. Our hypothesis is that regularly spaced loss events lead to window synchronization in long lived flows. We argue that window synchronization is likely to be more common in the Internet than previously reported. We support our argument with evidence of the existence and evaluation of the characteristics of periodic discrete congestion events using active probe data gathered in the Surveyor infrastructure. When connections experience loss events which are periodic, the aggregate offered load to neighboring links rises and falls in cadence with the loss events. Connections whose cWnd values grow from W/2 to W at approximately the same rate as the loss event period soon synchronize their cWnd additive increases and multiplicative decreases. We find that this window synchronization can scale to large numbers of connections depending on the diversity of roundtrip times of individual flows. A model is presented that predicts important characteristics of the loss events in window synchronized flows including the quantity, intensity, and duration. The model effectively explains the prevalence of discrete loss events in fast links with high multiplexing factors as well as the queue buildup and queue draining phases of congestion. },
}

@inproceedings{1249051,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {},
 year = {2003},
 publisher = {IEEE},
 title = {2003 IEEE International Workshop on Workload Characterization (IEEE Cat. No.03EX775)},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249051},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249051},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249051.pdf?arnumber=1249051},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { OLTP workloads,  auction sites,  computer networks,  data compression,  decoupled processor architecture,  disk characterization,  electronic commerce,  execution phases,  image recognition,  memory characterization,  multiprocessing systems,  network characterization,  storage management,  stream-based compression,  transaction processing,  transport protocols,  visual feature recognition,  workload characterization, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01249051.png" border="0"> },
}

@inproceedings{1249052,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Mathew, B. and Davis, A. and Evans, R.},
 year = {2003},
 pages = { 3-- 11},
 publisher = {IEEE},
 title = {A characterization of visual feature recognition},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249052},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249052},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249052.pdf?arnumber=1249052},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { algorithm understanding,  automated surveillance systems,  data cache,  embedded processors,  embedded space,  embedded systems,  embedded systems,  face detection,  face recognition,  face recognition codes,  feature extraction,  feature recognition workload,  filters,  flesh toning,  gesture based interfaces,  human computer interaction,  lip tracking,  memory system,  natural human interfaces,  perception tasks,  performance characteristics,  performance evaluation,  robotic vision,  smart cameras,  speech recognition,  stream processors,  ubiquitous computing,  ubiquitous computing,  user interfaces,  visual feature recognition, Automatic speech recognition, Character recognition, Embedded system, Humans, Intelligent robots, Robot sensing systems, Smart cameras, Speech recognition, Surveillance, Ubiquitous computing, },
 abstract = {Natural human interfaces are a key to realizing the dream of ubiquitous computing. This implies that embedded systems must be capable of sophisticated perception tasks. This paper analyzes the nature of a visual feature recognition workload. Visual feature recognition is a key component of a number of important applications, e.g. gesture based interfaces, lip tracking to augment speech recognition, smart cameras, automated surveillance systems, robotic vision, etc. Given the power sensitive nature of the embedded space and the natural conflict between low-power and high-performance implementations, a precise understanding of these algorithms is an important step in developing efficient visual feature recognition applications for the embedded space. In particular, this work analyzes the performance characteristics of flesh toning, face detection and face recognition codes based on well known algorithms. We show that the problem can be decomposed into a pipeline of filters which could lead to efficient implementations as stream processors. With better than 92\% hit rate for a modest 16KB L1 data cache, the algorithms have memory system behavior commensurate with embedded processors. However, our results indicate that their execution requirements strain the performance available on current embedded systems. },
}

@inproceedings{1249053,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Menasce, D.A. and Akula, V.},
 year = {2003},
 pages = { 12-- 20},
 publisher = {IEEE},
 title = {Towards workload characterization of auction sites},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249053},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249053},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249053.pdf?arnumber=1249053},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { Internet,  Web sites,  auction sites,  auction traffic,  auction winner,  business-oriented metrics,  closing time analysis,  electronic commerce,  electronic commerce,  entry price,  entry time,  multiscale analysis,  online auctions,  price variation,  resource management policies,  unique bidder analysis,  workload characterization, Buildings, Business, Computer science, Data analysis, Electronic commerce, Mirrors, Pattern analysis, Pricing, Process design, Resource management, },
 abstract = {The popularity of online auctions is growing with the participation of businesses and individual customers in various forms of auctions to buy and sell goods and services. This form of electronic commerce is expected to grow and become a significant form of exchange of goods and services competing in a global scale with traditional fixed-price commerce. A good understanding of the workload of auction sites should provide insights about their activities and help in the process of designing business-oriented metrics and designing novel resource management policies based on these metrics. This paper provides a workload characterization of auction sites including i) a multi-scale analysis of auction traffic and bid activity within auctions, ii) a closing time analysis in terms of number of bids and price variation within auctions, iii) the characteristics of the auction winner in terms of entry time, entry price, and bidding activity, and iv) unique bidder analysis. },
}

@inproceedings{4362179,
 booktitle = {Workload Characterization, 2007. IISWC 2007. IEEE 10th International Symposium on},
 author = {Murphy, R.},
 year = {2007},
 pages = {35--43},
 publisher = {IEEE},
 title = {On the Effects of Memory Latency and Bandwidth on Supercomputer Application Performance},
 date = {27-29 Sept. 2007},
 doi = {10.1109/IISWC.2007.4362179},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4362179},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4362166/4362167/04362179.pdf?arnumber=4362179},
 isbn = {978-1-4244-1561-8},
 language = {English},
 keywords = {Application software, Bandwidth, Clocks, Computational modeling, Computer architecture, Computer simulation, Delay, Laboratories, Physics computing, Supercomputers, bandwidth, combinatorial applications, computer architecture, floating point arithmetic, floating point oriented computation, integer oriented applications, irregular applications, memory latency, memory system performance, multiprocessing systems, parallel machines, problem decomposition, storage management, supercomputer application performance, supercomputer architecture, },
 abstract = {Since the first vector supercomputers in the mid-1970's, the largest scale applications have traditionally been floating point oriented numerical codes, which can be broadly characterized as the simulation of physics on a computer. Supercomputer architectures have evolved to meet the needs of those applications. Specifically, the computational work of the application tends to be floating point oriented, and the decomposition of the problem two or three dimensional. Today, an emerging class of critical applications may change those assumptions: they are combinatorial in nature, integer oriented, and irregular. The performance of both classes of applications is dominated by the performance of the memory system. This paper compares the memory performance sensitivity of both traditional and emerging HPC applications, and shows that the new codes are significantly more sensitive to memory latency and bandwidth than their traditional counterparts. Additionally, these codes exhibit lower base-line performance, which only exacerbates the problem. As a result, the construction of future supercomputer architectures to support these applications will most likely be different from those used to support traditional codes. Quantitatively understanding the difference between the two workloads will form the basis for future design choices. },
}

@inproceedings{1437393,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {Costa, C. and Ramos, C. and Cunha, I. and Almeida, J.M.},
 year = {2004},
 pages = { 29-- 36},
 publisher = {IEEE},
 title = {GENIUS: a generator of interactive user media sessions},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437393},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437393},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437393.pdf?arnumber=1437393},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 keywords = { GENIUS,  audio-visual systems,  heterogeneity,  hierarchical model,  interactive systems,  interactive user media session generator,  media distribution,  media user behavior,  multimedia systems,  realistic interactive synthetic streaming media,  streaming media workload generator, Access protocols, Algorithm design and analysis, Bandwidth, Character generation, Computer science, Degradation, Multicast protocols, Proposals, Streaming media, },
 abstract = {The generation of realistic interactive synthetic streaming media workloads is of great importance for the effective evaluation of alternative media distribution techniques. This paper fills a gap left by previous studies and proposes a hierarchical model that captures key aspects of media user behavior and workloads, in particular, interactivity and heterogeneity. It also introduces GENIUS, a highly flexible and realistic streaming media workload generator that implements the proposed model and is parameterized with results from an extensive characterization of a rich set of real media workloads. Preliminary experiments show that our generator accurately captures workload aspects of key impact to system performance. },
}

@inproceedings{809371,
 booktitle = {Workload Characterization: Methodology and Case Studies, 1998},
 author = {},
 year = {1999},
 pages = {150--150},
 publisher = {IEEE},
 title = {Author Index},
 date = {1999},
 doi = {10.1109/WWC.1998.809371},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809371},
 pdf_url = {http://ieeexplore.ieee.org/iel5/6564/17538/00809371.pdf?arnumber=809371},
 isbn = {0-7695-0450-7},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00809371.png" border="0"> },
}

@inproceedings{4086135,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Ratanaworabhan, P. and Burtscher, M.},
 year = {2006},
 pages = {71--79},
 publisher = {IEEE},
 title = {Load Instruction Characterization and Acceleration of the BioPerf Programs},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302731},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086135},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086135.pdf?arnumber=4086135},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Acceleration, Application software, BioPerf program, BioPerf suite, Bioinformatics, Biology computing, Computer aided instruction, Delay, Dynamic scheduling, L1 hit latency, Microprocessors, Optimizing compilers, Processor scheduling, benchmark program, bioinformatics application, biology computing, data cache, dynamic load execution, dynamically scheduled execution core, load instruction characterization, resource allocation, source-code transformation, },
 abstract = {The load instructions of some of the bioinformatics applications in the BioPerf suite possess interesting characteristics: only a few static loads cover almost the entire dynamic load execution and they almost always hit in the data cache. Nevertheless, these load instructions represent a major performance bottleneck. They often precede or follow branches that are hard to predict, which makes their L1 hit latency difficult to hide even in dynamically scheduled execution cores. This paper investigates this behavior and suggests simple source-code transformations to improve the performance of these benchmark programs by up to 92\% },
}

@inproceedings{990759,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Serrano, M.J. and Youfeng Wu},
 year = {2001},
 pages = { 184-- 192},
 publisher = {IEEE},
 title = {Memory performance analysis of SPEC2000C for the Intel(R) ItaniumTM processor},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990759},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990759},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990759.pdf?arnumber=990759},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { Intel Itanium processor,  SPEC2000C,  benchmarks,  cache misses,  cache profiling,  data translation performance,  hardware performance counters,  load sites,  memory overhead,  memory performance analysis,  microprocessor chips,  performance evaluation,  storage management, Collaboration, Counting circuits, Delay, Educational institutions, Hardware, Instruments, Operating systems, Performance analysis, Prefetching, Software performance, },
 abstract = {We describe our memory performance analysis of SPEC2000C using the newly released Intel(R) Itanium<sup>TM</sup> processor (IPF). Memory overhead is very significant for SPEC200OC; on the average 39\% cycles are spent in data stalls. Cache misses are significant, but also data translation performance (DTLB) affects many benchmarks. We present a study based on collecting measurements from the hardware performance counters and cache profiling using program instrumentation of loads/stores. We define important loads as the load sites that contribute at least 95\% of the cache misses at all levels. Our measurements show that the number of important loads in a program is relatively small. Our analysis show that important loads are most of the time contained in inner loops, and that the trip counts of these loops is significantly high. We present preliminary results on using stride profiling to reduce cache misses of important loads, bringing an average of 6\% improvement to SPEC2000C. Finally, we present our study of data translation performance and propose design choices. },
}

@inproceedings{5306789,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Murphy, M. and Keutzer, K. and Hong Wang},
 year = {2009},
 pages = {138--147},
 publisher = {IEEE},
 title = {Image feature extraction for mobile processors},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306789},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306789},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306789.pdf?arnumber=5306789},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, Augmented reality, Cameras, Computer architecture, Computer vision, Core 2 Duo, Feature extraction, GMA X3100, Image analysis, Intel Atom, Mobile computing, Portable computers, Runtime, code-generation framework, feature extraction, high-quality cameras, image feature extraction, laptop CPU, low-power CPU, low-power GPU, microprocessor chips, mobile processors, scale invariant feature transform, sparse localized interest point detection, transforms, },
 abstract = {High-quality cameras are a standard feature of mobile platforms, but the computational capabilities of mobile processors limit the applications capable of exploiting them. Emerging mobile application domains, for example mobile augmented reality (MAR), rely heavily on techniques from computer vision, requiring sophisticated analyses of images followed by higher-level processing. An important class of image analyses is the detection of sparse localized interest points. The scale invariant feature transform (SIFT), the most popular such analysis, is computationally representative of many other feature extractors. Using a novel code-generation framework, we demonstrate that a small set of optimizations produce high-performance SIFT implementations for three very different architectures: a laptop CPU (Core 2 Duo), a low-power CPU (Intel Atom), and a low-power GPU (GMA X3100). We improve the runtime of SIFT by more than 5X on our low-power architectures, enabling a low-power mobile device to extract SIFT features up to 63\% as fast as the laptop CPU. },
}

@inproceedings{4086136,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Hoste, K. and Eeckhout, L.},
 year = {2006},
 pages = {83--92},
 publisher = {IEEE},
 title = {Comparing Benchmarks Using Key Microarchitecture-Independent Characteristics},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302732},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086136},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086136.pdf?arnumber=4086136},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {47-dimensional space, 8-dimensional space, Application software, Bioinformatics, Biometrics, Character generation, Counting circuits, Hardware, Microarchitecture, Microprocessors, Performance analysis, Process design, benchmark characterization, benchmark suites comparison, benchmark testing, computer architecture, inherent program characteristics, microarchitecture-dependent characterization, microarchitecture-independent characteristics, microarchitecture-independent characterization, performance evaluation, workload characterization, },
 abstract = {Understanding the behavior of emerging workloads is important for designing next generation microprocessors. For addressing this issue, computer architects and performance analysts build benchmark suites of new application domains and compare the behavioral characteristics of these benchmark suites against well-known benchmark suites. Current practice typically compares workloads based on microarchitecture-dependent characteristics generated from running these workloads on real hardware. There is one pitfall though with comparing benchmarks using microarchitecture-dependent characteristics, namely that completely different inherent program behavior may yield similar microarchitecture-dependent behavior. This paper proposes a methodology for characterizing benchmarks based on microarchitecture-independent characteristics. This methodology minimizes the number of inherent program characteristics that need to be measured by exploiting correlation between program characteristics. In fact, we reduce our 47-dimensional space to an 8-dimensional space without compromising the methodology's ability to compare benchmarks. The important benefits of this methodology are that (i) only a limited number of microarchitecture-independent characteristics need to be measured, and (ii) the resulting workload characterization is easy to interpret. Using this methodology we compare 122 benchmarks from 6 recently proposed benchmark suites. We conclude that some benchmarks in emerging benchmark suites are indeed similar to benchmarks from well-known benchmark suites as suggested through a microarchitecture-dependent characterization. However, other benchmarks are dissimilar based on a microarchitecture-independent characterization although a microarchitecture-dependent characterization suggests the opposite to be true },
}

@inproceedings{4086131,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Contreras, G. and Martonosi, M.},
 year = {2006},
 pages = {29--38},
 publisher = {IEEE},
 title = {Techniques for Real-System Characterization of Java Virtual Machine Energy and Power Behavior},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302727},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086131},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086131.pdf?arnumber=4086131},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Analytical models, Application software, Cooling, Energy consumption, Heat sinks, Java, Java, Java Virtual Machine, Java runtime system, Jikes RVM, Power dissipation, Runtime, Temperature, Virtual machining, class loader, energy consumption, garbage collector, object-oriented programming, performance characterization, power aware computing, power dissipation, real-system characterization, runtime compilation subsystem, software performance evaluation, storage management, virtual machines, },
 abstract = {The Java platform has been adopted in a wide variety of systems ranging from portable embedded devices to high-end commercial servers. As energy, power dissipation, and thermal challenges begin to affect all design spaces, Java Virtual Machines will need to evolve in order to respond to these and other emerging issues. Developing a power-conscious Java runtime system begins with a detailed per-component understanding of the energy, performance and power behavior of the system, as well as each component's impact on overall application execution. This paper presents techniques for characterizing Java power and performance, as well as results from applying these techniques to the Jikes RVM, for some of the most salient Java Virtual Machine components. Components studied include the garbage collector, the class loader, and the runtime compilation subsystem. Real-system measurements with our efficient, low-perturbation infrastructure offer valuable insights that can aid virtual machine designers in improving energy-efficiency. For example, our results show that JVM energy consumption can comprise as much as 60\% of the total energy consumed. In addition, we find that generational garbage collectors offer the best energy-performance for small heap sizes and that this efficiency is challenged by non-generational collectors for large heaps. Overall, given the rising importance of Java systems and of power/thermal challenges, this paper's detailed real-systems examination can lend useful insights for many real-world systems },
}

@inproceedings{4086130,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Roca, J. and Moya, V. and Gonzalez, C. and Solis, C. and Fernandez, A. and Espasa, R.},
 year = {2006},
 pages = {17--26},
 publisher = {IEEE},
 title = {Workload Characterization of 3D Games},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302726},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086130},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086130.pdf?arnumber=4086130},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {3D games, ALU instructions, API call level, Attila simulator, Computer aided instruction, Computer architecture, Functional programming, GPU, Geometry, Graphics, Information filtering, Microarchitecture, Pipelines, Rendering (computer graphics), Solid modeling, computational geometry, computer games, computer graphic equipment, computer graphics, geometry metrics, microarchitectural level metrics, performance analysis, texture cache performance, workload characterization, },
 abstract = {The rapid pace of change in 3D game technology makes workload characterization necessary for every game generation. Comparing to CPU characterization, far less quantitative information about games is available. This paper focuses on analyzing a set of modern 3D games at the API call level and at the micro architectural level using the Attila simulator. In addition to common geometry metrics and, in order to understand tradeoffs in modern GPUs, the microarchitectural level metrics allow us to analyze performance key characteristics such as the balance between texture and ALU instructions in fragment programs, dynamic anisotropic ratios, vertex, z-stencil, color and texture cache performance },
}

@inproceedings{4086133,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Sanchez, F. and Salami, E. and Ramirez, A. and Valero, M.},
 year = {2006},
 pages = {51--60},
 publisher = {IEEE},
 title = {Performance Analysis of Sequence Alignment Applications},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302729},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086133},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086133.pdf?arnumber=4086133},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Altivec SIMD extension, Application software, BLAST, Bioinformatics, Biology, Chemistry, Computer architecture, Computer science, FASTA, Mathematics, Performance analysis, Performance loss, SSEARCH34, Sequences, Smith-Waterman algorithm, bioinformatic applications, biology computing, branch predictor, computer architecture, computer architecture, configuration management, memory subsystem, microarchitectural design, molecular biology, molecular biophysics, parallel algorithms, performance analysis, pipeline configurations, sequence alignment applications, simulation-based methodology, storage management, workload characterization, },
 abstract = {Advances in molecular biology have led to a continued growth in the biological information generated by the scientific community. Additionally, this area has become a multi-disciplinary field, including components of mathematics, biology, chemistry, and computer science, generating several challenges in the scientific community from different points of view. For this reason, bioinformatic applications represent an increasingly important workload. However, even though the importance of this field is clear, common bioinformatic applications and their implications on micro-architectural design have not received enough attention from the computer architecture community. This paper presents a micro-architecture performance analysis of recognized bioinformatic applications for the comparison and alignment of biological sequences, including BLAST, FASTA and some recognized parallel implementations of the Smith-Waterman algorithm that use the Altivec SIMD extension to speed-up the performance. We adopt a simulation-based methodology to perform a detailed workload characterization. We analyze architectural and micro-architectural aspects like pipeline configurations, issue widths, functional unit mixes, memory hierarchy and their implications on the performance behavior. We have found that the memory subsystem is the component with more impact in the performance of the BLAST heuristic, the branch predictor is responsible for the major performance loss for FASTA and SSEARCH34, and long dependency chains are the limiting factor in the SIMD implementations of Smith-Waterman },
}

@inproceedings{4086132,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Rajamani, K. and Hanson, H. and Rubio, J. and Ghiasi, S. and Rawson, F.},
 year = {2006},
 pages = {39--48},
 publisher = {IEEE},
 title = {Application-Aware Power Management},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302728},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086132},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086132.pdf?arnumber=4086132},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Battery management systems, Circuits, Dynamic voltage scaling, Energy consumption, Energy management, Environmental management, Frequency, Monitoring, Pentium M platform, PerformanceMaximizer, Power system management, PowerSave, SPEC-CPU2000 suite, Thermal management, application-aware power management, critical workload indicators, power aware computing, workload characterization, },
 abstract = {This paper presents our approach for application-aware power management. We combine continuous monitoring of critical workload indicators, online power and performance model usage and timely p-state control to realize application-aware power management. We present two new power management solutions enabled by our methodology: PerformanceMaximizer (PM) finds the best possible performance under specified power constraints and PowerSave (PS) saves energy while keeping performance above specified requirements. We evaluate both using the SPEC-CPU2000 suite on a Pentium M platform discussing implications of workload characteristics and benefits of being workload-aware },
}

@inproceedings{990750,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Gannamaraju, R. and Chandra, S.},
 year = {2001},
 pages = { 111-- 119},
 publisher = {IEEE},
 title = {Palmist: a tool to log Palm system activity},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990750},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990750},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990750.pdf?arnumber=990750},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { PDA,  Palm OS 3.5 system calls,  Palm system call logging tool,  Palmist,  call arguments,  notebook computers,  performance evaluation,  system call, Application software, Batteries, Computer science, Delay, Energy consumption, Instruments, Operating systems, Personal digital assistants, Statistics, Usability, },
 abstract = {In this paper we describe a Palm system call logging tool called Palmist. Palmist allows the practitioner to selectively collect statistics such as the system call invoked, application that invoked the system call, the time of the call and the call arguments. The logging mechanism adds a latency of about 10 msec per call to collect the log. On an average, the system uses about 20 bytes of memory on the PDA to store the log record. The mechanism has limitations in collecting logs for system calls that are needed by the collection mechanism itself. Our logging mechanism works for about 80\% (707 of 881) of Palm OS 3.5 system calls. Our system can be utilized by system developers to customize their application behavior to optimize system parameters such as energy consumption, ease of use etc. },
}

@inproceedings{990751,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Ahola, J.},
 year = {2001},
 pages = { 120-- 126},
 publisher = {IEEE},
 title = {Compressing address traces with RECET},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990751},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990751},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990751.pdf?arnumber=990751},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { address traces compression,  cache configurations,  cache storage,  discrete event simulation,  lossless compression,  real-time cache evaluation tool,  storage management,  storage space, Cache memory, Cache storage, Computational modeling, Computer architecture, Computer simulation, Hardware, Laboratories, Operating systems, Software systems, Space technology, },
 abstract = {Storing very long address traces for the simulation of cache configurations requires vast amounts of storage space. This storage space requirement can be lowered considerably using lossless compression of the original address trace. In this paper a new real-time address trace compression method is presented based on the RECET (Real-time Cache Evaluation Tool) platform. With the RECET address trace compression method a compression ratio is achieved that is almost 70 per cent higher than that of earlier published methods. },
}

@inproceedings{990752,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Sorenson, E.S. and Flanagan, J.K.},
 year = {2001},
 pages = { 129-- 139},
 publisher = {IEEE},
 title = {Cache characterization surfaces and predicting workload miss rates},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990752},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990752},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990752.pdf?arnumber=990752},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { cache characterization surfaces,  cache miss rates,  cache storage,  storage management,  stride relationships,  workload miss rates prediction, Computer science, Delay effects, Displays, Information filtering, Information filters, },
 abstract = {In this paper, we use locality surfaces to predict cache miss rates. To do this, we introduce two new surfaces. The miss surface characterizes how a trace is filtered by a particular cache in terms of locality. A cache characterization surface helps us examine caches in terms of what stride/delay relationships are likely to cause misses in the cache. The cache characterization surface is independent of any workload. We use these surfaces to quantitatively predict cache miss rates with some degree of accuracy. },
}

@inproceedings{990753,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Cirne, W. and Berman, F.},
 year = {2001},
 pages = { 140-- 148},
 publisher = {IEEE},
 title = {A comprehensive model of the supercomputer workload},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990753},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990753},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990753.pdf?arnumber=990753},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { arrival instant,  comprehensive model,  job cancellation,  parallel processing,  partition size,  performance evaluation,  performance evaluation,  supercomputer workload, Computational modeling, Computer science, Concurrent computing, Displays, Job design, NASA, Processor scheduling, Supercomputers, US Department of Energy, Upper bound, },
 abstract = {As with any computer system, the performance of supercomputers depends upon the workloads that serve as their input. Unfortunately, however, there are many important aspects of the supercomputer workloads that have not been modeled, or that have been modeled only incipiently. This paper attacks this problem by considering requested time (and its relation with execution time) and the possibility of job cancellation, two aspects of the supercomputer workload that have not been modeled yet. Moreover, we also improve upon existing models for the arrival instant and partition size. },
}

@inproceedings{990754,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Snavely, A. and Wolter, N. and Carrington, L.},
 year = {2001},
 pages = { 149-- 156},
 publisher = {IEEE},
 title = {Modeling application performance by convolving machine signatures with application profiles},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990754},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990754},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990754.pdf?arnumber=990754},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { High Performance Computing benchmarks,  performance estimation,  performance evaluation,  performance modeling, Application software, Computational fluid dynamics, Computational modeling, Computer applications, Concurrent computing, High performance computing, Kernel, Predictive models, Supercomputers, Testing, },
 abstract = {This paper presents a performance modeling methodology that is faster than traditional cycle-accurate simulation, more sophisticated than performance estimation based on system peak-performance metrics, and is shown to be effective on a class of High Performance Computing benchmarks. The method yields insight into the factors that affect performance on single-processor and parallel computers. },
}

@inproceedings{990755,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Bestavros, A. and Mehrotra, S.},
 year = {2001},
 pages = { 159-- 168},
 publisher = {IEEE},
 title = {DNS-based Internet client clustering and characterization},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990755},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990755},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990755.pdf?arnumber=990755},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { Internet,  Internet domain name system,  Internet host names,  Internet locations,  L-DNS cluster,  Web clients,  Web server,  content distribution network,  disjoint sets,  file servers,  naming services,  protocol,  protocols, Availability, Clustering algorithms, Computer science, Domain Name System, Instruments, Internet, Network servers, Performance evaluation, Protocols, Web server, },
 abstract = {This paper proposes a novel protocol which uses the Internet domain name system (DNS) to partition Web clients into disjoint sets, each of which is associated with a single DNS server. We define an L-DNS cluster to be a grouping of Web clients that use the same Local DNS server to resolve Internet host names. We identify such clusters in real-time using data obtained from a Web Server in conjunction with that server's authoritative DNS-both instrumented with an implementation of our clustering algorithm. Using these clusters, we perform measurements from four distinct Internet locations. Our results show that L-DNS clustering enables a better estimation of proximity of a Web client to a Web server than previously proposed techniques. Thus, in a content distribution network, a DNS-based scheme that redirects a request from a web client to one of many servers based on the client's name server coordinates (e.g., hops/latency/loss-rates) would perform better with our algorithm. },
}

@inproceedings{990756,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Shi, W. and MacGregor, M.H. and Gburzynski, P.},
 year = {2001},
 pages = { 169-- 174},
 publisher = {IEEE},
 title = {Synthetic trace generation for the Internet},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990756},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990756},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990756.pdf?arnumber=990756},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { IP packets,  Internet,  Internet,  Internet router,  LRU stack model,  aggregate LP traffic,  caching protocols,  destination addresses,  memory protocols,  power law function,  routing,  routing table lookups,  spatial locality,  synthetic IP traffic,  synthetic memory reference strings,  synthetic trace generation, Acceleration, Aggregates, Character generation, Computer aided instruction, Costs, Distributed computing, Hardware, Internet, Routing protocols, Table lookup, },
 abstract = {We consider the distribution of destination addresses in IP packets arriving at an Internet router and show that the spatial locality of those addresses is well characterized by an empirical power law function. We demonstrate how the LRU stack model implied by this function can be used to generate synthetic IP traffic, e.g., for experimental studies of routing and caching protocols. We also show how this model (which was originally devised to generate synthetic memory reference strings of programs) can be modified to better capture the temporal locality of destination addresses in aggregate IP traffic. Our observations are illustrated by comparing the footprints of real and synthetic traces, and by simulating routing table lookups driven by both types of traces. },
}

@inproceedings{990757,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Kingsum Chow and Bhat, M. and Jha, A. and Cunningham, C.},
 year = {2001},
 pages = { 175-- 181},
 publisher = {IEEE},
 title = {Characterization of JavaTM application server workloads},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990757},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990757},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990757.pdf?arnumber=990757},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { Java,  Java 2 Enterprise Edition,  Java application server workloads,  application program interfaces,  computer architects,  e-commerce transactions,  performance analysis,  performance characteristics,  performance simulation,  program behavior,  software performance evaluation,  system observable behavior,  workload characterization, Business, Counting circuits, Ethernet networks, Hardware, Interpolation, Java, Measurement, Operating systems, Switches, Transaction databases, },
 abstract = {This paper examines the workload characterization of a Java 2 Enterprise Edition (J2EE<sup>TM</sup>) application server workload. The application provides services for a mixture of e-commerce transactions continuously. This paper examines the variation of system observable behavior, program behavior and performance characteristics at the CPU level and their impact as a result of the variation of the payload of those different kinds of e-commerce transactions. Early results indicate that some methods could be useful in establishing the relationship between observable system configurations and program behavior. Furthermore, performance at the CPU level may be linked to these configurations. It may also help computer architects in collecting traces of workload that can cover a wide spectrum for performance simulation and analysis. },
}

@inproceedings{1526001,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Eeckhout, L. and Sundareswara, R. and Yi, J.J. and Lilja, D.J. and Schrater, P.},
 year = {2005},
 pages = { 56-- 66},
 publisher = {IEEE},
 title = {Accurate statistical approaches for generating representative workload compositions},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526001},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526001},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526001.pdf?arnumber=1526001},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { benchmark testing,  independent component analysis,  independent components analysis,  microprocessor design,  representative workload composition,  workload representativeness, Cities and towns, Data analysis, Gaussian distribution, Independent component analysis, Microarchitecture, Microprocessors, Principal component analysis, Process design, Standardization, },
 abstract = {Composing a representative workload is a crucial step during the design process of a microprocessor. The workload should be composed in such a way that it is representative for the target domain of application and yet, the amount of redundancy in the workload should be minimized as much as possible in order not to overly increase the total simulation time. As a result, there is an important trade-off that needs to be made between workload representativeness and simulation accuracy versus simulation speed. Previous work used statistical data analysis techniques to identify representative benchmarks and corresponding inputs, also called a subset, from a large set of potential benchmarks and inputs. These methodologies measure a number of program characteristics on which principal components analysis (PCA) is applied before identifying distinct program behaviors among the benchmarks using cluster analysis. In this paper we propose independent components analysis (ICA) as a better alternative to PCA as it does not assume that the original data set has a Gaussian distribution, which allows ICA to better find the important axes in the workload space. Our experimental results using SPEC CPU2000 benchmarks show that ICA significantly outperforms PCA in that ICA achieves smaller benchmark subsets that are more accurate than those found by PCA. },
}

@inproceedings{4086151,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Alarm, S.R. and Barrett, R.F. and Kuehn, J.A. and Roth, P.C. and Vetter, J.S.},
 year = {2006},
 pages = {225--236},
 publisher = {IEEE},
 title = {Characterization of Scientific Workloads on Systems with Multi-Core Processors},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302747},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086151},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086151.pdf?arnumber=4086151},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {AMD Opteron, AMD Opteron Dual-Core processor systems, Application software, Bandwidth, Concurrent computing, Government, Kernel, MPI task, Memory management, Multi-core processor, Multicore processing, Parallel processing, Performance characterization, Scientific computing, Sockets, all next-generation HPC systems, memory placement management, message passing, micro-benchmarking, multicore processors, multicore systems, multiprocessing systems, processor affinity, scientific applications, scientific workloads, storage management, },
 abstract = {Multi-core processors are planned for virtually all next-generation HPC systems. In a preliminary evaluation of AMD Opteron Dual-Core processor systems, we investigated the scaling behavior of a set of micro-benchmarks, kernels, and applications. In addition, we evaluated a number of processor affinity techniques for managing memory placement on these multi-core systems. We discovered that an appropriate selection of MPI task and memory placement schemes can result in over 25\% performance improvement for key scientific calculations. We collected detailed performance data for several large-scale scientific applications. Analyses of the application performance results confirmed our micro-benchmark and scaling results },
}

@inproceedings{4086150,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Brevik, J. and Nurmi, D. and Wolski, R.},
 year = {2006},
 pages = {213--224},
 publisher = {IEEE},
 title = {Predicting Bounds on Queuing Delay in Space-shared Computing Environments},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302746},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086150},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086150.pdf?arnumber=4086150},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Delay, Environmental management, High performance computing, History, Job production systems, Processor scheduling, Resource management, Supercomputers, Throughput, Time sharing computer systems, high performance computing, job scheduling, processor scheduling, queueing theory, queuing delay, scheduler logs, space-shared computing environment, },
 abstract = {Most space-sharing resources presently operated by high performance computing centers employ some sort of batch queueing system to manage resource allocation to multiple users. In this work, we explore a new method for providing end-users with predictions of the bounds on queuing delay individual jobs will experience when waiting to be scheduled to a machine partition. We evaluate this method using scheduler logs that cover a 10 year period from 10 large HPC systems. Our results show that it is possible to predict delay bounds with specified confidence levels for jobs in different queues, and for jobs requesting different ranges of processor counts },
}

@inproceedings{990740,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Todi, R.},
 year = {2001},
 pages = { 15-- 23},
 publisher = {IEEE},
 title = {SPEClite: using representative samples to reduce SPEC CPU2000 workload},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990740},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990740},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990740.pdf?arnumber=990740},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { Vortex benchmark,  design evaluation,  microarchitecture,  microprocessor chips,  microprocessor simulator,  performance evaluation,  representative samples, Costs, Hardware, Microarchitecture, Microprocessors, Pipelines, Predictive models, Runtime, Sampling methods, Statistics, Very large scale integration, },
 abstract = {An execution-driven microarchitecture-accurate microprocessor simulator requires a complex software program. The simulator must be highly detailed and accurate if it is used for microarchitecture design evaluation. The detail and accuracy comes at the high cost of enormous simulation time. A simulator that models a modern super-scalar processor is 10<sup>5</sup> to 10<sup>6</sup> times slower than the actual hardware being modeled. Running a benchmark in full microarchitecture mode (UA) can be execution time prohibitive. Hence, simulators are less effective than they could be due to slowness in the simulated result. This paper presents a methodology of selecting and executing representative samples that reduce the simulation time and maintains the accuracy of the simulated result. We illustrate our methodology using the Vortex benchmark from the SPEC CPU2000 suite (SPEC2K). },
}

@inproceedings{1226488,
 booktitle = {Workload Characterization, 2002. WWC-5. 2002 IEEE International Workshop on},
 author = {},
 year = {2002},
 publisher = {IEEE},
 title = {2002 IEEE International Workshop on Workload Characterization. WWC-5 (Cat. No.02EX633)},
 date = {25 Nov. 2002},
 doi = {10.1109/WWC.2002.1226488},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1226488},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8689/27524/01226488.pdf?arnumber=1226488},
 isbn = {0-7803-7681-1},
 language = {English},
 keywords = { Java,  Java,  Web site benchmarks,  Web sites,  commercial platforms,  commercial workloads,  computer networks,  handheld environments,  multimedia,  multimedia computing,  network environments,  notebook computers,  synthetic trace models,  workload characterization, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01226488.png" border="0"> },
}

@inproceedings{1437401,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 72-- 72},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437401},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437401},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437401.pdf?arnumber=1437401},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{1249061,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Milenkovic, A. and Milenkovic, M.},
 year = {2003},
 pages = { 99-- 107},
 publisher = {IEEE},
 title = {Exploiting streams in instruction and data address trace compression},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249061},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249061},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249061.pdf?arnumber=1249061},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { SBC algorithm,  SPEC CPU2000 traces,  Sequitur,  compression algorithms,  computer architecture,  computer architecture,  data address trace compression,  data compression,  digital simulation,  gzip,  instruction sets,  instruction streams,  performance evaluation,  trace reduction,  trace-driven simulation,  trace-specific solutions, Clocks, Compression algorithms, Computational modeling, Computer architecture, Computer simulation, Information analysis, Instruments, Modems, Predictive models, Redundancy, },
 abstract = {Novel research ideas in computer architecture are frequently evaluated using trace-driven simulation. The large size of traces incited different techniques for trace reduction. These techniques often combine standard compression algorithms with trace-specific solutions, taking into account the tradeoff between reduction in the trace size and simulation slowdown due to compression. This paper introduces SBC, a new algorithm for instruction and data address trace compression based on instruction streams. The proposed technique significantly reduces trace size and simulation time, and can be successfully combined with general compression algorithms. The SBC technique combined with gzip reduces the size of SPEC CPU2000 traces 59-97930 times, and combined with Sequitur 65-185599 times. },
}

@inproceedings{1249060,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Chalainanont, N. and Nurvitadhi, E. and Morrison, R. and Lixin Su and Kingsum Chow and Shih-Lien Lu and Lai, K.},
 year = {2003},
 pages = { 86-- 95},
 publisher = {IEEE},
 title = {Real-time L3 cache simulations using the Programmable Hardware-Assisted Cache Emulator (PHA$E)},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249060},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249060},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249060.pdf?arnumber=1249060},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { CPU,  PHA$E,  Programmable Hardware-Assisted Cache Emulator,  SPECcpu2000,  SPECjAppServer2002,  SPECjbb2000,  cache storage,  computer memory,  digital simulation,  memory architecture,  memory-hierarchy designs,  performance evaluation,  real-time L3 cache simulations,  real-time systems,  speech recognition,  speech recognition system,  trace-driven simulation, Analytical models, Costs, Emulation, Hardware, Real time systems, Runtime, Space exploration, Speech analysis, Speech recognition, Vocabulary, },
 abstract = {As the gap between the CPU and memory speeds increases, there has been an increasingly important need to study the memory-hierarchy designs. Investigations of memory performance have typically been conducted using trace-driven simulation, which could take tremendous resources (e.g. long simulation time, large storage requirements for traces, and high overall cost). Recent works have proposed the use of hardware for performing cache simulations. Such approach is advantageous as it can be done in real-time, which eliminates the need or large storage for traces, reduces the simulation time, and improves the accuracy of the results. This paper discusses our preliminary work with theProgrammable Hardware-Assisted Cache Emulator (PHA\$E), a system for emulating cache in real-time. We discuss the design and implementation of our system. Furthermore, the results of simulating varying sizes of off-chip L3 caches on various workloads (SPECcpu2000, SPECjbb2000, SPECjAppServer2002, and a large vocabulary continuous speech recognition system are presented and analyzed. Lastly, future research directions are elaborated on. },
}

@inproceedings{1249063,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Djabelkhir, A. and Seznec, A.},
 year = {2003},
 pages = { 119-- 127},
 publisher = {IEEE},
 title = {Characterization of embedded applications for decoupled processor architecture},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249063},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249063},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249063.pdf?arnumber=1249063},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { LOD events,  MiBench,  benchmark testing,  code regularity,  computer architecture,  control dependency,  control regularity,  decoupled architecture,  decoupled processor architecture,  decoupling loss,  dynamic execution,  embedded applications,  embedded systems,  interprocessor dependencies,  memory data referencing,  memory dependency,  out-of-order superscalar cores,  performance evaluation,  power dissipation,  program processors,  silicon area,  workload characterization, Application software, Automobiles, Computer architecture, Costs, Delay, Kernel, Parallel programming, Processor scheduling, Telephony, Vehicle dynamics, },
 abstract = {Needs for performance on embedded applications leads to the use of dynamic execution on embedded processors in the next few years. However, complete out-of-order superscalar cores are still expensive in terms of silicon area and power dissipation. In this paper, we study the adequacy of a more limited form of dynamic execution, namely decoupled architecture, to embedded applications. Decoupled architecture is known to work very efficiently whenever the execution does not suffer from inter-processor dependencies causing some loss of decoupling, called LOD events. In this study, we address regularity of codes in terms of the LOD events that may occur. We address three aspects of regularity: control regularity, control/memory dependency, and patterns of referencing memory data. Most of the kernels in MiBench will be amenable to efficient performance on a decoupled architecture. },
}

@inproceedings{1249062,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {Isci, C. and Martonosi, M.},
 year = {2003},
 pages = { 108-- 118},
 publisher = {IEEE},
 title = {Identifying program power phase behavior using power vectors},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249062},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249062},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249062.pdf?arnumber=1249062},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 keywords = { benchmark testing,  clock gating,  digital simulation,  execution phases,  gzip benchmark,  hardware research,  microprocessor chips,  modern processors,  performance evaluation,  power consumption,  power dissipation,  power management,  power signatures,  power similarity metric,  power vectors,  power-oriented research,  processor components,  program behavior,  program phases,  program power phase behavior,  program processors,  program testing,  simulation times,  software research,  thermal management,  thresholding algorithm, Analytical models, Clocks, Computer architecture, Energy management, Hardware, Large-scale systems, Power dissipation, Power generation, Runtime, Thermal management, },
 abstract = {Characterizing program behavior is important for both hardware and software research. Most modern applications exhibit distinctly different behavior throughout their runtimes, which constitute several phases of execution that share a greater amount of resemblance within themselves compared to other regions of execution. These execution phases can occur at very large scales, necessitating prohibitively long simulation times for characterization. Due to the implementation of extensive clock gating and additional power and thermal management techniques in modern processors, these program phases are also reflected in program power behavior, which can be used as an alternative means of program behavior characterization for power-oriented research. In this paper, we present our methodology for identifying phases in program power behavior and determining execution points that correspond to these phases, as well as defining a small set of power signatures representative of overall program power behavior. We define a power similarity metric as an intersection of both magnitude based and ratio-wise similarities in the power dissipation of processor components. We then develop a thresholding algorithm in order to partition the power behavior into similarity groups. We illustrate our methodology with the gzip benchmark for its whole runtime and characterize gzip power behavior with both the selected execution points and defined signature vectors. },
}

@inproceedings{1249064,
 booktitle = {Workload Characterization, 2003. WWC-6. 2003 IEEE International Workshop on},
 author = {},
 year = {2003},
 pages = { 129-- 130},
 publisher = {IEEE},
 title = {Author Index},
 date = {27 Oct. 2003},
 doi = {10.1109/WWC.2003.1249064},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1249064},
 pdf_url = {http://ieeexplore.ieee.org/iel5/8833/27961/01249064.pdf?arnumber=1249064},
 issn = {           },
 isbn = {0-7803-8229-3},
 language = {English},
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01249064.png" border="0"> },
}

@inproceedings{1526000,
 booktitle = {Workload Characterization Symposium, 2005. Proceedings of the IEEE International},
 author = {Xiaofeng Gao and Laurenzano, M. and Simon, B. and Snavely, A.},
 year = {2005},
 pages = { 46-- 55},
 publisher = {IEEE},
 title = {Reducing overheads for acquiring dynamic memory traces},
 date = {6-8 Oct. 2005},
 doi = {10.1109/IISWC.2005.1526000},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1526000},
 pdf_url = {http://ieeexplore.ieee.org/iel5/10230/32621/01526000.pdf?arnumber=1526000},
 issn = {           },
 isbn = {0-7803-9461-5},
 language = {English},
 keywords = { MetaSim tracer,  SimPoint-guided sampling,  dynamic memory trace acquisition,  instrumentation tool routine optimization,  low-level binary instrumentation tracing,  program diagnostics,  static application analysis,  storage management,  subroutines, Atomic measurements, Collaborative work, Computational modeling, Costs, High performance computing, Instruments, Large-scale systems, Performance analysis, Sampling methods, Supercomputers, },
 abstract = {Tools for acquiring dynamic memory address information for large scale applications are important for performance modeling, optimization, and for trace-driven simulation. However, straightforward use of binary instrumentation tools for such a fine-grained task as address tracing can cause astonishing slowdown in application run time. For example, in a large scale FY05 collaboration with the Department of Defense High Performance Computing Modernization Office (HPCMO), over 1 million processor hours were expended in order to gather address information on 7 parallel applications. In this paper, we discuss in detail the issues surrounding the performance of memory address acquisition using low-level binary instrumentation tracing. We present three techniques and optimizations to improve performance: 1) SimPoint-guided sampling, 2) instrumentation tool routine optimization, and 3) reduction of instrumentation points through static application analysis. The use of these three techniques together reduces instrumented application slowdown by an order of magnitude. The techniques are generally applicable and have been deployed in the MetaSim tracer thereby enabling memory address acquisition for real-sized applications. We expect the optimizations reported here reduce the HPCMO effort by approximately 80\% in FY06. },
}

@inproceedings{990747,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Cook, J. and Oliver, R.L. and Johnson, E.E.},
 year = {2001},
 pages = { 82-- 90},
 publisher = {IEEE},
 title = {Examining performance differences in workload execution phases},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990747},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990747},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990747.pdf?arnumber=990747},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { SPEC95 benchmark suite,  behavioral characteristics,  computer architecture,  computer architectures,  performance behavior,  performance differences,  performance evaluation,  resource allocation,  simulation-based performance analysis,  workload execution phases, Analytical models, Character generation, Computational modeling, Computer architecture, Computer simulation, Measurement, Microarchitecture, Performance analysis, Sampling methods, Testing, },
 abstract = {Workload characterization is vital to the design and performance analysis of new generation computer architectures. In many simulation-based performance analysis studies, only a small "representative" portion of the total workload execution is used for analysis. This is due to the prohibitive amount of time it takes to simulate or execute a workload to completion. Methods of choosing the portion of the workload for use vary from sampling to simply simulating an arbitrary number of instructions after initialization. The primary challenge in utilizing only a portion of a workload in a performance analysis study is ensuring that it accurately represents the whole. A significant amount of research has shown that often, the "representativeness" of a selected portion of a workload is quite low. This work identifies the execution phases and some of their behavioral characteristics in a subset of the SPEC95 benchmark suite. We quantitatively and qualitatively show that after several billion executed instructions, many commonly used workloads exhibit phases of execution characterized by previously unseen behavior. Additionally, these phases are characterized by very different performance behavior. },
}

@inproceedings{990746,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Yi, J.J. and Lija, D.J.},
 year = {2001},
 pages = { 74-- 81},
 publisher = {IEEE},
 title = {An analysis of the amount of global level redundant computation in the SPEC 95 and SPEC 2000 benchmarks},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990746},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990746},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990746.pdf?arnumber=990746},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { SPEC 2000,  SPEC 95,  benchmark suites,  benchmarks,  global level,  local level,  performance evaluation,  redundancy,  redundant computation,  redundant computations,  value reuse, Cities and towns, Computer aided instruction, Counting circuits, Delay, Hardware, History, Pipelines, },
 abstract = {This paper analyzes the amount of global level redundant computation within selected benchmarks of the SPEC 95 and SPEC 2000 benchmark suites. Local level redundant computations are redundant computations that are the result of a single static instruction (i.e. PC dependent) while global level redundant computations are redundant computations that are the result of multiple static instructions (i.e. PC independent). The results show that for all benchmarks more than 90\% of the unique computations account for only 1.2\% to 31.5\% of the total number of instructions. In fact, less than 1000 (0.14\%) of the most frequently occurring unique computations accounted for 19.4\%-95.5\% of the dynamic instructions. Furthermore, more redundant computation exists at the global level as compared to the traditional local level. For an equal number of unique computations-approximately 100 for each benchmark at both the global and local levels, the global level unique computations accounted for an additional 1.5\% to 12.6\% of the total number of dynamic instructions as compared to the local level unique computations. As a result, exploiting redundant computations through value reuse at the global level should yield a significant performance improvement as compared to exploiting redundant computations only at the local level. 7. },
}

@inproceedings{990745,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Thomas, R. and Franklin, M.},
 year = {2001},
 pages = { 65-- 73},
 publisher = {IEEE},
 title = {Characterization of data value unpredictability to improve predictability},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990745},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990745},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990745.pdf?arnumber=990745},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { data flow computing,  data value prediction,  dataflow,  parallel architectures,  parallelism,  performance evaluation,  predictability,  prediction accuracy, Accuracy, Educational institutions, Fluctuations, Parallel processing, Performance analysis, Predictive models, },
 abstract = {Recent research has shown that it is possible to overcome the parallelism limits imposed by dataflow by predicting instruction results based on previously produced values or a sequence thereof. Unlike branch prediction schemes where prediction accuracies of 90\% and above are the norms, data value prediction schemes have been able to correctly predict only about 40-70\% of the result-producing instructions. In order to further improve the performance of data value predictors, it is very important to do a thorough analysis of the instructions that are currently unpredictable. In this paper, we study the characteristics of unpredictability of data values. The major insights obtained from this stud, are: (i) An instruction often becomes unpredictable due to multiple causes. This means that tackling a single cause in isolation will not provide substantial improvements in prediction accuracy. (ii) Among the different causes, control flow fluctuations are a major cause. (iii) Load instructions are a significant component of unpredictable instructions. Future data value predictors need to tackle these causes, and provide "better contexts" so as to convert current unpredictability into predictability. },
}

@inproceedings{990744,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Bui, T.H.},
 year = {2001},
 pages = { 54-- 62},
 publisher = {IEEE},
 title = {Performance characteristics of ItaniumTM processor on data encryption algorithms},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990744},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990744},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990744.pdf?arnumber=990744},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { cryptography,  data encryption,  multi-precision arithmetic,  performance characteristics,  performance evaluation,  public key encryption,  symmetric key encryption, Arithmetic, Cryptographic protocols, Elliptic curve cryptography, Internet, ItaniumTM processor, Parallel processing, Public key, Public key cryptography, Registers, Relays, Throughput, },
 abstract = {In this paper, performance characteristics of the Itanium<sup>TM</sup> processor, the first implementation of the Itanium<sup>TM</sup> Processor Family (IPF), is examined for the task of data encryption using public key and symmetric key encryption algorithms. The Itanium<sup>TM</sup> processor key performance characteristics: wide issue width, large number of functional units, and large number of registers are examined for the task of performing multi-precision arithmetic dominant in public key algorithms and the computing requirements of symmetric key encryption algorithms. },
}

@inproceedings{990743,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Agaram, K. and Keckler, S.W. and Burger, D.},
 year = {2001},
 pages = { 45-- 53},
 publisher = {IEEE},
 title = {A characterization of speech recognition on modern computer systems},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990743},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990743},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990743.pdf?arnumber=990743},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { RASTA,  SPHINX,  benchmark applications,  instruction-level parallelism,  memory systems,  microcomputers,  microprocessors,  parallelization,  performance evaluation,  performance improvements,  speech recognition,  speech recognition, Application software, Computer architecture, Laboratories, Microprocessors, Parallel processing, Signal processing, Speech recognition, Streaming media, Throughput, Vocabulary, },
 abstract = {In this paper we describe and characterize the speech recognition process, and assess the suitability of current microprocessors and memory systems for running speech recognition applications. We use representative benchmark applications-RASTA to characterize the signal-processing on the front end, and SPHINX for the graph search on the back end Recognition time is dominated by the back end, which substantially exercises the memory system and exhibits low levels of instruction-level parallelism (ILP). As a result, SPHINX yields an average instructions per cycle (IPC) of 0.64 on a simulated 4-issue out-of-order microprocessor We identify intelligent layout and thread-level parallelization as the primary methods to improve throughput, showing tipper bounds on the performance improvements that these methods can achieve. },
}

@inproceedings{990742,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Seshadri, P. and Mericas, A.},
 year = {2001},
 pages = { 36-- 44},
 publisher = {IEEE},
 title = {Workload characterization of multithreaded Java servers on two PowerPC processors},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990742},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990742},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990742.pdf?arnumber=990742},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { IBM PowerPC architectures,  Java,  Java,  Java server,  SPEQbb2000,  VolanoMark 2.1.2,  characterize,  multi-threading,  multithreaded Java server benchmarks,  parallel architectures,  performance evaluation, Application software, Decision support systems, Interference, Java, Kernel, Microarchitecture, Multithreading, Programming profession, Web server, Yarn, },
 abstract = {Java has, in recent years, become fairly popular as a platform for commercial servers. However, the behavior of Java server applications has not been studied extensively. We characterize two multithreaded Java server benchmarks, SPECjbb2000 and VolanoMark 2.1.2, on two IBM PowerPC architectures, the RS64-111 and the POWER3-11, and compare them to more traditional workloads as represented by selected benchmarks from SPECint2000. We find that our Java server benchmarks have generally the same characteristics on both platforms: in particular, high instruction cache, ITLB, and BTAC (Branch Target Address Cache) miss rates. These benchmarks also exhibit high L2 miss rates due mostly to loads. As one would expect, instruction cache and L2 misses are primary contributors to CPI. Also, the proportion of zero dispatch cycles is high, indicating the difficulty in exploiting ILP for these workloads. },
}

@inproceedings{990741,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Clark, M. and Durg, A. and Lienenbrugger, K.},
 year = {2001},
 pages = { 26-- 35},
 publisher = {IEEE},
 title = {Characterization of TPC-H queries on AMD AthlonTM microprocessors},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990741},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990741},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990741.pdf?arnumber=990741},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { Athlon,  Decision Support System,  Internet based transactions,  Servers,  TPC-H based workloads,  TPC-H benchmark,  business data processing,  business oriented ad-hoc queries,  cache implementations,  characterization,  concurrent data modifications,  decision support benchmark,  decision support systems,  microcomputers,  performance evaluation,  performance-monitoring counters,  system level, Benchmark testing, Business, Councils, Counting circuits, Decision support systems, Internet, Microprocessors, Process design, Transaction databases, Web server, },
 abstract = {Studies on TPC-H based workloads have been few. The ever increasing, Internet based transactions necessitate better design of Servers running database applications. Business managers run DSS (Decision Support System) applications to analyze various business scenarios. This paper characterizes a small subset of a DSS workload obtained from the TPC-H benchmark from TPC (Transaction Processing Council). This characterization is done at the system level using performance-monitoring counters. Through this process, we attempt to analyze the queries behavior on the Athlon, the effect of the Athlon's micro-architecture on executing the queries, and compare and contrast two different L2 cache implementations. },
}

@inproceedings{1437405,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 91-- 91},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437405},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437405},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437405.pdf?arnumber=1437405},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{5649549,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Goswami, N. and Shankar, R. and Joshi, M. and Tao Li},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Exploring GPGPU workloads: Characterization methodology, analysis and microarchitecture evaluation implications},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5649549},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5649549},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05649549.pdf?arnumber=5649549},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Artificial neural networks, Cache memory, Decoding, GPGPU workload, GPU microarchitecture design, Gallium, MUMmerGPU, Nearest neighbor searches, Nvidia CUDA SDK, Parboil, Registers, Rodinia, benchmark characterization, benchmark testing, branch divergence characteristics, characterization methodology, clustering analysis, computer graphic equipment, coprocessors, correlated dimensionality reduction process, diversity analysis, high performance computing device, hybrid sort, memory coalescing behavior, microarchitecture independent space, multiprocessing systems, nearest neighbor workload, parallel reduction, pattern clustering, scan of large array, similarity score, },
 abstract = {The GPUs are emerging as a general-purpose high-performance computing device. Growing GPGPU research has made numerous GPGPU workloads available. However, a systematic approach to characterize these benchmarks and analyze their implication on GPU microarchitecture design evaluation is still lacking. In this research, we propose a set of microarchitecture agnostic GPGPU workload characteristics to represent them in a microarchitecture independent space. Correlated dimensionality reduction process and clustering analysis are used to understand these workloads. In addition, we propose a set of evaluation metrics to accurately evaluate the GPGPU design space. With growing number of GPGPU workloads, this approach of analysis provides meaningful, accurate and thorough simulation for a proposed GPU architecture design choice. Architects also benefit by choosing a set of workloads to stress their intended functional block of the GPU microarchitecture. We present a diversity analysis of GPU benchmark suites such as Nvidia CUDA SDK, Parboil and Rodinia. Our results show that with a large number of diverse kernels, workloads such as Similarity Score, Parallel Reduction, and Scan of Large Arrays show diverse characteristics in different workload spaces. We have also explored diversity in different workload subspaces (e.g. memory coalescing and branch divergence). Similarity Score, Scan of Large Arrays, MUMmerGPU, Hybrid Sort, and Nearest Neighbor workloads exhibit relatively large variation in branch divergence characteristics compared to others. Memory coalescing behavior is diverse in Scan of Large Arrays, K-Means, Similarity Score and Parallel Reduction. },
}

@inproceedings{5650174,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Inoue, H. and Nakatani, T.},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Performance of multi-process and multi-thread processing on multi-core SMT processors},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5650174},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5650174},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05650174.pdf?arnumber=5650174},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Intel Xeon processor, Java, Java benchmarks, PHP, SMT scalability, Scalability, Sun UltraSPARC T1 processor, Throughput, core aware allocator, microprocessor chips, multi-threading, multicore SMT processors, multiprocess processing, multiprocessing systems, parallelization models, real world server program, simultaneous multithreading, },
 abstract = {Many modern high-performance processors support multiple hardware threads in the form of multiple cores and SMT (Simultaneous Multi-Threading). Hence achieving good performance scalability of programs with respect to the numbers of cores (core scalability) and SMT threads in one core (SMT scalability) is critical. To identify a way to achieve higher performance on the multi-core SMT processors, this paper compares the performance scalability with two parallelization models (using multiple processes and using multiple threads in one process) on two types of hardware parallelism (core scalability and SMT scalability). We tested standard Java benchmarks and a real-world server program written in PHP on two platforms, Sun's UltraSPARC T1 (Niagara) processor and Intel's Xeon (Nehalem) processor. We show that the multi-thread model achieves better SMT scalability compared to the multi-process model by reducing the number of cache misses and DTLB misses. However both models achieve roughly equal core scalability. We show that the multi-thread model generates up to 7.4 times more DTLB misses than the multi-process model when multiple cores are used. To take advantage of the both models, we implemented a memory allocator for a PHP runtime to reduce DTLB misses on multi-core SMT processors. The allocator is aware of the core that is running each software thread and allocates memory blocks from same memory page for each processor core. When using all of the hardware threads on a Niagara, the core-aware allocator reduces the DTLB misses by 46.7\% compared to the default allocator, and it improves the performance by 3.0\%. },
}

@inproceedings{990749,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {Prestor, U. and Davis, A.L.},
 year = {2001},
 pages = { 101-- 110},
 publisher = {IEEE},
 title = {An application-centric ccNUMA memory profiler},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990749},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990749},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990749.pdf?arnumber=990749},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { SGI Origin systems,  application-centric ccNUMA memory profiler,  cache coherent shared memory multiprocessors,  data structures,  memory allocation,  memory architecture,  memory profiling tool,  parallel multi-threaded applications,  performance evaluation,  shared memory systems,  snperf,  storage allocation,  task allocation, Costs, Counting circuits, Delay, Hardware, Information analysis, Memory architecture, Performance analysis, Resource management, Scalability, Yarn, },
 abstract = {Cache coherent shared memory multiprocessors are an attractive and available target for parallel multi-threaded applications. However, achieving the expected levels of performance has proven difficult. ccNUMA permance depends critically on memory and task allocation, and by the amount and type of the coherency transactions. No performance analysis tool to date has done an adequate job of providing high fidelity information about these application memory performance issues. This paper presents a new memory profiling tool called snperf which does provide such information for SGI Origin systems. },
}

@inproceedings{990748,
 booktitle = {Workload Characterization, 2001. WWC-4. 2001 IEEE International Workshop on},
 author = {de Alba, M.R. and Kaeli, D.R.},
 year = {2001},
 pages = { 91-- 98},
 publisher = {IEEE},
 title = {Runtime predictability of loops},
 date = {2 Dec. 2001},
 doi = {10.1109/WWC.2001.990748},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=990748},
 pdf_url = {http://ieeexplore.ieee.org/iel5/7772/21357/00990748.pdf?arnumber=990748},
 issn = {           },
 isbn = {0-7803-7315-4},
 language = {English},
 keywords = { SPECint2000 benchmark suite,  aggressive instruction delivery,  compiler,  design parameters,  dynamic branch predictors,  loop execution,  loop iterations,  parallel architectures,  path-based loop execution history,  performance evaluation,  program compilers,  program variable,  runtime predictability,  valid instructions, Computer aided instruction, Computer architecture, Dynamic range, History, Iterative algorithms, Parallel processing, Programming profession, Runtime, Terminology, },
 abstract = {To obtain the benefits of aggressive, wide-issue, architectures, a large window of valid instructions must be available. While researchers have been successful in obtaining high accuracies with a range of dynamic branch predictors, there still remains the need for more aggressive instruction delivery. Loop bodies possess a large amount of spatial and temporal locality. A large percentage of a program's entire execution can be attributed to code found in loop bodies. If we retain this code in a buffer or the cache, we do not have to refetch this code on subsequent loop iterations., Loops tend to iterate multiple times before exiting, thus providing us with the opportunity to speculatively issue multiple iterations. While some loops can be unrolled by a compiler many contain conditional branches. The number of times a loop iterates may be dependent on a program variable. These issues can hinder our ability to speculatively issue multiple iterations of a loop. If we are able to profile loops during runtime, we can use this information to more accurately issue speculative paths through loop bodies. In this paper we present a characterization of loop execution across the SPECint2000 benchmark suite. We intend for this study to serve as a guide in the selection of design parameters of a loop path predictor We characterize the patterns exhibited during multiple visits to a loop body. We present the design of a table that records path-based loop execution history and allows us to predict multiple loop iterations dynamically. },
}

@inproceedings{4086122,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {},
 year = {2006},
 pages = {iv--iv},
 publisher = {IEEE},
 title = {Message from the Program Chair},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302718},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086122},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086122.pdf?arnumber=4086122},
 isbn = {1-4244-0508-4},
 language = {English},
 abstract = {},
}

@inproceedings{4086123,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {},
 year = {2006},
 pages = {v--v},
 publisher = {IEEE},
 title = {IISWC-2006 People},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302719},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086123},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086123.pdf?arnumber=4086123},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Bonding, Finance, Instruments, Organizing, Reservoirs, Sun, },
 abstract = {},
}

@inproceedings{4086120,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {},
 year = {2006},
 pages = {ii--ii},
 publisher = {IEEE},
 title = {IEEE International Symposium on Workload Characterization},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302716},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086120},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086120.pdf?arnumber=4086120},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Java, Java workload, application energy behavior, application power behavior, benchmark construction, benchmark evaluation, benchmark testing, bioinformatics workload, biology computing, computer games, games workload characterization, high performance computing workload, multimedia computing, multimedia workload characterization, power aware computing, },
 abstract = {The following topics are dealt with: multimedia workload characterization; games workload characterization; application energy behavior; application power behavior; bioinformatics workload; benchmark construction; benchmark evaluation; Java workload; and high performance computing workload },
}

@inproceedings{4086121,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {},
 year = {2006},
 pages = {iii--iii},
 publisher = {IEEE},
 title = {Message from the General Chair},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302717},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086121},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086121.pdf?arnumber=4086121},
 isbn = {1-4244-0508-4},
 language = {English},
 abstract = {},
}

@inproceedings{4086126,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Altherr, R. and Du Bois, R. and Hammond, L. and Miller, E.},
 year = {2006},
 pages = {1--1},
 publisher = {IEEE},
 title = {Software Performance Tuning with the Apple CHUD Tools},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302722},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086126},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086126.pdf?arnumber=4086126},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Apple CHUD tools, Data visualization, Hardware, MacOS X, Performance analysis, Performance evaluation, Phase measurement, Programming profession, Software measurement, Software performance, Software tools, Timing, UNIX, assembly code, data visualisation, function call behavior tracing, gprof, graphical data visualization, hardware event counter measurements, open source performance analysis tools, operating systems (computers), performance-oriented programming, program behaviors, program diagnostics, program execution timing, public domain software, software engineering, software events tracing, software performance evaluation, software performance tuning, software tools, source code, system calls, },
 abstract = {Summary form only given. Many tools have been created to allow software engineers to analyze the execution of their code. While tools such as gprof often work well, most are not integrated very well with each other or the rest of the development environment, and interpreting the data that they provide can be a challenge. Because Apple's MacOS X is based on UNIX, most open source performance analysis tools can be used. However, we have also integrated several key performance tools together and added graphical data visualization to produce the CHUD toolset (Available for at http://developer.apple.com/tools/download/). With the CHUD tools, programmers can examine the performance of their code using a set of integrated tools that can perform most common performance-measurement tasks, including: traces of function call behavior (like gprof); sampled measurements of program execution timing; traces of software events, such as system calls; and hardware event counter measurements; Moreover, instead of just presenting a few key figures from these measurements in a brief report, the CHUD tools present their results in several textual and graphical formats, with integrated hyperlinks to related assembly and source code, so that programmers can easily examine both how their programs work on a large-scale level or zoom in and look at individual program phases in several different ways. This tutorial is targeted primarily at students and software engineers who work on UNIX-based systems and want to expand the repertoire of tools that they can use to analyze and improve the performance of their code. However, the material should also be useful to educators who teach performance-oriented programming techniques, as the graphical nature of Shark's output makes it easy to demonstrate program behaviors in an eye-catching manner },
}

@inproceedings{4086127,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Nethercote, N. and Walsh, R. and Fitzhardinge, J.},
 year = {2006},
 pages = {2--2},
 publisher = {IEEE},
 title = {Building Workload Characterization Tools with Valgrind},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302723},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086127},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086127.pdf?arnumber=4086127},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, Buildings, C programs, C++ programs, Computer architecture, Instruments, Linux, Memcheck, Memory architecture, Open source software, Robustness, Software packages, Synthetic aperture sonar, Valgrind, data analysis, dynamic binary instrumentation, instruction set architecture, memory errors, memory hierarchy, open-source software, program analysis tools, program diagnostics, program executions, public domain software, software tools, storage management, system monitoring, workload characterization tools, },
 abstract = {Summary form only given. Workload characterization relies heavily on robust and powerful tools to quickly and accurately gather and analyse large amounts of data about program executions. Valgrind is a dynamic binary instrumentation framework for building program analysis tools. Valgrind is best known for a tool, Memcheck, that finds memory errors common in C and C++ programs, but its ability to instrument every instruction and system call a program executes, and inspect every value a program manipulates, without slowing down program execution excessively, makes it an excellent platform for buildings tools suitable for workload characterization. In this tutorial, we introduce Valgrind, describing how you can use it to create powerful tools for doing profiling and trace collection, and to help characterize how workloads affect different machine aspects such as instruction set architecture, the memory hierarchy, and I/O. Valgrind provides powerful analysis tools without excessive slow-down, which allows very large workloads to be analysed easily. Valgrind is open-source (GPL) software, available on x86/Linux, AMD64/Linux, PPC32/Linux, PPC64/Linux, and work is underway to support other platforms. Valgrind tools are regularly used by the developers of many software packages, such as Firefox, OpenOffice, KDE, GNOME, MySQL, Perl, Python, PHP, Samba, RenderMan, SAS, The GIMP, Unreal Tournament, Squid, plus many scientific applications },
}

@inproceedings{4086124,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {},
 year = {2006},
 pages = {vi--vi},
 publisher = {IEEE},
 title = {IISWC-2006 Reviewers},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302720},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086124},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086124.pdf?arnumber=4086124},
 isbn = {1-4244-0508-4},
 language = {English},
 abstract = {},
}

@inproceedings{4086125,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {},
 year = {2006},
 pages = {vii--viii},
 publisher = {IEEE},
 title = {Table of Contents},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302721},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086125},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086125.pdf?arnumber=4086125},
 isbn = {1-4244-0508-4},
 language = {English},
 abstract = {},
}

@inproceedings{1437400,
 booktitle = {Workload Characterization, 2004. WWC-7. 2004 IEEE International Workshop on},
 author = {},
 year = {2004},
 pages = { 71-- 71},
 publisher = {IEEE},
 title = {Breaker page},
 date = {25 Oct. 2004},
 doi = {10.1109/WWC.2004.1437400},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1437400},
 pdf_url = {http://ieeexplore.ieee.org/iel5/9807/30916/01437400.pdf?arnumber=1437400},
 issn = {           },
 isbn = {0-7803-8828-3},
 language = {English},
 abstract = {},
}

@inproceedings{4086128,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Barroso, L.},
 year = {2006},
 pages = {3--3},
 publisher = {IEEE},
 title = {Warehouse-Sized Workloads},
 date = {Oct.  2006},
 doi = {10.1109/IISWC.2006.302724},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086128},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086128.pdf?arnumber=4086128},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {Application software, Biographies, Computer networks, Design engineering, Fault detection, Hardware, Network servers, Optimization, Software design, Software performance, },
 abstract = {<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article<img class="img-abs-container" style="width: 95\%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/04086128.png" border="0"> },
}

@inproceedings{4086129,
 booktitle = {Workload Characterization, 2006 IEEE International Symposium on},
 author = {Wenlong Li and Li, E. and Dulong, C. and Yen-Kuang Chen and Tao Wang and Yimin Zhang},
 year = {2006},
 pages = {7--16},
 publisher = {IEEE},
 title = {Workload Characterization of a Parallel Video Mining Application on a 16-Way Shared-Memory Multiprocessor System},
 date = {25-27 Oct. 2006},
 doi = {10.1109/IISWC.2006.302725},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4086129},
 pdf_url = {http://ieeexplore.ieee.org/iel5/4086118/4086119/04086129.pdf?arnumber=4086129},
 isbn = {1-4244-0508-4},
 language = {English},
 keywords = {3.1 GByte/s, 6.2 GByte/s, Application software, Bandwidth, Data mining, Games, Hardware, Multiprocessing systems, Parallel processing, Prefetching, Scalability, Video sharing, bus bandwidth utilization, computer architecture, data mining, data-slicing-level scheme, hybrid parallel scheme, multimedia computing, multimedia data mining, parallel processing, parallel video mining, playfield detection, shared memory systems, shared-memory multiprocessor system, sport, sports video mining systems, task level scheme, thread-level parallelism, video signal processing, workload characterization, },
 abstract = {As video data become more and more pervasive, mining information from multimedia data sources becomes increasingly important, e.g., automatically extracting highlights from soccer game video content. However, the huge computation requirement of mining interested data limits its wide use in practice. Since the hardware imperative behind computer architecture is shifting from uniprocessors to multi-core processors, exploiting thread-level parallelism existing in multimedia mining applications is critical to utilizing the hardware resources and accelerating the complex processing of highlight events detection. In this paper we analyze the view type and playfield detection application, a widely used application in sports video mining systems, and we present several different schemes (task level, data-slicing-level, and a hybrid parallel scheme, as well as variations of the hybrid parallel scheme) for parallelizing this application. The hybrid parallel scheme, which exploits data-level and task-slicing-level parallelism, outperforms basic task-level and data-slicing-level schemes, delivering much better performance in terms of execution time and speedup. On a 16-way shared-memory multi-processing system with hardware prefetch enabled, the hybrid scheme achieves a speedup of 10.6x. Detailed performance analysis shows that because of the large working set, the workload often requires data from the off-chip memory. Therefore, the saturated bus bandwidth utilization is the likely cause of bottlenecks for achieving perfect scalability performance. With hardware prefetch enabled, the bus utilization rate on 16-processors system is about 76\% for the hybrid scheme, and the projected bus bandwidth requirement for perfect scalability is about 3.1GB/s for 16 processors and 6.2 GB/s for 32 processors. In addition, our experiments also reveal that there are also no obvious scaling limiting factors, e.g., very low synchronization and load imbalance problems even with up to 16 processo- - rs },
}

@inproceedings{5306779,
 booktitle = {Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on},
 author = {Jibaja, I. and Shaw, K.A.},
 year = {2009},
 pages = {227--236},
 publisher = {IEEE},
 title = {Understanding the applicability of CMP performance optimizations on data mining applications},
 date = {4-6 Oct. 2009},
 doi = {10.1109/IISWC.2009.5306779},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5306779},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5289333/5306778/05306779.pdf?arnumber=5306779},
 isbn = {978-1-4244-5156-2},
 language = {English},
 keywords = {Application software, CMP performance optimization, Computer science, Concurrent computing, Data mining, Finance, Hardware, Mathematics, Network-on-a-chip, Optimization, Stress, application performance, cache organization, cache storage, chip multiprocessor, communication resources, data management, data mining, data mining application, data usage characteristics, hardware optimization, microprocessor chips, multiprocessing systems, on-chip memory, parallel data mining, pattern discovery, },
 abstract = {A major challenge to the creation of chip multiprocessors is designing the on-chip memory and communication resources to efficiently support parallel workloads. A variety of cache organizations, data management techniques, and hardware optimizations that take advantage of specific data characteristics have been developed to improve application performance. The success of these approaches depends on applications exhibiting the presumed data characteristics. Data mining applications are a growing class of applications that discover patterns in large sets of collected data. Because these applications tend to be highly parallelizable, they represent an important workload for chip multiprocessors. However, the memory intensive nature of these applications means that they will stress these chips' memory and communication resources. In this paper, we examine the data usage characteristics of a set of parallel data mining applications to determine the applicability of existing chip multiprocessor approaches to these applications. We show diversity of characteristics across and within these applications, making some techniques more applicable than others. We also discuss software approaches that could be used to either provide information to the hardware or assist the hardware in dynamically discovering data characteristics needed for the deployment of these techniques. },
}

@inproceedings{5648852,
 booktitle = {Workload Characterization (IISWC), 2010 IEEE International Symposium on},
 author = {Srinivasan, S. and Li Zhao and Lin Sun and Zhen Fang and Peng Li and Tao Wang and Iyer, R. and Illikkal, R. and Dong Liu},
 year = {2010},
 pages = {1--10},
 publisher = {IEEE},
 title = {Performance characterization and acceleration of Optical Character Recognition on handheld platforms},
 date = {2-4 Dec. 2010},
 doi = {10.1109/IISWC.2010.5648852},
 abstract_html_url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5648852},
 pdf_url = {http://ieeexplore.ieee.org/iel5/5644749/5648811/05648852.pdf?arnumber=5648852},
 isbn = {978-1-4244-9297-8},
 language = {English},
 keywords = {Character recognition, Image recognition, Image resolution, Image segmentation, Intel&#x00AE,  Atom&#x2122,  processor, Layout, Pixel, Text recognition, architectural characterization, cameras, editable text, handheld device, handwritten character recognition, hotspot function, image convertors, image sampling, low power general purpose processor, miscellaneous code optimization, mobile handsets, mobile platform, multiprocessing systems, multithreading, optical character recognition, optical character recognition, optimisation, performance characterization, printed text, software optimization, text analysis, },
 abstract = {Optical Character Recognition (OCR) converts images of handwritten or printed text captured by camera or scanner into editable text. OCR has seen limited adoption in mobile platforms due to the performance constraints of these systems. Intel<sup>\&#x00AE;</sup> Atom\&#x2122; processors have enabled general purpose applications to be executed on handheld devices. In this paper, we analyze a reference implementation of the OCR workload on a low power general purpose processor and identify the primary hotspot functions that incur a large fraction of the overall response time. We also present a detailed architectural characterization of the hotspot functions in terms of CPI, MPI, etc. We then implement and analyze several software/algorithmic optimizations such as i) Multi-threading, ii) image sampling for a hotspot function and Hi) miscellaneous code optimization. Our results show that up to 2X performance improvement in execution time of the application and almost 9X improvement for a hotspot can be achieved by using various software optimizations. We designed and implemented a hardware accelerator for one of the hotspots to further reduce the execution time and power. Overall, we believe our analysis provides a detailed understanding of the processing overheads for OCR running on a new class of low power compute platforms. },
}

@inproceedings{,
 booktitle = {},
 author = {},
 year = {},
 abstract = {},
}

