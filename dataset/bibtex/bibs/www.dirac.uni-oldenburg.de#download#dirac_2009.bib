% This file was created with JabRef 2.5.
% Encoding: UTF8

@CONFERENCE{Bach2009,
  author = {Bach, Joerg-Hendrik and Anemüller, Jörn},
  title = {Acoustic Object Detection in Adverse Conditions},
  booktitle = {NAG-DAGA 2009 International Conference on Acoustics},
  year = {2009},
  pages = {315},
  address = {Rotterdam},
  month = {March, 23 - 26},
  organization = {NAG-DAGA International Conference on Acoustics},
  note = {abstract and conference presentation},
  abstract = {In the current work, the problem of acoustic object detection in adverse
	
	and realistic background noise is tackled based on a recently proposed
	
	method for speech detection in noise. The method is based on the
	
	decomposition of the signals into amplitude modulation spectrograms.
	
	These perceptually motivated features provide a signal description
	in
	
	terms of power density at combinations of acoustic frequencies and
	modulation
	
	frequencies within meso-scale temporal windows of about 1s
	
	length. The target object sounds -ranging from speech to animal voices
	
	and characteristic elements of office activities- are embedded in
	recordings
	
	from noisy scenes at various SNRs. These acoustic background
	
	scenes include, e.g., recordings from heavy traffic streets, pedestrian
	
	zones, landscape. Numerous combinations of objects and environments
	
	have been analysed to provide some typical real-world acoustic scenes.
	
	These rather adverse conditions are compared to the performance in
	
	white noise as a reference. The results from our perceptually motivated
	
	features are compared to those obtained from standard of mel frequency
	
	cepstral coefficients (MFCC). Classification is based on the discriminative
	
	model of support vector machines (SVM) that are particularly suited
	
	for high-dimensional discrimination tasks.},
  url = {http://www.nag-daga.nl/NAG_DAGA_2009_Program.pdf}
}

@INPROCEEDINGS{Boenke2009_ASSC,
  author = {Boenke, L.T. and Ball, F. and Deliano, M. and Ohl, F.W.},
  title = {Different effects of within- and across-experiment variation of auditory
	and visual stimulus intensity on the conscious perception of temporal
	order},
  booktitle = {13th annual meeting of the Association for the Scientific Study of
	Consciousness (ASSC13)},
  year = {2009},
  pages = {59-60},
  address = {Berlin, Germany},
  month = {5-8 June 2009},
  organization = {Proc. 13th annual meeting of the Association for the Scientific Study
	of Consciousness (ASSC13)},
  abstract = {It is known that variation of physical stimulus parameters
	
	within experiments and across experiments can have diff erent
	
	eff ects on both neuronal processing and conscious perception ( Boenke
	
	et al. 2009 ). In multisensory temporal-order perception, it is well
	
	established that the perceived temporal order depends on the physical
	
	intensities of the stimuli involved. For example, Neumann et al. (
	1992 )
	
	demonstrated eff ects of both variation of the auditory stimulus intensity
	
	and variation of the visual intensity on the point of subjective simultaneity
	
	( pss ) by combining 3 light intensities and 3 tone intensities. However,
	
	Roufs ( 1963 ), when performing a series of experiments in each of
	which
	
	the auditory intensity was held constant and only the visual intensity
	
	was varied, and then compared across this series of experiments, found
	
	only an eff ect of the light intensity but not of the auditory intensity
	on
	
	the pss. To test whether the eff ect of intensity variation of a given
	sensory
	
	modality in a multisensory stimulus on the conscious perception
	
	of temporal order depends on whether it is held constant or is independently
	
	varied in an experiment, we have conducted 5 experiments. In
	
	experiment 1, using two sound intensities ( Alow, Ahigh ) and two
	light
	
	intensities ( Vlow, Vhigh ), we could replicate the fi ndings of Neumann
	
	et al. ( 1992 ). In experiments 2 ( Alow, Vlow, Vhigh ) and 3 ( Ahigh,
	Vlow,
	
	Vhigh ) we only varied the light intensity and held the sound intensity
	
	constant. In experiments 4 and 5 we set the light intensity to a constant
	
	low or high value, respectively, and only varied auditory intensities.
	
	In experiments 2 and 3 we found a similar result as Roufs ( 1963 ),
	
	
	namely that varying the sound intensities across experiments did not
	
	aff ect the pss. In experiments 4 and 5 we found that the variation
	of light
	
	intensities across experiments was similar to variation within an
	experiment
	
	( cf. experiment 1 ). In conclusion, these data imply a diff erence
	in
	
	the ability of the auditory and visual sensory system to compensate
	intensity
	
	variation of their adequate modality in an audiovisual compound
	
	stimulus.},
  url = {http://www.assc13.com/images/assc13_abstracts_screen.pdf}
}

@INPROCEEDINGS{Boenke2009_5,
  author = {Boenke, LT. and Deliano, M. and Ohl, FW.},
  title = {Stimulus duration influences perceived simultaneity in audiovisual
	temporal order judgment},
  booktitle = {10th Annual Meeting of the International Multisensory Research Forum},
  year = {2009},
  address = {New York},
  month = {June 29th- July 2nd},
  organization = {International Multisensory Research Forum},
  note = {198(2-3):233-44},
  abstract = {The temporal integration of stimuli in different sensory modalities
	plays a crucial role in multisensory processing. Previous studies
	using temporal-order judgments to determine the point of subjective
	simultaneity (PSS) with multisensory stimulation yielded conflicting
	results on modality-specific delays. While it is known that the relative
	stimulus intensities of stimuli from different sensory modalities
	affect their perceived temporal order, we have hypothesized that
	some of these discrepancies might be explained by a previously overlooked
	confounding factor, namely the duration of the stimulus. We therefore
	studied the influence of both factors on the PSS in a spatial-audiovisual
	temporal-order task. In addition to confirming previous results on
	the role of stimulus intensity, we report that varying the temporal
	duration of an audiovisual stimulus pair also affects the perceived
	temporal order of the auditory and visual stimulus components. Although
	individual PSS values varied from negative to positive values across
	participants, we found a systematic shift of PSS values in all participants
	toward a common attractor value with increasing stimulus duration.
	This resulted in a stabilization of PSS values with increasing stimulus
	duration, indicative of a mechanism that compensates individual imbalances
	between sensory modalities, which might arise from attentional biases
	toward one modality at short stimulus durations.},
  timestamp = {2009.10.26},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19590862}
}

@ARTICLE{Boenke2009_6,
  author = {Boenke, L. and Deliano, M. and Ohl, F.W.},
  title = {Stimulus duration influences perceived simultaneity in audiovisual
	temporal order judgment},
  journal = {Experimental Brain Research},
  year = {2009},
  volume = {198(2-3)},
  pages = {233-244},
  abstract = {The temporal integration of stimuli in diVerent
	
	sensory modalities plays a crucial role in multisensory processing.
	
	Previous studies using temporal-order judgments
	
	to determine the point of subjective simultaneity (PSS) with
	
	multisensory stimulation yielded conXicting results on
	
	modality-speciWc delays. While it is known that the relative
	
	stimulus intensities of stimuli from diVerent sensory modalities
	
	aVect their perceived temporal order, we have hypothesized
	
	that some of these discrepancies might be explained
	
	by a previously overlooked confounding factor, namely the
	
	duration of the stimulus. We therefore studied the inXuence
	
	of both factors on the PSS in a spatial-audiovisual temporal-
	
	order task. In addition to conWrming previous results on
	
	the role of stimulus intensity, we report that varying the
	
	temporal duration of an audiovisual stimulus pair also
	
	aVects the perceived temporal order of the auditory and
	
	visual stimulus components. Although individual PSS values
	
	varied from negative to positive values across participants,
	
	we found a systematic shift of PSS values in all
	
	participants toward a common attractor value with increasing
	
	stimulus duration. This resulted in a stabilization of
	
	PSS values with increasing stimulus duration, indicative of
	
	a mechanism that compensates individual imbalances
	
	between sensory modalities, which might arise from attentional
	
	biases toward one modality at short stimulus durations.},
  doi = {10.1007/s00221-009-1917-z},
  timestamp = {2009.10.26},
  url = {http://www.springerlink.com/content/70865rhg254m771k/fulltext.pdf}
}

@ARTICLE{ISI:000264378500024,
  author = {Boenke, Lars T. and Ohl, Frank W. and Nikolaev, Andrey R. and Lachmann,
	Thomas and Van Leeuwen, Cees},
  title = {Different time courses of Stroop and Garner effects in perception
	- An Event-Related Potentials Study},
  journal = {NEUROIMAGE},
  year = {2009},
  volume = {45(4)},
  pages = {1272-1288},
  number = {4},
  month = {MAY 1},
  abstract = {Visual integration between target and irrelevant features leads to
	effects of irrelevant feature congruency (Stroop) or variation (Garner)
	on target classification performance. Presenting closed geometrical
	shapes as stimuli, we obtained Stroop and Garner effects of one part
	of their contour on another, in response times and error rates. The
	correlates of these effects in brain activity were observed in event-related
	potentials (ERP). Stroop effects occurred in ERP amplitude of the
	N1 and N2 components, starting about 170 ms after stimulus onset;
	Garner effects occurred in amplitude of the rising part of the P3
	component, starting about 330 ms after stimulus onset. A subsequent
	point-wise analysis of Stroop and Garner effects in ERP showed that
	they belong to different, cascaded processing stages. The difference
	in time course between Stroop and Garner effects in ERP is in accordance
	with the view that both are produced by different mechanisms, the
	former sensitive to interference within presentations and the latter
	sensitive to interference between presentations. The brief interval
	of 330-370 ms after stimulus onset when these two mechanisms overlap
	may correspond to the central processing bottleneck, responsible
	for the combinations of Stroop and Garner effects generally found
	in response times. (C) 2009 Elsevier Inc. All rights reserved.},
  doi = {10.1016/j.neuroimage.2009.01.019},
  issn = {1053-8119},
  unique-id = {ISI:000264378500024},
  url = {http://dx.doi.org/10.1016/j.neuroimage.2009.01.019}
}

@INPROCEEDINGS{Bruemmer2009_1,
  author = {Niko Brümmer and Albert Strasheim and Valiantsina Hubeika and Pavel
	Matějka and Lukáš Burget and Ondřej Glembek},
  title = {Discriminative Acoustic Language Recognition via Channel-Compensated
	GMM Statistics},
  booktitle = {Proc. Interspeech 2009},
  year = {2009},
  number = {9},
  pages = {2187--2190},
  publisher = {International Speech Communication Association},
  issn = {1990-9772},
  journal = {Proceedings of Interspeech},
  language = {english},
  location = {Brighton, GB},
  url = {http://www.fit.vutbr.cz/research/view_pub.php?id=9042}
}

@INPROCEEDINGS{Cernocky2009_1,
  author = {Cernocky, Jan},
  title = {Brno University of Technology detecting OOVs in DIRAC project},
  booktitle = {SLTC newsletter 07/09 (Speech and Language Processing Technical Committee)
	
	IEEE Signal Processing Society},
  year = {2009},
  timestamp = {2010.02.07},
  url = {http://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2009-07/brno-oov-dirac}
}

@INPROCEEDINGS{Cesa-Bianchi2009_1,
  author = {Cesa-Bianchi, N. and Gentile, C. and Orabona, F.},
  title = {Robust Bounds for Classification via Selective Sampling},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2009},
  volume = {Vol. 382},
  pages = {121-128},
  address = {Montreal},
  organization = {International Conference on Machine Learning (ICML)},
  publisher = {ACM International Conference Proceeding Series},
  note = {ISBN:978-1-60558-516-1},
  abstract = {We introduce a new algorithm for binary classification in the selective
	sampling protocol. Our algorithm uses Regularized Least Squares (RLS)
	as base classifier, and for this reason it can be efficiently run
	in any RKHS. Unlike previous margin-based semi-supervised algorithms,
	our sampling condition hinges on a simultaneous upper bound on bias
	and variance of the RLS estimate under a simple linear label noise
	model. This fact allows us to prove performance bounds that hold
	for an arbitrary sequence of instances. In particular, we show that
	our sampling strategy approximates the margin of the Bayes optimal
	classifier to any desired accuracy ε by asking Õ (d/ε2) queries (in
	the RKHS case d is replaced by a suitable spectral quantity). While
	these are the standard rates in the fully supervised i.i.d. case,
	the best previously known result in our harder setting was Õ (d3/ε4).
	Preliminary experiments show that some of our algorithms also exhibit
	a good practical performance.},
  doi = {doi:10.1145/1553374.1553390},
  timestamp = {2010.01.15},
  url = {http://portal.acm.org/citation.cfm?doid=1553374.1553390}
}

@ARTICLE{DeBane2009_1,
  author = {De Baene, W. and Vogels, R.},
  title = {Effects of adaptation on the stimulus selectivity of macaque inferior
	temporal spiking activity and local field potentials},
  journal = {Cerebral Cortex},
  year = {2009},
  abstract = {Stimulus repetition reduces neural response in cortical areas. Such
	adaptation is used in functional magnetic resonance imaging to infer
	the selectivity of neuronal populations; however, the mechanisms
	of adaptation remain elusive, especially in higher areas. We measured
	adaptation of spiking activity and local field potentials (LFPs)
	in macaque inferior temporal (IT) cortex for parameterized shapes
	by comparing tuning for test stimuli following a brief adaptation
	with predictions derived from different models of adaptation. Adaptation
	was similar during passive fixation or an attention-demanding task.
	We found consistent adaptation of spiking activity and LFP power
	in high- (gamma) but not low-frequency bands when repeating shapes.
	Contrary to sharpening models, repetition did not affect shape selectivity.
	The degree of similarity between adapter and test shapes was a stronger
	determinant of adaptation than was the response to the adapter. Adaptation
	still occurred when adapter and test stimuli did not spatially overlap,
	but adaptation was stronger for same, compared with different, adapters
	and test stimulus positions. These adaptation effects were similar
	for spiking and for gamma activity. In conclusion, adaptation of
	IT spiking activity and LFPs in IT is strongly dependent on feature
	similarities in the adapter and test stimuli, in agreement with input,
	but not firing-rate fatigue models.},
  doi = {doi:10.1093/cercor/bhp277},
  timestamp = {2010.01.15},
  url = {http://cercor.oxfordjournals.org/cgi/content/abstract/bhp277v1}
}

@ARTICLE{Deliano2009_1,
  author = {Deliano and Ohl},
  title = {Neurodynamics of category learning: Towards understanding the creation
	of meaning in the brain},
  journal = {New Mathematics and Natural Computation (NMNC)},
  year = {2009},
  volume = {5},
  pages = {61-81},
  abstract = {Category learning, the formation and use of categories (equivalence
	classes of meaning), is an elemental function of cognition. We report
	our approach to study the physiological mechanisms underlying category
	learning using high-density multi-channel recordings of electrocorticograms
	in rodents. These data suggest the coexistence of separate coding
	principles for representing physical stimulus attributes (stimulus
	representation) and subjectively relevant information (meaning) about
	stimuli, respectively. The implications of these findings for the
	construction of interactive cortical sensory neuroprostheses are
	discussed.},
  doi = {10.1142/S1793005709001192},
  url = {http://dx.doi.org/10.1142/S1793005709001192}
}

@ARTICLE{Deliano2009_2,
  author = {Deliano, M. and Scheich, H. and Ohl, F.W.},
  title = {Intracortical Microstimulation and its Role for Sensory Processing
	and Learning},
  journal = {Journal of Neuroscience},
  year = {2009},
  volume = {29(50)},
  pages = {15898-15909},
  abstract = {Several studies have shown that animals can learn to make specific
	use of intracortical microstimulation (ICMS) of sensory cortex within
	behavioral tasks. Here, we investigate how the focal, artificial
	activation by ICMS leads to a meaningful, behaviorally interpretable
	signal. In natural learning, this involves large-scale activity patterns
	in widespread brain-networks. We therefore trained gerbils to discriminate
	closely neighboring ICMS sites within primary auditory cortex producing
	evoked responses largely overlapping in space. In parallel, during
	training, we recorded electrocorticograms (ECoGs) at high spatial
	resolution. Applying a multivariate classification procedure, we
	identified late spatial patterns that emerged with discrimination
	learning from the ongoing poststimulus ECoG. These patterns contained
	information about the preceding conditioned stimulus, and were associated
	with a subsequent correct behavioral response by the animal. Thereby,
	relevant pattern information was mainly carried by neuron populations
	outside the range of the lateral spatial spread of ICMS-evoked cortical
	activation (~1.2 mm). This demonstrates that the stimulated cortical
	area not only encoded information about the stimulation sites by
	its focal, stimulus-driven activation, but also provided meaningful
	signals in its ongoing activity related to the interpretation of
	ICMS learned by the animal. This involved the stimulated area as
	a whole, and apparently required large-scale integration in the brain.
	However, ICMS locally interfered with the ongoing cortical dynamics
	by suppressing pattern formation near the stimulation sites. The
	interaction between ICMS and ongoing cortical activity has several
	implications for the design of ICMS protocols and cortical neuroprostheses,
	since the meaningful interpretation of ICMS depends on this interaction.},
  doi = {doi:10.1523/JNEUROSCI.1949-09.2009},
  timestamp = {2010.01.22},
  url = {http://dx.doi.org/doi:10.1523/JNEUROSCI.1949-09.2009}
}

@INPROCEEDINGS{Fillbrandt2009_3,
  author = {Fillbrandt, A. and Ohl, FW.},
  title = {Audiovisual category transfer in rodents},
  booktitle = {10th Annual Meeting of the International Multisensory Research Forum},
  year = {2009},
  number = {No.62},
  pages = {34},
  address = {New York},
  month = {June 29th- July 2nd},
  organization = {10th International Multisensory Research Forum},
  note = {Poster},
  abstract = {A basic process in the build up of conceptual knowledge is the formation
	of categories involving the abstraction of shared features from the
	specific sensory experiences. Our previous work in the rodent (gerbil)
	auditory system has demonstrated that the formation of auditory categories
	is accompanied by the emergence of category-specific spatiotemporal
	activity patterns in auditory cortex (Ohl et al., Nature, 2001).
	Here, the investigation of the formation of category-specific activity
	patterns was extended to the multisensory domain. We have examined
	whether perceptual categories, after being formed in one sensory
	modality (audition or vision), can be transferred to another sensory
	modality (vision or audition, resp.), and have suggested a physiological
	basis for this audiovisual category transfer.
	
	We trained Mongolian Gerbils (Meriones unguiculatus) to associate
	a slow (0.5 Hz) and a fast (5 Hz) presentation rate of stimuli with
	the Go response and NoGo response, respectively, in an active avoidance
	paradigm (shuttle box). One group of animals was trained with auditory
	tone pips, the other with visual flashes as stimuli. After sufficient
	training with the first sensory modality, a second training phase
	was initiated in which the sensory modality of the stimuli was changed
	from auditory to visual, or from visual to auditory, respectively.
	In this second training phase, groups were further split into a group
	with congruent training (contingency between presentation rate and
	required response remained unchanged) and a group with incongruent
	training (contingency between presentation rate and required response
	was reversed).
	
	After the modality switch, the congruent groups showed a higher acquisition
	rate of the conditioned responses than the incongruent groups indicating
	a crossmodal transfer of the rate-response association.
	
	During training, the electrocorticogram was recorded from two 16 -electrode
	arrays chronically implanted onto the epidural surface of primary
	auditory and the visual cortex.
	
	Cortical activity patterns in the ongoing electrocorticogram (ECoG)
	associated with the Go- and the No-Go stimuli were determined in
	the spatial distribution of signal power using a multivariate pattern
	classification procedure (Barrie et al., J. Neurophysiol., 1996;
	Freeman, J. Neurophysiol., 2000).
	
	During auditory training, in animals discriminating the auditory stimuli,
	patterns in auditory cortex developed with learning in accordance
	with our previous results. In addition, patterns could also be observed
	in the visual cortex in later training sessions, and at later post-stimulus
	time points within a trial. During visual training, in animals not
	showing correct responses during the first training sessions, we
	suspect that there was no transfer learning. Also, in these animals
	no significant patterns could be detected in the early training sessions.
	Nevertheless, some animals started to develop correct responses in
	later phases of the training. For these animals patterns could be
	identified in the visual cortex. For animals showing correct discrimination
	already during the first visual training sessions we suspect that
	they transferred the rate-response association learned during auditory
	training to the visual training. In these animals patterns could
	be detected in both the auditory and the visual cortex already during
	the first training sessions. We suggest that in these animals activity
	in both auditory and visual cortex was instrumental for achieving
	the crossmodal transfer of learned associations.},
  timestamp = {2009.10.26},
  url = {http://imrf.mcmaster.ca/IMRF/ocs/index.php/meetings/2009/paper/view/651}
}

@CONFERENCE{Fillbrandt2009_4,
  author = {Fillbrandt, A. and Ohl, F.W.},
  title = {Cortical neurodynamics during audiovisual category transfer in rodents},
  booktitle = {3rd International Conference on Auditory Cortex},
  year = {2009},
  address = {Magdeburg, Germany},
  month = {August 29th to September 2nd},
  organization = {Conference on Auditory Cortex},
  note = {P 033},
  abstract = {A basic process in the build?up of conceptual
	
	knowledge is the formation of categories involving
	
	the abstraction of shared features from the
	
	specific sensory experiences. Our previous work
	
	in the rodent (gerbil) auditory system has demonstrated
	
	that the formation of auditory categories
	
	is accompanied by the emergence of categoryspecific
	
	spatiotemporal activity patterns in auditory
	
	cortex (Ohl et al., Nature, 2001). Here, the
	
	investigation of the formation of category-specific
	
	activity patterns was extended to the multisensory
	
	domain. We have examined whether perceptual
	
	categories, after being formed in the auditory modality,
	
	can be transferred to the visual modality,
	
	and have suggested a physiological basis for this
	
	audiovisual category transfer.
	
	We trained Mongolian Gerbils (Meriones unguiculatus)
	
	to associate a slow (0.5 Hz) and a fast (5
	
	Hz) presentation rate of auditory tone pips with
	
	the Go response and NoGo response, respectively,
	
	in an active avoidance paradigm (shuttle
	
	box). After a predefined performance criterion was
	
	reached for the discrimination task in the auditory
	
	modality, a second training phase was initiated
	
	in which the sensory modality of the stimuli was
	
	changed from auditory to visual. For one animal
	
	group (congruent group) the contingency of the
	
	two stimulus presentation rates with the Go/Nogoresponses
	
	stayed the same irrespective of the
	
	modality of stimulation, for a second group (incongruent
	
	group) it was reversed across modalities.
	
	After the modality switch, the congruent groups
	
	showed a higher acquisition rate of the conditioned
	
	responses than the incongruent groups indicating
	
	a crossmodal transfer of the rate-response association.
	
	During training, the electrocorticogram was
	
	recorded from two 16-electrode arrays chronically
	
	implanted onto the epidural surface
	
	of primary auditory and the visual cortex.
	
	Cortical activity patterns in the ongoing electrocorticogram
	
	associated with the Go- and the NoGo
	
	stimuli were determined in the spatial distribution
	
	of signal power using a multivariate pattern classification
	
	procedure (Barrie et al., J. Neurophysiol.,
	
	1996; Freeman, J. Neurophysiol., 2000).
	
	For animals showing correct discrimination already
	
	during the first visual training sessions we
	
	suspect that they transferred the rate-response
	
	association learned during auditory training to the
	
	visual training. In these animals patterns could be
	
	detected in both the auditory and the visual cortex
	
	already during the first training sessions. We suggest
	
	that in these animals activity in both auditory
	
	and visual cortex was instrumental for achieving
	
	the crossmodal transfer of learned associations.},
  timestamp = {2009.10.26},
  url = {http://www.auditory-cortex.de/assets/pdf/AC2009_Program_End.pdf}
}

@CONFERENCE{Fillbrandt2009_2,
  author = {Fillbrandt, A. and Zeghbib, A. and Ohl, F.W.},
  title = {Trial-to-trial variability of interaction dynamics between auditory
	and visual cortex during asynchronous audiovisual stimulation},
  booktitle = {8th Göttingen Meeting of the German Neuroscience Society},
  year = {2009},
  address = {Goettingen},
  month = {March 25-29},
  organization = {8th Goettingen Meeting of the German Neuroscience Society},
  note = {T23 - 3B (P. 1120)},
  abstract = {Crossmodal integration of sensory input requires large-scale coordinative
	interactions between distant
	
	cortical areas. Here we investigate how states of coordination change
	continuously over time and across
	
	trials in periods with and without external stimulation.
	
	Auditory pure tone and visual flash stimuli with fixed interstimulus
	onset asynchrony were presented
	
	continuously to awake Mongolian gerbils while local field potential
	activity was recorded from depth
	
	electrodes implanted in the primary auditory and visual cortex, respectively.
	
	The frequency and direction of coordinative interactions between auditory
	and visual cortex was analysed
	
	in single trials using the Directed Transfer Function (DTF, Kaminski
	& Blinowska, 1991). In the
	
	prestimulus interval the dominant frequency of interaction showed
	a high variability in that it changed
	
	constantly. In contrast, the overall rates of occurrence of certain
	dominant frequencies were however highly
	
	invariant across sessions and animals. The distributions of rates
	of occurrences of certain dominant
	
	frequencies differed depending on the direction of interaction.
	
	In the poststimulus interval these distributions changed: certain
	dominant frequencies occurred more often
	
	at certain time points during the course of the trial. We show that
	the frequency-dependent amplitude of the
	
	trial-averaged DTF is shaped not only by the amplitudes of the single-trial
	DTF at these frequencies but
	
	also by the rates of occurrence of trials with significant amplitude
	values at these frequencies.
	
	The implications of these findings for (1) the putative mechanism
	of crossmodal interaction, and (2) for
	
	theories conceptualizing stimulus-evoked cortical responses as processes
	"adding" neuronal activity to the
	
	ongoing cortical activity will be discussed.},
  url = {https://www.nwg-goettingen.de/2009/upload/file/Proceedings_Goettingen2009.pdf}
}

@ARTICLE{ISI:000262672600002,
  author = {Ganapathy, Sriram and Thomas, Samuel and Hermansky, Hynek},
  title = {Modulation frequency features for phoneme recognition in noisy speech},
  journal = {Journal of the Acoustical Society of America},
  year = {2009},
  volume = {125},
  pages = {EL8-EL12},
  number = {1},
  month = {JAN},
  abstract = {In this letter, a new feature extraction technique based on modulation
	spectrum derived from syllable-length segments of subband temporal
	envelopes is proposed. These subband envelopes are derived from autoregressive
	modeling of Hilbert envelopes of the signal in critical bands, processed
	by both a static (logarithmic) and a dynamic (adaptive loops) compression.
	These features are then used for machine recognition of phonemes
	in telephone speech. Without degrading the performance in clean conditions,
	the proposed features show significant improvements compared to other
	state-of-the-art speech analysis techniques. In addition to the overall
	phoneme recognition rates, the performance with broad phonetic classes
	is reported.},
  doi = {10.1121/1.3040022},
  issn = {0001-4966},
  unique-id = {ISI:000262672600002},
  url = {http://dx.doi.org/10.1121/1.3040022}
}

@ARTICLE{Goldschmidt2009_1,
  author = {Goldschmidt, J. and Wanger, T. and Engelhorn, A. and Friedrich, H.
	and Happel, M. and Ilango, A. and Engelmann, M. and Stuermer, I.W.
	and Ohl, F.W. and Scheich, H.},
  title = {High-resolution mapping of neuronal activity using the lipophilic
	thallium chelate complex TlDDC: Protocol and validation of the method},
  journal = {Neuroimage},
  year = {2009},
  volume = {49},
  pages = {303-315},
  doi = {doi:10.1016/j.neuroimage.2009.08.012},
  timestamp = {2010.02.08},
  url = {http://dx.doi.org/doi:10.1016/j.neuroimage.2009.08.012}
}

@CONFERENCE{Hammer2009_1,
  author = {Hammer, R. and Brechmann, A. and Ohl, F.W. and Weinshall, D. and
	Hochstein, S.},
  title = {The neuronal basis of category learning by comparison},
  booktitle = {Neuroscience},
  year = {2009},
  organization = {Neuroscience},
  note = {Annual Meeting Publications
	
	Abstr. 503.7},
  abstract = {Previous studies provided evidence for the existence of multiple neuronal
	mechanisms for category learning. These studies suggested that different
	neuronal mechanisms are required for learning different category
	structures, such as rule-based, information-integration, or prototype-based
	categories. Specifically it has been shown that the dorsal striatum
	is mainly involved in explicit rule learning whereas the ventral
	striatum and the posterior occipital cortex are associated with information-integration
	and prototype learning, respectively. Other recent studies demonstrate
	the importance of object comparison for category learning. For example,
	we found that both children and adults are capable of learning a
	categorization rule simply by comparing a few exemplar pairs identified
	as belonging to the same category. On the other hand, the proficiency
	of learning the same categorization rule by comparing exemplar pairs
	from different categories develops only at late childhood. Even then,
	this process is not always intuitively employed, though it enables
	better categorization performance when it is employed. Here we present
	an fMRI study of the underling neuronal mechanism(s) of these two
	comparison processes. We find that these two processes differ also
	in the brain sites of activation associated with them. Specifically,
	when participants learn a new complex rule-based category structure,
	(identifying two out of a possible four features as task relevant),
	by comparing only different-class exemplars, the BOLD response in
	the dorsal striatum is significantly higher than when learning is
	done by comparing same-class exemplars, or when the task does not
	require category learning at all. That is, when performing a rule-based
	category learning task, the dorsal striatum is more involved in processing
	informative between-category differences but not the within-category
	similarities and differences which are similarly informative for
	this task. The current findings are the first to provide evidence
	for this kind of differential activation in the context of category
	learning. Accordingly, we suggest that neuronal processes involved
	in category learning are not only associated with the structure of
	the learned categories, but also with the nature of the information
	provided during the learning process.},
  timestamp = {2010.01.22}
}

@ARTICLE{Hammer2009_3,
  author = {Hammer, Rubi and Diesendruck, Gil and Weinshall, Daphna and Hochstein,
	Shaul},
  title = {The development of category learning strategies: What makes the difference?"},
  journal = {Cognition},
  year = {2009},
  volume = {112(1)},
  pages = {105-119},
  abstract = {Category learning can be achieved by identifying common features among
	category members, distinctive features among non-members, or both.
	These processes are psychologically and computationally distinct,
	and may have implications for the acquisition of categories at different
	hierarchical levels. The present study examines an account of children’s
	difficulty in acquiring categories at the subordinate level grounded
	on these distinct comparison processes. Adults and children performed
	category learning tasks in which they were exposed either to pairs
	of objects from the same novel category or pairs of objects from
	different categories. The objects were designed so that for each
	category learning task, two features determined category membership
	whereas two other features were task irrelevant. In the learning
	stage participants compared pairs of objects noted to be either from
	the same category or from different categories. Object pairs were
	chosen so that the objective amount of information provided to the
	participants was identical in the two learning conditions. We found
	that when presented only with object pairs noted to be from the same
	category, young children (6 less-than-or-equals, slant YO less-than-or-equals,
	slant 9.5) learned the novel categories just as well as older children
	(10 less-than-or-equals, slant YO less-than-or-equals, slant 14)
	and adults. However, when presented only with object pairs known
	to be from different categories, unlike older children and adults,
	young children failed to learn the novel categories. We discuss cognitive
	and computational factors that may give rise to this comparison bias,
	as well as its expected outcomes.},
  doi = {doi:10.1016/j.cognition.2009.03.012},
  timestamp = {2010.02.06},
  url = {http://dx.doi.org/doi:10.1016/j.cognition.2009.03.012}
}

@ARTICLE{Hammer2009_2,
  author = {Hammer, Rubi and Hertz, Tomer and Hochstein, Shaul and Weinshall,
	Daphana},
  title = {Category Learning from Equivalence Constraints},
  journal = {Cognitive Processing},
  year = {2009},
  volume = {10(3)},
  pages = {211-232},
  abstract = {Information for category learning may be provided as positive or negative
	equivalence constraints (PEC/NEC)—indicating that some exemplars
	belong to the same or different categories. To investigate categorization
	strategies, we studied category learning from each type of constraint
	separately, using a simple rule-based task. We found that participants
	use PECs differently than NECs, even when these provide the same
	amount of information. With informative PECs, categorization was
	rapid, reasonably accurate and uniform across participants. With
	informative NECs, performance was rapid and highly accurate for only
	some participants. When given directions, all participants reached
	high-performance levels with NECs, but the use of PECs remained unchanged.
	These results suggest that people may use PECs intuitively, but not
	perfectly. In contrast, using informative NECs enables a potentially
	more accurate categorization strategy, but a less natural, one which
	many participants initially fail to implement—even in this simplified
	setting.},
  doi = {doi:10.1007/s10339-008-0243-x},
  timestamp = {2010.02.06},
  url = {http://dx.doi.org/doi:10.1007/s10339-008-0243-x}
}

@CONFERENCE{Happel2009_1,
  author = {Happel, MFK. and Jeschke, M. and Handschuh, J. and Deliano, M. and
	Ohl, FW.},
  title = {Parallel electrophysiological and behavioral analysis of layer-specific
	electrical microstimulation in primary auditory cortex - implications
	for the subcortical-loop hypothesis},
  booktitle = {3rd International Conference on Auditory Cortex},
  year = {2009},
  address = {Magdeburg},
  organization = {3rd Int. Conf. on Auditory Cortex},
  note = {P044},
  abstract = {Electrical stimulation is a widely used tool in
	
	systemic neurosciences although the neuronal
	
	generators of electrically evoked responses and
	
	their neuronal dynamics are still not well understood.
	
	For instance we do not have a detailed
	
	understanding of how intracortical microstimulation
	
	(ICMS) interfaces with cortical processing.
	
	The impact of intracortically (IC) neuronal activities
	
	and backpropagating feedback activations
	
	through corticothalamic (CT) connections in particular,
	
	are not well known [1]. Behavioral studies
	
	using ICMS as a perceptually relevant stimulus
	
	revealed that infragranular stimulation has the
	
	lowest threshold [2] that could in principle be due
	
	to activation of subcortical loops.
	
	The objective of this study was a parallel electrophysiological
	
	and behavioral analysis of layerspecific
	
	ICMS in primary auditory cortex AI of the
	
	Mongolian gerbil. We used current source density
	
	(CSD) analysis to compare acoustical stimulation
	
	and ICMS in terms of spatiotemporal arrangement
	
	of current sources and sinks. By inactivating
	
	intracortical processing using topical application
	
	of the GABAA-agonist Muscimol we disentangled
	
	TC and IC contributions to the CSD profiles [3].
	
	After the pharmacological blockade we still found
	
	remaining sinks and sources when the stimulation
	
	site was less than 600 ?m afar from the recording
	
	axis. This indicates the principal relevance
	
	of intracortical connections for the broad cortical
	
	spread after ICMS, but points to an additional
	
	activation of precise feedback-connections of the
	
	thalamocortical system.
	
	In order to exploit the neuronal dynamics evoked
	
	by ICMS for neuroprosthetic applications [4] we
	
	determined the laminar threshold variation in a
	
	behavioral ICMS detection task using a shuttlebox
	
	paradigm. Analysis of psychometric functions
	
	revealed that ICMS in the granular and infragranular
	
	layers (IV-VI) yielded lower behavioral
	
	thresholds compared to ICMS in supragranular
	
	layers. We found biphasic bipolar stimulation to
	
	be more reliable and efficient than biphasic monopolar
	
	stimulation. Further implications of the
	
	combined electrophysiological and behavioral
	
	results for the role of subcortical loops in ICMSevoked
	
	perception will be discussed.},
  timestamp = {2009.10.23},
  url = {http://www.auditory-cortex.de/assets/pdf/AC2009_Program_End.pdf}
}

@INPROCEEDINGS{Happel2009_2,
  author = {Happel, MFK. and Jeschke, M. and Handschuh, J. and Deliano, M. and
	Ohl, FW.},
  title = {Parallel electrophysiological and behavioral analysis of layer-specific
	electrical microstimulation in primary auditory cortex - implications
	for the subcortical-loop hypothesis},
  booktitle = {8th Meeting of the German Neuroscience Society},
  year = {2009},
  organization = {32st Goettingen Neurobiology Conference},
  note = {T18-12A},
  timestamp = {2009.10.23},
  url = {https://www.nwg-goettingen.de/2009/upload/file/Proceedings_Goettingen2009.pdf}
}

@INPROCEEDINGS{Havlena2009_2,
  author = {Havlena, M. and Ess, A. and Moreau, W. and Torii, A. and Jancosek,
	M. and Pajdla, T. and Gool, L. van},
  title = {AWEAR 2.0 System: Omni-directional Audio-Visual Data Acquisition
	and Processing},
  booktitle = {1st Workshop on Egocentric Vision, CVPR},
  year = {2009},
  pages = {49-56},
  address = {Miami Beach},
  month = {20 June},
  organization = {1st Workshop on Egocentric Visio},
  note = {workshop proceedings \& oral presentation},
  abstract = {We present a wearable audio-visual capturing system,
	
	termed AWEAR 2.0, along with its underlying vision components
	
	that allow robust self-localization, multi-body pedestrian
	
	tracking, and dense scene reconstruction. Designed as
	
	a backpack, the system is aimed at supporting the cognitive
	
	abilities of the wearer. In this paper, we focus on the design
	
	issues for the hardware platform and on the performance of
	
	the current state-of-the-art computer vision methods on the
	
	acquired sequences. We describe the calibration procedure
	
	of the two omni-directional cameras present in the system
	
	as well as a Structure-from-Motion pipeline that allows for
	
	stable multi-body tracking even from rather shaky video sequences
	
	thanks to ground plane stabilization. Furthermore,
	
	we show how a dense scene reconstruction can be obtained
	
	from the data acquired with the platform.},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5204361}
}

@INPROCEEDINGS{Havlena2009_1,
  author = {Havlena, M. and Torii, A. and Knopp, J. and Pajdla, T.},
  title = {Randomized Structure from Motion Based on Atomic 3D Models from Camera
	Triplets},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2009},
  address = {Miami Beach},
  month = {22-24 June},
  organization = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  note = {conference proceedings \& poster},
  abstract = {This paper presents a new efficient technique for large-scale structure
	from motion from unordered data sets. We avoid costly computation
	of all pairwise matches and geometries by sampling pairs of images
	using the pairwise similarity scores based on the detected occurrences
	of visual words leading to a significant speedup. Furthermore, atomic
	3D models reconstructed from camera triplets are used as the seeds
	which form the final large-scale 3D model when merged together. Using
	three views instead of two allows us to reveal most of the outliers
	of pairwise geometries at an early stage of the process hindering
	them from derogating the quality of the resulting 3D structure at
	later stages. The accuracy of the proposed technique is shown on
	a set of 64 images where the result of the exhaustive technique is
	known. Scalability is demonstrated on a landmark reconstruction from
	hundreds of images.},
  doi = {http://doi.ieeecomputersociety.org/10.1109/CVPRW.2009.5206677},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206677}
}

@INPROCEEDINGS{Havlena2009_3,
  author = {Havlena, M. and Torii, A. and Pajdla, T.},
  title = {Randomized Structure from Motion Based on Atomic 3D Models from Camera
	Triplets},
  booktitle = {Computer Vision Winter Workshop},
  year = {2009},
  address = {Eibiswald, Austria},
  organization = {TU Vienna},
  timestamp = {2010.02.05}
}

@CONFERENCE{Hengel2009_2,
  author = {Hengel, P. van and Anemüller, J.},
  title = {Audio Event Detection for In Home Care.},
  booktitle = {NAG-DAGA 2009 International Conference on Acoustics},
  year = {2009},
  pages = {209},
  address = {Rotterdam},
  month = {March, 23-26},
  organization = {NAG-DAGA International Conference on Acoustics},
  note = {abstract \& conference presentation},
  abstract = {One in eight persons attending hospital following an accident in the
	home,
	
	are aged 65 and over. This number will increase with the demographic
	
	change and the desire of the elderly to remain in their own homes.
	
	Hence, the demand for unobtrusive monitoring systems based on autonomous
	
	sensor technologies will increase. Such systems can limit observation
	
	by a human operator to cases when there is evidence of an
	
	incident, by responding to events in a human-like fashion. To do so,
	the
	
	system must pick up situations that also would attract human attention,
	
	including situations it has not encountered before. Hearing is an
	important
	
	sense in terms of steering the attention of a human observer. For
	
	this reason the detection of sound events that could indicate an incident
	
	is a key part of such a monitoring system. Formerly, an acoustic system
	
	was designed for the detection of verbal aggression. This technology
	is
	
	now expanded, combined with acoustic localization and integrated with
	
	video analysis and artificial intelligence. The resulting methods
	will be
	
	tested in the context of in-home care under real-life conditions.
	For this
	
	purpose a realistic test-home has been constructed.},
  url = {http://www.nag-daga.nl/NAG_DAGA_2009_Program.pdf}
}

@INBOOK{Hengel2009_1,
  chapter = {Human Factors - Security and Safety},
  title = {Sounds like trouble},
  publisher = {Shaker Publishing},
  year = {2009},
  editor = {Dick de Waard, Hans Godthelp, Frank Kooi and Karel Brookhuis},
  author = {Hengel, P. van and Huisman, M. and Appell, J.E.},
  note = {ISBN 978-90-423-0373-7, Paperback, 437 pages},
  abstract = {The newly founded project group Hearing, Speech and Audio Technology
	of the
	
	Fraunhofer Institute for Digital Media Technology IDMT focuses on
	applications
	
	based on models of the human hearing. One of the domains for application
	of these
	
	models is the detection of incidents in the surveillance domain based
	on audio
	
	information. For this application a system designed to detect human
	verbal
	
	aggression was developed by Sound Intelligence. A case study with
	this system will
	
	be described here as an example of what can be achieved using audio
	technology in a
	
	supportive role for surveillance. Models of human sound processing
	incorporating
	
	aspects of neural processing are now further developed to indicate
	a surveillance
	
	operator possible incidents or unexpected events.
	
	
	Introduction
	
	To improve the overall usefulness of camera-based surveillance systems
	it is
	
	important that situations with a high risk of injury and a relatively
	fast development,
	
	such as street-fights, collapses or accidents, are detected as quickly
	and as reliably as
	
	possible. Only then, appropriate action can be initiated. A system
	which can
	
	prioritize potentially dangerous situations autonomously and presents
	high priority
	
	events to a human observer for further analysis of the situation,
	would greatly reduce
	
	the chances of incidents being missed.
	
	An evaluation of CCTV in one of the London boroughs in 2004 revealed
	that only
	
	about 30% of incidents such as criminal damage and emergency incidents
	happening
	
	in view of a surveillance camera, were detected by CCTV (Gill & Hemming,
	2004).
	
	This rather low percentage is in contrast with the expectation of
	over half the people
	
	in residential areas believing that the police will respond quickly
	to incidents when
	
	CCTV cameras are installed (Spriggs et al., 2005).
	
	With each operator handling about 80 cameras on average, the chance
	of looking at
	
	the right place at the right time is low. As one of the CCTV operators
	is quoted in
	
	(Gill & Hemming, 2004): We are the eyes of the police but we dont
	know where to
	
	look?.},
  timestamp = {2010.01.21}
}

@CONFERENCE{Herrmann2009_2,
  author = {Herrmann, C. and Fruend, I. and Ohl, F.W.},
  title = {A biologically plausible network of spiking neurons can simulate
	human EEG responses},
  booktitle = {Bernstein Conference on Computational Neuroscience BCCN},
  year = {2009},
  pages = {160-161},
  organization = {Bernstein Conference on Computational Neuroscience BCCN},
  abstract = {Early gamma band responses (GBRs) of the human electroencephalogram
	(EEG) accompany sensory stimulation. These GBRs are modulated by
	exogenous stimulus properties such as size or contrast (size effect).
	In addition, cognitive processes modulate GBRs, e.g. if a subject
	has a memory representation of a perceived stimulus (known stimulus)
	the GBR is larger as if the subject had no such memory representation
	(unknown stimulus) (memory effect). Here, we simulate both effects
	in a simple random network of 1000 spiking neurons. The network was
	composed of 800 excitatory and 200 inhibitory Izhikevich neurons.
	During a learning phase, different stimuli were presented to the
	network, i.e. certain neurons received input currents. Synaptic connections
	were modified according to a spike timing dependent plasticity (STDP)
	learning rule. In a subsequent test phase, we stimulated the network
	with (i) patterns of different sizes to simulate the abovementioned
	size effect and (ii) with patterns that were or were not presented
	during the learning phase to simulate the abovementioned memory effect.
	In order to compute a simulated EEG from this network, the membrane
	voltage of all neurons was averaged. After about 1 hour of learning,
	the network displayed event-related responses. After 24 hours of
	learning, these responses were qualitatively similar to the human
	early GBRs. There was a general increase in response strength with
	increasing stimulus size and slightly stronger responses for learned
	stimuli. We demonstrated that within one neural architecture early
	GBRs can be modulated both by stimulus properties and by basal learning
	mechanisms mediated via spike timing dependent plasticity.},
  doi = {doi: 10.3389/conf.neuro.10.2009.14.111},
  timestamp = {2010.01.22}
}

@BOOK{Hermann2009_1,
  title = {Cognitive adequacy in brain-like intelligence, Creating Brain-like
	Intelligence},
  publisher = {Springer Verlag, Berlin, LNCS},
  year = {2009},
  editor = {Sendhoff, B and Koerner, E and Sporns, O and Ritter, H and Doya,
	K},
  author = {Herrmann, C.S. and Ohl, F.W.},
  volume = {5436/2009},
  pages = {314-327},
  note = {ISBN:978-3-642-00615-9},
  abstract = {A variety of disciplines have dealt with the design of intelligent
	algorithms --- among them Artificial Intelligence and Robotics. While
	some approaches were very successful and have yielded promising results,
	others have failed to do so which was -- at least partly -- due to
	inadequate architectures and algorithms that were not suited to mimic
	the behavior of biological intelligence. Therefore, in recent years,
	a quest for "brain-like" intelligence has arosen. Soft- and hardware
	are supposed to behave like biological brains -- ideally like the
	human brain. This raises the questions of what exactly defines the
	attribute brain-like, how can the attribute be implemented and how
	tested. This chapter suggests the concept of cognitive adequacy in
	order to get a rough estimate of how brain-like an algorithm behaves.},
  doi = {10.1007/978-3-642-00616-6_15}
}

@INPROCEEDINGS{Hurych2009_1,
  author = {Hurych, T. and Svoboda, J. and Trojanova, Y.},
  title = {Active Shape Model and Linear Predictors for Face Association Refinement},
  booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops},
  year = {2009},
  address = {Kyoto},
  organization = {ICCV 2009 IEEE International Workshop on Visual Surveillance},
  abstract = {This paper summarizes results of face association experiments on real
	low resolution data from airport and the Labeled faces in the Wild
	(LFW) database. The objective of experiments is to evaluate different
	face alignment methods and their contribution to face association
	as such. The first alignment method used is Sequential Learnable
	Linear Predictor (SLLiP), originally developed for object tracking.
	The second method is well known face alignment method Active Shape
	Model (ASM). Both methods are compared versus face association without
	alignment. In case of high resolution LFW database the ASM rapidly
	increases the association results, on the other hand for real low
	resolution airport data the SLLiP method brought more improvement
	than ASM.},
  timestamp = {2009.10.23},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5457473}
}

@ARTICLE{Ilango2009_1,
  author = {Ilango, A. and Wetzel, W. and Scheich, H. and Ohl, F.W.},
  title = {The combination of appetitive and aversive reinforcers and the nature
	of their interaction during auditory learning},
  journal = {Neuroscience},
  year = {2009},
  note = {in press},
  abstract = {Learned changes in behavior can be elicited by either appetitive or
	aversive reinforcers. It is, however, not clear whether the two types
	of motivation, (approaching appetitive stimuli and avoiding aversive
	stimuli) drive learning in the same or different ways, nor is their
	interaction understood in situations where the two types are combined
	in a single experiment. To investigate this question we have developed
	a novel learning paradigm for Mongolian gerbils, which not only allows
	rewards and punishments to be presented in isolation or in combination
	with each other, but also can use these opposite reinforcers to drive
	the same learned behavior. Specifically, we studied learning of tone-conditioned
	hurdle crossing in a shuttle box driven by either an appetitive reinforcer
	(brain stimulation reward) or an aversive reinforcer (electrical
	footshock), or by a combination of both. Combination of the two reinforcers
	potentiated speed of acquisition, led to maximum possible performance,
	and delayed extinction as compared to either reinforcer alone. Additional
	experiments, using partial reinforcement protocols and experiments
	in which one of the reinforcers was omitted after the animals had
	been previously trained with the combination of both reinforcers,
	indicated that appetitive and aversive reinforcers operated together
	but acted in different ways: in this particular experimental context,
	punishment appeared to be more effective for initial acquisition
	and reward more effective to maintain a high level of conditioned
	responses (CRs). The results imply that learning mechanisms in problem
	solving were maximally effective when the initial punishment of mistakes
	was combined with the subsequent rewarding of correct performance.},
  doi = {doi:10.1016/j.neuroscience.2010.01.010},
  keywords = {reinforcement learning; rodent; reward; punishment; shuttle-box},
  timestamp = {2010.01.22},
  url = {http://dx.doi.org/doi:10.1016/j.neuroscience.2010.01.010}
}

@CONFERENCE{Jie2009_2,
  author = {Jie, L. and Caputo, B. and Ferrari, V.},
  title = {Who's Doing What: Joint Modeling of Names and Verbs for Simultaneous
	Face and Pose Annotation},
  booktitle = {Advances in Neural Information Processing Systems 22 (NIPS09)},
  year = {2009},
  address = {Vancouver},
  month = {December},
  owner = {sita},
  timestamp = {2011.03.17},
  url = {http://publications.idiap.ch/index.php/publications/show/1749}
}

@ARTICLE{Jie2009_1,
  author = {Jie, L. and Orabona, F. and Caputo, B.},
  title = {An online framework for learning novel concepts over multiple cues},
  journal = {Proceedings of Asian Conference on Computer Vision (ACCV)},
  year = {2009},
  volume = {1},
  pages = {1-12},
  note = {accepted for publication},
  abstract = {We propose an online learning algorithm to tackle the problem
	
	of learning under limited computational resources in a teacher-student
	
	scenario, over multiple visual cues. For each separate cue, we train
	an
	
	online learning algorithm that sacrifices performance in favor of
	bounded
	
	memory growth and fast update of the solution. We then recover back
	
	performance by using multiple cues in the online setting. To this
	end,
	
	we use a two-layers structure. In the first layer, we use a budget
	online
	
	learning algorithm for each single cue. Thus, each classifier provides
	confidence
	
	interpretations for target categories. On top of these classifiers,
	
	a linear online learning algorithm is added to learn the combination
	of
	
	these cues. As in standard online learning setups, the learning takes
	place
	
	in rounds. On each round, a new hypothesis is estimated as a function
	
	of the previous one. We test our algorithm on two student-teacher
	experimental
	
	scenarios and in both cases results show that the algorithm
	
	learns the new concepts in real time and generalizes well.},
  url = {http://publications.idiap.ch/index.php/publications/show/1710}
}

@PHDTHESIS{Karafiat2009_1,
  author = {Karafiát, Martin},
  title = {Study of linear transformations applied to training of cross-domain
	adapted large vocabulary continuous speech recognition systems},
  year = {2009},
  address = {Brno, CZ},
  abstract = {This thesis investigates into two important issues of acoustic modeling
	for automatic speech recognition (ASR). The first topic are robust
	discriminative transforms in feature extraction. Two approaches of
	smoothing the popular Heteroscedastic Linear Discriminant Analysis
	(HLDA) were investigated: Smoothed HLDA (SHLDA) and Maximum A-Posteriori
	(MAP) adapted SHLDA. Both variants perform better than the basic
	HLDA. Moreover, we have found, that removing the silence class from
	the HLDA estimations (Silence-reduced HLDA) is equally effective
	and cheaper in computation. The second part deals with using heterogeneous
	data resources in ASR training. For a task, where little data is
	available for the target domain (meetings -- 16kHz ``wide-band''
	(WB) speech), techniques that allow to make use of abundant data
	from other domain, yet different in the acoustic channel (telephone
	data -- 8kHz ``narrow-band'' -- NB) were investigated. We successfully
	implemented an adaptation with WB data transformed to the NB domain
	based on Constrained Maximum Likelihood Linear Regression (CMLLR).
	A solution of how to apply this transform for HLDA and speaker-adaptive
	trained (SAT) systems was given using maximum likelihood. Finally,
	integration of this method with discriminative approaches was investigated
	and successfully solved. All experimental results are presented on
	standard data from NIST Rich Transcription (RT) 2005 evaluations.},
  institution = {Brno University of Technology, CZ},
  language = {english},
  pages = {73},
  url = {http://www.fit.vutbr.cz/research/view_pub.php?id=8782}
}

@ARTICLE{ISI:000269041800001,
  author = {Kayser, H. and Ewert, S. D. and Anemüller, J. and Rohdenburg, T.
	and Hohmann, V. and Kollmeier, B.},
  title = {Database of Multichannel In-Ear and Behind-the-Ear Head-Related and
	Binaural Room Impulse Responses},
  journal = {EURASIP Journal on Advances in Signal Processing},
  year = {2009},
  volume = {2009},
  pages = {1-10},
  note = {Article ID 298605},
  abstract = {An eight-channel database of head-related impulse responses (HRIRs)
	and binaural room impulse responses (BRIRs) is introduced. The impulse
	responses (IRs) were measured with three-channel behind-the-ear (BTEs)
	hearing aids and an in-ear microphone at both ears of a human head
	and torso simulator. The database aims at providing a tool for the
	evaluation of multichannel hearing aid algorithms in hearing aid
	research. In addition to the HRIRs derived from measurements in an
	anechoic chamber, sets of BRIRs for multiple, realistic head and
	sound-source positions in four natural environments reflecting daily-life
	communication situations with different reverberation times are provided.
	For comparison, analytically derived IRs for a rigid acoustic sphere
	were computed at the multichannel microphone positions of the BTEs
	and differences to real HRIRs were examined. The scenes' natural
	acoustic background was also recorded in each of the real-world environments
	for all eight channels. Overall, the present database allows for
	a realistic construction of simulated sound fields for hearing instrument
	research and, consequently, for a realistic evaluation of hearing
	instrument algorithms. Copyright (C) 2009 H. Kayser et al.},
  article-number = {298605},
  doi = {10.1155/2009/298605},
  issn = {1687-6172},
  unique-id = {ISI:000269041800001},
  url = {http://dx.doi.org/10.1155/2009/298605}
}

@CONFERENCE{Kayser2009_2,
  author = {Kayser, H. and Kollmeier, B. and Anemüller, J.},
  title = {Blind and Non-Blind Spatial Signal Processing Using Head-Related
	Impulse Responses},
  booktitle = {NAG-DAGA International Conference on Acoustics},
  year = {2009},
  address = {Rotterdam},
  month = {March, 23-26},
  organization = {NAG/DAGA International Conference on Acoustics},
  note = {abstract \& conference presentation},
  abstract = {Spatial signal processing for hearing aids is of particular importance
	in
	
	acoustically challenging environments. The development and testing
	of
	
	algorithms performing digital signal processing, e.g., blind source
	separation
	
	(BSS), beamforming and direction-of-arrival estimation requires
	
	realistic test conditions. For the evaluation of such blind and non-blind
	
	algorithms, virtual sound fields are generated by convolution of sound
	
	signals with impulse responses measured for different spatial source
	positions.
	
	By the use of head-related impulse responses measured in real
	
	scenarios, a simulation is achieved that approximates real-world recordings.
	
	For this purpose, multi-channel hearing aids were mounted on a
	
	head and torso simulator. Hence, the number and type of superimposed
	
	sound sources can be chosen arbitrarily. Background noise recorded
	in
	
	the underlying scene can be added with a selectable SNR. Due to the
	
	knowledge of the entire transmission characteristics from each source
	to
	
	each microphone, optimal linear filtering techniques are applicable.
	The
	
	results achieved by multi-channel Wiener filtering serve as benchmark
	
	for practical filtering algorithms having less knowledge about the
	underlying
	
	situation. A BSS approach is applied to multiple mixtures of speech
	
	signals in anechoic conditions and an office room. The quality of
	separation
	
	is evaluated subject to the spatial arrangement of the sources.}
}

@TECHREPORT{Knopp2009,
  author = {Knopp, J. and Sivic, J. and Pajdla, T.},
  title = {Location recognition using large vocabularies and fast spatial matching},
  institution = {Willow \& CTU Research report},
  year = {2009},
  month = {January},
  url = {ftp://cmp.felk.cvut.cz/pub/cmp/articles/knopp/Knopp-TR-2009-01.pdf}
}

@CONFERENCE{Kombrink2009_1,
  author = {Kombrink, S. and Burget, L. and Matejka, P. and Karafiat, M. and
	Hermansky, H.},
  title = {Posterior-based Out of Vocabulary Word Detection in Telephone Speech},
  booktitle = {Interspeech 2009},
  year = {2009},
  editor = {ISCA},
  pages = {80-83},
  address = {Brighton, GB},
  organization = {Interspeech 2009},
  note = {ISSN 1990-9772},
  abstract = {In this paper we present an out-of-vocabulary word detector suitable
	for English conversational and read speech. We use an approach based
	on phone posteriors created by a Large Vocabulary Continuous Speech
	Recognition system and an additional phone recognizer, that allows
	detection of OOV and misrecognized words. In addition, the recognized
	word output can be transcribed more detailed using several classes.
	Reported results are on CallHome English and Wall Street Journal
	data.},
  timestamp = {2009.10.23},
  url = {http://www.fit.vutbr.cz/research/groups/speech/publi/2009/kombrink_is2009.pdf}
}

@CONFERENCE{Meyer2009_2,
  author = {Meyer, A.F. and Happel, M.F.K. and Ohl, F.W. and Anemüller, J.},
  title = {Estimation of spectro-temporal receptive fields based on linear support
	vector machine classification},
  booktitle = {3rd International Conference on Auditory Cortex},
  year = {2009},
  address = {Magdeburg},
  organization = {3rd Int. Conf. on Auditory Cortex},
  note = {P068},
  abstract = {Introduction and Methods: The spectro-temporal
	
	receptive field (STRF) of a neuron is defined as
	
	the linear filter that, when convolved with the
	
	spectro-temporal representation of an arbitrary
	
	stimulus, gives a linear estimate of the evoked
	
	firing rate [2]. A common method of STRF estimation
	
	uses the spike-triggered average (STA) that
	
	computes the mean stimulus pattern preceding
	
	every spike.
	
	Here, we present a method that does not only
	
	consider stimulus patterns that evoke spikes but
	
	also those after which no spikes occur. This results
	
	in a binary classification problem.
	
	We show that the STRF model is equivalent to the
	
	structure of a linear support vector machine
	
	(SVM) which we propose for the estimation of the
	
	STRF. Based on this approach, we demonstrate
	
	that the obtained STRFs are a better predictor for
	
	spiking and non-spiking behavior of a neuron.
	
	Experiments and Results: The SVM is trained
	
	using real spike data from anesthetized gerbils
	
	[3] and zebra finches [4]. The parts of the stimulus
	
	spectrogram preceding a spike are labeled as
	
	class 1, whereas the remaining (non spike-evoking)
	
	parts are labeled as class 0. We used 80% of
	
	the data for training and 20% for prediction.
	
	In comparison to classic STA estimation, the
	
	method proposed here is characterized by a
	
	notably finer structure in the temporal evolution
	
	of spike rate prediction. In particular the non spikeeliciting
	
	time intervals are better captured by
	
	the novel approach. This behaviour is likely a
	
	result of the learning procedure employed which
	
	is based on the binary classification paradigm
	
	with a linear classifier. The averaging approach
	
	of the STA results in smoother estimates for the
	
	neuronal receptive field (due to the temporal lowpass
	
	envelope characteristics of natural stimuli),
	
	consequently producing less-detailed spike rate
	
	predictions.},
  timestamp = {2009.10.23},
  url = {http://www.auditory-cortex.de/assets/pdf/AC2009_Program_End.pdf}
}

@ARTICLE{Meyer2009_3,
  author = {Meyer, Arne and Happel, Max and Ohl, Frank and Anemüller, Jörn},
  title = {Estimation of spectro-temporal receptive fields based on linear support
	vector machine classification},
  journal = {BMC Neuroscience},
  year = {2009},
  volume = {10},
  pages = {147},
  number = {10(Suppl 1)},
  abstract = {The spectro-temporal receptive field (STRF) of a neuron is defined
	as the linear filter that, when convolved with the spectro-temporal
	representation of an arbitrary stimulus, gives a linear estimate
	of the evoked firing rate [1]. A common method for STRF estimation
	uses the spike-triggered average (STA) to compute the mean stimulus
	pattern preceding every spike.
	
	
	Here, we present a method that not only considers stimulus patterns
	that evoke spikes but also those after which no spikes occur. This
	results in a binary classification problem. We show that the STRF
	model is equivalent to the structure of a linear support vector machine
	(SVM) and propose the use of SVMs for the estimation of the STRF.
	Based on this approach, we demonstrate that the obtained STRFs are
	a better predictor for spiking and non-spiking behavior of a neuron.},
  doi = {10.1186/1471-2202-10-S1-P147},
  issn = {1471-2202},
  timestamp = {2009.10.23},
  url = {http://www.biomedcentral.com/1471-2202/10/S1/P147}
}

@INPROCEEDINGS{Nater2009_1,
  author = {Nater, F. and Grabner, H. and Jaeggli, T. and Gool, L. van},
  title = {Tracker trees for unusual event detection},
  booktitle = {ICCV 2009 Workshop on Visual Surveillance},
  year = {2009},
  organization = {IEEE Workshop on Visual Surveillance},
  note = {accepted},
  abstract = {We present an approach for unusual event detection,
	
	based on a tree of trackers. At lower levels, the trackers
	
	are trained on broad classes of targets. At higher levels,
	
	they aim at more specific targets. For instance, at the root,
	
	a general blob tracker could operate which may track any
	
	object. The next level could already use information about
	
	human appearance to better track people. A further level
	
	could go after specific types of actions like walking, running,
	
	or sitting. Yet another level up, several walking trackers
	
	can be tuned to the gait of a particular person each.
	
	Thus, at each layer, one or more families of more specific
	
	trackers are available. As long as the target behaves according
	
	to expectations, a member of a higher up such family
	
	will be better tuned to the data than its parent tracker
	
	at a lower level. Typically, a better informed tracker performs
	
	more robustly. But in cases where unusual events occur
	
	and the normal assumptions about the world no longer
	
	hold, they loose their reliability. In such cases, a less informed
	
	tracker, not relying on what has now become false
	
	information, has a good chance of performing better. Such
	
	performance inversion signals an unusual event. Inversions
	
	between levels higher up represent deviations that are semantically
	
	more subtle than inversions lower down: for instance
	
	an unknown intruder entering a house rather than
	
	seeing a non-human target.},
  url = {http://www.vision.ee.ethz.ch/publications/get_abstract.cgi?procs=669&mode=&lang=en}
}

@INPROCEEDINGS{Noceti2009_1,
  author = {Noceti, N. and Caputo, B. and Castellini, C. and Baldassarre, L.
	and Barla, A. and Rosasco, L. and Odone, F. and Sandini, G.},
  title = {Towards a theoretical framework for learning multi-modal patterns
	for embodied agents},
  booktitle = {LNCS - Image Analysis and Processing (ICIAP 2009)},
  year = {2009},
  volume = {5716/2009},
  pages = {239-248},
  organization = {International Conference on Image Analysis and Processing (ICIAP)},
  publisher = {Springer Berlin / Heidelberg},
  note = {ISBN 978-3-642-04145-7},
  abstract = {Multi-modality is a fundamental feature that characterizes biological
	systems and lets them achieve high robustness in understanding skills
	while coping with uncertainty. Relatively recent studies showed that
	multi-modal learning is a potentially effective add-on to artificial
	systems, allowing the transfer of information from one modality to
	another. In this paper we propose a general architecture for jointly
	learning visual and motion patterns: by means of regression theory
	we model a mapping between the two sensorial modalities improving
	the performance of artificial perceptive systems. We present promising
	results on a case study of grasp classification in a controlled setting
	and discuss future developments.},
  doi = {10.1007/978-3-642-04146-4_27},
  url = {http://dx.doi.org/10.1007/978-3-642-04146-4_27}
}

@INPROCEEDINGS{Ohl2009_2,
  author = {Ohl, F.W.},
  title = {Neurodynamics in auditory cortex during categorization and rare-event
	processing},
  booktitle = {Neuromorphic Workshop},
  year = {2009},
  address = {Telluride, CO, USA},
  month = {July 9},
  organization = {Telluride Neuromorphic Workshop},
  timestamp = {2010.02.07},
  url = {https://neuromorphs.net/ws2009}
}

@BOOK{Ohl2009_1,
  title = {The role of neuronal populations in auditory cortex for learning
	- Information processing by neuronal populations},
  publisher = {Cambridge University Press},
  year = {2009},
  editor = {Holscher, C. and Munk, M.},
  author = {Ohl, F.W. and Scheich, H.},
  pages = {224-246},
  doi = {10.1017/CBO9780511541650.010},
  url = {http://cambridge.org/catalogue/catalogue.asp?isbn=9780521873031}
}

@INPROCEEDINGS{Orabona2009_3,
  author = {Orabona, F. and Caputo, B. and Fillbrandt, A. and Ohl, F.W.},
  title = {A Theoretical Framework for Transfer of Knowledge Across Modalities
	in Artificial and Biological Systems},
  booktitle = {IEEE 8th International Conference on Development and Learning, 2009.
	ICDL 2009.},
  year = {2009},
  organization = {Proc. ICDL},
  note = {art. no. 5175515},
  url = {http://www.icdl09.org/index_files/Program.htm}
}

@INPROCEEDINGS{Orabona2009_ICRA,
  author = {Orabona, F. and Castellini, C. and Caputo, B. and Fiorilla, A. and
	E. and Sandini G.},
  title = {Model adaptation with least-squares SVM for adaptive hand prosthetics},
  booktitle = {International Conference on Robotics and Automation (ICRA) 2009},
  year = {2009},
  organization = {IEEE International Conference on Robotics and Automation (ICRA)},
  note = {accepted for publication},
  abstract = {The state-of-the-art in control of hand prosthetics is far from optimal.
	The main control interface is represented by surface electromyography
	(EMG): the activation potentials of the remnants of large muscles
	of the stump are used in a non-natural way to control one or, at
	best, two degrees-of-freedom. This has two drawbacks: first, the
	dexterity of the prosthesis is limited, leading to poor interaction
	with the environment; second, the patient undergoes a long training
	time. As more dexterous hand prostheses are put on the market, the
	need for a finer and more natural control arises. Machine learning
	can be employed to this end. A desired feature is that of providing
	a pre-trained model to the patient, so that a quicker and better
	interaction can be obtained. To this end we propose model adaptation
	with least-squares SVMs, a technique that allows the automatic tuning
	of the degree of adaptation. We test the effectiveness of the approach
	on a database of EMG signals gathered from human subjects. We show
	that, when pre-trained models are used, the number of training samples
	needed to reach a certain performance is reduced, and the overall
	performance is increased, compared to what would be achieved by starting
	from scratch.},
  url = {https://ras.papercept.net/conferences/scripts/abstract.pl?ConfID=18&Number=176}
}

@ARTICLE{Orabona2009_2,
  author = {Orabona, F. and Castellini, C. and Caputo, B. and Luo, J. and Sandini,
	G.},
  title = {Towards Life-long Learning for Cognitive Systems: Online Independent
	Support Vector Machine},
  journal = {Pattern Recognition},
  year = {2009},
  volume = {43},
  pages = {1402-1412},
  number = {4},
  abstract = {Support vector machines (SVMs) are one of the most successful algorithms
	for classification. However, due to their space and time requirements,
	they are not suitable for on-line learning, that is, when presented
	with an endless stream of training observations.
	
	In this paper we propose a new on-line algorithm, called on-line independent
	support vector machines (OISVMs), which approximately converges to
	the standard SVM solution each time new observations are added; the
	approximation is controlled via a user-defined parameter. The method
	employs a set of linearly independent observations and tries to project
	every new observation onto the set obtained so far, dramatically
	reducing time and space requirements at the price of a negligible
	loss in accuracy. As opposed to similar algorithms, the size of the
	solution obtained by OISVMs is always bounded, implying a bounded
	testing time. These statements are supported by extensive experiments
	on standard benchmark databases as well as on two real-world applications,
	namely place recognition by a mobile robot in an indoor environment
	and human grasping posture classification.},
  doi = {10.1016/j.patcog.2009.09.021},
  url = {http://dx.doi.org/10.1016/j.patcog.2009.09.021}
}

@ARTICLE{Orabona2009_1,
  author = {Orabona, F. and Keshet, J. and Caputo, B.},
  title = {Bounded kernel-based perceptrons},
  journal = {Journal of Machine Learning Research},
  year = {2009},
  volume = {10},
  pages = {2643−2666},
  month = {November},
  abstract = {A common problem of kernel-based online algorithms, such as the kernel-based
	Perceptron algorithm, is the amount of memory required to store the
	online hypothesis, which may increase without bound as the algorithm
	progresses. Furthermore, the computational load of such algorithms
	grows linearly with the amount of memory used to store the hypothesis.
	To attack these problems, most previous work has focused on discarding
	some of the instances, in order to keep the memory bounded. In this
	paper we present a new algorithm, in which the instances are not
	discarded, but are instead projected onto the space spanned by the
	previous online hypothesis. We call this algorithm Projectron. While
	the memory size of the Projectron solution cannot be predicted before
	training, we prove that its solution is guaranteed to be bounded.
	We derive a relative mistake bound for the proposed algorithm, and
	deduce from it a slightly different algorithm which outperforms the
	Perceptron. We call this second algorithm Projectron++. We show that
	this algorithm can be extended to handle the multiclass and the structured
	output settings, resulting, as far as we know, in the first online
	bounded algorithm that can learn complex classification tasks. The
	method of bounding the hypothesis representation can be applied to
	any conservative online algorithm and to other online algorithms,
	as it is demonstrated for ALMA2. Experimental results on various
	data sets show the empirical advantage of our technique compared
	to various bounded online algorithms, both in terms of memory and
	accuracy.},
  url = {http://jmlr.csail.mit.edu/papers/volume10/orabona09a/orabona09a.pdf}
}

@TECHREPORT{Pajdla-TR-2009-19,
  author = {Pajdla, Tom{\'a}{\v s} and Havlena, Michal and Heller, Jan and Kayser,
	Hendrik and Bach, J{\"o}rg-Hendrik and Anem{\"u}ller, J{\"o}rn},
  title = {Incongruence Detection for Detecting, Removing, and Repairing Incorrect
	Functionality in Low-Level Processing},
  institution = {CTU Research Report},
  year = {2009},
  number = {CTU--CMP--2009--19},
  abstract = {The theory of incongruence, which deals with inconsistent decisions
	of direct and composite classifiers of the same concept, can be used
	to improve low-level processing by detecting incorrect functionality
	and repairing it through re-defining the composite classifier. In
	this report, we summarize the advancements in the direct audio and
	the direct audio-visual classifiers yielding two speaker detectors
	which can cause an incongruence. Then, we show how this incongruence
	could be used for learning a new concept, direct position consistency
	classifier, which can be used to re-define the composite speaker
	classifier.},
  url = {ftp://cmp.felk.cvut.cz/pub/cmp/articles/pajdla/Pajdla-TR-2009-19.pdf}
}

@CONFERENCE{Schaer2009_1,
  author = {Schaer, T. and Ewert, S.D. and Anemueller, J. and Kollmeier, B.},
  title = {Measurement, modelling and compensation of nonlinearities in hearing
	aid receivers},
  booktitle = {NAG-DAGA International Conference on Acoustics 2009},
  year = {2009},
  address = {Rotterdam},
  month = {March, 23-26},
  organization = {NAG/DAGA International Conference on Acoustics},
  note = {abstract \& conference presentation (P. 208-209)},
  abstract = {Hearing aid receivers operate over a large dynamic range and reach
	
	output levels of up to 130 dB. Electromagnetic receivers are commonly
	
	used in hearing aids since they offer a very high efficiency. Receivers
	
	are typically characterized by their linear transfer function. These
	linear
	
	distortions are easily measured with standard methods and if compensation
	
	is applied it is often limited to the magnitude transfer function.
	
	The current study addresses a more complete characterization of the
	
	hearing aid receiver by measurement of the nonlinear transfer function.
	
	Nonlinear distortions of the receiver are output level dependent and
	
	might negatively influence measures like speech intelligibility of
	an aided
	
	hearing impaired person, particularly at high levels. Moreover, nonlinear
	
	distortions are problematic in the context of future high-fidelity
	hearing
	
	aids and compensation methods are of interest. Here, a fast and
	
	efficient sinesweep method proposed by Farina was employed to estimate
	
	the linear and nonlinear transfer function. The measurement was
	
	repeated at different output levels. From the data, nonlinear input-output
	
	functions can be constructed or alternatively, the diagonal elements
	of a
	
	Volterra-series based description of the nonlinear transfer function
	can
	
	be derived. A nonlinear receiver model and compensation methods are
	
	suggested and tested.},
  url = {http://www.nag-daga.nl/NAG_DAGA_2009_Program.pdf}
}

@PHDTHESIS{Schwarz2009_1,
  author = {Schwarz, Petr},
  title = {Phoneme recognition based on long temporal context},
  year = {2009},
  institution = {Brno University of Technology, CZ},
  language = {english},
  pages = {95},
  url = {http://www.fit.vutbr.cz/research/view_pub.php?id=9132}
}

@ARTICLE{Shumake2009_1,
  author = {Shumake, J. and Ilango, A. and Scheich, H. and Wetzel, W. and Ohl,
	F.W.},
  title = {Acquisition and Retrieval of Avoidance Learning by the Lateral Habenula
	and Ventral Tegmental Area},
  journal = {Journal of Neuroscience},
  year = {2009},
  note = {in press},
  timestamp = {2010.01.22}
}

@INPROCEEDINGS{Shumake2009_2,
  author = {Shumake, J.D. and Ilango, A. and Scheich, H. and Wetzel, W. and Ohl,
	F.W.},
  title = {Differential neuromodulation of acquisition and retrieval of avoidance
	learning by the lateral habenula and ventral tegmental area},
  booktitle = {Society for Neuroscience},
  year = {2009},
  note = {Abstr. 877.10},
  timestamp = {2010.02.07}
}

@INPROCEEDINGS{Takagaki2009_1,
  author = {Takagaki, K. and Ohl, F.W.},
  title = {Cortical plasticity of audiovisual mass action},
  booktitle = {International Multisensory Research Forum},
  year = {2009},
  organization = {International Multisensory Research Forum (IMRF), 9th Annual Meeting},
  abstract = {Many brain regions are now known to respond to input from multiple
	sensory modalities1. Even primary sensory areas, which were previously
	thought to respond almost exclusively to area-specific inputs, are
	now known to reflect multimodal sensory input. These multimodal responses
	manifest as either physiological response to non-area-specific stimulation
	alone, or as modulatory enhancement of the area-specific response
	by non-area-specific stimulation. However, it is unknown whether
	such multimodal responsivity is modulated by sensory experience.
	Here we show that multimodal population activity can be altered by
	patterned audiovisual input. By using voltage-sensitive dye imaging
	in the Mongolian gerbil (Meriones unguiculatus), we demonstrate that
	trains of patterned audiovisual stimulation can alter the spatial
	distribution, propagation patterns and magnitude of population mass
	action in the mammalian cortex. This modulation depends upon timing
	contingencies of the auditory and visual stimuli within the patterned
	stimulus trains. Our results demonstrate, for the first time, that
	sensory input can induce plasticity in the spatial organization of
	cortical mass action. These results further suggest that propagating
	patterns of population mass action may play a role in integrating
	information from multiple sensory modalities.},
  timestamp = {2010.01.22},
  url = {http://imrf.mcmaster.ca/IMRF/ocs/index.php/meetings/2008/paper/view/234}
}

@INPROCEEDINGS{Takagaki2009_2,
  author = {Takagaki, K. and Wanger, T. and Ohl, F.W.},
  title = {State-dependent patterns of interareal and intraareal oscillatory
	coupling in gerbil auditory cortex},
  booktitle = {3rd International Conference on Auditory Cortex},
  year = {2009},
  pages = {144},
  organization = {3rd Int. Conf. on Auditory Cortex},
  abstract = {Oscillatory population activity is ubiquitous in the
	
	mammalian neocortex. Oscillations are present
	
	across many frequency bands, and are postulated
	
	to reflect the integration of information
	
	over neural populations. Such oscillations can
	
	be evoked by sensory stimuli, and neocortical
	
	populations can exhibit distinct forms of resonance
	
	to the stimuli.
	
	While the tonotopic distribution of average neuronal
	
	firing rates, local field potentials, and slow
	
	intrinsic metabolic measures in auditory cortex
	
	has been well-documented, the distribution and
	
	spatial coupling of higher-frequency oscillations
	
	of auditory population activity is unexplored.
	
	Here, we investigate sensory-evoked population
	
	oscillations in the gerbil auditory cortex, using
	
	voltage-sensitive dye imaging with high signal-tonoise
	
	ratio. This method allows us to observe fast
	
	population oscillations with a temporal sampling
	
	rate of 1.6 kHz and a spatial sampling of approx.
	
	150 μm. Auditory stimulation with different frequencies
	
	and bandwidths evoke distinct spatial
	
	patterns of oscillation and spatial coupling, both
	
	within primary and higher cortical areas.
	
	To investigate the state-dependence of this coupling,
	
	we recorded under a carefully modulated
	
	biphasic anesthetic state, modeling REM sleep
	
	and slow-wave sleep. The location and spatial
	
	extent of these oscillations and their coupling
	
	depends greatly on cortical state, with the REMlike
	
	state (awake-like intracortical state) showing
	
	more dynamic coupling patterns.},
  comment = {P 105},
  timestamp = {2010.01.22},
  url = {http://www.auditory-cortex.de/assets/pdf/AC2009_Program_End.pdf}
}

@INPROCEEDINGS{Tommasi2009_1,
  author = {Tommasi, T. and Caputo, B.},
  title = {The more you know, the less you learn: from knowledge transfer to
	one-shot learning of object categories},
  booktitle = {British Machine Vision Conference (BMVC) 2009},
  year = {2009},
  organization = {British Machine Vision Conference 2009 (BMVC)},
  abstract = {Learning a category from few examples is a challenging task for vision
	algorithms,
	
	while psychological studies have shown that humans are able to generalise
	correctly
	
	even from a single instance (one-shot learning). The most accredited
	hypothesis is that
	
	humans are able to exploit prior knowledge when learning a new related
	category. This
	
	paper presents an SVM-based model adaptation algorithm able to perform
	knowledge
	
	transfer for a new category when very limited examples are available.
	Using a leaveone-
	
	out estimate of the weighted error-rate the algorithm automatically
	decides from
	
	where to transfer (on which known category to rely), how much to transfer
	(the degree
	
	of adaptation) and if it is worth transferring something at all. Moreover
	a weighted
	
	least-squares loss function takes optimally care of data unbalance
	between negative and
	
	positive examples. Experiments presented on two different object category
	databases
	
	show that the proposed method is able to exploit previous knowledge
	avoiding negative
	
	transfer. The overall classification performance is increased compared
	to what would
	
	be achieved by starting from scratch. Furthermore as the number of
	already learned
	
	categories grows, the algorithm is able to learn a new category from
	one sample with
	
	increasing precision, i.e. it is able to perform one-shot learning.},
  url = {http://www.bmva.org/bmvc/2009/Papers/Paper353/Paper353.pdf}
}

@TECHREPORT{Torii-TR-2009-20,
  author = {Torii, Akihiko and Havlena, Michal and Pajdla, Tom{\'a}{\v s}},
  title = {Camera Tracking and Autocalibration for Detecting and Correcting
	Camera De-Calibration},
  institution = {CTU Research Report},
  year = {2009},
  number = {CTU--CMP--2009--20},
  abstract = {In this report, we present several contributions to the dynamic 3D
	scene analysis supported by image and video processing from omnidirectional
	video data acquired by the AWEAR 2.0 platform. First, we summarize
	the upgrades of our structure from motion (SfM) pipelines for the
	autocalibration of the AWEAR 2.0 camera platform. Next, we examine
	several examples of detecting abnormal situations using statistics
	resulted from camera tracking: (i) feature detection, (ii) sequential
	matching, and (iii) stereo matching on the top of SfM. Finally, we
	demonstrate the detection and classification of abnormal situations,
	and correction of the contaminated camera calibrations according
	to the abnormal events on real video sequences.},
  url = {ftp://cmp.felk.cvut.cz/pub/cmp/articles/torii/Torii-TR-2009-20.pdf}
}

@ARTICLE{Torii2009_1,
  author = {Torii, A. and Havlena, M. and Pajdla, T.},
  title = {Omnidirectional image stabilization by computing camera trajectory},
  journal = {LNCS Computer Science (PSIVT'09) - Advances in Image and Video Technology},
  year = {2009},
  volume = {5414},
  pages = {71-82},
  note = {ISBN 978-3-540-92956-7},
  abstract = {In this paper we present a pipeline for camera pose and trajectory
	estimation, and image stabilization and rectification for dense as
	well as wide baseline omnidirectional images. The input is a set
	of images taken by a single hand-held camera. The output is a set
	of stabilized and rectified images augmented by the computed camera
	3D trajectory and reconstruction of feature points facilitating visual
	object recognition. The paper generalizes previous works on camera
	trajectory estimation done on perspective images to omnidirectional
	images and introduces a new technique for omnidirectional image rectification
	that is suited for recognizing people and cars in images. The performance
	of the pipeline is demonstrated on a real image sequence acquired
	in urban as well as natural environments.},
  doi = {10.1007/978-3-540-92957-4_7},
  url = {http://www.springerlink.com/content/d3m2183624214167/fulltext.pdf}
}

@INPROCEEDINGS{Torii2009_2,
  author = {Torii, Akihiko and Havlena, Michal and Pajdla, Tomas},
  title = {Omnidirectional Image Stabilization by Computing Camera Trajectory},
  booktitle = {The 3rd Pacific-Rim Symposium on Image and Video Technology},
  year = {2009},
  volume = {5414},
  pages = {71-82},
  address = {Tokyo, Japan},
  month = {13-16 January},
  abstract = {In this paper we present a pipeline for camera pose and trajectory
	estimation, and image stabilization and rectification for dense as
	well as wide baseline omnidirectional images. The input is a set
	of images taken by a single hand-held camera. The output is a set
	of stabilized and rectified images augmented by the computed camera
	3D trajectory and reconstruction of feature points facilitating visual
	object recognition. The paper generalizes previous works on camera
	trajectory estimation done on perspective images to omnidirectional
	images and introduces a new technique for omnidirectional image rectification
	that is suited for recognizing people and cars in images. The performance
	of the pipeline is demonstrated on a real image sequence acquired
	in urban as well as natural environments.},
  timestamp = {2010.02.06},
  url = {http://portal.acm.org/citation.cfm?id=1505942.1505951}
}

@ARTICLE{Torii2009_3,
  author = {Torii, A. and Havlena, M. and Pajdla, T.},
  title = {From Google Street View to 3D City Models},
  journal = {The ICCV 2009 IEEE Workshop on Omnidirectional Vision, Camera Networks
	and Non-Classical Cameras. Kyoto},
  year = {2009},
  timestamp = {2009.10.23},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5457551}
}

@INPROCEEDINGS{Ullah2009_1,
  author = {Ullah, M. M. and Orabona, F. and Caputo, B.},
  title = {You Live, You Learn, You Forget: Continuous Learning of Visual Places
	with a Forgetting Mechanism},
  booktitle = {International Conference on Intelligent Robots and Systems 2009},
  year = {2009},
  organization = {International Conference on Intelligent Robots and Systems},
  abstract = {To fulfill the dream of having autonomous robots at home, there is
	a need for spatial representations augmented with semantic concepts.
	Vision has emerged recently as the key modality to recognize semantic
	categories like places (office, corridor, kitchen, etc). A crucial
	aspect of these semantic place representations is that they change
	over time, due to the dynamism of the world. This call for visual
	algorithms able to learn from experience while at the same time managing
	the continuous flow of incoming data. This paper addresses these
	issues by presenting an SVM-based algorithm able to (a) learn continuously
	from experience with a fast updating rule, and (b) control the memory
	growth via a random forgetting mechanism while at the same time preserving
	an accuracy comparable to that of the batch algorithm. We apply our
	method to two different scenarios where learning from experience
	plays an important role: (1) continuous learning of visual places
	under dynamic changes, and (2) knowledge transfer of visual concepts
	across robot platforms. For both scenarios, results confirm the effectiveness
	of our approach.},
  url = {https://ras.papercept.net/conferences/scripts/abstract.pl?ConfID=20&Number=512}
}

@ARTICLE{Vangeneugden2009_1,
  author = {Vangeneugden, Joris and Pollick, Frank and Vogels, Rufin},
  title = {Functional differentiation of macaque visual temporal cortical neurons
	using a parametric action space},
  journal = {Cerebral Cortex},
  year = {2009},
  volume = {19(3)},
  pages = {593-611},
  abstract = {Neurons in the rostral superior temporal sulcus (STS) are responsive
	to displays of body movements. We employed a parametric action space
	to determine how similarities among actions are represented by visual
	temporal neurons and how form and motion information contributes
	to their responses. The stimulus space consisted of a stick-plus-point-light
	figure performing arm actions and their blends. Multidimensional
	scaling showed that the responses of temporal neurons represented
	the ordinal similarity between these actions. Further tests distinguished
	neurons responding equally strongly to static presentations and to
	actions ("snapshot" neurons), from those responding much less strongly
	to static presentations, but responding well when motion was present
	("motion" neurons). The "motion" neurons were predominantly found
	in the upper bank/fundus of the STS, and "snapshot" neurons in the
	lower bank of the STS and inferior temporal convexity. Most "motion"
	neurons showed strong response modulation during the course of an
	action, thus responding to action kinematics. "Motion" neurons displayed
	a greater average selectivity for these simple arm actions than did
	"snapshot" neurons. We suggest that the "motion" neurons code for
	visual kinematics, whereas the "snapshot" neurons code for form/posture,
	and that both can contribute to action recognition, in agreement
	with computation models of action recognition.},
  doi = {doi:10.1093/cercor/bhn109},
  timestamp = {2010.01.15},
  url = {http://cercor.oxfordjournals.org/cgi/content/abstract/19/3/593}
}

@INPROCEEDINGS{Vangeneugden2009_2,
  author = {Vangeneugden, J. and Vancleef, K. and Jaeggli, T. and Gool, L. van
	and Vogels, R.},
  title = {Coding of walking direction by macaque visual temporal cortical neurons},
  booktitle = {Society for Neuroscience},
  year = {2009},
  organization = {Neuroscience 2009},
  publisher = {Society for Neuroscience},
  abstract = {Many psychophysical studies examining biological motion perception
	utilize displays of treadmill walking, i.e. locomotion without a
	translatory component (intrinsic motion only), while the displays
	in all single cell studies employed walkers that translated across
	the visual field (intrinsic and extrinsic motion; e.g. Oram & Perrett,
	JCN, 1994). We determined the coding of walking direction by macaque
	temporal cortical neurons for displays containing no extrinsic motion.
	Stimuli were human-like figures defined by cylinder-like primitives
	connecting the joints. Motion of the joints was based on motion capture
	data. The figure walked forward or backward in eight different facing
	directions (e.g. forward walking to right or left). The backward
	and forward movies contained identical postures, differing only in
	their temporal sequence. Thus the forward differed from backward
	locomotion displays in motion cues only, while the facing directions
	differed in both motion and form cues. Monkeys were trained to discriminate
	forward from backward and leftward from rightward walking prior to
	the recordings. Longer training was required for the forward-backward
	compared to the left-right discrimination. Single-unit recordings
	in the trained animals during a passive fixation task targeted both
	banks of the rostral superior temporal sulcus (STS) and the lateral
	convexity of the inferotemporal (IT) cortex. The large majority of
	responsive neurons (69%; N = 75) showed a main effect of facing direction,
	while a minority distinguished forward from backward walking (main
	effect: 9%; interaction: 15%). Sensitivity for forward-backward walking
	of the neurons selective for forward/backward walking was 0.8 (median
	d’ (best-worst)) which was significantly smaller (p < .0001) than
	sensitivity for facing direction of neurons selective for facing
	direction (median d’ (best-worst) = 2.4). The large majority of responsive
	neurons were recorded in the lower bank of the STS/IT and responded
	also to static presentations of snapshots of the walker (“snapshot
	neurons”; Vangeneugden et al., Cer Cor, 2009). 53% of the neurons
	exhibited significant tuning for different postures (ANOVA; 7 snapshots)
	of a walking sequence. These results show that (1) lower bank STS/IT
	neurons are selective for walking direction even when these are defined
	without translatory motion; (2) single neurons poorly distinguish
	locomotion directions that differ only in snapshot sequence; (3)
	single neurons are selective for postures of a walking cycle. The
	latter agrees with form-based modules postulated in action recognition
	models (Giese & Poggio, Nat Rev Neurosci, 2003; Lange et al., J Neurosci,
	2006).},
  timestamp = {2010.01.15}
}

@ARTICLE{Vangeneugden2009_3,
  author = {Vangeneugden, J. and Vancleef, K. and Jaeggli, T. and Van Gool, L.
	and Vogels, R.},
  title = {Discrimination of locomotion direction in impoverished displays of
	walkers by macaque monkeys},
  journal = {Journal of Vision},
  year = {2009},
  note = {in press},
  timestamp = {2010.01.15}
}

@ARTICLE{Vogels2009_1,
  author = {Vogels, R.},
  title = {Mechanisms of visual perceptual learning in macaque visual cortex},
  journal = {Topics in Cognitive Science},
  year = {2009},
  note = {in press},
  abstract = {The neural mechanisms underlying behavioral improvement in the detection
	or discrimination of visual stimuli following learning are still
	ill understood. Studies in nonhuman primates have shown relatively
	small and, across studies, variable effects of fine discrimination
	learning in primary visual cortex when tested outside the context
	of the learned task. At later stages, such as extrastriate area V4,
	extensive practice in fine discrimination produces more consistent
	effects upon responses and neural tuning. In V1 and V4, the effects
	of learning were most prominent in those neurons that can contribute
	the most reliable information about the trained stimuli. I suggest
	that, depending on the particulars of the task demands, neurons at
	various stages of stimulus and task processing can change their tuning
	and responses, so that execution of the task will produce a higher
	frequency of reward. I speculate that the sort of changes that will
	occur depend on the task and on stimulus analysis requirements, and
	they may vary from changes in bottom-up stimulus processing/tuning
	within early visual areas or more efficient readout of early visual
	areas to top-down driven changes in response properties of these
	areas.},
  doi = {doi:10.1111/j.1756-8765.2009.01051.x},
  timestamp = {2010.01.15},
  url = {http://dx.doi.org/doi:10.1111/j.1756-8765.2009.01051.x}
}

@INPROCEEDINGS{Willems2009_1,
  author = {Willems, G. and Becker, J.H. and Tuytelaars, T. and Gool, L. van},
  title = {Exemplar-based action recognition in video},
  booktitle = {20th British Machine Vision Conference (BMVC), London, UK},
  year = {2009},
  organization = {Proceedings BMVC 2009},
  timestamp = {2009.10.23},
  url = {http://homes.esat.kuleuven.be/~gwillems/v2/publications/BMVC09.pdf}
}

@INPROCEEDINGS{Zeghbib2009_1,
  author = {Zeghbib, A. and Fillbrandt, A. and Ohl, Frank W.},
  title = {Bidirectional information transfer between auditory and visual cortices},
  booktitle = {3rd International Conference on Auditory Cortex},
  year = {2009},
  pages = {158},
  address = {Magdeburg},
  timestamp = {2010.02.07}
}

@INPROCEEDINGS{Zeghbib2009_2,
  author = {Zeghbib, A. and Fillbrandt, A. and Ohl, Frank W.},
  title = {Alteration of brain states with high phase-coherence and transient
	states indicate the intermittency information processing in brain
	dynamics},
  booktitle = {7th Meeting of the German Neuroscience Society},
  year = {2009},
  number = {2007},
  organization = {31st Göttingen Neurobiology Conference},
  note = {T31-1B},
  timestamp = {2010.02.07}
}

@INPROCEEDINGS{Zeghbib2009_3,
  author = {Zeghbib, A. and Fillbrandt, A. and Ohl, Frank W.},
  title = {Phase coherence evolution in cortical networks: adaptation to audiovisual
	stimulation with fixed inter-modality asynchrony},
  booktitle = {International Multisensory Research Forum (IMRF)},
  year = {2009},
  pages = {391},
  organization = {10th Annual Meeting International Multisensory Research Forum},
  timestamp = {2010.02.07}
}

@MISC{Zeghbib2009_4,
  author = {Zeghbib, A. and Fillbrandt, A. and Ohl, Frank W.},
  title = {Phase change and Mutual Inter-cortical information flow under adaptation
	underlaying neuro-dynamical models},
  howpublished = {The XI. IfN-Forschungsseminars in Freyburg},
  month = {5-7. Oktober},
  year = {2009},
  note = {Talk (Presentation)},
  owner = {sita},
  timestamp = {2010.08.24}
}

@ARTICLE{ISI:000263396100008,
  author = {Zimmermann, Karel and Matas, Jiri and Svoboda, Tomas},
  title = {Tracking by an Optimal Sequence of Linear Predictors},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2009},
  volume = {31},
  pages = {677-692},
  number = {4},
  month = {APR},
  abstract = {We propose a learning approach to tracking explicitly minimizing the
	computational complexity of the tracking process subject to user-defined
	probability of failure (loss-of-lock) and precision. The tracker
	is formed by a Number of Sequences of Learned Linear Predictors (NoSLLiP).
	Robustness of NoSLLiP is achieved by modeling the object as a collection
	of local motion predictors-object motion is estimated by the outlier-tolerant
	RANSAC algorithm from local predictions. The efficiency of the NoSLLiP
	tracker stems 1) from the simplicity of the local predictors and
	2) from the fact that all design decisions, the number of local predictors
	used by the tracker, their computational complexity (i.e., the number
	of observations the prediction is based on), locations as well as
	the number of RANSAC iterations, are all subject to the optimization
	(learning) process. All time-consuming operations are performed during
	the learning stage-tracking is reduced to only a few hundred integer
	multiplications in each step. On PC with 1xK8 3200+, a predictor
	evaluation requires about 30 mu s. The proposed approach is verified
	on publicly available sequences with approximately 12,000 frames
	with ground truth. Experiments demonstrate superiority in frame rates
	and robustness with respect to the SIFT detector, Lucas-Kanade tracker,
	and other trackers.},
  doi = {10.1109/TPAMI.2008.119},
  issn = {0162-8828},
  unique-id = {ISI:000263396100008},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4522559}
}

