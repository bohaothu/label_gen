%% Daniel Abadi's publications in bibtex format

%% Conference and Journal Papers

@inproceedings{split-execution,
        title = "Efficient Processing of Data Warehousing Queries in a Split Execution Environment",
        year = "2011",
        booktitle = "SIGMOD",
        venue = "SIGMOD",
        author = {Kamil Bajda-Pawlikowski and Daniel J. Abadi and Avi Silberschatz and Erik Paulson},
	publicationtype = "Conference Paper",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/split-execution-hadoopdb.pdf",
	pdfKB = "609",
}

@inproceedings{dbms-determinism,
        title = "The Case for Determinism in Database Systems",
        year = "2010",
        booktitle = "VLDB",
        venue = "VLDB",
        author = {Alexander Thomson and Daniel J. Abadi},
	publicationtype = "Conference Paper",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/determinism-vldb10.pdf",
	pdfKB = "550",
}

@inproceedings{hstore-cc,
        title = "Low Overhead Concurrency Control for Partitioned Main Memory Databases",
        year = "2010",
        booktitle = "SIGMOD",
        venue = "SIGMOD",
        author = {Evan P. C. Jones and Daniel J. Abadi and Samuel Madden},
	publicationtype = "Conference Paper",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/hstore-cc.pdf",
	pdfKB = "300",
}

@misc{mr-cacm,
  author = {Michael Stonebraker and Daniel J. Abadi and David. J. DeWitt and Samuel Madden and Erik Paulson and Andrew Pavlo and Alexander Rasin},
  title = {MapReduce and Parallel DBMSs: Friends or Foes?},
  howpublished = {CACM, 53(1)},
  month = {January},
  year = {2010},
  pages = {64--71},
  venue = {CACM},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/mr-cacm2010.pdf",
  pdfKB = "3625",
  publicationtype = "Journal Article",
}

@inproceedings{hadoopdb,
        title = "HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads",
        year = "2009",
        booktitle = "VLDB",
        venue = "VLDB",
        address = "Lyon, France",
        author = {Azza Abouzied and Kamil Bajda-Pawlikowski and Daniel J. Abadi and Avi Silberschatz and Alexander Rasin},
	publicationtype = "Conference Paper",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/hadoopdb.pdf",
	pdfKB = "400",
}

@inproceedings{benchmarks-sigmod09,
        title = "A Comparison of Approaches to Large Scale Data Analysis",
        year = "2009",
        booktitle = "SIGMOD",
        venue = "SIGMOD",
        address = "Providence, Rhode Island, USA",
        author = {Andrew Pavlo and Erik Paulson and Alexander Rasin and Daniel J. Abadi and David J. DeWitt and Samuel R. Madden and Michael Stonebraker},
	publicationtype = "Conference Paper",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/benchmarks-sigmod09.pdf",
	pdfKB = "251",
}

@misc{abadi-ieee-cloud,
  author = {Daniel J. Abadi},
  title = {Data Management in the Cloud: Limitations and Opportunities},
  howpublished = {IEEE Data Engineering Bulletin, 32(1)},
  month = {March},
  year = {2009},
  pages = {3-12},
  venue = {IEEE Data Engineering Bulletin},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadi-cloud-ieee09.pdf",
  pdfKB = "62",
  publicationtype = "Journal Article",
}

@misc{abadi-swstore,
  author = {Daniel J. Abadi and Adam Marcus and Samuel R. Madden and Kate Hollenbach},
  title = {SW-Store: A Vertically Partitioned DBMS for Semantic Web Data Management},
  howpublished = {VLDB Journal, 18(2)},
  month = {April},
  year = {2009},
  pages = {385--406},
  venue = {VLDB Journal},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadi-rdf-vldbj.pdf",
  pdfKB = "1124",
  publicationtype = "Journal Article",
}

@inproceedings{abadi-sigmod08,
        title = "Column-Stores vs. Row-Stores: How Different Are They Really?",
        year = "2008",
        booktitle = "SIGMOD",
        venue = "SIGMOD",
        address = "Vancouver, Canada",
        author = {Daniel J. Abadi and Samuel R. Madden and Nabil Hachem},
	publicationtype = "Conference Paper",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadi-sigmod08.pdf",
	pdfKB = "424",
}

@inproceedings{oltp-perf,
        title = "OLTP Through the Looking Glass, And What We Found There",
        year = "2008",
        booktitle = "SIGMOD",
        venue = "SIGMOD",
        address = "Vancouver, Canada",
        author = {Stavros Harizopoulos and Daniel J. Abadi and Samuel R. Madden and Michael Stonebraker},
	publicationtype = "Conference Paper",
	pdfKB = "287",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/oltpperf-sigmod08.pdf",
}

@inproceedings{abadi-rdf,
	title = "Scalable Semantic Web Data Management Using Vertical Partitioning",
	year = "2007",
        booktitle = "VLDB",
        address = "Vienna, Austria",
	venue = "VLDB",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadirdf.pdf",
	author = {Daniel J. Abadi and Adam Marcus and Samuel R. Madden and Kate Hollenbach},
	abstract = "Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, 'property tables'. We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution: vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than 50 million triples) catalog of library data.  Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.",
	pdfKB = "246",
	publicationtype = "Conference Paper",
        award = "Best Paper Award",
}

@inproceedings{hstore,
  author    = {Michael Stonebraker and Samuel R. Madden and Daniel J. Abadi and Stavros Harizopoulos and Nabil Hachem and Pat Helland},
  title     = {The End of an Architectural Era (It's Time for a Complete Rewrite)},
  booktitle = {VLDB},
  year      = {2007},
  address = {Vienna, Austria},
  venue = "VLDB",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/vldb07hstore.pdf",
  abstract = "In previous papers, some of us predicted the end of 'one size fits all' as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major relational RDBMS vendors can be outperformed by 1-2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T., to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that current RDBMS code lines, while attempting to be a 'one size fits all' solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of 'from scratch' specialized engines. The DBMS vendors (and research community) should start with a clean sheet of paper and design systems for tomorrow's requirements, not continue to push code lines and architectures designed for yesterday's needs.",
  pdfKB = "444",
  publicationtype = "Conference Paper",
}

@inproceedings{abadi-cidr,
  author    = {Daniel J. Abadi},
  title     = {Column Stores for Wide and Sparse Data},
  booktitle = {CIDR},
  year      = {2007},
  address  = {Asilomar, CA, USA},
  venue = "CIDR",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadicidr07.pdf",
  abstract = "While it is generally accepted that data warehouses and OLAP workloads are excellent applications for column-stores, this paper speculates that column-stores may well be suited for additional applications. In particular we observe that column-stores do not see a performance degradation when storing extremely wide tables, and column-stores handle sparse data very well. These two properties lead us to conjecture that column-stores may be good storage layers for Semantic Web data, XML data, and data with GEM-style schemas.",
  pdfKB = "156",
  publicationtype = "Conference Paper",
}

@inproceedings{cstore-mat,
  author    = {Daniel J. Abadi and Daniel S. Myers and David J. DeWitt and Samuel R. Madden},
  title     = {Materialization Strategies in a Column-Oriented DBMS},
  booktitle = {ICDE},
  year      = {2007},
  address  = {Istanbul, Turkey},
  pages = {466--475},
  venue = "ICDE",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadiicde2007.pdf",
  abstract = "There has been renewed interest in column-oriented database architectures in recent years. For read-mostly query workloads such as those found in data warehouse and decision support applications, column-stores have been shown to perform particularly well relative to row-stores. In order for column-stores to be readily adopted as a replacement for row-stores, however, they must present the same interface to client applications as do row stores, which implies that they must output row-store-style tuples. Thus, the input columns stored on disk must be converted to rows at some point in the query plan, but the optimal point at which to do the conversion is not obvious. This problem can be considered as the opposite of the projection problem in row-store systems: while row-stores need to determine where in query plans to place projection operators to make tuples narrower, column-stores need to determine when to combine single-column projections into wider tuples. This paper describes a variety of strategies for tuple construction and intermediate result representations and provides a systematic evaluation of these strategies.",
  pdfKB = "327",
  publicationtype = "Conference Paper",
}

@inproceedings{cstore-perf,
  author = {Stavros Harizopoulos and Velen Liang and Daniel J. Abadi and Samuel R. Madden},
  title = {Performance Tradeoffs in Read-Optimized Databases},
  booktitle = {VLDB},
  year = {2006},
  pages = {487--498},
  address = {Seoul, Korea},
  venue = "VLDB",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/VLDB06.pdf",
  abstract = "Database systems have traditionally optimized performance for write-intensive workloads. Recently, there has been renewed interest in architectures that optimize read performance by using column-oriented data representation and light-weight compression. This previous work has shown that under certain broad classes of workloads, column-based systems can outperform row-based systems. Previous work, however, has not characterized the precise conditions under which a particular query workload can be expected to perform better on a column-oriented database. In this paper we first identify the distinctive components of a read-optimized DBMS and describe our implementation of a high-performance query engine that can operate on both row and column-oriented data. We then use our prototype to perform an in-depth analysis of the tradeoffs between column and row-oriented architectures. We explore these tradeoffs in terms of disk bandwidth, CPU cache latency, and CPU cycles. We show that for most database workloads, a carefully designed column system can outperform a carefully designed row system, sometimes by an order of magnitude. We also present an analytical model to predict whether a given workload on a particular hardware configuration is likely to perform better on a row or column-based system.",
  pdfKB = "354",
  publicationtype = "Conference Paper",
 }

@inproceedings{cstore-comp,
  author    = {Daniel J. Abadi and Samuel R. Madden and Miguel Ferreira},
  title     = {Integrating Compression and Execution in Column-Oriented Database Systems},
  booktitle = {SIGMOD},
  year      = {2006},
  address   = {Chicago, IL, USA},
  pages     = {671--682},
  venue = "SIGMOD",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadisigmod06.pdf",
  abstract = "Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems.  We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.",
  pdfKB = "265",
  publicationtype = "Conference Paper",
}

@inproceedings{reed,
  author    = {Daniel J. Abadi and Samuel R. Madden and Wolfgang Lindner},
  title     = {REED: Robust, Efficient Filtering and Event Detection in Sensor Networks},
  booktitle = {VLDB},
  year      = {2005},
  address   = {Trondheim, Norway},
  pages     = {769--780},
  venue = "VLDB",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadivldb05.pdf",
  abstract = "This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.",
  pdfKB = "292",
  publicationtype = "Conference Paper",
}

@inproceedings{cstore,
  author    = {Michael Stonebraker and Daniel J. Abadi and Adam Batkin and Xuedong Chen and Mitch Cherniack and Miguel Ferreira and Edmond Lau and Amerson Lin and Samuel R. Madden and Elizabeth J. O'Neil and Patrick E. O'Neil and Alexander Rasin and Nga Tran and Stan B. Zdonik},
  title     = {C-Store: A Column-Oriented DBMS},
  booktitle = {VLDB},
  year      = {2005},
  address   = {Trondheim, Norway},
  pages     = {553--564},
  venue = "VLDB",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/vldb.pdf",
  abstract = "This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures. We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.",
  pdfKB = "170",
  publicationtype = "Conference Paper",
}

@inproceedings{borealis,
  author    = {Daniel J. Abadi and Yanif Ahmad and Magdalena Balazinska and Ugur Cetintemel and Mitch Cherniack and Jeong-Hyon Hwang and Wolfgang Lindner and Anurag S. Maskey and Alexander Rasin and Esther Ryvkina and Nesime Tatbul and Ying Xing and Stan B. Zdonik},
  title     = {The Design of the Borealis Stream Processing Engine},
  booktitle = {CIDR},
  year      = {2005},
  address  = {Asilomar, CA, USA},
  venue = "CIDR",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/cidr05.pdf",
  abstract = "Borealis is a second-generation distributed stream processing engine that is being developed at Brandeis University, Brown University, and MIT. Borealis inherits core stream processing functionality from Aurora and distribution functionality from Medusa. Borealis modifies and extends both systems in non-trivial and critical ways to provide advanced capabilities that are commonly required by newly-emerging stream processing applications. In this paper, we outline the basic design and functionality of Borealis. Through sample real-world applications, we motivate the need for dynamically revising query results and modifying query specifications. We then describe how Borealis addresses these challenges through an innovative set of features, including revision records, time travel, and control lines. Finally, we present a highly flexible and scalable QoS-based optimization model that operates across server and sensor networks and a new fault-tolerance model with flexible consistency-availability trade-offs.",
  pdfKB = "143",
  publicationtype = "Conference Paper",
}

@misc{aurora,
  author = {Daniel J. Abadi and Don Carney and  Ugur Cetintemel and Mitch Cherniack and Christian Convey and Sangdon Lee and Michael Stonebraker and Nesime Tatbul and Stan B. Zdonik},
  title = {Aurora: A New Model and Architecture for Data Stream Management},
  howpublished = {VLDB Journal, 12(2)},
  month = {September},
  year = {2003},
  pages = {120--139},
  venue = {VLDB Journal},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/vldb095.pdf",
  abstract = "This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.",
  pdfKB = "984",
  publicationtype = "Journal Article",
}

%% Technical Reports

@techreport{barton-benchmark,
  title = {Using The Barton Libraries Dataset As An RDF Benchmark},
  number = {MIT-CSAIL-TR-2007-036},
  institution = {MIT},
  year      = {2007},
  author = {Daniel J. Abadi and Adam Marcus and Samuel R. Madden and Kate Hollenbach},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/bench.pdf",
  abstract = "This report describes the Barton Libraries RDF dataset and Longwell query benchmark that we use for our recent VLDB paper on Scalable Semantic Web Data Management Using Vertical Partitioning",
  pdfKB = "357",
  publicationtype = "Technical Report",
}

%% Tutorials and Panels

@inproceedings{columnstore-tutorial,
        title = "Column oriented Database Systems",
        year = "2009",
        booktitle = "VLDB",
        venue = "VLDB",
        author = {Daniel J. Abadi and Peter A. Boncz and Stavros Harizopoulos},
	publicationtype = "Tutorial",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/columnstore-tutorial.pdf",
	pdfKB = "23",
	slides = "http://www.slideshare.net/abadid/vldb-2009-tutorial-on-columnstores",
}

@inproceedings{abadi-vldb09-panel,
        title = "How best to build web-scale data managers?",
        year = "2009",
        booktitle = "VLDB",
        venue = "VLDB",
        author = {Philip A. Bernstein and Daniel J. Abadi and Michael J. Cafarella and Joseph M. Hellerstein and Donald Kossmann and Samuel Madden},
	publicationtype = "Panel Presentation",
	pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/vldb2009-panel.pdf",
	pdfKB = "600",
	slides = "http://www.slideshare.net/abadid/daniel-abadi-vldb-2009-panel",
}

%% Demonstrations

@misc{hadoopdb-demo,
        title = {HadoopDB in Action: Building Real World Applications},
        year = {2010},
        howpublished = {Demonstration. SIGMOD},
        venue = {SIGMOD},
        address = {Indianapolis, USA},
        author = {Azza Abouzied and Kamil Bajda-Pawlikowski and Jiewen Huang and Daniel J. Abadi and Avi Silberschatz},
	publicationtype = {Demonstration},
	pdfKB = {520},
	pdfURL = {http://cs-www.cs.yale.edu/homes/dna/papers/hadoopdb-demo.pdf},
}

@misc{hstore-demo,
        title = {H-Store: A High-Performance, Distributed Main Memory Transaction Processing System},
        year = {2008},
        howpublished = {Demonstration. VLDB},
        venue = {VLDB},
        address = {Aukland, New Zealand},
        author = {Robert Kallman and Hideaki Kimura and Jonathan Natkins and Andrew Pavlo and Alex Rasin and Stan Zdonik and Evan Jones and Yang Zhang and Samuel Madden and Michael Stonebraker and John Hugg and Daniel J. Abadi},
	publicationtype = {Demonstration},
	pdfKB = {440},
	pdfURL = {http://cs-www.cs.yale.edu/homes/dna/papers/hstore-demo.pdf},
}

@misc{sensor-stream-integration,
  author    = {Daniel J. Abadi and Wolfgang Lindner and Samuel R. Madden and Jorg Schuler},
  title     = {An Integration Framework for Sensor Networks and Data Stream Management Systems},
  howpublished = {Demonstration. VLDB},
  year      = {2004},
  pages     = {1361--1364},
  address   = {Toronto, Canada},
  venue = "VLDB",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/vldb04.pdf",
  abstract = "This demonstration shows an integrated query processing environment where users can seamlessly query both a data stream management system and a sensor network with one query expression. By integrating the two query processing systems, the optimization goals of the sensor network (primarily power) and server network (primarily latency and quality) can be unified into one quality of service metric.",
  pdfKB = "116",
  publicationtype = "Demonstration",
}

@misc{aurora-demo,
  author    = {Daniel J. Abadi and Don Carney and Ugur Cetintemel and Mitch Cherniack and Christian Convey and Christina Erwin and Eddie Galvez and Matt Hatoun and Jeong-Hyon Hwang and Anurag S. Maskey and Alexander Rasin and A. Singer and Michael Stonebraker and Nesime Tatbul and Ying Xing and R. Yan and Stan B. Zdonik},
  title     = {Aurora: A Data Stream Management System},
  howpublished = {Demonstration. SIGMOD},
  year      = {2003},
  pages     = {666-666},
  address   = {San Diego, CA, USA},
  venue = "SIGMOD",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/AuroraDemo.pdf",
  abstract = "Streams are continuous data feeds generated by such sources as sensors, satellites, and stock feeds. Monitoring applications track data from numerous streams, filtering them for signs of abnormal activity, and processing them for purposes of filtering, aggregation, reduction, and correlation. Aurora is a general-purpose data stream manager that is being designed and implemented (at Brandeis University, Brown University, and M.I.T.) to efficiently support a variety of real-time monitoring applications.",
  pdfKB = "151",
  publicationtype = "Demonstration",
}

@misc{vcoko,
  author    = {Daniel J. Abadi and Mitch Cherniack},
  title     = {Visual COKO: A Debugger for Query Optimizer Development},
  howpublished = {Demonstration. SIGMOD},
  year      = {2002},
  address  = {Madison, Wisconsin},
  pages    = {617--617},
  venue = "SIGMOD",
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/vcoko.pdf",
  abstract = "Query optimization generates plans to retrieve data requested by queries. Query rewriting, which is the first step of this process, rewrites a query expression into an equivalent form to prepare it for plan generation. COKO-KOLA introduced a new approach to query rewriting that enables query rewrites to be formally verified using an automated theorem prover. KOLA is a language for expressing term rewriting rules that can be 'fired' on query expressions. COKO is a language for expressing query rewriting transformations that are too complex to express with simple KOLA rules. COKO is a programming language designed for query optimizer development. Programming languages require debuggers, and in this demonstration, we illustrate our COKO debugger: Visual COKO. Visual COKO enables a query optimization developer to visually trace the execution of a COKO transformation. At every step of the transformation, the developer can view a tree-display that illustrates how the original query expression has evolved.",
  pdfKB = "111",
  publicationtype = "Demonstration",
}

%% Theses

@misc{abadi-phd,
  author = {Daniel J. Abadi},
  title = {Query Execution in Column-Oriented Database Systems},
  howpublished = {MIT PhD Dissertation},
  year = {2008},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/abadiphd.pdf",
  abstract = "There are two obvious ways to map a two-dimension relational database table onto a one-dimensional storage interface: store the table row-by-row, or store the table column-by-column. Historically, database system implementations and research have focused on the row-by row data layout, since it performs best on the most common application for database systems: business transactional data processing. However, there are a set of emerging applications for database systems for which the row-by-row layout performs poorly. These applications are more analytical in nature, whose goal is to read through the data to gain new insight and use it to drive decision making and planning. In this dissertation, we study the problem of poor performance of row-by-row data layout for these emerging applications, and evaluate the column-by-column data layout opportunity as a solution to this problem. There have been a variety of proposals in the literature for how to build a database system on top of column-by-column layout. These proposals have different levels of implementation effort, and have different performance characteristics. If one wanted to build a new database system that utilizes the column-by-column data layout, it is unclear which proposal to follow. This dissertation provides (to the best of our knowledge) the only detailed study of mutliple implementation approaches of such systems, categorizing the different approaches into three broad categories, and evaluating the tradeoffs between approaches. We conclude that building a query executer specifically designed for the column-by-column query layout is essensial to acheive good performance. Consequently, we describe the implementation of C-Store, a new database system with a storage layer and query executer built for column-by-column data layout. We introduce three new query execution technqiues that significantly improve performance. First, we look at the problem of integrating compression and execution so that the query executer is capable of directly operating on compressed data. This improves performance by improving I/O (less data needs to be read off disk), and CPU (the data need not be decompressed). We describe our solution to the problem of executer extensibility -- how can new compression techniques be added to the system without having to rewrite the operator code? Second, we analyze the problem of tuple construction (stitching together attributes from multiple columns into a row-oriented ``tuple''). Tuple construction is required when operators need to access multiple attributes from the same tuple; however, if done at the wrong point in a query plan, a significant performance penalty is paid. We introduce an analytical model and some heuristics to use that help decide when in a query plan tuple construction should occur. Third, we introduce a new join technique, the ``invisible join'' that improves performance of a specific type of join that is common in the applications for which column-by-column data layout is a good idea. Finally, we benchmark performance of the complete C-Store database system against other column-oriented database system implementation approachs, and against row-oriented databases. We benchmark two applications. The first application is a typical analytical application for which column-by-column data layout is known to outperform row-by-row data layout. The second application is another emerging application, the Semantic Web, for which column-oriented database systems are not currently used. We find that on the first application, the complete C-Store system performed 10 to 18 times faster than alternative column-store implementation approaches, and 6 to 12 times faster than a commercial database system that uses a row-by-row data layout. On the Semantic Web application, we find that C-Store outperforms other state-of-the-art data management techniques by an order of magnitude, and outperforms other common data management technqiues by almost two orders of magnitude. Benchmark queries, which used to take multiple minutes to execute, can now be answered in several seconds.",
  pdfKB = "1301",
  publicationtype = "Thesis",
  note = "PhD Thesis",
  award = "2008 SIGMOD Jim Gray Doctoral Dissertation Award",
}

@misc{abadi-anaphora,
  author    = {Daniel J. Abadi},
  title     = {Comparing Domain-Specific and Non-Domain-Specific Anaphora Resolution Techniques},
  howpublished = {Cambridge University MPhil Dissertation},
  year      = {2003},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/FinalMPhil.pdf",
  abstract = "Three different pronominal anaphora resolution techniques are examined. The first two techniques compare traditional salience-based approaches when different amounts of syntactic information are available. The improvement in pronoun resolution precision is quantified when a large scale grammar is used to extract detailed syntactic information rather than inferring this information robustly using pattern matching. The third technique uses domain knowledge instead of syntactic information to resolve pronouns. The domain knowledge required for this algorithm can be automatically acquired from a database backend schema representation of the domain. Each of these three techniques is evaluated separately, and then the domain-specific and non-domain-specific algorithms are combined and evaluated.",
  pdfKB = "164",
  publicationtype = "Thesis",
  note = "M.Phil. Thesis",
}

@misc{abadi-ugrad-thesis,
  author    = {Daniel J. Abadi},
  title     = {Visual COKO: A Debugger for Query Optimizer Development},
  howpublished = {Brandeis University Senior Honors Thesis},
  year      = {2002},
  pdfURL = "http://cs-www.cs.yale.edu/homes/dna/papers/VisualCOKOThesis.pdf",
  abstract = "Query optimization generates plans to retrieve data requested by queries, and query rewriting (rewriting a query expression into an equivalent form to prepare it for plan generation) is typically the first step. COKO-KOLA introduced a new approach to query rewriting that enables query rewrites to be formally verified using an automated theorem prover. KOLA is a language for expressing rewriting rules that can be fired on query expressions. COKO is a language for expressing query rewriting transformations that are too complex to express with simple KOLA rules. COKO is a programming language designed for query optimizer development. Programming languages require debuggers, and this paper describes a COKO debugger: Visual COKO. Visual COKO enables a query optimization developer to visually trace the execution of a COKO transformation. At every step of the transformation, the developer can view a tree-display that illustrates how the original query expression has evolved. Rule-based query rewriting and the COKO-KOLA project are described for background. Then the COKO syntax is summarized from the point of view of the COKO programmer using an example query transformation that converts query predicates to conjunctive normal form. Visual COKO is described and instructions for its use are presented. Finally, a description of its implementation is given.",
  pdfKB = "132",
  publicationtype = "Thesis",
  note = "Undergraduate Thesis",
}
