@inproceedings{zhu01heavy,
    author = "Xiaoyun Zhu and Jie Yu and John Doyle",
    title = "Heavy Tails, Generalized Coding, and Optimal Web Layout",
    booktitle = "Proceedings of {IEEE INFOCOM}",
    pages = "1617--1626",
    year = "2001",
    month = apr,
    http = "http://citeseer.nj.nec.com/zhu01heavy.html",
    abstract = "This paper considers Web layout design in the spirit of source coding for data compression and rate distortion theory, with the aim of minimizing the average size of files downloaded during Web browsing sessions. The novel aspect here is that the object of design is layout rather than codeword selection, and is subject to navigability constraints. This produces statistics for file transfers that are heavy tailed, completely unlike standard Shannon theory, and provides a natural and plausible explanation for the origin of observed power laws in Web traffic. We introduce a series of theoretical and simulation models for optimal Web layout design with varying levels of analytic tractability and realism with respect to modeling of structure, hyperlinks, and user behavior. All models produce power laws which are striking both for their consistency with each other and with observed data, and their robustness to modeling assumptions. These results suggest that heavy tails are a permanent and ubiquitous feature of Internet traffic, and not an artifice of current applications or user behavior. They also suggest new ways of thinking about protocol design that combines insights from information and control theory with traditional networking."
}

@inproceedings{alderson-toward,
  author = "David Alderson and John Doyle and Ramesh Govindan and Walter Willinger",
  title = "Toward an Optimization-Driven Framework for Designing and Generating Realistic Internet Topologies",
  booktitle = "ACM HotNets-I",
  year = "2002",
  month = oct,
  address = "Princeton, NJ",
  http = "http://citeseer.nj.nec.com/alderson02toward.html",
  abstract = "We propose a novel approach to the study of Internet topology in which we use an optimization framework to model the mechanisms driving incremental growth. While previous methods of topology generation have focused on explicit replication of statistical properties, such as node hierarchies and node degree distributions, our approach addresses the economic tradeoffs, such as cost and performance, and the technical constraints faced by a single ISP in its network design. By investigating plausible objectives and constraints in the design of actual networks, observed network properties such as certain hierarchical structures and node degree distributions can be expected to be the natural by-product of an approximately optimal solution chosen by network designers and operators. In short, we advocate here essentially an approach to network topology design, modeling, and generation that is based on the concept of Highly Optimized Tolerance (HOT). In contrast with purely descriptive topology modeling, this opens up new areas of research that focus on the causal forces at work in network design and aim at identifying the economic and technical drivers responsible for the observed large-scale network behavior. As a result, the proposed approach should have significantly more predictive power than currently pursued efforts and should provide a scientific foundation for the investigation of other important problems, such as pricing, peering, or the dynamics of routing protocols. "
}

@inproceedings{fabrikant-heuristically,
  author = "Alex Fabrikant and Elias Koutsoupias and Christos H.~Papadimitriou",
  title = "Heuristically Optimized Trade-offs: A New Paradigm for Power Laws in the Internet (Extended Abstract)",
  booktitle = "Proceedings of ICALP",
  year = "2002",
  pages = "110--122",
  publisher = "Springer",
  http = "http://citeseer.nj.nec.com/fabrikant02heuristically.html",
  abstract = "We give a plausible explanation of the power law distributions of degrees observed in the graphs arising in the Internet topology \cite{faloutsos3_99} based on a toy model of Internet growth in which two objectives are optimized simultaneously: ``last mile'' connection cost, and transmission delays measured in hops. We also point out a similar phenomenon, anticipated in \cite{carlson_doyle99}, in the distribution of file sizes. Our results seem to suggest that power laws tend to arise as a result of complex, multi-objective optimization."
}

@Article{carlson_doyle99,
  author = 		 {J.~M.~Carlson and John Doyle},
  title = 		 {Highly optimized tolerance: a mechanism for poer laws in designed systems},
  journal = 	 {Physical Review E},
  year = 		 {1999},
  volume = 	 {60},
  number = 	 {2},
  pages = 	 {1412--1427},
  http = {http://www.physics.ucsb.edu/~complex/pubs/with.htm},
  abstract = {We introduce a  mechanism for generating power law distributions, referred to as {\it highly optimized tolerance} (HOT), which is motivated by biological  organisms and advanced engineering technologies. Our focus is on systems  which are optimized, either through natural selection or engineering design, to provide robust performance despite uncertain environments. We suggest that power laws in these systems are due to tradeoffs between yield, cost of resources,  and tolerance to risks. These tradeoffs lead to highly  optimized designs that allow for occasional large events.  We investigate the mechanism  in the context of  percolation and sand pile models in order emphasize the sharp contrasts between HOT and self organized criticality (SOC), which has been  widely suggested as the origin for power laws in complex systems.  Like SOC,  HOT produces power laws. However,  compared to SOC, HOT states exist for densities which are higher than the critical density, and the power laws are not restricted to special values of the density.  The characteristic features of HOT systems  include: (1) high efficiency, performance, and robustness to designed-for uncertainties, (2) hypersensitivity to design flaws and unanticipated perturbations, (3) nongeneric, specialized, structured configurations, and (4) power laws. The first three of these are in contrast to the traditional hallmarks of criticality, and are obtained  by simply adding the element of design to percolation and  sand pile models, which completely changes their characteristics. }
}
@Article{carlson_doyle00,
  author = 		 {J.~M.~Carlson and John Doyle},
  title = 		 {Highly optimized tolerance: Robustness and design in complex systems},
  journal = 	 {Physical Review Letters},
  year = 		 {2000},
  volume = 	 {84},
  number = {11},
  month = mar,
  pages = 	 {2529--2532},
  http = {http://www.physics.ucsb.edu/~complex/pubs/with.htm},
  abstract = {We introduce HOT, a mechanism that connects evolving structure and power laws in interconnected systems. HOT systems arise, e.g. in biology and engineeringr, where design and evolution create complex systems sharing common features, including (1) high efficiency, performance, and robustness to designed-for uncertainties, (2) hypersentivity to design flaws and unanticipated perturbations, (3) nongeneric, specialized, structured configurations, and (4) power laws. We introduce HOT states in the context of percolation, and contrast properties of the high density HOT states with random configurations near the critical point. While both cases exhibit power laws, only HOT states display properties (1-3) associated with design and evolution. }
}
@Article{robert_carlson_doyle_01,
  author = 		 {Carl Robert and J.~M.~Carlson and John Doyle},
  title = 		 {Highly Optimized Tolerance in epidemic models incorporating local optimization and regrowth},
  journal = 	 {Physical Review E},
  year = 		 {2001},
  volume = 	 {63},
  note = 	 {056122},
  http = {http://www.physics.ucsb.edu/~complex/pubs/with.htm},
  abstract = {In the context of a coupled map model of population dynamics, which includes the rapid spread of fatal epidemics, we investigate the consequences of two new features in Highly Optimized Tolerance (HOT), a mechanism which describes how complexity arises in systems which are optimized for robust performance in the presence of a harsh external environment. Specifically, we (1) contrast global and local optimization criteria and (2) investigate the effects of time dependent regrowth. We find that both local and global optimization lead to HOT states, which may differ in their specific layouts, but share many qualitative features. Time dependent regrowth leads to HOT states which deviate from the optimal configurations in the corresponding static models in order to protect the system from slow (or impossible) regrowth which follows the largest losses and extinctions. While the associated map can exhibit complex, chaotic solutions, HOT states are confined to relatively simple dynamical regimes.}
}

@InProceedings{faloutsos3_99,
  author = 		 {Michalis Faloutsos and Petros Faloutsos and Christos Faloutsos},
  title = 		 {On Power-Law Relationships of the Internet Topology},
  booktitle = 	 {Proceedings of ACM SIGCOMM},
  year = 	 {1999},
  month = 	 sep,
  http = {http://citeseer.nj.nec.com/faloutsos99powerlaw.html},
  abstract = {Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45\% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96\% or higher. \par Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.}
}

@InProceedings{chen+_02,
  author = 		 {Qian Chen and Hyunseok Chang and Ramesh Govindan and Sugih Jamin and Scott J.~Shenker and Walter Willinger},
  title = 		 {The Origin of Power Laws in Internet Topologies Revisited},
  booktitle = 	 {Proceedings of IEEE INFOCOM},
  year = 	 {2002},
  month = 	 jun,
  http = {http://citeseer.nj.nec.com/461619.html},
  abstract = {In a recent paper, Faloutsos et al. \cite{faloutsos3_99} found that the inter Autonomous System (AS) topology exhibits a power-law vertex degree distribution. This result was quite unexpected in the networking community and stirred significant interest in exploring the possible causes of this phenomenon. The work of Barabasi and Albert \cite{barabasi_albert_99} and its application to network topology generation in the work of Medina et al. \cite{medina_matta_00} have explored a promising class of models that yield strict power-law vertex degree distributions. In this paper, we reexamine the BGP measurements that form the basis for the results reported in \cite{faloutsos3_99}. We find that by their very nature (i.e., being strictly BGP-based), the data provides a very incomplete picture of Internet connectivity at the AS level. The AS connectivity maps constructed from this data (the original maps) typically miss 20 50\% or even more of the physical links in AS maps constructed using additional sources (the extended maps). Subsequently, we find that while the vertex degree distributions resulting from the extended maps are heavy-tailed, they deviate significantly from a strict power law. Finally, we show that available historical data does not support the connectivity-based dynamics assumed in \cite{barabasi_albert_99}. Together, our results suggest that the Internet topology at the AS level may well have developed over time following a very different set of growth processes than those proposed in \cite{barabasi_albert_99}. }
}
@TechReport{medina_matta_00,
  author = 		 {Alberto Medina and Ibrahim Matta},
  title = 		 {Brite: A flexible generator of internet topologies},
  institution =  {Boston University},
  year = 		 {2000},
  number = 	 {BU-CS-TR-2000-005},
  address = 	 {Boston, MA},
}

@Article{barabasi_albert_99,
  author = 		 {Albert-L{\'a}szl{\'o} Barab{\'a}si and R{\'e}ka Albert},
  title = 		 {Emergence of scaling in random networks},
  journal = 	 {Science},
  year = 		 {1999},
  volume = 	 {286},
  pages = 	 {509--512},
  http = {http://www.nd.edu/~alb/public.html},
  abstract = {Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.}
}
@Article{barabasi_albert_jeong_99,
  author = 		 {Albert-L{\'a}szl{\'o} Barab{\'a}si and R{\'e}ka Albert and Hawoong Jeong},
  title = 		 {Mean-field theory for scale-free random networks},
  journal = 	 {Physica A},
  year = 		 {1999},
  volume = 	 {272},
  pages = 	 {173--187},
  http = {http://www.nd.edu/~alb/public.html},
  abstract = {Random networks with complex topology are common in Nature, describing systems as diverse as the world wide web or social and business networks. Recently, it has been demonstrated that most large networks for which topological information is available display scale-free features. Here we study the scaling properties of the recently introduced scale-free model, that can account for the observed power-law distribution of the connectivities. We develop a mean-field method to predict the growth dynamics of the individual vertices, and use this to calculate analytically the connectivity distribution and the scaling exponents. The mean-field method can be used to address the properties of two variants of the scale-free model, that do not display power-law scaling.}
}
@Article{albert_jeong_barabasi_00,
  author = 		 {R{\'e}ka Albert and Hawoong Jeong and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  title = 		 {Error and attack tolerance in complex networks},
  journal = 	 {Nature},
  year = 		 {2000},
  volume = 	 {406},
  pages = 	 {387--482},
  http = {http://www.nd.edu/~alb/public.html},
}
@Article{albert_barabasi_02,
  author = 		 {R{\'e}ka Albert and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  title = 		 {Statistical mechanics of complex networks},
  journal = 	 {Reviews of Modern Physics},
  year = 		 {2002},
  volume = 	 {74},
  pages = 	 {47--97},
  month = 	 jan,
  http = {http://www.nd.edu/~alb/public.html},
  abstract = {Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks is governed by robust organizing principles. Here we review the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, we discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, as well as the interplay between topology and the network's robustness against failures and attacks.}
}
@Article{dezso_barabasi_02,
  author = 		 {Zolt{\'a}n Dezs{\"o} and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  title = 		 {Halting viruses in scale-free networks},
  journal = 	 {Physical Review E},
  year = 		 {2002},
  volume = 	 {65},
  note = 	 {055103},
  http = {http://www.nd.edu/~alb/public.html},
  abstract = {The vanishing epidemic threshold for viruses spreading on scale-free networks indicate that traditional methods, aiming to decrease a virus' spreading rate cannot succeed in eradicating an epidemic. We demonstrate that policies that discriminate between the nodes, curing mostly the highly connected nodes, can restore a finite epidemic threshold and potentially eradicate a virus. We find that the more biased a policy is towards the hubs, the more chance it has to bring the epidemic threshold above the virus' spreading rate. Furthermore, such biased policies are more cost effective, requiring less cures to eradicate the virus.}
}

@INCOLLECTION{newm02,
  AUTHOR = {Mark E.~J.~Newman},
  TITLE = {Random graphs as models of networks},
  BOOKTITLE = {Handbook of Graphs and Networks},
  PUBLISHER = {Wiley-VCH},
  YEAR = 2002,
  EDITOR = {S. Bornholdt and H. G. Schuster},
  ADDRESS = {Berlin},
  NOTE = {To appear},
  HTTP = {http://arxiv.org/abs/cond-mat/0202208/},
  ABSTRACT = {The random graph of Erdos and Renyi is one of the oldest and best studied models of a network, and possesses the considerable advantage of being exactly solvable for many of its average properties. However, as a model of real-world networks such as the Internet, social networks or biological networks it leaves a lot to be desired. In particular, it differs from real networks in two crucial ways: it lacks network clustering or transitivity, and it has an unrealistic Poissonian degree distribution. In this paper we review some recent work on generalizations of the random graph aimed at correcting these shortcomings. We describe generalized random graph models of both directed and undirected networks that incorporate arbitrary non-Poisson degree distributions, and extensions of these models that incorporate clustering too. We also describe two recent applications of random graph models to the problems of network robustness and of epidemics spreading on contact networks.},
}

@Article{newman+_02,
  author = 		 {Mark E.~J.~Newman and Michelle Girvan and J.~Doyne Farmer},
  title = 		 {Optimal design, robustness, and risk aversion},
  journal = 	 {Phys. Rev. Lett},
  year = 		 {2002},
  volume = 	 {89},
  month = 	 jul,
  note = 	 {028301},
  http = {http://aps.arxiv.org/abs/cond-mat/0202330/},
  abstract = {Highly optimized tolerance is a model of optimization in engineered systems, which gives rise to power-law distributions of failure events in such systems. The archetypal example is the highly optimized forest fire model. Here we give an analytic solution for this model which explains the origin of the power laws. We also generalize the model to incorporate risk aversion, which results in truncation of the tails of the power law so that the probability of disastrously large events is dramatically lowered, giving the system more robustness.}
}

@Article{stubna_fowler_03,
  author = 		 {M.D.~Stubna and J.~Fowler},
  title = 		 {An Application of the Highly Optimized Tolerance Model to Electrical Blackouts},
  journal = 	 {International Journal of Bifurcation and Chaos},
  year = 		 {2003},
  volume = 	 {13},
  number = 	 {1},
  pages = 	 {237--242},
  pdf = {http://www.chaos.cornell.edu/hot3.pdf},
  http = {http://www.worldscinet.com/journals/ijbc/13/1301/S0218127403006492.html},
  abstract = {The recently proposed {\it Highly Optimized Tolerance} (H.O.T.) model \cite{carlson_doyle99}, \cite{carlson_doyle00}, which aims to describe the statistics of robust complex systems in uncertain environments, is compared with data from the Western United States (W.S.C.C.) power distribution system. We use for comparison a 15-year record of all power outages occurring on the grid, measured in the size of megawatts lost and the number of customers without service. In applying the model to the power grid data, we find that the problem of determining how the resources in the system scale with event size is nontrivial given the assumptions of the model and the information about how the power grid actually operates. Further, we observe that the model agrees closely with the W.S.C.C. data for the megawatts but not the customers, and consequently propose that the assumption in the model of optimal resource distribution is not valid in general when more than one measure of event size is used. A modified H.O.T. model which allows for {\it resource misallocation} is introduced and we find that this model can be made to fit both data sets reasonably well.}
}

@Article{newman_forrest_balthrop_02,
  author = 		 {M.~E.~J. Newman and Stephanie Forrest and Justin Balthrop},
  title = 		 {Email Networks and the Spread of Computer Viruses},
  journal = 	 {Physical Review E},
  year = 		 {2002},
  volume = 	 {66},
  note = 	 {035101},
  http = {http://www.cs.unm.edu/~immsec/papers.htm},
  abstract = {Many computer viruses spread via electronic mail, making use of computer users  email address books as a source for email addresses of new victims. These address books form a directed social network of connections between individuals over which the virus spreads. Here we investigate empirically the structure of this network using data drawn from a large computer installation, and discuss the implications of this structure for the understanding and prevention of computer virus epidemics.}
}

@TechReport{leveille_02,
  author = 		 {Jasmin Leveille},
  title = 		 {Epidemic Spreading in Technological Networks},
  institution =  {HP Laboratories Bristol},
  year = 		 {2002},
  number = 	 {HPL-2002-287},
  month = 	 oct,
  http = {http://www.hpl.hp.com/techreports/2002/HPL-2002-287.html},
  abstract = {Recent computer worms pose a major threat to large computer networks, and it is a general belief that understanding their means of propagation will help to devise efficient control strategies. This dissertation proposes a new epidemiological model to account for particular characteristics of computer worm epidemics. This new model, termed the Progressive Susceptible- Infected-Detected-Removed (PSIDR) epidemiological model, incorporates new aspects related to the availability of antivirus signatures, to the existence of direct immunization, and to the presence of a curing phase. Various costs are incorporated in the model, which allow us to determine the best strategies to fight worms. The model undergoes an extensive series of validation tests, its properties being evaluated mostly numerically. The model shows good agreement with empirical data. The paper then investigates current response strategies as well as the effect of virus throttling. The model yields both practical recommendations and new insights about the observed low prevalence of worms over the Internet.}
}

@Article{pastorsatorras_vespignani_01,
  author = 		 {Romualdo Pastor-Satorras and Alessandro Vespignani},
  title = 		 {Epidemic Spreading in Scale-Free Networks},
  journal = 	 {Physical Review Letters},
  year = 		 {2001},
  volume = 	 {86},
  pages = 	 {3200--3203},
  http = {http://xxx.lanl.gov/abs/cond-mat/0010317},
  abstract = {The Internet, as well as many other networks, has a very complex connectivity recently modeled by the class of scale-free networks. This feature, which appears to be very efficient for a communications network, favors at the same time the spreading of computer viruses. We analyze real data from computer virus infections and find the average lifetime and prevalence of viral strains on the Internet. We define a dynamical model for the spreading of infections on scale-free networks, finding the absence of an epidemic threshold and its associated critical behavior. This new epidemiological framework rationalize data of computer viruses and could help in the understanding of other spreading phenomena on communication and social networks.}
}
@Article{pastorsatorras_vespignani_02,
  author = 		 {Romualdo Pastor-Satorras and Alessandro Vespignani},
  title = 		 {Epidemic dynamics in finite size scale-free networks},
  journal = 	 {Physcal Review E},
  year = 		 {2002},
  volume = 	 {65},
  note = 	 {035108},
  http = {http://xxx.lanl.gov/abs/cond-mat/0202298},
  abstract = {Many real networks present a bounded scale-free behavior with a connectivity cut-off due to physical constraints or a finite network size. We study epidemic dynamics in bounded scale-free networks with soft and hard connectivity cut-offs. The finite size effects introduced by the cut-off induce an epidemic threshold that approaches zero at increasing sizes. The induced epidemic threshold is very small even at a relatively small cut-off, showing that the neglection of connectivity fluctuations in bounded scale-free networks leads to a strong over-estimation of the epidemic threshold. We provide the expression for the infection prevalence and discuss its finite size corrections. The present work shows that the highly heterogeneous nature of scale-free networks does not allow the use of homogeneous approximations even for systems of a relatively small number of nodes.}
}
@InBook{pastorsatorras_vespignani_02b,
  author = 	 {Romualdo Pastor-Satorras and Alessandro Vespignani},
  editor = 	 {S.~Bornholdt and H.G.~Schuster},
  title = 		 {Handbook of Graphs and Networks: From the Genome to the Internet},
  chapter = 	 {Epidemics and immunization in scale-free networks},
  publisher = 	 {Wiley-VCH},
  year = 		 {2002},
  address = 	 {Berlin},
  month = may,
  http = {http://xxx.lanl.gov/abs/cond-mat/0205260},
  abstract = {In this chapter we want to provide a review of the main results obtained in the modeling of epidemic spreading in scale-free networks. In particular, we want to show the different epidemiological framework originated by the lack of any epidemic threshold and how this feature is rooted in the extreme heterogeneity of the scale-free networks' connectivity pattern.}
}

@Article{klemm_eguiluz_02a,
  author = 		 {Konstantin Klemm and Victor M.~Egu{\'i}luz},
  title = 		 {Highly clustered scale-free networks},
  journal = 	 {Physical Review E},
  year = 		 {2002},
  volume = {65},
  month = 	 dec,
  note = 	 {036123},
  http = {http://xxx.lanl.gov/abs/cond-mat/0107606},
  abstract = {We propose a model for growing networks based on a finite memory of the nodes. The model shows stylized features of real-world networks: power law distribution of degree, linear preferential attachment of new links and a negative correlation between the age of a node and its link attachment rate. Notably, the degree distribution is conserved even though only the most recently grown part of the network is considered. This feature is relevant because real-world networks truncated in the same way exhibit a power-law distribution in the degree. As the network grows, the clustering reaches an asymptotic value larger than for regular lattices of the same average connectivity. These high-clustering scale-free networks indicate that memory effects could be crucial for a correct description of the dynamics of growing networks. }
}
@Article{klemm_eguiluz_02b,
  author = 		 {Konstantin Klemm and Victor M.~Egu{\'i}luz},
  title = 		 {Growing Scale-Free Networks with Small World Behavior},
  journal = 	 {Physical Review E},
  year = 		 {2002},
  volume = {65},
  note = 	 {057102},
  http = {http://xxx.lanl.gov/abs/cond-mat/0107607},
  abstract = {In the context of growing networks, we introduce a simple dynamical model that unifies the generic features of real networks: scale-free distribution of degree and the small world effect. While the average shortest path length increases logartihmically as in random networks, the clustering coefficient assumes a large value independent of system size. We derive expressions for the clustering coefficient in two limiting cases: random (C ~ (ln N)^2 / N) and highly clustered (C = 5/6) scale-free networks.}
}
@Article{eguiluz_klemm_02,
  author = 		 {Victor M.~Egu{\'i}luz and Konstantin Klemm},
  title = 		 {Epidemic threshold in structured scale-free networks},
  journal = 	 {Physical Review Letters},
  year = 		 {2002},
  volume = 	 {89},
  number = 	 {10},
  month = 	 sep,
  note = 	 {108701},
  http = {http://xxx.lanl.gov/abs/cond-mat/0205439},
  abstract = {We analyze the spreading of viruses in scale-free networks with high clustering and degree correlations, as found in the Internet graph. For the Suscetible-Infected-Susceptible model of epidemics the prevalence undergoes a phase transition at a finite threshold of the transmission probability. Comparing with the absence of a finite threshold in networks with purely random wiring, our result suggests that high clustering and degree correlations protect scale-free networks against the spreading of viruses. We introduce and verify a quantitative description of the epidemic threshold based on the connectivity of the neighborhoods of the hubs.}
}
@Unpublished{eguiluz+_03,
  author = 		 {Victor M.~Egu{\'i}luz and Emilio Hern{\'a}ndez-Garc{\'i}a and Oreste Piro and Konstantin Klemm},
  title = 		 {Effective dimensions and percolation in hierarchically structured scale-free networks},
  note = 		 {submitted to Physical Review E},
  year = 	 {2003},
  http = {http://xxx.lanl.gov/abs/cond-mat/0302515},
  abstract = {We introduce appropriate definitions of dimensions in order to characterize the fractal properties of complex networks. We compute these dimensions in a hierarchically structured network of particular interest. In spite of the nontrivial character of this network that displays scale-free connectivity among other features, it turns out to be approximately one-dimensional. The dimensional characterization is in agreement with the results on statistics of site percolation and other dynamical processes implemented on such a network.}
}
