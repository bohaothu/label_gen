@inproceedings {INPROC-2011-61,
   author = {Lars Geiger and Frank D{\"u}rr and Kurt Rothermel},
   title = {{Adaptive Routing in a Contextcast Overlay Network}},
   booktitle = {Proceedings of the IEEE 7th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob 2011)},
   publisher = {IEEE Xplore},
   institution = {Universit{\"a}t Stuttgart : Sonderforschungsbereich SFB 627 (Nexus: Umgebungsmodelle f{\"u}r mobile kontextbezogene Systeme), Germany},
   pages = {1--8},
   type = {Konferenz-Beitrag},
   month = {Oktober},
   year = {2011},
   language = {Englisch},
   cr-category = {C.2.1 Network Architecture and Design,
                   C.2.2 Network Protocols,
                   C.2.4 Distributed Systems,
                   C.2.6 Internetworking},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Context-based communication allows for the dissemination of messages to mobile
      users with a specified context, i.e., at a location and with certain attribute
      values. This enables, e.g., a message to students on campus attending a certain
      class, with information about a study group for an upcoming exam. An overlay
      network of context-aware routers efficiently disseminate the messages to all
      matching receivers. Directed forwarding of such messages requires that the
      routers maintain knowledge about the contexts of connected users. Global
      knowledge, i.e., each router knowing about every user, scales poorly, though,
      because of the necessary updates.
      
      To overcome this challenge, a router can selectively propagate context
      information that actually allows its neighbors to prune a message distribution
      tree. In this paper, we present an approach to adaptively propagate only those
      user contexts that offer a reduction in overall system load. The algorithm
      automatically and locally adapts to the observed messages and user contexts on
      each node.
      
      Our solution significantly improves the scalability of the system by reducing
      the overall load by almost 50\%.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-61&amp;engl=0}
}

@inproceedings {INPROC-2011-59,
   author = {Bilal Hameed and Jorge Minguez and Michael W{\"o}rner and Philip Hollstein and Sema Zor and Stefan Silcher and Frank D{\"u}rr and Kurt Rothermel},
   title = {{The Smart Real-Time Factory as a Product Service System}},
   booktitle = {3rd CIRP International Conference on Industrial Product Service Systems},
   address = {Braunschweig, Germany},
   publisher = {IRP},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--6},
   type = {Konferenz-Beitrag},
   month = {Januar},
   year = {2011},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {In modern manufacturing landscape, companies are increasingly relying on
      product service systems i.e. bundling of products and services together in
      order to gain a competitive edge. In this article we present the Smart
      Real-Time Factory, a smart digital manufacturing environment that can transform
      the process of production into an informational service for the customers. The
      different components of the smart factory are discussed at length along with a
      discussion of the different services that can be offered by the smart factory.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-59&amp;engl=0}
}

@inproceedings {INPROC-2011-51,
   author = {Patrick Baier and Harald Weinschrott and Frank D{\"u}rr and Kurt Rothermel},
   title = {{MapCorrect: Automatic Correction and Validation of Road Maps Using Public Sensing}},
   booktitle = {To appear in 36th Annual IEEE Conference on Local Computer Networks (LCN 2011)},
   address = {Bonn, Germany},
   publisher = {IEEE Computer Society},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--8},
   type = {Konferenz-Beitrag},
   month = {Oktober},
   year = {2011},
   keywords = {ad-hoc; mobile; public sensing},
   language = {Englisch},
   cr-category = {C.2 Computer-Communication Networks},
   contact = {patrick.baier@ipvs.uni-stuttgart.de},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {With the increasing proliferation of small and cheap GPS receivers, a new way
      of generating road maps could be witnessed over the last few years.
      Participatory mapping approaches like OpenStreetMap, for instance, introduced a
      way to generate road maps collaboratively from scratch. Nevertheless, one of
      the main problems of these maps is their unknown quality in terms of accuracy.
      To address this issue, we propose MapCorrect: An automatic map correction and
      validation system. MapCorrect automatically collects GPS traces from people's
      mobile devices to correct a given road map and validate it. Since the
      collection of GPS data raises concerns about the energy consumption of the
      participating mobile devices, we tackle this issue by introducing a selective
      sensing mechanism. Furthermore, we show by simulation that using this approach
      up to 50\% of energy on the mobile phones can be saved while not impairing the
      map correction and validation process at all.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-51&amp;engl=0}
}

@inproceedings {INPROC-2011-49,
   author = {Stefan F{\"o}ll and Klaus Herrmann and Kurt Rothermel},
   title = {{PreCon - Expressive Context Prediction using Stochastic Model Checking}},
   booktitle = {Proceedings of the 8th International Conference on Ubiquitous Intelligence and Computing (UIC 2011)},
   address = {Banff, Canada},
   publisher = {Springer-Verlag},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   series = {Lecture Notes in Computer Science},
   pages = {1--15},
   type = {Konferenz-Beitrag},
   month = {September},
   year = {2011},
   keywords = {Context Prediction, Semi-Markov Model, Stochastic Model Checking, Temporal Logic},
   language = {Deutsch},
   cr-category = {G.3 Probability and Statistics,
                   I.2.6 Artificial Intelligence Learning,
                   I.6.4 Model Validation and Analysis},
   contact = {stefan.foell@ipvs.uni-stuttgart.de},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Ubiquitous systems need to determine the context of humans to deliver the right
      services at the right time. As the needs of humans are often coupled to their
      future context, the ability to predict relevant changes in a user's context is
      a key factor for providing intelligence and proactivity. Current context
      prediction systems only allow applications to query for the next user context
      (e.g. the user's next location). This severely limits the benefit of context
      prediction since these approaches cannot answer more expressive time-dependent
      queries (e.g. will the user enter location X within the next 10 minutes?).
      Neither can they handle predictions of multi-dimensional context (e.g. activity
      and location). We propose PreCon, a new approach to predicting
      multi-dimensional context. PreCon improves query expressiveness, providing
      clear formal semantics by applying stochastic model checking methods. PreCon is
      composed of three major parts: a stochastic model to represent context changes,
      an expressive temporal-logic query language, and stochastic algorithms for
      predicting context. In our evaluations, we apply PreCon to real context traces
      from the domain of healthcare and analyse the performance using well-known
      metrics from information retrieval. We show that PreCon reaches an F-score
      (combined precision and recall) of about 0.9 which indicates a very good
      performance.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-49&amp;engl=0}
}

@inproceedings {INPROC-2011-44,
   author = {Marco V{\"o}lz and Boris Koldehofe and Kurt Rothermel},
   title = {{Supporting Strong Reliability for Distributed Complex Event Processing Systems}},
   booktitle = {To appear in Proceedings of 13th IEEE International Conference on High Performance Computing and Communications (HPCC-2011)},
   publisher = {IEEE Computer Society Press},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--12},
   type = {Konferenz-Beitrag},
   month = {September},
   year = {2011},
   language = {Deutsch},
   cr-category = {C.2.4 Distributed Systems},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Many application classes such as monitoring applications, involve processing a
      massive amount of data from a possibly huge number of data sources. Complex
      Event Processing (CEP) has evolved as the paradigm of choice to determine
      meaningful situations (complex events) by performing stepwise correlation over
      event streams. To keep up with the high scalability demands of growing input
      streams, recent approaches distribute event correlation over several
      correlation nodes. However, the distribution of event correlation severely
      limits the reliability of a CEP system. Already a failure of a single
      correlation node impacts the correctness of the final correlation result.
      Increasing the availability by a naive application of established replication
      principles introduces new problems in the context of CEP. In particular,
      ensuring the lossless delivery of events and the detection of duplicate events
      before processing them is a challenging task. In this paper, we illustrate the
      importance of a strong reliability semantics for CEP in the context of a
      monitoring application in a distributed production environment. Strong
      reliability ensures each complex event is detected and delivered exactly once
      to each application entity. We present a replication scheme which ensures
      strong reliability in an asynchronous system model and can be applied to an
      arbitrary distributed CEP system. The algorithm tolerates f simultaneous
      failures by introducing f additional replicas for each correlation node. We
      prove correctness as well as evaluate the overhead introduced by the algorithm.
      Results show, that the overhead scales linearly with the number of deployed
      replicas and the node failure rate.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-44&amp;engl=0}
}

@inproceedings {INPROC-2011-43,
   author = {Bj{\"o}rn Schilling and Boris Koldehofe and Kurt Rothermel},
   title = {{Efficient and Distributed Rule Placement in Heavy Constraint-Driven Event Systems}},
   booktitle = {Proceedings of the 13th IEEE International Conference on High Performance Computing and Communications (HPCC-2011)},
   publisher = {IEEE},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   type = {Konferenz-Beitrag},
   month = {September},
   year = {2011},
   language = {Deutsch},
   cr-category = {C.2.4 Distributed Systems},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Complex Event Processing (CEP) is of increasing importance in many industrial
      applications to integrate a huge number of events in a scalable manner. A core
      challenge towards scalable CEP is to efficiently distribute the rules which
      define how correlations between events can be detected within an event
      processing network. Furthermore, migration of rules is essential to adapt to
      changing conditions. While recently significant effort has been spent on
      optimizing CEP with respect to dedicated optimization goals, such as minimizing
      latency and bandwidth usage, there remains a fundamental gap in supporting
      requirements that emerge from deploying CEP over heterogeneous and independent
      processing environments. Heterogeneity typically imposes many constraints on
      the placement of rules, which increases the complexity of the underlying
      optimization problem and cannot be handled efficiently by existing solutions.
      
      In this paper we examine the distributed placement, migration and optimization
      of rules in the context of the constraint optimization problem to minimize
      network usage. We propose and evaluate a placement algorithm that efficiently
      finds valid solutions in scenarios where the solution space is heavily
      restricted by constraints. The algorithm operates in a decentralized way and is
      adaptive to dynamic changes of processing nodes, rules, and load
      characteristics of the event processing network. The optimization algorithm
      adopts techniques from simulated annealing to avoid local minima. Furthermore,
      the proposed rule migration policies resolve invalid placements quickly and
      therefore ensure high availability. The evaluations show that the proposed
      algorithm is able to efficiently find near optimum solutions within heavy
      constraint-driven network conditions.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-43&amp;engl=0}
}

@inproceedings {INPROC-2011-31,
   author = {Daniel Fischer and Stefan F{\"o}ll and Klaus Herrmann and Kurt Rothermel},
   title = {{Energy-efficient Workflow Distribution}},
   booktitle = {Proceedings of the Fifth International Conference on Communication System Software and Middleware(COMSWARE 2011)},
   address = {Verona, Italy},
   publisher = {ACM},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--8},
   type = {Konferenz-Beitrag},
   month = {Juli},
   year = {2011},
   keywords = {Workflow distribution; Energy efficiency; Minimum Cut},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems,
                   H.4 Information Systems Applications},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Pervasive computing and business process modeling are joining forces, as mobile
      human users shall be seamlessly integrated into business processes. This
      current trend gains momentum. In respective scenarios, humans use mobile
      devices and wireless communication technology to interact with electronic
      workflows that are entirely running in some powerful back-end infrastructure.
      However, the high degree of interaction between humans and their workflows
      causes a high communication overhead, which consumes a significant amount of
      energy on the mobile devices. This incurs a negative impact on the usability
      and on the efficiency of the overall business process due to rapidly drained
      batteries and the resulting short life-times of the devices and applications.
      We present an approach based on the well-known minimum-cut algorithm for
      reducing the costly data transmissions during workflow execution by
      distributing parts of a workflow to the users' devices. Our main motivation is
      to reduce the energy consumption on the mobile devices and, thus, avoid
      situations in which batteries are drained in the field, rendering the usage of
      mobile devices more efficient. We prove that our algorithm finds the optimal
      solution for a given network and a workflow. Our evaluations show that our
      approach decreases the energy consumed on mobile devices by 32-37\% compared to
      an approach where the entire workflow is executed in a central infrastructure.
      Thus, if mobile devices are primarily used for executing workflows (as seen in
      application domains like logistics and health care), one third of the energy
      can be saved. This either means that devices have to be charged less
      frequently, leading to less distraction in the business process, or that mobile
      device specifications can be lowered. Significant cost reductions result in
      both cases.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-31&amp;engl=0}
}

@inproceedings {INPROC-2011-30,
   author = {Andreas Benzing and Boris Koldehofe and Kurt Rothermel},
   title = {{Efficient Support for Multi-Resolution Queries in Global Sensor Networks}},
   booktitle = {Proceedings of the Fifth International Conference on COMmunication System softWAre and middlewaRE: COMSWARE 2011},
   publisher = {ACM},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--12},
   type = {Konferenz-Beitrag},
   month = {Juli},
   year = {2011},
   doi = {10.1145/2016551.2016562},
   keywords = {DSPS, global sensor network, indexing, query processing},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems},
   ee = {ftp://ftp.informatik.uni-stuttgart.de/pub/library/ncstrl.ustuttgart_fi/INPROC-2011-30/INPROC-2011-30.pdf,
      http://doi.acm.org/10.1145/2016551.2016562},
   contact = {andreas.benzing@ipvs.uni-stuttgart.de},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Stream processing has evolved as a paradigm for efficiently sharing and
      integrating a massive amount of data into applications. However, the
      integration of globally dispersed sensor data imposes challenges in the
      effective utilization of the IT infrastructure that forms the global sensor
      network. Especially, simulations require the integration of sensor streams at
      widely differing spatial and temporal resolutions. For current stream
      processing solutions it is necessary to generate a separate data stream for
      each requested resolution. Therefore, these systems will suffer from high
      redundancy in data streams, wasting a significant amount of bandwidth and
      limiting their scalability.
      
      This paper presents a new approach to scalable distributed stream processing of
      data which stems from globally dispersed sensor networks. The approach supports
      applications in establishing continuous queries for sensor data at different
      resolutions and ensures efficient bandwidth usage of the data distribution
      network. Unlike existing work in the context of video stream processing which
      provides multiple resolutions by establishing separate channels for each
      resolution, this paper presents a stream processing system that can efficiently
      split/combine data streams in order to decrease/increase their resolution
      without loss in data precision. In addition the system provides mechanisms for
      load balancing of sensor data streams that allow efficient utilization of the
      bandwidth of the global sensor network.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-30&amp;engl=0}
}

@inproceedings {INPROC-2011-22,
   author = {Andreas Grau and Klaus Herrmann and Kurt Rothermel},
   title = {{NETbalance: Reducing the Runtime of Network Emulation using Live Migration}},
   booktitle = {Proceedings of the 20th International Conference on Computer Communication Networks (ICCCN'11)},
   address = {Maui, HI, USA},
   publisher = {IEEE Computer Society},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--6},
   type = {Konferenz-Beitrag},
   month = {August},
   year = {2011},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems},
   ee = {ftp://ftp.informatik.uni-stuttgart.de/pub/library/ncstrl.ustuttgart_fi/INPROC-2011-22/INPROC-2011-22.pdf,
      http://dx.doi.org/10.1109/ICCCN.2011.6005793},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Network emulation is an efficient method for evaluating distributed
      applications and communication protocols by combining the benefits of real
      world experiments and network simulation. The process of network emulation
      involves the execution of connected instances of the software under test
      (called virtual nodes) in a controlled environment. In previous work, we
      introduced an approach to minimize the runtime of network emulation experiments
      based on prior known average resource requirements of virtual nodes.
      
      In this paper, we introduce NETbalance, a novel approach to runtime reduction
      for experiments with unknown or varying resource requirements. NETbalance
      migrates virtual nodes during an experiment to distribute the load evenly
      across the physical nodes, avoiding overloaded nodes and exploiting the idle
      resources on underloaded nodes for speeding up the experiment execution. We
      make the following contributions: First, we present an emulation architecture
      for efficiently supporting live migration of virtual nodes. Second, we propose
      a cost model for determining the runtime reduction achieved through the
      migration. Third, we introduce an algorithm for calculating placements that
      minimize the experiment runtime. Our evaluations of the NETbalance prototype
      show, that it is able to reduce the experiment runtime by up to 70\%.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-22&amp;engl=0}
}

@inproceedings {INPROC-2011-19,
   author = {Christian Hiesinger and Daniel Fischer and Stefan F{\"o}ll and Herrmann Klaus and Kurt Rothermel},
   title = {{Minimizing Human Interaction Time in Workflows}},
   booktitle = {Proceedings of the Sixth International Conference on Internet and Web Applications and Services (ICIW 2011)},
   address = {St. Maarten, the Netherlands Antilles},
   publisher = {IARIA},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {22--28},
   type = {Konferenz-Beitrag},
   month = {M{\"a}rz},
   year = {2011},
   keywords = {Workflow distribution; human interaction; pervasive workflows},
   language = {Englisch},
   cr-category = {D.2.4 Software Engineering Software/Program Verification,
                   H.4 Information Systems Applications},
   contact = {christian.hiesinger@ipvs.uni-stuttgart.de},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Many business scenarios require humans to interact with workflows. To support
      humans as unobtrusively as possible in the execution of their activities, it is
      important to keep the interaction time experienced by humans as low as
      possible. The time required for such interactions is influenced by two factors:
      First, by the runtime of the services that are used by a workflow during an
      interaction. Second, by the time required to transfer data between workflow
      servers and services that may be distributed in a global network. We propose an
      algorithm that computes a suitable distribution of a workflow in such a
      network. The goal of our algorithm is to minimize the time required for
      interactions between a human and a workflow. Current approaches in the domain
      of workflow optimization pay little attention towards optimizing a workflow to
      increase the usability for humans. We show the feasibility of our approach by
      comparing our algorithm with two non-distributed approaches and a distributed
      approach which is based on a greedy algorithm and show that our algorithm
      outperforms these approaches.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-19&amp;engl=0}
}

@inproceedings {INPROC-2011-11,
   author = {Frank D{\"u}rr and Pavel Skvortsov and Kurt Rothermel},
   title = {{Position Sharing for Location Privacy in Non-trusted Systems}},
   booktitle = {Proceedings of the 9th IEEE International Conference on Pervasive Computing and Communications (PerCom 2011)},
   address = {Seattle, USA},
   publisher = {IEEE Computer Society},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {189--196},
   type = {Konferenz-Beitrag},
   month = {M{\"a}rz},
   year = {2011},
   keywords = {location-based service; privacy; obfuscation; sharing; location management},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems,
                   H.3.5 Online Information Services},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Many novel location-based services (LBS) such as a friend finder service
      require knowledge about the positions of mobile users. Usually, location
      services are used to manage these positions, and for providing basic
      functionality like spatial range queries or spatial events to the LBS. Managing
      and using the positions of mobile users raises privacy issues, in particular,
      if the providers of LBS and location services are only partially trusted. Many
      different approaches for preserving a user’s privacy have been proposed in the
      literature, e.g. location obfuscation and the k-anonymity concept. However,
      most of them are not suitable if both LBS and location service providers are
      non-trusted. In contrast to these approaches, we present a novel approach for
      the secure management of private position information in partially trusted
      system environments. The main contribution in this paper is a position sharing
      concept which allows for the distribution of position information (shares) of
      strictly limited accuracy onto several location servers of different providers.
      With this approach, a compromised server will only reveal information of
      limited accuracy. Moreover, we will show how position shares of coarse
      granularity from multiple location servers can be fused into information of
      higher precision to satisfy the accuracy requirements of different LBS.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-11&amp;engl=0}
}

@inproceedings {INPROC-2011-10,
   author = {Harald Weinschrott and Julian Weisser and Frank D{\"u}rr and Kurt Rothermel},
   title = {{Participatory Sensing Algorithms for Mobile Object Discovery in Urban Areas}},
   booktitle = {Proceedings of the 9th Annual IEEE International Conference on Pervasive Computing and Communications},
   publisher = {IEEE Computer Society},
   institution = {Universit{\"a}t Stuttgart : Sonderforschungsbereich SFB 627 (Nexus: Umgebungsmodelle f{\"u}r mobile kontextbezogene Systeme), Germany},
   pages = {1--8},
   type = {Konferenz-Beitrag},
   month = {M{\"a}rz},
   year = {2011},
   language = {Englisch},
   cr-category = {C.2 Computer-Communication Networks},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {This paper introduces mechanisms for the automated detection of mobile objects
      in urban areas. Widely available devices such as mobile phones with integrated
      proximity sensors such as RFID readers or Bluetooth cooperatively perform
      sensing operations to discover mobile objects. In this paper, we propose a
      coverage metric for assessing the completeness of sensing that considers
      spatial and temporal aspects. To maximize coverage while minimizing energy
      consumption of mobile nodes, we propose both a centralized and a distributed
      coordination algorithm for selecting nodes that need to sense. Moreover, we
      present strategies that allow selected nodes to perform efficient sense
      operations. By extensive simulations, we show that distributed coordination
      achieves drastic energy savings of up to 63\%, while limiting the coverage loss
      to 13\%. Moreover, we show that the centralized algorithm loses less than 1\%
      coverage compared to the maximum possible coverage.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-10&amp;engl=0}
}

@inproceedings {INPROC-2011-09,
   author = {Umakishore Ramachandran and Liviu Iftode and Rajnish Kumar and Santosh Pande and Kurt Rothermel and Boris Koldehofe},
   title = {{Large-scale Situational Awareness with Camera Networks and Multimodal Sensing}},
   booktitle = {NSF Workshop on Pervasive Computing at Scale (PeCS)},
   publisher = {online},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   pages = {1--2},
   type = {Workshop-Beitrag},
   month = {Januar},
   year = {2011},
   keywords = {Distributed Systems},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {This white paper raises the challenges and solution approaches for dealing with
      large-scale media-rich infrastructures for addressing the needs of large-scale
      sensor-based applications, often classified as situation awareness
      applications, using smart surveillance as a canonical example.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-09&amp;engl=0}
}

@inproceedings {INPROC-2011-08,
   author = {Gerald G. Koch and Andreas Benzing and Christoph P. Mayer},
   title = {{An Approach for Urban Sensing with Quality-Aware Situation Detection and Efficient Communication}},
   booktitle = {Proceedings of the Workshops der wissenschaftlichen Konferenz Kommunikation in verteilten Systemen 2011 (WowKiVS 2011); Kiel, Germany, March 11th, 2011},
   editor = {Horst Hellbr{\"u}ck and Norbert Luttenberger and Volker Turau},
   publisher = {EASST},
   institution = {Universit{\"a}t Stuttgart, Fakult{\"a}t Informatik, Elektrotechnik und Informationstechnik, Germany},
   series = {Electronic Communications of the EASST},
   pages = {1--10},
   type = {Workshop-Beitrag},
   month = {M{\"a}rz},
   year = {2011},
   issn = {1863-2122},
   keywords = {Urban sensing; Internet of Things; Complex Event Processing; CEP; Delay Tolerant Network; DTN; Distributed Diagnostic Simulation},
   language = {Englisch},
   cr-category = {C.2.2 Network Protocols,
                   C.2.4 Distributed Systems,
                   I.6.3 Simulation and Modeling Applications},
   ee = {ftp://ftp.informatik.uni-stuttgart.de/pub/library/ncstrl.ustuttgart_fi/INPROC-2011-08/INPROC-2011-08.pdf},
   contact = {gerald.koch@ipvs.uni-stuttgart.de andreas.benzing@ipvs.uni-stuttgart.de mayer@kit.edu},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Urban Sensing employs physical-world mining to create a digital model of the
      physical world using a large number of sensors. Handling the large amount of
      data generated by sensors is costly and requires energy-saving measures for
      sensing and sensor data transmission. Such schemes often affect data quality
      and message delay. However, the detection of real-world situations using
      Complex Event Processing on sensor data has to be dependable and timely and
      requires precise data. In this position paper, we propose an approach to
      integrate the contradicting optimization goals of energy-efficient wireless
      sensor networks and dependable situation detection. It separates the system
      into the following tiers: First, to support energy-efficiency and allow sparse,
      unconnected sensor networks, we exploit the mobility of people through Delay
      Tolerant Networking for collecting sensor data. This frees sensor nodes from
      energy-expensive routing. Second, we employ Diagnostic Simulation which
      provides data that is complete, precise and in time and therefore supports
      quality-aware situation detection.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2011-08&amp;engl=0}
}

@article {ART-2011-11,
   author = {Ralph Lange and Frank D{\"u}rr and Kurt Rothermel},
   title = {{Efficient real-time trajectory tracking}},
   journal = {The VLDB Journal},
   publisher = {Springer Berlin Heidelberg},
   pages = {1--24},
   type = {Artikel in Zeitschrift},
   month = {Juni},
   year = {2011},
   doi = {10.1007/s00778-011-0237-7},
   issn = {1066-8888},
   keywords = {moving objects database; MOD; trajectory tracking; dead reckoning; line simplification},
   language = {Englisch},
   cr-category = {H.2.8 Database Applications},
   contact = {ralph.lange@ipvs.uni-stuttgart.de},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Moving objects databases (MOD) manage trajectory information of vehicles,
      animals, and other mobile objects. A crucial problem is how to efficiently
      track an object's trajectory in real-time, in particular if the trajectory data
      is sensed at the mobile object and thus has to be communicated over a wireless
      network. We propose a family of tracking protocols that allow trading the
      communication cost and the amount of trajectory data stored at a MOD off
      against the spatial accuracy. With each of these protocols, the MOD manages a
      simplified trajectory that does not deviate by more than a certain accuracy
      bound from the actual movement. Moreover, the different protocols enable
      several trade-offs between computational costs, communication cost, and the
      reduction in the trajectory data: Connection-Preserving Dead Reckoning
      minimizes the communication cost using dead reckoning, a technique originally
      designed for tracking an object's current position. Generic Remote Trajectory
      Simplification (GRTS) further separates between tracking of the current
      position and simplification of the past trajectory and can be realized with
      different line simplification algorithms. For both protocols, we discuss how to
      bound the space consumption and computing time at the moving object and thereby
      present an effective compression technique to optimize the reduction
      performance of real-time line simplification in general. Our evaluations with
      hundreds of real GPS traces show that a realization of GRTS with a simple
      simplification heuristic reaches 85-90\% of the best possible reduction rate,
      given by retrospective offline simplification. A realization with the optimal
      line simplification algorithm by Imai and Iri even reaches more than 97\% of the
      best possible reduction rate.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=ART-2011-11&amp;engl=0}
}

@article {ART-2011-10,
   author = {Anders Gidenstam and Boris Koldehofe and Marina Papatriatafilou and Philippas Tsigas},
   title = {{Scalable group communication supporting configurable levels of consistency}},
   journal = {Scalable group communication supporting configurable levels of consistency},
   editor = {To appear: Concurrency and Computation: Practice and Experience.},
   publisher = {John Wiley \& Sons, Inc.},
   pages = {1--22},
   type = {Artikel in Zeitschrift},
   month = {Juni},
   year = {2011},
   keywords = {Distributed Systems, Group Communication, Consistency},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Group communication is deployed in many evolving Internet-scale cooperative
      applications such as multiplayer online games and virtual worlds to efficiently
      support interaction on information relevant to a potentially very large number
      of users or objects. Especially peer-to-peer based group communication
      protocols have evolved as a promising approach to allow intercommunication
      between many distributed peers. Yet the delivery semantics of robust and
      scalable protocols such as gossiping is not sufficient to support consistency
      semantics beyond eventual consistency since no relationship on the order of
      events is enforced. On the other hand traditional consistency models provided
      by reliable group communication providing causal or even total order are
      restricted to support only small groups. This article proposes the cluster
      consistency model which bridges the gap between traditional and current
      approaches in supporting both scalability and ordered event delivery. We
      introduce a dynamic and fault tolerant cluster management method that can
      coordinate concurrent access to resources in a peer-to-peer system and can be
      used to establish fault-tolerant configurable cluster consistency with
      predictable reliability, running on top of decentralised probabilistic
      protocols supporting scalable group communication. This is achieved by a
      general two-layered architecture that can be applied on top of the standard
      Internet communication layers and offers a modular, layered set of services to
      the applications that need them. Further, we present a fault-tolerant method
      implementing causal cluster consistency with predictable reliability, running
      on top of decentralised probabilistic protocols supporting group communication.
      This paper provides analytical and experimental evaluation of the properties
      regarding the fault tolerance of the approach. Furthermore, our experimental
      study, conducted by implementing and evaluating the twolayered architecture on
      top of standard Internet transport services, shows that the approach scales
      well, imposes an even load on the system, and provides high-probability
      reliability guarantees.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=ART-2011-10&amp;engl=0}
}

@article {ART-2011-01,
   author = {Muhammad Adnan Tariq and Boris Koldehofe and Gerald G. Koch and Imran Khan and Kurt Rothermel},
   title = {{Meeting subscriber-defined QoS constraints in publish/subscribe systems}},
   journal = {Concurrency and Computation: Practice and Experience},
   publisher = {John Wiley \& Sons, Inc.},
   pages = {1--15},
   type = {Artikel in Zeitschrift},
   month = {Februar},
   year = {2011},
   language = {Englisch},
   cr-category = {C.2.4 Distributed Systems},
   department = {Universit{\"a}t Stuttgart, Institut f{\"u}r Parallele und Verteilte Systeme, Verteilte Systeme},
   abstract = {Current distributed publish/subscribe systems assume that all participants have
      similar QoS requirements and equally contribute to the system's resources.
      However, in many real-world applications, the message delay tolerance of
      individual peers may differ widely. Disseminating messages according to
      individual delay requirements not only allows for the satisfaction of
      user-specific needs but also significantly improves the utilization of the
      resources in a publish/subscribe system. In this paper, we propose a
      peer-to-peer-based approach to satisfy the individual delay requirements of
      subscribers in the presence of bandwidth constraints. Our approach allows
      subscribers to dynamically adjust the granularity of their subscriptions
      according to their bandwidth constraints and delay requirements. Subscribers
      maintain the publish/subscribe overlay in a decentralized manner by
      establishing connections to peers that provide messages meeting exactly their
      subscription granularity and complying to their delay requirements. Evaluations
      show that for practical workloads, the proposed system scales up to a large
      number of subscribers and performs robustly in a very dynamic setting.},
   url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=ART-2011-01&amp;engl=0}
}

