
@TechReport{	  Arunagiri:07:Impact,
    author	= {Sarala Arunagiri and Seetharami Seelam and Ron A. Oldfield
		  and Maria Ruiz Varela and Patricia J. Teller and Rolf
		  Riesen},
    title	= {Impact of Checkpoint Latency on the Optimal Checkpoint
		  Interval and Execution Time},
    year	= {2007},
    number	= {07-55},
    institution	= {University of Texas at El Paso},
    type	= {Technical report},
    pdf		= {http://www.cs.utep.edu/vladik/2007/tr07-55.pdf}
}

@InProceedings{	  Bolen:95:Massively,
    author	= {Jerry Bolen and Arlin Davis and Bill Dazey and Satya Gupta
		  and Greg Henry and David Robboy and Guy Schiffler and Mack
		  Stallcup and Amir Taraghi and Stephen Wheat and Lee Ann
		  Fisk and Gabi Istrail and Chu Jong and Rolf Riesen and
		  Lance Shuler},
    title	= {Massively Parallel Distributed Computing},
    booktitle	= {Proceedings of the {Intel} Supercomputer Users' Group.
		  1995 Annual North America Users' Conference},
    month	= jun,
    year	= {1995},
    annote	= {{\sc Note:} David Robboy's paper about the world record
		  breaking MPLINPACK run},
    abstract	= {This paper describes the joint Intel and Sandia effort to
		  combine two Intel MP Paragon$^{\mbox{\tiny TM}}$
		  supercomputers via multiple HIPPI channels. This effort
		  broke the world speed record on the massively parallel
		  LINPACK benchmark by approximately fifty percent, attaining
		  281.1 Gigaflops. The two MP Paragon supercomputers operated
		  under the SUNMOS operating system.},
    pdf		= {Papers/Misc/bolen_95_massively.pdf}
}

@TechReport{	  Brightwell:99:Portals,
    author	= {Ron Brightwell and Tramm Hudson and Rolf Riesen and Arthur
		  B. Maccabe},
    title	= {The {Portals} 3.0 Message Passing Interface},
    month	= dec,
    year	= {1999},
    number	= {SAND99-2959},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {{\sc Note:} There are newer version of this document
		  available; e.g., Portals 3.3 and 4.0},
    abstract	= {This report presents a specification for the Portals 3.0
		  message passing interface. Portals 3.0 is intended to allow
		  scalable, high-performance network communication between
		  nodes of a parallel computing system. Specifically, it is
		  designed to support a parallel computing platform composed
		  of clusters of commodity workstations connected by a
		  commodity system area network fabric. In addition, Portals
		  3.0 is well suited to massively parallel processing and
		  embedded systems. Portals 3.0 represents an adoption of the
		  data movement layer developed for massively parallel
		  processing platforms, such as the 4500-node Intel TeraFLOPS
		  machine.},
    doi		= {http://dx.doi.org/10.2172/15154},
    pdf		= {Papers/Misc/brightwell_99_portals.pdf}
}

@Article{	  Brightwell:00:Massively,
    author	= {Ron Brightwell and Lee Ann Fisk and David S. Greenberg and
		  Tramm Hudson and Mike Levenhagen and Arthur B. Maccabe and
		  Rolf Riesen},
    title	= {Massively Parallel Computing Using Commodity Components},
    journal	= {Parallel Computing},
    month	= feb,
    year	= {2000},
    volume	= {26},
    number	= {2-3},
    pages	= {243--266},
    annote	= {{\sc Note:} This paper describes \cplant\/, and the ideas
		  and history behind it. It descibes the first prototype and
		  the first production system at Sandia. It lists the
		  experiences gained from buidling these two systems. The
		  second part of the paper describes the \cplant\/ system
		  software and a few initial performance results.},
    abstract	= {The Computational Plant (Cplant) project at Sandia
		  National Laboratories is developing a large-scale,
		  massively parallel computing resource from a cluster of
		  commodity computing and networking components. We are
		  combining the benefits of commodity cluster computing with
		  our expertise in designing, developing, using, and
		  maintaining large-scale, massively parallel processing
		  (MPP) machines. In this paper, we present the design goals
		  of the cluster and an approach to developing a
		  commodity-based computational resource capable of
		  delivering performance comparable to production-level MPP
		  machines. We provide a description of the hardware
		  components of a 96-node Phase I prototype machine and
		  discuss the experiences with the prototype that led to the
		  hardware choices for a 400-node Phase II production
		  machine. We give a detailed description of the management
		  and runtime software components of the cluster and offer
		  computational performance data as well as performance
		  measurements of functions that are critical to the
		  management of large systems.},
    doi		= {http://dx.doi.org/10.1016/S0167-8191(99)00104-0},
    indexmark	= {\bibindex{Cplant}},
    pdf		= {http://www.sandia.gov/~rbbrigh/papers/cplant-journal.pdf}
}

@InProceedings{	  Brightwell:02:Performance,
    author	= {Ron Brightwell and William Lawry and Mike Levenhagen and
		  Arthur B. Maccabe and Rolf Riesen},
    title	= {A Performance Comparison of {Myrinet} Protocol Stacks},
    booktitle	= {Proceedings of Third Linux Clusters Institute Conference
		  on Linux Clusters},
    month	= oct,
    year	= {2002},
    abstract	= {This paper describes a portable benchmark suite that
		  assesses the ability of cluster networking hardware and
		  software to overlap MPI communication and computation. The
		  Communication Offload MPI-based Benchmark, or COMB, uses
		  two different methods to haracterize the ability of
		  messages to make progress concurrently with computational
		  processing on the host processor(s). COMB measures the
		  relationship between overall MPI communication bandwidth
		  and host CPU availability. In this paper, we describe the
		  two different approaches used by the benchmark suite, and
		  we present results three different Myrinet protocol stacks
		  that are used to support the Portals 3.0 message passing
		  interface. We demonstrate the utility of the suite by
		  examining the results and comparing and contrasting the
		  different protocol stacks.},
    pdf		= {Papers/Workshop/brightwell_02_performance.pdf}
}

@InProceedings{	  Brightwell:02:Design,
    author	= {Ron Brightwell and Arthur B. Maccabe and Rolf Riesen},
    editor	= {Dieter Kranzlm{\"u}ller and Peter Kacsuk and Jack Dongarra
		  and Jens Volkert},
    title	= {Design and Implementation of {MPI} on {Portals} 3.0},
    booktitle	= {Recent Advances in Parallel Virtual Machine and Message
		  Passing Interface: 9th European {PVM/MPI} Users' Group
		  Meeting, {Linz}, {Austria}, September 29 - October 2, 2002.
		  Proceedings},
    year	= {2002},
    volume	= {2474},
    pages	= {331--340},
    publisher	= {Springer Verlag},
    series	= {Lecture Notes in Computer Science},
    abstract	= {This paper describes an implementation of the Message
		  Passing Interface (MPI) on the Portals 3.0 data movement
		  layer. Portals 3.0 provides low-level building blocks that
		  are flexible enough to support higher-level message passing
		  layers such as MPI very effiently. Portals 3.0 is also
		  designed to allow for programmable network interface cards
		  to offload message pcessing from the host processor. We
		  will describe the basic building blocks in Portals 3.0,
		  show how they can be put together to implement MPI, and
		  describe the protcols of an MPI implementation. We will
		  look at several key operations within an MPI implementation
		  and describe the effects that a Portals 3.0 implementation
		  has on scalability and performance.},
    issn_isbn	= {3-540-44296-0},
    doi		= {http://dx.doi.org/10.1007/3-540-45825-5_50},
    pdf		= {http://www.sandia.gov/~rbbrigh/papers/p3-mpi.pdf}
}

@InProceedings{	  Brightwell:02:Portals,
    author	= {Ron Brightwell and Rolf Riesen and Bill Lawry and Arthur
		  B. Maccabe},
    title	= {Portals 3.0: Protocol Building Blocks for Low Overhead
		  Communication},
    booktitle	= {Workshop on Communication Architecture for Clusters
		  {CAC'02}},
    month	= apr,
    year	= {2002},
    address	= {Fort Lauderdale, Florida},
    pages	= {164-173},
    publisher	= {IEEE},
    annote	= {{\sc Note:} CAC'02 was a workshop at IPDPS. The paper
		  gives a short history of Portals and describes how the
		  failure of the Portals 2.0 port to Linux led to version
		  3.0. The description is largely based on the
		  techreport~\shortcite{Sand99-2959}. The paper has a nice
		  section on why application bypass is important, how it
		  helps with the MPI progress rule, and some measurements to
		  show the effect.},
    abstract	= {This paper describes the evolution of the Portals message
		  passing architecture and programming interface from its
		  initial development on tightly-coupled massively parallel
		  platforms to the current implementation running on a
		  1792-node commodity PC Linux cluster. Portals provides the
		  basic building blocks needed for higher-level protocols to
		  implement scalable, low-overhead communication. Portals has
		  several unique characteristics that differentiate it from
		  other high-performance system-area data movement layers.
		  This paper discusses several of these features and
		  illustrates how they can impact the scalability and
		  performance of higher-level message passing protocols.},
    doi		= {http://dx.doi.org/10.1109/IPDPS.2002.1015467},
    isbn	= {0-7695-1573-8},
    pdf		= {Papers/Workshop/brightwell_02_portals.pdf}
}

@InProceedings{	  Brightwell:03:Performance,
    author	= {Ron Brightwell and Rolf Riesen and Keith Underwood and
		  Patrick G. Bridges and Arthur B. Maccabe and Trammel
		  Hudson},
    title	= {A Performance Comparison of {L}inux and a Lightweight
		  Kernel},
    booktitle	= {IEEE International Conference on Cluster Computing},
    month	= {December},
    year	= {2003},
    pages	= {251-258},
    abstract	= {In this paper, we compare running the Linux operating
		  system on the compute nodes of ASCI Red hardware to running
		  a specialized, highly-optimized lightweight kernel (LWK)
		  operating system. We have ported Linux to the compute and
		  service nodes of the ASCI Red supercomputer, and have run
		  several benchmarks. We present performance and scalability
		  results for Linux compared with the LWK environment. To our
		  knowledge, this is the first direct comparison on identical
		  hardware of Linux and an operating system designed
		  specifically for large-scale supercomputers. In addition to
		  presenting these results, we discuss the limitations of
		  both operating systems, in terms of the empirical evidence
		  as well as other important factors.},
    doi		= {http://dx.doi.org/10.1109/CLUSTR.2003.1253322},
    accepted	= {29.3%},
    pdf		= {https://cfwebprod.sandia.gov/cfdocs/CCIM/docs/lwk-vs-linux.pdf}
		  
}

@Article{	  Brightwell:03:Design,
    author	= {Ron Brightwell and Arthur B. Maccabe and Rolf Riesen},
    title	= {Design, Implementation, and Performance of {MPI} on
		  {Portals} 3.0},
    journal	= {The International Journal of High Performance Computing
		  Applications},
    year	= {2003},
    volume	= {17},
    number	= {1},
    pages	= {7--20},
    publisher	= {Sage Publications},
    annote	= {{\sc Note:} This is an expanded version of the paper at
		  EuroPVM/MPI\shortciteN{Brightwell02b}.},
    abstract	= {This paper describes an implementation of the Message
		  Passing Interface (MPI) on the Portals 3.0 data movement
		  layer. Portals 3.0 provides low-level building blocks that
		  are flexible enough to support higher-level message passing
		  layers such as MPI very efficiently. Portals 3.0 is also
		  designed to allow for programmable network interface cards
		  to offload message processing from the host processor,
		  allowing for the ability to overlap computation and MPI
		  communication. We describe the basic building blocks in
		  Portals 3.0, show how they can be put together to implement
		  MPI, and describe the protcols of our MPI implementation.
		  We look at several key operations within the implementation
		  and describe the effects that a Portals 3.0 implementation
		  has on scalability and performance. We also present
		  preliminary performance results from our implementation for
		  Myrinet.},
    issn_isbn	= {1094-3420},
    doi		= {http://dx.doi.org/10.1177/1094342003017001002},
    anote	= {This is the journal version
		  of~\cite{Brightwell:02:Design}.},
    pdf		= {http://www.sandia.gov/~rbbrigh/copyrighted-papers/sage-7.pdf}
		  
}

@InProceedings{	  Brightwell:03:Appropriateness,
    author	= {Ron Brightwell and Arthur B. Maccabe and Rolf Riesen},
    title	= {On the Appropriateness of Commodity Operating Systems for
		  Large-Scale, Balanced Computing Systems},
    booktitle	= {International Parallel and Distributed Processing
		  Symposium (IPDPS '03)},
    month	= apr,
    year	= {2003},
    address	= {Washington, DC, USA},
    publisher	= {IEEE Computer Society},
    abstract	= {In the past five years, we have been involved in the
		  design and development of Cplant. An important goal was to
		  take advantages of commodity approaches wherever possible.
		  In particular, we selected Linux, a commonly available
		  operating system, for the compute nodes of Cplant. While
		  the use of commodity solutions, including Linux, was
		  critical to the success of Cplant, we believe that such an
		  approach will not be viable in the development of the next
		  generation of very large-scale systems. We present our
		  definition of a balanced system and discuss several
		  limitations of commodity operating systems in the context
		  of balanced systems.},
    doi		= {http://dx.doi.org/10.1109/IPDPS.2003.1213164},
    isbn	= {0-7695-1926-1},
    accepted	= {29.2%},
    pdf		= {http://www.sandia.gov/~rbbrigh/copyrighted-papers/01213164.pdf}
		  
}

@InProceedings{	  Brightwell:04:Initial,
    author	= {Ron Brightwell and Rolf Riesen and Keith D. Underwood},
    editor	= {Dieter Kranzlm{\"u}ller and Peter Kacsuk and Jack
		  Dongarra},
    title	= {An initial analysis of the impact of overlap and
		  independent progress for {MPI}},
    booktitle	= {Recent Advances in Parallel Virtual Machine and Message
		  Passing Interface: 11th European {PVM/MPI} Users' Group
		  Meeting, {Budapest}, {Hungary}, September 19 - 22, 2004.
		  Proceedings},
    year	= {2004},
    volume	= {3241},
    pages	= {370--377},
    publisher	= {Springer Verlag},
    series	= {Lecture Notes in Computer Science},
    abstract	= {The ability to offload functionality to a programmable
		  network interface is appealing, both for increasing message
		  passing performance and for reducing the overhead on the
		  host processor(s). Two important features of an MPI
		  implementation are independent progress and the ability to
		  overlap computation with communication. In this paper, we
		  compare the performance of several application benchmarks
		  using an MPI implementation that takes advantage of a
		  programmable NIC to implement MPI semantics with an
		  implementation that does not. Unlike previous such
		  comparisons, we compare identical network hardware using
		  virtually the same software stack. This comparison isolates
		  these two important features of an MPI implementation.},
    issn_isbn	= {3-540-23163-3},
    doi		= {http://dx.doi.org/10.1007/b100820},
    pdf		= {https://cfwebprod.sandia.gov/cfdocs/CCIM/docs/tports-vs-shmem.pdf}
		  
}

@Article{	  Brightwell:05:Analyzing,
    author	= {Ron Brightwell and Rolf Riesen and Keith D. Underwood},
    title	= {Analyzing the Impact of Overlap, Offload, and Independent
		  Progress for Message Passing Interface Applications},
    journal	= {International Journal of High Performance Computing
		  Applications},
    month	= may,
    year	= {2005},
    address	= {Thousand Oaks, CA, USA},
    volume	= {19},
    number	= {2},
    pages	= {103--117},
    publisher	= {Sage Publications, Inc.},
    annote	= {{\sc Note:} Extended journal version of
		  \cite{Brightwell:04:Initial}.},
    abstract	= {The overlap of computation and communication has long been
		  considered to be a significant performance benefit for
		  applications. Similarly, the ability of the Message Passing
		  Interface (MPI) to make independent progress (that is, to
		  make progress on outstanding communication operations while
		  not in the MPI library) is also believed to yield
		  performance benefits. Using an intelligent network
		  interface to offload the work required to support overlap
		  and independent progress is thought to be an ideal
		  solution, but the benefits of this approach have not been
		  studied in depth at the application level. This lack of
		  analysis is complicated by the fact that most MPI
		  implementations do not sufficiently support overlap or
		  independent progress. Recent work has demonstrated a
		  quantifiable advantage for an MPI implementation that uses
		  offload to provide overlap and independent progress. The
		  study is conducted on two different platforms with each
		  having two MPI implementations (one with and one without
		  independent progress). Thus, identical network hardware and
		  virtually identical software stacks are used. Furthermore,
		  one platform, ASCI Red, allows further separation of
		  features such as overlap and offload. Thus, this paper
		  extends previous work by further qualifying the source of
		  the performance advantage: offload, overlap, or independent
		  progress.},
    doi		= {http://dx.doi.org/10.1177/1094342005054257},
    pdf		= {http://www.sandia.gov/~rbbrigh/copyrighted-papers/sage-103-1.pdf}
		  
}

@InProceedings{	  Brightwell:05:Implementation,
    author	= {Ron Brightwell and Trammell Hudson and Rolf Riesen and
		  Keith Underwood},
    title	= {Implementation and Performance of Portals 3.3 on the
		  {Cray} {XT3}},
    booktitle	= {IEEE International Conference on Cluster Computing
		  (CLUSTER'05)},
    month	= sep,
    year	= {2005},
    abstract	= {The Portals data movement interface was developed at
		  Sandia National Laboratories in collaboration with the
		  University of New Mexico over the last ten years. Portals
		  is intended to provide the functionality necessary to scale
		  a distributed memory parallel computing system to thousands
		  of nodes. Previous versions of Portals ran on several
		  large-scale machines, including a 1024-node nCUBE-2, a
		  1800-node Intel Paragon, and the 4500-node Intel ASCI Red
		  machine. The latest version of Portals was initially
		  developed for an 1800-node Linux/Myrinet cluster and has
		  since been adopted by Cray as the lowest-level network
		  programming interface for their XT3 platform. In this
		  paper, we describe the implementation of Portals 3.3 on the
		  Cray XT3 and present some initial performance results from
		  several micro-benchmark tests. Despite some limitations,
		  the implementation of Portals is able to achieve a
		  zero-length one-way latency of under six microseconds and a
		  uni-directional bandwidth of more than 1.1 GB/s.},
    doi		= {http://dx.doi.org/10.1109/CLUSTR.2005.347061},
    accepted	= {32.6%},
    pdf		= {http://www.sandia.gov/~rbbrigh/papers/p3-cluster05.pdf}
}

@InProceedings{	  Brightwell:05:Portals,
    author	= {Ron Brightwell and Trammell Hudson and Kevin Pedretti and
		  Rolf Riesen and Keith D. Underwood},
    title	= {Portals 3.3 on the {Sandia}/{Cray} {Red} {Storm} System},
    booktitle	= {Cray User Group (CUG)},
    year	= {2005},
    abstract	= {The Portals 3.3 data movement interface was developed at
		  Sandia National Laboratories in collaboration with the
		  University of New Mexico over the last ten years. Portals
		  is intended to provide the functionality necessary to scale
		  a distributed memory parallel computing system to thousands
		  of nodes. Previous versions of Portals ran on several
		  large-scale machines, including a 1024-node nCUBE-2, a
		  1800-node Intel Paragon, and the 4500-node Intel ASCI Red
		  machine. The latest version of Portals is the lowestlevel
		  network transport layer on the Sandia/Cray Red Storm
		  platform. In this paper, we describe how Portals are
		  implemented for Red Storm and discuss many of the important
		  features and benefits that Portals provide for supporting
		  various serices in the Red Storm environment.},
    pdf		= {http://www.sandia.gov/~rbbrigh/papers/p3-cug05.pdf}
}

@Misc{		  Brightwell:06:Portals,
    author	= {Ron Brightwell and Rolf Riesen and Arthur B. Maccabe},
    title	= {Tutorial Slides: {Portals} Programming on the {XT3}},
    booktitle	= {Cray User Group (CUG)},
    year	= {2006},
    pdf		= {http://www.sandia.gov/~rbbrigh/slides/other/portals-tutorial-cu06-slides.pdf}
		  
}

@InProceedings{	  Brightwell:10:Transparent,
    author	= {Ron Brightwell and Kurt Ferreira and Rolf Riesen},
    editor	= {Rainer Keller and Edgar Gabriel and Michel Resch and Jack
		  Dongarra},
    title	= {Transparent Redundant Computing with {MPI}},
    booktitle	= {Recent Advances in the Message Passing Interface: 17th
		  European {MPI} Users' Group Meeting, EuroMPI 2010,
		  {Stuttgart}, {Germany}, September 2010. Proceedings},
    year	= {2010},
    volume	= {6305},
    pages	= {208--218},
    publisher	= {Springer Verlag},
    series	= {Lecture Notes in Computer Science},
    abstract	= {Extreme-scale parallel systems will require alternative
		  methods for applications to maintain current levels of
		  uninterrupted execution. Redundant computation is one
		  approach to consider, if the benefits of increased
		  resiliency outweigh the cost of consuming additional
		  resources. We describe a transparent redundancy approach
		  for MPI applications and detail two different
		  implementations that provide the ability to tolerate a
		  range of failure scenarios, including loss of application
		  processes and connectivity. We compare these two approaches
		  and show performance results from micro-benchmarks that
		  bound worst-case message passing performance degradation.
		  We propose several enhancements that could lower the
		  overhead of providing resiliency through redundancy.},
    issn_isbn	= {978-3-642-15645-8},
    pdf		= {Papers/Conf/brightwell_10_transparent.pdf}
}

@TechReport{	  Ferreira:09:Increasing,
    author	= {Kurt Ferreira and Rolf Riesen and Ron Oldfield and Jon
		  Stearley and James Laros and Kevin Pedretti and Ron
		  Brightwell and Todd Kordenbrock},
    title	= {Increasing Fault Resiliency in a Message-Passing
		  Environment},
    month	= oct,
    year	= {2009},
    number	= {SAND2009-6753},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    abstract	= {Petaflops systems will have tens to hundreds of thousands
		  of compute nodes which increases the likelihood of faults.
		  Applications use checkpoint/restart to recover from these
		  faults, but even under ideal conditions, applications
		  running on more than 30,000 nodes will likely spend more
		  than half of their total run time saving checkpoints,
		  restarting, and redoing work that was lost.
		  
		  We created a library that performs redundant computations
		  on additional nodes allocated to the application. An active
		  node and its redundant partner form a node bundle which
		  will only fail, and cause an application restart, when both
		  nodes in the bundle fail. The goal of this library is to
		  learn whether this can be done entirely at the user level,
		  what requirements this library places on a Reliability,
		  Availability, and Serviceability (RAS) system, and what its
		  impact on performance and run time is.
		  
		  We find that our redundant MPI layer library imposes a
		  relatively modest performance penalty for applications, but
		  that it greatly reduces the number of applications
		  interrupts. This reduction in interrupts leads to huge
		  savings in restart and rework time. For large-scale
		  applications the savings compensate for the performance
		  loss and the additional nodes required for redundant
		  computations.},
    pdf		= {Papers/Misc/ferreira_09_increasing.pdf}
}

@InProceedings{	  Greenberg:93:Achieving,
    author	= {David S. Greenberg and Barney Maccabe and Rolf Riesen and
		  Stephen Wheat and David Womble},
    title	= {Achieving High Performance on the {Intel} {Paragon}},
    booktitle	= {Proceedings of the {Intel} Supercomputer Users' Group.
		  1993 Annual North America Users' Conference},
    month	= oct,
    year	= {1993},
    pages	= {203--208},
    abstract	= {When presented with a new supercomputer most users will
		  first ask ``How much faster will my applications run?'' and
		  then add a fearful ``How much effort will it take me to
		  convert to the new machine?'' This paper describes some
		  lessons learned at Sandia while asking these questions
		  about the new 1800+ node Intel Paragon. The authors
		  conclude that the operating system is crucial to both
		  achieving high performance and allowing easy conversion
		  from previous parallel implementations to a new machine.
		  Using the Sandia/UNM Operating System (SUNMOS) they were
		  able to port a LU factorization of dense matrices from the
		  nCUBE2 to the Paragon and achieve 92\% scaled speed-up on
		  1024 nodes. Thus on a 44,000 by 44,000 matrix which had
		  required over 10 hours on the previous machine, they
		  completed in less than 1/2 hour at a rate of over 40
		  GFLOPS. Two keys to achieving such high performance were
		  the small size of SUNMOS (less than 256 kbytes) and the
		  ability to send large messages with very low overhead.},
    doi		= {http://www.osti.gov/bridge/product.biblio.jsp?query_id=1&amp;page=0&amp;osti_id=10105178}
		  ,
    pdf		= {Papers/Misc/greenberg_93_achieving.pdf}
}

@InProceedings{	  Greenberg:93:Communication,
    author	= {David Greenberg and Barney Maccabe and Kevin S. McCurley
		  and Rolf Riesen and Stephen Wheat},
    title	= {Communication on the {Paragon}},
    booktitle	= {Proceedings of the {Intel} Supercomputer Users' Group.
		  1993 Annual North America Users' Conference},
    month	= oct,
    year	= {1993},
    pages	= {117--124},
    abstract	= {In this note the authors describe the results of some
		  tests of the message-passing performance of the Intel
		  Paragon. These tests have been carried out under both the
		  Intel-supplied OSF/1 operating system with an NX library,
		  and also under an operating system called SUNMOS (Sandia
		  UNM Operating System). For comparison with the previous
		  generation of Intel machines, they have also included the
		  results on the Intel Touchstone Delta. The source code used
		  for these tests is identical for all systems. As a result
		  of these tests, the authors can conclude that SUNMOS
		  demonstrates that the Intel Paragon hardware is capable of
		  very high bandwidth communication, and that the message
		  coprocessor on Paragon nodes can be used to give quite
		  respectable latencies. Further tuning can be expected to
		  yield even better performance.},
    doi		= {http://dx.doi.org/10.2172/10107556},
    location	= {\url{http://www.osti.gov/bridge/product.biblio.jsp?query_id=1&amp;page=0&amp;osti_id=10107556}}
		  ,
    pdf		= {Papers/Misc/greenberg_93_communication.pdf}
}

@InProceedings{	  Greenberg:97:System,
    author	= {David S. Greenberg and Ron Brightwell and Lee Ann Fisk and
		  Arthur B. Maccabe and Rolf Riesen},
    title	= {A System Software Architecture for High-End Computing},
    booktitle	= {{SC}'97: High Performance Networking and Computing:
		  Proceedings of the 1997 {ACM}\slash {IEEE} {SC97}
		  Conference: November 15--21, 1997, San Jose, California,
		  {USA}.},
    month	= nov,
    year	= {1997},
    publisher	= {ACM Press and IEEE Computer Society Press},
    annote	= {{\sc Note:} A paper about the design ideas and goals
		  behind Cplant and Portals. Cplant was in the early design
		  stages, but many of the concept outlined in this paper are
		  present in one form or another in the current Cplant
		  system.},
    abstract	= {Large MPP systems can neither solve grand-challenge
		  scientific problems nor enable large scale industrial and
		  governmental simulations if they rely on extensions to
		  workstation system software. At Sandia National
		  Laboratories we have developed, with our vendors, a new
		  system architecture for high-end computing. Highest
		  performance is achieved by providing applications with a
		  light-weight interface to a collection of processing nodes.
		  Usability is provided by creating node partitions
		  specialized for user access, networking, and I/O. The
		  entire system is glued together by a data movement
		  interface which we call portals. Portals allow data to flow
		  between processing nodes with minimal system overhead while
		  maintaining a suitable degree of protection and
		  reconfigurability.},
    doi		= {http://doi.acm.org/10.1145/509593.509646},
    indexmark	= {\bibindex{Cplant} \bibindex{Portals}},
    accepted	= {19.2%},
    pdf		= {http://www.sandia.gov/~rbbrigh/copyrighted-papers/01592634.pdf}
		  
}

@InProceedings{	  Hsieh:10:Framework,
    author	= {M. Hsieh and K. Thompson and W. Song and A.F. Rodrigues
		  and R. Riesen},
    title	= {A Framework for Architecture-Level Power, Area and Thermal
		  Simulation and its Application to Network-on-chip Design
		  Exploration},
    booktitle	= {1st International Workshop on Performance Modeling,
		  Benchmarking and Simulation of High Performance Computing
		  Systems (PMBS 10) held as part of SC10},
    month	= nov,
    year	= {2010},
    abstract	= {In this paper, we describe the integrated power, area and
		  thermal modeling framework in the Structural Simulation
		  Toolkit (SST) for large scale high performance computer
		  simulation. It integrates various power and thermal
		  modeling tools and computes run-time energy dissipation for
		  core, network on chip, memory controller and shared cache.
		  It also provides functionality to update the leakage power
		  as temperature change.
		  
		  We Illustrate the utilization of the framework by applying
		  it to explore interconnect options in manycore systems with
		  consideration of temperature variation and leakage
		  feedback. We compare power, energy-delay-area product
		  (EDAP), and energy-delay product (EDP) of four manycore
		  configurations-1 core, 2 core, 4 core and 8 core per
		  cluster. Results from simulation with or without
		  consideration of temperature variation both show that the
		  4- core per cluster configuration has the best EDAP and
		  EDP. Even so, considering temperature variation increases
		  total power dissipation. We demonstrate the importance of
		  considering temperature variation in the design flow. With
		  this power, area and thermal modeling capability, SST can
		  be used for Hardware/Software co-design of future Exascale
		  systems.}
}

@InProceedings{	  Leon:09:Instruction-Level,
    author	= {Edgar A. Le\'{o}n and Rolf Riesen and Arthur B. Maccabe
		  and Patrick G. Bridges},
    title	= {Instruction-Level Simulation of a Cluster at Scale},
    booktitle	= {{SC}'09: High Performance Networking and Computing:
		  Proceedings of the 2009 {ACM}\slash {IEEE} {SC09}
		  Conference: November 14--20, 2009, Portland, Oregon,
		  {USA}.},
    month	= nov,
    year	= {2009},
    publisher	= {ACM Press and IEEE Computer Society Press},
    abstract	= {Instruction-level simulation is necessary to evaluate new
		  architectures. However, single-node simulation cannot
		  predict the behavior of a parallel application on a
		  supercomputer. We present a scalable simulator that couples
		  a cycle-accurate node simulator with a supercomputer
		  network model. Our simulator executes individual instances
		  of IBM's Mambo PowerPC simulator on hundreds of cores. We
		  integrated a NIC emulator into Mambo and model the network
		  instead of fully simulating it. This decouples the
		  individual node simulators and makes our design scalable.
		  
		  Our simulator runs unmodified parallel message-passing
		  applications on hundreds of nodes. We can change network
		  and detailed node parameters, inject network traffic
		  directly into caches, and use different policies to decide
		  when that is an advantage.
		  
		  This paper describes our simulator in detail, evaluates it,
		  and demonstrates its scalability. We show its suitability
		  for architecture research by evaluating the impact of cache
		  injection on parallel application performance.},
    doi		= {http://doi.acm.org/10.1145/1654059.1654063},
    accepted	= {22.6%}
}

@InProceedings{	  Maccabe:94:SUNMOS,
    author	= {Arthur B. Maccabe and Kevin S. McCurley and Rolf Riesen
		  and Stephen R. Wheat},
    title	= {{SUNMOS} for the {Intel} {Paragon}: {A} Brief User's
		  Guide},
    booktitle	= {Proceedings of the {Intel} Supercomputer Users' Group.
		  1994 Annual North America Users' Conference},
    month	= jun,
    year	= {1994},
    pages	= {245--251},
    abstract	= {No abstract},
    doi		= {http://dx.doi.org/10.2172/10167604},
    location	= {\url{http://www.osti.gov/bridge/product.biblio.jsp?query_id=1&amp;page=0&amp;osti_id=10167604}}
		  ,
    indexmark	= {\bibindex{SUNMOS}},
    pdf		= {Papers/Misc/maccabe_94_sunmos.pdf}
}

@Article{	  Maccabe:96:Dynamic,
    author	= {Arthur B. Maccabe and Rolf Riesen and David W. van
		  Dresser},
    title	= {Dynamic Processor Modes in Puma},
    journal	= {Bulletin of the Technical Committee on Operating Systems
		  and Application Environments (TCOS)},
    year	= {1996},
    volume	= {8},
    number	= {2},
    pages	= {4--12},
    abstract	= {In this paper, we describe the dynamic processor modes
		  supported by Puma, an operating system for massively
		  parallel computing systems. Puma was designed for computing
		  systems that consist of thousands of nodes connected by a
		  communication network with high bandwidth and low
		  message-passing latency. The individual nodes of such a
		  system may have multiple processors with a shared memory.
		  This presents the potential to exploit parallelism at two
		  levels: parallelism between nodes and parallelism within a
		  node. In this paper, we describe the mechanisms we have
		  developed to support library and application writers in
		  their efforts to exploit the parallelism provided by a
		  single node. Following the Puma philosophy, the mechanisms
		  provided by the kernel represent a very thin (but safe)
		  level above the actual hardware. In addition to the kernel
		  mechanisms, we also describe higher-level library functions
		  that application writers or people writing runtime systems
		  can use.},
    indexmark	= {\bibindex{Puma}},
    pdf		= {Papers/Journal/maccabe_96_dynamic.pdf}
}

@TechReport{	  Maccabe:02:Distributing,
    author	= {Arthur B. Maccabe and William Lawry and Christopher Wilson
		  and Rolf Riesen},
    title	= {Distributing Application and {OS} Functionality to Improve
		  Application Performance},
    month	= apr,
    year	= {2002},
    number	= {TR-CS-2002-11},
    institution	= {Computer Science Department, The University of New
		  Mexico},
    abstract	= {In this paper we demonstrate that the placement of
		  functionality can have a significant impact on the
		  performance of applications. OS bypass distributes OS
		  policies to the network interface and protocol processing
		  to the application to enhance application performance. We
		  take this notion one step further and consider the
		  distribution of application functionality to the network
		  interface and-or the operating system. We illustrate the
		  advantages of this approach by considering double buffering
		  at the application level, a standard technique used to hide
		  communication latency.},
    pdf		= {Papers/Misc/maccabe_02_distributing.pdf}
}

@InProceedings{	  Maccabe:02:Experience,
    author	= {Arthur B. Maccabe and Wenbin Zhu and Jim Otto and Rolf
		  Riesen},
    title	= {Experience in Offloading Protocol Processing to a
		  Programmable {NIC}},
    booktitle	= {Proceedings of IEEE 2002 international conference on
		  cluster computing},
    month	= sep,
    year	= {2002},
    address	= {Washington, DC, USA},
    pages	= { 67-74},
    publisher	= {IEEE Computer Society},
    abstract	= {Offloading protocol processing will become an important
		  tool in supporting our efforts to deliver increasing
		  bandwidth to applications. In this paper we describe our
		  experience in offloading protocol processing to a
		  programmable gigabit Ethernet network interface card. For
		  our experiments, we selected a simple RTS/CTS (request to
		  send/clear to send) protocol called RMPP (Reliable Message
		  Passing Protocol). This protocol provides end-to-end flow
		  control and full message retransmit in the case of a lost
		  or corrupt packet. By carefully selecting parts of the
		  protocol for offloading, we were able to improve the
		  bandwidth delivered to MPI applications from approximately
		  280 Mb/s to approximately 700 Mb/s using standard, 1500
		  byte, Ethernet frames. Using "jumbo", 9000 byte, frames the
		  bandwidth improves from approximately 425 Mb/s to 840 Mb/s.
		  Moreover, we were able to show a significant increase in
		  the availability of the host processor.},
    doi		= {http://dx.doi.org/10.1109/CLUSTR.2002.1137730},
    isbn	= {0-7695-1745-5},
    accepted	= {38.8%}
}

@TechReport{	  Maccabe:02:ExperienceTR,
    author	= {Arthur B. Maccabe and Wenbin Zhu and Jim Otto and Rolf
		  Riesen},
    title	= {Experience in Offloading Protocol Processing to a
		  Programmable {NIC}},
    month	= apr,
    year	= {2002},
    number	= {TR-CS-2002-12},
    institution	= {Computer Science Department, The University of New
		  Mexico},
    abstract	= {Offloading protocol processing will become an important
		  tool in supporting our efforts to deliver increasing
		  bandwidth to applications. In this paper we describe our
		  experience in offloading protocol processing to a
		  programmable gigabit Ethernet network interface card. For
		  our experiments, we selected a simple RTS-CTS (request to
		  send, clear to send) protocol called RMPP (Reliable Message
		  Passing Protocol). This protocol provides end-to-end flow
		  control and full message retransmit in the case of a lost
		  or corrupt packet. By carefully selecting parts of the
		  protocol for offloading, we were able to improve the
		  bandwidth delivered to MPI applications from approximately
		  280 Mb per sec to approximately 700 Mb per sec using
		  standard, 1500 byte, Ethernet frames. Using ``jumbo'', 9000
		  byte, frames the bandwidth improves from approximately 425
		  Mbs to 840 Mb per sec. Moreover, we were able to show a
		  significant increase in the availability of the host
		  processor.},
    pdf		= {Papers/Misc/maccabe_02_experienceTR.pdf}
}

@InProceedings{	  Maccabe:04:Highly,
    author	= {Arthur B. Maccabe and Patrick G. Bridges and Ron
		  Brightwell and Rolf Riesen and Trammell Hudson},
    title	= {Highly Configurable Operating Systems for Ultrascale
		  Systems},
    booktitle	= {First International Workshop on Operating Systems,
		  Programming Environments and Management Tools for
		  High-Performance Computing on Clusters (COSET-1)},
    month	= jun,
    year	= {2004},
    address	= {Saint-Malo (France)},
    pages	= {33--40},
    url		= {http://coset.irisa.fr},
    abstract	= {Modern ultrascale machines have a diverse range of usage
		  models, programming models, architectures, and shared
		  services that place a wide range of demands on operating
		  and runtime systems. Full-featured operating systems can
		  support a broad range of these requirements, but sacrifice
		  optimal solutions for general ones. Lightweight operating
		  systems, in contrast, can provide optimal solutions at
		  specific design points, but only for a limited set of
		  requirements. In this paper, we present preliminar numbers
		  quantifying the penalty paid by general-purpose operating
		  systems and propose an approach to overcome the limitations
		  of previous designs. The proposed approach focuses on the
		  implementation and composition of fine-grained composable
		  micro-services, portions of operating and runtime system
		  functionality that can be combined based on the needs of
		  the hardware and software. We also motivate our approach by
		  presenting concrete examples of the changing demands placed
		  on operating systems and runtimes in ultrascale
		  environments.},
    pdf		= {Papers/Workshop/maccabe_04_highly.pdf}
}

@InProceedings{	  Maccabe:06:Recent,
    author	= {Arthur Maccabe and Patrick Bridges and Ron Brightwell and
		  Rolf Riesen},
    title	= {Recent Trends in Operating Systems and their Applicability
		  to {HPC}},
    booktitle	= {Cray User Group (CUG)},
    year	= {2006},
    abstract	= {In this paper we consider recent trends in operating
		  systems and discuss their applicability to high performance
		  computing systems. In particular, we will consider the
		  relationship between lightweight kernels, hypervisors,
		  microkernels, modular kernels, and approaches to building
		  systems with a single system image. We then describe how
		  the Catamount lightweight kernel can be extended to support
		  the Xen hypervisor API. This will, in turn, support use of
		  Linux on the compute nodes of a large scale parallel system
		  while minimizing the effort needed to support both, a
		  lightweight OS and a full-featured OS.},
    pdf		= {Papers/Misc/maccabe_06_recent.pdf}
}

@InProceedings{	  Oldfield:06:Lightweight-B,
    author	= {Ron A. Oldfield and Lee Ward and Rolf Riesen and Arthur B.
		  Maccabe and Patrick Widener and Todd Kordenbrock},
    title	= {Lightweight {I/O} for Scientific Applications},
    booktitle	= {IEEE International Conference on Cluster Computing
		  (CLUSTER'06)},
    month	= sep,
    year	= {2006},
    abstract	= {Today's high-end massively parallel processing (MPP)
		  machines have thousands to tens of thousands of processors,
		  with next-generation systems planned to have in excess of
		  one hundred thousand processors. For systems of such scale,
		  efficient I/O is a significant challenge that cannot be
		  solved using traditional approaches. In particular, general
		  purpose parallel file systems that limit applications to
		  standard interfaces and access policies do not scale and
		  will likely be a performance bottleneck for many scientific
		  applications. In this paper, we investigate the use of a
		  ``lightweight'' approach to I/O that requires the
		  application or I/O-library developer to extend a core set
		  of critical I/O functionality with the minimum set of
		  features and services required by its target applications.
		  We argue that this approach allows the development of I/O
		  libraries that are both scalable and secure. We support our
		  claims with preliminary results for a lightweight
		  checkpoint operation on a development cluster at Sandia},
    doi		= {http://dx.doi.org/10.1109/CLUSTR.2006.311853},
    accepted	= {33.1%}
}

@TechReport{	  Oldfield:06:Lightweight,
    author	= {Ron A. Oldfield and Arthur B. Maccabe and Sarala Arunagiri
		  and Todd Kordenbrock and Rolf Riesen and Lee Ward and
		  Patrick Widener},
    title	= {Lightweight {I/O} for Scientific Applications},
    month	= may,
    year	= {2006},
    number	= {SAND2006-3057},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {{\sc Note:} Extended version of paper submitted to the
		  2006 Cluster computing conference.},
    abstract	= {Today's high-end massively parallel processing (MPP)
		  machines have thousands to tens of thousands of processors,
		  with next-generation systems planned to have in excess of
		  one hundred thousand processors. For systems of such scale,
		  efficient I/O is a significant challenge that cannot be
		  solved using traditional approaches. In particular, general
		  purpose parallel file systems that limit applications to
		  standard interfaces and access policies do not scale and
		  will likely be a performance bottleneck for many scientific
		  applications.
		  
		  In this paper, we investigate the use of a ``lightweight''
		  approach to I/O that requires the application or
		  I/O-library developer to extend a core set of critical I/O
		  functionality with the minimum set of features and services
		  required by its target applications. We argue that this
		  approach allows the development of I/O libraries that are
		  both scalable and secure. We support our claims with
		  preliminary results for a lightweight checkpoint operation
		  on a development cluster at Sandia.},
    pdf		= {Papers/Misc/oldfield_06_Lightweight.pdf}
}

@InProceedings{	  Oldfield:07:Modeling,
    author	= {Ron A. Oldfield and Sarala Arunagiri and Patricia J.
		  Teller and Seetharami Seelam and Maria Ruiz Varela and Rolf
		  Riesen and Philip C. Roth},
    title	= {Modeling the Impact of Checkpoints on Next-Generation
		  Systems},
    booktitle	= {24th IEEE Conference on Mass Storage Systems and
		  Technologies},
    month	= sep,
    year	= {2007},
    pages	= {30--46},
    publisher	= {IEEE Computer Society},
    abstract	= {The next generation of capability-class, massively
		  parallel processing (MPP) systems is expected to have
		  hundreds of thousands of processors. For
		  application-driven, periodic checkpoint operations, the
		  state-of-the-art does not provide a solution that scales to
		  next-generation systems. We demonstrate this by using
		  mathematical modeling to compute a lower bound of the
		  impact of these approaches on the performance of
		  applications executed on three massive-scale,
		  in-production, DOE systems and a theoretical petaflop
		  system. We also adapt the model to investigate a proposed
		  optimization that makes use of ``lightweight'' storage
		  architectures and overlay networks to overcome the storage
		  system bottleneck. Our results indicate that (1) as we
		  approach the scale of next-generation systems, traditional
		  checkpoint/restart approaches will increasingly impact
		  application performance, accounting for over 50\% of total
		  application execution time; (2) although our alternative
		  approach improves performance, it has limitations of its
		  own; and (3) there is a critical need for new approaches to
		  checkpoint/restart that allow continuous computing with
		  minimal impact on the scalability of applications.},
    doi		= {http://dx.doi.org/10.1109/MSST.2007.24},
    isbn	= {0-7695-3025-7}
}

@Unpublished{	  Riesen:93:Experience,
    author	= {Rolf Riesen and Arthur B. Maccabe and Stephen R. Wheat},
    title	= {Experience in Implementing a Parallel File System},
    month	= mar,
    year	= {1993},
    abstract	= {With ever increasing processor and memory speeds, new
		  methods to overcome the ``I/O bottleneck'' need to be
		  found. This is especially true for massively parallel
		  computers that need to store and retrieve large amounts of
		  data fast and reliably, to fully utilize the available
		  processing power.
		  
		  We have designed and implemented a parallel file system,
		  that distributes the work of transferring data to and from
		  mass storage, across several I/O nodes and communication
		  channels.
		  
		  The prototype parallel file system makes use of the
		  existing single threaded file system of the
		  Sandia/University of New Mexico Operating System (SUNMOS).
		  SUNMOS is a joint project between Sandia National
		  Laboratory and the University of New Mexico to create a
		  small and efficient OS for Massively Parallel (MP) Multiple
		  Instruction, Multiple Data (MIMD) machines.
		  
		  We chose file striping to interleave files across sixteen
		  disks. By using source-routing of messages we were able to
		  increase throughput beyond the maximum single channel
		  bandwidth the default routing algorithm of the nCUBE 2
		  hypercube allows. We describe our implementation, the
		  results of our experiments, and the influence this work has
		  had on the design of the Performance-oriented,
		  User-managed, Messaging Architecture (PUMA) operating
		  system, the successor to SUNMOS.},
    keywords	= {parallel I/O, multiprocessor file system},
    comment	= {They describe their experience building a file system for
		  SUNMOS. Paper describes tuning the SCSI device, their
		  striping strategy, their message-passing tricks, and some
		  performance results.},
    pdf		= {Papers/Misc/riesen_93_experience.pdf}
}

@InProceedings{	  Riesen:94:Active,
    author	= {Rolf Riesen and Arthur B. Maccabe and Stephen R. Wheat},
    title	= {Active Messages Versus Explicit Message Passing Under
		  {SUNMOS}},
    booktitle	= {Proceedings of the {Intel} Supercomputer Users' Group.
		  1994 Annual North America Users' Conference},
    month	= jun,
    year	= {1994},
    pages	= {297--303},
    abstract	= {In the past few years much effort has been devoted to
		  finding faster and more convenient ways to exchange data
		  between nodes of massively parallel distributed memory
		  machines. One such approach, taken by Thorsten von Eicken
		  et al.\cite{ACTIVE-MSG} is called active messages. The idea
		  is to hide message passing latency and continue to compute
		  while data is being sent and delivered.
		  
		  We have implemented active messages under SUNMOS for the
		  Intel Paragon and performed various experiments to
		  determine their efficiency and utility. In this paper we
		  concentrate on the subset of the active message layer that
		  is used by our implementation of the Split-C library. We
		  compare performance to explicit message passing under
		  SUNMOS and explore new ways to support Split-C without
		  active messages. We also compare our implementation to the
		  original one on the Thinking Machines CM-5 and try to
		  determine what the effects of low latency and low bandwidth
		  versus high latency and high bandwidth are on user codes.},
    doi		= {http://www.osti.gov/bridge/product.biblio.jsp?query_id=1&amp;page=0&amp;osti_id=10167234}
		  ,
    indexmark	= {\bibindex{SUNMOS} \bibindex{Active Messages}
		  \bibindex{Split-C}},
    pdf		= {Papers/Misc/riesen_94_active.pdf}
}

@Misc{		  Riesen:96:SUNMOS,
    author	= {Rolf Riesen and Lee Ann Fisk and Chu Jong and Barney
		  Maccabe and Kevin McCurley and Lance Shuler and Mack
		  Stallcup and David van Dresser},
    title	= {{SUNMOS}},
    year	= {1996},
    abstract	= {No abstract},
    pdf		= {Papers/Misc/riesen_96_sunmos.pdf}
}

@TechReport{	  Riesen:96:Using,
    author	= {Rolf Riesen},
    title	= {Using Kernel Extensions to Decrease the Latency of
		  User-Level Communication Primitives},
    month	= dec,
    year	= {1996},
    number	= {CS96-4},
    institution	= {University of New Mexico},
    type	= {Technical Report},
    abstract	= {CPUs, network interfaces, and networks are improving,
		  providing higher bandwidths and lower latencies. System
		  software overhead makes it impossible for a user
		  application to achieve bandwidths and latencies near the
		  hardware limits. This is especially true for remote handler
		  invocation. Typically, the remote node has to trap into the
		  kernel and perform an expensive context switch to the
		  handler. This hampers global communication operations and
		  runtime systems, such as the one for Cilk and Split-C for
		  example. Executing the untrusted remote handler inside the
		  operating system kernel eliminates the overhead of context
		  switches and disrupted cashes. Several methods to execute
		  untrusted user code in a privileged environment exist. The
		  research proposed in this paper compares these methods and
		  attempts to prove that a kernel embedded interpreter has
		  the necessary performance and safety characteristics to be
		  the ideal method for remote handler invocation in massively
		  parallel systems.},
    indexmark	= {\bibindex{Split-C}},
    pdf		= {Papers/Misc/riesen_96_using.pdf}
}

@TechReport{	  Riesen:98:Differences,
    author	= {Rolf Riesen and Ron Brightwell and Arthur B. Maccabe},
    title	= {Differences Between Distributed and Parallel Systems},
    year	= {1998},
    number	= {SAND98-2221},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {{\sc Note:} Originally written in 1996 but not published
		  as a techreport until 1998. Therefore, some of the examples
		  refer to systems not in use anymore. The report also
		  pre-dates the popularity of COTS clusters. The report lists
		  a set of criteria that distinguish MP (parallalel) systems
		  from distributed systems. It notes that with the arrival of
		  networks of workstations, the distinction gets blurred as
		  the systems converge. The second part of the report gives
		  an overview of the Puma kernel and Puma Portals, the
		  predecessor to Portals 3~\shortcite{Sand99-2959}.},
    abstract	= {Distributed systems have been studied for about twenty
		  years and are now coming into wider use as fast networks
		  and powerful workstations become more readily available. In
		  many respects a massively parallel computer resembles a
		  network of workstations and it is tempting to port a
		  distributed operating system to such a machine. However,
		  there are significant differences between these two
		  environments and a parallel operating system is needed to
		  get the best performance out of a massively parallel system.
		  
		  This paper characterizes the differences between
		  distributed systems, networks of workstations, and
		  massively parallel systems and analyzes the impact of these
		  differences on operating system design.
		  
		  In the second part of the paper we introduce Puma, an
		  operating system specifically developed for massively
		  parallel systems. We describe Puma portals, the basic
		  building blocks for message passing paradigms implemented
		  on top of Puma, and show how the differences observed in
		  the first part of the paper have influenced the design and
		  implementation of Puma.},
    doi		= {http://dx.doi.org/10.2172/1518},
    indexmark	= {\bibindex{Puma} \bibindex{Portals}},
    pdf		= {Papers/Misc/riesen_98_differences.pdf}
}

@InProceedings{	  Riesen:99:Cplant,
    author	= {Rolf Riesen and Ron Brightwell and Lee Ann Fisk and Tramm
		  Hudson and Jim Otto and Arthur B. Maccabe},
    title	= {Cplant},
    booktitle	= {Proceedings of the Second Extreme Linux workshop at the
		  1999 USENIX Annual Technical Conference},
    month	= jun # {~8--11,},
    year	= {1999},
    address	= {Monterey, California},
    abstract	= {The Computational Plant project at Sandia National
		  Laboratories is developing a large-scale, massively
		  parallel computing resource from a cluster of commodity
		  computing and networking components. We are combining the
		  knowledge and research of previous and ongoing commodity
		  cluster projects with our expertise in designing,
		  developing, using, and maintaining large-scale MPP machines.
		  
		  This paper describes the main parts of the architecture and
		  discusses the most important design choices and decisions.
		  Scaling to hundreds and thousands of nodes requires more
		  than simply combining readily-available software and
		  hardware. We will highlight some of the more crucial pieces
		  that make Cplant scalable.},
    indexmark	= {\bibindex{Cplant}},
    pdf		= {Papers/Workshop/riesen_99_cplant.pdf}
}

@PhDThesis{	  Riesen:02:Message-Based,
    author	= {Rolf Riesen},
    title	= {Message-Based, Error-Correcting Protocols for Scalable
		  High-Performance Networks},
    month	= jul,
    year	= {2002},
    address	= {Computer Science Department, Albuquerque, NM 87131},
    school	= {The University of New Mexico},
    abstract	= {Long distance communications and the associated protocols
		  have existed for thousands of years. Fire beacons were used
		  in antiquity and Polybius describes various devices and
		  protocols used to transmit messages during the Roman
		  period. Since computers have started communicating with
		  each other over electronic networks, the number of
		  different protocols has exploded. Communication protocols
		  aim to move information across a network in an efficient
		  and reliable manner. This often requires congestion and
		  flow control, error detection and correction, and
		  handshaking to coordinate the information transfer.
		  
		  Massively parallel supercomputers, such as the nCUBE 2,
		  Intel Paragon, Cray T3E, and IBM SP-2, contain very fast,
		  highly reliably networks. Similar network technology is now
		  available in PC clusters. Examples include Myrinet and
		  Quadrics networks. These networks, which are descendants of
		  the supercomputer networks from the 1980's, are much more
		  reliable, exhibit different error and degradation behavior,
		  and are utilized in a different fashion than wide area
		  networks commonly used in intranets and internets.
		  
		  Most network communication protocols are implemented as
		  part of the operating system (OS) on the computers wishing
		  to communicate. With very high-speed networks, software
		  overhead can lower the achievable performance of a network.
		  OS bypass is a method to move data directly to and from
		  user space, avoiding activation of the OS for data
		  movement. This means the protocol has to be implemented on
		  the network interface cards (NIC) which move the data, or
		  in user space as part of the application. These NICs often
		  have limitations, such as slow processors and limited
		  memory, that make an efficient, scalable implementation of
		  a communication protocol difficult.
		  
		  This dissertation asserts that the differences in networks
		  and usage warrant network communication protocols designed
		  specifically for high-performance local area networks used
		  in scientific computing systems. We present one such
		  protocol, compare it against existing protocols, and show
		  that it is more efficient and scalable than the more
		  traditional protocols.},
    isbn	= {0-493-74499-1}
}

@InProceedings{	  Riesen:02:RMPP,
    author	= {Rolf Riesen and Arthur B. Maccabe},
    title	= {{RMPP}: The Reliable Message Passing Protocol},
    booktitle	= {Workshop on High-Speed Local Networks {HSLN'02}},
    month	= nov,
    year	= {2002},
    address	= {Tampa, Florida},
    pages	= {658--668},
    abstract	= {Large-scale clusters built out of commercial components
		  face similar scalability obstacles as the massively
		  parallel processors (MPP) of the 1980's. This is especially
		  true when they are used for scientific computing. Their
		  networks are the descendants of the MPP networks, but the
		  software in use has been designed for wide-area networks
		  with client/server applications in mind.
		  
		  We present a communication protocol which has been designed
		  specifically for large-scale clusters with a scientific
		  application workload. The protocol takes advantage of the
		  low error rate and high performance of these networks. It
		  is adapted to the peculiarities of these MPP-like networks
		  and the communication characteristics of scientific
		  applications.},
    isbn	= {0-7695-1591-6},
    day		= {6-8},
    pdf		= {Papers/Workshop/riesen_02_rmpp.pdf}
}

@InProceedings{	  Riesen:03:Measuring,
    author	= {Rolf Riesen and Ron Brightwell and Arthur B. Maccabe},
    editor	= {Jack Dongarra and Domenico Laforenza and Salvatore
		  Orlando},
    title	= {Measuring {MPI} Latency Variance},
    booktitle	= {Recent Advances in Parallel Virtual Machine and Message
		  Passing Interface: 10th European {PVM/MPI} Users' Group
		  Meeting, {Venice}, {Italy}, September/October 2003
		  Proceedings},
    year	= {2003},
    volume	= {2840},
    pages	= {112--116},
    publisher	= {Springer-Verlag},
    series	= {Lecture Notes in Computer Science},
    abstract	= {Point to point latency and bandwidth measurements are
		  often the first benchmarks run on a new machine or MPI
		  implementation. In this paper we show that the common way
		  of measuring latency hides valuable information about the
		  transmission time characteristics. We introduce a simple
		  benchmark to measure transmission time variance and report
		  on the results from a variety of systems.},
    doi		= {http://dx.doi.org/10.1007/b14070},
    issn_isbn	= {3-540-20149-1}
}

@InProceedings{	  Riesen:03:Simple,
    author	= {Rolf Riesen and Arthur B. Maccabe},
    title	= {Simple, Scalable protocols for High-Performance Local
		  Networks (poster)},
    booktitle	= {Workshop on High-Speed Local Networks {HSLN'03} as part of
		  the IEEE LCN conference},
    month	= oct,
    year	= {2003},
    address	= {Washington, DC, USA},
    publisher	= {IEEE Computer Society},
    day		= {21},
    isbn	= {0-7695-2037-5},
    pdf		= {Papers/Workshop/riesen_03_simple.pdf}
}

@InProceedings{	  Riesen:06:Hybrid,
    author	= {Rolf Riesen},
    title	= {A Hybrid {MPI} Simulator},
    booktitle	= {IEEE International Conference on Cluster Computing
		  (CLUSTER'06)},
    year	= {2006},
    abstract	= {Performance modeling of large scale applications and
		  predicting the impact of architectural changes on the
		  behavior of an application is difficult. Traditional
		  approaches to measuring applications often change their
		  behavior. Modeling an application on a new architecture
		  requires in-depth knowledge of the application. Both tasks
		  often involve studying an application to learn where probes
		  have to be inserted.
		  
		  In this paper we describe a novel approach that is a hybrid
		  between discrete event simulation and measuring the
		  behavior of a running application. We explain how our early
		  prototype works and how it can be used. We mention several
		  experiments that we have already performed with this
		  protype and show its potential for future research areas.},
    doi		= {http://dx.doi.org/10.1109/CLUSTR.2006.311852},
    accepted	= {33.1%}
}

@InProceedings{	  Riesen:06:Communication,
    author	= {Rolf Riesen},
    title	= {Communication Patterns},
    booktitle	= {Workshop on Communication Architecture for Clusters
		  {CAC'06}},
    month	= apr,
    year	= {2006},
    address	= {Rhodes Island, Greece},
    publisher	= {IEEE},
    abstract	= {Parallel applications have message-passing patterns that
		  are important to understand. Network topology, routing
		  decisions, and connection and buffer management need to
		  match the communication patterns of an application for it
		  to run efficiently and scale well. These patterns are not
		  easily discerned from the source code of an application,
		  and even when the data is available it is not easy to
		  categorize it appropriately such that meaningful knowledge
		  emerges.
		  
		  We describe a novel system to gather the information we
		  need to discover an application's communication pattern. We
		  create five categories that help us analyze that data and
		  explain how information from each category can be useful in
		  the design of networking hardware and software. We use the
		  NAS parallel benchmarks as examples on how to apply our
		  techniques.},
    doi		= {http://dx.doi.org/10.1109/IPDPS.2006.1639567},
    pdf		= {Papers/Workshop/riesen_06_communication.pdf}
}

@InProceedings{	  Riesen:06:Supercomputer,
    author	= {Rolf Riesen},
    title	= {Supercomputer Simulation Design Through Simulation},
    booktitle	= {Cray User Group (CUG)},
    year	= {2006},
    abstract	= {Performance modeling of large scale applications and
		  predicting the impact of architectural changes on the
		  behavior of an application is difficult. Traditional
		  approaches to measuring applications often change their
		  behavior. Modeling an application on a new architecture
		  requires in-depth knowledge of the application. Both tasks
		  often involve studying an application to learn where probes
		  have to be inserted.
		  
		  In this paper we describe a novel approach that is a hybrid
		  between discrete event simulation and measuring the
		  behavior of a running application. We explain how our early
		  prototype works and how it can be used. We mention several
		  experiments that we have already performed with this
		  protype and show its potential for future research areas.},
    pdf		= {Papers/Misc/riesen_06_supercomputer.pdf}
}

@TechReport{	  Riesen:06:Portals,
    author	= {Rolf Riesen and Ron Brightwell and Arthur B. Maccabe and
		  Trammell Hudson and Kevin Pedretti},
    title	= {The {Portals} 3.3 Message Passing Interface: Document
		  Revision 2.0},
    month	= jan,
    year	= {2006},
    number	= {SAND2006-0420},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {{\sc Note:} This is the final version of the document for
		  Portals version 3.3. It supersedes SAND99-2959},
    abstract	= {This report presents a specification for the portals 3.3
		  message passing interface. Portals 3.3 are intended to
		  allow scalable, high-performance network communication
		  between nodes of a parallel computing system. Specifically,
		  it is designed to support a parallel computing platform
		  composed of clusters of commodity workstations connected by
		  a commodity system area network fabric. In addition,
		  Portals 3.3 are well suited to massively parallel
		  processing and embedded systems. Portals 3.3 represent an
		  adaption of the data movement layer developed for massively
		  parallel processing platforms, such as the 4500-node Intel
		  TeraFLOPS machine. Version 3.0 of Portals runs on the
		  Cplant cluster at Sandia National Laboratories, and version
		  3.3 is running on Cray's Red Storm system.},
    doi		= {http://dx.doi.org/10.2172/882925},
    pdf		= {Papers/Misc/riesen_06_portals.pdf}
}

@InProceedings{	  Riesen:06:What,
    author	= {Rolf Riesen and Courtenay Vaughan and Torsten Hoefler},
    title	= {What if {MPI} Collective Operations Were Instantaneous?},
    booktitle	= {Cray User Group (CUG)},
    year	= {2006},
    abstract	= {Collective MPI operations are an interesting research
		  topic, since their implementation is complex and there are
		  many different possible ways to improve their performance.
		  It is not always obvious how much a given application will
		  gain from such improvements and some of the methods to
		  improve collective performance are costly and
		  time-consuming to implement.
		  
		  In this paper we use a hybrid MPI simulator to run
		  benchmarks and applications with the cost of collective
		  operations set to zero. This allows us to gather
		  performance data that shows how much at most an application
		  could benefit from better collective operations.},
    pdf		= {Papers/Misc/riesen_06_what.pdf}
}

@InProceedings{	  Riesen:07:Scalable,
    author	= {Rolf Riesen},
    title	= {Scalable Collection of Large {MPI} Traces on {Red}
		  {Storm}},
    booktitle	= {Cray User Group (CUG)},
    month	= may,
    year	= {2007},
    abstract	= {Gathering large MPI traces and statistics is important for
		  performance analysis and trouble shooting of applications.
		  Traces, with detailed information about each single message
		  an application has sent, are crucial to characterize the
		  message passing behavior of an application. On massively
		  parallel systems like Red Storm the amount of data
		  collected impacts the performance and behavior of the
		  application and is therefore not feasible. We present a new
		  tool to enable the scalable collection of large amounts of
		  data on Red Storm class systems.},
    pdf		= {Papers/Misc/riesen_07_scalable.pdf}
}

@InProceedings{	  Riesen:07:Seshat,
    author	= {Rolf Riesen},
    editor	= {Franck Cappello and Thomas Herault and Jack Dongarra},
    title	= {Seshat Collects {MPI} Traces: Extended Abstract},
    booktitle	= {Recent Advances in Parallel Virtual Machine and Message
		  Passing Interface: 14th European {PVM/MPI} Users' Group
		  Meeting, {Paris}, {France}, September/October 2007
		  Proceedings},
    year	= {2007},
    volume	= {4757},
    pages	= {384--386},
    publisher	= {Springer-Verlag},
    series	= {Lecture Notes in Computer Science},
    abstract	= {No abstract},
    issn_isbn	= {978-3-540-75415-2},
    doi		= {http://dx.doi.org/10.1007/978-3-540-75416-9_52}
}

@TechReport{	  Riesen:08:Portals,
    author	= {Rolf Riesen and Ron Brightwell and Kevin Pedretti and
		  Keith Underwood and Arthur B. Maccabe and Trammell Hudson},
    title	= {The {Portals} 4.0 Message Passing Interface},
    month	= apr,
    year	= {2008},
    number	= {SAND2008-2639},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    abstract	= {This report presents a specification for the Portals 4.0
		  message passing interface. Portals 4.0 are intended to
		  allow scalable, high-performance network communication
		  between nodes of a parallel computing system. Portals 4.0
		  are well suited to massively parallel processing and
		  embedded systems. Portals 4.0 represent an adaption of the
		  data movement layer developed for massively parallel
		  processing platforms, such as the 4500-node Intel TeraFLOPS
		  machine. Version 3.0 of Portals runs on the Cplant cluster
		  at Sandia National Laboratories, and version 3.3 is running
		  on Cray's Red Storm system. Version 4.0 is targeted to the
		  next generation of machines employing advanced network
		  interface architectures to support enhanced offload
		  capabilities.},
    pdf		= {Papers/Misc/riesen_08_portals.pdf}
}

@TechReport{	  Riesen:09:Extensible,
    author	= {Rolf Riesen and Kurt Ferreira},
    title	= {An Extensible Operating System Design for Large-Scale
		  Parallel Machines},
    month	= apr,
    year	= {2009},
    number	= {SAND09-2660},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {{\sc Note:} This paper did not get accepted to HotOS XII.
		  We added a section to address some of the comments and
		  questions the reviewers had.},
    abstract	= {Running untrusted user-level code inside an operating
		  system kernel has been studied in the 1990's but has not
		  really caught on. We believe the time has come to resurrect
		  kernel extensions for operating systems that run on
		  highly-parallel clusters and supercomputers. The reason is
		  that the usage model for these machines differs
		  significantly from a desktop machine or a server. In
		  addition, vendors are starting to add features, such as
		  floating-point accelerators, multicore processors, and
		  reconfigurable compute elements. An operating system for
		  such machines must be adaptable to the requirements of
		  specific applications and provide abstractions to access
		  next-generation hardware features, without sacrificing
		  performance or scalability.},
    pdf		= {Papers/Misc/riesen_09_extensible.pdf}
}

@Article{	  Riesen:09:Designing,
    author	= {Rolf Riesen and Ron Brightwell and Patrick G. Bridges and
		  Trammell Hudson and Arthur B. Maccabe and Patrick M.
		  Widener and Kurt Ferreira},
    title	= {Designing and Implementing Lightweight Kernels for
		  Capability Computing},
    journal	= {Concurrency and Computation: Practice and Experience},
    month	= apr,
    year	= {2009},
    volume	= {21},
    number	= {6},
    pages	= {793--817},
    publisher	= {Wiley InterScience},
    abstract	= {In the early 1990s, researchers at Sandia National
		  Laboratories and the University of New Mexico began
		  development of customized system software for massively
		  parallel ``capability'' computing platforms. These
		  \emph{lightweight kernels} have proven to be essential for
		  delivering the full power of the underlying hardware to
		  applications. This claim is underscored by the success of
		  several supercomputers, including the Intel Paragon, Intel
		  ASCI Red, and the Cray XT series of systems, each having
		  established a new standard for high-performance computing
		  upon introduction. In this paper, we describe our approach
		  to lightweight compute node kernel design and discuss the
		  design principles that have guided several generations of
		  implementation and deployment. A broad strategy of
		  \emph{operating system specialization} has led to a focus
		  on \emph{user-level resource management},
		  \emph{deterministic behavior}, and \emph{scalable system
		  services}. The relative importance of each of these areas
		  has changed over the years in response to changes in
		  applications and hardware and system architecture. We
		  detail our approach and the associated principles, describe
		  how our application of these principles has changed over
		  time, and provide design and performance comparisons to
		  contemporaneous supercomputing operating systems.},
    doi		= {http://dx.doi.org/10.1002/cpe.v21:6}
}

@TechReport{	  Riesen:10:Redundant,
    author	= {Rolf Riesen and Kurt Ferreira and Jon Stearley and Ron
		  Oldfield and James H. Laros III and Kevin Pedretti and Ron
		  Brightwell},
    title	= {Redundant Computing for Exascale Systems},
    month	= dec,
    year	= {2010},
    number	= {SAND2010-8709},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    abstract	= {Exascale systems will have hundred thousands of compute
		  nodes and millions of components which increases the
		  likelihood of faults. Today, applications use
		  checkpoint/restart to recover from these faults. Even under
		  ideal conditions, applications running on more than 50,000
		  nodes will spend more than half of their total running time
		  saving checkpoints, restarting, and redoing work that was
		  lost.
		  
		  Redundant computing is a method that allows an application
		  to continue working even when failures occur. Instead of
		  each failure causing an application interrupt, multiple
		  failures can be absorbed by the application until
		  redundancy is exhausted. In this paper we present a method
		  to analyze the benefits of redundant computing, present
		  simulation results of the cost, and compare it to other
		  proposed methods for fault resilience.},
    pdf		= {Papers/Misc/riesen_10_redundant.pdf}
}

@InProceedings{	  Riesen:10:See,
    author	= {Rolf Riesen and Kurt Ferreira and Jon Stearley},
    title	= {See Applications Run and Throughput Jump: The Case for
		  Redundant Computing in {HPC}},
    booktitle	= {1st International Workshop on Fault-Tolerance for HPC at
		  Extreme Scale (FTXS 2010) in conjunction with The 40th
		  Annual IEEE/IFIP International Conference on Dependable
		  Systems and Networks (DSN 2010)},
    month	= jun,
    year	= {2010},
    address	= {Chicago, IL, USA},
    abstract	= {For future parallel computing systems with one hundred
		  thousand nodes or more we propose redundant computing to
		  reduce the number of application interrupts. The frequency
		  of faults in exascale systems will be so high that
		  traditional checkpoint/restart methods will break down.
		  Applications will experience interruptions so often that
		  they will spend more time restarting and recovering lost
		  work, than computing the solution. We show that redundant
		  computation at large scale can be cost effective and allows
		  applications to complete their work in significantly less
		  wall-clock time. On truly large systems, redundant
		  computing can increase system throughput by an order of
		  magnitude.},
    day		= {28},
    pdf		= {Papers/Workshop/riesen_10_see.pdf}
}

@InProceedings{	  Rodrigues:10:Structural,
    author	= {A.F. Rodrigues and J. Cook and E. Cooper-Balis and K.
		  Scott Hemmert and C. Kersey and R. Riesen and P. Rosenfield
		  and R. Oldfield and M. Weston and B. Barrett and B. Jacob},
    title	= {The Structural Simulation Toolkit},
    booktitle	= {1st International Workshop on Performance Modeling,
		  Benchmarking and Simulation of High Performance Computing
		  Systems (PMBS 10) held as part of SC10},
    month	= nov,
    year	= {2010},
    abstract	= {As supercomputers grow, understanding their behavior and
		  performance has become increasingly challenging. New
		  hurdles in scalability, programmability, power consumption,
		  reliability, cost, and cooling are emerging, along with new
		  technologies such as 3D integration, GP-GPUs, silicon
		  photonics, and other ``game changers''. Currently, they HPC
		  community lacks a unified toolset to evaluate these
		  technologies and design for these challenges.
		  
		  To address this problem, a number of institutions have
		  joined together to create the Structural Simulation Toolkit
		  (SST), an open, modular, parallel, multi-criteria,
		  multi-scale simulation framework. The SST includes a number
		  of processor, memory, and network models. The SST has been
		  used in a variety of network, memory, and application
		  studies and aims to become the standard simulation
		  framework for designing and procuring HPC systems.}
}

@InProceedings{	  Shuler:95:Puma,
    author	= {Lance Shuler and Chu Jong and Rolf Riesen and David van
		  Dresser and Arthur B. Maccabe and Lee Ann Fisk and T. Mack
		  Stallcup},
    title	= {{The Puma Operating System for Massively Parallel
		  Computers}},
    booktitle	= {Proceeding of the 1995 Intel Supercomputer User's Group
		  Conference},
    year	= {1995},
    organization = {Intel Supercomputer User's Group},
    abstract	= {This paper describes the Puma operating system, the
		  successor to the Sandia and University of New Mexico
		  operating system (SUNMOS). Puma is a multiprocessor
		  operating system that is made up of two basic structures,
		  the quintessential kernel (Q-kernel) and the process
		  control thread (PCT). Together, they provide the user with
		  a flexible, lightweight, high performance, message passing
		  environment for massively parallel computers. In this
		  paper, we discuss the structure of Puma and its unique
		  message passing design based on portals.},
    indexmark	= {\bibindex{Puma}},
    pdf		= {Papers/Misc/shuler_95_puma.pdf}
}

@Article{	  Tournier:06:Towards,
    author	= {Jean-Charles Tournier and Patrick G. Bridges and Arthur B.
		  Maccabe and Patrick M. Widener and Zaid Abudayyeh and Ron
		  Brightwell and Rolf Riesen and Trammel Hudson},
    title	= {Towards a framework for dedicated operating systems
		  development in high-end computing systems},
    journal	= {ACM SIGOPS Operating Systems Review},
    month	= apr,
    year	= {2006},
    address	= {New York, NY, USA},
    volume	= {40},
    number	= {2},
    pages	= {16--21},
    publisher	= {ACM Press},
    abstract	= {In the context of high-end computing systems,
		  general-purpose operating systems impose overhead on the
		  applications they support due to unneeded services.
		  Although dedicated operating systems overcome this issue,
		  they are difficult to develop or adapt. In this paper, we
		  propose a framework, based on the component programming
		  paradigm, which supports the development and adaptation of
		  such operating systems. This framework makes possible the a
		  la carte construction of operating systems which provide
		  specific high-end computing system characteristics.},
    doi		= {http://doi.acm.org/10.1145/1131322.1131330},
    issn	= {0163-5980},
    pdf		= {http://www.sandia.gov/~rbbrigh/papers/configos-osr.pdf}
}

@Misc{		  Ruiz:10:Fault,
    author	= {Maria Ruiz Varela and Kurt B. Ferreira and Rolf Riesen},
    title	= {Fault-Tolerance for Exascale Systems},
    month	= sep,
    year	= {2010},
    howpublished = {Poster at IEEE International Conference on Cluster
		  Computing},
    abstract	= {Periodic, coordinated, checkpointing to disk is the most
		  prevalent fault tolerance method used in modern
		  large-scale, capability class, high-performance computing
		  (HPC) systems. Previous work has shown that as the system
		  grows in size, the inherent synchronization of coordinated
		  checkpoint/restart (CR) limits application scalability; at
		  large node counts the application spends most of its time
		  checkpointing instead of executing useful work.
		  Furthermore, a single component failure forces an
		  application restart from the last correct checkpoint.
		  Suggested alternatives to coordinated CR include
		  uncoordinated CR with message logging, redundant
		  computation, and RAID-inspired, in-memory distributed
		  checkpointing schemes. Each of these alternatives have
		  differing overheads that are dependent on both the scale
		  and communication characteristics of the application. In
		  this work, using the Structural Simulation Toolkit (SST)
		  simulator, we compare the performance characteristics of
		  each of these resilience methods for a number of HPC
		  application patterns on a number of proposed exascale
		  machines. The result of this work provides valuable
		  guidance on the most efficient resilience methods for
		  exascale systems.},
    pdf		= {Papers/Misc/ruiz_10_fault.pdf}
}

@InProceedings{	  Wagner:04:NIC-Based,
    author	= {Adam Wagner and Hyun-Wook Jin and Dhabaleswar K. Panda and
		  Rolf Riesen},
    title	= {{NIC}-Based Offload of Dynamic User-Defined Modules for
		  {Myrinet} Clusters},
    booktitle	= {IEEE Cluster Computing 2004},
    month	= sep,
    year	= {2004},
    address	= {San Diego, California},
    pages	= {205--214},
    abstract	= {Many of the modern networks used to interconnect nodes in
		  cluster-based computing systems provide network interface
		  cards (NICs) that offer programmable processors. Much
		  research has been done with the focus of offloading
		  processing from the host to the NIC processor. However, the
		  research has mainly focused on offloading ad-hoc features
		  to the NIC, mainly to support the optimization of common
		  collective and synchronization-based communications. In
		  this paper, we describe the design and implementation of a
		  new framework based on MPICH-GM to support the dynamic
		  NIC-based offload of user-defined modules for Myrinet
		  clusters. We evaluate our implementation on a 16-node
		  cluster using a NIC-based version of the common broadcast
		  operation and we find a factor of improvement of up to
		  nearly 1.3 for our implementation versus the standard
		  host-based implementation of broadcast. In addition, we see
		  that this factor of improvement increases with system size,
		  indicating that our implementation is more scalable than
		  the default host-based approach. To the best of our
		  knowledge, this is the first such study for Myrinet
		  clusters.},
    doi		= {http://dx.doi.org/10.1109/CLUSTR.2004.1392618},
    accepted	= {32.0%}
}

@TechReport{	  Wheat:93:Puma,
    author	= {Stephen R. Wheat and Arthur B. Maccabe and Rolf Riesen and
		  David W. van Dresser and T. Mack Stallcup},
    title	= {{PUMA}: An Operating System for Massively Parallel
		  Systems},
    year	= {1993},
    number	= {SAND93-2171C},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {This is the tech report for the HICSS paper.},
    abstract	= {This paper presents an overview of PUMA,
		  (Performance-oriented, User-managed Messaging
		  Architecture), a message passing kernel. Message passing in
		  PUMA is based on portals-an opening in the address space of
		  an application process. Once an application process has
		  established a portal, other processes can write values into
		  the portal using a simple send operation. Because messages
		  are written directly into the address space of the
		  receiving process, there is no need to buffer messages in
		  the PUMA kernel and later copy them into the applications
		  address space. PUMA consists of two components: The
		  quintessential kernel (Q-Kernel) and the process control
		  thread (PCT) While the PCT provides management decisions,
		  the Q-Kernel controls access and implements the policies
		  specified by the PCT},
    indexmark	= {\bibindex{Puma}},
    pdf		= {Papers/Misc/wheat_93_puma.pdf}
}

@Article{	  Wheat:94:Puma,
    author	= {Stephen R. Wheat and Arthur B. Maccabe and Rolf Riesen and
		  David W. van Dresser and T. Mack Stallcup},
    title	= {{PUMA}: An Operating System for Massively Parallel
		  Systems},
    journal	= {Scientific Programming},
    year	= {1994},
    volume	= {3},
    pages	= {275--288},
    annote	= {This is an extended versions of the Hawaii paper
		  \cite{Wheat:94:PumaHICSS}. It contains the new stuff about
		  capabilities.},
    abstract	= {This article presents an overview of PUMA
		  (Performance-oriented, User-managed Messaging
		  Architecture), a message-passing kernel for massively
		  parallel systems. Message passing in PUMA is based on
		  portals - an opening in the address space of an application
		  process. Once an application process has established a
		  portal, other processes can write values into the portal
		  using a simple send operation. Because messages are written
		  directly into the address space of the receiving process,
		  there is no need to buffer messages in the PUMA kernel and
		  later copy them into the applications address space. PUMA
		  consists of two components: the quintessential kernel
		  (Q-Kernel) and the process control thread (PCT). Although
		  the PCT provides management decisions, the Q-Kernel
		  controls access and implements the policies specified by
		  the PCT.},
    indexmark	= {\bibindex{Puma}},
    issn	= {1058-9244},
    doi		= {http://iospress.metapress.com/content/y134v687071xq103/}
}

@InProceedings{	  Wheat:94:PumaHICSS,
    author	= {Stephen R. Wheat and Arthur B. Maccabe and Rolf Riesen and
		  David W. van Dresser and T. Mack Stallcup},
    title	= {{PUMA}: An Operating System for Massively Parallel
		  Systems},
    booktitle	= {Proceedings of the Twenty-Seventh Annual Hawaii
		  International Conference on System Sciences},
    year	= {1994},
    pages	= {56--65},
    publisher	= {IEEE Computer Society Press},
    annote	= {THE Puma paper. Describes the kernel and portals.},
    abstract	= {This paper presents an overview of PUMA
		  (Performance-oriented, User-managed Messaging
		  Architecture), a message passing kernel. Message passing in
		  PUMA is based an portals-an opening in the address space of
		  an application process. Once an application process has
		  established a portal, other processes can write values into
		  the portal using a simple send operation. Because messages
		  are written directly into the address space of the
		  receiving process, there is no need to buffer messages in
		  the PUMA kernel and later copy them into the applications
		  address space. PUMA consists of two components: the
		  quintessential kernel (Q-Kernel) and the process control
		  thread (PCT). While the PCT provides management decisions,
		  the Q-Kernel controls access and implements the policies
		  specified by the PCT},
    doi		= {http://dx.doi.org/10.1109/HICSS.1994.323279},
    indexmark	= {\bibindex{Puma}},
    issn_isbn	= {0-8186-5060-5},
    accepted	= {50.0%},
    pdf		= {Papers/Conf/wheat_94_pumaHICSS.pdf}
}

@InProceedings{	  Womble:93:Beyond,
    author	= {David Womble and David Greenberg and Stephen Wheat and
		  Rolf Riesen},
    title	= {Beyond Core: Making Parallel Computer {I/O} Practical},
    booktitle	= {DAGS'93 Proceedings},
    month	= jun,
    year	= {1993},
    pages	= {56--63},
    abstract	= {The solution of Grand Challenge Problems will require
		  computations which are too large to fit in the memories of
		  even the largest machines. Inevitably, new designs of I/O
		  systems will be necessary to support them. Through our
		  implementations of an out-of-core LU factorization we have
		  learned several important lessons about what I/O systems
		  should be like. In particular we believe that the I/O
		  system must provide the programmer with the ability to
		  explicitly manage storage. One method of doing so is to
		  have a partitioned secondary storage in which each
		  processor owns a logical disk. Along with operating system
		  enhancements which allow overheads such as buffer copying
		  to be avoided, this sort of I/O system meets the needs of
		  high performance computing.},
    pdf		= {Papers/Conf/womble_93_beyond.pdf}
}

@InProceedings{	  Womble:93:Out,
    author	= {David E. Womble and David S. Greenberg and Rolf E. Riesen
		  and Stephen R. Wheat},
    title	= {Out of Core, Out of Mind: Practical Parallel {I/O}},
    booktitle	= {Proceedings of the Scalable Libraries Conference},
    month	= oct,
    year	= {1993},
    pages	= {10--16},
    organization = {Mississippi State University},
    abstract	= {Parallel computers are becoming more powerful and more
		  complex in response to the demand for computing power by
		  scientists and engineers. Inevitably, new and more complex
		  I/O systems will be developed for these systems. In
		  particular we believe that the I/O system must provide the
		  programmer with the ability to explicitly manage storage
		  (despite the trend toward complex parallel file systems and
		  caching schemes). One method of doing so is to have a
		  partitioned secondary storage in which each processor owns
		  a logical disk. Along with operating system enhancements
		  which allow overheads such as buffer copying to be avoided
		  and libraries to support optimal remapping of data, this
		  sort of I/O system meets the needs of high performance
		  computing.},
    doi		= {http://dx.doi.org/10.1109/SPLC.1993.365587},
    pdf		= {Papers/Conf/womble_93_out.pdf}
}

@TechReport{	  Womble:94:LU,
    author	= {David Womble and David Greenberg and Stephen Wheat and
		  Rolf Riesen},
    title	= {{LU} Factorization and the {LINPACK} Benchmark on the
		  {Intel} {Paragon}},
    year	= {1994},
    number	= {SAND94-0428},
    institution	= {Sandia National Laboratories},
    type	= {Technical report},
    annote	= {72.9 Gflops/sec on 1872 nodes},
    abstract	= {An implementation of the LINPACK benchmark is described
		  which achieves 72.9 Gflop/sec on 1872 nodes of an Intel
		  Paragon. Implications concerning the architecture of the
		  Paragon and the necessity of a high performance operating
		  system like SUNMOS.},
    pdf		= {Papers/Misc/womble_94_lu.pdf}
}

@TechReport{	  Zhu:03:Identifying,
    author	= {Wenbin Zhu and Arthur B. Maccabe and Rolf Riesen},
    title	= {Identifying the Sources of Latency in a Splintered
		  Protocol},
    year	= {2003},
    number	= {TR-CS-2003-44},
    institution	= {Computer Science Department, The University of New
		  Mexico},
    abstract	= {Communication overhead is a critical factor for
		  application performance in cluster computing based on
		  commodity hardware. We propose a general strategy,
		  splintering, to improve the system's performance. In the
		  splintering strategy, previously centralized functionality
		  is broken into pieces, and the pieces are distributed among
		  the processors in a system in way that ensures system
		  integrity and improves performance.
		  
		  In this paper, we describe our efforts to use splintering
		  to reduce latency. To date, our efforts have not resulted
		  in the improvement that we originally anticipated. In order
		  to identify the sources of latency, we have done a thorough
		  instrumentation of our implementation. Based on our
		  analysis of our measurements, we propose several
		  modifications to the MPI library and the NIC firmware.},
    pdf		= {Papers/Misc/zhu_03_identifying.pdf}
}
