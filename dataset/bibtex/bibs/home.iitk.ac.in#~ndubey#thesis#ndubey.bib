@Article{Guo/Zhu:2005
	author=	{	Guo, Cheng-en, Zhu, Song-Chun, Wu, Ying N},
	year=	{	2005},
	keywords=	{	PRIMAL SKETCH, TEXTURE, TEXTON, SPARSE CODING, SKETCHABLE IMAGE},
	institution=	{UC Los Angeles},
	title=	{	Primal Sketch: Integrating Texture and Structure},
	journal=	{	Computer Vision and Image Understanding Special issue on Generative Model Based Vision
},
	month=	{ April},
	volume=	{106, Issue 1},
	pages=	{5-19},
	annote=	{
	
	Publication Date:
	03-21-2005
Permalink:
http://www.escholarship.org/uc/item/10h2s0kq

They have divided the image in texture and textons. Texton is nothing but object boundaries edge kind of thing. So an image is divided in sketchable and non-sketchanble part. Sketchable part is texton. they have modeled the image via generative model. to draw Textons first they have evaluated edges and corner of the image by LOG(laplacian of gaussian). Then we will do pursue algorithm by joining the edges and corners then comparing how near the image we are.
This is sketch pursuit algorithm. So we will iterate on image drawing/joining edges to build the sketchable part of the image. After this there is another phase in sketch pursuit which just refine the sketch by some reversible graph operator that is by deleting some junction it compares with original image.

Texture is synthesized by MRF. It works like for each pixel we apply series of functions and then store this histogram as signature for the texture.
functions are called filters and so we have 1D histogram of filter response.
In other words let us say \delta is set of filters then for all F_i \belongs \delta
we calculate F_i(I(x,y)) for image I.


Following Marr's insight, they have proposed a generative image representation called primal sketch, which integrates two modeling components. The first component explains the structural part of an image, such as object boundaries, by a hidden layer of image primitives. The second component models the remaining textural part without distinguishable elements by Markov random fields that interpolate the structural part of the image. They call the two components, sketchable and non-sketchable.
The inference of the sketch graph consists of two phases. Phase I sequentially adds the most prominent image primitives in a procedure similar to matching pursuit. Phase II edits the sketch graph by a number of graph operators to achieve good Gestalt organizations. Experiments show that the primal sketch model produces satisfactory results for a large number of generic images. The primal sketch model is not only a parsimonious image representation for lossy image coding, but also provides a meaningful mid-level generic representation for other vision tasks. 

The paper also discuss about dictionary of tetanus which is bar, blob T-junctions etc. These are stored with different parameters in dictionary e.g. a bar is defined by start location (x,y) coordinate, length orientation and width.


	- nandan 1/10},
}

@InProceedings{Shi/Zhu:2007
	author=	{	Shi, Kent, Zhu, Song-Chun},
	year=	{	2007},
	keywords=	{	IMAGE PATCHES, EXPLICIT MANIFOLD, IMPLICIT MANIFOLD},
	institution=	{UC Los Angeles},
	title=	{	Mapping Natural Image Patches by Explicit and Implicit Manifolds},
	booktitle=	{CVPR}
	proceeding=	{	IEEE Conference on Computer Vision and Pattern Recognition},
	month=	{ June},
	pages=	{1-7},
	annote=	{
DOI Bookmark: http://doi.ieeecomputersociety.org/10.1109/CVPR.2007.382980

In this paper they have divided the image in two part one is basically geometric property of image sparsely modeled low dimensional manifold explicitly modeled other is texture morkov random field modeled high dimensional implicit manifold.	
Now they have given a manifold pursue algorithm which tries to generate the image via texture and textons comparing the resultant image with original via KL-divergance.
KL-divergance is just a way to evaluate difference between two probability distribution. So Markov random field model and texton model compete to generate the image.
The difference between dimension of manifold is due to coding scheme of texture and textons. Textons can be easily coded with 3-4 parameters i.e. position, width, hight, orientation, type etc while for texture the information is too much and coding is done via histogram of filters.
General image contains both types so manifold is a mixture of both high and low dimension. 

We study the structures of this ensemble by mapping natural image patches into two types of subspaces which we call "explicit manifolds" and "implicit manifolds" respectively. On explicit manifolds, one finds those simple and regular image primitives, such as edges, bars, corners and junctions. On implicit manifolds, one finds those complex and stochastic image patches, such as textures and clutters. On different types of manifolds, different perceptual metrics are used. We propose a method for learning a probabilistic distribution on the space of patches by pursuing both types of manifolds using a common information theoretical criterion. The connection between the two types of manifolds is realized by image scaling, which changes the entropy of the image patches. The explicit manifolds live in low entropy regimes while the implicit manifolds live in high entropy regimes. We study the transition between the two types of manifolds over scale and show that the complexity of the manifolds peaks in a middle entropy regime.

	- nandan 1/10},
	
}

@InProceedings{Wu/Si:2009
	author=	{	 Ying Wu, Zhangzhang Si, Haifeng Gong, Song-Chun Zhu},
	year=	{	2009},
	keywords=	{	Deformable Template, Generative model, Shared sketch algorithm, Sum maps, max maps, wavelet sparse coding},
	institution=	{UC Los Angeles},
	title=	{	Learning Active Basis Model for Object Detection and Recognition},
	proceeding=	{	nternational Journal of Computer Vision},
	month=	{ August},
	annote=	{
Permanent Link:
http://www.stat.ucla.edu/~sczhu/papers/IJCV_ActiveBasis.pdf

This article is about learning a template from similar object then recognizing based on these deformable learned template.
First Gabor filter of the image is calculated with specified number of gabber elements this gives us the sketch template of the image. The algorithm iterate over whole set of training images then calculate the shared template sketch. These Gabor elements are called active because we allow slight perturbation (in orientation and location).

After learning shared sketch for an object category recognition of finding the same object in other image is simple They have called the procedure of searching or identification sum-max map they iterate between these two procedure. Sum map is just determining the gabber element of an image and max map is enhancing locally based on deformable template. So recognition is actually not possible they can only search the object inside an image. 

	- nandan 2/10}
	
}


@InProceedings{Shi/Zhu:2009
	author=	{	Song-Chun Zhua, Kent Shia, Zhangzhang Sia},
	year=	{	2009},
	keywords=	{	IMAGE PATCHES, EXPLICIT MANIFOLD, IMPLICIT MANIFOLD, TEXTURE, TEXTONS, PRIMAL SKETCH},
	institution=	{UC Los Angeles},
	title=	{	Learning explicit and implicit visual manifolds by information projection},
	booktitle=	{Elsevier}
	proceeding=	{	Pattern Recognition Letters},
	month=	{ August},
	annote=	{
DOI Bookmark: http://doi.ieeecomputersociety.org/10.1109/CVPR.2007.382980
	
	Image patches are fundamental elements for object modeling and recognition. However, there has not been a panoramic study of the structures of the whole ensemble of natural image patches in the literature. In this article, we study the structures of this ensemble by mapping natural image patches into two types of subspaces which we call "explicit manifolds" and "implicit manifolds" respectively. On explicit manifolds, one finds those simple and regular image primitives, such as edges, bars, corners and junctions. On implicit manifolds, one finds those complex and stochastic image patches, such as textures and clutters. On different types of manifolds, different perceptual metrics are used. We propose a method for learning a probabilistic distribution on the space of patches by pursuing both types of manifolds using a common information theoretical criterion. The connection between the two types of manifolds is realized by image scaling, which changes the entropy of the image patches. The explicit manifolds live in low entropy regimes while the implicit manifolds live in high entropy regimes. We study the transition between the two types of manifolds over scale and show that the complexity of the manifolds peaks in a middle entropy regime.

	- nandan 2/10},

}


@INPROCEEDINGS{Tenenbaum98mappinga,
    author = {Joshua B. Tenenbaum},
    title = {Mapping a Manifold of Perceptual Observations},
    booktitle = {Advances in Neural Information Processing Systems 10},
    year = {1998},
    pages = {682--688},
    publisher = {MIT Press}
	annote=	{

A high dimensional data can be modeled through manifold which not only reduce it's dimensionality but also provide us useful information. Like a set of face images which are in all aspect similar other than lightning(intensity) or orientation change can be modeled through a low dimension manifold which actually is in only 2 dimension telling us exactly what is changing in which dimension.
Global distance are misleading for perceptual observation while Geodesic distance gives us important information.
So here author has given a two step process first all data point is represented in high dimensional space then we find the network by Isomap algorithm which gives us approximation of geodesic distance between data points. After this we find the feature space transformation to map this manifold in low dimension space which preserve the geodesic distance.
for this we select some data points from the manifold we have then start building a graph by joining points which have at least one neighbor common. Then we calculate approximate geodesic distance between each pair of node by floyd'd algorithm. Here distance between connected node is just euclidian distance between them and for non connected we find length of shortest connected path length. Now we find k-dimensional euclidian embedding that preserve as closely as possible the graph distance. for this he has used Ordinal multidimensional scaling(MDS: Cox and Cox 1994)


	- nandan 2/10},
}
@INPROCEEDINGS{Zhu02whatare,
    author = {Song-chun Zhu and Cheng-en Guo and Yingnian Wu and Yizhou Wang},
    title = {What are textons?},
    booktitle = {International Journal of Computer Vision},
    year = {2002},
    pages = {121--143},
	annote=	{
	
Textons are fundamental unit of images with some geometric and photometric properties.
Decomposing image help us in modeling and recognition understanding features and reduce redundancy.
In this paper we have studied image via generative model. sparse coding is also a generative model. Here we have some base features which are translated rotated to generate the dictionary.

In this paper author has used k-mean clustering to get the fetters. So initially there is pyramid of functions which he applies on patch of image to generate features at different levels.
So we have D={F_1, F_2,  . . , F_n} image filters. Now feature vector is generated by applying these filters as
F(I) = {F_1* I (x,y), F_2 * I(x,y), . . , F_n * I (x,y)} after this we take k-mean clustering to get the image icon. So, F_c = f_c1, . .. , f_cN) be a cluster center. Now these cluster centers are primitive feature or textons of image.

	- nandan 3/10}
}


@INPROCEEDINGS{Saxena04non-lineardimensionality,
    author = {Ashutosh Saxena and Abhinav Gupta and Amitabha Mukerjee},
    title = {Non-linear dimensionality reduction by locally linear isomaps. Lecture},
    booktitle = {Neural Information Processing 2004},
    year = {2004},
    pages = {1038--1043}
   annote=	{
	Current algorithm for Non linear dimensionality reduction using manifold are Isomaps, Local Linear Embeding and Laplacian Eigenmaps. there is one major drawback of using manifold i.e. they require a lot of non noisy data to perform well. When data points is sparse non neighbors may also get connected.
In this article authors have tried to recover from this drawback
LLE is based on local embedding so if different folds of manifold are close by they may get connected and whole point of manifold construction is distorted.
So here they have done K-nearest neighbor approach with weight assigned to each connected pair based on actual euclidian distance.
The neighbors whose Euclidean distance is less and those lying on the locally linear patch of the manifold get higher weights, and hence are selected preferably. 

  - nandan 3/10 },
}

@inproceedings{1097700,
 author = {Jurie, Frederic and Triggs, Bill},
 title = {Creating Efficient Codebooks for Visual Recognition},
 booktitle = {ICCV '05: Proceedings of the Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
 year = {2005},
 isbn = {0-7695-2334-X-01},
 pages = {604--610},
 doi = {http://dx.doi.org/10.1109/ICCV.2005.66},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},

annote=	{
Visual codebook extracted from local image patches is one another way for object recognition.
But by just doing k-mean clustering on the descriptor vectors of patches gives a codebook which is not too much descriptive only general patches are determined by clustering which are common to many images. So to reduce the densest few regions of images and getting the discriminative features new clustering ago is provided in this paper. This is important because the patches that are most informative for classification tend to have intermediate frequency.
Codebook algorithm has three stages first is dividing image in patch second is getting feature vector out of a patch i.e. appearance descriptor and finally some king of quantization.
Features spaces are typically designed to capture perceptually meaningful distinctions while providing good resistance to extraneous details such as changes in illumination. 
Simple Clustering algorithm has problems like it is too much biased toward denser regions also if points lying far from any center that they are assigned to tend to distort the posit of center, also we must know the number of cluster beforehand.
The authors clustering algorithm is variant of mean shift clustering. In this ago we draw N patches uniformly randomly from currently unlabeled part of dataset compute their maximal density region by running mean shift estimator with Gaussian kernel of width h. Allocate a new center at maximal density position  and then eliminating these maximal density regions. Eliminating the labelled patches prevents the algorithm to reassign centers to the same density region. the algorithm can be stopped when sufficiently large number of cluster centers are found.

	- nandan 5/10},
 }



@Article{Lazebnik/Schmid:2006
	author=	{	S. Lazebnik, C. Schmid, J. Ponce},
	year=	{	2006},
	keywords=	{	Scene Recognition, spatial pyramid},
	title=	{	Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories},
	journal=	{	Computer Vision and Pattern Recognition},
	month=	{ April},
	volume=	{2},
	pages=	{2169-2178},
	annote=	{
	
Permalink:
http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/cvpr06b.pdf

The paper is about recognizing scene category using bags of feature technique.
Generally bags of feature doesn't preserve spatial location of features inside the image it works on histogram of features but in this paper they have done local histogram analysis. So they have divided the scene in small subimages and then have done the bags of feature histogram analysis.
They have devised a spatial pyramid matching technique which start matching from top and then splitting the scene at different level in multiple small scene then again doing bag of feature analysis at that level.
So at any fixed resolution, two points are said to match if they fall into the same cell of the grid; matches found at finer resolutions are weighted more highly than matches found at coarser resolutions.
Feature extraction is important part of bags of feature technique they have used "weak features" which is nothing but edge,corner etc and then strong feature which is SIFT. then they have done k-mean clustering with k=200 or 400 to get the dictionary.

A very easy paper to understand there addition is just converting the orderless bag of feature technique to somewhat spatial local pyramid based bag of feature technique.

They have claimed good performance on caltech101 database and Graz dataset.

  - nandan 5/10},

}

@article{608265,
 author = {Srivastava, A. and Lee, A. B. and Simoncelli, E. P. and Zhu, S.-C.},
 title = {On Advances in Statistical Modeling of Natural Images},
 journal = {J. Math. Imaging Vis.},
 volume = {18},
 number = {1},
 year = {2003},
 issn = {0924-9907},
 pages = {17--33},
 doi = {http://dx.doi.org/10.1023/A:1021889010444},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 annote=	{
	
Permalink:
http://www.stat.ucla.edu/~sczhu/papers/srivastavapaper.pdf
This paper is kind of serve for natural image statistics.
Natural image statistics can be used in wide variety of application like compression, denoising.
Non gaussian behavior of image statistics and invariance in statistics of image after scaling are tried to explained here.

	- nandan 5/10 },
	
 }

@article{MRF,
 author = {Brani Vidakovic},
 title = {Markov Random Fields},

 annote=	{
Permalink:
http://www2.isye.gatech.edu/~brani/isyebayes/bank/handout16.pdf

n-dimension random process defined on a discrete lattice where state of a particular point depends only on the state of it's direct neighbors.
Ising Model:To understand MRF we first have to understand Ising model. Ising model is an idealized system of interacting particles, arranged onto a regular planar grid. Each particle can have one of the two spin orientations generally labelled +1 or -1. Each particle interact with it's nearest neighbor the total energy of whole system depends on spin orientation of nearest neighbors.
	- nandan 6/10 },

}




@inproceedings{1290088,
 author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
 title = {Regularized regression on image manifold for retrieval},
 booktitle = {MIR '07: Proceedings of the international workshop on Workshop on multimedia information retrieval},
 year = {2007},
 isbn = {978-1-59593-778-0},
 pages = {11--20},
 location = {Augsburg, Bavaria, Germany},
 doi = {http://doi.acm.org/10.1145/1290082.1290088},
 publisher = {ACM},
 address = {New York, NY, USA},
 
 annote=	{
Image Retrieval(searching in database)

The main benefit of manifold is relevancy factor because nearby images are less in geodesic distance.
In this paper they have used manifold locally with linear regression.
The problem of image retrieval is defined as follows.
you have a database X={x_1, x_2, . . . , x_m} then you have some query image q now you have to output images from database which are similar or say near to q. So you have a function say f which applied ti x_i gives 1 or -1 based on relevancy with q.
For this f first you train your function which is simple if you do linear regression 
f(x) = a^T x (a transpose a is a matrix)
s.t. function Sumation_{i=1}^l (1- y_i a^T x_i) is minimized.
where y_i = 1 if x_i is relevant otherwise -1.
First in training we construct adjacency graph by joining imgaes which are listed a p nearest neighbor of other image. Also connect images which are labeled similar. Now remove edges between those samples which are labeled different.
for m datasample we have a matrix m * m with weight assigned to each joining vertices i to j.
This is forming kind of local manifold that's all in the paper.

  - nandan 6/10 },
}


@inproceedings{1101259,
 author = {Lyu, Siwei},
 title = {Automatic image orientation determination with natural image statistics},
 booktitle = {MULTIMEDIA '05: Proceedings of the 13th annual ACM international conference on Multimedia},
 year = {2005},
 isbn = {1-59593-044-2},
 pages = {491--494},
 location = {Hilton, Singapore},
 doi = {http://doi.acm.org/10.1145/1101149.1101259},
 publisher = {ACM},
 address = {New York, NY, USA},

 annote=	{
An image statistics is collected from various type of images which are rotated at varied angles at different scales.
An SVM classifier is trained to determine the angle of orientation.

Statistics are collected from a multi-scale multi- orientation image decomposition based on separable quadrature mirror filters (QMFs).
which I don't know some kind of frequency analysis of image may be based on wavelet decomposition.
The decomposition are low-pass, vertical, horizontal, and diagonal subbands at different scales.
Now using the statistics SVM classifier is trained 

  - nandan 6/10},
 }



@inproceedings{bb60621,
        AUTHOR = "Ni, K.S. and Nguyen, T.Q.",
        TITLE = "A model for image patch-based algorithms",
        BOOKTITLE = ICIP08,
        YEAR = "2008",
        PAGES = "2588-2591",
        BIBSOURCE = "http://www.visionbib.com/bibliography/pattern641.html#TT57853"
	annote=	{
Statistics of image patches depend loosely on the size of patch window for at least small windows. So statistics is different for 7 * 7 patch for different size of video sequence. But the effect become minimal given a sufficiently larger window.
Image statistics is seldom gaussian generally they have high kurtosis.
Sharply picked positive kurtosis.
Mixture of Gaussian is good model for such statistics.
Author has taken an example of histogram which is good for statistics analysis. As we can see patches with texture will have gaussian distribution(small variance) but patches with lot of edges will give somewhat multi peak statistics.
Another traditional distribution modeling technique is k-nearest neighbor
  - nandan 6/10},

}




@article{Souvenir2007365,
title = "Image distance functions for manifold learning",
journal = "Image and Vision Computing",
volume = "25",
number = "3",
pages = "365 - 373",
year = "2007",
note = "Articulated and Non-rigid motion",
issn = "0262-8856",
doi = "DOI: 10.1016/j.imavis.2006.01.016",
url = "http://www.sciencedirect.com/science/article/B6V09-4JKHP70-1/2/82eff8e038d1000e58e01136e70c4ac9",
author = "Richard Souvenir and Robert Pless",
keywords = "Isomap",
keywords = "Manifolds",
keywords = "Non-parametric registration"

	annote=	{

Many natural image sequence lie on very low dimensional manifold. Generally linear dimensional reduction technique do not work for image because of their transformation property. So if we model image sequence with PCA or ICA we don't get good representation. PCA which works as having base images learned it tries to represent the new image by linear combination of base image. This kind of representation generally fails if we have translated rotated or at  different lighting condition or scaled images.

Recently ISOMAP and LLE and similar other dimension reduction technique are  widely used for images because of their non linear dimension reduction method. They are able to model some sequence of images naturally as human sees them. The Geodesic distance we calculate between images matches with our perception. But their is great chance of amendment in these method to make them useful for wide spectra of images.
In this paper authors have tried to give distance measure which is basic requirement of all these manifold methods. In particular they have only considered ISOMAP which first calculate the distance between each pair of image in the database then tries to fit the images in a low dimension space such that the distance between the dataset images remain proportional to what is calculated. It does this by joining the nearby images(nearest neighbor generally between 5-10) images with edges by this forming a graphical structure.

Work can be done on this:
So if I is the data set images ISOMAP does f : I -> R^d . This is one more disadvantage of ISOMAP and is shared by most of the manifold methods that they can't give us a function f : R^D -> R^d . i.e. they can't give us a function by which we can map all D (generally total number of pixel in an image) dimension vector in small d dimension space. They just do this for particular set of dtaset vectors and it is irreversible. So once the manifold is made for a particular dataset their is no way to extend the same manifold to a dataset where some more images are added and also there is no reverse mapping so we can't come back from any random point on manifold to some image. Only mapping is from the dataset images.

Work can be done on this:
There can be many ways to evaluate distance between images. Euclidian distance used by old ISOMAP generally not work well. Consider the translation of an object in a image sequence. As the object translate pixels value changes abruptly so evaluating distance per corresponding pixel doesn't make sense. It will give us a large distance while it is just a translation every other thing in image is as it is.
So we must have a good distance metric. Here author have discussed 3 ways to evaluate distance which are already in literature they have just pointed out that we can use those distance.

They started with image sequence in which a female is running on a treadmill. The manifold they get is 2 dimension. Since running is repetitive process they are getting a circular path with some width. The width is because there are other motion like shifting of the body left or right.

The distance formula they have given is 

I_{ah}(p ) = I_a(p - hv( p)) + h sigma^2 z ( p) + N( p).

I_a is first image I_{ah} is next sequence image.

N( p) is noise it is best modeled with euclidian distance. 
Rigid motions/projection changes(global translation, rotation, scaling) best modeled with motion estimation or transformation matrix.
Non rigid motion is modeled via local motion estimator.
Intensity variation is modeled via local contrast change estimator.

In noise both image are exactly same so only difference is noise so euclidian works best.

Rigid motion distance: We can express the allowable warping of an image as I_b to A I_b where A \belongs to T all possible transformation for rigid motion.
the distance measure is the magnitude of transform that minimizes the image difference.

Non rigid motion: eg. motion of only hand or a part of the object. This is best done by collection of Gabor filters to estimate local motions. Gabors are applied to same part of image in bit the imgaes response are summed over all part of the images. the phase difference gives us the distance.Not clear to me.
Other distance measure for local motion is Hausdorff distance which works for segmented image like silhouette where image is just binary.

They have given two application example one is a bird flying in clear sky so background is fixed and there is rigid transformation + non rigid transformation. Rigid is bird is coming near so scaling zoom in effect. Non rigid is its' wing flapping.
the whole sequence lie on 2 D manifold due to cyclic nature again data lie on a circle with width. Radial direction of manifold gives us same pose of bird with zoom in effect. while along circle. bird is flapping so pose is changing.
Other example is biomedical imaging where an MR image is considered. 

  - nandan 6/10
},
}

@article{Robert Pless200983,
  title={A Survey of Manifold Learning for Images},
  author={Robert Pless and Richard Souvenir},
  journal={IPSJ Transactions on Computer Vision and Applications},
  volume={1},
  number={ },
  pages={83-94},
  year={2009}
  annotate={
Example image manifold. Varying intensity condition, Rotating object while taking pictures. Camera view angle can also be rotated, flying bird in sky other movement may be coming toward the camera etc. These action sequence are best modeled by manifold.
These manifold are drawn from image set which have just few degree of freedom(1 or 2 ). This is all we can do for now because for manifold we require data to be densely sampled and so dimensionality must be less otherwise no good can be achieved.
Dimensionality reduction in broad sense using manifold is of great interest but is not discussed here. Here they have discussed narrow reductionality where we reduce dimension for predefined set of data/images. F: I -> R^d
and not f: R^D -> R^d .
ISOMAP and LLE are two traditional method for this.
ISOMAP is simple:
first calculate distance between each pair of points from dataset.
Then we have two techniques either use k-nearest neighbor approach to join edges and make graphical manifold structure or use epsilon-ball approach i.e. join two points if distance is less than epsilon.
For each edge weight equals the distance.
Now solve all pair shoortest paths on this sparse graph to calculate a complete pair wise distance matrix.
Solve for low dimension embedding. MDS.

MDS(Multi dimensional scaling):
Eigenvalue problem to convert a matrix of pairwise distance into absolute coordinate of low dimensional space.
Given n x n matrix D such that D(i,j) is the distance between point i to j.
Define t= -HDH/2. where H(centric matrix)= I-ee'/n
where e=[1,1,É,1]'
. . .
. . 

LLE:
This algorithm attempts to represent manifold locally by reconstructing each input point as weighted combination of it's neighbor.

Define neighbor as with ISOMAP via nearest neighbor or via epsilon closer. Now solve the reconstruction weight W by reconstructing each X_i from it's neighbor W(i,j) will be the wight used for reconstructing i using j. If x_j is not in nearest neighbor then w(i,j) =0.
Now normalize each row i. Now define the embedding coordinate using this W matrix.
The traditional algorithm doesn't work well for many kind of data so their are many variants of above algorithm to help us out.
ST-Isomap for spatiao temporal data e.g. video.
Generally the final step of embedding the points in low dimension is same for all MDS.
But before this the construction of the weight or similarity matrix defer.
There are some real drawbacks of manifold due to natural image statistics. Natural image manifolds are really curved. Cyclic manifold make task tough. many manifold algorithm does provide a way to project and re-project new images onto manifold.
There is no way to find the embedded coordinate of new image nor is for to generate new images from manifold points.

there are many ways to evaluate the distance matrix for image dataset pairs used by manifold algorithm. Simple and widely used is euclidian but it is not fit for many images.Earth mover's distance or local deformation measure using gabber filter phase shift response. also image features are tried to calculate distance.
Some example application MRI imaging, Birds flying, fish swimming, tracking pose posture estimation , lips movement while speaking,
	- nandan 6/10},

}


@inproceedings{1015357,
 author = {Jenkins, Odest Chadwicke and Matari\'{c}, Maja J.},
 title = {A spatio-temporal extension to Isomap nonlinear dimension reduction},
 booktitle = {ICML '04: Proceedings of the twenty-first international conference on Machine learning},
 year = {2004},
 isbn = {1-58113-828-5},
 pages = {56},
 location = {Banff, Alberta, Canada},
 doi = {http://doi.acm.org/10.1145/1015330.1015357},
 publisher = {ACM},
 address = {New York, NY, USA},
 annotate={

A better way to evaluate the distance matrix by taking the spatio and temporal displacement into account.
The distance evaluation which takes the spatial displacement into account must also take these two things into account:
1. Proximal disambiguation: Nearby datapoints which are structurally different so they are not the same point displaced.
2. Distal correspondence: Datapoints which are far off but are same so are just displaced.
General Framework for ST-Isomap:

1. windowing the input data into temporal blocks. Segmentation also help in calculation since it reduces the nubbier of datapoint for which we have to do iteration. If N x N image is segmented in only N_s blocks so.


2. compute sparse distance matrix from local neighborhood about each window ( S_i )using euclidian distance.

3.Locally identify common temporal neighbors for each point S_i.
as local segmented common temporal neighbor or k-nearest nontrivial neighbors.

4. Reduce distances in D^l between points with common and adjacent temporal relationships

5. Now compute D^0 with full all-pairs shortest-path distance.

6 Use MDS.

Paper doesn't mention a method to calculate the temporal windowing. These window are non overlapping continuous they maintain temporal coherency.

Paper is too much unclear you can't implement the thing it just gives an overall idea exactly how we do this has to be done.
Although there was code written by authors for the examples described.
	-nandan 7/10 },


 }


@inproceedings{1097706,
 author = {Souvenir, Richard and Pless, Robert},
 title = {Manifold Clustering},
 booktitle = {ICCV '05: Proceedings of the Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
 year = {2005},
 isbn = {0-7695-2334-X-01},
 pages = {648--653},
 doi = {http://dx.doi.org/10.1109/ICCV.2005.149},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 annotate={

This paper has tried to apply manifold learning algorithm on unlabeled data which lie on multiple intersecting manifolds. If the work can be extended then it will be applicable for all set of data. While today's manifold learning application scope is very limited.

Work can be done on this:
Can we apply manifold learning on general sequence of images where multiple things are happenh=ing can we do some king of segmentation then separate out manifold of each object or can get something good for each object.

Traditional manifold algorithm fails when we have multiple intersecting manifold they give wrong manifold .
There is already some work done on mixed manifold. One is where they take commonalities in tangential space of different manifold.

the goal of this paper is disguising the different manifold in which data lie so we have to cluster the data based on the low manifold they lie on.
they assumed that the number of manifolds on which the complete dataset will lie on and dimensionality of each manifold is known.
So if {X_1,X_2, . . , X_n} are datapoint x_i \belongs R^D
We have to output the set of labels
{c_1, c_2, . . ,c_n} for each data point from x where c_i \belongs 1,2, . .,k where k is number of manifold in which data from X lie on.
For this an EM (expectation maximization) approach is given. First we calculate the all pair shortest path distance matrix as in ISOMAP as it is.
Then the two step EM algorithm is applied E-step assigning the datapoint to best fit manifold. E-step seeks to update the assignment of labels of each point by determining how well they fit.

M-step estimation of parameter for those models.
They have used a weight matrix which is updated on each EM -algorithm run.
A k x n matrix where w_{ci} is the probability of x_i  belonging to c. W is initialized randomly unless some domain specific data is given(In that case we know for sure where x_i will lie}.
For updating the weight in matrix we have E step and for using it we have Node-weighted MDS algorithm which embed the points by taking weights into account.
MDS algorithm:
Given N x N matrix D of distace
We define \tau = -HDH/2
Where H is called centering matrix.

Now we calculate eigenvalue of the matrix and let s_1,s_2, .  . be the sorted eigenvalues of \tau. Let v_1,v_2, . . . be corresponding eigenvectors. Now Y = ..  has row vectors which are coordinate of k-dimensional embedding.
this process finds the k-dimensional embedding which minimizes \sum_{i,j}(|Y_i - Y_j|_2^2-D(i,j))^2
while MDS seeks to minimize 
 \sum_{i,j}w_iw_j(|Y_i - Y_j|_2^2-D(i,j))^2
this can be done by changing the centering matrix.
H' = (I - ww^t)/\sum_{i=1}^nw_i
and then defining the correlation matrix \tau = -H' D H' /2

After each E-step we have to re-weight the points based on how well they fit on each of the k-manifolds.the idea is to comapare original geodesic distances between points with the distance implied by weighted embedding.

The EM steps finally converge to give us labels.

 -nandan 7/10};
 }



@INPROCEEDINGS{Elgammal04inferring3d,
    author = {Ahmed Elgammal and Chan-su Lee},
    title = {Inferring 3D Body Pose from Silhouettes using Activity Manifold Learning},
    booktitle = {In CVPR},
    year = {2004},
    pages = {681--688}
	annotate={
Given a visual input human pose silhouette the objective is to recover intrinsic body pose and camera view angle.
For this we first learn view based activity manifold and then also from this to 3 D body pose.
From the silhouettes it is easy to learn the manifold is easy for any tradition algorithm the main catch is when we have to project a new pose silhouette to manifold and then determine it's pose and 3D model. Also when we want inverse i.e. getting a body pose from the manifold point.
Author has chosen his own way to represent the silhouettes 
y(x) at each pixel value x such that y(x) = 0 on the contour y(x) > 0 inside contour and y(x) < 0 outside contour.
exactly
y(x) = sign * d_c(x) d_c is minimum distance from contour
sign is 0 1 and -1 depending upon x is on inside or outside the contour.
Now we do non linear embedding using LLE Isomap can also be used.
The dimension on which we project the input depends upon the viewpoint.
Learning mapping:
Input to manifold:
Given a visual input silhouette project it on manifold.
R^d to R^e.Learning such mapping is not feasible require large number of example images. Authors have mentioned GRBF(Generalized Radial basis function) for this if the mapping is constrained then this is achievable.
Let the set of input instances be
Y={y_i \belongs R^d i=1,2,. . ., N}
and let their corresponding points in embedding space be X={x_i \belongs R^e i=1,2,..,N}


  -nandan 7/10 },
}
