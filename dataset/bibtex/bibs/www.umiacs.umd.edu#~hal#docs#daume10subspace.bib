@InProceedings{daume10subspace,
  author =       {Piyush Rai and Hal {Daum\'e III}},
  title =        {Infinite Predictor Subspace Models for Multitask Learning},
  booktitle =    {Proceedings of the Conference on Artificial Intelligence and Statistics (AI-Stats)},
  year =         {2010},
  address =      {Sardinia, Italy},
  abstract =     {
    Given several related learning tasks, we propose
    a nonparametric Bayesian model that
    captures task relatedness by assuming that
    the task parameters (i.e., predictors) share
    a latent subspace. More specifically, the intrinsic
    dimensionality of the task subspace is
    not assumed to be known a priori. We use an
    infinite latent feature model to automatically
    infer this number (depending on and limited
    by only the number of tasks). Furthermore,
    our approach is applicable when the underlying
    task parameter subspace is inherently
    sparse, drawing parallels with l1 regularization
    and LASSO-style models. We also propose
    an augmented model which can make
    use of (labeled, and additionally unlabeled
    if available) inputs to assist learning this
    subspace, leading to further improvements
    in the performance. Experimental results
    demonstrate the efficacy of both the proposed
    approaches, especially when the number
    of examples per task is small. Finally, we
    discuss an extension of the proposed framework
    where a nonparametric mixture of linear
    subspaces can be used to learn a nonlin-
    ear manifold over the task parameters, and
    also deal with the issue of negative transfer
    from unrelated tasks.
  },
  keywords = {ml bayes da},
  tagline = {We present a non-parametric Bayesian model for multitask learning based on the assumption that task parameters live in a common, latent subspace.},
  url = {http://pub.hal3.name/#daume10subspace}
}
