@STRING{colt93	= "Proceedings of the Sixth Annual ACM Conference on
		  Computational Learning Theory" }
@STRING{jacm	= "Journal of the Association for Computing Machinery" }
@STRING{jma	= "Journal of Multivariate Analysis" }
@STRING{lncs	= "Lecture Notes of Computer Science, Springer" }
@STRING{lncs	= "Lecture Notes in Computer Science" }
@STRING{nips	= "Advances in Neural Information Processing Systems" }
@STRING{pecs	= "Colloquia Mathematica Societatis Janos Bolai, 57.\ Limit
		  Theorem in Probability and Statistics, Pecs (Hungary)" }
@STRING{spl	= "Statistics \& Probability Letters" }

@Article{A:Circle-Homogen,
  author	= {P. Auer},
  title		= {The circle homogeneously covered by random walk on
		  {Z}$^2$},
  journal	= spl,
  year		= 1990,
  pages		= {403--407},
  volume	= 9
}

@InProceedings{A:CombinationTheories,
  author	= {P. Auer},
  booktitle	= {Word Equations and Related Topics},
  pages		= {177--186},
  publisher	= {Lecture Notes of Computer Science 677, Springer},
  title		= {Unification in the Combination of Disjoint Theories},
  year		= {1991}
}

@InProceedings{A:Hitting-Prob,
  author	= {P. Auer},
  booktitle	= pecs,
  pages		= {9--25},
  title		= {Some Hitting Probabilities of Random Walks on {Z}$^2$},
  year		= 1989
}

@InProceedings{A:Relative-Frequ,
  author	= {P. Auer and P. R\'{e}v\'{e}sz},
  booktitle	= pecs,
  pages		= {27--33},
  title		= {On the Relative Frequency of Points Visited by random Walk
		  on {Z}$^2$},
  year		= 1989
}

@InProceedings{A:WordEquations,
  author	= {P. Auer},
  booktitle	= {Word Equations and Related Topics},
  key		= {A:WordEquations},
  pages		= {103--132},
  publisher	= {Lecture Notes of Computer Science 677, Springer},
  title		= {Solving String Equations with Constant Restrictions},
  year		= {1991}
}

@InProceedings{AC94,
  author	= {P. Auer and N. C{esa-Bianchi}},
  booktitle	= {Algorithmic Learning Theory, AII'94, ALT'94},
  editor	= {Setsuo Arikawa and Klaus P. Jantke},
  pages		= {229--247},
  publisher	= {Lecture Notes in Artificial Intelligence 872, Springer},
  title		= {On-line Learning with Malicious Noise and the Closure
		  Algorithm},
  year		= {1994}
}

@Article{AC94j,
  author	= {P. Auer and N. C{esa-Bianchi}},
  journal	= {Annals of Mathematics and Artificial Intelligence},
  title		= {On-line Learning with Malicious Noise and the Closure
		  Algorithm},
  year		= {1998},
  volume	= {23},
  pages		= {83--99},
  note		= {A preliminary version has appeared in {\em Lecture Notes
		  in Artificial Intelligence} 872, Springer}
}

@InProceedings{AC96,
  author	= {P. Auer and P. Caianiello and N. Cesa-Bianchi},
  booktitle	= {Proceedings of the 15th Annual ACM Symposium on Principles
		  of Distributed Computing},
  note		= {Abstract},
  pages		= {312},
  title		= {Tight Bounds on the Cumulative Profit of Distributed
		  Voters},
  year		= {1996}
}

@InProceedings{ACFS95,
  author	= {P. Auer and N. Cesa-Bianchi and Y. Freund and R. E.
		  Schapire},
  booktitle	= {Proceedings of the 36th Annual Symposium on Foundations of
		  Computer Science},
  pages		= {322--331},
  publisher	= {IEEE Computer Society Press, Los Alamitos, CA},
  title		= {Gambling in a Rigged Casino: The Adversarial Multi-Armed
		  Bandit Problem},
  year		= {1995}
}

@Article{AH94gL,
  author	= {P. Auer and K. Hornik},
  journal	= {Journal of Multivariate Analysis},
  pages		= {37--51},
  title		= {The Number of Points of an Empirical or {Poisson} Process
		  Covered by Unions of Sets},
  volume	= {57},
  year		= {1996}
}

@Article{AH94jma,
  author	= {P. Auer and K. Hornik},
  journal	= jma,
  number	= {1},
  pages		= {115--156},
  title		= {On the Number of Points of a Homogeneous {P}oisson
		  Process},
  volume	= {48},
  year		= {1994}
}

@Article{AH96,
  author	= {P. Auer and K. Hornik},
  journal	= {Studia Scientiarum Mathematicarum Hungarica},
  pages		= {1--13},
  title		= {Limit Laws for the Maximal and Minimal Increments of the
		  {P}oisson Process},
  volume	= {31},
  year		= {1996}
}

@Article{AHR91,
  author	= {P. Auer and K. Hornik and P. R\'{e}v\'{e}sz},
  journal	= spl,
  pages		= {91--96},
  title		= {Some Limit Theorems for the Homogeneous {P}oisson
		  Process},
  volume	= 12,
  year		= 1991
}

@InCollection{AHW95,
  author	= {P. Auer and M. Herbster and M. K. Warmuth},
  booktitle	= {Advances in Neural Information Processing Systems 8},
  editor	= {D. S. Touretzky and M. C. Mozer and M. E. Hasselmo},
  pages		= {316--322},
  publisher	= {MIT Press},
  title		= {Exponentially Many Local Minima for Single Neurons},
  year		= {1996}
}

@Article{AL93,
  author	= {P. Auer and P. M. Long},
  title		= {Structural Results About On-line Learning Models With and
		  Without Queries},
  journal	= {Machine Learning},
  year		= {1999},
  volume	= {36},
  pages		= {147--181},
  note		= {A preliminary version has appeared in {\em Proceedings of
		  the 26th ACM Symposium on the Theory of Computing}}
}

@InProceedings{AL94,
  author	= {P. Auer and P. M. Long},
  booktitle	= {Proceedings of the 26th Annual ACM Symposium on the Theory
		  of Computing},
  pages		= {263--272},
  publisher	= {ACM Press},
  title		= {Simulating access to hidden information while learning},
  year		= {1994}
}

@InProceedings{ALS97,
  author	= {P. Auer and P. M. Long and A. Srinivasan},
  booktitle	= {Proc.\ 29th Ann.\ Symp.\ Theory of Computing},
  month		= {May},
  pages		= {314--323},
  publisher	= {ACM},
  title		= {Approximating Hyper-Rectangles: Learning and Pseudo-random
		  Sets},
  year		= {1997}
}

@Article{ALS97j,
  author	= {P. Auer and P. M. Long and A. Srinivasan},
  title		= {Approximating Hyper-Rectangles: Learning and Pseudorandom
		  Sets},
  journal	= {Journal of Computer and System Sciences},
  year		= {1998},
  volume	= {57},
  number	= {3},
  pages		= {376--388},
  note		= {A preliminary version has appeared in {\em Proc.\ 29th
		  Ann.\ Symp.\ Theory of Computing}}
}

@InProceedings{AW95,
  author	= {P. Auer and M. K. Warmuth},
  booktitle	= {Proceedings of the 36th Annual Symposium on Foundations of
		  Computer Science},
  pages		= {312--321},
  publisher	= {IEEE Computer Society Press},
  title		= {Tracking the Best Disjunction},
  year		= {1995}
}

@Article{AW95j,
  author	= {P. Auer and M. K. Warmuth},
  title		= {Tracking the Best Disjunction},
  journal	= {Machine Learning},
  year		= {1998},
  volume	= {32},
  pages		= {127--150},
  note		= {A preliminary version has appeared in {\em Proceedings of
		  the 36th Annual Symposium on Foundations of Computer
		  Science}}
}

@Article{AlonMaass:86,
  author	= {N. Alon and W. Maass},
  journal	= {Proceedings of the 27th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {410--417},
  title		= {Meanders, Ramsey's theorem and lower bounds for branching
		  programs},
  year		= {1986}
}

@Article{AlonMaass:88,
  author	= {N. Alon and W. Maass},
  journal	= {J. Comput. System Sci.},
  note		= {Invited paper for a special issue of J. Comput. System
		  Sci.},
  pages		= {118--129},
  title		= {Meanders and their applications in lower bound arguments},
  volume	= {37},
  year		= {1988}
}

@InProceedings{Aue93,
  author	= {P. Auer},
  booktitle	= {Proceedings of the Sixth Annual ACM Conference on
		  Computational Learning Theory},
  pages		= {253--261},
  publisher	= {ACM Press, New York, NY},
  title		= {On-line Learning of Rectangles in Noisy Environments},
  year		= {1993}
}

@InProceedings{Aue95,
  author	= {P. Auer},
  booktitle	= {6th International Workshop, ALT`95, Proceedings},
  editor	= {Klaus P. Jantke and Takeshi Shinohara and Thomas
		  Zeugmann},
  note		= {LNAI 997},
  pages		= {123--137},
  publisher	= {Springer},
  title		= {Learning Nested Differences in the Presence of Malicious
		  Noise},
  year		= {1995}
}

@Article{Aue95j,
  author	= {P. Auer},
  title		= {Learning Nested Differences in the Presence of Malicious
		  Noise},
  journal	= {Theoretical Computer Science},
  year		= {1997},
  volume	= {185},
  pages		= {159--175},
  note		= {A preliminary version has appeared in {\em Proceedings of
		  the 6th International Workshop on Algorithmic Learning
		  Theory, ALT`95.}}
}

@InProceedings{Aue96,
  author	= {P. Auer},
  booktitle	= {Proc.\ 14th Int.\ Conf.\ Machine Learning},
  editor	= {D. H. Fisher},
  pages		= {21--29},
  publisher	= {Morgan Kaufmann},
  title		= {On Learning from Multi-Instance Examples: Empirical
		  Evaluation of a Theoretical Approach},
  year		= {1997}
}

@InProceedings{AuerETAL:01,
  author	= {P. Auer and H. Burgsteiner and W. Maass},
  title		= {Reducing Communication for Distributed Learning in Neural
		  Networks},
  booktitle	= {<a
		  href="http://www.springer.de/comp/lncs/index.html">Proc. of
		  the International Conference on Artificial Neural Networks
		  -- ICANN 2002</a>},
  series	= {Lecture Notes in Computer Science},
  editor	= {Jos\'{e} R. Dorronsoro},
  pages		= {123--128},
  volume	= {2415},
  year		= {2002},
  publisher	= {Springer},
  keywords	= {learning algorithm, perceptrons, parallel, aVLSI },
  abstract	= {A learning algorithm is presented for circuits consisting
		  of a single layer of perceptrons. We refer to such circuits
		  as parallel perceptrons. In spite of their simplicity,
		  these circuits are universal approximators for arbitrary
		  boolean and continuous functions. In contrast to backprop
		  for multi-layer perceptrons, our new learning algorithm -
		  the parallel delta rule (p-delta rule) - only has to tune a
		  single layer of weights, and it does not require the
		  computation and communication of analog values with high
		  precision. This distinguishes our new learning rule also
		  from other learning rules for such circuits such as
		  MADALINE with far higher communication. Our algorithm also
		  provides an interesting new hypothesis for the organization
		  of learning in biological neural systems. A theoretical
		  analysis shows that the p-delta rule does in fact implement
		  gradient descent - with regard to a suitable error measure
		  - although it does not require to compute derivatives.
		  Furthermore it is shown through experiments on common
		  real-world benchmark datasets that its performance is
		  competitive with that of other learning approaches from
		  neural networks and machine learning.}
}

@Article{AuerETAL:01a,
  author	= {P. Auer and H. Burgsteiner and W. Maass},
  title		= {A learning rule for very simple universal approximators
		  consisting of a single layer of perceptrons},
  journal	= {Neural Networks},
  year		= {2008},
  volume	= {21},
  number	= {5},
  pages		= {786--795},
  abstract	= {A learning algorithm is presented for circuits consisting
		  of a single layer of perceptrons (= threshold gates, or
		  equivalently gates with a Heaviside activation function).
		  We refer to such circuits as parallel perceptrons. In spite
		  of their simplicity, these circuits can compute any boolean
		  function if one views the majority of the binary perceptron
		  outputs as the binary outputs of the parallel perceptron,
		  and they are universal approximators for arbitrary
		  continuous functions with values in [0,1] if one views the
		  fraction of perceptrons that output 1 as the analog output
		  of the parallel perceptron. For a long time one has thought
		  that there exists no competitive learning algorithms for
		  these extremely simple circuits consisting of gates with
		  binary outputs, which also became known as committee
		  machines. It is commonly believed that one has to replace
		  the hard threshold gates by sigmoidal gates and that one
		  has to tune the weights on at least two successive layers
		  in order to get satisfactory learning results. We show that
		  this is not true by exhibiting a simple learning algorithm
		  for parallel perceptrons - the parallel delta rule (p-delta
		  rule), whose performance is comparable to that of backprop
		  for multilayer networks consisting of sigmoidal gates. In
		  contrast to backprop, the p-delta rule does not require the
		  computation and communication of analog values with high
		  precision, although it does in fact implement gradient
		  descent - with regard to a suitable error measure.
		  Therefore it provides an interesting new hypothesis for the
		  organization of learning in biological neural systems.}
}

@InProceedings{AuerETAL:93,
  author	= {P. Auer and P. M. Long and W. Maass and G. J. Woeginger},
  booktitle	= {Proceedings of the 5th Annual ACM Conference on
		  Computational Learning Theory},
  pages		= {392--401},
  title		= {On the complexity of function learning},
  year		= {1993}
}

@Article{AuerETAL:93j,
  author	= {P. Auer and P. M. Long and W. Maass and G. J. Woeginger},
  title		= {On the complexity of function learning},
  journal	= {Machine Learning},
  note		= {Invited paper in a special issue of Machine Learning},
  year		= {1995},
  volume	= {18},
  pages		= {187--230}
}

@InProceedings{AuerETAL:95,
  author	= {P. Auer and R. C. Holte and W. Maass},
  booktitle	= {Proc. of the 12th International Machine Learning
		  Conference, Tahoe City (USA)},
  publisher	= {Morgan Kaufmann (San Francisco)},
  pages		= {21--29},
  title		= {Theory and applications of agnostic {PAC}-learning with
		  small decision trees},
  year		= {1995}
}

@InProceedings{AuerETAL:96,
  author	= {P. Auer and S. Kwek and W. Maass and M. K. Warmuth},
  booktitle	= {Proc. of the 9th Conference on Computational Learning
		  Theory 1996},
  pages		= {333--343},
  publisher	= {ACM-Press (New York)},
  title		= {Learning of depth two neural nets with constant fan-in at
		  the hidden nodes},
  year		= {1996}
}

@Article{AuerMaass:98,
  author	= {P. Auer and W. Maass},
  title		= {Introduction to the Special Issue on Computational
		  Learning Theory},
  journal	= {Algorithmica},
  year		= {1998},
  volume	= {22},
  number	= {1/2},
  pages		= {1--2}
}

@MastersThesis{Bachler:05,
  author	= {M. Bachler},
  title		= {Task dependent feature optimization in machine learning},
  school	= {Technische Universitaet Graz},
  year		= 2005
}

@TechReport{Bachler:06,
  author	= {M. Bachler},
  title		= {Bayesian and information theoretic methods for the
		  selection and creation of high-level features applied to
		  real-world object classification tasks},
  institution	= {Technische Universitaet Graz},
  year		= {2006}
}

@Unpublished{BachlerMaass:06,
  author	= {M. Bachler and W. Maass},
  title		= {A {B}ayesian {H}ebb Rule for Incremental Learning of
		  Optimal Inference in {B}ayesian Networks},
  note		= {submitted for publication},
  year		= {2006}
}

@InCollection{BartlettMaass:03,
  author	= {Peter L. Bartlett and W. Maass},
  title		= {Vapnik-{C}hervonenkis Dimension of Neural Nets},
  booktitle	= {The Handbook of Brain Theory and Neural Networks},
  publisher	= {MIT Press (Cambridge)},
  year		= {2003},
  editor	= {M. A. Arbib},
  edition	= {2nd},
  pages		= {1188--1192}
}

@Article{BertschingerNatschlaeger:03,
  author	= {N. Bertschinger and T. Natschlaeger},
  title		= {Real-Time Computation at the Edge of Chaos in Recurrent
		  Neural Networks},
  journal	= {Neural Computation},
  year		= 2004,
  volume	= 16,
  number	= 7,
  pages		= {1413--1436},
  urlwillbe	= {psfiles/eoc-v11pl1.ps},
  abstract	= {Depending on the connectivity recurrent networks of simple
		  computational units can show very different types of
		  dynamics ranging from totally ordered to chaotic. We
		  analyze how the type of dynamics (ordered or chaotic)
		  exhibited by randomly connected networks of threshold gates
		  driven by a time varying input signal depends on the
		  parameters describing the distribution of the connectivity
		  matrix. In particular we calculate the critical boundary in
		  parameter space where the transition from ordered to
		  chaotic dynamics takes places. Employing a recently
		  developed framework for analyzing real-time computations we
		  show that only near the critical boundary such networks can
		  perform complex computations on time series. Hence, this
		  result strongly supports conjectures that dynamical systems
		  which are capable of doing complex computational tasks
		  should operate near the edge of chaos, i.e. the transition
		  from ordered to chaotic dynamics.}
}

@MastersThesis{Bill:08,
  author	= {J. Bill},
  title		= {Self-Stabilizing Network Architectures on a Neuromorphic
		  Hardware System},
  school	= {Universitaet Heidelberg},
  year		= {2008}
}

@Article{BillETAL:10,
  author	= {J. Bill and K. Schuch and D. Br\"uderle and J. Schemmel
		  and W. Maass and K. Meier},
  title		= {Compensating inhomogeneities of neuromorphic {VLSI}
		  devices via short-term synaptic plasticity},
  journal	= {Frontiers in Computational Neuroscience},
  year		= {2010},
  volume	= {4},
  number	= {},
  pages		= {1--14},
  abstract	= {Recent developments in neuromorphic hardware engineering
		  make mixed-signal VLSI neural network models promising
		  candidates for neuroscientific research tools and massively
		  parallel computing devices, especially for tasks which
		  exhaust the computing power of software simulations. Still,
		  like all analog hardware systems, neuromorphic models
		  suffer from a constricted configurability and
		  production-related fluctuations of device characteristics.
		  Since also future systems, involving ever-smaller
		  structures, will inevitably exhibit such inhomogeneities on
		  the unit level, self-regulation properties become a crucial
		  requirement for their successful operation. By applying a
		  cortically inspired self-adjusting network architecture, we
		  show that the activity of generic spiking neural networks
		  emulated on a neuromorphic hardware system can be kept
		  within a biologically realistic firing regime and gain a
		  remarkable robustness against transistorlevel variations.
		  As a first approach of this kind in engineering practice,
		  the short-term synaptic depression and facilitation
		  mechanisms implemented within an analog VLSI model of I\&F
		  neurons are functionally utilized for the purpose of
		  network level stabilization. We present experimental data
		  acquired both from the hardware model and from comparative
		  software simulations which prove the applicability of the
		  employed paradigm to neuromorphic VLSI devices.},
  note		= {article 129}
}

@Article{BorgwardtETAL:06,
  author	= {K. M. Borgwardt and A. Gretton and M. J. Rasch and H.-P.
		  Kriegel and B. Schoelkopf and A. J. Smola},
  journal	= {Bioinformatics},
  title		= {Integrating structured biological data by kernel {M}aximum
		  {M}ean {D}iscrepancy},
  year		= {2006},
  volume	= {22},
  number	= {14},
  pages		= {349--e57}
}

@Article{BretteETAL:07,
  author	= {R. Brette and M. Rudolph and T. Carnevale and M. Hines and
		  D. Beeman and J. M. Bower and M. Diesmann and A. Morrison
		  and P. H. Goodman and F. C. Harris and M. Zirpe and T.
		  Natschlaeger and D. Pecevski and B. Ermentrout and M.
		  Djurfeldt and A. Lansner and O. Rochel and T. Vieville and
		  E. Muller and A. P. Davison and S. ElBoustani and A.
		  Destexhe},
  title		= {Simulation of networks of spiking neurons: A review of
		  tools and strategies},
  journal	= {J. of Computational Neuroscience},
  year		= {2007},
  volume	= {23},
  number	= {3},
  pages		= {349--398},
  note		= {},
  abstract	= {We review different aspects of the simulation of spiking
		  neural networks. We start by reviewing the different types
		  of simulation strategies and algorithms that are currently
		  implemented. We next review the precision of those
		  simulation strategies, in particular in cases where
		  plasticity depends on the exact timing of the spikes. We
		  overview different simulators and simulation environments
		  presently available (restricted to those freely available,
		  open source and documented). For each simulation tool, its
		  advantages and pitfalls are reviewed, with an aim to allow
		  the reader to identify which simulator is appropriate for a
		  given task. Finally, we provide a series of benchmark
		  simulations of different types of networks of spiking
		  neurons, including Hodgkin-Huxley type, integrate-and-fire
		  models, interacting with current-based or conductance-based
		  synapses, using clock-driven or event-driven integration
		  strategies. The same set of models are implemented on the
		  different simulators, and the codes are made available. The
		  ultimate goal of this review is to provide a resource to
		  facilitate identifying the appropriate integration strategy
		  and simulation tool to use for a given modeling problem
		  related to spiking neural networks.}
}

@Article{BuesingETAL:09,
  author	= {L. Buesing and B. Schrauwen and R. Legenstein},
  title		= {Connectivity, Dynamics, and Memory in Reservoir Computing
		  with Binary and Analog Neurons},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  number	= {5},
  pages		= {1272-1311},
  abstract	= {Reservoir Computing (RC) systems are powerful models for
		  online computations on input sequences. They consist of a
		  memoryless readout neuron which is trained on top of a
		  randomly connected recurrent neural network. RC systems are
		  commonly used in two flavors: with analog or binary
		  (spiking) neurons in the recurrent circuits. Previous work
		  indicated a fundamental difference in the behavior of these
		  two implementations of the RC idea. The performance of a RC
		  system built from binary neurons seems to depend strongly
		  on the network connectivity structure. In networks of
		  analog neurons such clear dependency has not been observed.
		  In this article we address this apparent dichotomy by
		  investigating the influence of the network connectivity
		  (parametrized by the neuron in-degree) on a family of
		  network models that interpolates between analog and binary
		  networks. Our analyses are based on a novel estimation of
		  the Lyapunov exponent of the network dynamics with the help
		  of branching process theory, rank measures which estimate
		  the kernel-quality and generalization capabilities of
		  recurrent networks, and a novel mean-field predictor for
		  computational performance. These analyses reveal that the
		  phase transition between ordered and chaotic network
		  behavior of binary circuits qualitatively differs from the
		  one in analog circuits, leading to differences in the
		  integration of information over short and long time scales.
		  This explains the decreased computational performance
		  observed in binary circuits that are densely connected. The
		  mean-field predictor is also used to bound the memory
		  function of recurrent circuits of binary neurons.}
}

@Article{BuesingETAL:11,
  author	= {L. B\"using and J. Bill and B. Nessler and W. Maass},
  title		= {Neural Dynamics as Sampling: A Model for Stochastic
		  Computation in Recurrent Networks of Spiking Neurons},
  journal	= {PLoS Computational Biology},
  year		= {2011},
  pages		= {},
  volume	= {},
  abstract	= {The organization of computations in networks of spiking
		  neurons in the brain is still largely unknown, in
		  particular in view of the inherently stochastic features of
		  their firing activity and the experimentally observed
		  trial-to-trial variability of neural systems in the brain.
		  In principle there exists a powerful computational
		  framework for stochastic computations, probabilistic
		  inference by sampling, which can explain a large number of
		  macroscopic experimental data in neuroscience and cognitive
		  science. But it has turned out to be surprisingly difficult
		  to create a link between these abstract models for
		  stochastic computations and more detailed models of the
		  dynamics of networks of spiking neurons. Here we create
		  such a link, and show that under some conditions the
		  stochastic firing activity of networks of spiking neurons
		  can be interpreted as probabilistic inference via Markov
		  chain Monte Carlo (MCMC) sampling. Since common methods for
		  MCMC sampling in distributed systems, such as Gibbs
		  sampling, are inconsistent with the dynamics of spiking
		  neurons, we introduce a different approach based on
		  non-reversible Markov chains, that is able to reflect
		  inherent temporal processes of spiking neuronal activity
		  through a suitable choice of random variables. We propose a
		  neural network model and show by a rigorous theoretical
		  analysis that its neural activity implements MCMC sampling
		  of a given distribution, both for the case of discrete and
		  continuous time. This provides a step towards closing the
		  gap between abstract functional models of cortical
		  computations and more detailed models of networks of
		  spiking neurons.},
  note		= {in press}
}

@InProceedings{BuesingMaass:07,
  author	= {L. Buesing and W. Maass},
  title		= {Simplified Rules and Theoretical Analysis for Information
		  Bottleneck Optimization and {PCA} with Spiking Neurons},
  booktitle	= {Proc. of NIPS 2007, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2008},
  volume	= {20},
  pages		= {},
  abstract	= {We show that under suitable assumptions (primarily
		  linearization) a simple and perspicuous online learning
		  rule for Information Bottleneck optimization with spiking
		  neurons can be derived. This rule performs on common
		  benchmark tasks as well as a rather complex rule that has
		  previously been proposed [2]. Furthermore, the transparency
		  of this new learning rule makes a theoretical analysis of
		  its convergence properties feasible. If this learning rule
		  is applied to an assemble of neurons, it provides a
		  theoretically founded method for performing principal
		  component analysis ({PCA}) with spiking neurons. In
		  addition it makes it possible to preferentially extract
		  those principal components from incoming signals X that are
		  related to some additional target signal $Y_T$ . This
		  target signal $Y_T$ (also called relevance variable) could
		  represent in a biological interpretation proprioception
		  feedback, input from other sensory modalities, or top-down
		  signals.}
}

@Article{BuesingMaass:09,
  author	= {Buesing, L. and Maass, W.},
  title		= {A Spiking Neuron as Information Bottleneck},
  journal	= {submitted for publication},
  year		= {2009}
}

@Article{BuesingMaass:10,
  author	= {L. Buesing and W. Maass},
  title		= {A Spiking Neuron as Information Bottleneck},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  number	= {},
  pages		= {1961--1992},
  abstract	= {Neurons receive thousands of presynaptic input spike
		  trains while emitting a single output spike train. This
		  drastic dimensionality reduction suggests to consider a
		  neuron as a bottleneck for information transmission.
		  Extending recent results, we propose a simple learning rule
		  for the weights of spiking neurons derived from the
		  Information Bottleneck (IB) framework that minimizes the
		  loss of relevant information transmitted in the output
		  spike train. In the IB framework relevance of information
		  is defined with respect to contextual information, the
		  latter entering the proposed learning rule as a "third"
		  factor besides pre- and postsynaptic activities. This
		  renders the theoretically motivated learning rule a
		  plausible model for experimentally observed synaptic
		  plasticity phenomena involving three factors. Furthermore,
		  we show that the proposed IB learning rule allows spiking
		  neurons to learn a "predictive code",i.e. to extract those
		  parts of their input that are predictive for future
		  input.}
}

@InProceedings{BultmanMaass:91,
  author	= {W. J. Bultman and W. Maass},
  booktitle	= {Proceedings of the 4th Annual ACM Workshop on
		  Computational Learning Theory,},
  pages		= {337--353},
  title		= {Fast identification of geometric objects with membership
		  queries},
  year		= {1991}
}

@Article{BultmanMaass:91j,
  author	= {W. J. Bultman and W. Maass},
  title		= {Fast identification of geometric objects with membership
		  queries},
  journal	= {Information and Computation},
  year		= 1995,
  volume	= 118,
  pages		= {48--64}
}

@Article{BuonomanoMaass:08,
  author	= {D. Buonomano and W. Maass},
  title		= {State-dependent Computations: Spatiotemporal Processing in
		  Cortical Networks.},
  journal	= {Nature Reviews in Neuroscience},
  year		= {2009},
  volume	= {10},
  number	= {2},
  pages		= {113--125},
  note		= {},
  abstract	= {A conspicuous ability of the brain is to seamlessly
		  assimilate and process spatial and temporal features of
		  sensory stimuli. This ability is indispensable for the
		  recognition of natural stimuli. Yet, a general
		  computational framework for processing spatiotemporal
		  stimuli remains elusive. Recent theoretical and
		  experimental work suggests that spatiotemporal processing
		  emerges from the interaction between incoming stimuli and
		  the internal dynamic state of neural networks which
		  includes not only ongoing spiking activity, but also
		  'hidden' neuronal states such as short-term synaptic
		  plasticity.}
}

@MastersThesis{Burgsteiner:98,
  author	= {H. Burgsteiner},
  title		= {{N}eural {N}etworks with {S}piking {N}eurons},
  school	= {Graz University of Technology},
  year		= 1998,
  month		= {November},
  keywords	= {spiking neurons, dynamic synapses, pools of dynamic
		  synapses, aVLSI, hardware model, simulations},
  abstract	= {In this diploma thesis I investigate new models of
		  synapses called Dynamic Synapses (DS) and a possible
		  implementation in analog electronics which could be
		  realized in an analog very large scale integrated silicon
		  chip (aVLSI). These mathematical models of DS were first
		  introduced by Wolfgang Maass and Anthony Zador in 1998,
		  after recent results in neurobiology suggested that
		  synapses -- in their computational meanings -- could be
		  more than just static ``weights''. It soon became obvious
		  that this model exhibits a behavior very close to their
		  biological counterparts. For a better understanding of the
		  later presented material a few essential models of neurons
		  and also the typical use of synapses are discussed in the
		  beginning. The pages following after the introduction are
		  dedicated to the original model of the DS. Then a newer and
		  slightly different model of a dynamic synapse is introduced
		  which is easier to implement in aVLSI. I present results of
		  simulations that reveal the computational power in
		  comparison to the original model. At the end a possible
		  implementation in analog electronics and results of
		  simulations of such implementations are shown. In future
		  work a small neural network with these types of synapses
		  could be realized in aVLSI and applied in real world
		  studies.}
}

@TechReport{Burgsteiner:99,
  author	= {H. Burgsteiner},
  title		= {Integration of XVision and Khepera},
  institution	= {Institute for Theoretical Computer Science, Graz
		  University of Technology},
  year		= 1999,
  abstract	= {This paper was written as a part of a documentation for a
		  project at our institute, combining machine learning and
		  machine vision. I present an approach of integrating the
		  mobile robot platform Khepera with the machine vision
		  package XVision. Both products are introduced in brief. The
		  neccessary modifications and extensions to a typical
		  XVision source file are described for use under the free
		  operating system Linux. As an example, a simple tracker
		  based on XVision and the Khepera equipped with a camera is
		  presented. It uses blob detection and basic Khepera control
		  functions. Khepera tracks a ball and is able to rotate
		  around its z-axis to keep the ball centered in its view.},
  keywords	= {xvision, khepera, programming, robotic, vision}
}

@Article{BurgsteinerETAL:07,
  author	= {H. Burgsteiner and M. Kr\"{o}ll and A. Leopold and G.
		  Steinbauer},
  title		= {Movement Prediction from real-world Images using a Liquid
		  State Machine},
  journal	= {Applied Intelligence},
  volume	= {26},
  number	= {2},
  pages		= {99--109},
  year		= {2007}
}

@InProceedings{ChenMaass:92,
  author	= {Z. Chen and W. Maass},
  booktitle	= {Proceedings of the 5th Annual ACM Workshop on
		  Computational Learning Theory},
  pages		= {16--28},
  title		= {On-line learning of rectangles},
  year		= {1992}
}

@InProceedings{ChenMaass:92a,
  author	= {Z. Chen and W. Maass},
  booktitle	= {Proceedings of the 3rd Int. Workshop on Analogical and
		  Inductive Inference},
  pages		= {26--34},
  publisher	= {Springer},
  series	= {Lecture Notes in Artificial Intelligence},
  title		= {A solution of the credit assignment problem in the case of
		  learning rectangles},
  volume	= {642},
  year		= {1992}
}

@Article{ChenMaass:94,
  author	= {Z. Chen and W. Maass},
  journal	= {Machine Learning},
  note		= {Invited paper for a special issue of Machine Learning},
  pages		= {201--223},
  title		= {On-line learning of rectangles and unions of rectangles},
  volume	= {17},
  year		= {1994}
}

@Article{ClopathETAL:08,
  author	= {C. Clopath and L. Ziegler and E. Vasilaki and L. Buesing
		  and W. Gerstner},
  title		= {Tag-Trigger-Consolidation: A Model of Early and Late
		  Long-Term-Potentiation and Depression},
  journal	= {PLOS Computational Biology},
  year		= {2008},
  volume	= {4},
  number	= {12},
  pages		= {e1000248},
  note		= {},
  abstract	= {Changes in synaptic efficacies need to be long-lasting in
		  order to serve as a substrate for memory.
		  Experimentally,synaptic plasticity exhibits phases covering
		  the induction of long-term potentiation and depression
		  (LTP/LTD) during the early phase of synaptic plasticity,
		  the setting of synaptic tags, a trigger process for protein
		  synthesis, and a slow transition leading to synaptic
		  consolidation during the late phase of synaptic plasticity.
		  We present a mathematical model that describes these
		  different phases of synaptic plasticity. The model explains
		  a large body of experimental data on synaptic tagging and
		  capture, cross-tagging, and the late phases of LTP and LTD.
		  Moreover, the model accounts for the dependence of LTP and
		  LTD induction on voltage and presynaptic stimulation
		  frequency. The stabilization of potentiated synapses during
		  the transition from early to late LTP occurs by protein
		  synthesis dynamics that are shared by groups of synapses.
		  The functional consequence of this shared process is that
		  previously stabilized patterns of strong or weak synapses
		  onto the same postsynaptic neuron are well protected
		  against later changes induced by LTP/LTD protocols at
		  individual synapses. }
}

@Article{DietzfelbingerETAL:91a,
  author	= {M. Dietzfelbinger and W. Maass and G. Schnitger},
  journal	= {Theoretical Computer Science},
  pages		= {113--129},
  title		= {The complexity of matrix transposition on one-tape
		  off-line {T}uring machines},
  volume	= {82},
  year		= {1991}
}

@InProceedings{DietzfelbingerMaass:85,
  author	= {M. Dietzfelbinger and W. Maass},
  booktitle	= {Proceedings of the 1984 Recursion Theory Week Oberwolfach,
		  Germany},
  pages		= {89--120},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Mathematics},
  title		= {Strong reducibilities in alpha- and beta-recursion
		  theory},
  volume	= {1141},
  year		= {1985}
}

@InProceedings{DietzfelbingerMaass:86,
  author	= {M. Dietzfelbinger and W. Maass},
  booktitle	= {Proceedings of the Structure in Complexity Theory
		  Conference, Berkeley 1986},
  pages		= {163--183},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {Two lower bound arguments with ``inaccessible'' numbers},
  volume	= {223},
  year		= {1986}
}

@Article{DietzfelbingerMaass:88,
  author	= {M. Dietzfelbinger and W. Maass},
  journal	= {J. Comput. System Sci.},
  note		= {Invited paper for a special issue of J. Comput. System
		  Sci.},
  pages		= {313--335},
  title		= {Lower bound arguments with ``inaccesible'' numbers},
  volume	= {36},
  year		= {1988}
}

@InProceedings{DietzfelbingerMaass:88a,
  author	= {M. Dietzfelbinger and W. Maass},
  booktitle	= {Proceedings of the 15th International Colloquium on
		  Automata, Languages and Programming},
  pages		= {188--200},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {The complexity of matrix transposition on one-tape
		  off-line {T}uring machines with output tape},
  volume	= {317},
  year		= {1988}
}

@Article{DietzfelbingerMaass:93,
  author	= {M. Dietzfelbinger and W. Maass},
  journal	= {Theoretical Computer Science},
  pages		= {271--290},
  title		= {The complexity of matrix transposition on one-tape
		  off-line {T}uring machines with output tape},
  volume	= {108},
  year		= {1993}
}

@Article{DobkinETAL:96,
  author	= {D. P. Dobkin and D. Gunopulos and W. Maass},
  journal	= {Journal of Computer and System Sciences},
  month		= {June},
  number	= {3},
  pages		= {453--470},
  title		= {Computing the maximum bichromatic discrepancy, with
		  applications to computer graphics and machine learning},
  volume	= {52},
  year		= {1996}
}

@InProceedings{FregnacETAL:05,
  author	= {Y. Fregnac and M. Blatow and J.-P. Changeux and J. de
		  Felipe and A. Lansner and W. Maass and D. A. McCormick and
		  C. M. Michel and H. Monyer and E. Szathmary and R. Yuste},
  title		= {U{P}s and {DOWN}s in Cortical Computation},
  booktitle	= {The Interface between Neurons and Global Brain Function},
  editor	= {S. Grillner and A. M. Graybiel},
  year		= {2006},
  pages		= {393--433},
  chapter	= {19},
  publisher	= {MIT Press},
  series	= {Dahlem Workshop Report 93}
}

@Article{GrettonETAL:05,
  author	= {A. Gretton and R. Herbrich and A. Smola and O. Bousquet
		  and B. Sch\"olkopf},
  title		= {Kernel Methods for Measuring Independence},
  journal	= {Journal of Machine Learning Research},
  year		= {2005},
  pages		= {2075--2129},
  volume	= {6}
}

@Article{GrettonETAL:06,
  author	= {A. Gretton and K. M. Borgwardt and M. J. Rasch and A. J.
		  Smola},
  title		= {Data-content based schema matching via {M}aximum {M}ean
		  {D}iscrepancy},
  journal	= {submitted for publication},
  year		= {2006}
}

@InProceedings{GrettonETAL:06b,
  author	= {A. Gretton and K. M. Borgwardt and M. J. Rasch and B.
		  Sch\"olkopf and A. J. Smola},
  title		= {A Kernel Method for the Two-Sample-Problem},
  booktitle	= {Proceedings of the 2006 Conference Advances in Neural
		  Information Processing Systems 19},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2006},
  volume	= {20},
  pages		= {513--520}
}

@InProceedings{GrettonETAL:06c,
  author	= {A. Gretton and K. M. Borgwardt and M. J. Rasch and B.
		  Sch\"olkopf and A. J. Smola},
  title		= {A Kernel Method to Comparing Distributions},
  booktitle	= {Proceedings of the Twenty-Second AAAI Conference on
		  Artificial Intelligence (AAAI-07)},
  editor	= {},
  publisher	= {AAAI Press, Menlo Park, CA, USA},
  year		= {2007},
  volume	= {22},
  pages		= {1637--1641}
}

@Article{HSWA,
  author	= {K. Hornik and M. Stinchcombe and H. White and P. Auer},
  journal	= {Neural Computation},
  pages		= {1262--1275},
  title		= {Degree of approximation results for feedforward networks
		  approximating unknown mappings and their derivatives},
  volume	= {6},
  year		= {1994}
}

@Article{HaeuslerETAL:03,
  author	= {S. Haeusler and H. Markram and W. Maass},
  title		= {Perspectives of the High Dimensional Dynamics of Neural
		  Microcircuits from the Point of View of Low-Dimensional
		  Readouts},
  journal	= {Complexity (Special Issue on Complex Adaptive Systems)},
  year		= {2003},
  volume	= {8},
  number	= {4},
  pages		= {39--50}
}

@Article{HaeuslerETAL:07,
  author	= {S. Haeusler and W. Singer and W. Maass and D. Nikolic},
  title		= {Superposition of information in large ensembles of neurons
		  in primary visual cortex},
  journal	= {37th Annual Conference of the Society for Neuroscience,
		  Program 176.2, Poster II23},
  year		= {2007},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {We applied methods from machine learning in order to
		  analyze the temporal evolution of stimulus-related
		  information in the spiking activity of large ensembles of
		  around 100 neurons in primary visual cortex of anesthetized
		  cats. We present ed sequences of up to 3 different visual
		  stimuli (letters) that lasted 100 ms and followed at
		  intervals of 100 ms. We f ound that most of the information
		  about visual stimuli extractable by advanced methods from
		  machine learning (e.g., Sup port Vector Machines) could
		  also be extracted by simple linear classifiers
		  (perceptrons). Hence, in principle this info rmation can be
		  extracted by a biological neuron. A surprising result was
		  that new stimuli did not erase information abo ut previous
		  stimuli. In fact, information about the nature of the
		  preceding stimulus remained as high as the informatio n
		  about the current stimulus. Separately trained linear
		  readouts could retrieve information about both the current
		  and the preceding stimulus from responses to the current
		  stimulus. This information was encoded both in the
		  discharge rates (response amplitudes) of the ensemble of
		  neurons and in the precise timing of individual spikes, and
		  persisted for seve ral 100 ms beyond the offset of stimuli.
		  This superposition of information about sequentially
		  presented stimuli constrains computational models for
		  visual proce ssing. It poses a conundrum for models that
		  assume separate classification processes for each frame of
		  visual input and supports models for cortical computation
		  ([Buonomano, Merzenich, 1995], [Maass, Natschlaeger,
		  Markram, 2002]) which arg ue that a frame-by frame
		  processing is neither feasible within highly recurrent
		  networks nor useful for classifying and predicting rapidly
		  changing stimulus sequences. Specific predictions of these
		  alternative computational models are tha i) information
		  from different frames of visual input is superimposed in
		  recurrent circuits and ii) nonlinear combinations of
		  different information components are immediately provided
		  in the spike output. Our results indicate that the network
		  from which we recorded provided nonlinear combinations of
		  information from sequen tial frames. Such nonlinear
		  preprocessing increases the discrimination capability of
		  any linear readout neurons receivi ng distributed input
		  from the kind of cells we recorded from. These readout
		  neurons could be implemented within V1 and/ or at
		  subsequent processing levels.}
}

@Article{HaeuslerETAL:08,
  author	= {S. Haeusler and K. Schuch and W. Maass},
  title		= {Motif distribution, dynamical properties, and
		  computational performance of two data-based cortical
		  microcircuit templates},
  journal	= {J. of Physiology (Paris)},
  year		= {2009},
  volume	= {103},
  number	= {1-2},
  pages		= {73--87},
  abstract	= {The neocortex is a continuous sheet composed of rather
		  stereotypical local microcircuits that consist of neurons
		  on several laminae with characteristic synaptic
		  connectivity patterns. An understanding of the structure
		  and computational function of these cortical microcircuits
		  may hold the key for understanding the enormous
		  computational power of the neocortex. Two templates for the
		  structure of laminar cortical microcircuits have recently
		  been published by Thomson et al. and Binzegger et al., both
		  resulting from long-lasting experimental studies (but based
		  on different methods). We analyze and compare in this
		  article the structure of these two microcircuit templates.
		  In particular, we examine the distribution of network
		  motifs, i.e. of subcircuits consisting of a small number of
		  neurons. The distribution of these building blocks has
		  recently emerged as a method for characterizing
		  similarities and differences among complex networks. We
		  show that the two microcircuit templates have quite
		  different distributions of network motifs, although they
		  both have a characteristic small-world property. In order
		  to understand the dynamical and computational properties of
		  these two microcircuit templates, we have generated
		  computer models of them, consisting of Hodgkin-Huxley point
		  neurons with conductance based synapses that have a
		  biologically realistic short-term plasticity. The
		  performance of these two cortical microcircuit models was
		  studied for seven generic computational tasks that require
		  accumulation and merging of information contained in two
		  afferent spike inputs. Although the two models exhibit a
		  different performance for some of these tasks, their
		  average computational performance is very similar. When we
		  changed the connectivity structure of these two
		  microcircuit models in order to see which aspects of it are
		  essential for computational performance, we found that the
		  distribution of degrees of nodes is a common key factor for
		  their computational performance. We also show that their
		  computational performance is correlated with specific
		  statistical properties of the circuit dynamics that is
		  induced by a particular distribution of degrees of nodes.}
}

@Article{HaeuslerETAL:08a,
  author	= {S. Haeusler and K. Schuch and W. Maass},
  title		= {Motif distribution and computational performance of two
		  data-based cortical microcircuit templates},
  journal	= {38th Annual Conference of the Society for Neuroscience,
		  Program 220.9},
  year		= {2008},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {The neocortex is a continuous sheet composed of rather
		  stereotypical local microcircuits that consist of neurons
		  on several laminae with characteristic synaptic
		  connectivity patterns. An understanding of the structure
		  and computational function of these cortical microcircuits
		  may hold the key for understanding the enormous
		  computational power of the neocortex. Two templates for the
		  structure of laminar cortical microcircuits have recently
		  been published by Thomson et al. (2002) and Binzegger et
		  al. (2004), both resulting from long-lasting experimental
		  studies (but based on different methods). We analyze and
		  compare in this study the structure and computational
		  properties of these two microcircuit templates. In
		  particular, we examine the distribution of network motifs,
		  i.e. of sub-circuits consisting of a small number of
		  neurons. The distribution of these building blocks of
		  complex networks has recently emerged as a method for
		  characterizing similarities and differences among complex
		  networks. We show that the two microcircuit templates have
		  quite different distributions of network motifs, although
		  they both share characteristic global structural
		  properties, like degree distributions (distribution of the
		  number of synapses per neuron) and small-world properties.
		  In order to understand the computational properties of the
		  two microcircuit templates, we have generated computer
		  models of them, consisting of Hodgkin-Huxley point neurons
		  with conductance based synapses that have a biologically
		  realistic short-term plasticity. The information processing
		  capabilities of the two cortical microcircuit models were
		  studied for 7 generic computational tasks that require
		  accumulation and merging of information contained in two
		  afferent spike inputs. Although the two models exhibit a
		  different performance for some of these tasks, their
		  average computational performance is very similar. When we
		  changed the connectivity structure of these two
		  microcircuit models in order to see which aspects of it are
		  essential for computational performance, we found that the
		  distribution of degrees of nodes is a key factor for their
		  computational performance. References Thomson et al.
		  (2002), Cerebral Cortex, 12(9):936 Binzegger et al. (2004),
		  J. Neurosci., 24(39):8441}
}

@Article{HaeuslerMaass:04,
  author	= {S. Haeusler and W. Maass},
  title		= {A statistical analysis of information processing
		  properties of lamina-specific cortical microcircuit
		  models},
  journal	= {Cerebral Cortex},
  year		= 2007,
  volume	= {17},
  number	= {1},
  pages		= {149-162}
}

@Article{HaeuslerMaass:06,
  author	= {S. Haeusler and W. Maass},
  title		= {Computational impact of laminar structure and small world
		  properties of cortical microcircuit models},
  journal	= {submitted for publication},
  year		= 2006
}

@InProceedings{HajnalETAL:87a,
  author	= {A. Hajnal and W. Maass and P. Pudlak and M. Szegedy and G.
		  Turan},
  booktitle	= {Proceedings of the 28th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {99--110},
  title		= {Threshold circuits of bounded depth},
  year		= {1987}
}

@InProceedings{HajnalETAL:88,
  author	= {A. Hajnal and W. Maass and G. Turan},
  booktitle	= {Proceedings of the 20th Annual ACM Symposium on Theory of
		  Computing},
  pages		= {186--191},
  title		= {On the communication complexity of graph properties},
  year		= {1988}
}

@Article{HajnalETAL:93,
  author	= {A. Hajnal and W. Maass and P. Pudlak and M. Szegedy and G.
		  Turan},
  journal	= {J. Comput. System Sci.},
  pages		= {129--154},
  title		= {Threshold circuits of bounded depth},
  abstract	= {We examine a powerful model of parallel computation:
		  polynomial size threshold circuits of bounded depth (the
		  gates compute threshold functions with polynomial weights).
		  Lower bounds are given to separate polynomial size
		  threshold circuits of depth 2 from polynomial size
		  threshold circuits of depth 3 and from probabilistic
		  polynomial size circuits of depth 2. With regard to the
		  unreliability of bounded depth circuits, it is shown that
		  the class of functions computed reliably with bounded depth
		  circuits of unreliable A, v , 1 gates is narrow. On the
		  other hand, functions computable by bounded depth,
		  polynomial-size threshold circuits can also be computed by
		  such circuits of unreliable threshold gates. Furthermore we
		  examine to what extent imprecise threshold gates (which
		  behave unpredictably near the threshold value) can compute
		  nontrivial functions in bounded depth and a bound is given
		  for the permissible amount of imprecision. We also discuss
		  threshold quantifiers and prove an undefinability result
		  for graph connectivity.},
  volume	= {46},
  year		= {1993}
}

@InProceedings{HauserETAL:07,
  author	= {H. Hauser and G. Neumann and A. J. Ijspeert and W. Maass},
  booktitle	= {Proceedings of the {IEEE}-{RAS} 7th {I}nternational
		  {C}onference on {H}umanoid {R}obots ({H}umanoids 2007)},
  title		= {Biologically Inspired Kinematic Synergies Provide a New
		  Paradigm for Balance Control of Humanoid Robots},
  publisher	= {},
  year		= {2007},
  pages		= {},
  abstract	= {Nature has developed methods for controlling the movements
		  of organisms with many degrees of freedom which differ
		  strongly from existing approaches for balance control in
		  humanoid robots: Biological organisms employ kinematic
		  synergies that simultaneously engage many joints, and which
		  are apparently designed in such a way that their
		  superposition is approximately linear. We show in this
		  article that this control strategy can in principle also be
		  applied to balance control of humanoid robots. In contrast
		  to existing approaches, this control strategy reduces the
		  need to carry out complex computations in real time
		  (replacing the iterated solution of quadratic optimization
		  problems by a simple linear controller), and it does not
		  require knowledge of a dynamic model of the robot.
		  Therefore it can handle unforeseen changes in the dynamics
		  of the robot that may for example arise from wind or other
		  external forces. We demonstrate the feasibility of this
		  novel approach to humanoid balance control through
		  simulations of the humanoid robot HOAP-2 for tasks that
		  require balance control on a randomly moving surfboard.},
  note		= {Best {P}aper {A}ward.
		  http://planning.cs.cmu.edu/humanoids07/p/37.pdf}
}

@Article{HauserETAL:11,
  author	= {H. Hauser and G. Neumann and A. J. Ijspeert and W. Maass},
  title		= {Biologically Inspired Kinematic Synergies Enable Linear
		  Balance Control of a Humanoid Robot},
  journal	= {Biological Cybernetics},
  year		= {2011},
  pages		= {},
  volume	= {},
  note		= {Published online: 27 April 2011},
  abstract	= {Despite many efforts, balance control of humanoid robots
		  in the presence of unforeseen external or internal forces
		  has remained an unsolved problem. The difficulty of this
		  problem is a consequence of the high dimensionality of the
		  action space of a humanoid robot, due to its large number
		  of degrees of freedom (joints), and of nonlinearities in
		  its kinematic chains. Biped biological organisms face
		  similar difficulties, but have nevertheless solved this
		  problem. Experimental data reveal that many biological
		  organisms reduce the high dimensionality of their action
		  space by generating movements through linear superposition
		  of a rather small number of stereotypical combinations of
		  simultaneous movements of many joints, to which we refer as
		  kinematic synergies in this paper. We show that by
		  constructing two suitable nonlinear kinematic synergies for
		  the lower part of the body of a humanoid robot, balance
		  control can in fact be reduced to a linear control problem,
		  at least in the case of relatively slow movements. We
		  demonstrate for a variety of tasks that the humanoid robot
		  HOAP-2 acquires through this approach the capability to
		  balance dynamically against unforeseen disturbances that
		  may arise from external forces or from manipulating unknown
		  loads.}
}

@InProceedings{HochbaumMaass:84,
  author	= {D. Hochbaum and W. Maass},
  booktitle	= {Proceedings of Symp. on Theoretical Aspects of Computer
		  Science (Paris 1984)},
  pages		= {55--62},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {Approximation schemes for covering and packing problems in
		  robotics and {VLSI} (extended abstract)},
  volume	= {166},
  year		= {1984}
}

@Article{HochbaumMaass:85,
  author	= {D. Hochbaum and W. Maass},
  journal	= {J. Assoc. Comp. Mach.},
  pages		= {130--136},
  title		= {Approximation algorithms for covering and packing problems
		  in image processing and {VLSI}},
  volume	= {32},
  year		= {1985}
}

@Article{HochbaumMaass:87,
  author	= {D. Hochbaum and W. Maass},
  journal	= {J. Algorithms},
  pages		= {305--323},
  title		= {Fast approximation algorithms for a nonconvex problem},
  volume	= {8},
  year		= {1987}
}

@TechReport{HoeflerBachler:06,
  author	= {M. Hoefler and M. Bachler},
  title		= {A software framework for adaptive biologically inspired
		  image classification},
  institution	= {Technische Universitaet Graz},
  year		= {2006}
}

@MastersThesis{Hoerzer:08,
  author	= {G. Hoerzer},
  title		= {Extraction of information about the behavioral state of
		  monkeys from neuronal recordings with methods from machine
		  learning},
  school	= {Graz University of Technology, Institute for Theoretical
		  Computer Sciences},
  year		= {2008}
}

@TechReport{Hoerzer:09,
  author	= {G. Hoerzer},
  title		= {Methods for the Analysis of Uni-, Bi- and Multivariate
		  Data},
  institution	= {University of Technology, Institute for Theoretical
		  Computer Sciences},
  year		= {2009}
}

@Article{HomerMaass:83,
  author	= {S. Homer and W. Maass},
  journal	= {Theoretical Computer Science},
  pages		= {279--289},
  title		= {Oracle dependent properties of the lattice of {NP}-sets},
  volume	= {24},
  year		= {1983}
}

@InProceedings{JahrerETAL:10,
  author	= {M. Jahrer and A. T\"oscher and R. Legenstein},
  title		= {Combining Predictions for Accurate Recommender Systems},
  booktitle	= {KDD '10: Proceedings of the 16th ACM SIGKDD international
		  conference on Knowledge discovery and data mining},
  year		= {2010},
  isbn		= {978-1-4503-0055-1},
  pages		= {693--702},
  location	= {Washington, DC, USA},
  doi		= {http://doi.acm.org/10.1145/1835804.1835893},
  publisher	= {ACM},
  address	= {New York, NY, USA},
  abstract	= {We analyze the application of ensemble learning to
		  recommender systems on the Netflix Prize dataset. For our
		  analysis we use a set of diverse state-of-the-art
		  collaborative filtering (CF) algorithms, which include:
		  SVD, Neighborhood Based Approaches, Restricted Boltzmann
		  Machine, Asymmetric Factor Model and Global Effects. We
		  show that linearly combining (blending) a set of CF
		  algorithms increases the accuracy and outperforms any
		  single CF algorithm. Furthermore, we show how to use
		  ensemble methods for blending predictors in order to
		  outperform a single blending algorithm. The dataset and the
		  source code for the ensemble blending are available
		  online.}
}

@MastersThesis{Joshi:02,
  author	= {P. Joshi},
  title		= {Synthesis of a Liquid State Machine with Hopfield/Brody
		  Transient Synchrony},
  school	= {Center for Advanced Computer Studies, University of
		  Louisiana, Lafayette, USA},
  year		= 2002,
  month		= {November},
  abstract	= {Understanding the mechanism of spatiotemporal integration
		  used by our brain to perform recognition of complex
		  temporal sequences is a challenge for current researchers
		  in neuroscience. Recent research has proposed transient
		  synchrony as a plausible mechanism for spatiotemporal
		  integration. This thesis studies a biologically plausible
		  network architecture made of simulated minicolumns that
		  performs a temporal integration task, specifically
		  spoken-word recognition. The network's ability to recognize
		  a spoken word and its natural variants is independent of
		  variations across speakers, simple masking noises and
		  variations in system parameters. The network demonstrates
		  inter columnar and intra columnar synchrony, which in turn
		  leads to word recognition. The intra columnar synchrony of
		  minicolumns acts as an event detection mechanism for events
		  in a particular frequency band. The inter columnar
		  transient synchrony enables the network to recognize words.
		  Each of the minicolumns exhibit a very unique temporal
		  signature when presented with a temporal input. These
		  signatures looked nearly the same for similar inputs (e.g.,
		  the same word spoken by different speakers etc.) and were
		  strikingly different for different temporal inputs (e.g.,
		  different words).}
}

@InProceedings{Joshi:06,
  author	= {P. Joshi},
  title		= {Modeling working memory and decision making using generic
		  neural microcircuits},
  editor	= {Stefanos Kollias and Andreas Stafylopatis and
		  W{\l}odzis{\l}aw Duch and Erkki Oja},
  booktitle	= {Artificial Neural Networks -- ICANN 2006},
  pages		= {515--524},
  year		= {2006},
  volume	= {4131},
  series	= {Lecture Notes in Computer Science},
  publisher	= {Springer},
  isbn		= {3-540-38625-4},
  abstract	= {Classical behavioral experiments to study working memory
		  typically involve three phases. First the subject receives
		  a stimulus, then holds it in the working memory, and
		  finally makes a decision by comparing it with another
		  stimulus. A neurocomputational model using generic neural
		  microcircuits with feedback is presented here that
		  integrates the three computational stages into a single
		  unified framework. The architecture is tested using the
		  two-interval discrimination and delayed-match-to-sample
		  experimental paradigms as benchmarks.}
}

@Article{Joshi:06b,
  author	= {P. Joshi},
  title		= {From memory based decisions to decision based movements: A
		  model of interval discrimination followed by action
		  selection},
  journal	= {Neural Networks},
  volume	= {20},
  year		= {2007},
  publisher	= {},
  pages		= {298--311},
  abstract	= {The interval discrimination task is a classical
		  experimental paradigm that is employed to study working
		  memory and decision making and typically involves four
		  phases. First, the subject receives a stimulus, then holds
		  it in the working memory, then makes a decision by
		  comparing it with another stimulus and finally acts on this
		  decision, usually by pressing one of the two buttons
		  corresponding to the binary decision. This article
		  demonstrates that simple linear readouts from generic
		  neural microcircuits that send feedback of their activity
		  to the circuit, can be trained using identical learning
		  mechanisms to perform quite separate tasks of decision
		  making and generation of subsequent motor commands. In this
		  sense, the neurocomputational algorithm presented here is
		  able to integrate the four computational stages into a
		  single unified framework. The algorithm is tested using
		  two-interval discrimination and delayed-match-to-sample
		  experimental paradigms as benchmarks.}
}

@PhDThesis{Joshi:07,
  author	= {P. Joshi},
  title		= {On the role of feedback in enhancing the computational
		  power of generic neural microcircuits},
  school	= {Graz University of Technology},
  year		= 2007,
  abstract	= {Circuits of neurons in the brain perform diverse cortical
		  computations in parallel, endowing the organism with
		  diverse cortical modalities, e.g. motor control, vision,
		  and audition; and higher order cognitive processes, e.g.
		  planning, and decision making. It is believed that these
		  computations are carried out by network of neurons in
		  cortical microcircuits, where each microcircuit is composed
		  of rather stereotypical circuit of neurons within a
		  cortical column. A characteristic property of these
		  cortical circuits is the presence of abundant feedback
		  connections, be it on the level of recurrent axon
		  collaterals projecting back onto the same neuron, or on the
		  network level between different cortical areas. This thesis
		  explores the functional role of neural feedback in
		  enhancing the computational power of generic neural
		  microcircuits. It is shown that feedback endows standard
		  models for neural circuits with the capability to emulate
		  arbitrary Turing machines. In fact, with a suitable
		  feedback such circuits can simulate any dynamical system,
		  in particular any conceivable analog computer. Under
		  realistic noise conditions the computational power of these
		  circuits is obviously reduced. However it is demonstrated
		  through computer simulations that feedback also provides a
		  significant gain in computational power for quite detailed
		  models of cortical microcircuits with in-vivo-like high
		  levels of noise. Furthermore neurocomputational models
		  using generic neural microcircuits with feedback are
		  explored in the context of motor control, decision making,
		  and ``action selection in presence of decisions''.}
}

@InProceedings{Joshi:08a,
  author	= {P. Joshi and J. Triesch},
  title		= {A Globally Asymptotically Stable Plasticity Rule for
		  Firing Rate Homeostasis},
  editor	= {},
  booktitle	= {Artificial Neural Networks -- ICANN 2008},
  pages		= {},
  year		= {2008},
  volume	= {},
  series	= {},
  publisher	= {},
  isbn		= {},
  abstract	= {How can neural circuits maintain stable activity states
		  when they are constantly being modified by Hebbian
		  processes that are notori- ous for being unstable? A new
		  synaptic plasticity mechanism is presented here that
		  enables a neuron to obtain homeostasis of its firing rate
		  over longer timescales while leaving the neuron free to
		  exhibit fluctuating dynamics in response to external
		  inputs. Mathematical results demon- strate that this rule
		  is globally asymptotically stable. Performance of the rule
		  is benchmarked through simulations from single neuron to
		  network level, using sigmoidal neurons as well as spiking
		  neurons with dynamic synapses. }
}

@InProceedings{Joshi:08b,
  author	= {C. Savin and P. Joshi and J. Triesch},
  title		= {ICA with spiking neurons},
  booktitle	= {Submitted for publication},
  abstract	= {We propose a biologically plausible mechanism for
		  performing ICA with spiking neurons. We show that a
		  stochastically spiking neuron is able to learn one inde-
		  pendent component in the input, by combining spike-timing
		  dependent plasticity (STDP) with an intrinsic plasticity
		  rule, which regulates the neuron response prop- erties to
		  maximize information transmission, under the constraint of
		  a fixed mean firing rate. A population of such neurons is
		  shown to perform ICA when their activities are decorrelated
		  by adaptive lateral inhibition. },
  year		= {2008},
  volume	= {},
  pages		= {},
  publisher	= {},
  editor	= {}
}

@InProceedings{Joshi:08c,
  author	= {P. Joshi and J. Triesch},
  title		= {Rules for information-maximization in spiking neurons
		  using intrinsic plasticity},
  booktitle	= {Submitted for publication},
  abstract	= {Information theory predicts the need for information
		  maximization as sensory in- formation must be compressed
		  into a limited range of responses that spiking neu- rons
		  can generate. We propose computational theory and learning
		  rules based on information theory that lead to information
		  maximization using intrinsic plasticity in a stochastically
		  spiking neuron model. Computer simulations are used to
		  verify the theoretical results. Further experiments show
		  that the intrinsic plasticity rules described in this
		  article lead to a desired exponential output distribution,
		  firing- rate homeostasis, and adaptation to sensory
		  deprivation in our model as observed in cortical neurons.
		  },
  year		= {2008},
  volume	= {},
  pages		= {},
  publisher	= {},
  editor	= {}
}

@Article{JoshiETAL:09,
  author	= {P. Joshi and G. Rainer and W. Maass},
  title		= {Computational role of theta oscillations in
		  delayed-decision tasks},
  journal	= {in preparation},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@InProceedings{JoshiMaass:03,
  author	= {P. Joshi and W. Maass},
  title		= {Movement Generation and Control with Generic Neural
		  Microcircuits},
  booktitle	= {Biologically Inspired Approaches to Advanced Information
		  Technology. First International Workshop, Bio{ADIT} 2004,
		  Lausanne, Switzerland, January 2004, Revised Selected
		  Papers},
  pages		= {258--273},
  year		= {2004},
  editor	= {A. J. Ijspeert and M. Murata and N. Wakamiya},
  volume	= {3141},
  series	= {Lecture Notes in Computer Science},
  publisher	= {Springer Verlag},
  abstract	= {Simple linear readouts from generic neural microcircuit
		  models consisting of spiking neurons and dynamic synapses
		  can be trained to generate and control basic movements, for
		  example, reaching with an arm to various target points.
		  After suitable training of these readouts on a small number
		  of target points; reaching movements to other target points
		  can also be generated. Sensory or proprioceptive feedback
		  turns out to be essential for such movement control, even
		  if it is noisy and substantially delayed. Such feedback
		  turns out to optimally improve the performance of the
		  neural microcircuit model if it arrives with a biologically
		  realistic delay of 100 to 200 ms. Furthermore, additional
		  feedbacks of ``prediction of sensory variables'' are shown
		  to improve the performance significantly. The proposed
		  model also provides a new approach for movement control in
		  robotics. Existing control methods in robotics that take
		  the particular dynamics of the sensors and actuators into
		  account (``embodiment of robot control'') are taken one
		  step further by this approach, which provides methods for
		  also using the ``embodiment of computation'', i.e. the
		  inherent dynamics and spatial structure of neural circuits,
		  for the design of robot movement controllers.}
}

@Article{JoshiMaass:04,
  author	= {P. Joshi and W. Maass},
  journal	= {Neural Computation},
  title		= {Movement Generation with Circuits of Spiking Neurons},
  year		= {2005},
  volume	= 17,
  number	= 8,
  pages		= {1715--1738},
  abstract	= {How can complex movements that take hundreds of
		  milliseconds be generated by stereotypical neural
		  microcircuits consisting of spiking neurons with a much
		  faster dynamics? We show that linear readouts from generic
		  neural microcircuit models can be trained to generate basic
		  arm movements. Such movement generation is independent of
		  the arm-model used and the type of feedbacks that the
		  circuit receives. We demonstrate this by considering two
		  different models of a two-jointed arm, a standard model
		  from robotics and a standard model from biology, that each
		  generate different kinds of feedback. Feedbacks that arrive
		  with biologically realistic delays of 50--280 ms turn out
		  to give rise to the best performance. If a feedback with
		  such desirable delay is not available, the neural
		  microcircuit model also achieves good performance if it
		  uses internally generated estimates of such feedback.
		  Existing methods for movement generation in robotics that
		  take the particular dynamics of sensors and actuators into
		  account (``embodiment of motor systems'') are taken one
		  step further with this approach, which provides methods for
		  also using the ``embodiment of motion generation
		  circuitry'', i.e., the inherent dynamics and spatial
		  structure of neural circuits, for the generation of
		  movements.}
}

@Article{KWA97,
  author	= {J. Kivinen and M. K. Warmuth and P. Auer},
  title		= {The Perceptron algorithm vs. {Winnow}: linear vs.
		  logarithmic mistake bounds when few input variables are
		  relevant},
  journal	= {Artificial Intelligence},
  year		= {1997},
  pages		= {325--343}
}

@Article{KaskeBertschinger:05,
  author	= {A. Kaske and N. Bertschinger},
  title		= {Travelling wave patterns in a model of the spinal pattern
		  generator using spiking neurons},
  journal	= {Biol. Cybern.},
  year		= 2005,
  volume	= 92,
  number	= 3,
  pages		= {206--218}
}

@Article{KaskeMaass:04,
  author	= {A. Kaske and W. Maass},
  title		= {A model for the interaction of oscillations and pattern
		  generation with real-time computing in generic neural
		  microcircuit models},
  journal	= {Neural Networks},
  year		= {2006},
  volume	= {19},
  number	= {5},
  pages		= {600--609},
  abstract	= {It is shown that real-time computations on spike patterns
		  and temporal integration of information in neural
		  microcircuit models are compatible with potentially
		  descruptive additional inputs such as oscillations. A minor
		  change in the connection statistics of such circuits
		  (making synaptic connections to more distal target neurons
		  more likely for excitatory than for inhibitory neurons)
		  endows such generic neural microcircuit model with the
		  ability to generate periodic patterns autonomously. We show
		  that such pattern generation can also be multiplexed with
		  pattern classification and temporal integration of
		  information in the same neural circuit. These results can
		  be interpreted as showing that periodic activity provides a
		  second channel for communication in neural systems which
		  can be used to synchronize or coordinate spatially
		  separated processes, without encumbering local real-time
		  computations on spike trains in diverse neural circuits.}
}

@MastersThesis{Klampfl:06,
  author	= {S. Klampfl},
  title		= {Extracting Statistically Independent Components with a
		  Generalized {BCM} Rule for Spiking Neurons},
  school	= {Graz University of Technology},
  year		= {2006}
}

@Article{KlampflETAL:07,
  author	= {S. Klampfl and R. Legenstein and W. Maass},
  title		= {Spiking neurons can learn to solve information bottleneck
		  problems and extract independent components},
  journal	= {Neural Computation},
  year		= {2009},
  volume	= {21},
  number	= {4},
  pages		= {911--959},
  abstract	= {Independent Component Analysis (or blind source
		  separation) is assumed to be an essential component of
		  sensory processing in the brain and could provide a less
		  redundant representation about the external world. Another
		  powerful processing strategy is the optimization of
		  internal representations according to the information
		  bottleneck method. This method would allow to extract
		  preferentially those components from high-dimensional
		  sensory input streams that are related to other information
		  sources, such as internal predictions or proprioceptive
		  feedback. However there exists a lack of models that could
		  explain how spiking neurons could learn to execute either
		  of these two processing strategies. We show in this article
		  how stochastically spiking neurons with refractoriness
		  could in principle learn in an unsupervised manner to carry
		  out both information bottleneck optimization and the
		  extraction of independent components. We derive suitable
		  learning rules, which extend the well known BCM-rule, from
		  abstract information optimization principles. These rules
		  will simultaneously keep the firing rate of the neuron
		  within a biologically realistic range.}
}

@InProceedings{KlampflETAL:07b,
  author	= {S. Klampfl and R. Legenstein and W. Maass},
  title		= {Information Bottleneck Optimization and Independent
		  Component Extraction with Spiking Neurons},
  booktitle	= {Proc. of NIPS 2006, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2007},
  volume	= {19},
  pages		= {713--720},
  abstract	= {The extraction of statistically independent components
		  from high-dimensional multi-sensory input streams is
		  assumed to b e an essential component of sensory processing
		  in the brain. Such independent component analysis (or blind
		  source separat ion) could provide a less redundant
		  representation of inform ation about the external world.
		  Another powerful processing strategy is to extract
		  preferentially those components from high-dimensional input
		  streams that are related to other inf ormation sources,
		  such as internal predictions or propriocep tive feedback.
		  This strategy allows the optimization of inte rnal
		  representation according to the information bottleneck
		  method. However, concrete learning rules that implement
		  thes e general unsupervised learning principles for spiking
		  neuro ns are still missing. We show how both information
		  bottlenec k optimization and the extraction of independent
		  components can in principle be implemented with
		  stochastically spiking neurons with refractoriness. The new
		  learning rule that achi eves this is derived from abstract
		  information optimization principles.}
}

@Article{KlampflETAL:09,
  author	= {S. Klampfl and S.V. David and P. Yin and S.A. Shamma and
		  W. Maass},
  title		= {Integration of stimulus history in information conveyed by
		  neurons in primary auditory cortex in response to tone
		  sequences},
  journal	= {39th Annual Conference of the Society for Neuroscience,
		  Program 163.8, Poster T6 },
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {A critical component of auditory processing is integrating
		  information about sound features that change over time.
		  Previous studies have shown that the context of a sound --
		  the immediate history of auditory stimulation -- can have a
		  substantial effect on responses of auditory neurons to a
		  current sound. In order to characterize these effects, we
		  measured the information contained in the neural activity
		  in primary auditory cortex (A1) about both current and
		  preceding sounds. Neural recordings were made from single
		  A1 neurons (n=122) isolated from 23 multi-channel
		  recordings in 4 passively listening ferrets. The stimulus
		  was a sequence of tones (150 ms duration). The frequency
		  step between two consecutive tones was always half an
		  octave up or down. For each neuron, we measured at
		  particular points in time the mutual information (MI)
		  between its response during a sliding window (20ms) and the
		  identity of the current and preceding tone. Since direct
		  estimates of MI from spike trains typically suffer from a
		  systematic error (bias) due to the limited number of
		  available response trials for a given stimulus, we used a
		  recently proposed shuffling-based estimator with additional
		  quadratic extrapolation bias correction (Panzeri et al.,
		  2007). This method produces reliable information estimates
		  for this particular setup. We found that most responses
		  (102 out of 122 neurons) contained significant information
		  about the stimulus throughout the duration of the tone. Of
		  this information, on average, 60\% was about the current
		  tone, while 40\% was about the previous tone. We also
		  trained linear classifiers (Support Vector Machines with
		  linear kernel) on the low-pass filtered response spike
		  trains of multiple simultaneously recorded neurons (4-10)
		  to discriminate between the two possible predecessors for a
		  given tone. The performance the linear classifier can be
		  viewed as a lower bound on the information contained in the
		  responses about the previous tone. Performance of up to
		  80\% was achieved. These results quantify the amount of
		  information contained in the responses of A1 neurons about
		  both currently and previously played tones and demonstrate
		  that neurons in A1 integrate information about previous
		  input into their current responses. References: Panzeri et
		  al. (2007), J Neurophysiol, 98(3):1064}
}

@Article{KlampflMaass:09,
  author	= {S. Klampfl and W. Maass},
  title		= {A neuron can learn anytime classification of trajectories
		  of network states without supervision},
  journal	= {submitted for publication},
  year		= {Feb. 2009},
  volume	= {},
  number	= {},
  pages		= {},
  note		= {}
}

@Article{KlampflMaass:09a,
  author	= {S. Klampfl and W. Maass},
  title		= {A theoretical basis for emergent pattern discrimination in
		  neural systems through slow feature extraction},
  journal	= {submitted},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@InProceedings{KlampflMaass:09b,
  author	= {S. Klampfl and W. Maass},
  title		= {Replacing supervised classification learning by {S}low
		  {F}eature {A}nalysis in spiking neural networks},
  booktitle	= {Proc. of NIPS 2009: Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2010},
  volume	= {22},
  pages		= {988--996},
  abstract	= {Many models for computations in recurrent networks of
		  neurons assume that the network state moves from some
		  initial state to some fixed point attractor or limit cycle
		  that represents the output of the computation. However
		  experimental data show that in response to a sensory
		  stimulus the network state moves from its initial state
		  through a trajectory of network states and eventually
		  returns to the initial state, without reaching an attractor
		  or limit cycle in between. This type of network response,
		  where salient information about external stimuli is encoded
		  in characteristic trajectories of continuously varying
		  network states, raises the question how a neural system
		  could compute with such code, and arrive for example at a
		  temporally stable classification of the external stimulus.
		  We show that a known unsupervised learning algorithm, Slow
		  Feature Analysis (SFA), could be an important ingredient
		  for extracting stable information from these network
		  trajectories. In fact, if sensory stimuli are more often
		  followed by another stimulus from the same class than by a
		  stimulus from another class, SFA approaches the
		  classification capability of Fishers Linear Discriminant
		  (FLD), a powerful algorithm for supervised learning. We
		  apply this principle to simulated cortical microcircuits,
		  and show that it enables readout neurons to learn
		  discrimination of spoken digits and detection of repeating
		  firing patterns within a stream of spike trains with the
		  same firing statistics, without requiring any supervision
		  for learning.}
}

@Article{KlampflMaass:10,
  author	= {S. Klampfl and W. Maass},
  title		= {A theoretical basis for emergent pattern discrimination in
		  neural systems through slow feature extraction},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  number	= {12},
  pages		= {2979--3035},
  abstract	= {Neurons in the brain are able to detect and discriminate
		  salient spatio-temporal patterns in the firing activity of
		  presynaptic neurons. It is open how they can learn to
		  achieve this, especially without the help of a supervisor.
		  We show that a well-known unsupervised learning algorithm
		  for linear neurons, Slow Feature Analysis (SFA), is able to
		  acquire the discrimination capability of one of the best
		  algorithms for supervised linear discrimination learning,
		  the Fisher Linear Discriminant (FLD), given suitable input
		  statistics. We demonstrate the power of this principle by
		  showing that it enables readout neurons from simulated
		  cortical microcircuits to learn without any supervision to
		  discriminate between spoken digits, and to detect repeated
		  firing patterns that are embedded into a stream of noise
		  spike trains with the same firing statistics. Both these
		  computer simulations and our theoretical analysis show that
		  slow feature extraction enables neurons to extract and
		  collect information that is spread out over a trajectory of
		  firing states that lasts several hundred ms. In addition,
		  it enables neurons to learn without supervision to keep
		  track of time (relative to a stimulus onset, or the
		  initiation of a motor response). Hence these results
		  elucidate how the brain could compute with trajectories of
		  firing states, rather than only with fixed point
		  attractors. It also provides a theoretical basis for
		  understanding recent experimental results on the emergence
		  of view- and position-invariant classification of visual
		  objects in inferior temporal cortex.},
  note		= {Epub 2010 Sep 21}
}

@InProceedings{KranakisETAL:95,
  author	= {E. Kranakis and D. Krizanc and B. Ruf and J. Urrutia and
		  G. Woeginger},
  booktitle	= {Graph-theoretic concepts in computer science},
  editor	= {M. Nagl},
  pages		= {1--13},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {{VC}-dimensions for graphs},
  volume	= {1017},
  year		= {1995},
  keywords	= {VC-Dimensions, graphs}
}

@Article{KranakisETAL:97,
  author	= {E. Kranakis and D. Krizanc and B. Ruf and J. Urrutia and
		  G. Woeginger},
  journal	= {Journal of Discrete Applied Mathematics},
  pages		= {237--257},
  title		= {The {VC}-dimension of set-systems defined by graphs},
  volume	= {77},
  year		= {1997},
  keywords	= {VC-dimensions, graph}
}

@PhDThesis{Legenstein:02a,
  author	= {R. A. Legenstein},
  title		= {The Wire-Length Complexity of Neural Networks},
  school	= {Graz University of Technology},
  year		= 2002,
  abstract	= {The ability of our nervous system to rapidly process and
		  react on the huge amount of sensory input data is grounded
		  on its massively parallel architecture. Arguably, physical
		  cost for communication, that is to say the space needed for
		  wires, is the most severe bottleneck in biological as well
		  as in artificial architectures of this type. In this thesis
		  the complexity of wiring in biological and artificial
		  neural networks, the implications of wiring constraints to
		  models for brain circuits, and the implementation of
		  wire-efficient circuit designs in hardware are studied. We
		  present a simple mathematical framework that allows us to
		  study the wiring complexity of neural circuits in a formal
		  and general manner. In this model, the complexity of a
		  circuit is measured by the total length of wires needed to
		  implement the circuit, a complexity measure that is one of
		  the most salient ones if real-world constraints of
		  implementations in hardware or ``wetware'' are
		  considered.Furthermore, we study the layout of general
		  computational structures like tree computations. We give
		  tight upper and lower bounds on the wire length of
		  constrained tree layouts and show efficient layout
		  strategies for prefix computations.}
}

@Article{Legenstein:02b,
  author	= {R. A. Legenstein},
  title		= {On the Complexity of Knock-Knee Channel Routing with
		  3-Terminal Nets},
  journal	= {Technical Report},
  year		= {2002},
  abstract	= {In this article we consider a basic problem in the layout
		  of VLSI-circuits, the channel-routing problem in the
		  knock-knee mode. We show that knock-knee channel routing
		  with 3-terminal nets is NP-complete and thereby settling a
		  problem that was open for more than a decade. In 1987,
		  Sarrafzadeh showed that knock-knee channel routing with
		  5-terminal nets is NP-complete. Furthermore, it is known
		  that this problem is solvable in polynomial time if only
		  2-terminal nets are involved (This problem was addressed
		  for example by Frank in 1982 and by Formann, D. Wagner, and
		  F. Wagner in 1993).}
}

@MastersThesis{Legenstein:99,
  author	= {R. A. Legenstein},
  title		= {Effizientes {L}ayout von {N}euronalen {N}etzen},
  school	= {Technische Universitaet Graz},
  year		= 1999,
  month		= {September}
}

@Article{LegensteinETAL:03,
  author	= {R. Legenstein and H. Markram and W. Maass},
  title		= {Input Prediction and Autonomous Movement Analysis in
		  Recurrent Circuits of Spiking Neurons},
  year		= {2003},
  journal	= {Reviews in the Neurosciences (Special Issue on
		  Neuroinformatics of Neural and Artificial Computation)},
  volume	= {14},
  number	= {1--2},
  pages		= {5--19},
  abstract	= {Temporal integration of information and prediction of
		  future sensory inputs are assumed to be important
		  computational tasks of generic cortical microcircuits.
		  However it has remained open how cortical microcircuits
		  could possibly achieve this, especially since they consist
		  in contrast to most neural network models of neurons and
		  synapses with heterogeneous dynamic responses. However it
		  turns out that the diversity of computational units
		  increases the capability of microcircuit models for
		  temporal integration. Furthermore the prediction of future
		  input may be rather easy for such circuits since it
		  suffices to train the readouts from such microcircuits. In
		  this article we show that very simple readouts from a
		  generic recurrently connected circuit of integrate-and-fire
		  neurons with diverse dynamic synapses can be trained in an
		  unsupervised manner to predict movements of different
		  objects, that move within an unlimited number of
		  combinations of speed, angle, and offset over a simulated
		  sensor field. The autonomously trained microcircuit model
		  is also able to compute the direction of motion, which is a
		  computationally difficult problem ("aperture problem")
		  since it requires disambiguation of local sensor readings
		  through the context of other sensor readings at the current
		  and preceding moments. Furthermore the same circuit can be
		  trained simultaneously in a supervised manner to also
		  report the shape and velocity of the moving object. Finally
		  it is shown that the trained neural circuit supports
		  novelty detection and the generation of "imagined
		  movements". Altogether the results of this article suggest
		  that it is not necessary to construct specific and
		  biologically unrealistic neural circuit models for specific
		  sensory processing tasks, since "found" generic cortical
		  microcircuit models in combination with very simple
		  perceptron-like readouts can easily be trained to solve
		  such computational tasks.}
}

@Article{LegensteinETAL:04,
  author	= {R. Legenstein and C. Naeger and W. Maass},
  title		= {What can a Neuron Learn with Spike-Timing-Dependent
		  Plasticity?},
  journal	= {Neural Computation},
  year		= {2005},
  volume	= {17},
  number	= {11},
  pages		= {2337--2382},
  abstract	= {Spiking neurons are very flexible computational modules,
		  which can implement with different values of their
		  adjustable synaptic parameters an enormous variety of
		  different transformations F from input spike trains to
		  output spike trains. We examine in this letter the question
		  to what extent a spiking neuron with biologically realistic
		  models for dynamic synapses can be taught via
		  spike-timing-dependent plasticity (STDP) to implement a
		  given transformation F. We consider a supervised learning
		  paradigm where during training, the output of the neuron is
		  clamped to the target signal (teacher forcing). The
		  well-known perceptron convergence theorem asserts the
		  convergence of a simple supervised learning algorithm for
		  drastically simplified neuron models (McCulloch-Pitts
		  neurons). We show that in contrast to the perceptron
		  convergence theorem, no theoretical guarantee can be given
		  for the convergence of STDP with teacher forcing that holds
		  for arbitrary input spike patterns. On the other hand, we
		  prove that average case versions of the perceptron
		  convergence theorem hold for STDP in the case of
		  uncorrelated and correlated Poisson input spike trains and
		  simple models for spiking neurons. For a wide class of
		  cross-correlation functions of the input spike trains, the
		  resulting necessary and sufficient condition can be
		  formulated in terms of linear separability, analogously as
		  the well-known condition of learnability by perceptrons.
		  However, the linear separability criterion has to be
		  applied here to the columns of the correlation matrix of
		  the Poisson input. We demonstrate through extensive
		  computer simulations that the theoretically predicted
		  convergence of STDP with teacher forcing also holds for
		  more realistic models for neurons, dynamic synapses, and
		  more general input distributions. In addition, we show
		  through computer simulations that these positive learning
		  results hold not only for the common interpretation of
		  STDP, where STDP changes the weights of synapses, but also
		  for a more realistic interpretation suggested by
		  experimental data where STDP modulates the initial release
		  probability of dynamic synapses.}
}

@InProceedings{LegensteinETAL:04a,
  author	= {R. Legenstein and W. Maass},
  title		= {A criterion for the convergence of learning with spike
		  timing dependent plasticity},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {Y. Weiss and B. Schoelkopf and J. Platt},
  volume	= {18},
  pages		= {763--770},
  year		= 2006,
  publisher	= {MIT Press},
  abstract	= {We investigate under what conditions a neuron can learn by
		  experimentally supported rules for spike timing dependent
		  plasticity (STDP) to predict the arrival times of strong
		  ``teacher inputs'' to the same neuron. It turns out that in
		  contrast to the famous Perceptron Convergence Theorem,
		  which predicts convergence of the perceptron learning rule
		  for a strongly simplified neuron model whenever a stable
		  solution exists, no equally strong convergence guarantee
		  can be given for spiking neurons with STDP. But we derive a
		  criterion on the statistical dependency structure of input
		  spike trains which characterizes exactly when learning with
		  STDP will converge on average for a simple model of a
		  spiking neuron. This criterion is reminiscent of the linear
		  separability criterion of the Perceptron Convergence
		  Theorem, but it applies here to the rows of a correlation
		  matrix related to the spike inputs. In addition we show
		  through computer simulations for more realistic neuron
		  models that the resulting analytically predicted positive
		  learning results not only hold for the common
		  interpretation of STDP where STDP changes the weights of
		  synapses, but also for a more realistic interpretation
		  suggested by experimental data where STDP modulates the
		  initial release probability of dynamic synapses.}
}

@InProceedings{LegensteinETAL:08,
  author	= {R. Legenstein and D. Pecevski and W. Maass},
  title		= {Theoretical Analysis of Learning with Reward-Modulated
		  Spike-Timing-Dependent Plasticity},
  booktitle	= {Proc. of NIPS 2007, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2008},
  volume	= {20},
  pages		= {881--888},
  abstract	= {Reward-modulated spike-timing-dependent plasticity
		  ({STDP}) has recently emerged as a candidate for a learning
		  rule that could explain how local learning rules at single
		  synapses support behaviorally relevant adaptive changes in
		  complex networks of spiking neurons. However the potential
		  and limitations of this learning rule could so far only be
		  tested through computer simulations. This article provides
		  tools for an analytic treatment of reward-modulated {STDP},
		  which allow us to predict under which conditions
		  reward-modulated {STDP} will be able to achieve a desired
		  learning effect. In particular, we can produce in this way
		  a theoretical explanation and a computer model for a
		  fundamental experimental finding on biofeedback in monkeys
		  (reported in [1])}
}

@Article{LegensteinETAL:08a,
  author	= {R. Legenstein and D. Pecevski and W. Maass},
  title		= {A Learning Theory for Reward-Modulated
		  Spike-Timing-Dependent Plasticity with Application to
		  Biofeedback},
  journal	= {PLoS Computational Biology},
  year		= {2008},
  volume	= {4},
  number	= {10},
  pages		= {1--27},
  abstract	= {Reward-modulated spike-timing-dependent plasticity
		  ({STDP}) has recently emerged as a candidate for a learning
		  rule that could explain how behaviorally relevant adaptive
		  changes in complex networks of spiking neurons could be
		  achieved in a self-organizing manner through local synaptic
		  plasticity. However the capabilities and limitations of
		  this learning rule could so far only be tested through
		  computer simulations. This article provides tools for an
		  analytic treatment of reward-modulated {STDP}, which allows
		  us to predict under which conditions reward-modulated
		  {STDP} will achieve a desired learning effect. These
		  analytical results imply that neurons can learn through
		  reward-modulated {STDP} to classify not only spatial, but
		  also temporal firing patterns of presynaptic neurons. They
		  also can learn to respond to specific presynaptic firing
		  patterns with particular spike patterns. Finally, the
		  resulting learning theory predicts that even difficult
		  credit-assignment problems, where it is very hard to tell
		  which synaptic weights should be modified in order to
		  increase the global reward for the system, can be solved in
		  a self-organizing manner through reward-modulated {STDP}.
		  This yields an explanation for a fundamental experimental
		  result on biofeedback in monkeys by Fetz and Baker. In this
		  experiment monkeys were rewarded for increasing the firing
		  rate of a particular neuron in the cortex, and were able to
		  solve this extremely difficult credit assignment problem.
		  Our model for this experiment relies on a combination of
		  reward-modulated {STDP} with variable spontaneous firing
		  activity. Hence it also provides a possible functional
		  explanation for trial-to-trial variability, which is
		  characteristic for cortical networks of neurons, but has no
		  analogue in currently existing artificial computing
		  systems. In addition our model demonstrates that
		  reward-modulated {STDP} can be applied to all synapses in a
		  large recurrent neural network without endangering the
		  stability of the network dynamics.}
}

@Article{LegensteinETAL:08b,
  author	= {R. Legenstein and D. Pecevski and W. Maass},
  title		= {Supplementary Information to: "{A} Learning Theory for
		  Reward-Modulated Spike-Timing-Dependent Plasticity with
		  Application to Biofeedback"},
  journal	= {PLoS Computational Biology},
  year		= {2008},
  volume	= {4},
  number	= {10},
  pages		= {}
}

@Article{LegensteinETAL:08c,
  author	= {R. Legenstein and S. A. Chase and A. B. Schwartz and W.
		  Maass},
  title		= {A model for learning effects in motor cortex that may
		  facilitate the brain control of neuroprosthetic devices},
  journal	= {38th Annual Conference of the Society for Neuroscience,
		  Program 517.6},
  year		= {2008},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {Recent experimental results have shown that the direction
		  preference of neurons in monkey motor cortex changes in
		  order to compensate for purposeful misreading of preferred
		  directions for brain control of a robot arm. We show that a
		  simple neural network model in combination with a new rule
		  for reward-modulated Hebbian plasticity can explain this
		  effect. This rule requires substantial trial-to-trial
		  variability of the neuronal output for exploration. In
		  contrast to previously proposed rules for reward-modulated
		  Hebbian plasticity, the new rule does not require that the
		  plasticity mechanism `knows' the noise explicitly. It is
		  able to optimize the performance of the model system within
		  biologically realistic periods of time and under high noise
		  levels. When the neuronal noise is fitted to experimental
		  data, the model produces learning effects similar to those
		  found in monkey experiments. We quantified these effects
		  and found a surprisingly good match to those observed in
		  experiments. This study shows that reward-modulated
		  learning can explain detailed experimental results about
		  neuronal tuning changes in a motor control task and
		  suggests that reward-modulated learning is an essential
		  plasticity mechanism in the cortex for the acquisition of
		  goal-directed behavior. Self-tuning effects of the type
		  considered in this model are obviously important for
		  successful use of neuroprosthetic devices.}
}

@InProceedings{LegensteinETAL:09,
  author	= {B. Schrauwen and L. Buesing and R. Legenstein},
  title		= {On Computational Power and the Order-Chaos Phase
		  Transition in Reservoir Computing},
  booktitle	= {Proc. of NIPS 2008, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2009},
  volume	= {21},
  pages		= {1425--1432},
  abstract	= {Randomly connected recurrent neural circuits have proven
		  to be very powerful models for online computations when a
		  trained memoryless readout function is appended. Such {\em
		  Reservoir Computing (RC)} systems are commonly used in two
		  flavors: with analog or binary (spiking) neurons in the
		  recurrent circuits. Previous work showed a fundamental
		  difference between these two incarnations of the RC idea.
		  The performance of a RC system build from binary neurons
		  seems to depend strongly on the network connectivity
		  structure. In networks of analog neurons such dependency
		  has not been observed. In this article we investigate this
		  apparent dichotomy in terms of the in-degree of the circuit
		  nodes. Our analyses based amongst others on the Lyapunov
		  exponent reveal that the phase transition between ordered
		  and chaotic network behavior of binary circuits
		  qualitatively differs from the one in analog circuits. This
		  explains the observed decreased computational performance
		  of binary circuits of high node in-degree. Furthermore, a
		  novel mean-field predictor for computational performance is
		  introduced and shown to accurately predict the numerically
		  obtained results.}
}

@InProceedings{LegensteinETAL:09a,
  author	= {R. Legenstein and S. A. Chase and A. B. Schwartz and W.
		  Maass},
  title		= {Functional network reorganization in motor cortex can be
		  explained by reward-modulated {H}ebbian learning},
  booktitle	= {Proc. of NIPS 2009: Advances in Neural Information
		  Processing Systems},
  editor	= {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
  publisher	= {MIT Press},
  year		= {2010},
  volume	= {22},
  pages		= {1105--1113},
  abstract	= {The control of neuroprosthetic devices from the activity
		  of motor cortex neurons benefits from learning effects
		  where the function of these neurons is adapted to the
		  control task. It was recently shown that tuning properties
		  of neurons in monkey motor cortex are adapted selectively
		  in order to compensate for an erroneous interpretation of
		  their activity. In particular, it was shown that the tuning
		  curves of those neurons whose preferred directions had been
		  misinterpreted changed more than those of other neurons. In
		  this article, we show that the experimentally observed
		  self-tuning properties of the system can be explained on
		  the basis of a simple learning rule. This learning rule
		  utilizes neuronal noise for exploration and performs
		  Hebbian weight updates that are modulated by a global
		  reward signal. In contrast to most previously proposed
		  reward-modulated Hebbian learning rules, this rule does not
		  require extraneous knowledge about what is noise and what
		  is signal. The learning rule is able to optimize the
		  performance of the model system within biologically
		  realistic periods of time and under high noise levels. When
		  the neuronal noise is fitted to experimental data, the
		  model produces learning effects similar to those found in
		  monkey experiments.}
}

@Article{LegensteinETAL:09b,
  author	= {R. Legenstein and S. M. Chase and A. B. Schwartz and W.
		  Maass},
  title		= {A reward-modulated {H}ebbian learning rule can explain
		  experimentally observed network reorganization in a brain
		  control task},
  journal	= {The Journal of Neuroscience},
  year		= {2010},
  volume	= {30},
  number	= {25},
  pages		= {8400--8410},
  abstract	= {It has recently been shown in a brain-computer interface
		  experiment that motor cortical neurons change their tuning
		  properties selectively to compensate for errors induced by
		  displaced decoding parameters. In particular, it was shown
		  that the three-dimensional tuning curves of neurons whose
		  decoding parameters were reassigned changed more than those
		  of neurons whose decoding parameters had not been
		  reassigned. In this article, we propose a simple learning
		  rule that can reproduce this effect. Our learning rule uses
		  Hebbian weight updates driven by a global reward signal and
		  neuronal noise. In contrast to most previously proposed
		  learning rules, this approach does not require extrinsic
		  information to separate noise from signal. The learning
		  rule is able to optimize the performance of a model system
		  within biologically realistic periods of time under high
		  noise levels. Furthermore, when the model parameters are
		  matched to data recorded during the brain-computer
		  interface learning experiments described above, the model
		  produces learning effects strikingly similar to those found
		  in the experiments.}
}

@InProceedings{LegensteinETAL:09sup,
  author	= {B. Schrauwen and L. Buesing and R. Legenstein},
  title		= {Supplementary Material to: On Computational Power and the
		  Order-Chaos Phase Transition in Reservoir Computing},
  booktitle	= {Proc. of NIPS 2008, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2009},
  volume	= {21},
  note		= {in press},
  pages		= {}
}

@Article{LegensteinETAL:10,
  author	= {R. Legenstein and N. Wilbert and L. Wiskott},
  title		= {Reinforcement Learning on Slow Features of
		  High-Dimensional Input Streams},
  journal	= {PLoS Computational Biology},
  year		= {2010},
  volume	= {6},
  number	= {8},
  pages		= {e1000894},
  abstract	= {Humans and animals are able to learn complex behaviors
		  based on a massive stream of sensory information from
		  different modalities. Early animal studies have identified
		  learning mechanisms that are based on reward and punishment
		  such that animals tend to avoid actions that lead to
		  punishment whereas rewarded actions are reinforced.
		  However, most algorithms for reward-based learning are only
		  applicable if the dimensionality of the state-space is
		  sufficiently small or its structure is sufficiently simple.
		  Therefore, the question arises how the problem of learning
		  on high-dimensional data is solved in the brain. In this
		  article we propose a biologically plausible generic
		  two-stage learning system that can directly be applied to
		  raw high-dimensional input streams. The system is composed
		  of a hierarchical slow feature analysis (SFA) network for
		  preprocessing and a simple neural network on top that is
		  trained based on rewards. We demonstrate by computer
		  simulations that this generic architecture is able to learn
		  quite demanding reinforcement learning tasks on
		  high-dimensional visual input streams in a time that is
		  comparable to the time needed when an explicit highly
		  informative low-dimensional state-space representation is
		  given instead of the high-dimensional visual input. The
		  learning speed of the proposed architecture in a task
		  similar to the Morris water maze task is comparable to that
		  found in experimental studies with rats. This study thus
		  supports the hypothesis that slowness learning is one
		  important unsupervised learning principle utilized in the
		  brain to form efficient state representations for
		  behavioral learning. }
}

@TechReport{LegensteinMaass:04,
  author	= {R. Legenstein and W. Maass},
  title		= {Additional material to the paper: What can a Neuron Learn
		  with Spike-Timing-Dependent Plasticity?},
  institution	= {Institute for Theoretical Computer Science, Graz
		  University of Technology},
  htmlnote	= {(<a href="./psfiles/154suplement.pdf">PDF</a>)},
  year		= {2004}
}

@InCollection{LegensteinMaass:05,
  author	= {R. Legenstein and W. Maass},
  title		= {What makes a dynamical system computationally powerful?},
  booktitle	= {New Directions in Statistical Signal Processing: From
		  Systems to Brains},
  publisher	= {MIT Press},
  editor	= {S. Haykin and J. C. Principe and T.J. Sejnowski and J.G.
		  McWhirter},
  pages		= {127--154},
  year		= {2007},
  abstract	= {We review methods for estimating the computational
		  capability of a complex dynamical system. The main examples
		  that we discuss are models for cortical neural
		  microcircuits with varying degrees of biological accuracy,
		  in the context of online computations on complex input
		  streams. We address in particular the question to what
		  extent earlier results ab out the relationship between the
		  edge of chaos and the compu tational power of dynamical
		  systems in discrete time for off -line computing also apply
		  to this case.}
}

@Article{LegensteinMaass:05a,
  author	= {R. Legenstein and W. Maass},
  title		= {Edge of Chaos and Prediction of Computational Performance
		  for Neural Circuit Models},
  journal	= {Neural Networks},
  year		= {2007},
  volume	= {20},
  number	= {3},
  pages		= {323--334},
  note		= {},
  abstract	= {We analyze in this article the significance of the edge of
		  chaos for real-time computations in neural microcircuit
		  models consisting of spiking neurons and dynamic synapses.
		  We find that the edge of chaos predicts quite well those
		  values of circuit parameters that yield maximal
		  computational performance. But obviously it makes no
		  prediction of their computational performance for other
		  parameter values. Therefore, we propose a new method for
		  predicting the computational performance of neural
		  microcircuit models. The new measure estimates directly the
		  kernel property and the generalization capability of a
		  neural microcircuit.We validate the proposed measure by
		  comparing its prediction with direct evaluations of the
		  computational performance of various neural microcircuit
		  models. The proposed method also allows us to quantify
		  differences in the computational performance and
		  generalization capability of neural circuits in different
		  dynamic regimes (UP- and DOWN-states) that have been
		  demonstrated through intracellular recordings in vivo.}
}

@Article{LegensteinMaass:07,
  author	= {R. Legenstein and W. Maass},
  title		= {On the classification capability of sign-constrained
		  perceptrons},
  journal	= {Neural Computation},
  volume	= {20},
  number	= {1},
  pages		= {288--309},
  year		= {2008},
  abstract	= {The perceptron (also referred to as McCulloch-Pitts
		  neuron, or linear threshold gate) is commonly used as a
		  simplified model for the discrimination and learning
		  capability of a biological neuron. Criteria that tell us
		  when a perceptron can implement (or learn to implement) all
		  possible dichotomies over a given set of input patterns are
		  well-known, but only for the idealized case where one
		  assumes that the sign of a synaptic weight can be switched
		  during learning. We present in this article an analysis of
		  the classification capability of the biologically more
		  realistic model of a sign-constrained perceptron, where the
		  signs of synaptic weights remain fixed during learning
		  (which is the case for most types of biological synapses).
		  In particular, the VC-dimension of sign-constrained
		  perceptrons is determined, and a necessary and sufficient
		  criterion is provided that tells us when all $2^m$
		  dichotomies over a given set of m patterns can be learned
		  by sign-constrained perceptron. We also show that
		  uniformity of {L1} norms of input patterns is a sufficient
		  condition for full representation power in the case where
		  all weights are required to be nonnegative. Finally, we
		  also exhibit cases where the sign-constraint of a
		  perceptron drastically reduces its classification
		  capability. Our theoretical analysis is complemented by
		  computer simulations, which demonstrate in particular that
		  sparse input patterns improve the classification capability
		  of sign-constrained perceptrons.}
}

@Article{LegensteinMaass:09,
  author	= {R. Legenstein and W. Maass},
  title		= {An integrated learning rule for branch strength
		  potentiation and {STDP}},
  journal	= {39th Annual Conference of the Society for Neuroscience,
		  Program 895.20, Poster HH36},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {Recent experimental data (Losonczy, Makara, and Magee,
		  Nature 2008) show that not only the strength of synaptic
		  efficacy is plastic, but also the coupling between
		  dendritic branches and the soma (via dendritic spikes).
		  More precisely, the strength of this coupling can be
		  increased both through a coincidence of dendritic branch
		  activations with action potential generation, and through a
		  coincidence of branch activation with ACh. This effect has
		  been called Branch Strength Potentiation (BSP). We show
		  through theoretical analysis and computer simulations that
		  the learning capability of single neurons is substantially
		  increased if STDP is combined with BSP. More precisely, we
		  show that a simple learning rule, based on a
		  error-minimization principle, contains both BSP and STDP as
		  special cases. The learning rule includes a homeostatic
		  mechanism which acts locally at the site of the dendritic
		  branch. The depression that was observed for
		  post-before-pre pairings in standard STDP experiments is
		  also observed in simulations of this learning rule. It can
		  be explained by the combined effect of this local
		  homeostatic mechanism and the backpropagating action
		  potential. This powerful new learning rule endows single
		  neurons with learning capabilities which were previously
		  unattainable. For example, a single neuron acquires through
		  this new learning rule the capability to solve a "binding
		  problem". I.e., a single neuron can learn to respond to
		  fire upon activation of presynaptic pools A and B, and also
		  upon activation of presynaptic pools C and D, but NOT in
		  response to concurrent activation of presynaptic pools A
		  and C, or B and D. We also consider a variation of this
		  learning rule where changes at synapses and branches are
		  not only based on local activity, but also on a global
		  reward signal that is indicated to the neuron by the
		  concentration of a neuromodulatory signal such as ACh. We
		  show that this biologically plausible learning rule for
		  reward-based learning is much more efficient than
		  previously proposed rules based on simple neuron models
		  without nonlinear branches.}
}

@Article{LegensteinMaass:11,
  author	= {R. Legenstein and W. Maass},
  title		= {Branch-specific plasticity enables self-organization of
		  nonlinear computation in single neurons},
  journal	= {The Journal of Neuroscience},
  year		= {2011},
  volume	= {31},
  number	= {30},
  pages		= {10787--10802},
  abstract	= {It has been conjectured that nonlinear processing in
		  dendritic branches endows individual neurons with the
		  capability to perform complex computational operations that
		  are needed in order to solve for example the binding
		  problem. However, it is not clear how single neurons could
		  acquire such functionality in a self-organized manner,
		  since most theoretical studies of synaptic plasticity and
		  learning concentrate on neuron models without nonlinear
		  dendritic properties. In the meantime, a complex picture of
		  information processing with dendritic spikes and a variety
		  of plasticity mechanisms in single neurons has emerged from
		  experiments. In particular, new experimental data on
		  dendritic branch strength potentiation in rat hippocampus
		  have not yet been incorporated into such models. In this
		  article, we investigate how experimentally observed
		  plasticity mechanisms, such as depolarization-dependent
		  STDP and branch-strength potentiation could be integrated
		  to self-organize nonlinear neural computations with
		  dendritic spikes. We provide a mathematical proof that in a
		  simplified setup these plasticity mechanisms induce a
		  competition between dendritic branches, a novel concept in
		  the analysis of single neuron adaptivity. We show via
		  computer simulations that such dendritic competition
		  enables a single neuron to become member of several
		  neuronal ensembles, and to acquire nonlinear computational
		  capabilities, such as for example the capability to bind
		  multiple input features. Hence our results suggest that
		  nonlinear neural computation may self-organize in single
		  neurons through the interaction of local synaptic and
		  dendritic plasticity mechanisms.},
  note		= {}
}

@Article{LiebeETAL:09,
  author	= {S. Liebe and G. Hoerzer and N.K. Logothetis and W. Maass
		  and G. Rainer},
  title		= {Long range coupling between {V4} and {PF} in theta band
		  during visual short-term memory},
  journal	= {39th Annual Conference of the Society for Neuroscience,
		  Program 652.20, Poster Y31},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {Both extrastriate area V4 and the lateral prefrontal
		  cortex (PF) are thought to be part of a neural network
		  contributing to sensory and mnemonic processing of visual
		  information. However, it is not well understood how V4 and
		  PF might interact during visual memory. Here, we addressed
		  this question by recording Local Field Potentials (LFP)
		  simultaneously in both brain regions while two rhesus
		  monkeys performed a delayed matching-to sample task. In the
		  task, a sample stimulus (250ms) was presented followed by a
		  probe stimulus (600ms) after a delay period (1500ms). A
		  lever press was required if the sample stimulus matched the
		  probe. We assessed coupling between LFP sites within and
		  between the different brain regions by both measuring
		  pair-wise phase-synchrony (phase locking value, PLV) using
		  a wavelet based method and employing a coupling measure
		  that relies on the concept of Granger causality
		  (partial-directed coherence; PDC) using multivariate
		  autoregressive (MVAR) modeling. In both monkeys we
		  consistently found increases in theta-band phase synchrony
		  (3.5-7 Hz) between V4 and PF LFP site pairs during the
		  delay period of the task. Specifically, a significant
		  proportion of pairs (26.1\%, 62/231 for monkey 1 and 25\%,
		  40/160 for monkey 2, p<0.001) showed increased coherence
		  during the delay phase compared to the pre-stimulus
		  baseline period. In contrast, only a small proportion of
		  sites showed significant coupling in gamma (42-97 Hz,
		  5.9\%/13\% for monkeys 1/2, respectively) or beta (16-36Hz,
		  6.9\%/16\%) frequencies. In addition, we obtained
		  comparable results using PDC, which also assesses the
		  directionality of information flow between the brain areas.
		  Our preliminary results indicate that the interaction
		  between V4 and PF during short-term memory might be
		  primarily mediated through neuronal coherence in the theta
		  band. Furthermore, our analyses using MVAR modeling suggest
		  that this interaction can be characterized by a
		  bidirectional information flow between these areas. These
		  findings support the idea that long-range interactions play
		  an important role in short-term maintenance of short-term
		  memory.}
}

@InCollection{Maass:00,
  author	= {W. Maass},
  title		= {Spike trains -- Im {R}hythmus neuronaler {Z}ellen},
  booktitle	= {Katalog der steirischen Landesausstellung gr2000az},
  pages		= {36--42},
  publisher	= {Springer Verlag},
  year		= {2000},
  editor	= {H. Konrad, R. Kriesche}
}

@InCollection{Maass:00a,
  author	= {W. Maass},
  title		= {Lernende {M}aschinen},
  booktitle	= {Katalog der steirischen Landesausstellung gr2000az},
  pages		= {50--56},
  publisher	= {Springer Verlag},
  year		= 2000,
  editor	= {H. Konrad, R. Kriesche}
}

@InCollection{Maass:00b,
  author	= {W. Maass},
  title		= {Neural computation: a research topic for theoretical
		  computer science? {S}ome thoughts and pointers},
  booktitle	= {Current Trends in Theoretical Computer Science, Entering
		  the 21th Century},
  publisher	= {World Scientific Publishing},
  year		= {2001},
  pages		= {680--690},
  editor	= {Rozenberg G. and Salomaa A. and Paun G.}
}

@InProceedings{Maass:00c,
  author	= {W. Maass},
  title		= {Neural computation: a research topic for theoretical
		  computer science? {S}ome thoughts and pointers},
  booktitle	= {Bulletin of the European Association for Theoretical
		  Computer Science (EATCS)},
  pages		= {149--158},
  year		= {2000},
  volume	= {72}
}

@InProceedings{Maass:01a,
  author	= {W. Maass},
  title		= {wetware ({E}nglish version)},
  booktitle	= {{TAKEOVER}: {W}ho is {D}oing the {A}rt of {T}omorrow
		  ({A}rs {E}lectronica 2001)},
  pages		= {148--152},
  year		= {2001},
  publisher	= {Springer}
}

@InProceedings{Maass:01b,
  author	= {W. Maass},
  title		= {wetware (deutsche {V}ersion)},
  booktitle	= {{TAKEOVER}: {W}ho is {D}oing the {A}rt of {T}omorrow
		  ({A}rs {E}lectronica 2001)},
  year		= {2001},
  pages		= {153--157},
  publisher	= {Springer}
}

@Article{Maass:02,
  author	= {W. Maass},
  title		= {Computing with Spikes},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  year		= {2002},
  pages		= {32--36},
  volume	= {8},
  number	= {1}
}

@InProceedings{Maass:02a,
  author	= {W. Maass},
  title		= {On the Computational Power of Neural Microcircuit Models:
		  Pointers to the Literature},
  booktitle	= {<a
		  href="http://www.springer.de/comp/lncs/index.html">Proc. of
		  the International Conference on Artificial Neural Networks
		  -- ICANN 2002</a>},
  pages		= {254--256},
  year		= {2002},
  editor	= {Jos\'{e} R. Dorronsoro},
  volume	= {2415},
  series	= {Lecture Notes in Computer Science},
  publisher	= {Springer}
}

@InCollection{Maass:03,
  author	= {W. Maass},
  title		= {Computation with Spiking Neurons},
  booktitle	= {<a href="http://mitpress.mit.edu/0262011972">The Handbook
		  of Brain Theory and Neural Networks</a>},
  edition	= {2nd},
  publisher	= {MIT Press (Cambridge)},
  editor	= {M. A. Arbib},
  year		= {2003},
  pages		= {1080--1083}
}

@Article{Maass:06,
  author	= {W. Maass},
  title		= {Book Review of "{I}mitation of life: how biology is
		  inspiring computing" by {N}ancy {F}orbes},
  journal	= {Pattern Analysis and Applications},
  year		= {2006},
  volume	= {8},
  number	= {4},
  pages		= {390--391},
  note		= {Springer (London)}
}

@InProceedings{Maass:07,
  author	= {W. Maass},
  booktitle	= {Proceedings of the Conference CiE'07: {COMPUTABILITY IN
		  EUROPE} 2007, Siena (Italy)},
  title		= {Liquid Computing},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  volume	= {},
  year		= {2007},
  pages		= {507--516},
  abstract	= {This review addresses structural differences between that
		  type of computation on which computability theory and
		  computational complexity theory have focused so far, and
		  those computations that are usually carried out in
		  biological organisms (either in the brain, or in the form
		  of gene regulation within a single cell). These differences
		  concern the role of time, the way in which the input is
		  presented, the way in which an algorithm is implemented,
		  and in the end also the definition of what a computation
		  is. This article describes liquid computing as a new
		  framework for analyzing those types of computations that
		  are usually carried out in biological organisms.}
}

@InCollection{Maass:09,
  author	= {W. Maass},
  title		= {Liquid State Machines: Motivation, Theory, and
		  Applications},
  booktitle	= {Computability in Context: Computation and Logic in the
		  Real World},
  editor	= {B. Cooper and A. Sorbi},
  year		= {2010},
  publisher	= {Imperial College Press},
  pages		= {275--296},
  keywords	= {},
  note		= {},
  abstract	= {The Liquid State Machine (LSM) has emerged as a
		  computational model that is more adequate than the Turing
		  machine for describing computations in biological networks
		  of neurons. Characteristic features of this new model are
		  (i) that it is a model for adaptive computational systems,
		  (ii) that it provides a method for employing randomly
		  connected circuits, or eve "found" physical objects for
		  meaningful computations, (iii) that it provides a
		  theoretical context where heterogeneous, rather than
		  stereotypical, local gates or processors increase the
		  computational power of a circuit, (iv) that it provides a
		  method for multiplexing different computations (on a common
		  input) within the same circuit. This chapter reviews the
		  motivation for this model, its theoretical background, and
		  current work on implementations of this model in innovative
		  artificial computing devices.}
}

@InProceedings{Maass:75,
  author	= {W. Maass},
  booktitle	= {Proof Theory Symposium Kiel 1974},
  editor	= {J. Diller and G. H. Mueller},
  journal	= {Lecture Notes in Mathematics, Springer (Berlin)},
  pages		= {257--263},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Mathematics},
  title		= {Church Rosser Theorem fuer Lambda-Kalkuele mit unendlich
		  langen Termen},
  volume	= {500},
  year		= {1975}
}

@Article{Maass:76,
  author	= {W. Maass},
  journal	= {Archive Math. Logik Grundlagen},
  pages		= {27--46},
  title		= {Eine {F}unktionalinterpretation der praedikativen
		  {A}nalysis},
  volume	= {18},
  year		= {1976}
}

@Article{Maass:77,
  author	= {W. Maass},
  journal	= {Archive Math. Logik Grundlagen},
  pages		= {169--186},
  title		= {On minimal pairs and minimal degrees in higher recursion
		  theory},
  volume	= {18},
  year		= {1977}
}

@Article{Maass:78,
  author	= {W. Maass},
  journal	= {Ann. of Math. Logic},
  pages		= {149--170},
  title		= {Inadmissibility, tame r.e. sets and the admissible
		  collapse},
  volume	= {13},
  year		= {1978}
}

@Article{Maass:78a,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {270-279},
  title		= {The uniform regular set theorem in alpha-recursion
		  theory},
  volume	= {43},
  year		= {1978}
}

@InProceedings{Maass:78b,
  author	= {W. Maass},
  booktitle	= {Higher Set Theory},
  editor	= {G. H. Mueller and D. Scott},
  pages		= {339--359},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Mathematics},
  title		= {Fine structure theory of the constructible universe in
		  alpha- and beta-recursion theory},
  volume	= {669},
  year		= {1978}
}

@MastersThesis{Maass:78c,
  author	= {W. Maass},
  note		= {Minerva Publikation (Muenchen)},
  school	= {Ludwig-Maximilians-Universitaet Muenchen},
  title		= {Contributions to alpha- and beta-recursion theory},
  type		= {Habilitationsschrift},
  year		= {1978}
}

@Conference{GuptaMaass:91,
  author	= {A. Gupta and W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {R. P. Lippmann and J. E. Moody and D. S. Touretzky},
  pages		= {825--831},
  publisher	= {Morgan Kaufmann, (San Mateo)},
  title		= {A method for the efficient design of {B}oltzmann machines
		  for classification problems},
  volume	= {3},
  year		= {1991}
}

@InCollection{Maass:78d,
  author	= {W. Maass},
  booktitle	= {Generalized Recursion Theory II},
  editor	= {E. Fenstad and R. O. Gandy and G. E. Sacks},
  pages		= {239--269},
  publisher	= {North-Holland (Amsterdam)},
  title		= {High alpha-recursively enumerable degrees},
  year		= {1978}
}

@Article{Maass:79,
  author	= {W. Maass},
  journal	= {Ann. of Math. Logic},
  pages		= {205--231},
  title		= {On alpha- and beta-recursively enumerable degrees},
  volume	= {16},
  year		= {1979}
}

@InProceedings{Maass:81,
  author	= {W. Maass},
  booktitle	= {Proceedings of the Conf. on Recursion Theory and
		  Computational Complexity},
  editor	= {G. Lolli},
  pages		= {229--236},
  publisher	= {Liguori editore (Napoli)},
  title		= {Recursively invariant beta-recursion theory -- a
		  preliminary survey},
  year		= {1981}
}

@Article{Maass:81a,
  author	= {W. Maass},
  journal	= {Proceedings Amer. Math. Soc.},
  pages		= {267--270},
  title		= {A countable basis for sigma-one-two sets and recursion
		  theory on aleph-one},
  volume	= {82},
  year		= {1981}
}

@Article{Maass:81b,
  author	= {W. Maass},
  journal	= {Ann. of Math. Logic},
  pages		= {27--73},
  title		= {Recursively invariant beta-recursion theory},
  volume	= {21},
  year		= {1981}
}

@Article{Maass:83,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {809--823},
  title		= {Recursively enumerable generic sets},
  volume	= {47},
  year		= {1983}
}

@Article{Maass:83a,
  author	= {W. Maass},
  journal	= {Trans. Amer. Math. Soc.},
  pages		= {311--336},
  title		= {Characterization of recursively enumerable sets with
		  supersets effectively isomorphic to all recursively
		  enumerable sets},
  volume	= {279},
  year		= {1983}
}

@Article{Maass:84,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {51--62},
  title		= {On the orbits of hyperhypersimple sets},
  volume	= {49},
  year		= {1984}
}

@InProceedings{Maass:84a,
  author	= {W. Maass},
  booktitle	= {Proceedings of 16th Annual ACM Symp. on Theory of
		  Computing},
  pages		= {401--408},
  title		= {Quadratic lower bounds for deterministic and
		  nondeterministic one-tape {T}uring machines},
  abstract	= {We introduce new techniques for proving quadratic lower
		  bounds for deterministic and nondeterministic i-tape Turing
		  machines (all considered Turing machines have an additional
		  oneway input tape). In particular we produce quadratic
		  lower bounds for the simulation of 2-tape TM's by l-tape
		  TM's and thus answer a rather old question (problem No.1
		  and No.7 in the l i s t of Duris, Galil, Paul, Reischuk
		  [3]). Further we demo6strate a substantial superiority of
		  nondeterminism over determinism and of co-nondeterminism
		  over nondeterminism for l-tape TM's.},
  year		= {1984}
}

@Article{Maass:85,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {138--148},
  title		= {Variations on promptly simple sets},
  volume	= {50},
  year		= {1985}
}

@Article{Maass:85a,
  author	= {W. Maass},
  journal	= {Proceedings of Symposia in Pure Mathematics},
  pages		= {21--32},
  title		= {Major subsets and automorphisms of recursively enumerable
		  sets},
  volume	= {42},
  year		= {1985}
}

@Article{Maass:85b,
  author	= {W. Maass},
  journal	= {Transactions of the American Mathematical Society},
  pages		= {675--693},
  title		= {Combinatorial lower bound arguments for deterministic and
		  nondeterministic {T}uring machines},
  abstract	= {We introduce new techniques for proving quadratic lower
		  bounds for deterministic and nondeterminisitc 1-tape
		  {T}uring machines (all considered {T}uring machines have an
		  additional one-way input tape). In particular, we derive
		  for the simulation of 2-tape {T}uring machines by 1-tape
		  {T}uring machines an optimal quadratic lower bound in the
		  deterministic case and a nearly optimal lower bound in the
		  nonderterministic case. This answers the rather old
		  question whether the computing power of the considered
		  types of {T}uring machines is significantly increased when
		  more than one tape is used (problem Nos. 1 and 7 in the
		  list of {D}uris, {G}alil, {P}aul, {R}eischuk [3]). Further,
		  we demonstrate a substantial superiority of nonderterminism
		  over determinism and of co-nondeterminism over
		  nondeterminism for 1-tage {T}uring machines},
  volume	= {292},
  number	= {2},
  year		= {1985},
  note		= {hard copy}
}

@InProceedings{Maass:86,
  author	= {W. Maass},
  booktitle	= {Proceedings of the International Conference on Logic,
		  Methodology and Philosphy of Science, Salzburg 1983},
  pages		= {141--158},
  publisher	= {North-Holland (Amsterdam)},
  title		= {Are recursion theoretic arguments useful in complexity
		  theory},
  year		= {1986}
}

@Article{Maass:86a,
  author	= {W. Maass},
  journal	= {SIAM J. Computing},
  pages		= {453--467},
  title		= {On the complexity of nonconvex covering},
  volume	= {15},
  year		= {1986}
}

@Article{Maass:88,
  author	= {W. Maass},
  journal	= {J. Symbolic Logic},
  pages		= {1098--1109},
  title		= {On the use of inaccessible numbers and order
		  indiscernibles in lower bound arguments for random access
		  machines},
  volume	= {53},
  year		= {1988}
}

@InProceedings{Maass:91,
  author	= {W. Maass},
  booktitle	= {Proceedings of the 4th Annual ACM Workshop on
		  Computational Learning Theory},
  pages		= {167--175},
  publisher	= {Morgan Kaufmann (San Mateo)},
  title		= {On-line learning with an oblivious environment and the
		  power of randomization},
  year		= {1991}
}

@InProceedings{Maass:93,
  author	= {W. Maass},
  booktitle	= {Proceedings of the 25th Annual ACM Symposium on Theory
		  Computing},
  pages		= {335-344},
  title		= {Bounds for the computational power and learning complexity
		  of analog neural nets},
  year		= {1993}
}

@Article{Maass:93j,
  author	= {W. Maass},
  title		= {Bounds for the computational power and learning complexity
		  of analog neural nets},
  journal	= {SIAM J. on Computing},
  year		= 1997,
  volume	= 26,
  number	= 3,
  pages		= {708--732}
}

@InProceedings{Maass:94,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {311--318},
  title		= {Agnostic {PAC}-learning of functions on analog neural
		  nets},
  editors	= {G. Tesauro and D. S. Touretzky and T. K. Leen},
  volume	= {7},
  year		= {1995}
}

@InProceedings{Maass:94a,
  author	= {W. Maass},
  booktitle	= {Proceedings of the International Conference on Artificial
		  Neural Networks 1994 (ICANN'94)},
  pages		= {581--584},
  publisher	= {Springer (Berlin)},
  title		= {Neural nets with superlinear {VC}-dimension},
  year		= {1994}
}

@InCollection{Maass:94b,
  author	= {W. Maass},
  booktitle	= {Theoretical Advances in Neural Computation and Learning},
  editor	= {V. P. Roychowdhury and K. Y. Siu and A. Orlitsky},
  pages		= {295-336},
  publisher	= {Kluwer Academic Publishers (Boston)},
  title		= {Perspectives of current research about the complexity of
		  learning on neural nets},
  year		= {1994}
}

@InProceedings{Maass:94c,
  author	= {W. Maass},
  booktitle	= {Theoretical Andvances in Neural Computation and Learning},
  editor	= {V. P. Roychowdhury and K. Y. Siu and A. Orlitsky},
  pages		= {153--172},
  publisher	= {Kluwer Academics Publisher (Boston)},
  title		= {Computing on analog neural nets with arbitrary real
		  weights},
  year		= {1994}
}

@InProceedings{Maass:94d,
  author	= {W. Maass},
  booktitle	= {Proc. of the 7th Annual ACM Conference on Computational
		  Learning Theory},
  pages		= {67--75},
  title		= {Efficient agnostic {PAC}-learning with simple hypotheses},
  year		= {1994}
}

@InCollection{Maass:94e,
  author	= {W. Maass},
  booktitle	= {Computational Learning Theory: EuroColt'93},
  editor	= {J. Shawe-Taylor and M. Anthony},
  pages		= {1--17},
  publisher	= {Oxford University Press (Oxford)},
  title		= {On the complexity of learning on neural nets},
  year		= {1994}
}

@Article{Maass:94f,
  author	= {W. Maass},
  title		= {Agnostic {PAC}-learning of functions on analog neural
		  nets},
  journal	= {Neural Computation},
  year		= 1995,
  volume	= 7,
  pages		= {1054--1078}
}

@Article{Maass:94j,
  author	= {W. Maass},
  title		= {Neural nets with superlinear {VC}-dimension},
  journal	= {Neural Computation},
  year		= 1994,
  volume	= 6,
  pages		= {877--884}
}

@InProceedings{Maass:95,
  author	= {W. Maass},
  booktitle	= {Proc. of the 7th Italian Workshop on Neural Nets 1995},
  pages		= {99--104},
  publisher	= {World Scientific (Singapore)},
  title		= {Analog computations on networks of spiking neurons
		  (extended abstract)},
  year		= {1996}
}

@InCollection{Maass:95a,
  author	= {W. Maass},
  booktitle	= {The Handbook of Brain Theory and Neural Networks},
  editor	= {M.~A.~Arbib},
  pages		= {1000--1003},
  publisher	= {MIT Press (Cambridge)},
  title		= {Vapnik-{C}hervonenkis dimension of neural nets},
  year		= {1995}
}

@InProceedings{Maass:95b,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {G. Tesauro and D. S. Touretzky and T. K. Leen},
  pages		= {183--190},
  publisher	= {MIT Press (Cambridge)},
  title		= {On the computational complexity of networks of spiking
		  neurons},
  volume	= {7},
  year		= {1995}
}

@Article{Maass:95c,
  author	= {W. Maass},
  journal	= {Telematik},
  pages		= {53--60},
  volume	= {1},
  title		= {{N}euronale {N}etze und {M}aschinelles {L}ernen am
		  {I}nstitut fuer {G}rundlagen der {I}nformationsverarbeitung
		  an der {T}echnischen {U}niversitaet {G}raz},
  year		= {1995}
}

@Article{Maass:96,
  author	= {W. Maass},
  journal	= {Neural Computation},
  pages		= {1--40},
  title		= {Lower Bounds for the Computational Power of Networks of
		  Spiking Neurons},
  volume	= {8},
  number	= {1},
  year		= {1996}
}

@InProceedings{Maass:96a,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {D. Touretzky and M. C. Mozer and M. E. Hasselmo},
  pages		= {211--217},
  publisher	= {MIT Press (Cambridge)},
  title		= {On the computational power of noisy spiking neurons},
  volume	= {8},
  year		= {1996}
}

@InProceedings{Maass:96b,
  author	= {W. Maass},
  title		= {Networks of spiking neurons: the third generation of
		  neural network models},
  booktitle	= {Proc. of the 7th Australian Conference on Neural Networks
		  1996 in Canberra, Australia},
  pages		= {1-10},
  year		= {1996}
}

@Article{Maass:97a,
  author	= {W. Maass},
  journal	= {Neural Computation},
  pages		= {279--304},
  title		= {Fast sigmoidal networks via spiking neurons},
  volume	= {9},
  year		= {1997}
}

@Article{Maass:97b,
  author	= {W. Maass},
  howpublished	= {FTP-host: archive.cis.ohio-state.edu FTP-filename:
		  /pub/neuroprose/maass.third-generation.ps.Z},
  journal	= {Neural Networks},
  pages		= {1659--1671},
  title		= {Networks of spiking neurons: the third generation of
		  neural network models},
  volume	= {10},
  year		= {1997}
}

@InProceedings{Maass:97c,
  author	= {W. Maass},
  booktitle	= {Computational Neuroscience: Trends in research},
  editor	= {James Bower},
  pages		= {123--127},
  title		= {A model for fast analog computations with noisy spiking
		  neurons},
  year		= {1997}
}

@InCollection{Maass:97d,
  author	= {W. Maass},
  booktitle	= {Spatiotemporal Models in Biological and Artificial
		  Systems},
  editor	= {F. L. Silva},
  pages		= {97-104},
  publisher	= {IOS-Press},
  title		= {Analog computations with temporal coding in networks of
		  spiking neurons},
  year		= {1997}
}

@InProceedings{Maass:97e,
  author	= {W. Maass},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {M. Mozer and M. I. Jordan and T. Petsche},
  pages		= {211--217},
  publisher	= {MIT Press (Cambridge)},
  title		= {Noisy spiking neurons with temporal coding have more
		  computational power than sigmoidal neurons},
  volume	= {9},
  year		= {1997}
}

@InProceedings{Maass:97f,
  author	= {W. Maass},
  booktitle	= {Proc. of the 8th International Conference on Algorithmic
		  Learning Theory in Sendai (Japan)},
  editor	= {M. Li and A. Maruoka},
  pages		= {364--384},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {On the relevance of time in neural computation and
		  learning},
  volume	= {1316},
  year		= {1997}
}

@Article{Maass:97g,
  author	= {W. Maass},
  title		= {On the relevance of time in neural computation and
		  learning},
  journal	= {Theoretical Computer Science},
  year		= 2001,
  volume	= {261},
  pages		= {157-178}
}

@InProceedings{Maass:98a,
  author	= {W. Maass},
  booktitle	= {Proc. of the Federated Conference of CLS'98 and MFCS'98,
		  Mathematical Foundations of Computer Science 1998},
  title		= {On the role of time and space in neural computation},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  volume	= {1450},
  year		= {1998},
  pages		= {72-83},
  note		= {Invited talk}
}

@Article{Maass:98b,
  author	= {W. Maass},
  journal	= {Network: Computation in Neural Systems},
  number	= {3},
  pages		= {381-397},
  title		= {A simple model for neural computation with firing rates
		  and firing correlations},
  volume	= {9},
  year		= {1998}
}

@InProceedings{Maass:98d,
  author	= {W. Maass},
  title		= {Models for fast analog computation with spiking neurons},
  booktitle	= {Proc. of the International Conference on Neural
		  Information Processing 1998 (ICONIP'98) in Kytakyusyu,
		  Japan},
  pages		= {187--188},
  year		= 1998,
  publisher	= {IOS Press (Amsterdam)},
  note		= {Invited talk at the special session on ``Dynamic Brain''}
}

@InProceedings{Maass:98e,
  author	= {W. Maass},
  title		= {Spiking neurons},
  booktitle	= {Proceedings of the ICSC/IFAC Symposium on Neural
		  Computation 1998 (NC'98)},
  pages		= {16--20},
  year		= 1998,
  publisher	= {ICSC Academic Press (Alberta)},
  note		= {Invited talk}
}

@InCollection{Maass:98f,
  author	= {W. Maass},
  title		= {Computing with spiking neurons},
  booktitle	= {Pulsed Neural Networks},
  pages		= {55--85},
  publisher	= {MIT Press (Cambridge)},
  year		= {1999},
  editor	= {W. Maass and C.~M.~Bishop}
}

@InProceedings{Maass:99,
  author	= {W. Maass},
  title		= {Neural Computation with Winner-Take-All as the only
		  Nonlinear Operation},
  booktitle	= {Advances in Information Processing Systems},
  editor	= {Sara A. Solla and Todd K. Leen and Klaus-Robert Mueller},
  volume	= {12},
  publisher	= {MIT Press (Cambridge)},
  year		= {2000},
  pages		= {293--299}
}

@InCollection{Maass:99a,
  author	= {W. Maass},
  title		= {Das menschliche {G}ehirn -- nur ein {R}echner?},
  booktitle	= {Zur Kunst des Formalen Denkens},
  publisher	= {Passagen Verlag (Wien)},
  year		= 2000,
  pages		= {209-233},
  editor	= {R. E. Burkard and W. Maass and P. Weibel}
}

@InCollection{Maass:99c,
  author	= {W. Maass},
  title		= {Paradigms for computing with spiking neurons},
  booktitle	= {Models of Neural Networks. Early Vision and Attention},
  publisher	= {Springer (New York)},
  year		= {2002},
  editor	= {J. L. van Hemmen and J. D. Cowan and E. Domany},
  volume	= {4},
  chapter	= {9},
  pages		= {373--402}
}

@Article{Maass:99e,
  author	= {W. Maass},
  title		= {On the computational power of winner-take-all},
  year		= {2000},
  journal	= {Neural Computation},
  volume	= {12},
  number	= {11},
  pages		= {2519--2535}
}

@Article{MaassETAL:00,
  author	= {W. Maass and A. Pinz and R. Braunstingl and G. Wiesspeiner
		  and T. Natschlaeger and O. Friedl and H. Burgsteiner},
  title		= {Konstruktion von {L}ernfaehigen {R}obotern im
		  {S}tudentenwettbewerb ``{R}obotik 2000'' an der
		  {T}echnischen {U}niversitaet {G}raz},
  journal	= {in: Telematik},
  year		= {2000},
  pages		= {20--24}
}

@Article{MaassETAL:01a,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {Real-time Computing Without Stable States: A New Framework
		  for Neural Computation Based on Perturbations},
  journal	= {Neural Computation},
  volume	= 14,
  number	= 11,
  pages		= {2531-2560},
  year		= 2002,
  abstract	= {A key challenge for neural modeling is to explain how a
		  continuous stream of multi-modal input from a rapidly
		  changing environment can be processed by stereotypical
		  recurrent circuits of integrate-and-fire neurons in
		  real-time. We propose a new framework for neural
		  computation that provides an alternative to previous
		  approaches based on attractor neural networks. It is shown
		  that the inherent transient dynamics of the
		  high-dimensional dynamical system formed by a neural
		  circuit may serve as a universal source of information
		  about past stimuli, from which readout neurons can extract
		  particular aspects needed for diverse tasks in real-time.
		  Stable internal states are not required for giving a stable
		  output, since transient internal states can be transformed
		  by readout neurons into stable target outputs due to the
		  high dimensionality of the dynamical system. Our approach
		  is based on a rigorous computational model, the liquid
		  state machine, that unlike Turing machines, does not
		  require sequential transitions between discrete internal
		  states. Like the Turing machine paradigm it allows for
		  universal computational power under idealized conditions,
		  but for real-time processing of time-varying input. The
		  resulting new framework for neural computation has novel
		  implications for the interpretation of neural coding, for
		  the design of experiments and data-analysis in
		  neurophysiology, and for neuromorphic engineering.}
}

@InProceedings{MaassETAL:02,
  author	= {W. Maass and G. Steinbauer and R. Koholka},
  title		= {Autonomous fast learning in a mobile robot},
  booktitle	= {Sensor Based Intelligent Robots. International Workshop,
		  Dagstuhl Castle, Germany, October 15--25, 2000, Selected
		  Revised Papers },
  pages		= {345--356},
  year		= {2002},
  editor	= {G. D. Hager and H. I. Christensen and H. Bunke and R.
		  Klein},
  volume	= {2238},
  series	= lncs,
  publisher	= {<a
		  href="http://www.springer.de/comp/lncs/index.html">Springer</a>
		  (Berlin)}
}

@InProceedings{MaassETAL:02a,
  author	= {W. Maass and R. Legenstein and H. Markram},
  title		= {A New Approach towards Vision suggested by Biologically
		  Realistic Neural Microcircuit Models},
  booktitle	= {Biologically Motivated Computer Vision. Proc. of the
		  Second International Workshop, BMCV 2002, Tuebingen,
		  Germany, November 22--24, 2002},
  editor	= {H. H. Buelthoff and S. W. Lee and T. A. Poggio and C.
		  Wallraven},
  series	= {Lecture Notes in Computer Science},
  volume	= {2525},
  pages		= {282--293},
  year		= {2002},
  publisher	= {Springer (Berlin)},
  abstract	= {We propose an alternative paradigm for processing
		  time-varying visual inputs, in particular for tasks
		  involving temporal and spatial integration, which is
		  inspired by hypotheses about the computational role of
		  cortical microcircuits. Since detailed knowledge about the
		  precise structure of the microcircuit is not needed for
		  that, it can in principle also be implemented with
		  partially unknown or faulty analog hardware. In addition,
		  this approach supports parallel realtime processing of
		  time-varying visual inputs for diverse tasks, since
		  different readouts can be trained to extract concurrently
		  from the same microcircuit completely different information
		  components.}
}

@InProceedings{MaassETAL:02b,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {A Model for Real-Time Computation in Generic Neural
		  Microcircuits},
  booktitle	= {Proc. of NIPS 2002, Advances in Neural Information
		  Processing Systems},
  editor	= {S. Becker and S. Thrun and K. Obermayer},
  publisher	= {MIT Press},
  year		= {2003},
  volume	= {15},
  pages		= {229--236},
  abstract	= {A key challenge for neural modeling is to explain how a
		  continuous stream of multi-modal input from a rapidly
		  changing environment can be processed by stereotypical
		  recurrent circuits of integrate-and-fire neurons in
		  real-time. We propose a new computational model that does
		  not require a task-dependent construction of neural
		  circuits. Instead it is based on principles of high
		  dimensional dynamical systems in combination with
		  statistical learning theory, and can be implemented on
		  generic evolved or found recurrent circuitry.}
}

@Article{MaassETAL:02c,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {Fading Memory and Kernel Properties of Generic Cortical
		  Microcircuit Models},
  journal	= {Journal of Physiology -- Paris},
  year		= {2004},
  pages		= {315--330},
  volume	= {98},
  number	= {4--6},
  abstract	= {It is quite difficult to construct circuits of spiking
		  neurons that can carry out complex computational tasks. On
		  the other hand even randomly connected circuits of spiking
		  neurons can in principle be used for complex computational
		  tasks such as time-warp invariant speech recognition. This
		  is possible because such circuits have an inherent tendency
		  to integrate incoming information in such a way that simple
		  linear readouts can be trained to transform the current
		  circuit activity into the target output for a very large
		  number of computational tasks. Consequently we propose to
		  analyze circuits of spiking neurons in terms of their roles
		  as analog fading memory and nonlinear kernels, rather than
		  as implementations of specific computational operations and
		  algorithms. This article is a sequel to \cite{LSM}, and
		  contains new results about the performance of generic
		  neural microcircuit models for the recognition of speech
		  that is subject to linear and nonlinear time-warps, as well
		  as for computations on time-varying firing rates. These
		  computations rely, apart from general properties of generic
		  neural microcircuit models, just on capabilities of simple
		  linear readouts trained by linear regression. This article
		  also provides detailed data on the fading memory property
		  of generic neural microcircuit models, and a quick review
		  of other new results on the computational power of such
		  circuits of spiking neurons.}
}

@InCollection{MaassETAL:03,
  author	= {W. Maass and T. Natschlaeger and H. Markram},
  title		= {Computational Models for Generic Cortical Microcircuits},
  booktitle	= {Computational Neuroscience: A Comprehensive Approach},
  publisher	= {Chapman \& Hall/CRC},
  year		= {2004},
  editor	= {J. Feng},
  chapter	= {18},
  pages		= {575--605},
  address	= {Boca Raton},
  abstract	= {The human nervous system processes a continuous stream of
		  multi-modal input from a rapidly changing environment. A
		  key challenge for neural modeling is to explain how the
		  neural microcircuits (columns, minicolumns, etc.) in the
		  cerebral cortex whose anatomical and physiological
		  structure is quite similar in many brain areas and species
		  achieve this enormous computational task. We propose a
		  computational model that could explain the potentially
		  universal computational capabilities and does not require a
		  task-dependent construction of neural circuits. Instead it
		  is based on principles of high dimensional dynamical
		  systems in combination with statistical learning theory,
		  and can be implemented on generic evolved or found
		  recurrent circuitry. This new approach towards
		  understanding neural computation on the micro-level also
		  suggests new ways of modeling cognitive processing in
		  larger neural systems. In particular it questions
		  traditional ways of thinking about neural coding.}
}

@InProceedings{MaassETAL:04,
  author	= {W. Maass and R. Legenstein and N. Bertschinger},
  booktitle	= {Advances in Neural Information Processing Systems},
  title		= {Methods for Estimating the Computational Power and
		  Generalization Capability of Neural Microcircuits},
  year		= {2005},
  volume	= {17},
  pages		= {865--872},
  editor	= {L. K. Saul and Y. Weiss and L. Bottou},
  publisher	= {MIT Press},
  abstract	= {What makes a neural microcircuit computationally powerful?
		  Or more precisely, which measurable quantities could
		  explain why one microcircuit $C$ is better suited for a
		  particular family of computational tasks than another
		  microcircuit $C'$? We propose in this article quantitative
		  measures for evaluating the computational power and
		  generalization capability of a neural microcircuit, and
		  apply them to generic neural microcircuit models drawn from
		  different distributions. We validate the proposed measures
		  by comparing their prediction with direct evaluations of
		  the computational performance of these microcircuit models.
		  This procedure is applied first to microcircuit models that
		  differ with regard to the spatial range of synaptic
		  connections and with regard to the scale of synaptic
		  efficacies in the circuit, and then to microcircuit models
		  that differ with regard to the level of background input
		  currents and the level of noise on the membrane potential
		  of neurons. In this case the proposed method allows us to
		  quantify differences in the computational power and
		  generalization capability of circuits in different dynamic
		  regimes (UP- and DOWN-states) that have been demonstrated
		  through intracellular recordings in vivo.}
}

@InProceedings{MaassETAL:06,
  author	= {W. Maass and P. Joshi and E. D. Sontag},
  title		= {Principles of real-time computing with feedback applied to
		  cortical microcircuit models},
  booktitle	= {Advances in Neural Information Processing Systems},
  abstract	= {The network topology of neurons in the brain exhibits an
		  abundance of feedback connections, but the computational
		  function of these feedback connections is largely unknown.
		  We present a computational theory that characterizes the
		  gain in computational power achieved through feedback in
		  dynamical systems with fading memory. It implies that many
		  such systems acquire through feedback universal
		  computational capabilities for analog computing with a
		  non-fading memory. In particular, we show that feedback
		  enables such systems to process time-varying input streams
		  in diverse ways according to rules that are implemented
		  through internal states of the dynamical system. In
		  contrast to previous attractor-based computational models
		  for neural networks, these flexible internal states are
		  {\em high-dimensional} attractors of the circuit dynamics,
		  that still allow the circuit state to absorb new
		  information from online input streams. In this way one
		  arrives at novel models for working memory, integration of
		  evidence, and reward expectation in cortical circuits. We
		  show that they are applicable to circuits of
		  conductance-based Hodgkin-Huxley (HH) neurons with high
		  levels of noise that reflect experimental data on in-vivo
		  conditions. },
  year		= {2006},
  volume	= {18},
  pages		= {835--842},
  publisher	= {MIT Press},
  editor	= {Y. Weiss and B. Schoelkopf and J. Platt}
}

@Article{MaassETAL:06a,
  author	= {W. Maass and P. Joshi and E. D. Sontag},
  title		= {Computational aspects of feedback in neural circuits},
  journal	= {PLoS Computational Biology},
  year		= 2007,
  volume	= {3},
  number	= {1},
  pages		= {e165, 1--20}
}

@Article{MaassETAL:07,
  author	= {H. Jaeger and W. Maass and J. Principe},
  title		= {Introduction to the Special Issue on Echo State Networks
		  and Liquid State Machines},
  journal	= {Neural Networks},
  year		= {2007},
  volume	= {20},
  number	= {3},
  pages		= {287--289},
  note		= {}
}

@Article{MaassETAL:81,
  author	= {W. Maass and A. Shore and M. Stob},
  journal	= {Israel J. Math.},
  pages		= {210--224},
  title		= {Splitting properties and jump classes},
  volume	= {39},
  year		= {1981}
}

@InProceedings{MaassETAL:87,
  author	= {W. Maass and G. Schnitger and E. Szemeredi},
  booktitle	= {Proceedings of the 19th Annual ACM Symposium on Theory of
		  Computing},
  pages		= {94--100},
  title		= {Two tapes are better than one for off-line Turing
		  machines},
  year		= {1987}
}

@InProceedings{MaassETAL:91,
  author	= {W. Maass and G. Schnitger and E. Sontag},
  booktitle	= {Proc. of the 32nd Annual IEEE Symposium on Foundations of
		  Computer Science 1991},
  pages		= {767-776},
  title		= {On the computational power of sigmoid versus boolean
		  threshold circuits},
  year		= {1991}
}

@InCollection{MaassETAL:91a,
  author	= {W. Maass and G. Schnitger and E. Sontag},
  title		= {On the computational power of sigmoid versus boolean
		  threshold circuits},
  booktitle	= {Theoretical Advances in Neural Computation and Learning},
  pages		= {127--151},
  publisher	= {Kluwer Academic Publishers (Boston)},
  year		= {1994},
  editor	= {V.~P.~Roychowdhury and K.~Y.~Siu and A.~Orlitsky}
}

@Article{MaassETAL:93,
  author	= {W. Maass and G. Schnitger and E. Szemeredi and G. Turan},
  journal	= {Computational Complexity},
  pages		= {392--401},
  title		= {Two tapes versus one for off-line {T}uring machines},
  volume	= {3},
  year		= {1993}
}

@InProceedings{MaassLegenstein:01,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Foundations for a circuit complexity theory of sensory
		  processing},
  booktitle	= {Proc. of NIPS 2000, Advances in Neural Information
		  Processing Systems},
  editor	= {T. K. Leen and T. G. Dietterich and V. Tresp},
  year		= {2001},
  volume	= {13},
  pages		= {259--265},
  publisher	= {MIT Press},
  address	= {Cambridge},
  htmlnote	= {The poster presented at NIPS is available as <a
		  href="./psfiles/122_poster.ps.gz">gzipped Postscript</a>.},
  abstract	= {We introduce {\em total wire length} as salient complexity
		  measure for an analysis of the circuit complexity of
		  sensory processing in biological neural systems and
		  neuromorphic engineering. Furthermore we introduce a set of
		  basic computational problems that apparently need to be
		  solved by circuits for translation- and scale-invariant
		  sensory processing. Finally we exhibit a number of circuit
		  design strategies for these new benchmark functions that
		  can be implemented within realistic complexity bounds, in
		  particular with linear or almost linear total wire length.}
}

@Article{MaassLegenstein:01a,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Wire Length as a Circuit Complexity Measure},
  journal	= {Journal of Computer and System Sciences},
  year		= {2005},
  volume	= {70},
  pages		= {53--72},
  abstract	= { We introduce {\em wire length} as a salient complexity
		  measure for analyzing the circuit complexity of sensory
		  processing in biological neural systems. This new
		  complexity measure is applied in this paper to two basic
		  computational problems that arise in translation- and
		  scale-invariant pattern recognition, and hence appear to be
		  useful as benchmark problems for sensory processing. We
		  present new circuit design strategies for these benchmark
		  problems that can be implemented within realistic
		  complexity bounds, in particular with linear or almost
		  linear wire length. Finally we derive some general bounds
		  which provide information about the relationship between
		  new complexity measure wire length and traditional circuit
		  complexity measures.}
}

@Article{MaassLegenstein:01c,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Neural circuits for pattern recognition with small total
		  wire length},
  journal	= {Theoretical Computer Science},
  volume	= {287},
  pages		= {239--249},
  year		= {2002},
  abstract	= {One of the most basic pattern recognition problems is
		  whether a certain local feature occurs in some linear array
		  to the left of some other local feature. We construct in
		  this article circuits that solve this problem with an
		  asymptotically optimal number of threshold gates.
		  Furthermore it is shown that much fewer threshold gates are
		  needed if one employs in addition a small number of
		  winner-take-all gates. In either case the circuits that are
		  constructed have linear or almost linear total wire length,
		  and are therefore not unrealistic from the point of view of
		  physical implementations.}
}

@Article{MaassLegenstein:01d,
  author	= {R. A. Legenstein and W. Maass},
  title		= {Optimizing the Layout of a Balanced Tree},
  journal	= {Technical Report},
  year		= {2001},
  abstract	= { It is shown that the total wire length of layouts of a
		  balanced binary tree on a 2-dimensional grid can be reduced
		  by 33% if one does not choose the obvious ``symmetric''
		  layout strategy. Furthermore it is shown that the more
		  efficient layout strategy that is presented in this article
		  is optimal, not only for binary trees but for m-ary trees
		  with any m >= 2.}
}

@Article{MaassMarkram:00,
  author	= {W. Maass and H. Markram},
  title		= {Synapses as dynamic memory buffers},
  journal	= {Neural Networks},
  volume	= {15},
  pages		= {155--161},
  year		= {2002}
}

@Article{MaassMarkram:02,
  author	= {W. Maass and H. Markram},
  title		= {On the Computational Power of Recurrent Circuits of
		  Spiking Neurons},
  journal	= {Journal of Computer and System Sciences},
  year		= {2004},
  volume	= {69},
  number	= {4},
  pages		= {593--616}
}

@InCollection{MaassMarkram:02a,
  author	= {W. Maass and H. Markram},
  title		= {Temporal Integration in Recurrent Microcircuits},
  booktitle	= {<a href="http://mitpress.mit.edu/0262011972">The Handbook
		  of Brain Theory and Neural Networks</a>},
  publisher	= {MIT Press (Cambridge)},
  year		= {2003},
  editor	= {M. A. Arbib},
  edition	= {2nd},
  pages		= {1159--1163}
}

@InProceedings{MaassMarkram:04,
  author	= {W. Maass and H. Markram},
  title		= {Theory of the Computational Function of Microcircuit
		  Dynamics},
  booktitle	= {The Interface between Neurons and Global Brain Function},
  editor	= {S. Grillner and A. M. Graybiel},
  year		= {2006},
  pages		= {371--390},
  chapter	= {18},
  publisher	= {MIT Press},
  series	= {Dahlem Workshop Report 93}
}

@Article{MaassNatschlaeger:97a,
  author	= {W. Maass and T. Natschlaeger},
  title		= {Networks of Spiking Neurons can Emulate Arbitrary
		  {H}opfield nets in Temporal Coding},
  journal	= {Network: Computation in Neural Systems},
  year		= 1997,
  volume	= 8,
  number	= 4,
  pages		= {355--371},
  keywords	= {spiking neurons, hopfield net, temporal coding},
  userlabel	= {4},
  abstract	= {A theoretical model for analog computation with temporal
		  coding is introduced and tested through simulations in
		  GENESIS. It turns out that the use of multiple synapses
		  yields very noise robust mechanisms for analog computations
		  with temporal coding in networks of detailed compartmental
		  neuron models. One arrives in this way at a method for
		  emulating arbitrary Hopfield nets with spiking neurons in
		  temporal coding, yielding new models for associative recall
		  of spatio-temporal firing patterns. A corresponding layered
		  architecture yields a refinement of the synfire-chain model
		  that can assume a fairly large set of different firing
		  patterns for different inputs.}
}

@InProceedings{MaassNatschlaeger:98,
  author	= {W. Maass and T. Natschlaeger},
  title		= {Associative Memory with Networks of Spiking Neurons in
		  Temporal Coding},
  booktitle	= {Neuromorphic Systems: Engineering Silicon from
		  Neurobiology},
  editor	= {L. S. Smith and A. Hamilton},
  year		= 1998,
  publisher	= {World Scientific},
  pages		= {21--32},
  keywords	= {spiking neurons, hopfield net, temporal coding},
  abstract	= {A theoretical model for analog computation with temporal
		  coding is introduced and tested through simulations in
		  GENESIS. It turns out that the use of multiple synapses
		  yields very noise robust mechanisms for analog computations
		  with temporal coding in networks of detailed compartmental
		  neuron models. One arrives in this way at a method for
		  emulating arbitrary Hopfield nets with spiking neurons in
		  temporal coding, yielding new models for associative recall
		  of spatio-temporal firing patterns.}
}

@InProceedings{MaassNatschlaeger:98b,
  author	= {W. Maass and T. Natschlaeger},
  title		= {Emulation of {H}opfield Networks with Spiking Neurons in
		  Temporal Coding},
  booktitle	= {Computational Neuroscience: Trends in Research},
  editor	= {J. M. Bower},
  year		= 1998,
  publisher	= {Plenum Press},
  pages		= {221--226},
  keywords	= {spiking neurons, hopfield net, temporal coding},
  abstract	= {A theoretical model for analog computation with temporal
		  coding is introduced and tested through simulations in
		  GENESIS. It turns out that the use of multiple synapses
		  yields very noise robust mechanisms for analog computations
		  with temporal coding in networks of detailed compartmental
		  neuron models. One arrives in this way at a method for
		  emulating arbitrary Hopfield nets with spiking neurons in
		  temporal coding, yielding new models for associative recall
		  of spatio-temporal firing patterns. A corresponding layered
		  architecture yields a refinement of the synfire-chain model
		  that can assume a fairly large set of different firing
		  patterns for different inputs.}
}

@Article{MaassNatschlaeger:99,
  author	= {W. Maass and T. Natschlaeger},
  title		= {A model for Fast Analog Computation Based on Unreliable
		  Synapses},
  journal	= {Neural Computation},
  year		= 2000,
  pages		= {1679--1704},
  volume	= {12},
  number	= {7},
  keywords	= {unreliable synapses, universal function approximation,
		  fast analog computation, time series, hebbian learning,
		  space rate coding, population activity},
  abstract	= {We investigate through theoretical analysis and computer
		  simulations the consequences of unreliable synapses for
		  fast analog computations in networks of spiking neurons,
		  with analog variables encoded by the current firing
		  activities of pools of spiking neurons. Our results suggest
		  that the known unreliability of synaptic transmission may
		  be viewed as a useful tool for analog computing, rather
		  than as a ``bug'' in neuronal hardware. We also investigate
		  computations on time series and Hebbian learning in this
		  context of space-rate coding.}
}

@InProceedings{MaassOrponen:97,
  author	= {W. Maass and P. Orponen},
  booktitle	= {Advances in Neural Information Processing Systems},
  editor	= {M. Mozer and M. I. Jordan and T. Petsche},
  pages		= {218--224},
  publisher	= {MIT Press (Cambridge)},
  title		= {On the effect of analog noise in discrete-time analog
		  computations},
  volume	= {9},
  year		= {1997}
}

@Article{MaassOrponen:97j,
  author	= {W. Maass and P. Orponen},
  title		= {On the effect of analog noise in discrete-time analog
		  computations},
  journal	= {Neural Computation},
  year		= 1998,
  volume	= 10,
  pages		= {1071--1095}
}

@InProceedings{MaassRuf:95,
  author	= {W. Maass and B. Ruf},
  address	= {Paris},
  booktitle	= {Proc. of the International Conference on Artificial Neural
		  Networks ICANN},
  pages		= {515--520},
  publisher	= {EC2\&Cie},
  title		= {On the Relevance of the Shape of Postsynaptic Potentials
		  for the Computational Power of Networks of Spiking
		  Neurons},
  year		= {1995},
  keywords	= {spiking neuron, postsynaptic potential, computational
		  complexity}
}

@Article{MaassRuf:97,
  author	= {W. Maass and B. Ruf},
  journal	= {Information and Computation},
  title		= {On computation with pulses},
  year		= {1999},
  volume	= {148},
  pages		= {202--218},
  keywords	= {spiking neuron, computational complexity, postsynaptic
		  potential}
}

@InProceedings{MaassSchmitt:97,
  author	= {W. Maass and M. Schmitt},
  booktitle	= {Proc. of the 10th Conference on Computational Learning
		  Theory 1997},
  note		= {See also Electronic Proc. of the Fifth International
		  Symposium on Artificial Intelligence and Mathematics
		  (http://rutcor.rutgers.edu/\~{}amai)},
  pages		= {54--61},
  publisher	= {ACM-Press (New York)},
  title		= {On the complexity of learning for a spiking neuron},
  year		= {1997}
}

@Article{MaassSchmitt:98,
  author	= {W. Maass and M. Schmitt},
  journal	= {Information and Computation},
  title		= {On the complexity of learning for spiking neurons with
		  temporal coding},
  year		= {1999},
  volume	= {153},
  pages		= {26--46}
}

@InProceedings{MaassSchnitger:86,
  author	= {W. Maass and G. Schnitger},
  booktitle	= {Proceedings of the Structure in Complexity Theory
		  Conference, Berkeley 1986},
  pages		= {249--264},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {An optimal lower bound for {T}uring machines with one Work
		  Tape and Two-Way Input Tape},
  year		= {1986},
  volume	= {223}
}

@Article{MaassSchorr:87,
  author	= {W. Maass and A. Schorr},
  journal	= {SIAM J. Comput.},
  pages		= {195--202},
  title		= {Speed-up of {T}uring machines with one work tape and a
		  two-way input tape},
  volume	= {16},
  year		= {1987}
}

@InProceedings{MaassSlaman:89,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the Logic Colloquium '88, Padova, Italy},
  editor	= {Ferro and Bonotto and Valentini and Zanardo},
  pages		= {79-89},
  publisher	= {Elsevier Science Publishers (North-Holland)},
  title		= {Some problems and results in the theory of actually
		  computable functions},
  year		= {1989}
}

@InProceedings{MaassSlaman:89a,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the 4th Annual Conference on Structure in
		  Complexity Theory},
  pages		= {231--239},
  publisher	= {IEEE Computer Society Press (Washington)},
  title		= {The complexity types of computable sets (extended
		  abstract)},
  year		= {1989}
}

@InProceedings{MaassSlaman:89b,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the 7th International Conference on
		  Fundamentals of Computation Theory},
  pages		= {318--326},
  publisher	= {Springer (Berlin)},
  series	= {Lecture Notes in Computer Science},
  title		= {Extensional properties of sets of time bounded complexity
		  (extended abstract)},
  volume	= {380},
  year		= {1989}
}

@InProceedings{MaassSlaman:90,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of the 1989 Recursion Theory Week
		  Oberwolfach},
  pages		= {297--322},
  publisher	= {Springer (Berlin)},
  title		= {On the relationship between the complexity, the degree,
		  and the extension of a computable set},
  year		= {1990}
}

@InProceedings{MaassSlaman:91,
  author	= {W. Maass and T. A. Slaman},
  booktitle	= {Proceedings of a Workshop on Logic from Computer Science},
  editor	= {Y. N. Moschovakis},
  pages		= {359--372},
  publisher	= {Springer (Berlin)},
  title		= {Splitting and density for the recursive sets of a fixed
		  time complexity},
  year		= {1991}
}

@Article{MaassSlaman:92,
  author	= {W. Maass and T. A. Slaman},
  journal	= {J. Comput. Syst. Sci.},
  note		= {Invited paper for a special issue of the J. Comput. Syst.
		  Sci.},
  pages		= {168--192},
  title		= {The complexity types of computable sets},
  volume	= {44},
  year		= {1992}
}

@Article{MaassSontag:97,
  author	= {W. Maass and E. Sontag},
  journal	= {Neural Computation},
  title		= {Analog neural nets with {G}aussian or other common noise
		  distributions cannot recognize arbitrary regular
		  languages},
  year		= {1999},
  volume	= {11},
  pages		= {771--782}
}

@Article{MaassSontag:99a,
  author	= {W. Maass and E. D. Sontag},
  title		= {Neural systems as nonlinear filters},
  journal	= {Neural Computation},
  volume	= {12},
  number	= {8},
  year		= {2000},
  pages		= {1743--1772}
}

@InProceedings{MaassSontag:99b,
  author	= {W. Maass and E. D. Sontag},
  title		= {A precise characterization of the class of languages
		  recognized by neural nets under {G}aussian and other common
		  noise distributions},
  booktitle	= {Advances in Neural Information Processing Systems},
  year		= 1999,
  volume	= {11},
  pages		= {281--287},
  editor	= {M.~S.~Kearns and S.~S.~Solla and D.~A.~Cohn},
  publisher	= {MIT Press (Cambridge)}
}

@Article{MaassStob:83,
  author	= {W. Maass and M. Stob},
  journal	= {Ann. of Pure and Applied Logic},
  pages		= {189--212},
  title		= {Intervals of the lattice of recursively enumerable sets
		  determined by major subsets},
  volume	= {24},
  year		= {1983}
}

@Article{MaassSutner:88,
  author	= {W. Maass and K. Sutner},
  journal	= {Acta Informatica},
  pages		= {93-122},
  title		= {Motion planning among time dependent abstacles},
  volume	= {26},
  year		= {1988}
}

@InProceedings{MaassTuran:89,
  author	= {W. Maass and G. Turan},
  booktitle	= {Proceedings of the 30th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {262--267},
  title		= {On the complexity of learning from counterexamples
		  (extended abstract)},
  year		= {1989}
}

@InProceedings{MaassTuran:90,
  author	= {W. Maass and G. Turan},
  booktitle	= {Proceedings of the 31th Annual IEEE Symposium on
		  Foundations of Computer Science},
  pages		= {203--210},
  title		= {On the complexity of learning from counterexamples and
		  membership queries},
  year		= {1990}
}

@Article{MaassTuran:92,
  author	= {W. Maass and G. Turan},
  journal	= {Machine Learning},
  note		= {Invited paper for a special issue of Machine Learning},
  pages		= {107--145},
  title		= {Lower bound methods and separation results for on-line
		  learning models},
  volume	= {9},
  year		= {1992}
}

@InCollection{MaassTuran:94,
  author	= {W. Maass and G. Turan},
  booktitle	= {Computational Learning Theory and Natural Learning System:
		  Constraints and Prospects},
  editor	= {S. J. Hanson and G. A. Drastal and R. L. Rivest},
  pages		= {381--414},
  publisher	= {MIT Press (Cambridge)},
  title		= {How fast can a threshold gate learn},
  year		= {1994}
}

@Article{MaassTuran:94a,
  author	= {W. Maass and G. Turan},
  journal	= {Machine Learning},
  pages		= {251--269},
  title		= {Algorithms and lower bounds for on-line learning of
		  geometrical concepts},
  volume	= {14},
  year		= {1994}
}

@InProceedings{MaassTuran:95,
  author	= {W. Maass and G. Turan},
  address	= {Jerusalem},
  booktitle	= {Proc. of the 4th Bar-Ilan Symposium on Foundations of
		  Artificial Intelligence (BISFAI'95)},
  title		= {On learnability and predicate logic (extended abstract)},
  year		= {1995},
  pages		= {75--85}
}

@InProceedings{MaassWarmuth:95,
  author	= {W. Maass and M. Warmuth},
  booktitle	= {Proc. of the 12th International Machine Learning
		  Conference, Tahoe City, USA},
  editor	= {Morgan Kaufmann (San Francisco)},
  pages		= {378-386},
  title		= {Efficient learning with virtual threshold gates},
  year		= {1995}
}

@Article{MaassWarmuth:95j,
  author	= {W. Maass and M. Warmuth},
  title		= {Efficient learning with virtual threshold gates},
  journal	= {Information and Computation},
  year		= 1998,
  volume	= 141,
  number	= 1,
  pages		= {66--83}
}

@InCollection{MaassWeibel:97,
  author	= {W. Maass and P. Weibel},
  booktitle	= {{Jenseits von Kunst}},
  editor	= {P. Weibel},
  pages		= {745--747},
  publisher	= {Passagen Verlag},
  title		= {{I}st die {V}ertreibung der {V}ernunft reversibel?
		  {U}eberlegungen zu einem {W}issenschafts- und
		  {M}edienzentrum},
  year		= {1997}
}

@InProceedings{MaassZador:98,
  author	= {W. Maass and A. M. Zador},
  booktitle	= {Advances in Neural Processing Systems},
  publisher	= {MIT Press (Cambridge)},
  title		= {Dynamic stochastic synapses as computational units},
  volume	= {10},
  pages		= {194--200},
  year		= {1998}
}

@InCollection{MaassZador:98a,
  author	= {W. Maass and A. Zador},
  booktitle	= {Pulsed Neural Networks},
  editor	= {W. Maass and C. Bishop},
  publisher	= {MIT-Press (Cambridge)},
  title		= {Computing and learning with dynamic synapses},
  year		= {1998},
  pages		= {321-336}
}

@Article{MaassZador:98j,
  author	= {W. Maass and A. M. Zador},
  title		= {Dynamic stochastic synapses as computational units},
  journal	= {Neural Computation},
  year		= 1999,
  volume	= 11,
  number	= 4,
  pages		= {903--917}
}

@Article{MelamedETAL:03,
  author	= {O. Melamed and W. Gerstner and W. Maass and M. Tsodyks and
		  H. Markram},
  title		= {Coding and Learning of Behavioral Sequences},
  abstract	= {A major challenge to understanding behavior is how the
		  nervous system allows the learning of behavioral sequences
		  that can occur over arbitrary timescales, ranging from
		  milliseconds up to seconds, using a fixed millisecond
		  learning rule. This article describes some potential
		  solutions, and then focuses on a study by Mehta et al. that
		  could contribute towards solving this puzzle. They have
		  discovered that an experience-dependent asymmetric shape of
		  hippocampal receptive fields combined with oscillatory
		  inhibition can serve to map behavioral sequences on a fixed
		  timescale.},
  journal	= {Trends in Neurosciences},
  volume	= 27,
  number	= 1,
  year		= {2004},
  pages		= {11--14}
}

@Article{MontemurroETAL:08,
  author	= {M. A. Montemurro and M. J. Rasch and Y. Murayam and N. K.
		  Logothetis and S. Panzeri},
  title		= {Phase-of-Firing Coding of Natural Visual Stimuli in
		  Primary Visual Cortex},
  journal	= {Current Biology},
  year		= {2008},
  volume	= {18},
  number	= {},
  pages		= {375--380},
  note		= {},
  abstract	= {We investigated the hypothesis that neurons encode rich
		  naturalistic stimuli in terms of their spike times relative
		  to the phase of ongoing network fluctuations rather than
		  only in terms of their spike count. We recorded local field
		  potentials (LFPs) and multiunit spikes from the primary
		  visual cortex of anaesthetized macaques while binocularly
		  presenting a color movie. We found that both the spike
		  counts and the low-frequency LFP phase were reliably
		  modulated by the movie and thus conveyed information about
		  it. Moreover, movie periods eliciting higher firing rates
		  also elicited a higher reliability of LFP phase across
		  trials. To establish whether the LFP phase at which spikes
		  were emitted conveyed visual information that could not be
		  extracted by spike rates alone,wecompared the Shannon
		  information about the movie carried by spike counts to that
		  carried by the phase of firing. We found that at low LFP
		  frequencies, the phase of firing conveyed 54 \% additional
		  information beyond that conveyed by spike counts. The extra
		  information available in the phase of firing was crucial
		  for the disambiguation between stimuli eliciting high spike
		  rates of similar magnitude. Thus, phase coding may allow
		  primary cortical neurons to represent several effective
		  stimuli in an easily decodable format.}
}

@Article{MullerETAL:07,
  author	= {E. Muller and L. Buesing and J. Schemmel and K. Meier},
  title		= {Spike-frequency adapting neural ensembles: Beyond mean
		  adaptation and renewal theories},
  journal	= {Neural Computation},
  year		= {2007},
  volume	= {19},
  number	= {11},
  note		= {},
  pages		= {},
  abstract	= {}
}

@MastersThesis{Natschlaeger:96,
  author	= {T. Natschlaeger},
  title		= {{R}aum- zeitliche {S}trukturen von {B}erechnungen in
		  biologisch realistischen neuronalen {N}etzwerken},
  school	= {Technische Universitaet Graz},
  year		= 1996,
  month		= {February},
  userlabel	= {2},
  keywords	= {spiking neurons, hopfield net, temporal coding}
}

@InCollection{Natschlaeger:96a,
  author	= {T. Natschlaeger},
  title		= {{N}etzwerke von {S}piking {N}euronen: {D}ie dritte
		  {G}eneration von {M}odellen fuer neuronale {N}etzwerke},
  booktitle	= {Jenseits von Kunst},
  publisher	= {Passagen Verlag},
  year		= 1996,
  htmlnote	= {<a href="online/3rd_gen_ger/3rd_gen_ger.html">Online
		  version</a>},
  userlabel	= 3,
  keywords	= {spiking neurons, hopfield net, CPG, central pattern
		  generator},
  abstract	= {Dieser Artikel beschreibt in allgemein verstaendlicher
		  Form einige Ergebnisse ueber Netzwerke von ``spiking''
		  Neuronen, die an unserem Institut erarbeitet wurden. Einer
		  kurzen Einfuehrung, die die biologischen Grundlagen
		  beleuchtet, folgt eine Erklaerung des Modells und inwiefern
		  sich dieses Modell von der ersten und zweiten Generation
		  von Modellen fuer neuronale Netzwerke unterscheidet. Daran
		  anschliessend werden einige Arbeiten, die von den
		  Mitarbeitern unseres Institutes durchgefuehrt wurden,
		  vorgestellt. Der Kernpunkt dieser Ergebnisse betrifft die
		  grosse Berechnungsstaerke dieser Netzwerkmodelle. In diesem
		  Uebersichtsartikel wird kein fundiertes a priori Wissen
		  ueber diese Art von Modellen von neuronalen Netzwerken
		  vorausgesetzt.}
}

@InCollection{Natschlaeger:98b,
  author	= {T. Natschlaeger},
  title		= {Networks of Spiking Neurons: A New Generation of Neural
		  Network Models},
  booktitle	= {Jenseits von Kunst},
  publisher	= {Passagen Verlag},
  year		= 1998,
  htmlnote	= {<a href="online/3rd_gen_eng/3rd_gen_eng.html">Online
		  version</a>}
}

@PhDThesis{Natschlaeger:99,
  author	= {T. Natschlaeger},
  title		= {Efficient Computation in Networks of Spiking Neurons --
		  Simulations and Theory},
  school	= {Graz University of Technology},
  year		= 1999,
  abstract	= {One of the most prominent features of biological neural
		  systems is that individual neurons communicate via short
		  electrical pulses, the so called action potentials or
		  spikes. In this thesis we investigate possible mechanisms
		  which can in principle explain how complex computations in
		  spiking neural networks (SNN) can be performed very fast,
		  i. e. within a few 10 milliseconds. Some of these models
		  are based on the assumption that relevant information is
		  encoded by the timing of individual spikes (temporal
		  coding). We will also discuss a model which is based on a
		  population code and still is able to perform fast complex
		  computations. In their natural environment biological
		  neural systems have to process signals with a rich temporal
		  structure. Hence it is an interesting question how neural
		  systems process time series. In this context we explore
		  possible links between biophysical characteristics of
		  single neurons (refractory behavior, connectivity, time
		  course of postsynaptic potentials) and synapses
		  (unreliability, dynamics) on the one hand and possible
		  computations on times series on the other hand. Furthermore
		  we describe a general model of computation that exploits
		  dynamic synapses. This model provides a general framework
		  for understanding how neural systems process time-varying
		  signals. }
}

@InCollection{NatschlaegerETAL-SOM:01,
  author	= {T. Natschlaeger and B. Ruf and M. Schmitt},
  title		= {Unsupervised Learning and Self-Organization in Networks of
		  Spiking Neurons},
  booktitle	= {Self-Organizing Neural Networks. Recent Advances and
		  Applications},
  publisher	= {Springer-Verlag},
  year		= 2001,
  editor	= {U. Seiffert and L. C. Jain},
  volume	= 78,
  series	= {Springer Series on Studies in Fuzziness and Soft
		  Computing},
  address	= {Heidelberg},
  note		= {in press},
  abstract	= {One of the most prominent features of biological neural
		  systems is that individual neurons communicate via short
		  electrical pulses, the so-called action potentials or
		  spikes. In this chapter we investigate possible mechanisms
		  of unsupervised learning and self-organization in networks
		  of spiking neurons. After giving a brief introduction to
		  spiking neuron networks we describe a biologically
		  plausible algorithm for these networks to find clusters in
		  a high dimensional input space or a subspace of it. The
		  algorithm is shown to work even in a dynamically changing
		  environment. Furthermore, we study self-organizing maps of
		  spiking neurons showing that networks of spiking neurons
		  using temporal coding can achieve a topology preserving
		  behavior quite similar to that of Kohonen's self-organizing
		  map. For these networks a mechanism of competitive
		  computation is proposed that is based on action potential
		  timing. Thus, the winner in a population of competing
		  neurons can be determined locally and in generally faster
		  than in approaches which use rate coding. The models and
		  algorithms presented in this chapter establish further
		  steps toward more realistic descriptions of unsupervised
		  learning in biological neural systems.}
}

@InProceedings{NatschlaegerETAL:01,
  author	= {T. Natschlaeger and W. Maass and E. D. Sontag and A.
		  Zador},
  title		= {Processing of Time Series by Neural Circuits with
		  Biologically Realistic Synaptic Dynamics},
  booktitle	= {Advances in Neural Information Processing Systems 2000
		  ({NIPS '2000})},
  editor	= {Todd K. Leen and Thomas G. Dietterich and Volker Tresp},
  year		= 2001,
  volume	= 13,
  pages		= {145--151},
  address	= {Cambridge},
  publisher	= {MIT Press},
  abstract	= {Experimental data show that biological synapses behave
		  quite differently from the symbolic synapses in common
		  artificial neural network models. Biological synapses are
		  dynamic, i.e., their weight changes on a short time scale
		  by several hundred percent in dependence of the past input
		  to the synapse. In this article we explore the consequences
		  that this synaptic dynamics entails for the computational
		  power of feedforward neural networks. It turns out that
		  even with just a single hidden layer such networks can
		  approximate a surprisingly large class of nonlinear
		  filters: all filters that can be characterized by Volterra
		  series. This result is robust with regard to various
		  changes in the model for synaptic dynamics. Furthermore we
		  show that simple gradient descent suffices to approximate a
		  given quadratic filter by a rather small neural system with
		  dynamic synapses. We demonstrate that with this approach
		  the nonlinear filter considered in (Back and Tsoi 93) can
		  be approximated even better than by their model.},
  htmlnote	= {The poster presented at NIPS is available as <a
		  href="./psfiles/dynsyn-poster.pdf">Acrobat PDF</a> file.}
}

@Article{NatschlaegerETAL:01a,
  author	= {T. Natschlaeger and W. Maass and A. Zador},
  title		= {Efficient Temporal Processing with Biologically Realistic
		  Dynamic Synapses},
  journal	= {Network: Computation in Neural Systems},
  year		= 2001,
  pages		= {75--87},
  volume	= 12,
  abstract	= {Experimental data show that biological synapses behave
		  quite differently from the symbolic synapses in common
		  artificial neural network models. Biological synapses are
		  dynamic, i.e., their ``weight'' changes on a short time
		  scale by several hundred percent in dependence of the past
		  input to the synapse. Here we describe a general model of
		  computation that exploits dynamic synapses, and use a
		  backpropagation-like algorithm to adjust the synaptic
		  parameters. We show that such gradient descent suffices to
		  approximate a given quadratic filter by a rather small
		  neural system with dynamic synapses. We demonstrate that
		  with this approach the nonlinear filter considered in (Back
		  and Tsoi, 1993) can be approximated slightly better than by
		  their model. Our numerical results are complemented by
		  theoretical analysis which show that even with just a
		  single hidden layer such networks can approximate a
		  surprisingly large class of nonlinear filters: all filters
		  that can be characterized by Volterra series. This result
		  is robust with regard to various changes in the model for
		  synaptic dynamics. }
}

@Article{NatschlaegerETAL:02,
  author	= {T. Natschlaeger and W. Maass and H. Markram},
  title		= {The "Liquid Computer": A Novel Strategy for Real-Time
		  Computing on Time Series},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  year		= {2002},
  pages		= {39--43},
  volume	= {8},
  number	= {1},
  abstract	= {We will discuss in this survey article a new framework for
		  analysing computations on time series and in particular on
		  spike trains, introduced in (Maass et. al. 2002). In
		  contrast to common computational models this new framework
		  does not require that information can be stored in some
		  stable states of a computational system. It has recently
		  been shown that such models where all events are transient
		  can be successfully applied to analyse computations in
		  neural systems and (independently) that the basic ideas can
		  also be used to solve engineering tasks such as the design
		  of nonlinear controllers. Using an illustrative example we
		  will develop the main ideas of the proposed model. This
		  illustrative example is generalized and cast into a
		  rigorous mathematical model: the Liquid State Machine. A
		  mathematical analysis shows that there are in principle no
		  computational limitations of liquid state machines in the
		  domain of time series computing. Finally we discuss several
		  successful applications of the framework in the area of
		  computational neuroscience and in the field of artificial
		  neural networks.}
}

@InCollection{NatschlaegerETAL:03,
  author	= {T. Natschlaeger and H. Markram and W. Maass},
  title		= {Computer Models and Analysis Tools for Neural
		  Microcircuits},
  booktitle	= {Neuroscience Databases. A Practical Guide},
  publisher	= {Kluwer Academic Publishers (Boston)},
  year		= {2003},
  editor	= {R. Koetter},
  chapter	= {9},
  pages		= {121--136},
  abstract	= {This chapter surveys web resources regarding computer
		  models and analysis tools for neural microcircuits. In
		  particular it describes the features of a new website
		  (www.lsm.tugraz.at) that facilitates the creation of
		  computer models for cortical neural microcircuits of
		  various sizes and levels of detail, as well as tools for
		  evaluating the computational power of these models in a
		  Matlabenvironment.}
}

@InCollection{NatschlaegerETAL:04,
  author	= {T. Natschlaeger and N. Bertschinger and R. Legenstein},
  title		= {At the Edge of Chaos: Real-time Computations and
		  Self-Organized Criticality in Recurrent Neural Networks},
  booktitle	= {Advances in Neural Information Processing Systems 17},
  editor	= {Lawrence K. Saul and Yair Weiss and {L\'{e}on} Bottou},
  publisher	= {MIT Press},
  address	= {Cambridge, MA},
  pages		= {145-152},
  year		= 2005,
  abstract	= {In this paper we analyze the relationship between the
		  computational capabilities of randomly connected networks
		  of threshold gates in the timeseries domain and their
		  dynamical properties. In particular we propose a complexity
		  measure which we find to assume its highest values near the
		  edge of chaos, i.e. the transition from ordered to chaotic
		  dynamics. Furthermore we show that the proposed complexity
		  measure predicts the computational capabilities very well:
		  only near the edge of chaos are such networks able to
		  perform complex computations on time series. Additionally a
		  simple synaptic scaling rule for self-organized criticality
		  is presented and analyzed.}
}

@Article{NatschlaegerMaass:01a,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Computing the Optimally Fitted Spike Train for a Synapse},
  journal	= {Neural Computation},
  year		= 2001,
  volume	= 13,
  number	= 11,
  pages		= {2477--2494},
  abstract	= {Experimental data have shown that synapses are
		  heterogeneous: different synapses respond with different
		  sequences of amplitudes of postsynaptic responses to the
		  same spike train. Neither the role of synaptic dynamics
		  itself nor the role of the heterogeneity of synaptic
		  dynamics for computations in neural circuits is well
		  understood. We present in this article two computational
		  methods that make it feasible to compute for a given
		  synapse with known synaptic parameters the spike train that
		  is optimally fitted to the synapse in a certain sense. With
		  the help of these methods one can compute for example the
		  temporal pattern of a spike train (with a given number of
		  spikes) that produces the largest sum of postsynaptic
		  responses for a specific synapse. Several other
		  applications are also discussed. To our surprise we find
		  that most of these optimally fitted spike trains match
		  common firing patterns of specific types of neurons that
		  are discussed in the literature. Hence our analysis
		  provides a possible functional explanation for the
		  experimentally observed regularity in the combination of
		  specific types of synapses with specific types of neurons
		  in neural circuits. }
}

@InProceedings{NatschlaegerMaass:01b,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Finding the Key to a Synapse},
  booktitle	= {Advances in Neural Information Processing Systems ({NIPS
		  '2000})},
  editor	= {Todd K. Leen and Thomas G. Dietterich and Volker Tresp},
  year		= 2001,
  pages		= {138--144},
  volume	= 13,
  address	= {Cambridge},
  publisher	= {MIT Press},
  abstract	= {Experimental data have shown that synapses are
		  heterogeneous: different synapses respond with different
		  sequences of amplitudes of postsynaptic responses to the
		  same presynaptic spike train. Neither the role of synaptic
		  dynamics itself nor the role of the heterogeneity of
		  synaptic dynamics for computations in neural circuits is
		  well understood. We present in this article methods that
		  make it feasible to compute for a given synapse with known
		  synaptic parameters the spike train that is optimally
		  fitted to the synapse, in the sense that it produces the
		  largest sum of postsynaptic responses. To our surprise we
		  find that most of these optimally fitted spike trains match
		  common firing patterns of specific types of neurons that
		  are discussed in the literature. Hence our analysis
		  provides a possible functional explanation for the
		  experimentally observed regularity in the combination of
		  specific types of synapses with specific types of neurons
		  in neural circuits.},
  htmlnote	= {The poster presented at NIPS is available as <a
		  href="./psfiles/synkey-poster.pdf">Acrobat PDF</a> file.}
}

@InProceedings{NatschlaegerMaass:03,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Information Dynamics and Emergent Computation in Recurrent
		  Circuits of Spiking Neurons},
  booktitle	= {Proc. of NIPS 2003, Advances in Neural Information
		  Processing Systems},
  year		= {2004},
  volume	= {16},
  pages		= {1255--1262},
  editor	= {S. Thrun and L. Saul and B. Schoelkopf},
  publisher	= {MIT Press},
  address	= {Cambridge},
  abstract	= {An efficient method using Bayesian and linear classifiers
		  is presented for analyzing the dynamics of information in
		  high dimensional circuit states, and applied to investigate
		  emergent computation in generic cortical microcircuit
		  models. It is shown that such recurrent circuits of spiking
		  neurons have an inherent capability to carry out rapid
		  computations on complex spike patterns, merging information
		  contained in the order of spike arrival with previously
		  acquired context information.}
}

@Article{NatschlaegerMaass:04,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Dynamics of Information and Emergent Computation in
		  Generic Neural Microcircuit Models},
  journal	= {Neural Networks},
  volume	= {18},
  number	= {10},
  pages		= {1301--1308},
  year		= {2005},
  abstract	= {Numerous methods have already been developed to estimate
		  the information contained in single spike trains. In this
		  article we explore efficient methods for estimating the
		  information contained in the simultaneous firing activity
		  of hundreds of neurons. Obviously such methods are needed
		  to analyze data from multi-unit recordings. We test these
		  methods on generic neural microcircuit models consisting of
		  800 neurons, and analyze the temporal dynamics of
		  information about preceding spike inputs in such circuits.
		  It turns out that information spreads with high speed in
		  such generic neural microcircuit models, thereby supporting
		  -- without the postulation of any additional neural or
		  synaptic mechanisms -- the possibility of ultra-rapid
		  computations on the first input spikes.}
}

@Article{NatschlaegerMaass:2001,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Spiking Neurons and the Induction of Finite State
		  Machines},
  journal	= {Theoretical Computer Science: Special Issue on Natural
		  Computing},
  volume	= 287,
  year		= 2002,
  pages		= {251--265},
  abstract	= {We discuss in this short survey article some current
		  mathematical models from neurophysiology for the
		  computational units of biological neural systems: neurons
		  and synapses. These models are contrasted with the
		  computational units of common artificial neural network
		  models, which reflect the state of knowledge in
		  neurophysiology 50 years ago. We discuss the problem of
		  carrying out computations in circuits consisting of
		  biologically realistic computational units, focusing on the
		  biologically particularly relevant case of computations on
		  time series. Finite state machines are frequently used in
		  computer science as models for computations on time series.
		  One may argue that these models provide a reasonable common
		  conceptual basis for analyzing computations in computers
		  and biological neural systems, although the emphasis in
		  biological neural systems is shifted more towards
		  asynchronous computation on analog time series. In the
		  second half of this article some new computer experiments
		  and theoretical results are discussed, which address the
		  question whether a biological neural system can in
		  principle learn to behave like a given simple finite state
		  machine. }
}

@InProceedings{NatschlaegerMaass:99,
  author	= {T. Natschlaeger and W. Maass},
  title		= {Fast Analog Computation in Networks of Spiking Neurons
		  Using Unreliable Synapses},
  booktitle	= {{ESANN'99} Proceedings of the European Symposium on
		  Artificial Neural Networks},
  year		= 1999,
  address	= {Bruges, Belgium},
  pages		= {417--422},
  keywords	= {unreliable synapses, universal function approximation,
		  fast analog computation, time series, hebbian learning,
		  space rate coding, population activity},
  abstract	= {We investigate through theoretical analysis and computer
		  simulations the consequences of unreliable synapses for
		  fast analog computations in networks of spiking neurons,
		  with analog variables encoded by the firing activities of
		  pools of spiking neurons. Our results suggest that the
		  known unreliability of synaptic transmission may be viewed
		  as a useful tool for analog computing, rather than as a
		  ``bug'' in neuronal hardware. We also investigate
		  computations on analog time series encoded by the firing
		  activities of pools of spiking neurons.}
}

@Article{NatschlaegerRuf:98a,
  author	= {T. Natschlaeger and B. Ruf},
  title		= {Spatial and temporal pattern analysis via spiking
		  neurons},
  journal	= {Network: Computation in Neural Systems},
  year		= 1998,
  volume	= 9,
  number	= 3,
  pages		= {319--332},
  userlabel	= 7,
  keywords	= {spiking neurons, RBF networks, clustering, hebbian
		  learning},
  abstract	= {Spiking neurons, receiving temporally encoded inputs, can
		  compute radial basis functions (RBFs) by storing the
		  relevant information in their delays. In this paper we show
		  how these delays can be learned using exclusively locally
		  available information (basically the time difference
		  between the pre- and postsynaptic spike). Our approach
		  gives rise to a biologically plausible algorithm for
		  finding clusters in a high dimensional input space with
		  networks of spiking neurons, even if the environment is
		  changing dynamically. Furthermore, we show that our
		  learning mechanism makes it possible that such RBF neurons
		  can perform some kind of feature extraction where they
		  recognize that only certain input coordinates carry
		  relevant information. Finally we demonstrate that this
		  model allows the recognition of temporal sequences even if
		  they are distorted in various ways.}
}

@InProceedings{NatschlaegerRuf:98b,
  author	= {T. Natschlaeger and B. Ruf},
  title		= {Online Clustering with Spiking Neurons Using Temporal
		  Coding},
  booktitle	= {Neuromorphic Systems: Engineering Silicon from
		  Neurobiology},
  editor	= {L. S. Smith and A. Hamilton},
  year		= 1998,
  publisher	= {World Scientific},
  pages		= {33--42},
  userlabel	= {6},
  keywords	= {spiking neurons, RBF networks, clustering, hebbian
		  learning},
  abstract	= {Spiking neurons, receiving temporally encoded inputs, can
		  compute radial basis functions in a biologically realistic
		  way. They store the relevant information in their delays.
		  In this paper we show how these delays can be learned using
		  exclusively locally available information (basically the
		  time difference between the pre- and postsynaptic spike).
		  Our approach gives rise to a biologically plausible
		  algorithm for finding clusters in a high dimensional input
		  space with networks of spiking neurons, even if the
		  environment is changing dynamically.}
}

@Article{NatschlaegerRuf:99c,
  author	= {T. Natschlaeger and B. Ruf},
  title		= {Pattern Analysis with Spiking Neurons using Delay Coding},
  journal	= {Neurocomputing},
  year		= 1999,
  pages		= {463-469},
  volume	= {26-27},
  number	= {1-3},
  keywords	= {spiking neurons, RBF networks, clustering, hebbian
		  learning},
  abstract	= {Spiking neurons, receiving temporally encoded inputs, can
		  compute radial basis functions (RBFs) by storing the
		  relevant information in their delays. These delays can be
		  learned using exclusively locally available information
		  (basically the time difference between the pre- and
		  postsynaptic spike). Our approach gives rise to a
		  biologically plausible algorithm for finding clusters in a
		  high dimensional input space. Furthermore, we show that our
		  learning mechanism makes it possible that such RBF neurons
		  can perform some kind of feature extraction. Finally we
		  demonstrate that this model allows the recognition of
		  temporal sequences even if they are distorted in various
		  ways.}
}

@Article{NatschlaegerSchmitt:96,
  author	= {T. Natschlaeger and M. Schmitt},
  title		= {Exact {VC}-{D}imension of Boolean Monomials},
  journal	= {Information Processing Letters},
  year		= 1996,
  volume	= 59,
  pages		= {19--20},
  userlabel	= {1},
  keywords	= {monome, VC-dimension, combinatorial problems,
		  computational complexity, learnability },
  abstract	= {We show that the Vapnik-Chervonenkis dimension of Boolean
		  monomials over $n$ variables is at most $n$ for all $n <
		  2$. It follows that the VC-dimension is determined exactly
		  and is, except for $n=1$, equal to the VC-dimension of the
		  proper subclass of monotone monomials.}
}

@Article{NesslerETAL:08,
  author	= {B. Nessler and M. Pfeiffer and W. Maass},
  title		= {Hebbian learning of {B}ayes optimal decisions},
  journal	= {In Proc. of NIPS 2008: Advances in Neural Information
		  Processing Systems},
  year		= {2009},
  volume	= {21},
  number	= {},
  pages		= {},
  note		= {MIT Press},
  abstract	= {When we perceive our environment, make a decision, or take
		  an action, our brain has to deal with multiple sources of
		  uncertainty. The Bayesian framework of statistical
		  estimation provides computational methods for dealing
		  optimally with uncertainty. Bayesian inference however is
		  algorithmically quite complex, and learning of Bayesian
		  inference involves the storage and updating of probability
		  tables or other data structures that are hard to implement
		  in neural networks. Hence it is unclear how our nervous
		  system could acquire the capability to approximate optimal
		  Bayesian inference and action selection. This article shows
		  that the simplest and experimentally best supported type of
		  synaptic plasticity, Hebbian learning, can in principle
		  achieve this. Even inference in complex Bayesian networks
		  can be approximated by Hebbian learning in combination with
		  population coding and lateral inhibition
		  (``Winner-Take-All'') in cortical microcircuits that
		  produce a sparse encoding of complex sensory stimuli. We
		  also show that a corresponding reward-modulated Hebbian
		  plasticity rule provides a principled framework for
		  understanding how Bayesian inference could support fast
		  reinforcement learning in the brain. In particular we show
		  that recent experimental results by Yang and Shadlen on
		  reinforcement learning of probabilistic inference in
		  primates can be modeled in this way.}
}

@InProceedings{NesslerETAL:10,
  author	= {B. Nessler and M. Pfeiffer and W. Maass},
  title		= {{STDP} enables spiking neurons to detect hidden causes of
		  their inputs},
  booktitle	= {Proc. of NIPS 2009: Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2010},
  volume	= {22},
  pages		= {1357--1365},
  abstract	= {The principles by which spiking neurons contribute to the
		  astounding computational power of generic cortical
		  microcircuits, and how spike-timing-dependent plasticity
		  (STDP) of synaptic weights could generate and maintain this
		  computational function, are unknown. We show here that
		  STDP, in conjunction with a stochastic soft winner-take-all
		  (WTA) circuit, induces spiking neurons to generate through
		  their synaptic weights implicit internal models for
		  subclasses (or causes) of the high-dimensional spike
		  patterns of hundreds of pre-synaptic neurons. Hence these
		  neurons will fire after learning whenever the current input
		  best matches their internal model. The resulting
		  computational function of soft WTA circuits, a common
		  network motif of cortical microcircuits, could therefore be
		  a drastic dimensionality reduction of information streams,
		  together with the autonomous creation of internal models
		  for the probability distributions of their input patterns.
		  We show that the autonomous generation and maintenance of
		  this computational function can be explained on the basis
		  of rigorous mathematical principles. In particular, we show
		  that STDP is able to approximate a stochastic online
		  Expectation-Maximization (EM) algorithm for modeling the
		  input data. A corresponding result is shown for Hebbian
		  learning in artificial neural networks.}
}

@InProceedings{NeumannETAL:07,
  author	= {G. Neumann and M. Pfeiffer and W. Maass},
  booktitle	= {Proceedings of the 18th European Conference on Machine
		  Learning (ECML) and the 11th European Conference on
		  Principles and Practice of Knowledge Discovery in Databases
		  (PKDD) 2007, Warsaw (Poland)},
  title		= {Efficient Continuous-Time Reinforcement Learning with
		  Adaptive State Graphs},
  publisher	= {Springer (Berlin)},
  year		= {2007},
  pages		= {},
  note		= {in press},
  abstract	= {We present a new reinforcement learning approach for
		  deterministic continuous control problems in environments
		  with unknown, arbitrary reward functions. The difficulty of
		  finding solution trajectories for such problems can be
		  reduced by incorporating limited prior knowledge of the
		  approximative local system dynamics. The presented
		  algorithm builds an adaptive state graph of sample points
		  within the continuous state space. The nodes of the graph
		  are generated by an efficient principled exploration scheme
		  that directs the agent towards promising regions, while
		  maintaining good online performance. Global solution
		  trajectories are formed as combinations of local
		  controllers that connect nodes of the graph, thereby
		  naturally allowing continuous actions and continuous time
		  steps. We demonstrate our approach on various movement
		  planning tasks in continuous domains.}
}

@InProceedings{NeumannETAL:09,
  author	= {G. Neumann and W. Maass and J. Peters},
  title		= {Learning Complex Motions by Sequencing Simpler Motion
		  Templates},
  booktitle	= {Proc. of the Int. Conf. on Machine Learning (ICML 2009),
		  Montreal},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  note		= {in press},
  abstract	= {Abstraction of complex, longer motor tasks into simpler
		  elemental movements enables humans and animals to exhibit
		  motor skills which have not yet been matched by robots.
		  Humans intuitively decompose complex motions into smaller,
		  simpler segments. For example when describing simple
		  movements like drawing a triangle with a pen, we can easily
		  name the basic steps of this movement. Surprisingly, such
		  abstractions have rarely been used in artificial motor
		  skill learning algorithms. These algorithms typically
		  choose a new action (such as a torque or a force) at a very
		  fast time-scale. As a result, both policy and temporal
		  credit assignment problem become unnecessarily complex -
		  often beyond the reach of current machine learning methods.
		  We introduce a new framework for temporal abstractions in
		  reinforcement learning (RL), i.e. RL with motion templates.
		  We present a new algorithm for this framework which can
		  learn high-quality policies by making only few abstract
		  decisions. }
}

@Article{NikolicETAL:06,
  author	= {D. Nikolic and S. Haeusler and W. Singer and W. Maass},
  title		= {Temporal dynamics of informational contents carried by
		  neurons in the primary visual cortex},
  journal	= {submitted for publication},
  year		= 2006
}

@Article{NikolicETAL:07,
  author	= {D. Nikolic and S. Haeusler and W. Singer and W. Maass},
  title		= {Distributed fading memory for stimulus properties in the
		  primary visual},
  journal	= {submitted for publication},
  year		= {2009},
  volume	= {},
  number	= {},
  pages		= {},
  abstract	= {}
}

@InProceedings{NikolicETAL:07a,
  author	= {D. Nikoli\'{c} and S. Haeusler and W. Singer and W. Maass},
  title		= {Temporal dynamics of information content carried by
		  neurons in the primary visual cortex},
  booktitle	= {Proc. of NIPS 2006, Advances in Neural Information
		  Processing Systems},
  editor	= {},
  publisher	= {MIT Press},
  year		= {2007},
  volume	= {19},
  pages		= {1041--1048},
  abstract	= {We use multi-electrode recordings from cat primary visual
		  cortex and investigate whether a simple linear classifier
		  ca n extract information about the presented stimuli. We
		  find that information is extractable and that it even las
		  ts for several hundred milliseconds after the stimulus has
		  b een removed. In a fast sequence of stimulus presentation,
		  information about both new and old stimuli is present
		  simultaneously and nonlinear relations between these
		  stimuli can be extracted. These results suggest nonlinear
		  properties of cortical representat ions. The implications
		  of these properties for the nonlinear brain theory are
		  discussed.}
}

@Article{NikolicETAL:09,
  author	= {D. Nikolic and S. Haeusler and W. Singer and W. Maass},
  title		= {Distributed fading memory for stimulus properties in the
		  primary visual cortex},
  journal	= {PLoS Biology},
  year		= {2009},
  volume	= {7},
  number	= {12},
  pages		= {1--19},
  abstract	= {It is currently not known how distributed neuronal
		  responses in early visual areas carry stimulus-related
		  information. We made multi-electrode recordings from cat
		  primary visual cortex and applied methods from machine
		  learning in order to analyze the temporal evolution of
		  stimulus-related information in the spiking activity of
		  large ensembles of around 100 neurons. We used sequences of
		  up to three different visual stimuli (letters) presented
		  for 100 ms and with intervals of 100 ms or larger. Most of
		  the information about visual stimuli extractable by
		  sophisticated methods of machine learning, i.e. support
		  vector machines with non-linear kernel functions, was also
		  extractable by simple linear classification such as can be
		  achieved by individual neurons. New stimuli did not erase
		  information about previous stimuli. The responses to the
		  most recent stimulus contained about equal amounts of
		  information about both this and the preceding stimulus.
		  This information was encoded both in the discharge rates
		  (response amplitudes) of the ensemble of neurons and, when
		  using short time-constants for integration (e.g., 20 ms),
		  in the precise timing of individual spikes (<= $~20$ ms),
		  and persisted for several 100 ms beyond the offset of
		  stimuli. The results indicate that the network from which
		  we recorded is endowed with fading memory and is capable of
		  performing online computations utilizing information about
		  temporally sequential stimuli. This result challenges
		  models assuming frame-by-frame analyses of sequential
		  inputs.},
  note		= {}
}

@Article{PecevskiETAL:07,
  author	= {R. Brette and D. Pecevski},
  title		= {Simulation of networks of spiking neurons: a review of
		  tools and strategies},
  journal	= {J. Computer Neuroscience},
  year		= {2007},
  volume	= {23},
  number	= {3},
  pages		= {349--398},
  abstract	= {}
}

@Article{PecevskiETAL:09,
  author	= {D. Pecevski and T. Natschlaeger and K. Schuch},
  title		= {{PCSIM}: A Parallel Simulation Environment for Neural
		  Circuits Fully Integrated with {P}ython},
  journal	= {Frontiers in Neuroinformatics},
  year		= {2009},
  volume	= {3},
  number	= {},
  pages		= {},
  note		= {doi:10.3389/neuro.11.011.2009},
  abstract	= {The {P}arallel {C}ircuit {SIM}ulator ({PCSIM}) is a
		  software package for simulation of neural circuits. It is
		  primarily designed for distributed simulation of large
		  scale networks of spiking point neurons. Although its
		  computational core is written in {C}++, {PCSIM}'s primary
		  interface is implemented in the {P}ython programming
		  language, which is a powerful programming environment and
		  allows the user to easily integrate the neural circuit
		  simulator with data analysis and visualization tools to
		  manage the full neural modeling life cycle. The main focus
		  of this paper is to describe {PCSIM}'s full integration
		  into {P}ython and the benefits thereof. In particular we
		  will investigate how the automatically generated
		  bidirectional interface and {PCSIM}'s object-oriented
		  modular framework enable the user to adopt a hybrid
		  modeling approach: using and extending {PCSIM}'s
		  functionality either employing pure {P}ython or {C}++ and
		  thus combining the advantages of both worlds. Furthermore,
		  we describe several supplementary {PCSIM} packages written
		  in pure {P}ython and tailored towards setting up and
		  analyzing neural simulations.}
}

@TechReport{PecevskiETAL:11,
  author	= {D. Pecevski and L. B\"using and W. Maass},
  title		= {Networks of spiking neurons are able to carry out
		  probabilistic inference in general graphical models through
		  their inherent stochastic dynamics},
  institution	= {Graz University of Technology},
  year		= {2011}
}

@InProceedings{PfeifferETAL:06,
  author	= {M. Pfeiffer and A. R. Saffari A. A. and A. Juffinger},
  title		= {Predicting Text Relevance from Sequential Reading
		  Behavior},
  booktitle	= {Proceedings of the NIPS 2005 Workshop on Machine Learning
		  for Implicit Feedback and User Modeling},
  year		= {2006},
  editor	= {K. Puolamaeki and S. Kaski},
  pages		= {25--30},
  address	= {Otaniemi, Finland},
  month		= {May},
  organization	= {Helsinki University of Technology}
}

@Article{PfeifferETAL:09,
  author	= {M. Pfeiffer and B. Nessler and W. Maass and R. J.
		  Douglas},
  title		= {Reward-modulated {H}ebbian Learning of Optimal Decision
		  Making},
  journal	= {Neural Computation},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@Article{PfeifferETAL:09b,
  author	= {M. Pfeiffer and B. Nessler and W. Maass},
  title		= {{STDP} approximates Expectation Maximization in networks
		  of spiking neurons with lateral inhibition},
  journal	= {in preparation},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@Article{PfeifferETAL:09c,
  author	= {M. Pfeiffer and B. Nessler and R. Douglas and W. Maass},
  title		= {Reward-modulated {H}ebbian {L}earning of {D}ecision
		  {M}aking},
  journal	= {Neural Computation},
  year		= {2010},
  volume	= {22},
  pages		= {1399--1444},
  abstract	= {We introduce a framework for decision making in which the
		  learning of decision making is reduced to its simplest
		  andbiologically most plausible form: Hebbian learning on a
		  linear neuron. We cast our Bayesian-Hebb learning rule as
		  reinforcement learning in which certain decisions are
		  rewarded and prove that each synaptic weight will on
		  average converge exponentially fast to the log-odd of
		  receiving a reward when its pre- and postsynaptic neurons
		  are active. In our simple architecture, a particular action
		  is selected from the set of candidate actions by a
		  winner-takeall operation. The global reward assigned to
		  this action then modulates the update of each synapse.
		  Apart from this global reward signal, our reward-modulated
		  Bayesian Hebb rule is a pure Hebb update that depends only
		  on the coactivation of the pre- and postsynaptic neurons,
		  not on theweighted sum of all presynaptic inputs to the
		  postsynaptic neuron as in the perceptron learning rule or
		  the Rescorla-Wagner rule. This simple approach to
		  action-selection learning requires that information about
		  sensory inputs be presented to the Bayesian decision stage
		  in a suitably preprocessed form resulting from other
		  adaptive processes (acting on a larger timescale) that
		  detect salient dependencies among input features. Hence our
		  proposed framework for fast learning of decisions also
		  provides interesting new hypotheses regarding neural nodes
		  and computational goals of cortical areas that provide
		  input to the final decision stage.}
}

@PhDThesis{Rasch:08,
  author	= {M. Rasch},
  title		= {Analysis of neural signals: Interdependence, information
		  coding, and relation to network models},
  school	= {Graz University of Technology, Institute for Theoretical
		  Computer Science},
  year		= {2008}
}

@TechReport{RaschETAL:06,
  author	= {M. Rasch and S. Haeusler and Z. Kisvarday and W. Maass and
		  N. Logothetis},
  title		= {Comparison of a detailed model for area {V}1 with
		  simultaneous recordings from {LGN} and {V}1},
  institution	= {Technische Universitaet Graz and MPI Tuebingen},
  year		= {2006}
}

@TechReport{RaschETAL:06b,
  author	= {M. Rasch and A. Gretton and Y. Murayama and W. Maass and
		  N. Logothetis},
  title		= {Interaction of local field potential and spiking activity
		  in area {V}1},
  institution	= {Technische Universitaet Graz and MPI Tuebingen},
  year		= {2006}
}

@Article{RaschETAL:07,
  author	= {M. J. Rasch and A. Gretton and Y. Murayama and W. Maass
		  and N. K. Logothetis},
  title		= {Inferring spike trains from local field potentials},
  journal	= {Journal of Neurophysiology},
  year		= {2008},
  volume	= {99},
  number	= {},
  pages		= {1461--1476},
  note		= {},
  abstract	= {We investigated whether it is possible to infer spike
		  trains solely on the basis of the underling local field
		  potentials ({LFP}s). Employing support vector machines and
		  linear regression models, we found that in the primary
		  visual cortex ({V}1) of monkeys, spikes can indeed be
		  inferred from {LFP}s, at least with moderate success.
		  Although there is a considerable degree of variation across
		  electrodes, the low-frequency structure in spike trains (in
		  the 100 ms range) can be inferred with reasonable accuracy,
		  whereas exact spike positions are not reliably predicted.
		  Two kinds of features of the {LFP} are exploited for
		  prediction: the frequency power of bands in the high
		  $\gamma$-range (40-90 {H}z), and information contained in
		  low-frequency oscillations (<10 {H}z), where both phase and
		  power modulations are informative. Information analysis
		  revealed that both features code (mainly) independent
		  aspects of the spike-to-{LFP} relationship, with the
		  low-frequency {LFP} phase coding for temporally clustered
		  spiking activity. Although both features and prediction
		  quality are similar during semi-natural movie stimuli and
		  spontaneous activity, prediction performance during
		  spontaneous activity degrades much more slowly with
		  increasing electrode distance. The general trend of data
		  obtained with anesthetized animals is qualitatively
		  mirrored in that of a more limited data set recorded in
		  {V}1 of awake monkeys. In contrast to the cortical field
		  potentials, thalamic {LFP}s (e.g. {LFP}s derived from
		  recordings in d{LGN}) hold no useful information for
		  predicting spiking activity.}
}

@Article{RaschETAL:09,
  author	= {M. J. Rasch and K. Schuch and N. K. Logothetis and W.
		  Maass},
  title		= {Statistical characterization of the spike response to
		  natural stimuli in monkey area {V}1 and in a detailed model
		  for a patch of {V}1},
  journal	= {submitted},
  year		= {2009},
  pages		= {},
  volume	= {}
}

@Article{RaschETAL:10,
  author	= {M. J. Rasch and K. Schuch and N. K. Logothetis and W.
		  Maass},
  title		= {Statistical Comparision of Spike Responses to Natural
		  Stimuli in Monkey Area {V}1 With Simulated Responses of a
		  Detailed Laminar Network Model for a Patch of {V}1},
  journal	= {J Neurophysiol},
  year		= {2011},
  pages		= {757--778},
  volume	= {105},
  abstract	= {A major goal of computational neuroscience is the creation
		  of computer models for cortical areas whose response to
		  sensory stimuli resembles that of cortical areas in vivo in
		  important aspects. It is seldom considered whether the
		  simulated spiking activity is realistic (in a statistical
		  sense) in response to natural stimuli. Because certain
		  statistical properties of spike responses were suggested to
		  facilitate computations in the cortex, acquiring a
		  realistic firing regimen in cortical network models might
		  be a prerequisite for analyzing their computational
		  functions. We present a characterization and comparison of
		  the statistical response properties of the primary visual
		  cortex (V1) in vivo and in silico in response to natural
		  stimuli. We recorded from multiple electrodes in area V1 of
		  4 macaque monkeys and developed a large state-of-the-art
		  network model for a 5 x 5-mm patch of V1 composed of 35,000
		  neurons and 3.9 million synapses that integrates previously
		  published anatomical and physiological details. By
		  quantitative comparison of the model response to the
		  "statistical fingerprint" of responses in vivo, we find
		  that our model for a patch of V1 responds to the same movie
		  in a way which matches the statistical structure of the
		  recorded data surprisingly well. The deviation between the
		  firing regimen of the model and the in vivo data are on the
		  same level as deviations among monkeys and sessions. This
		  suggests that, despite strong simplifications and
		  abstractions of cortical network models, they are
		  nevertheless capable of generating realistic spiking
		  activity. To reach a realistic firing state, it was not
		  only necessary to include both N-methyl- D-aspartate and
		  GABAB synaptic conductances in our model, but also to
		  markedly increase the strength of excitatory synapses onto
		  inhibitory neurons (more than 2-fold) in comparison to
		  literature values, hinting at the importance to carefully
		  adjust the effect of inhibition for achieving realistic
		  dynamics in current network models.}
}

@TechReport{RaschKisvarday:06,
  author	= {M. Rasch and Z. Kisvarday},
  title		= {Design of stimuli for investigating spatial integration of
		  information in optical recordings},
  institution	= {Technische Universitaet Graz and University of Debrecen},
  year		= {2006}
}

@MastersThesis{Ruf:93,
  author	= {B. Ruf},
  note		= {(in German)},
  school	= {Rheinisch-Westfaelische Technische Hochschule Aachen,
		  Germany},
  title		= {Sequentielle und parallele {L}ernverfahren fuer
		  {B}oltzmann-{M}aschinen},
  year		= {1993},
  keywords	= {boltzmann machine, learning algorithm}
}

@InProceedings{Ruf:94,
  author	= {B. Ruf},
  address	= {Brussels},
  booktitle	= {Proc. of the 2nd european symposium on artificial neural
		  networks},
  editor	= {M. Verleysen},
  pages		= {109--116},
  title		= {A stop criterion for the Boltzmann machine learning
		  algorithm},
  year		= {1994},
  keywords	= {boltzmann machine, learning}
}

@InProceedings{Ruf:96,
  author	= {B. Ruf},
  booktitle	= {Workshop on Neural Networks Dynamics and Pattern
		  Recognition, Toulouse},
  pages		= {25--26},
  publisher	= {Onera, Centre d'Etudes et de Recherches de Toulouse},
  title		= {Pattern Recognition with Networks of Spiking Neurons},
  year		= {1996},
  keywords	= {spiking neurons, pattern recognition, temporal coding}
}

@InProceedings{Ruf:97,
  author	= {B. Ruf},
  booktitle	= {Biological and artificial computation: From neuroscience
		  to technology},
  editor	= {J. Mira and R. Moreno-Diaz and J. Cabestany},
  pages		= {265--272},
  publisher	= {Springer, Berlin},
  series	= {Lecture Notes in Computer Science},
  title		= {Computing functions with spiking neurons in temporal
		  coding},
  volume	= {1240},
  year		= {1997},
  keywords	= {spiking neuron, linear function, temporal coding,
		  approximation}
}

@PhDThesis{Ruf:98a,
  author	= {B. Ruf},
  school	= {TU Graz},
  title		= {Computing and learning with spiking neurons - theory and
		  simulations},
  year		= {1998}
}

@InProceedings{Ruf:98b,
  author	= {B. Ruf},
  booktitle	= {Proc. of the VI-Dynn 98 conference},
  editor	= {T. Lindblad},
  publisher	= {Spie},
  title		= {Networks of spiking neurons can compute linear functions
		  using action potential timing},
  year		= {1998}
}

@Article{RufSchmitt:97,
  author	= {B. Ruf and M. Schmitt},
  journal	= {Neural Processing Letters},
  number	= {1},
  pages		= {9--18},
  title		= {Learning temporally encoded patterns in networks of
		  spiking neurons},
  volume	= {5},
  year		= {1997},
  keywords	= {spiking neuron, temporal coding, hebbian learning}
}

@InProceedings{RufSchmitt:97a,
  author	= {B. Ruf and M. Schmitt},
  booktitle	= {Biological and artificial computation: From neuroscience
		  to technology},
  editor	= {J. Mira and R. Moreno-Diaz and J. Cabestany},
  pages		= {380--389},
  publisher	= {Springer, Berlin},
  series	= {Lecture Notes in Computer Science},
  title		= {Hebbian Learning in Networks of Spiking Neurons Using
		  Temporal Coding},
  volume	= {1240},
  year		= {1997},
  keywords	= {spiking neuron, hebbian learning, temporal coding}
}

@InProceedings{RufSchmitt:97b,
  author	= {B. Ruf and M. Schmitt},
  booktitle	= {Proc. of the 7th International Conference on Artificial
		  Neural Networks},
  editor	= {W. Gerstner and A. Germond and M. Hasler and J. Nicoud},
  pages		= {361--366},
  publisher	= {Springer, Berlin},
  series	= {Lecture Notes in Computer Science},
  title		= {Unsupervised learning in networks of spiking neurons using
		  temporal coding},
  volume	= {1327},
  year		= {1997},
  keywords	= {unsupervised learning, self organizing map, spiking
		  neuron}
}

@Article{RufSchmitt:97d,
  author	= {B. Ruf and M. Schmitt},
  booktitle	= {Proc. of the 7th International Conference on Artificial
		  Neural Networks},
  journal	= {IEEE Transactions on Neural Networks},
  pages		= {575--578},
  title		= {Self-organization of spiking neurons using action
		  potential timing},
  volume	= {9:3},
  year		= {1998},
  keywords	= {unsupervised learning, self organizing map, spiking
		  neuron}
}

@InProceedings{RufSchmitt:98,
  author	= {B. Ruf and M. Schmitt},
  booktitle	= {Computational Neuroscience: Trends in Research 1998 (to
		  appear)},
  editor	= {J. Bower},
  publisher	= {Plenum Press},
  title		= {Self-organizing maps of spiking neurons using temporal
		  coding},
  year		= {1998},
  keywords	= {unsupervised learning, spiking neuron, self-organizing
		  map}
}

@MastersThesis{Schwaighofer00,
  author	= {Schwaighofer, A.},
  title		= {Fingerprint Matching with Spectral Features},
  school	= {Institute for Theoretical Computer Science, Graz
		  University of Technology, Austria},
  year		= 2000,
  month		= may,
  abstract	= {The underlying purpose of this thesis is to investigate
		  methods of fingerprint recognition which employ principles
		  from the field of machine learning, and that do not require
		  much image pre-processing. Fingerprint images are
		  represented by features derived from their spectrum. The
		  features are to a certain extent invariant with respect to
		  translation and rotation. The features are chosen such that
		  the two classes of identical and different fingerprints are
		  best separated. The features are used in different
		  classification methods, namely nearest neighbour
		  classifier, MLP and SVM, with specifically adapted training
		  methods. The resulting matchers are compared. In addition,
		  a pre-matcher based on these features is constructed,
		  together with figures to describe its characteristics. This
		  thesis makes two major contributions: The spectral features
		  are optimised such that a low-dimensional representation
		  with high discriminative power is obtained. Secondly, the
		  proposed pre-matching system drastically reduces both the
		  error rates and the overall runtime, while the pre-matcher
		  itself requires only little computational effort.},
  aschwaig_label= 1
}

@PhDThesis{Schwaighofer_thesis,
  author	= {Schwaighofer, Anton},
  title		= {Kernel Systems for Regression and Graphical Modelling},
  school	= {Institute for Theoretical Computer Science, Graz
		  University of Technology, Austria},
  year		= 2003,
  month		= {October},
  aschwaig_label= 12
}

@Article{SteimerETAL:09,
  author	= {A. Steimer and W. Maass and R. Douglas},
  title		= {Belief-propagation in networks of spiking neurons},
  journal	= {Neural Computation},
  year		= {2009},
  volume	= {21},
  number	= {},
  pages		= {2502--2523},
  abstract	= {From a theoretical point of view, statistical inference is
		  an attractive model of brain operation. However, it is
		  unclear how to implement these inferential processes in
		  neuronal networks. We offer a solution to this problem by
		  showing in detailed simulations how the Belief-Propagation
		  algorithm on a factor graph can be embedded in a network of
		  spiking neurons. We use pools of spiking neurons as the
		  function nodes of the factor graph. Each pool gathers
		  'messages' in the form of population activities from its
		  input nodes and combines them through its network dynamics.
		  The various output messages to be transmitted over the
		  edges of the graph are each computed by a group of readout
		  neurons that feed in their respective destination pools. We
		  use this approach to implement two examples of factor
		  graphs. The first example is drawn from coding theory. It
		  models the transmission of signals through an unreliable
		  channel and demonstrates the principles and generality of
		  our network approach. The second, more applied example, is
		  of a psychophysical mechanism in which visual cues are used
		  to resolve hypotheses about the interpretation of an
		  object's shape and illumination. These two examples, and
		  also a statistical analysis, all demonstrate good agreement
		  between the performance of our networks and the direct
		  numerical evaluation of beliefpropagation.}
}

@Article{SteinbauerETAL:02,
  author	= {G. Steinbauer and R. Koholka and W. Maass},
  title		= {A very short story about autonomous robots},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  year		= {2002},
  volume	= {8},
  number	= {1},
  pages		= {26--29}
}

@Article{SussilloETAL:07,
  author	= {D. Sussillo and T. Toyoizumi and W. Maass},
  title		= {Self-tuning of neural circuits through short-term synaptic
		  plasticity},
  journal	= {Journal of Neurophysiology},
  year		= {2007},
  volume	= {97},
  pages		= {4079--4095},
  abstract	= {Circuits of neurons in the cortex have a remarkable
		  capability to maintain functional and dynamic stability in
		  spite of changes in the level of external inputs, synaptic
		  plasticity and changes in the circuit structure that occur
		  during development and adult learning. The source of this
		  characteristic stability of cortical circuits has remained
		  a mystery, especially since even stronglysimplified models
		  of such circuits do not exhibit similar stability
		  properties. One simplification that is usually made in such
		  models is that the empirically found nonlinear and diverse
		  inherent short term dynamics (paired-pulse facilitation and
		  depression) of biological synapses is replaced by static
		  and uniform linear synapse models. We show in this article
		  that this is a mistake, since the complex and diverse
		  nonlinear dynamics of biological synapses supports the
		  implementation of powerful control principles that endow
		  circuits of spiking neurons with almost in-vivo like
		  stability properties.}
}

@InProceedings{ToescherETAL:08,
  author	= {Andreas Toescher and Michael Jahrer and Robert
		  Legenstein},
  title		= {Improved Neighborhood-Based Algorithms for Large-Scale
		  Recommender Systems},
  note		= {in press},
  pages		= {},
  booktitle	= {KDD-Cup and Workshop},
  publisher	= {ACM},
  year		= 2008,
  abstract	= {Neighborhood-based algorithms are frequently used modules
		  of recommender systems. Usually, the choice of the
		  similarity measure used for evaluation of neighborhood
		  relationships is crucial for the success of such
		  approaches. In this article we propose a way to calculate
		  similarities by formulating a regression problem which
		  enables us to extract the similarities from the data in a
		  problem-specific way. Another popular approach for
		  recommender systems is regularized matrix factorization
		  (RMF). We present an algorithm -- neighborhood-aware matrix
		  factorization -- which efficiently includes neighborhood
		  information in a RMF model. This leads to increased
		  prediction accuracy. The proposed methods are tested on the
		  Netflix dataset.}
}

@Article{UchizawaETAL:06,
  author	= {K. Uchizawa and R. Douglas and W. Maass},
  title		= {On the Computational Power of Threshold Circuits with
		  Sparse Activity},
  journal	= {Neural Computation},
  year		= 2006,
  volume	= 18,
  number	= 12,
  pages		= {2994--3008},
  abstract	= {Circuits composed of threshold gates (McCulloch-Pitts
		  neurons, or perceptrons) are simplified models of neural
		  circuits with the advantage that they are theoretically
		  more tractable than their biological counterparts. However,
		  when such threshold circuits are designed to perform a
		  specific computational task they usually differ in one
		  important respect from computations in the brain: they
		  require very high activity. On average every second
		  threshold gate fires (sets a ``1'' as output) during a
		  computation. By contrast, the activity of neurons in the
		  brain is much more sparse, with only about 1\% of neurons
		  firing. This mismatch between threshold and neuronal
		  circuits is due to the particular complexity measures
		  (circuit size and circuit depth) that have been minimized
		  in previous threshold circuit constructions. In this
		  article we investigate a new complexity measure for
		  threshold circuits, {\em energy complexity}, whose
		  minimization yields computations with sparse activity. We
		  prove that all computations by threshold circuits of
		  polynomial size with entropy $O(\log n)$ can be
		  restructured so that their energy complexity is reduced to
		  a level near the {\em entropy of circuit states}. This
		  entropy of circuit states is a novel circuit complexity
		  measure, which is of interest not only in the context of
		  threshold circuits, but for circuit complexity in general.
		  As an example of how this measure can be applied we show
		  that any polynomial size threshold circuit with entropy
		  $O(\log n)$ can be simulated by a polynomial size threshold
		  circuit of depth 3.
		  
		  Our results demonstrate that the structure of circuits
		  which result from a minimization of their energy complexity
		  is quite different from the structure which results from a
		  minimization of previously considered complexity measures,
		  and potentially closer to the structure of neural circuits
		  in the nervous system. In particular, different pathways
		  are activated in these circuits for different classes of
		  inputs. This article shows that such circuits with sparse
		  activity have a surprisingly large computational power.}
}

@InProceedings{UchizawaETAL:06a,
  author	= {K. Uchizawa and R. Douglas and W. Maass},
  booktitle	= {Proceedings of the 33rd International Colloquium on
		  Automata, Languages and Programming, ICALP (1) 2006,
		  Venice, Italy, July 10-14, 2006, Part I},
  title		= {Energy Complexity and Entropy of Threshold Circuits},
  year		= {2006},
  volume	= {4051},
  pages		= {631--642},
  publisher	= {Springer},
  series	= {Lecture Notes in Computer Science},
  editor	= {M. Bugliesi and B. Preneel and V. Sassone and I. Wegener},
  abstract	= {Circuits composed of threshold gates (McCulloch-Pitts
		  neurons, or perceptrons) are simplified models of neural
		  circuits with the advantage that they are theoretically
		  more tractable than their biological counterparts. However,
		  when such threshold circuits are designed to perform a
		  specific computational task they usually differ in one
		  important respect from computations in the brain: they
		  require very high activity. On average every second
		  threshold gate fires (sets a ``1'' as output) during a
		  computation. By contrast, the activity of neurons in the
		  brain is much more sparse, with only about 1\% of neurons
		  firing. This mismatch between threshold and neuronal
		  circuits is due to the particular complexity measures
		  (circuit size and circuit depth) that have been minimized
		  in previous threshold circuit constructions. In this
		  article we investigate a new complexity measure for
		  threshold circuits, {\em energy complexity}, whose
		  minimization yields computations with sparse activity. We
		  prove that all computations by threshold circuits of
		  polynomial size with entropy $O(\log n)$ can be
		  restructured so that their energy complexity is reduced to
		  a level near the {\em entropy of circuit states}. This
		  entropy of circuit states is a novel circuit complexity
		  measure, which is of interest not only in the context of
		  threshold circuits, but for circuit complexity in general.
		  As an example of how this measure can be applied we show
		  that any polynomial size threshold circuit with entropy
		  $O(\log n)$ can be simulated by a polynomial size threshold
		  circuit of depth 3.}
}

@TechReport{WinterETAL:06,
  author	= {M. Winter and H. Bischof and M. Rasch and W. Maass},
  title		= {Results and open problems regarding the role of local
		  feature descriptors for image classification in computer
		  vision, and image representations in primary visual
		  cortex},
  institution	= {Technische Universitaet Graz},
  year		= {2006}
}

@InProceedings{a-aagt-88,
  author	= {F. Aurenhammer},
  title		= {Algorithmic aspects of {G}ale transforms (abstract)},
  booktitle	= {Proc. $13^{th}$ 3-Ann. Int'l Symp. Mathematical
		  Programming},
  pages		= 176,
  year		= 1988,
  address	= {Tokyo, Japan}
}

@Article{a-caecc-87,
  author	= {F. Aurenhammer},
  title		= {A criterion for the affine equivalence of cell complexes
		  in {$R^d$} and convex polyhedra in {$R^{d+1}$}},
  journal	= {Discrete \& Computational Geometry},
  year		= 1987,
  volume	= 2,
  number	= 1,
  pages		= {49--64},
  note		= {[IIG-Report-Series 205, TU Graz, Austria, 1985]},
  abstract	= {A criterion is gives that decides, for a convex tiling $C$
		  of $R^d$, whether $C$ is the projection of the faces in the
		  boundary of some convex polyhedron $P$ in $R^{d+1}$. Its
		  applicability is restricted neither by the properties nor
		  by the dimension of $C$. It turns out to be simpler than
		  other criteria and allows the easy examination of various
		  classes of cell complexes. In addition, the criterion is
		  constructive, that is, it can be used to construct $P$
		  provided it exists.}
}

@PhDThesis{a-ccphn-97,
  author	= {O. Aichholzer},
  title		= {Combinatorial \& Computational Properties of the Hypercube
		  - New Results on Covering, Slicing, Clustering and
		  Searching on the Hypercube},
  school	= {IGI-TU Graz, Austria},
  year		= 1997,
  abstract	= { The central topic of this thesis is the $d$-dimensional
		  hypercube ($d$-cube). Despite of its simple definition, the
		  $d$-cube has been an object of study from various points of
		  view. The contributions of this thesis are twofold. In the
		  combinatorial part we investigate the structure of
		  hyperplanes intersecting the $d$-cube: How many and which
		  types of hyperplanes can be spanned by vertices of the
		  $d$-cube? What is the minimum number of skew hyperplanes
		  that cover all the vertices of the $d$-cube? What is the
		  best arrangement of slicing hyperplanes to linearly
		  separate all neighbours on the $d$-cube? Such results are
		  then used e.g. for determining the maximal number of facets
		  a 5-dimensional $0/1$-polytope can achieve. In the second
		  part of the thesis we consider algorithmical properties of
		  the $d$-cube. We first obtain efficient clustering methods
		  for objects represented as binary strings of fixed length
		  $d$, including various agglomerative hierarchical methods
		  like single linkage and complete linkage. Utilizing these
		  hierarchical structures we derive a new efficient approach
		  to the ${0,1}$-string searching problem, where for a given
		  set of binary strings of fixed length $d$ and a query
		  string one asks for the most similar string in this set.
		  Motivation for investigating these problems stems, among
		  other areas, from coding theory, communication theory, and
		  learning theory.}
}

@Article{a-cgeqr-01,
  author	= {F. Aurenhammer},
  title		= {Computational geometry -- some easy questions and their
		  recent solutions},
  journal	= {J. Universal Computer Science},
  year		= 2001,
  volume	= 7,
  pages		= {338--354},
  note		= {Special Issue.},
  abstract	= {We address three basic questions in computational geometry
		  which can be phrased in simple terms but have only recently
		  received (more or less) satisfactory answers: point set
		  enumeration, optimum triangulation, and polygon
		  decomposition.}
}

@TechReport{a-ch-96,
  author	= {O. Aichholzer},
  title		= {Clustering the Hypercube},
  institution	= {SFB 'Optimierung und Kontrolle', TU Graz, Austria},
  year		= 1996,
  type		= {SFB-Report},
  number	= {F003-93},
  abstract	= {In this paper we consider various clustering methods for
		  objects represented as binary strings of fixed length $d$.
		  The dissimilarity of two given objects is the number of
		  disagreeing bits, that is, their Hamming distance.
		  Clustering these objects can be seen as clustering a subset
		  of the vertices of a $d$-dimensional hypercube, and thus is
		  a geometric problem in d dimensions. We give algorithms for
		  various agglomerative hierarchical methods (including
		  single linkage and complete linkage) as well as for
		  two-clusterings and divisive methods.\\ We only present
		  linear space algorithms since for most practical
		  applications the number of objects to be clustered is
		  usually to large for non-linear space solutions to be
		  practicable. All algorithms are easy to implement and the
		  constants in their asymptotic runtime are small. We give
		  experimental results for all cluster methods considered,
		  and for uniformly distributed hypercube vertices as well as
		  for specially chosen sets. These experiments indicate that
		  our algorithms work well in practice.}
}

@InProceedings{a-e0ssb-98,
  author	= {O. Aichholzer},
  title		= {Efficient $\{0,1\}$-String Searching Based on
		  Pre-clustering},
  booktitle	= {Proc. $14^{th}$ European Workshop on Computational
		  Geometry CG '98},
  pages		= {11--13},
  year		= 1998,
  address	= {Barcelona, Spain},
  note		= {[SFB Report F003-94, TU Graz, Austria, 1996]},
  abstract	= {In this paper we consider the ${0,1}$-string searching
		  problem. For a given set $S$ of binary strings of fixed
		  length $d$ and a query string $q$ one asks for the most
		  similar string in $S$. Thereby the dissimilarity of two
		  given strings is the number of disagreeing bits, that is,
		  their Hamming distance. We present an efficient
		  ${0,1}$-string searching algorithm based on hierarchical
		  pre-clustering. To this end we give several useful
		  observations on the inter- and intra-cluster distances.\\
		  The presented algorithms are easy to implement and we give
		  exhaustive experimental results for uniformly distributed
		  sets as well as for specially chosen strings. These
		  experiments indicate that our algorithms work well in
		  practice.}
}

@InCollection{a-ep0pd-00,
  author	= {O. Aichholzer},
  title		= {Extremal Properties of 0/1-Polytopes of Dimension 5},
  booktitle	= {Polytopes - Combinatorics and Computation},
  publisher	= {Birkhaeuser},
  year		= 2000,
  editor	= {G. Ziegler and G. Kalai},
  pages		= {111--130},
  note		= {[SFB-Report F003-132, TU Graz, Austria, 1998]},
  htmlnote	= {You can also <A
		  HREF="http://www.igi.TUgraz.at/oaich/info01poly.html">investigate
		  0/1-polytopes by e-mail</A>!},
  abstract	= {In this paper we consider polytopes whose vertex
		  coordinates are $0$ or $1$, so called $0/1$-polytopes. For
		  the first time we give a complete enumeration of all
		  $0/1$-polytopes of dimension $5$, which enables us to
		  investigate various of their combinatorial extremal
		  properties.\\ For example we show that the maximum number
		  of facets of a five-dimensional $0/1$-polytope is $40$,
		  answering an open question of Ziegler. Based on the
		  complete enumeration for dimension $5$ we obtain new
		  results for $2$-neighbourly $0/1$-polytopes for higher
		  dimensions.}
}

@InProceedings{a-gcvtp-93,
  author	= {F. Aurenhammer},
  title		= {Geometric clustering and {V}oronoi-type partitions},
  booktitle	= {$16^{th}$ IFIP Conf. System Modelling and Optimization},
  year		= 1993,
  pages		= {93-94},
  address	= {Compiegne, France}
}

@Article{a-gefsi-95,
  author	= {F. Aurenhammer},
  title		= {Guest editor's foreword of the special issue on
		  computational geometry},
  journal	= {Int'l Journal of Computational Geometry \& Applications},
  year		= 1995,
  volume	= {5},
  pages		= {1}
}

@TechReport{a-gpd-83,
  author	= {F. Aurenhammer},
  title		= {On the generality of power diagrams},
  institution	= {TU Graz, Austria},
  year		= 1983,
  type		= {IIG Report},
  number	= {F126},
  abstract	= {A set $s = \{ x \mid <x-z>^2 = r^2 \}$ in Euclidean space
		  is called a sphere with center $z$ and positive radius $r$.
		  For an arbitrary point $x$, $<x-z>^2 = r^2$ is the power of
		  $x$ with respect to $s$. Extending these concepts to
		  imaginary radii, they are exploited to show the equivalence
		  of two types of cell complexes: those that can be obtained
		  by projection of convex polyhedra, and those that come from
		  projecting levels in arrangement of hyperplanes. As a
		  consequence, higher-order Voronoi diagrams can be
		  constructed by determining a convex hull.}
}

@InProceedings{a-gv-85,
  author	= {F. Aurenhammer},
  title		= {Gewichtete {V}oronoidiagramme},
  booktitle	= {Workshop on Computational Geometry CG '85},
  year		= 1985,
  address	= {Karlsruhe, Germany}
}

@PhDThesis{a-gvdgd-84,
  author	= {F. Aurenhammer},
  title		= {Gewichtete {V}oronoi {D}iagramme: {G}eometrische {D}eutung
		  und {K}onstruktions-{A}lgorithmen},
  school	= {IIG-TU Graz, Austria},
  year		= 1984,
  note		= {Report B53},
  abstract	= {Das oft als Computational Geometry bezeichnete Teilgebiet
		  der Computerwissenschaften beschaeftigt sich mit der
		  algorithmischen Loesung elementargeometrischer Probleme.
		  Einen Schwerpunkt in der Computational Geometry bildet die
		  Untersuchung von Abstandsproblemen. In diesem Zusammenhang
		  spielt das Voronoi Diagramm einer endlichen Punktmenge eine
		  zentrale Rolle. Die vorliegende Arbeit ist eine
		  geometrische und algorithmische Studie sogenannter
		  gewichteter Voronoi Diagramme. Diese Diagramme leiten sich
		  aus dem Original durch Ersetzen des Euklidabstandes durch
		  individuell gewichtete Abstaende her. Das
		  Anwendungsspektrum solcher Konzepte umfasst die
		  Wissenschaften Geographie, Oekonomie, Biologie, Mathematik,
		  Physik und Chemie. Die Hauptergebnisse dieser Arbeit sind
		  die Erforschung der Verwandtschaft gewichteter Diagramme zu
		  anderen geometrischen Objekten (konvexe Huellen,
		  Zellkomplexe und Arrangements) mit Hilfe geometrischer
		  Transformationen, und der Entwurf effizienter Algorithmen
		  fuer ihre Konstruktion. Ein Teil der Resultate wurde
		  bereits in Schnelldruckserien und wissenschaftlichen
		  Journalen veroeffentlicht.}
}

@MastersThesis{a-hhkq-92,
  author	= {O. Aichholzer},
  title		= {{H}yperebenen in {H}yperkuben - {E}ine {K}lassifizierung
		  und {Q}uantifizierung},
  school	= {IGI-TU Graz, Austria},
  year		= 1992,
  abstract	= {In dieser Arbeit werden affine Hyperebenen in
		  hoeherdimensionalen Raeumen behandelt, die den
		  $n$-dimensionalen Hyperkubus schneiden. Dabei konzentriert
		  sich die Untersuchung auf folgende Fragestellung: Wieviele
		  und welche Arten von Hyperebenen gibt es, die durch
		  Eckpunkte des Hyperkubus eindeutig festgelegt sind? Neben
		  der Betrachtung, wieviele solcher Hyperebenen existieren,
		  wird eine Klassifizierung nach verschiedenen Kriterien wie
		  Symmetrie, Parallelitaet zu den Koordinatenachsen, Anzahl
		  der geschnittenen Eckpunkte etc. untersucht. Die Arbeit
		  enthaelt sowohl eine vollstaendige enumerative Berechnung
		  aller relevanten Werte bis einschliesslich der achten
		  Dimension, als auch die theoretische Herleitung allgemein
		  gueltiger Saetze ueber solche Hyperebenen. Die Beitraege
		  dieser Arbeit fallen in das Gebiet der geometrischen
		  Kombinatorik und finden sowohl in der Codierungs- und
		  Lerntheorie als auch in der linearen Optimierung sowie im
		  VLSI-Design Anwendung.}
}

@Article{a-iadbu-88,
  author	= {F. Aurenhammer},
  title		= {Improved algorithms for discs and balls using power
		  diagrams},
  journal	= {Journal of Algorithms},
  year		= 1988,
  volume	= 9,
  number	= 2,
  pages		= {151--161},
  note		= {[IIG-Report-Series 209, TU Graz, Austria, 1985]},
  abstract	= {The properties of a particular generalization of Voronoi
		  diagrams called power diagrams are exploited to obtain new
		  and improved algorithms for union, intersection, and
		  measure problems for discs and balls.}
}

@InProceedings{a-jschc-87,
  author	= {F. Aurenhammer},
  title		= {{J}ordan sorting via convex hulls of certain non-simple
		  polygons},
  booktitle	= {Proc. $3^{rd}$ Ann. ACM Symp. Computational Geometry},
  pages		= {21--29},
  year		= 1987,
  address	= {Waterloo, Canada}
}

@Article{a-lai-97,
  author	= {P. Auer},
  title		= {On Learning from Ambiguous Information},
  journal	= {Periodica Polytechnica Electrical Engineering},
  year		= {1998},
  volume	= {42},
  number	= {1},
  pages		= {115-122}
}

@Article{a-lcpd-88,
  author	= {F. Aurenhammer},
  title		= {Linear combinations from power domains},
  journal	= {Geometriae Dedicata},
  year		= 1988,
  volume	= 28,
  pages		= {45--52},
  note		= {[IIG-Report-Series 243, TU Graz, Austria, 1987]},
  abstract	= {The well-known concept of power domains defined via
		  subsets of a finite set $S$ of weighted points in $R^d$ is
		  exploited to obtain various linear combinations among the
		  points in $S$. This generalizes a result of Sibson who
		  considered the special case of singleton subsets and
		  unweighted points.}
}

@InProceedings{a-lpt-95,
  author	= {O. Aichholzer},
  title		= {Local properties of triangulations},
  booktitle	= {Proc. $11^{th}$ European Workshop on Computational
		  Geometry CG '95},
  pages		= {27--30},
  year		= 1995,
  address	= {Hagenberg/Linz, Austria},
  abstract	= {In this paper we study local properties of two well known
		  triangulations of a planar point set $S$, both of which are
		  defined in a non-local way. The first one is the greedy
		  triangulation (GT) that is defined procedurally: it can be
		  obtained by starting with the empty set and at each step
		  adding the shortest compatible edge between two points of
		  $S$, where a compatible edge is defined to be an edge that
		  does not cross any of the previously inserted edges. The
		  other triangulation we deal with is the minimum-weight
		  triangulation (MWT) which minimizes the sum of the length
		  of the edges among all possible triangulations of $S$. We
		  present several results on exclusion- and inclusion-regions
		  for these two triangulations.}
}

@InProceedings{a-ndrcv-86,
  author	= {F. Aurenhammer},
  title		= {A new duality result concerning {V}oronoi diagrams},
  booktitle	= {Proc. $13^{th}$ Ann. ICALP, Lecture Notes in Computer
		  Science},
  pages		= {21--32},
  year		= 1986,
  volume	= 226,
  address	= {Rennes, France},
  publisher	= {Springer Verlag},
  abstract	= {A new duality between order-$k$ Voronoi diagrams in $E^d$
		  and convex hulls in $E^{d+1}$ is established. It implies a
		  reasonably simple algorithm for computing the order-$k$
		  Voronoi diagram for $n$ points in the palne in $O(k^2 n
		  \log n)$ time and optimal $O(k(n-k))$ space.}
}

@Article{a-ndrcv-90,
  author	= {F. Aurenhammer},
  title		= {A new duality result concerning {V}oronoi diagrams},
  journal	= {Discrete \& Computational Geometry},
  year		= 1990,
  volume	= 5,
  number	= 3,
  pages		= {243--254},
  note		= {[IIG-Report-Series 216, TU Graz, Austria, 1985]},
  abstract	= {A new duality between order-$k$ Voronoi diagrams in $E^d$
		  and convex hulls in $E^{d+1}$ is established. It implies a
		  reasonably simple algorithm for computing the order-$k$
		  Voronoi diagram for $n$ points in the palne in $O(k^2 n
		  \log n)$ time and optimal $O(k(n-k))$ space.}
}

@Article{a-odwvd-86,
  author	= {F. Aurenhammer},
  title		= {The one-dimensional weighted {V}oronoi diagram},
  journal	= {Information Processing Letters},
  year		= 1986,
  volume	= 22,
  number	= 3,
  pages		= {119--123},
  note		= {[IIG-Report-Series F110, TU Graz, Austria, 1983]}
}

@Article{a-olsts-88a,
  author	= {F. Aurenhammer},
  title		= {On-line sorting of twisted sequences in linear time},
  journal	= {BIT},
  year		= 1988,
  volume	= 28,
  pages		= {194--204},
  note		= {[IIG-Report-Series 232, TU Graz, Austria, 1987]},
  abstract	= {A sequence of real numbers is called twisted if it can be
		  produced from the sorted sequence by repeatedly reversing
		  the order of consecutive subsequences. It is shown that
		  twisted sequences constitute a class of exponentially many
		  members each of which can be recognized and sorted, by a
		  simple on-line algorithm, in linear time.}
}

@InProceedings{a-olsts-88b,
  author	= {F. Aurenhammer},
  title		= {On-line sorting of twisted sequences in linear time},
  booktitle	= {$2^{nd}$ Workshop on Computational Geometry and Discrete
		  Algorithms},
  year		= 1988,
  address	= {Osaka, Japan}
}

@Article{a-pdpaa-87,
  author	= {F. Aurenhammer},
  title		= {Power diagrams: properties, algorithms, and applications},
  journal	= {SIAM Journal on Computing},
  year		= 1987,
  volume	= 16,
  number	= 1,
  pages		= {78--96},
  note		= {[IIG-Report-Series F120, TU Graz, Austria, 1983]},
  abstract	= {The power $pow(x,s)$ of a point $x$ with respect to a
		  sphere $s$ in Euclidean $d$-space $E^d$ is given by
		  $d^2(x,z)-r^2$, where $d$ denotes the Euclidean distance
		  function, and $z$ and $r$ are the center and the radius of
		  $s$. The power diagram of a finite set $S$ of spheres in
		  $E^d$ is a cell complex that associates each $s \in S$ with
		  the convex domain $\{x \in E^d \mid pow(x,s) < pow(x,t),
		  \mbox{for all} t \in S - \{s\} \}$. The close relationship
		  to convex hulls and arrangements of hyperplanes is
		  investigated and exploited. Efficient algorithms that
		  compute the power diagram and its order-$k$ modifications
		  are obtained. Among the applications of these results are
		  algorithms for detecting $k$-sets, for union and
		  intersection problems for cones and paraboloids, and for
		  constructing weighted Voronoi diagrams and Voronoi diagrams
		  for spheres. Upper space bounds for these geometric
		  problems are derived.}
}

@InProceedings{a-pgdtf-05,
  author	= {F. Aurenhammer},
  title		= {Pre-triangulations: A generalization of {D}elaunay
		  triangulations and flips},
  booktitle	= {$2^{nd}$ Intern. Symp. on Voronoi Diagrams in Science and
		  Engineering},
  pages		= {235},
  year		= 2005,
  address	= {Hanyang University, Seoul, Korea},
  note		= {(plenar talk)}
}

@InProceedings{a-psd-03,
  author	= {F. Aurenhammer},
  title		= {Pseudo-simplices and their derivation},
  booktitle	= {Voronoi Conference on Analytic Number Theory and Spatial
		  Tesselations},
  pages		= {11},
  year		= 2003,
  address	= {Inst. Math. National Academy of Sciences, Kiev, Ukraine}
}

@InProceedings{a-pt-97,
  author	= {O. Aichholzer},
  title		= {The Path of a Triangulation},
  booktitle	= {Proc. $13^{th}$ European Workshop on Computational
		  Geometry CG '97},
  pages		= {1--3},
  year		= 1997,
  address	= {Wuerzburg, Germany},
  htmlnote	= {For an implementation see my page on <A
		  HREF="http://www.igi.TUgraz.at/oaich/infotricount.html">triangulation
		  counting</A>.},
  abstract	= {For a planar point set $S$ let $T$ be a triangulation of
		  $S$ and $l$ a line properly intersecting $T$. We show that
		  there always exists a unique path in $T$ with certain
		  properties with respect to $l$. This path is then
		  generalized to (non triangulated) point sets restricted to
		  the interior of simple polygons. This so-called
		  triangulation path enables us to treat several
		  triangulation problems on planar point sets in a divide \&
		  conquer-like manner. For example, we give the first
		  algorithm for counting triangulations of a planar point set
		  which is observed to run in time sublinear in the number of
		  triangulations. Moreover, the triangulation path proves to
		  be useful for the computation of optimal triangulations.}
}

@InProceedings{a-pt-99,
  author	= {O. Aichholzer},
  title		= {The Path of a Triangulation},
  booktitle	= {Proc. $15^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {14--23},
  year		= 1999,
  address	= {Miami Beach, Florida, USA},
  htmlnote	= {For an implementation see my page on <A
		  HREF="http://www.igi.TUgraz.at/oaich/infotricount.html">triangulation
		  counting</A>.},
  abstract	= {For a planar point set $S$ let $T$ be a triangulation of
		  $S$ and $l$ a line properly intersecting $T$. We show that
		  there always exists a unique path in $T$ with certain
		  properties with respect to $l$. This path is then
		  generalized to (non triangulated) point sets restricted to
		  the interior of simple polygons. This so-called
		  triangulation path enables us to treat several
		  triangulation problems on planar point sets in a divide \&
		  conquer-like manner. For example, we give the first
		  algorithm for counting triangulations of a planar point set
		  which is observed to run in time sublinear in the number of
		  triangulations. Moreover, the triangulation path proves to
		  be useful for the computation of optimal triangulations.}
}

@Article{a-rbgtv-90,
  author	= {F. Aurenhammer},
  title		= {A relationship between {G}ale transforms and {V}oronoi
		  diagrams},
  journal	= {Discrete Applied Mathematics},
  year		= 1990,
  volume	= 28,
  pages		= {83--91},
  note		= {[IIG-Report-Series 247, TU Graz, Austria, 1988]},
  abstract	= {Gale transforms and Voronoi diagrams for finite point sets
		  in $R^d$ are two structures well known in discrete and
		  computational geometry. It is shown that they are related
		  in the sense that Gale transforms of a point set can be
		  computed from (generalizations of) its Voronoi diagram.}
}

@Article{a-rpccc-87,
  author	= {F. Aurenhammer},
  title		= {Recognizing polytopical cell complexes and constructing
		  projection polyhedra},
  journal	= {Journal of Symbolic Computation},
  year		= 1987,
  volume	= 3,
  number	= 3,
  pages		= {249--255},
  note		= {[IIG-Report-Series 203, TU Graz, Austria, 1985]},
  abstract	= {A simple cell complex $C$ in Euclidean $d$-space $E^d$ is
		  a covering of $E^d$ by finitely many convex $j$-dimensional
		  polyhedra (the $j$-faces of $C$), each of which is in the
		  closure of exactly $d-j+1$ $d$-faces of $C$. An algorithm
		  that recognises when $C$ is the projection of the set of
		  faces bounding some convex polyhedron $P(C)$ in $E^{d+1}$,
		  and that constructs $P(C)$ provided its existence is
		  outlined. The method is optimal at least for $d=2$. No
		  complexity results were previously known for both problems.
		  The results have applications in statics, to the
		  recognition of Voronoi diagrams, and to planar point
		  location.}
}

@InProceedings{a-srlfa-99,
  author	= {P. Auer},
  title		= {An Improved On-line Algorithm for Learning Linear
		  Evaluation Functions},
  booktitle	= {Proc. 13th Ann. Conf. Computational Learning Theory},
  pages		= {118--125},
  year		= {2000},
  publisher	= {Morgan Kaufmann}
}

@InProceedings{a-tnnb-98,
  author	= {P. Auer},
  title		= {Some thoughts on Boosting and Neural Networks},
  booktitle	= {Beitraege zum 3.~Cottbuser Workshop `Aspekte des
		  Neuronalen Lernens' CoWAN'98},
  editor	= {L. Cromme and T. Kolb and H. Koch},
  optvolume	= {},
  optnumber	= {},
  optseries	= {},
  year		= {1998},
  optorganization={},
  publisher	= {Shaker Verlag},
  address	= {Cottbus, Germany},
  month		= {October},
  pages		= {11--28},
  note		= {Invited paper}
}

@Article{a-ucbeeto-2002,
  author	= {P. Auer},
  title		= {Using Confidence Bounds for Exploitation-Exploration
		  Trade-offs},
  journal	= {J. Machine Learning Research},
  year		= {2002},
  volume	= {3(Nov)},
  pages		= {397--422},
  note		= {A preliminary version has appeared in {\em Proc. of the
		  41th Annual Symposium on Foundations of Computer Science}}
}

@InProceedings{a-ugtcg-88,
  author	= {F. Aurenhammer},
  title		= {Using {G}ale transforms in computational geometry},
  booktitle	= {Proc. $4^{th}$ Workshop on Computational Geometry CG '88,
		  Lecture Notes in Computer Science},
  pages		= {202--216},
  year		= 1988,
  volume	= 333,
  address	= {Wuerzburg, Germany},
  publisher	= {Springer Verlag},
  abstract	= {Let $P$ denote a set of $n \geq d+1$ points in $d$-space
		  $R^d$. A Gale transform of $P$ assigns to each point in $P$
		  a vector in space $R^{d+1}$ such that the resulting
		  $n$-tuple of vectors reflects all affinely invariant
		  properties of $P$. First utilized by Gale in the 1950s,
		  Gale transforms have been recognized as a powerful tool in
		  combinatorial geometry. This paper introduces Gale
		  transforms to computational geometry. It offers a direct
		  algorithm for their construction and sketches applications
		  to convex hull and visibility problems. An application to
		  scene analysis is worked out in some more detail.}
}

@Article{a-ugtcg-91,
  author	= {F. Aurenhammer},
  title		= {Using {G}ale transforms in computational geometry},
  journal	= {Mathematical Programming},
  year		= 1991,
  volume	= {B 52},
  pages		= {179--190},
  note		= {Special Issue. [IIG-Report-Series 248, TU Graz, Austria,
		  1988]},
  abstract	= {Let $P$ denote a set of $n \geq d+1$ points in $d$-space
		  $R^d$. A Gale transform of $P$ assigns to each point in $P$
		  a vector in space $R^{d+1}$ such that the resulting
		  $n$-tuple of vectors reflects all affinely invariant
		  properties of $P$. First utilized by Gale in the 1950s,
		  Gale transforms have been recognized as a powerful tool in
		  combinatorial geometry. This paper introduces Gale
		  transforms to computational geometry. It offers a direct
		  algorithm for their construction and addresses applications
		  to convex hull and visibility problems. An application to
		  scene analysis is worked out in detail.}
}

@InProceedings{a-uucbol-00,
  author	= {P. Auer},
  title		= {Using Upper Confidence Bounds for Online Learning},
  booktitle	= {Proceedings of the 41th Annual Symposium on Foundations of
		  Computer Science},
  pages		= {270--293},
  year		= {2000},
  publisher	= {IEEE Computer Society}
}

@Article{a-vdsfg-91a,
  author	= {F. Aurenhammer},
  title		= {{V}oronoi diagrams -- a survey of a fundamental geometric
		  data structure},
  journal	= {ACM Computing Surveys},
  year		= 1991,
  volume	= 23,
  number	= 3,
  pages		= {345--405},
  note		= {{H}abilitationsschrift. [Report B 90-09, FU Berlin,
		  Germany, 1990]},
  abstract	= {This paper presents a survey of the Voronoi diagram, one
		  of the most fundamental data structures in computational
		  geometry. It demonstrates the importance and usefulness of
		  the Voronoi diagram in a wide variety of fields inside and
		  outside computer science and surveys the history of its
		  development. The paper puts particular emphasis on the
		  unified exposition of its mathematical and algorithmic
		  properties. Finally, the paper provides the first
		  comprehensive bibliography on Voronoi diagrams and related
		  structures.}
}

@Article{a-vdsfg-91b,
  author	= {F. Aurenhammer},
  title		= {{V}oronoi diagrams -- a survey of a fundamental geometric
		  data structure},
  journal	= {bit acm computing surveys '91 (Japanese translation),
		  Kyoritsu Shuppan Co., Ltd.},
  year		= 1993,
  pages		= {131--185}
}

@Article{a-wsdaq-02,
  author	= {P. Auer},
  title		= {Why Students Don't Ask Questions},
  journal	= {TELEMATIK},
  year		= {2002},
  pages		= {21--23},
  volume	= {8},
  number	= {1},
  note		= {Special Issue on Foundations of Information Processing for
		  the 21st Century},
  optannote	= {}
}

@Article{a-wsfsd-07,
  author	= {F. Aurenhammer},
  title		= {Weighted skeletons and fixed-share decomposition},
  journal	= {Computational Geometry: Theory and Applications},
  year		= {2007},
  volume	= {40},
  pages		= {93-101},
  abstract	= {We introduce the concept of weighted skeleton of a polygon
		  and present various decomposition and optimality results
		  for this skeletal structure when the underlying polygon is
		  convex.}
}

@InProceedings{aa-chh-94,
  author	= {O. Aichholzer and F. Aurenhammer},
  title		= {Classifying hyperplanes in hypercubes},
  booktitle	= {Proc. $10^{th}$ European Workshop on Computational
		  Geometry CG '94},
  pages		= {53--57},
  year		= 1994,
  address	= {Santander, Spain},
  abstract	= {We consider hyperplanes spanned by vertices of the unit
		  $d$-cube. We classify these hyperplanes by parallelism to
		  coordinate axes, by symmetry of the $d$-cube vertices they
		  avoid, as well as by so-called hull-honesty. (Hull-honest
		  hyperplanes are those whose intersection figure with the
		  $d$-cube coincides with the convex hull of the $d$-cube
		  vertices they contain; they do not cut $d$-cube edges
		  properly.) We describe relationships between these classes,
		  and give the exact number of hull-honest hyperplanes, in
		  general dimensions. An experimental enumeration of all
		  spanned hyperplanes up to dimension eight showed us the
		  intrinsic difficulty of developing a general enumeration
		  scheme. Motivation for considering such hyperplanes stems
		  from coding theory, from linear programming, and from the
		  theory of machine learning.}
}

@Article{aa-chh-96,
  author	= {O. Aichholzer and F. Aurenhammer},
  title		= {Classifying hyperplanes in hypercubes},
  journal	= {SIAM Journal on Discrete Mathematics},
  year		= 1996,
  volume	= 9,
  number	= 2,
  pages		= {225--232},
  note		= {[IIG-Report-Series 408, TU Graz, Austria, 1995]},
  abstract	= {We consider hyperplanes spanned by vertices of the unit
		  $d$-cube. We classify these hyperplanes by parallelism to
		  coordinate axes, by symmetry of the $d$-cube vertices they
		  avoid, as well as by so-called hull-honesty. (Hull-honest
		  hyperplanes are those whose intersection figure with the
		  $d$-cube coincides with the convex hull of the $d$-cube
		  vertices they contain; they do not cut $d$-cube edges
		  properly.) We describe relationships between these classes,
		  and give the exact number of hull-honest hyperplanes, in
		  general dimensions. An experimental enumeration of all
		  spanned hyperplanes up to dimension eight showed us the
		  intrinsic difficulty of developing a general enumeration
		  scheme. Motivation for considering such hyperplanes stems
		  from coding theory, from linear programming, and from the
		  theory of machine learning.}
}

@InProceedings{aa-ssgpf-96,
  author	= {O. Aichholzer and F. Aurenhammer},
  title		= {Straight skeletons for general polygonal figures},
  booktitle	= {Proc. $2^{nd}$ Ann. Int'l. Computing and Combinatorics
		  Conf. COCOON'96, Lecture Notes in Computer Science},
  pages		= {117--126},
  year		= 1996,
  volume	= 1090,
  address	= {Hong Kong},
  publisher	= {Springer Verlag},
  note		= {[IIG-Report-Series 423, TU Graz, Austria, 1995]},
  abstract	= {A novel type of skeleton for general polygonal figures,
		  the straight skeleton $S(G)$ of a planar straight line
		  graph $G$, is introduced and discussed. Exact bounds on the
		  size of $S(G)$ are derived. The straight line structure of
		  $S(G)$ and its lower combinatorial complexity may make
		  $S(G)$ preferable to the widely used Voronoi diagram (or
		  medial axis) of $G$ in several applications. We explain why
		  $S(G)$ has no Voronoi diagram based interpretation and why
		  standard construction techniques fail to work. A simple
		  $O(n)$ space algorithm for constructing $S(G)$ is proposed.
		  The worst-case running time is $O(n^3 \log n)$, but the
		  algorithm can be expected to be practically efficient, and
		  it is easy to implement. We also show that the concept of
		  $S(G)$ is flexible enough to allow an individual weighting
		  of the edges and vertices of $G$, without changes in the
		  maximal size of $S(G)$, or in the method of construction.
		  Apart from offering an alternative to Voronoi-type
		  skeletons, these generalizations of $S(G)$ have
		  applications to the reconstruction of a geographical
		  terrain from a given river map, and to the construction of
		  a polygonal roof above a given layout of ground walls.}
}

@InCollection{aa-ssgpf-98,
  author	= {O. Aichholzer and F. Aurenhammer},
  title		= {Straight skeletons for general polygonal figures in the
		  plane},
  booktitle	= {Voronoi's Impact on Modern Sciences II},
  pages		= {7--21},
  publisher	= {Proc. Institute of Mathematics of the National Academy of
		  Sciences of Ukraine},
  year		= 1998,
  editor	= {A.M. Samoilenko},
  volume	= 21,
  address	= {Kiev, Ukraine},
  abstract	= {A novel type of skeleton for general polygonal figures,
		  the straight skeleton $S(G)$ of a planar straight line
		  graph $G$, is introduced and discussed. Exact bounds on the
		  size of $S(G)$ are derived. The straight line structure of
		  $S(G)$ and its lower combinatorial complexity may make
		  $S(G)$ preferable to the widely used Voronoi diagram (or
		  medial axis) of $G$ in several applications. We explain why
		  $S(G)$ has no Voronoi diagram based interpretation and why
		  standard construction techniques fail to work. A simple
		  $O(n)$ space algorithm for constructing $S(G)$ is proposed.
		  The worst-case running time is $O(n^3 \log n)$, but the
		  algorithm can be expected to be practically efficient, and
		  it is easy to implement. We also show that the concept of
		  $S(G)$ is flexible enough to allow an individual weighting
		  of the edges and vertices of $G$, without changes in the
		  maximal size of $S(G)$, or in the method of construction.
		  Apart from offering an alternative to Voronoi-type
		  skeletons, these generalizations of $S(G)$ have
		  applications to the reconstruction of a geographical
		  terrain from a given river map, and to the construction of
		  a polygonal roof above a given layout of ground walls.}
}

@Article{aa-vdcgf-02,
  author	= {O. Aichholzer and F. Aurenhammer},
  title		= {Voronoi Diagrams - Computational Geometry's Favorite},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  pages		= {7--11},
  volume	= 1,
  year		= {2002},
  address	= {Graz, Austria}
}

@InProceedings{aaacjr-at-10,
  author	= {O. Aichholzer and W. Aigner and F. Aurenhammer and K.
		  \v{C}ech Dobi\.a\v{s}ov\.a and B. J\"uttler and G. Rote},
  title		= {Arc triangulations},
  booktitle	= {Proc. $26^{th}$ European Workshop on Computational
		  Geometry EuroCG '2010},
  pages		= {17--20},
  year		= 2010,
  address	= {Dortmund, Germany},
  abstract	= {An important objective in the choice of a triangulation of
		  a given point set is that the smallest angle becomes as
		  large as possible. In the straight line case, it is known
		  that the Delaunay triangulation is optimal in this respect.
		  We propose and study the concept of a circular arc
		  triangulation, a simple and effective alternative that
		  offers flexibility for additionally enlarging small angles.
		  We show that angle optimization and related questions lead
		  to linear programming problems that can be formulated as
		  simple graph-theoretic problems, and we define flipping
		  operations in arc triangles. Moreover, special classes of
		  arc triangulations are considered, for applications in
		  finite element methods and graph drawing.}
}

@Article{aaag-ntsp-95,
  author	= {O. Aichholzer and D. Alberts and F. Aurenhammer and B.
		  Gaertner},
  title		= {A novel type of skeleton for polygons},
  journal	= {Journal of Universal Computer Science},
  year		= 1995,
  volume	= 1,
  number	= 12,
  pages		= {752--761},
  htmlnote	= {Click here for the <A
		  HREF="http://www.iicm.edu/jucs_1_12/a_novel_type_of">Online
		  Version</A>},
  note		= {[IIG-Report-Series 424, TU Graz, Austria, 1995]},
  abstract	= {A new internal structure for simple polygons, the straight
		  skeleton, is introduced and discussed. It is composed of
		  pieces of angular bisectores which partition the interior
		  of a given $n$-gon $P$ in a tree-like fashion into $n$
		  monotone polygons. Its straight-line structure and its
		  lower combinatorial complexity may make the straight
		  skeleton preferable to the widely used medial axis of a
		  polygon. As a seemingly unrelated application, the straight
		  skeleton provides a canonical way of constructing a
		  polygonal roof above a general layout of ground walls.}
}

@InProceedings{aaag-sssp-95,
  author	= {O. Aichholzer and D. Alberts and F. Aurenhammer and B.
		  Gaertner},
  title		= {Straight skeletons of simple polygons},
  booktitle	= {Proc. $4^{th}$ Int. Symp. of LIESMARS},
  pages		= {114--124},
  year		= 1995,
  address	= {Wuhan, P. R. China},
  abstract	= {A new internal structure for simple polygons, the straight
		  skeleton, is introduced and discussed. It is a tree and
		  partitions the interior of a given $n$-gon $P$ into $n$
		  monotone polygons, one for each edge of $P$. Its
		  straight-line structure and its lower combinatorial
		  complexity may make the straight skeleton $S(P)$ preferable
		  to the widely used medial axis of $P$. We show that $S(P)$
		  has no Voronoi diagram structure and give an $O(n r \log
		  n)$ time and $O(n)$ space construction algorithm, where $r$
		  counts the reflex vertices of $P$. As a seemingly unrelated
		  application, the straight skeleton provides a canonical way
		  of constructing a roof of given slope above a polygonal
		  layout of ground walls.}
}

@InProceedings{aaahjpr-dcvdr-09,
  author	= {O. Aichholzer and W. Aigner and F. Aurenhammer and T.
		  Hackl and B. J\"uttler and E. Pilgerstorfer and M. Rabl},
  title		= {Divide-and conquer for {V}oronoi diagrams revisited},
  booktitle	= {Proc. $25^{th}$ Ann. ACM Symp. Computational Geometry},
  address	= {Aarhus, Denmark},
  pages		= {189--197},
  year		= 2009,
  abstract	= {We show how to divide the edge graph of a Voronoi diagram
		  into a tree that corresponds to the medial axis of an
		  (augmented) planar domain. Division into base cases is then
		  possible, which, in the bottom-up phase, can be merged by
		  trivial concatenation. The resulting construction
		  algorithm---similar to Delaunay triangulation methods---is
		  not bisector-based and merely computes dual links between
		  the sites, its atomic steps being inclusion tests for sites
		  in circles. This guarantees computational simplicity and
		  numerical stability. Moreover, no part of the Voronoi
		  diagram, once constructed, has to be discarded again. The
		  algorithm works for polygonal and curved objects as sites
		  and, in particular, for circular arcs which allows its
		  extension to general free-form objects by Voronoi diagram
		  preserving and data saving biarc approximations. The
		  algorithm is randomized, with expected runtime $O(n\logn)$
		  under certain assumptions on the input data. Experiments
		  substantiate an efficient behavior even when these
		  assumptions are not met. Applications to offset
		  computations and motion planning for general objects are
		  described.}
}

@Article{aaahjpr-dcvdr-10,
  author	= {O. Aichholzer and W. Aigner and F. Aurenhammer and T.
		  Hackl and B. J\"uttler and E. Pilgerstorfer and M. Rabl},
  title		= {Divide-and conquer for {V}oronoi diagrams revisited},
  journal	= {Computational Geometry: Theory and Applications},
  volume	= {43},
  pages		= {688-699},
  year		= 2010,
  abstract	= {We show how to divide the edge graph of a Voronoi diagram
		  into a tree that corresponds to the medial axis of an
		  (augmented) planar domain. Division into base cases is then
		  possible, which, in the bottom-up phase, can be merged by
		  trivial concatenation. The resulting construction
		  algorithm---similar to Delaunay triangulation methods---is
		  not bisector-based and merely computes dual links between
		  the sites, its atomic steps being inclusion tests for sites
		  in circles. This guarantees computational simplicity and
		  numerical stability. Moreover, no part of the Voronoi
		  diagram, once constructed, has to be discarded again. The
		  algorithm works for polygonal and curved objects as sites
		  and, in particular, for circular arcs which allows its
		  extension to general free-form objects by Voronoi diagram
		  preserving and data saving biarc approximations. The
		  algorithm is randomized, with expected runtime $O(n\logn)$
		  under certain assumptions on the input data. Experiments
		  substantiate an efficient behavior even when these
		  assumptions are not met. Applications to offset
		  computations and motion planning for general objects are
		  described.}
}

@InProceedings{aaahjpr1-dcvdr-09,
  author	= {O. Aichholzer and W. Aigner and F. Aurenhammer and T.
		  Hackl and B. J\"uttler and E. Pilgerstorfer and M. Rabl},
  title		= {Divide-and conquer for {V}oronoi diagrams revisited},
  booktitle	= {Proc. $25^{th}$ European Workshop on Computational
		  Geometry EuroCG'09},
  address	= {Brussels, Belgium},
  pages		= {293--296},
  year		= 2009,
  abstract	= {We propose a simple and practical divide-and-conquer
		  algorithm for constructing planar Voronoi diagrams. The
		  novel aspect of the algorithm is its emphasis on the
		  top-down phase, which makes it applicable to sites of
		  general shape.}
}

@Article{aaahjr-macpffs-08,
  author	= {O. Aichholzer and W. Aigner and F. Aurenhammer and T.
		  Hackl and B. Juettler and M. Rabl},
  title		= {Medial axis computation for planar free-form shapes},
  journal	= {Computer Aided Design},
  year		= 2009,
  volume	= 41,
  pages		= {339--349},
  note		= {Special Issue},
  abstract	= {We present a simple, efficient, and stable method for
		  computing---with any desired precision---the medial axis of
		  simply connected planar domains. The domain boundaries are
		  assumed to be given as polynomial spline curves. Our
		  approach combines known results from the field of geometric
		  approximation theory with a new algorithm from the field of
		  computational geometry. Challenging steps are (1) the
		  approximation of the boundary spline such that the medial
		  axis is geometrically stable, and (2) the efficient
		  decomposition of the domain into base cases where the
		  medial axis can be computed directly and exactly. We solve
		  these problems via spiral biarc approximation and a
		  randomized divide \& conquer algorithm.}
}

@TechReport{aaaj-emactsrplm-11,
  author	= {O. Aichholzer and W. Aigner and F. Aurenhammer and B.
		  J\"uttler},
  title		= {Exact medial axis computation for triangulated solids with
		  respect to piecewise linear metrics},
  institution	= {TU Graz},
  year		= 2011,
  abstract	= {We propose a novel approach for the medial axis
		  approximation of triangulated solids by using a polyhedral
		  unit ball $B$ instead of the standard Euclidean unit ball.
		  By this means, we compute the exact medial axis
		  $MA(\Omega)$ of a triangulated solid $\Omega$ with respect
		  to a piecewise linear (quasi-)metric $d_B$. The obtained
		  representation of $\Omega$ by the medial axis transform
		  $MAT(\Omega)$ allows for a convenient computation of the
		  trimmed offset of $\Omega$ with respect to $d_B$. All
		  calculations are performed within the field of rational
		  numbers, resulting in a robust and efficient implementation
		  of our approach. Adapting the properties of $B$ provides an
		  easy way to control the level of details captured by the
		  medial axis, making use of the implicit pruning at flat
		  boundary features.}
}

@InProceedings{aabekm-nesca-00,
  author	= {O. Aichholzer and F. Aurenhammer and B. Brandtstaetter and
		  T. Ebner and H. Krasser and C. Magele},
  title		= {Niching evolution strategy with cluster algorithms},
  booktitle	= {$9^{th}$ Biennial IEEE Conf. Electromagnetic Field
		  Computations},
  year		= 2000,
  address	= {Milwaukee, Wisconsin, USA},
  abstract	= {In most real world optimization problems one tries to
		  determine the global among some or even numerous local
		  solutions within the feasible region of parameters. On the
		  other hand, it could be worth to investigate some of the
		  local solutions as well. Therefore, a most desirable
		  behaviour would be, if the optimization strategy behaves
		  globally and yields additional information about local
		  minima detected on the way to the global solution. In this
		  paper a clustering algorithm has been implemented into an
		  Higher Order Evolution Strategy in order to achieve these
		  goals.}
}

@Article{aabk-ptsnt-03,
  author	= {O. Aichholzer and F. Aurenhammer and P. Brass and H.
		  Krasser},
  title		= {Pseudo-Triangulations from Surfaces and a Novel Type of
		  Edge Flip},
  journal	= {SIAM Journal on Computing},
  volume	= {32},
  year		= {2003},
  pages		= {1621--1653},
  abstract	= {We prove that planar pseudo-triangulations have
		  realizations as polyhedral surfaces in three-space. Two
		  main implications are presented: The spatial embedding
		  leads to a novel flip operation that allows for a drastical
		  reduction of flip distances, especially between (full)
		  triangulations. Moreover, several key results for
		  triangulations, like flipping to optimality, (constrained)
		  Delaunayhood, and a convex polytope representation, are
		  extended to pseudo-triangulations in a natural way.},
  address	= {Graz, Austria}
}

@InProceedings{aabk-sept-03,
  author	= {O. Aichholzer and F. Aurenhammer and P. Brass and H.
		  Krasser},
  title		= {Spatial Embedding of Pseudo-Triangulations},
  booktitle	= {Proc. $19^{th}$ Ann. ACM Symp. Computational Geometry},
  address	= {San Diego, California, USA},
  pages		= {144--153},
  year		= 2003,
  abstract	= {We show that pseudo-triangulations have natural embeddings
		  in three-space. As a consequence, various concepts for
		  triangulations, like flipping to optimality, (constrained)
		  Delaunayhood, and a polytope representation carry over to
		  pseudo-triangulations.}
}

@InProceedings{aabkmmr-eshc-01,
  author	= {O. Aichholzer and F. Aurenhammer and B. Brandtstaetter and
		  H. Krasser and C. Magele and M. Muehlmann and W. Renhart},
  title		= {Evolution trategy and ierarchical lustering},
  booktitle	= {$13^{th}$ COMPUMAG Conference on the Computation of
		  Electromagnetic Fields},
  year		= 2001,
  address	= {Lyon-Evian, France},
  abstract	= {Multi-objective optimization problems, in general, exhibit
		  several local optima besides a global one. A desirable
		  feature of any optimization strategy would therefore be to
		  supply the user with as many information as possible about
		  local optima on the way to the global solution. In this
		  paper a hierarchical clustering algorithm implemented into
		  a higher order Evolution Strategy is applied to achieve
		  these goals.}
}

@Article{aackrtx-tin-96,
  author	= {O. Aichholzer and F. Aurenhammer and S.-W. Cheng and N.
		  Katoh and G. Rote and M. Taschwer and Y.-F. Xu},
  title		= {Triangulations intersect nicely},
  journal	= {Discrete \& Computational Geometry},
  year		= 1996,
  volume	= 16,
  pages		= {339--359},
  note		= {Special Issue. [SFB Report F003-030, TU Graz, Austria,
		  1995]},
  abstract	= {We show that there is a matching between the edges of any
		  two triangulations of a planar point set such that an edge
		  of one triangulation is matched either to the identical
		  edge in the other triangulation or to an edge that crosses
		  it. This theorem also holds for the triangles of the
		  triangulations and in general independence systems. As an
		  application, we give some lower bounds for the minimum
		  weight triangulation which can be computed in polynomial
		  time by matching and network flow techniques. We exhibit an
		  easy-to-recognize class of point sets for which the
		  minimum-weight triangulation coincides with the greedy
		  triangulation.}
}

@InProceedings{aaclmp-vddsd-97,
  author	= {O. Aichholzer and F. Aurenhammer and D.Z. Chen and D.T.
		  Lee and A. Mukhopadhyay and E. Papadopoulou},
  title		= {{V}oronoi diagrams for direction-sensitive distances
		  (communication)},
  booktitle	= {Proc. $13^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {418--420},
  year		= 1997,
  address	= {Nice, France},
  note		= {[SFB Report F003-098, TU Graz, Austria, 1996]},
  abstract	= {On a tilted plane $T$ in three-space, direction-sensitive
		  distances are defined as the Euclidean distance plus a
		  multiple of the signed difference in height. These
		  direction-sensitive distances, called skew distances
		  generalize the Euclidean distance and may model realistic
		  environments more closely than the Euclidean distance.
		  Various Voronoi diagrams and related problems under this
		  kind of distances are investigated. A relationship to
		  convex distance functions and to Euclidean Voronoi diagrams
		  for planar circles is shown, and is exploited for a
		  geometric analysis and a plane-sweep construction of
		  Voronoi diagrams on $T$. Several optimal algorithms based
		  on the direction-sensitive distances on $T$ are presented.
		  For example, an output-sensitive algorithm is developed for
		  computing the skew distance Voronoi diagram with $n$ sites
		  on $T$, in $O(n \log h)$ time and $O(n)$ space, where $h$
		  is the number of sites which have non-empty Voronoi regions
		  ($1 \leq h \leq n$). $O(n \log n)$ time and $O(n)$ space
		  algorithms are also given for several other problems under
		  skew distances, including the all nearest neighbors and the
		  layers of Voronoi diagram. These algorithms have certain
		  features different from their 'ordinary' counterparts based
		  on the Euclidean distance.}
}

@Article{aaclp-svd-99,
  author	= {O. Aichholzer and F. Aurenhammer and D.Z. Chen and D.T.
		  Lee and E. Papadopoulou},
  title		= {Skew {V}oronoi diagrams},
  journal	= {Int'l. Journal of Computational Geometry \& Applications},
  year		= 1999,
  volume	= 9,
  pages		= {235--247},
  htmlnote	= {Click here for <A
		  HREF="http://www.igi.TUgraz.at/oaich/skewvd/Welcome.html">Figures
		  and Animations</A> of Skew Voronoi Diagrams},
  abstract	= {On a tilted plane $T$ in three-space, {\em skew
		  distances\/} are defined as the Euclidean distance plus a
		  multiple of the signed difference in height. Skew distances
		  may model realistic environ\-ments more closely than the
		  Euclidean distance. Voro\-noi diagrams and related problems
		  under this kind of distances are investigated. A
		  relationship to convex distance functions and to Euclidean
		  Voronoi diagrams for planar circles is shown, and is
		  exploited for a geometric analysis and a plane-sweep
		  construction of Voronoi diagrams on $T$. An
		  output-sensitive algorithm running in time $O(n \log h)$ is
		  developed, where $n$ and $h$ is the number of sites and
		  non-empty Voronoi regions, respectively. The all nearest
		  neighbors problem for skew distances, which has certain
		  features different from its Euclidean counterpart, is
		  solved in $O(n \log n)$ time.}
}

@Article{aadhru-okcp-11,
  author	= {O. Aichholzer and F. Aurenhammer and E.D. Demaine and F.
		  Hurtado and P. Ramos and J. Urrutia},
  title		= {On $k$-convex polygons},
  journal	= {Computational Geometry: Theory and Applications},
  year		= 2011,
  note		= {submitted},
  abstract	= {We introduce the notion of $k$-convexity and explore
		  polygons in the plane that have this property. Polygons
		  which are $k$-convex can be triangulated with fast yet
		  simple algorithms. However, recognizing them is a 3SUM-hard
		  problem. We give a characterization of 2-convex polygons, a
		  particularly interesting class, and show how to recognize
		  them in $O(n \log n)$ time. A description of their shape is
		  given as well, which leads to Erd\"os-Szekeres type results
		  regarding subconfigurations of their vertex sets. Finally,
		  we introduce the concept of generalized geometric
		  permutations, and show that their number can be exponential
		  in the number of 2-convex objects considered.}
}

@InProceedings{aadhtv-09,
  author	= {O. Aichholzer and F. Aurenhammer and O. Devillers and T.
		  Hackl and M. Teillaud and B. Vogtenhuber},
  title		= {Lower and upper bounds on the number of empty cylinders
		  and ellipsoids},
  booktitle	= {Proc. $25^{th}$ European Workshop on Computational
		  Geometry EuroCG'09},
  address	= {Brussels, Belgium},
  pages		= {139--141},
  year		= 2009
}

@InProceedings{aaghhhkrv-mefpp-05,
  author	= {O. Aichholzer and F. Aurenhammer and P. Gonzalez-Nava and
		  T. Hackl and C. Huemer and F. Hurtado and H. Krasser and S.
		  Ray and B. Vogtenhuber},
  title		= {Matching edges and faces in polygonal partitions},
  booktitle	= {Proc. $17^{th}$ Canadian Conference on Computational
		  Geometry CCCG '05},
  pages		= {123--126},
  year		= 2005,
  address	= {Windsor, Ontario},
  abstract	= {We define general Laman (count) conditions for edges and
		  faces of polygonal partitions in the plane. Several
		  well-known classes, including $k$-regular partitions,
		  $k$-angulations, and rank-$k$ pseudo-triangulations, are
		  shown to fulfill such conditions. As a consequence,
		  non-trivial perfect matchings exist between the edge sets
		  (or face sets) of two such structures when they live on the
		  same point set. We also describe a link to spanning tree
		  decompositions that applies to quadrangulations and certain
		  pseudo-triangulations.}
}

@Article{aaghhhkrv-mefpp-07,
  author	= {O. Aichholzer and F. Aurenhammer and P. Gonzalez-Nava and
		  T. Hackl and C. Huemer and F. Hurtado and H. Krasser and S.
		  Ray and B. Vogtenhuber},
  title		= {Matching edges and faces in polygonal partitions},
  journal	= {Computational Geometry: Theory and Applications},
  year		= {2008},
  volume	= 39,
  pages		= {134--141},
  abstract	= {We define general Laman (count) conditions for edges and
		  faces of polygonal partitions in the plane. Several
		  well-known classes, including $k$-regular partitions,
		  $k$-angulations, and rank-$k$ pseudo-triangulations, are
		  shown to fulfill such conditions. As a consequence,
		  non-trivial perfect matchings exist between the edge sets
		  (or face sets) of two such structures when they live on the
		  same point set. We also describe a link to spanning tree
		  decompositions that applies to quadrangulations and certain
		  pseudo-triangulations.}
}

@InProceedings{aah-eoncs-00,
  author	= {O. Aichholzer and F. Aurenhammer and F. Hurtado},
  title		= {Edge Operations on Non-Crossing Spanning Trees},
  booktitle	= {Proc. $16^{th}$ European Workshop on Computational
		  Geometry CG '2000},
  pages		= {121--125},
  year		= 2000,
  address	= {Eilat, Israel},
  htmlnote	= {You can download our <A
		  HREF="http://www.igi.TUgraz.at/oaich/trees/Welcome.html">MST-Tool</A>.}
		  ,
  abstract	= {Let $S$ be a set of $n$ points in the Euclidean plane.
		  Consider the set ${\cal T}_S$ of all non-crossing spanning
		  trees of $S$. A {\em tree graph\/} ${\cal TG}_{\tt op}(S)$
		  is the graph that has ${\cal T}_S$ as its vertex set and
		  that connects vertex (tree) $T$ to vertex $T'$ iff $T' =
		  {\tt op}(T)$, where ${\tt op}$ is some operation that
		  exchanges two tree edges following a specific rule. The
		  existence of a path between two vertices in ${\cal TG}_{\tt
		  op}(S)$ means transformability of the corresponding trees
		  into each other by repeated application of the operation
		  ${\tt op}$. The length of a shortest path corresponds to
		  the distance between the two trees with respect to the
		  operation ${\tt op}$. Distances of this kind provide a
		  measure of similarity between trees. We prove new results
		  on ${\cal TG}_{\tt op}(S)$ for two classical operations
		  ${\tt op}$, namely the (improving and crossing-free) {\em
		  edge move\/} and the (crossing-free) {\em edge slide\/}.
		  Applications to morphing of trees and to the continuous
		  deformation of sets of line segments seem reasonable. Our
		  results mainly rely on a fact of interest in its own right:
		  Let $MST(S)$ and $DT(S)$ be the minimum spanning tree and
		  the Delaunay triangulation of $S$, respectively. Then any
		  pair $(T,\Delta)$, for $T \in {\cal T}_S$ and $\Delta$
		  being $T's$ constrained Delaunay triangulation, can be
		  transformed into the pair $(MST(S),DT(S))$ via a canonical
		  tree/triangulation sequence.}
}

@InProceedings{aah-fps-01a,
  author	= {O. Aichholzer and L.S. Alboul and F. Hurtado},
  title		= {On Flips in Polyhedral Surfaces},
  booktitle	= {Proc. $17^{th}$ European Workshop on Computational
		  Geometry CG '2001},
  pages		= {27--30},
  year		= 2001,
  address	= {Berlin, Germany},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/polyhedra.html">interactive
		  web-page</A>.},
  abstract	= {Let $V$ be a finite point set in 3D-space, and let ${\cal
		  S}(V)$ be the set of triangulated polyhedral surfaces
		  homeomorphic to a sphere and with vertex set $V$. Let $abc$
		  and $cbd$ be two adjacent triangles belonging to a surface
		  $S\in {\cal S}(V)$; the {\sl flip} of the edge $bc$ would
		  replace these two triangles by the triangles $abd$ and
		  $adc$. The flip operation is only considered when it does
		  not produce a self--intersecting surface. In this paper we
		  show that given two surfaces $S_1, S_2\in {\cal S}(V)$, it
		  is possible that there is no sequence of flips transforming
		  $S_1$ into $S_2$, even in the case that $V$ consists of
		  points in convex position.}
}

@Article{aah-fps-01b,
  author	= {O. Aichholzer and L.S. Alboul and F. Hurtado},
  title		= {On Flips in Polyhedral Surfaces},
  journal	= {International Journal of Foundations of Computer Science
		  (IJFCS), special issue on Volume and Surface
		  Triangulations},
  year		= 2002,
  volume	= 13,
  number	= 2,
  pages		= {303--311},
  abstract	= {Let $V$ be a finite point set in 3-space, and let ${\cal
		  S}(V)$ be the set of triangulated polyhedral surfaces
		  homeomorphic to a sphere and with vertex set $V$. Let $abc$
		  and $cbd$ be two adjacent triangles belonging to a surface
		  $S\in {\cal S}(V)$; the {\sl flip} of the edge $bc$ would
		  replace these two triangles by the triangles $abd$ and
		  $adc$. The flip operation is only considered when it does
		  not produce a self--intersecting surface. In this paper we
		  show that given two surfaces $S_1, S_2\in {\cal S}(V)$, it
		  is possible that there is no sequence of flips transforming
		  $S_1$ into $S_2$, even in the case that $V$ consists of
		  points in convex position.}
}

@Article{aah-nrms-99,
  author	= {O. Aichholzer and F. Aurenhammer and R. Hainz},
  title		= {New results on {MWT} subgraphs},
  journal	= {Information Processing Letters},
  year		= 1999,
  volume	= 69,
  pages		= {215--219},
  note		= {[SFB Report F003-140, TU Graz, Austria, 1998]},
  abstract	= {Let $P$ be a polygon in the plane. We disprove the
		  conjecture that the so-called LMT-skeleton coincides with
		  the intersection of all locally minimal triangulations,
		  $LMT(P)$, even for convex polygons $P$. We introduce an
		  improved LMT-skeleton algorithm which, for any simple
		  polygon $P$, exactly computes $LMT(P)$, and thus a larger
		  subgraph of the minimum-weight triangulation $MWT(P)$. The
		  algorithm achieves the same in the general point set case
		  provided the connectedness of the improved LMT-skeleton,
		  which is given in allmost all practical instances. We
		  further observe that the $\beta$-skeleton of $P$ is a
		  subset of $MWT(P)$ for all values $\beta >
		  \sqrt{\frac{4}{3}}$ provided $P$ is convex or near-convex.
		  This gives evidence for the tightness of this bound in the
		  general point set case.}
}

@InProceedings{aah-ptlc-06,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl},
  title		= {Pre-triangulations and liftable complexes},
  booktitle	= {$22^{nd}$ Ann. ACM Symp. Computational Geometry},
  year		= 2006,
  pages		= {282--291},
  address	= {Sedona, Arizona, USA},
  abstract	= {We introduce and discuss the concept of
		  pre-triangulations, a relaxation of triangulations that
		  goes beyond the well-established class of
		  pseudo-triangulations.}
}

@Article{aah-ptlc-07,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl},
  title		= {Pre-triangulations and liftable complexes},
  journal	= {Discrete \& Computational Geometry},
  volume	= 38,
  year		= 2007,
  pages		= {701--725},
  abstract	= {We introduce the concept of pre-triangulations, a
		  relaxation of triangulations that goes beyond the
		  frequently used concept of pseudo-triangulations.
		  Pre-triangulations turn out to be more natural than
		  pseudo-triangulations in certain cases. We show that
		  pre-triangulations arise in three different contexts: In
		  the characterization of polygonal complexes that are
		  liftable to three-space in a strong sense, in flip
		  sequences for general polygonal complexes, and as graphs of
		  maximal locally convex functions.}
}

@Article{aah-sstft-00,
  author	= {O. Aichholzer and F. Aurenhammer and F. Hurtado},
  title		= {Sequences of spanning trees and a fixed tree theorem},
  year		= 2002,
  journal	= {Computational Geometry: Theory and Applications},
  volume	= {21},
  number	= {1--2},
  pages		= {3--20},
  note		= {Special Issue. [Report MA2-IR-00-00026, Universitat
		  Polite\'cnica de Catalunya, Barcelona, Spain, 2000]},
  htmlnote	= {You can also download the nice program <A
		  HREF="http://www.igi.TUGraz.at/oaich/trees/Welcome.html">MST-Tool</A>
		  we used to check and visualize some of the presented
		  results!},
  abstract	= {Let ${\cal T}_S$ be the set of all crossing-free spanning
		  trees of a planar $n$-point set $S$. We prove that ${\cal
		  T}_S$ contains, for each of its members $T$, a
		  length-decreasing sequence of trees $T_o,\ldots,T_k$ such
		  that $T_o=T$, $T_k=MST(S)$, $T_i$ does not cross $T_{i-1}$
		  for $i=1,\ldots,k$, and $k=O(\log n)$. Here $MST(S)$
		  denotes the Euclidean minimum spanning tree of the point
		  set $S$. As an implication, the number of length-improving
		  and planar edge moves needed to transform a tree $T \in
		  {\cal T}_S$ into $MST(S)$ is only $O(n\log n)$. Moreover,
		  it is possible to transform any two trees in ${\cal T}_S$
		  into each other by means of a local and constant-size edge
		  slide operation. Applications of these results to morphing
		  of simple polygons are possible by using a crossing-free
		  spanning tree as a skeleton description of a polygon.}
}

@Article{aahh-ccps-06,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and C.
		  Huemer},
  title		= {Connecting colored point sets},
  journal	= {Discrete Applied Mathematics},
  volume	= 155,
  year		= 2007,
  pages		= {271--278},
  abstract	= {We study the following Ramsey-type problem. Let \mbox{$S =
		  B \cup R$} be a two-colored set of $n$ points in the plane.
		  We show how to construct, in \mbox{$O(n \log n)$} time, a
		  crossing-free spanning tree $T(R)$ for~$R$, and a
		  crossing-free spanning tree $T(B)$ for~$B$, such that both
		  the number of crossings between $T(R)$ and $T(B)$ and the
		  diameters of~$T(R)$ and $T(B)$ are kept small. The
		  algorithm is conceptually simple and is implementable
		  without using any non-trivial data structure. This improves
		  over a previous method in Tokunaga~\cite{T} that is less
		  efficient in implementation and does not guarantee a
		  diameter bound.}
}

@InProceedings{aahhpv-3cpt,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and C.
		  Huemer and A. Pilz and B. Vogtenhuber},
  title		= {3-colorability of pseudo-triangulations},
  booktitle	= {Proc. $26^{th}$ European Workshop on Computational
		  Geometry EuroCG'10},
  address	= {Dortmund, Germany},
  pages		= {21--24},
  year		= 2010,
  abstract	= {Deciding $3$-colorability for general plane graphs is
		  known to be an NP-complete problem. However, for certain
		  classes of plane graphs, like triangulations, polynomial
		  time algorithms exist. We consider the family of
		  pseudo-triangulations (a generalization of triangulations)
		  and prove NP-completeness for this class, even if the
		  maximum face-degree is bounded to four, or pointed
		  pseudo-triangulations with maximum face degree five are
		  considered. As a complementary result, we show that for
		  pointed pseudo-triangulations with maximum face-degree
		  four, a $3$-coloring always exists and can be found in
		  linear time.}
}

@InProceedings{aahjos-csacbr-07,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and B.
		  Juettler and M. Oberneder and Z. Sir},
  title		= {Computational and Structural Advantages of Circular
		  Boundary Representation},
  booktitle	= {Proc. 10th Int. Workshop on Algorithms and Data
		  Structures, WADS'07, Springer LNCS 4619},
  year		= 2007,
  address	= {Halifax, Canada},
  pages		= {374-385},
  abstract	= {Boundary approximation of planar shapes by circular arcs
		  has quantitive and qualitative advantages compared to using
		  straight-line segments. We demonstrate this by way of three
		  basic and frequent computations on shapes -- convex hull,
		  decomposition, and medial axis. In particular, we propose a
		  novel medial axis algorithm that beats existing methods in
		  simplicity and practicality, and at the same time
		  guarantees convergence to the medial axis of the original
		  shape. }
}

@Article{aahjos-csacbr-10,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and B.
		  Juettler and M. Oberneder and Z. Sir},
  title		= {Computational and Structural Advantages of Circular
		  Boundary Representation},
  journal	= {Int'l Journal of Computational Geometry \& Applications},
  volume	= 21,
  year		= 2011,
  pages		= {47--69},
  abstract	= {Boundary approximation of planar shapes by circular arcs
		  has quantitive and qualitative advantages compared to using
		  straight-line segments. We demonstrate this by way of three
		  basic and frequent computations on shapes -- convex hull,
		  decomposition, and medial axis. In particular, we propose a
		  novel medial axis algorithm that beats existing methods in
		  simplicity and practicality, and at the same time
		  guarantees convergence to the medial axis of the original
		  shape.}
}

@InProceedings{aahk-tct-01,
  author	= {O. Aichholzer and F. Aurenhammer and F. Hurtado and H.
		  Krasser},
  title		= {Towards Compatible Triangulations},
  booktitle	= {Proc. $7^{th}$ Ann. Int'l. Computing and Combinatorics
		  Conf. COCOON'01, Lecture Notes in Computer Science},
  pages		= {101--110},
  year		= 2001,
  volume	= {2108},
  address	= {Guilin, China},
  editor	= {Jie Wang},
  publisher	= {Springer Verlag},
  abstract	= {We state the following conjecture: any two planar
		  $n$-point sets (that agree on the number of convex hull
		  points) can be triangulated in a compatible manner, i.e.,
		  such that the resulting two planar graphs are isomorphic.
		  The conjecture is proved true for point sets with at most
		  three interior points. We further exhibit a class of point
		  sets which can be triangulated compatibly with any other
		  set (that satisfies the obvious size and hull
		  restrictions). Finally, we prove that adding a small number
		  of Steiner points (the number of interior points minus two)
		  always allows for compatible triangulations.}
}

@Article{aahk-tct-01b,
  author	= {O. Aichholzer and F. Aurenhammer and F. Hurtado and H.
		  Krasser},
  title		= {Towards Compatible Triangulations},
  year		= 2003,
  journal	= {Theoretical Computer Science},
  note		= {Special Issue},
  volume	= {296},
  pages		= {3--13},
  publisher	= {Elsevier},
  abstract	= {We state the following conjecture: any two planar
		  $n$-point sets (that agree on the number of convex hull
		  points) can be triangulated in a compatible manner, i.e.,
		  such that the resulting two triangulations are
		  topologically equivalent. The conjecture is proved true for
		  point sets with at most three interior points. We further
		  exhibit a class of point sets which can be triangulated
		  compatibly with any other set that satisfies the obvious
		  size and hull restrictions. Finally, we prove that adding a
		  small number of extraneous points (the number of interior
		  points minus two) always allows for compatible
		  triangulations.}
}

@InProceedings{aahk-tspt-05,
  author	= {O. Aichholzer and F. Aurenhammer and C. Huemer and H.
		  Krasser},
  title		= {Transforming spanning trees and pseudo-triangulations},
  booktitle	= {Proc. $21^{st}$ European Workshop on Computational
		  Geometry EuroCG '05},
  pages		= {81--84},
  year		= 2005,
  address	= {Eindhoven, The Netherlands},
  abstract	= {Let $T_{S}$ be the set of all crossing-free straight line
		  spanning trees of a planar $n$-point set~$S$. Consider the
		  graph ${\cal T}_S$ where two members $T$ and $T'$ of
		  $T_{S}$ are adjacent if $T$ intersects $T'$ only in points
		  of~$S$ or in common edges. We prove that the diameter
		  of~${\cal T}_S$ is $O(\log k)$, where $k$ denotes the
		  number of convex layers of $S$. Based on this result, we
		  show that the flip graph~${\cal P}_S$ of
		  pseudo-triangulations of~$S$ (where two
		  pseudo-triangulations are adjacent if they differ in
		  exactly one edge -- either by replacement or by removal)
		  has a dia\-meter of $O(n \log k)$. This sharpens a known
		  $O(n \log n)$ bound. Let~${\cal \widehat{P}}_S$ be the
		  induced subgraph of pointed pseudo-triangulations of~${\cal
		  P}_S$. We present an example showing that the distance
		  between two nodes in~${\cal \widehat{P}}_S$ is strictly
		  larger than the distance between the corresponding nodes
		  in~${\cal P}_S$.}
}

@Article{aahk-tspt-06,
  author	= {O. Aichholzer and F. Aurenhammer and C. Huemer and H.
		  Krasser},
  title		= {Transforming spanning trees and pseudo-triangulations},
  journal	= {Information Processing Letters},
  pages		= {19--22},
  volume	= {97},
  year		= 2006,
  abstract	= {Let $T_{S}$ be the set of all crossing-free straight line
		  spanning trees of a planar $n$-point set~$S$. Consider the
		  graph ${\cal T}_S$ where two members $T$ and $T'$ of
		  $T_{S}$ are adjacent if $T$ intersects $T'$ only in points
		  of~$S$ or in common edges. We prove that the diameter
		  of~${\cal T}_S$ is $O(\log k)$, where $k$ denotes the
		  number of convex layers of $S$. Based on this result, we
		  show that the flip graph~${\cal P}_S$ of
		  pseudo-triangulations of~$S$ (where two
		  pseudo-triangulations are adjacent if they differ in
		  exactly one edge -- either by replacement or by removal)
		  has a dia\-meter of $O(n \log k)$. This sharpens a known
		  $O(n \log n)$ bound. Let~${\cal \widehat{P}}_S$ be the
		  induced subgraph of pointed pseudo-triangulations of~${\cal
		  P}_S$. We present an example showing that the distance
		  between two nodes in~${\cal \widehat{P}}_S$ is strictly
		  larger than the distance between the corresponding nodes
		  in~${\cal P}_S$.}
}

@InProceedings{aahkpp-abtob-07,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and B.
		  Kornberger and M. Peternell and H. Pottmann},
  title		= {Approximating boundary-triangulated objects with balls},
  booktitle	= {Proc. 23rd European Workshop on Computational Geometry,
		  EWCG'07},
  year		= 2007,
  address	= {Graz, Austria},
  pages		= {130--133},
  abstract	= {We compute a set of balls that approximates a given 3D
		  object, and we derive small additive bounds for the
		  overhead in balls with respect to the minimal solution with
		  the same quality. The algorithm has been implemented and
		  tested using the CGAL library.}
}

@InProceedings{aahkprsv-spia-08,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and B.
		  Kornberger and S. Plantinga and G. Rote and A. Sturm and G.
		  Vegter},
  title		= {Seed polytopes for incremental approximation},
  booktitle	= {Proc. $24^{th}$ European Workshop on Computational
		  Geometry EuroCG '08},
  year		= 2008,
  pages		= {13--16},
  address	= {Nancy, France},
  abstract	= {Approximating a three-dimensional object in order to
		  simplify its handling is a classical topic in computational
		  geometry and related fields. A typical approach is based on
		  incremental approximation algorithms, which start with a
		  small and topologically correct polytope representation
		  (the seed polytope) of a given sample point cloud or input
		  mesh. In addition, a correspondence between the faces of
		  the polytope and the respective regions of the object
		  boundary is needed to guarantee correctness.
		  
		  We construct such a polytope by first computing a
		  simplified though still homotopy equivalent medial axis
		  transform of the input object. Then, we inflate this medial
		  axis to a polytope of small size. Since our approximation
		  maintains topology, the simplified medial axis transform is
		  also useful for skin surfaces and envelope surfaces.}
}

@InProceedings{aahlrss-swe-05,
  author	= {B. Aronov and F. Aurenhammer and F. Hurtado and S.
		  Langerman and D. Rappaport and S. Smorodinsky and C.
		  Seara},
  title		= {Small weak epsilon nets},
  booktitle	= {Proc. $17^{th}$ Canadian Conference on Computational
		  Geometry CCCG '05},
  pages		= {51--54},
  year		= 2005,
  address	= {Windsor, Ontario},
  abstract	= {Let S be a family of sets in the plane. Let 0 <
		  epsilon(S,i) < 1 denote the minimum real number such that
		  for any finite point set P there exists a set Q of i points
		  that is a weak epsilon(S,i)-net for P with respect to S. We
		  derice upper and lower bounds on epsilon(S,i) for small
		  integers i and when S is the family of all convex sets, or
		  the family of all axis-parallel rectangles.}
}

@Article{aahlrss-swe-07,
  author	= {B. Aronov and F. Aurenhammer and F. Hurtado and S.
		  Langerman and D. Rappaport and S. Smorodinsky and C.
		  Seara},
  title		= {Small weak epsilon nets},
  journal	= {Computational Geometry: Theory and Applications},
  year		= 2009,
  volume	= 42,
  pages		= {455--462},
  note		= {Special Issue},
  abstract	= {Let S be a family of sets in the plane. Let 0 <
		  epsilon(S,i) < 1 denote the minimum real number such that
		  for any finite point set P there exists a set Q of i points
		  that is a weak epsilon(S,i)-net for P with respect to S. We
		  derice upper and lower bounds on epsilon(S,i) for small
		  integers i and when S is the family of all convex sets, or
		  the family of all axis-parallel rectangles.}
}

@InProceedings{aahru-tcp-09,
  author	= {O. Aichholzer and F. Aurenhammer and F. Hurtado and P.A.
		  Ramos and J. Urrutia},
  title		= {Two-convex polygons},
  booktitle	= {Proc. $25^{th}$ European Workshop on Computational
		  Geometry EuroCG'09},
  address	= {Brussels, Belgium},
  pages		= {117--120},
  year		= 2009,
  abstract	= {We introduce a notion of k-convexity and explore some
		  properties of polygons that have this property. In
		  particular, 2-convex polygons can be recognized in
		  \mbox{$O(n \log n)$} time, and k-convex polygons can be
		  triangulated in $O(kn)$ time.}
}

@Article{aahs-mwpt-08,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and B.
		  Speckmann},
  title		= {On minimum weight pseudo-triangulations},
  journal	= {Computational Geometry: Theory and Applications},
  year		= 2009,
  volume	= 42,
  pages		= {627--631},
  abstract	= {In this note we discuss some structural properties of
		  minimum weight pseudo-triangulations.}
}

@InProceedings{aahs-pmwpt-07,
  author	= {O. Aichholzer and F. Aurenhammer and T. Hackl and B.
		  Speckmann},
  title		= {On (pointed) minimum weight pseudo-triangulations},
  booktitle	= {Proc. $19^{th}$ Canadian Conference on Computational
		  Geometry CCCG '07},
  year		= 2007,
  address	= {Ottawa, Canada},
  pages		= {209-212},
  abstract	= {In this note we discuss some structural properties of
		  minimum weight (pointed) pseudo-triangulations.}
}

@InProceedings{aahv-gcepslg-06,
  author	= {O. Aichholzer and F. Aurenhammer and C. Huemer and B.
		  Vogtenhuber},
  title		= {Gray code enumeration of plane straight-line graphs},
  booktitle	= {Proc. $22^{nd}$ European Workshop on Computational
		  Geometry EuroCG '06},
  year		= 2006,
  pages		= {71--74},
  address	= {Delphi, Greece},
  abstract	= {We develop Gray code enumeration schemes for geometric
		  graphs in the plane. The considered graph classes include
		  plane straight-line graphs, plane spanning trees, and
		  connected plane straight-line graphs. Previous results were
		  restricted to the case where the underlying vertex set is
		  in convex position.}
}

@Article{aahv-gcepslg-07,
  author	= {O. Aichholzer and F. Aurenhammer and C. Huemer and B.
		  Vogtenhuber},
  title		= {Gray code enumeration of plane straight-line graphs},
  journal	= {Graphs and Combinatorics},
  year		= 2007,
  volume	= 23,
  pages		= {467--479},
  abstract	= {We develop Gray code enumeration schemes for geometric
		  graphs in the plane. The considered graph classes include
		  plane straight-line graphs, plane spanning trees, and
		  connected plane straight-line graphs. Previous results were
		  restricted to the case where the underlying vertex set is
		  in convex position.}
}

@InProceedings{aaiklr-gsac-98a,
  author	= {O. Aichholzer and F. Aurenhammer and C. Icking and R.
		  Klein and E. Langetepe and G. Rote},
  title		= {Generalized self-approaching curves},
  booktitle	= {Proc. $14^{th}$ European Workshop on Computational
		  Geometry CG '98},
  pages		= {15--18},
  year		= 1998,
  address	= {Barcelona, Spain},
  abstract	= {We consider all planar oriented curves that have the
		  following property. For each point $B$ on the curve, the
		  rest of the curve lies inside a wedge of angle $\varphi$
		  with apex in $B$, where $\varphi < \Pi$ is fixed. This
		  property restrains the curve's meandering. we provide an
		  upper bound $c(\varphi)$ for the length of such a curve,
		  divided by the distance between its endpoints, and prove
		  this bound to be tight. A main step is in proving that the
		  curve's length cannot exceed the perimeter of its convex
		  hull, divided by $1+\cos(\varphi)$.}
}

@InProceedings{aaiklr-gsac-98b,
  author	= {O. Aichholzer and F. Aurenhammer and C. Icking and R.
		  Klein and E. Langetepe and G. Rote},
  title		= {Generalized self-approaching curves},
  booktitle	= {Proc. $9^{th}$ Int. Symp. Algorithms and Computation
		  ISAAC'98, Lecture Notes in Computer Science},
  pages		= {317--326},
  year		= 1998,
  volume	= 1533,
  address	= {Taejon, Korea},
  publisher	= {Springer Verlag},
  abstract	= {We consider all planar oriented curves that have the
		  following property depending on a fixed angle $\varphi$.
		  For each point $B$ on the curve, the rest of the curve lies
		  inside a wedge of angle $\varphi$ with apex in $B$. This
		  property restrains the curve's meandering, and for $\varphi
		  \leq \Pi/2$ this means that a point running along the curve
		  always gets closer to all points on the remaining part. For
		  all $\varphi < \Pi$, we provide an upper bound $c(\varphi)$
		  for the length of such a curve, divided by the distance
		  between its endpoints, and prove this bound to be tight. A
		  main step is in proving that the curve's length cannot
		  exceed the perimeter of its convex hull, divided by
		  $1+\cos(\varphi)$.}
}

@Article{aaiklr-vsac-00,
  author	= {O. Aichholzer and F. Aurenhammer and C. Icking and R.
		  Klein and E. Langetepe and G. Rote},
  title		= {Generalized self-approaching curves},
  journal	= {Discrete Applied Mathematics},
  year		= 2001,
  note		= {Special Issue. [SFB-Report F003-134, TU Graz, Austria,
		  1998]},
  pages		= {3--24},
  volume	= {109},
  number	= {1-2},
  abstract	= {We consider all planar oriented curves that have the
		  following property depending on a fixed angle $\varphi$.
		  For each point $B$ on the curve, the rest of the curve lies
		  inside a wedge of angle $\varphi$ with apex in $B$. This
		  property restrains the curve's meandering, and for $\varphi
		  \leq \Pi/2$ this means that a point running along the curve
		  always gets closer to all points on the remaining part. For
		  all $\varphi < \Pi$, we provide an upper bound $c(\varphi)$
		  for the length of such a curve, divided by the distance
		  between its endpoints, and prove this bound to be tight. A
		  main step is in proving that the curve's length cannot
		  exceed the perimeter of its convex hull, divided by
		  $1+\cos(\varphi)$.}
}

@InProceedings{aak-aptnl-03,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {Adapting (Pseudo)-Triangulations with a Near-Linear Number
		  of Edge Flips},
  booktitle	= {Lecture Notes in Computer Science 2748, Proc. 8th
		  International Workshop on Algorithms and Data Structures
		  (WADS)},
  volume	= {2748},
  pages		= {12--24},
  year		= 2003,
  abstract	= {We provide two results on flip distances in
		  pseudo-triangulations -- for minimum pseudo-triangulations
		  when using traditional flips operations, as well as for
		  triangulations when a novel and natural edge flip operation
		  is included into the repertoire of admissible flips. The
		  obtained flip distance lengths are $O(n \log^2 n)$ and $O(n
		  \log n)$, respectively. Our results partially rely on new
		  partitioning results for pseudo-triangulations which may be
		  of separate interest.}
}

@InProceedings{aak-cncg-02,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {On the Crossing Number of Complete Graphs},
  year		= 2002,
  booktitle	= {Proc. $18^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {19--24},
  address	= {Barcelona, Spain},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/crossing.html">crossing
		  number homepage</A>.},
  abstract	= {Let $\overline{cr}(G)$ denote the rectilinear crossing
		  number of a graph $G$. We determine
		  $\overline{cr}(K_{11})=102$ and
		  $\overline{cr}(K_{12})=153$. Despite the remarkable hunt
		  for crossing numbers of the complete graph~$K_n$ --
		  initiated by R.~Guy in the 1960s -- these quantities have
		  been unknown for $n>10$ to~date. Our solution mainly relies
		  on a tailor-made method for enumerating all inequivalent
		  sets of points (so-called order types) of size $11$. Based
		  on these findings, we establish new upper and lower bounds
		  on $\overline{cr}(K_{n})$ for general~$n$. Specific values
		  for $n \leq 45$ are given, along with significantly
		  improved asymptotic values. The asymptotic lower bound is
		  immediate from the fact $\overline{cr}(K_{11})=102$,
		  whereas the upper bound stems from a novel construction of
		  drawings with few crossings. The construction is shown to
		  be optimal within its frame. The tantalizing question of
		  determining $\overline{cr}(K_{13})$ is left open. The
		  latest ra(n)ge is $\{221,223,225,227,229\}$; our conjecture
		  is $\overline{cr}(K_{13}) = 229$.}
}

@InProceedings{aak-cncg-02b,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {On the Crossing Number of Complete Graphs - Extended
		  Abstract},
  year		= 2002,
  booktitle	= {Proc. $18^{th}$ European Workshop on Computationl Geometry
		  CG '02 Warszawa},
  pages		= {90-92},
  address	= {Warszawa, Poland},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/crossing.html">crossing
		  number homepage</A>.},
  abstract	= {Let $\overline{cr}(G)$ denote the rectilinear crossing
		  number of a graph $G$. We determine
		  $\overline{cr}(K_{11})=102$ and
		  $\overline{cr}(K_{12})=153$. Despite the remarkable hunt
		  for crossing numbers of the complete graph~$K_n$ --
		  initiated by R.~Guy in the 1960s -- these quantities have
		  been unknown for $n>10$ to~date. Our solution mainly relies
		  on a tailor-made method for enumerating all inequivalent
		  sets of points (so-called order types) of size $11$. Based
		  on these findings, we establish new upper and lower bounds
		  on $\overline{cr}(K_{n})$ for general~$n$. Specific values
		  for $n \leq 45$ are given, along with significantly
		  improved asymptotic values. The asymptotic lower bound is
		  immediate from the fact $\overline{cr}(K_{11})=102$,
		  whereas the upper bound stems from a novel construction of
		  drawings with few crossings. The construction is shown to
		  be optimal within its frame. The tantalizing question of
		  determining $\overline{cr}(K_{13})$ is left open. The
		  latest ra(n)ge is $\{221,223,225,227,229\}$; our conjecture
		  is $\overline{cr}(K_{13}) = 229$.}
}

@Article{aak-cncg-06,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {On the Crossing Number of Complete Graphs},
  year		= 2006,
  journal	= {Computing},
  pages		= {165--176},
  volume	= {76},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/crossing.html">crossing
		  number homepage</A>.},
  abstract	= {Let $\overline{cr}(G)$ denote the rectilinear crossing
		  number of a graph~$G$. We determine
		  $\overline{cr}(K_{11})=102$ and
		  $\overline{cr}(K_{12})=153$. Despite the remarkable hunt
		  for crossing numbers of the complete graph~$K_n$ --
		  initiated by R.~Guy in the 1960s -- these quantities have
		  been unknown for \mbox{$n>10$} to~date. Our solution mainly
		  relies on a tailor-made method for enumerating all
		  inequivalent sets of points (order types) of size~$11$.
		  
		  Based on these findings, we establish a new upper bound on
		  $\overline{cr}(K_{n})$ for general~$n$. The bound stems
		  from a novel construction of drawings of $K_{n}$ with few
		  crossings.}
}

@InProceedings{aak-ctps-01,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {On Compatible Triangulations of Point Sets},
  booktitle	= {Proc. $17^{th}$ European Workshop on Computational
		  Geometry CG '2001},
  pages		= {23--26},
  year		= 2001,
  address	= {Berlin, Germany},
  abstract	= {Two conjectures on compatible triangulations for planar
		  point sets are stated and proven for small sets and for
		  special sets of arbitrary size.}
}

@InProceedings{aak-eotsp-01,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {Enumerating Order Types for Small Point Sets with
		  Applications},
  booktitle	= {Proc. $17^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {11--18},
  year		= 2001,
  address	= {Medford, Massachusetts, USA},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/ordertypes.html">order
		  type homepage</A>.},
  abstract	= {Order types are a means to characterize the combinatorial
		  properties of a finite point configuration. In particular,
		  the crossing properties of all straight-line segments
		  spanned by a planar $n$-point set are reflected by its
		  order type. We establish a complete and reliable data base
		  for all possible order types of size $n=10$ or less. The
		  data base includes a realizing point set for each order
		  type in small integer grid representation. To our
		  knowledge, no such project has been carried out before. We
		  substantiate the usefulness of our data base by applying it
		  to several problems in computational and combinatorial
		  geometry. Problems concerning triangulations, simple
		  polygonalizations, complete geometric graphs, and $k$-sets
		  are addressed. This list of possible applications is not
		  meant to be exhaustive. We believe our data base to be of
		  value to many researchers who wish to examine their
		  conjectures on small point configurations. }
}

@Article{aak-eotsp-01a,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {Enumerating Order Types for Small Point Sets with
		  Applications},
  journal	= {Order},
  pages		= {265--281},
  volume	= 19,
  year		= {2002},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/ordertypes.html">order
		  type homepage</A>.},
  abstract	= {Order types are a means to characterize the combinatorial
		  properties of a finite point configuration. In particular,
		  the crossing properties of all straight-line segments
		  spanned by a planar $n$-point set are reflected by its
		  order type. We establish a complete and reliable data base
		  for all possible order types of size $n=10$ or less. The
		  data base includes a realizing point set for each order
		  type in small integer grid representation. To our
		  knowledge, no such project has been carried out before. We
		  substantiate the usefulness of our data base by applying it
		  to several problems in computational and combinatorial
		  geometry. Problems concerning triangulations, simple
		  polygonalizations, complete geometric graphs, and $k$-sets
		  are addressed. This list of possible applications is not
		  meant to be exhaustive. We believe our data base to be of
		  value to many researchers who wish to examine their
		  conjectures on small point configurations. }
}

@Article{aak-pc-02,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {Points and Combinatorics},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  pages		= {12--17},
  volume	= 1,
  year		= {2002},
  address	= {Graz, Austria}
}

@TechReport{aak-prcn-01,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser},
  title		= {Progress on rectilinear crossing numbers},
  institution	= {IGI-TU Graz, Austria},
  year		= 2002,
  abstract	= {Let $\overline{cr}(G)$ denote the rectilinear crossing
		  number of a graph $G$. We show $\overline{cr}(K_{11})=102$
		  and $\overline{cr}(K_{12})=153$. Despite the remarkable
		  hunt for the crossing number of the complete graph $K_n$,
		  initiated by R. Guy in the 1960s, these quantities have
		  been unknown for $n>10$ to date. We also establish new
		  upper and lower bounds on $\overline{cr}(K_{n})$ for $13
		  \leq n \leq 20$, along with an improved general lower bound
		  for $\overline{cr}(K_{n})$. The results mainly rely on
		  recent methods developed by the authors for exhaustively
		  enumerating all combinatorially inequivalent sets of points
		  (so-called order types). },
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/crossing.html">crossing
		  number homepage</A>.}
}

@InProceedings{aakprsv-rsro-09,
  author	= {O. Aichholzer and F. Aurenhammer and B. Kornberger and S.
		  Plantinga and G. Rote and A. Sturm and G. Vegter},
  title		= {Recovering structure from $r$-sampled objects},
  booktitle	= {Eurographics Symposium on Geometry Processing, Computer
		  Graphics Forum},
  volume	= {28},
  year		= 2009,
  address	= {Berlin, Germany},
  pages		= {1349-1360},
  note		= {Special Issue}
}

@InProceedings{aaks-cmpt-02,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser and B.
		  Speckmann},
  title		= {Convexity Minimizes Pseudo-Triangulations},
  booktitle	= {Proc. $14th$ Annual Canadian Conference on Computational
		  Geometry CCCG 2002},
  pages		= {158--161},
  year		= 2002,
  address	= {Lethbridge, Alberta, Canada},
  abstract	= {For standard triangulations it is not known which sets of
		  points have the fewest or the most triangulations. In
		  contrast, we show that sets of points in convex position
		  minimize the number of minimum pseudo-triangulations. This
		  adds to the common belief that minimum
		  pseudo-triangulations are more tractable in many
		  respects.}
}

@Article{aaks-cmpt-03,
  author	= {O. Aichholzer and F. Aurenhammer and H. Krasser and B.
		  Speckmann},
  title		= {Convexity Minimizes Pseudo-Triangulations},
  journal	= {Computational Geometry: Theory and Applications},
  volume	= {28},
  pages		= {3--10},
  year		= 2004,
  abstract	= {The number of minimum pseudo-triangulations is minimized
		  for point sets in convex position.}
}

@InProceedings{aap-qpssc-02,
  author	= {O. Aichholzer and F. Aurenhammer and B. Palop},
  title		= {Quickest Paths, Straight Skeletons, and the City {V}oronoi
		  Diagram},
  year		= 2002,
  booktitle	= {Proc. $18^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {151--159},
  address	= {Barcelona, Spain},
  abstract	= {The city Voronoi diagram is induced by quickest paths, in
		  the $L_1$~plane speeded up by an isothetic transportation
		  network. We investigate the rich geometric and algorithmic
		  properties of city Voronoi diagrams, and report on their
		  use in processing quickest-path queries.\\ In doing so, we
		  revisit the fact that not every Voronoi-type diagram has
		  interpretations in both the distance model and the
		  wavefront model. Especially, straight skeletons are a
		  relevant example where an interpretation in the former
		  model is lacking. We clarify the relation between these
		  models, and further draw a connection to the
		  bisector-defined abstract Voronoi diagram model, with the
		  particular goal of computing the city Voronoi diagram
		  efficiently. }
}

@Article{aap-qpssc-03,
  author	= {O. Aichholzer and F. Aurenhammer and B. Palop},
  title		= {Quickest Paths, Straight Skeletons, and the City {V}oronoi
		  Diagram},
  journal	= {Discrete \& Computational Geometry},
  volume	= 31,
  number	= 1,
  pages		= {17--35},
  year		= {2004},
  abstract	= {The city Voronoi diagram is induced by quickest paths, in
		  the $L_1$~plane speeded up by an isothetic transportation
		  network. We investigate the rich geometric and algorithmic
		  properties of city Voronoi diagrams, and report on their
		  use in processing quickest-path queries.\\ In doing so, we
		  revisit the fact that not every Voronoi-type diagram has
		  interpretations in both the distance model and the
		  wavefront model. Especially, straight skeletons are a
		  relevant example where an interpretation in the former
		  model is lacking. We clarify the relation between these
		  models, and further draw a connection to the
		  bisector-defined abstract Voronoi diagram model, with the
		  particular goal of computing the city Voronoi diagram
		  efficiently. }
}

@InProceedings{aar-msrp-94a,
  author	= {O. Aichholzer and H. Alt and G. Rote},
  title		= {Matching Shapes with a Reference Point},
  booktitle	= {Proc. $10^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {85--92},
  year		= 1994,
  address	= {Stony Brook, New York, USA},
  abstract	= {For two given point sets, we present a very simple (almost
		  trivial) algorithm to translate one set so that the
		  Hausdorff distance between the two sets is not larger than
		  a constant factor times the minimum Hausdorff distance
		  which can be achieved in this way. The algorithm just
		  matches the so-called Steiner points of the two sets.\\ The
		  focus of our paper is the general study of reference points
		  (like the Steiner point) and their properties with respect
		  to shape matching.\\ For more general transformations than
		  just translations, our method eliminates several degrees of
		  freedom from the problem and thus yields good matchings
		  with improved time bounds.}
}

@InProceedings{aar-msrp-94b,
  author	= {O. Aichholzer and H. Alt and G. Rote},
  title		= {Matching Shapes with a Reference Point},
  booktitle	= {Proc. $10^{th}$ European Workshop on Computational
		  Geometry CG '94},
  pages		= {81--84},
  year		= 1994,
  address	= {Santander, Spain},
  abstract	= {For two given point sets, we present a very simple (almost
		  trivial) algorithm to translate one set so that the
		  Hausdorff distance between the two sets is not larger than
		  a constant factor times the minimum Hausdorff distance
		  which can be achieved in this way. The algorithm just
		  matches the so-called Steiner points of the two sets.\\ The
		  focus of our paper is the general study of reference points
		  (like the Steiner point) and their properties with respect
		  to shape matching.\\ For more general transformations than
		  just translations, our method eliminates several degrees of
		  freedom from the problem and thus yields good matchings
		  with improved time bounds.}
}

@Article{aar-msrp-97,
  author	= {O. Aichholzer and H. Alt and G. Rote},
  title		= {Matching Shapes with a Reference Point},
  journal	= {Int'l Journal of Computational Geometry \& Applications},
  year		= 1997,
  volume	= 7,
  number	= 4,
  pages		= {349--363},
  abstract	= {For two given point sets, we present a very simple (almost
		  trivial) algorithm to translate one set so that the
		  Hausdorff distance between the two sets is not larger than
		  a constant factor times the minimum Hausdorff distance
		  which can be achieved in this way. The algorithm just
		  matches the so-called Steiner points of the two sets.\\ The
		  focus of our paper is the general study of reference points
		  (like the Steiner point) and their properties with respect
		  to shape matching.\\ For more general transformations than
		  just translations, our method eliminates several degrees of
		  freedom from the problem and thus yields good matchings
		  with improved time bounds.}
}

@TechReport{aar-ogosa-95,
  author	= {O. Aichholzer and F. Aurenhammer and G. Rote},
  title		= {Optimal graph orientation with storage applications},
  institution	= {SFB 'Optimierung und Kontrolle', TU Graz, Austria},
  year		= 1995,
  type		= {SFB-Report},
  number	= {F003-51},
  abstract	= {We show that the edges of a graph with maximum edge
		  density $d$ can always be oriented such that each vertex
		  has in-degree at most $d$. Hence, for arbitrary graphs,
		  edges can always be assigned to incident vertices as
		  uniformly as possible. For example, in-degree 3 is achieved
		  for planar graphs. This immediately gives a space-optimal
		  data structure that answers edge membership queries in a
		  maximum edge density-$d$ graph in $O(\log d)$ time.}
}

@InProceedings{aart-tin-95,
  author	= {O. Aichholzer and F. Aurenhammer and G. Rote and M.
		  Taschwer},
  title		= {Triangulations intersect nicely},
  booktitle	= {Proc. $11^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {220--229},
  year		= 1995,
  address	= {Vancouver, Canada},
  abstract	= {We prove that two different triangulations of the same
		  planar point set always intersect in a systematic manner,
		  concerning both their edges and their triangles. As a
		  consequence, improved lower bounds on the weight of a
		  triangulation are obtained by solving an assignment
		  problem. The new bounds cover the previously known bounds
		  and can be computed in polynomial time. As a by-product, an
		  easy-to-recognize class of point sets is exhibited where
		  the minimum-weight triangulation coincides with the greedy
		  triangulation.}
}

@InProceedings{aarx-clgta-96,
  author	= {O. Aichholzer and F. Aurenhammer and G. Rote and Y.-F.
		  Xu},
  title		= {Constant-level greedy triangulations approximate the {MWT}
		  well},
  booktitle	= {Proc. $2^{nd}$ Int'l. Symp. Operations Research \&
		  Applications ISORA'96, Lecture Notes in Operations
		  Research},
  pages		= {309--318},
  year		= 1996,
  editor	= {Du, Zhang, Cheng},
  volume	= 2,
  address	= {Guilin, P. R. China},
  publisher	= {World Publishing Corporation},
  abstract	= {The well-known greedy triangulation $GT(S)$ of a finite
		  point set $S$ is obtained by inserting compatible edges in
		  increasing length order, where an edge is compatible if it
		  does not cross previously inserted ones. Exploiting the
		  concept of so-called light edges, we introduce a definition
		  of $GT(S)$ that does not rely on the length ordering of the
		  edges. Rather, it provides a decomposition of $GT(S)$ into
		  levels, and the number of levels allows us to bound the
		  total edge length of $GT(S)$. In particular, we show
		  $|GT(S)| \leq 3 \cdot 2^{k+1} |MWT(S)|$, where $k$ is the
		  number of levels and $MWT(S)$ is the minimum-weight
		  triangulation of $S$.}
}

@Article{aarx-clgta-99,
  author	= {O. Aichholzer and F. Aurenhammer and G. Rote and Y.-F.
		  Xu},
  title		= {Constant-level greedy triangulations approximate the {MWT}
		  well},
  journal	= {Journal of Combinatorial Optimization},
  year		= 1999,
  volume	= 2,
  pages		= {361--369},
  note		= {[SFB-Report F003-050, TU Graz, Austria, 1995]},
  abstract	= {The well-known greedy triangulation $GT(S)$ of a finite
		  point set $S$ is obtained by inserting compatible edges in
		  increasing length order, where an edge is compatible if it
		  does not cross previously inserted ones. Exploiting the
		  concept of so-called light edges, we introduce a definition
		  of $GT(S)$ that does not rely on the length ordering of the
		  edges. Rather, it provides a decomposition of $GT(S)$ into
		  levels, and the number of levels allows us to bound the
		  total edge length of $GT(S)$. In particular, we show
		  $|GT(S)| \leq 3 \cdot 2^{k+1} |MWT(S)|$, where $k$ is the
		  number of levels and $MWT(S)$ is the minimum-weight
		  triangulation of $S$.}
}

@InProceedings{aarx-ngta-96,
  author	= {O. Aichholzer and F. Aurenhammer and G. Rote and Y.-F.
		  Xu},
  title		= {New greedy triangulation algorithms},
  booktitle	= {Proc. $12^{th}$ European Workshop on Computational
		  Geometry CG '96},
  pages		= {11--14},
  year		= 1996,
  address	= {Muenster, Germany},
  abstract	= {The classical greedy triangulation (GT) of a set $S$ of
		  $n$ points in the plane is the triangulation obtained by
		  starting with the empty set (of edges) and at each step
		  adding the shortest compatible edge between two of the
		  points of $S$, where a compatible edge is defined to be an
		  edge that crosses none of the previously added edges. In
		  this paper we use the greedy method as a general concept to
		  compute a triangulation of a planar point set. We use
		  either edges or triangles as basic objects. Furthermore we
		  give different variants to compute the weight of the
		  objects, either in a static or dynamic way, leading to a
		  total of $156$ different greedy triangulation algorithms.
		  We investigate these algorithms in their quality of
		  approximating the MWT.}
}

@Article{aaw-afa-02,
  author	= {O. Aichholzer and F. Aurenhammer and T. Werner},
  title		= {Algorithmic Fun - {A}balone},
  journal	= {Special Issue on Foundations of Information Processing of
		  {TELEMATIK}},
  pages		= {4--6},
  year		= {2002},
  volume	= 1,
  address	= {Graz, Austria}
}

@InProceedings{abdhkkrsu-ggt-03,
  author	= {O. Aichholzer and D. Bremner and E.D. Demaine and F.
		  Hurtado and E. Kranakis and H. Krasser and S. Ramaswami and
		  S. Sethia and J. Urrutia},
  title		= {Geometric Games on Triangulations},
  booktitle	= {Proc. $19^{th}$ European Workshop on Computationl Geometry
		  CG '03 Bonn },
  pages		= {89--92},
  year		= 2003,
  address	= {Bonn, Germany},
  abstract	= {We analyze several perfect-information combinatorial games
		  played on planar triangulations. We describe main broad
		  categories of these games and provide in various situations
		  polynomial-time algorithms to determine who wins a given
		  game under optimal play, and ideally, to find a winning
		  strategy. Relations to relevant existing combinatorial
		  games, such as Kayles, are also shown.}
}

@InProceedings{abdhkkrsu-pwt-02,
  author	= {O. Aichholzer and D. Bremner and E.D. Demaine and F.
		  Hurtado and E. Kranakis and H. Krasser and S. Ramaswami and
		  S. Sethia and J. Urrutia},
  title		= {Playing with Triangulations},
  booktitle	= {Proc. Japan Conference on Discrete and Computational
		  Geometry JCDCG 2002},
  pages		= {46--54},
  year		= 2002,
  address	= {Tokyo, Japan},
  abstract	= {We analyze several perfect-information combinatorial games
		  played on planar triangulations. We describe main broad
		  categories of these games and provide in various situations
		  polynomial-time algorithms to determine who wins a given
		  game under optimal play, and ideally, to find a winning
		  strategy. Relations to relevant existing combinatorial
		  games, such as Kayles, are also shown.}
}

@InProceedings{abdhkkrsu-pwt-03,
  author	= {O. Aichholzer and D. Bremner and E.D. Demaine and F.
		  Hurtado and E. Kranakis and H. Krasser and S. Ramaswami and
		  S. Sethia and J. Urrutia},
  title		= {Playing with Triangulations},
  booktitle	= {Lecture Notes in Computer Science 2866, Japanese
		  Conference, JCDCG 2002},
  pages		= {22--37},
  year		= 2003,
  abstract	= {We analyze several perfect-information combinatorial games
		  played on planar triangulations. We introduce three broad
		  categories of such games constructing, transforming and
		  marking triangulations. In various situations, we develop
		  polynomial-time algorithms to determine who wins a given
		  game under optimal play, and to find a winning strategy.
		  Along the way we show connections to existing combinatorial
		  games, such as Kayles.}
}

@InProceedings{abdmss-lpuof-01,
  author	= {O. Aichholzer and D. Bremner and E.D. Demaine and D.
		  Meijer and V. Sacrist\'{a}n and M. Soss},
  title		= {Long Proteins with Unique Optimal Foldings in the H-P
		  Model},
  booktitle	= {Proc. $17^{th}$ European Workshop on Computational
		  Geometry CG '2001},
  pages		= {59--62},
  year		= 2001,
  address	= {Berlin, Germany},
  abstract	= {We explore a problem suggested by Brian Hayes in 1998:
		  what proteins in the two-dimensional
		  hydrophilic-hydrophobic (H-P) model have {\it unique}
		  optimal foldings? In particular, we prove that there are
		  closed chains of monomers (amino acids) with this property
		  for all (even) lengths; and that there are open monomer
		  chains with this property fo all lengths divisible by four.
		  Along the way, we prove and conjecture several results
		  about bonds in the H-P model.}
}

@Article{abdmss-lpuof-02,
  author	= {O. Aichholzer and D. Bremner and E.D. Demaine and D.
		  Meijer and V. Sacrist\'{a}n and M. Soss},
  title		= {Long Proteins with Unique Optimal Foldings in the H-P
		  Model},
  journal	= {Computational Geometry: Theory and Applications},
  year		= 2003,
  pages		= {139--159},
  volume	= {25},
  abstract	= {It is widely accepted that (1) the natural or folded state
		  of proteins is a global energy minimum, and (2) in most
		  cases proteins fold to a unique state determined by their
		  amino acid sequence. The H-P (hydrophobic-hydrophilic)
		  model is a simple combinatorial model designed to answer
		  qualitative questions about the protein folding process. In
		  this paper we consider a problem suggested by Brian Hayes
		  in 1998: what proteins in the two-dimensional H-P model
		  have \emph{unique} optimal (minimum energy) foldings? In
		  particular, we prove that there are closed chains of
		  monomers (amino acids) with this property for all (even)
		  lengths; and that there are open monomer chains with this
		  property for all lengths divisible by four.}
}

@Article{acbfs-nsmabp-2002,
  author	= {P. Auer and N. Cesa-Bianchi and Y. Freund and R. E.
		  Schapire },
  title		= {The Nonstochastic Multiarmed Bandit Problem},
  journal	= {SIAM Journal on Computing},
  year		= {2002},
  volume	= {32},
  number	= {1},
  pages		= {48--77},
  note		= {A preliminary version has appeared in {\em Proceedings of
		  the 36th Annual Symposium on Foundations of Computer
		  Science}}
}

@Article{acbg-ascolla-2003,
  author	= {P. Auer and N. Cesa-Bianchi and C. Gentile},
  title		= {Adaptive and Self-Confident On-Line Learning Algorithms},
  journal	= {JCSS},
  year		= {2002},
  volume	= {64},
  number	= {1},
  pages		= {48--75},
  note		= {A preliminary version has appeared in {\em Proc. 13th Ann.
		  Conf. Computational Learning Theory}}
}

@InProceedings{acddemoprt-fp-00a,
  author	= {O. Aichholzer and C. Cort\'{e}s and E.D. Demaine and V.
		  Dujmovi\'{c} and J. Erickson and H. Meijer and M. Overmars
		  and B. Palop and S. Ramaswami and G.T. Toussaint},
  title		= {Flipturning Polygons},
  booktitle	= {Proc. Japan Conference on Discrete and Computational
		  Geometry JCDCG 2000},
  year		= 2000,
  address	= {Tokay University, Tokyo, Japan},
  htmlnote	= {See also <A
		  HREF="http://compgeom.cs.uiuc.edu/\~{}jeffe/pubs/flipturn.html">Jeff'shomepage</A>
		  about this paper.},
  abstract	= {A flipturn is an operation that transforms a nonconvex
		  simple polygon into another simple polygon, by rotating a
		  concavity 180 degrees around the midpoint of its bounding
		  convex hull edge. Joss and Shannon proved in 1973 that a
		  sequence of flipturns eventually transforms any simple
		  polygon into a convex polygon. This paper describes several
		  new results about such flipturn sequences. We show that any
		  orthogonal polygon is convexified after at most $n-5$
		  arbitrary flipturns, or at most $5(n-4)/6$ well-chosen
		  flipturns, improving the previously best upper bound of
		  $(n-1)!/2$. We also show that any simple polygon can be
		  convexified by at most $n^2-4n+1$ flipturns, generalizing
		  earlier results of Ahn et al. These bounds depend
		  critically on how degenerate cases are handled; we
		  carefully explore several possibilities. We describe how to
		  maintain both a simple polygon and its convex hull in
		  $O(\log^4 n)$ time per flipturn, using a data structure of
		  size $O(n)$. We show that although flipturn sequences for
		  the same polygon can have very different lengths, the shape
		  and position of the final convex polygon is the same for
		  all sequences and can be computed in $O(n \log n)$ time.
		  Finally, we demonstrate that finding the longest
		  convexifying flipturn sequence of a simple polygon is
		  NP-hard.}
}

@Article{acddemoprt-fp-00b,
  author	= {O. Aichholzer and C. Cort\'{e}s and E.D. Demaine and V.
		  Dujmovi\'{c} and J. Erickson and H. Meijer and M. Overmars
		  and B. Palop and S. Ramaswami and G.T. Toussaint},
  title		= {Flipturning Polygons},
  journal	= {Discrete \& Computational Geometry},
  year		= 2002,
  pages		= {231--253},
  volume	= {28},
  note		= {[Report UU-CS-2000-31, Universiteit Utrecht, The
		  Netherlands, 2000]},
  htmlnote	= {See also <A
		  HREF="http://compgeom.cs.uiuc.edu/\~{}jeffe/pubs/flipturn.html">Jeff'shomepage</A>
		  about this paper.},
  abstract	= {A flipturn is an operation that transforms a nonconvex
		  simple polygon into another simple polygon, by rotating a
		  concavity 180 degrees around the midpoint of its bounding
		  convex hull edge. Joss and Shannon proved in 1973 that a
		  sequence of flipturns eventually transforms any simple
		  polygon into a convex polygon. This paper describes several
		  new results about such flipturn sequences. We show that any
		  orthogonal polygon is convexified after at most $n-5$
		  arbitrary flipturns, or at most $5(n-4)/6$ well-chosen
		  flipturns, improving the previously best upper bound of
		  $(n-1)!/2$. We also show that any simple polygon can be
		  convexified by at most $n^2-4n+1$ flipturns, generalizing
		  earlier results of Ahn et al. These bounds depend
		  critically on how degenerate cases are handled; we
		  carefully explore several possibilities. We describe how to
		  maintain both a simple polygon and its convex hull in
		  $O(\log^4 n)$ time per flipturn, using a data structure of
		  size $O(n)$. We show that although flipturn sequences for
		  the same polygon can have very different lengths, the shape
		  and position of the final convex polygon is the same for
		  all sequences and can be computed in $O(n \log n)$ time.
		  Finally, we demonstrate that finding the longest
		  convexifying flipturn sequence of a simple polygon is
		  NP-hard.}
}

@Article{acf-ftamabp-02,
  author	= {P. Auer and N. Cesa-Bianchi and P. Fischer},
  title		= {Finite Time Analysis of the Multiarmed Bandit Problem},
  journal	= {Machine Learning},
  year		= {2002},
  volume	= {47},
  number	= {2/3},
  pages		= {235--256},
  note		= {A preliminary version has appeared in {\em Proc. of the
		  15th International Conference on Machine Learning}}
}

@InProceedings{acf-ftamabp-99,
  author	= {P. Auer and N. Cesa-Bianchi and P. Fischer},
  title		= {Finite Time Analysis of the Multiarmed Bandit Problem},
  booktitle	= {IT Workshop on Decision, Estimation, Classification and
		  Imaging},
  year		= {1999},
  optorganization={},
  optpublisher	= {},
  address	= {Santa Fe},
  month		= {Feb}
}

@InProceedings{adehost-rcp-00a,
  author	= {O. Aichholzer and E.D. Demaine and J. Erickson and F.
		  Hurtado and M. Overmars and M.A. Soss and G.T. Toussaint},
  title		= {Reconfiguring Convex Polygons},
  booktitle	= {Proc. $12th$ Annual Canadian Conference on Computational
		  Geometry CCCG 2000},
  pages		= {17--20},
  year		= 2000,
  address	= {Fredericton, New Brunswick, Canada},
  abstract	= {We prove that there is a motion from any convex polygon to
		  any convex polygon with the same counterclockwise sequence
		  of edge lengths, that preserves the lengths of the edges,
		  and keeps the polygon convex at all times. Furthermore, the
		  motion is ``direct'' (avoiding any intermediate canonical
		  configuration like a subdivided triangle) in the sense that
		  each angle changes monotonically throughout the motion. In
		  contrast, we show that it is impossible to achieve such a
		  result with each vertex-to-vertex distance changing
		  monotonically.}
}

@Article{adehost-rcp-00b,
  author	= {O. Aichholzer and E.D. Demaine and J. Erickson and F.
		  Hurtado and M. Overmars and M.A. Soss and G.T. Toussaint},
  title		= {Reconfiguring Convex Polygons},
  journal	= {Computational Geometry: Theory and Applications},
  year		= 2001,
  pages		= {85--95},
  volume	= 20,
  note		= {[Report UU-CS-2000-30, Universiteit Utrecht, The
		  Netherlands, 2000]},
  abstract	= {We prove that there is a motion from any convex polygon to
		  any convex polygon with the same counterclockwise sequence
		  of edge lengths, that preserves the lengths of the edges,
		  and keeps the polygon convex at all times. Furthermore, the
		  motion is ``direct'' (avoiding any intermediate canonical
		  configuration like a subdivided triangle) in the sense that
		  each angle changes monotonically throughout the motion. In
		  contrast, we show that it is impossible to achieve such a
		  result with each vertex-to-vertex distance changing
		  monotonically. We also demonstrate that there is a motion
		  between any two such polygons using three-dimensional moves
		  known as pivots, although the complexity of the motion
		  cannot be bounded as a function of the number of vertices
		  in the polygon.}
}

@Article{adk-flsvd-06,
  author	= {F. Aurenhammer and R.L.S.Drysdale and H. Krasser},
  title		= {Farthest line segment {V}oronoi diagrams},
  journal	= {Information Processing Letters},
  year		= 2006,
  pages		= {220--225},
  volume	= {100},
  abstract	= {The farthest line segment Voronoi diagram shows properties
		  different from both the closest-segment Voronoi diagram and
		  the farthest-point Voronoi diagram. Surprisingly, this
		  structure did not receive attention in the computational
		  geometry literature. We analyze its combinatorial and
		  topological properties and outline an $O(n \log n)$ time
		  construction algorithm that is easy to implement. No
		  restrictions are placed upon the~$n$ input line segments;
		  they are allowed to touch or cross.}
}

@TechReport{adr-sltgt-95,
  author	= {O. Aichholzer and R.L.S. Drysdale and G. Rote},
  title		= {A Simple Linear Time Greedy Triangulation Algorithm for
		  Uniformly Distributed Points},
  institution	= {TU Graz, Austria},
  year		= 1995,
  type		= {IIG-Report-Series},
  number	= {408},
  note		= {Presented at the Workshop on Computational Geometry, Army
		  MSI Cornell, Stony Brook, 1994},
  abstract	= {The greedy triangulation (GT) of a set $S$ of $n$ points
		  in the plane is the triangulation obtained by starting with
		  the empty set and at each step adding the shortest
		  compatible edge between two of the points, where a
		  compatible edge is defined to be an edge that crosses none
		  of the previously added edges. In this paper we present a
		  simple, practical algorithm that computes the greedy
		  triangulation in expected time $O(n)$ and space $O(n)$, for
		  $n$ points drawn independently from a uniform distribution
		  over some fixed convex shape.\\ This algorithm is an
		  improvement of the $O(n \log n)$ algorithm of Dickerson,
		  Drysdale, McElfresh, and Welzl. It uses their basic
		  approach, but generates only $O(n)$ plausible greedy edges
		  instead of $O(n \log n)$. It uses some ideas similar to
		  those presented in Levcopoulos and Lingas's $O(n)$ expected
		  time algorithm. Since we use more knowledge about the
		  structure of a random point set and its greedy
		  triangulation, our algorithm needs only elementary data
		  structures and simple bucketing techniques. Thus it is a
		  good deal simpler to explain and to implement than the
		  algorithm of Levcopoulos and Lingas. }
}

@InProceedings{ads-ccq-08,
  author	= {F. Aurenhammer and M. Demuth and T. Schiffer},
  title		= {Computing convex quadrangulations},
  booktitle	= {Proc. 5th Ann. Int. Symp. Voronoi Diagrams in Science and
		  Engineering, Voronoi's Impact on Modern Science},
  year		= 2008,
  address	= {Kiev, Ukraine},
  volume	= 4,
  pages		= {32--43},
  abstract	= {We use projected Delaunay tetrahedra and a maximum
		  independent set approach to compute large subsets of convex
		  quadrangulations on a given set of points in the plane. The
		  new method improves over the popular pairing method based
		  on triangulating the point set.}
}

@Article{ae-oacwv-84,
  author	= {F. Aurenhammer and H. Edelsbrunner},
  title		= {An optimal algorithm for constructing the weighted
		  {V}oronoi diagram in the plane},
  journal	= {Pattern Recognition},
  year		= 1984,
  volume	= 17,
  number	= 2,
  pages		= {251--257},
  note		= {[IIG-Report-Series F109, TU Graz, Austria, 1983]},
  abstract	= {Let $S$ denote a set of $n$ points in the plane such that
		  each point $p$ has assigned a positive weight $w(p)$ which
		  expresses its capability to influence its neighborhood. In
		  this sense, the weighted distance of an arbitrary point $x$
		  from $p$ is gived by $d_e(x,p)/w(p)$, where $d_e$ denotes
		  the Euclidean distance function. The weighted Voronoi
		  diagram for $S$ is a subdivision of the plane such that
		  each point $p$ in $S$ is associated with a region
		  consisting of all points $x$ in the plane for which $p$ is
		  a weighted nearest point of $S$. An algorithm which
		  constructs the weighted Voronoi diagram for $S$ in $O(n^2)$
		  time is outlined in this paper. The method is optimal as
		  the diagram can consist of $\Theta(n^2)$ faces, edges, and
		  vertices.}
}

@Article{afisw-fiepc-94,
  author	= {F. Aurenhammer and M. Formann and R. Idury and A.
		  Schaeffer and F. Wagner},
  title		= {Faster isometric embedding in products of complete
		  graphs},
  journal	= {Discrete Applied Mathematics},
  year		= 1994,
  volume	= 52,
  pages		= {17--28},
  note		= {[Report B-90-06, FU Berlin, Germany, 1990]},
  abstract	= {An isometric embedding of a connected graph $G$ into a
		  cartesian product of complete graphs is equivalent to a
		  labeling of each vertex of $G$ by a string of fixed length
		  such that the distance in $G$ between two vertices is equal
		  to the Hamming distance between their labels. We give a
		  simple $O(D(m,n)+n^{2})$-time algorithm for deciding if $G$
		  admits such an embedding, and for labeling $G$ if one
		  exists, where $D(m,n)$ is the time needed to compute the
		  all-pairs distance matrix of a graph with $m$ edges and $n$
		  vertices. If the distance matrix is part of the input, our
		  algorithm runs in $O(n^{2})$ time. We also show that an
		  $n$-vertex subgraph of $(K_{a})^{d}$, the cartesian product
		  of $d$ equal-sized complete graphs, cannot have more than
		  $\frac{a-1}{2}n\log_a n$ edges. With this result our
		  algorithm can be used to decide whether a graph $G$ is an
		  $a$-ary Hamming graph in $O(n^2\log n)$ time (for fixed
		  $a$).}
}

@InProceedings{ag-asola-00,
  author	= {P. Auer and C. Gentile},
  title		= {Adaptive and Self-Confident On-Line Learning Algorithms},
  booktitle	= {Proc. 13th Ann. Conf. Computational Learning Theory},
  year		= {2000},
  pages		= {107--117},
  publisher	= {Morgan Kaufmann}
}

@InProceedings{ah-ceceg-89,
  author	= {F. Aurenhammer and J. Hagauer},
  title		= {Computing equivalence classes among the edges of a graph
		  with applications},
  booktitle	= {Proc. Int'l Conf. Algebraic Graph Theory},
  year		= 1989,
  pages		= {11},
  address	= {Leibnitz, Austria},
  abstract	= {For two edges $e=(x,y)$ and $e'=(x',y')$ of a connected
		  graph $G=(V,E)$ let $e \Theta e'$ iff $d(x,x') + d(y,y')
		  \neq d(x,y') + d(x',y)$. Here $d(x,y)$ denotes the length
		  of a shortest path in $G$ joining vertices $x$ and $y$. An
		  algorithm is presented that computes the equivalence
		  classes induced on $E$ by the transitive closure
		  $\hat{\Theta}$ of $\Theta$ in time $O(|V||E|)$ and space
		  $O(|V|^2)$. Finding the equivalence classes of
		  $\hat{\Theta}$ is the primary step of several graph algorithms.}
}

@Article{ah-ceceg-92,
  author	= {F. Aurenhammer and J. Hagauer},
  title		= {Computing equivalence classes among the edges of a graph
		  with applications},
  journal	= {Discrete Mathematics},
  year		= 1992,
  volume	= 109,
  pages		= {3--12},
  note		= {Special Issue. [IIG-Report-Series 271, TU Graz, Austria,
		  1989]},
  abstract	= {For two edges $e=(x,y)$ and $e'=(x',y')$ of a connected
		  graph $G=(V,E)$ let $e \Theta e'$ iff $d(x,x') + d(y,y')
		  \neq d(x,y') + d(x',y)$. Here $d(x,y)$ denotes the length
		  of a shortest path in $G$ joining vertices $x$ and $y$. An
		  algorithm is presented that computes the equivalence
		  classes induced on $E$ by the transitive closure
		  $\hat{\Theta}$ of $\Theta$ in time $O(|V||E|)$ and space
		  $O(|V|^2)$. Finding the equivalence classes of
		  $\hat{\Theta}$ is the primary step of several graph algorithms.}
}

@InProceedings{ah-fmmrr-93,
  author	= {O. Aichholzer and H. Hassler},
  title		= {A fast method for modulus reduction in Residue Number
		  System},
  booktitle	= {Proc. epp'93},
  pages		= {41--54},
  year		= 1993,
  address	= {Vienna, Austria},
  note		= {[IIG-Report-Series 312, TU Graz, Austria, 1991]},
  abstract	= {Over the last three decades there has been considerable
		  interest in the implementation of digital computer elements
		  using hardware based on the residue number system. We
		  propose a technique to compute a residue in this number
		  system using a parallel network. Our technique enables
		  scaling, to. We improve a former result of $O(n)$ cycles to
		  $O(\log n)$, where $n$ is the number of moduli. The
		  hardware expense is the same, $O(n^2)$. Further advantages
		  are that scaling factors can be chosen almost freely
		  allowing scaling with radix $2$. Negative numbers are
		  covered as well, requiring no additional effort.
		  Applications are RSA encryption and scaling.}
}

@InProceedings{ah-rbhgo-91,
  author	= {F. Aurenhammer and J. Hagauer},
  title		= {Recognizing binary {H}amming graphs in {$O(n^{2} \log n)$}
		  time},
  booktitle	= {Proc. $16^{th}$ Int'l Workshop on Graph-Theoretical
		  Concepts in Computer Science, Lecture Notes in Computer
		  Science},
  pages		= {90--98},
  year		= 1991,
  volume	= 484,
  address	= {Berlin, Germany},
  publisher	= {Springer Verlag},
  abstract	= {A graph $G$ is called a binary Hamming graph if each
		  vertex of $G$ can be assigned a binary address of fixed
		  length such that the Hamming distance between two addresses
		  equals the length of a shortest path between the
		  corresponding vertices. It is shown that $O(n^2 \log n)$
		  time suffices for deciding whether a given $n$-vertex graph
		  $G$ is a binary Hamming graph, and for computing a valid
		  addressing scheme for $G$ provided its existence. This is
		  not far from being optimal as $n$ addresses of length $n-1$
		  have to be computed in the worst case.}
}

@Article{ah-rbhgo-95,
  author	= {F. Aurenhammer and J. Hagauer},
  title		= {Recognizing binary {H}amming graphs in {$O(n^{2} \log n)$}
		  time},
  journal	= {Mathematical Systems Theory},
  year		= 1995,
  volume	= 28,
  pages		= {387--395},
  note		= {[IIG-Report-Series 273, TU Graz, Austria, 1989]},
  abstract	= {A graph $G$ is called a binary Hamming graph if each
		  vertex of $G$ can be assigned a binary address of fixed
		  length such that the Hamming distance between two addresses
		  equals the length of a shortest path between the
		  corresponding vertices. It is shown that $O(n^2 \log n)$
		  time suffices for deciding whether a given $n$-vertex graph
		  $G$ is a binary Hamming graph, and for computing a valid
		  addressing scheme for $G$ provided its existence. This is
		  not far from being optimal as $n$ addresses of length $n-1$
		  have to be computed in the worst case.}
}

@InProceedings{aha-lsp-92,
  author	= {F. Aurenhammer and F. Hoffmann and B. Aronov},
  title		= {Least-squares partitioning},
  booktitle	= {Proc. $8^{th}$ European Workshop on Computational Geometry
		  CG '92},
  pages		= {55--57},
  year		= 1992,
  address	= {Utrecht, the Netherlands}
}

@InProceedings{aha-mttls-92,
  author	= {F. Aurenhammer and F. Hoffmann and B. Aronov},
  title		= {{M}inkowski-type theorems and least-squares partitioning},
  booktitle	= {Proc. $8^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {350--357},
  year		= 1992,
  address	= {Berlin, Germany},
  note		= {[Report B-92-09, FU Berlin, Germany, 1992]},
  abstract	= {The power diagram of $n$ weighted sites partitions a given
		  $m$-point set into clusters, one cluster for each region of
		  the diagram. In this way, an assignment of points to sites
		  is induced. We show the equivalence of such assignments to
		  constrained Euclidean least-squares assignments. As a
		  corollary, there always exists a power diagram whose
		  regions partition a given $d$-dimensional $m$-point set
		  into clusters of prescribed sizes, no matter where the
		  sites are placed. Another consequence is that least-squares
		  assignments can be computed by finding suitable weights for
		  the sites. In the plane, this takes roughly $O(n^2m)$ time
		  and optimal space $O(m)$, which improves on previous
		  methods. We further show that a constrained least-squares
		  assignment can be computed by solving a particular linear
		  program in $n+1$ dimensions. This leads to an algorithm for
		  iteratively improving the weights. Aside from the obvious
		  application, least-squares assignments are shown to be
		  useful in solving a certain transportation problem, and in
		  finding least-squares fittings where translation and
		  scaling are allowed. Finally, we extend the concept of a
		  constrained least-squares assignment to continuous point
		  sets, thereby obtaining results on power diagrams with
		  prescribed region volumes that are related to Minkowski's
		  theorem for convex polytopes.}
}

@Article{aha-mttls-98,
  author	= {F. Aurenhammer and F. Hoffmann and B. Aronov},
  title		= {{M}inkowski-type theorems and least-squares clustering},
  journal	= {Algorithmica},
  year		= 1998,
  volume	= 20,
  pages		= {61--76},
  note		= {[SFB Report F003-075, TU Graz, Austria, 1996]},
  abstract	= {Dissecting Euclidean $d$-space with the power diagram of
		  $n$ weighted point sites partitions a given $m$-point set
		  into clusters, one cluster for each region of the diagram.
		  In this manner, an assignment of points to sites is
		  induced. We show the equivalence of such assignments to
		  constrained Euclidean least-squares assignments. As a
		  corollary, there always exists a power diagram whose
		  regions partition a given $d$-dimensional $m$-point set
		  into clusters of prescribed sizes, no matter where the
		  sites are placed. Another consequence is that constrained
		  least-squares assignments can be computed by finding
		  suitable weights for the sites. In the plane, this takes
		  roughly $O(n^2m)$ time and optimal space $O(m)$, which
		  improves on previous methods. We further show that a
		  constrained least-squares assignment can be computed by
		  solving a specially structured linear program in $n+1$
		  dimensions. This leads to an algorithm for iteratively
		  improving the weights, based on the gradient-descent
		  method. Besides having the obvious optimization property,
		  least-squares assignments are shown to be useful in solving
		  a certain transportation problem, and in finding a
		  least-squares fitting of two point sets where translation
		  and scaling are allowed. Finally, we extend the concept of
		  a constrained least-squares assignment to continuous
		  distributions of points, thereby obtaining existence
		  results for power diagrams with prescribed region volumes.
		  These results are related to Minkowski's theorem for convex
		  polytopes. The aforementioned iterative method for
		  approximating the desired power diagram applies to
		  continuous distributions as well.}
}

@Article{ahi-cgflc-92,
  author	= {F. Aurenhammer and J. Hagauer and W. Imrich},
  title		= {{C}artesian graph factorization at logarithmic cost per
		  edge},
  journal	= {Computational Complexity},
  year		= 1992,
  volume	= 2,
  pages		= {331--349},
  abstract	= {Let $G$ be a connected graph with $n$ vertices and $m$
		  edges. We develop an algorithm that finds the prime factors
		  of $G$ with respect to Cartesian multiplication in $O(m
		  \log n)$ time and $O(m)$ space. This shows that factoring
		  $G$ is at most as costly as sorting its edges. The
		  algorithm gains its efficiency and practicality from using
		  only basic properties of product graphs and simple data
		  structures.}
}

@InProceedings{ahi-fcpgl-90,
  author	= {F. Aurenhammer and J. Hagauer and W. Imrich},
  title		= {Factoring {C}artesian-product graphs at logarithmic cost
		  per edge},
  booktitle	= {Proc. MPS Conf. Integer Programming and Combinatorial
		  Optimization IPCO'90},
  pages		= {29--44},
  year		= 1990,
  address	= {Waterloo, Canada},
  note		= {[IIG-Report-Series 287, TU Graz, Austria, 1990]},
  abstract	= {Let $G$ be a connected graph with $n$ vertices and $m$
		  edges. We develop an algorithm that finds the prime factors
		  of $G$ with respect to Cartesian multiplication in $O(m
		  \log n)$ time and $O(m)$ space. This shows that factoring
		  $G$ is at most as costly as sorting its edges. The
		  algorithm gains its efficiency and practicality from using
		  only basic properties of product graphs and simple data
		  structures.}
}

@InProceedings{ahk-twpst-04,
  author	= {O. Aichholzer and C. Huemer and H. Krasser},
  title		= {Triangulations Without Pointed Spanning Trees - Extended
		  Abstract},
  booktitle	= {Proc. $20^{th}$ European Workshop on Computational
		  Geometry EWCG '04},
  year		= 2004,
  pages		= {221--224},
  address	= {Sevilla, Spain},
  abstract	= {Problem $50$ in the Open Problems Project~\cite{OPP} asks
		  whether any triangulation on a point set in the plane
		  contains a pointed spanning tree as a subgraph. We provide
		  a counterexample. As a consequence we show that there exist
		  triangulations which require a linear number of edge flips
		  to become Hamiltonian. }
}

@Article{ahn-lbntp-04,
  author	= {O. Aichholzer and F. Hurtado and M. Noy},
  title		= {A Lower Bound on the Number of Triangulations of Planar
		  Point Sets},
  year		= 2004,
  journal	= {Computational Geometry: Theory and Applications},
  volume	= {29},
  number	= {2},
  pages		= {135--145},
  htmlnote	= {See also the <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/counting/counting.html">Counting
		  Triangulations - Olympics</A>.},
  abstract	= {We show that the number of straight-edge triangulations
		  exhibited by any set of $n$ points in general position in
		  the plane is bounded from below by $\Omega(2.33^n)$.}
}

@InProceedings{ahn-ntepp-01,
  author	= {O. Aichholzer and F. Hurtado and M. Noy},
  title		= {On the Number of Triangulations Every Planar Point Set
		  Must Have},
  booktitle	= {Proc. $13th$ Annual Canadian Conference on Computational
		  Geometry CCCG 2001},
  pages		= {13--16},
  year		= 2001,
  address	= {Waterloo, Ontario, Canada},
  htmlnote	= {See also the <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/counting/counting.html">Counting
		  Triangulations - Olympics</A>.},
  abstract	= {We show that the number of straight line triangulations
		  exhibited by any set of $n$ points in general position in
		  the plane is bounded from below by
		  $\Omega((2+\varepsilon)^n)$ for some $\varepsilon > 0$. To
		  the knowledge of the authors this is the first non-trivial
		  lower bound.}
}

@InProceedings{ahst-dbcpt-03,
  author	= {O. Aichholzer and M. Hoffmann and B. Speckmann and C. D.
		  T\'oth},
  title		= {Degree Bounds for Constrained Pseudo-Triangulations},
  booktitle	= {Proc. $15th$ Annual Canadian Conference on Computational
		  Geometry CCCG 2003},
  pages		= {155--158},
  year		= 2003,
  address	= {Halifax, Nova Scotia, Canada},
  abstract	= {We introduce the concept of a constrained pointed
		  pseudo-triangulation $\mathcal{T}_G$ of a point set $S$
		  with respect to a pointed planar straight line graph $G =
		  (S, E)$. For the case that $G$ forms a simple polygon $P$
		  with vertex set $S$ we give tight bounds on the vertex
		  degree of $\mathcal{T}_G$. }
}

@InProceedings{ai-gravd-87,
  author	= {F. Aurenhammer and H. Imai},
  title		= {Geometric relations among {V}oronoi diagrams},
  booktitle	= {Proc. $4^{th}$ Ann. STACS, Lecture Notes in Computer
		  Science},
  pages		= {53--65},
  year		= 1987,
  volume	= 247,
  address	= {Passau, Germany},
  publisher	= {Springer Verlag},
  abstract	= {Two general classes of Voronoi diagrams are introduced
		  and, along with their modifications to higher order, are
		  shown to be geometrically related. This geometric
		  background, on the one hand, serves to analyze the size and
		  the combinatorial structure, and on the other hand, implies
		  general and efficient methods of construction, for various
		  important types of Voronoi diagrams considered in the
		  literature.}
}

@Article{ai-grvd-88,
  author	= {F. Aurenhammer and H. Imai},
  title		= {Geometric relations among {V}oronoi diagrams},
  journal	= {Geometriae Dedicata},
  year		= 1988,
  volume	= 27,
  pages		= {65--75},
  note		= {[IIG-Report-Series 228, TU Graz, Austria, 1986]},
  abstract	= {Two general classes of Voronoi diagrams are introduced
		  and, along with their modifications to higher order, are
		  shown to be geometrically related. This geometric
		  background, on the one hand, serves to analyze the size and
		  the combinatorial structure, and on the other hand, implies
		  general and efficient methods of construction, for various
		  important types of Voronoi diagrams considered in the
		  literature.}
}

@Article{ak-psclcf-06,
  author	= {F. Aurenhammer and H. Krasser},
  title		= {Pseudo-simplicial complexes from maximal locally convex
		  functions},
  journal	= {Discrete \& Computional Geometry},
  year		= 2006,
  pages		= {201--221},
  volume	= 35,
  abstract	= {We introduce and discuss pseudo-simplicial complexes
		  in~$R^d$ as generalizations of pseudo-triangulations
		  in~$R^2$. Our approach is based on the concept of maximal
		  locally convex functions on polytopal domains.}
}

@InProceedings{ak-psotd-01,
  author	= {O. Aichholzer and H. Krasser},
  title		= {The Point Set Order Type Data Base: A Collection of
		  Applications and Results},
  booktitle	= {Proc. $13th$ Annual Canadian Conference on Computational
		  Geometry CCCG 2001},
  pages		= {17--20},
  year		= 2001,
  address	= {Waterloo, Ontario, Canada},
  htmlnote	= {See also our <A
		  HREF="http://www.igi.TUgraz.at/oaich/triangulations/ordertypes.html">order
		  type homepage</A>.},
  abstract	= {Order types are a common tool to provide the combinatorial
		  structure of point sets in the plane. For many problems in
		  combinatorial and computational geometry only the order
		  type of the underlying point set has to be considered.
		  Recently a complete order type data base of $n$-point sets
		  has been developed for $n\leq 10$, which gives a way to
		  examine the combinatorial properties of all possible point
		  sets for fixed size $n$. Based on this result we present
		  applications and results for problems concerning
		  intersection properties, convexity, crossing-free straight
		  line graphs, and others, thus confirming or disproving
		  several conjectures on these topics. Besides providing
		  concrete results the aim of this work is to stimulate
		  further research by revealing structural relations of
		  extreme examples for $17$ geometrical and combinatorial
		  problems.}
}

@InProceedings{ak-ptc-05,
  author	= {F. Aurenhammer and H. Krasser},
  title		= {Pseudo-tetrahedral complexes},
  booktitle	= {Proc. $21^{st}$ European Workshop on Computational
		  Geometry EuroCG '05},
  pages		= {85--88},
  year		= 2005,
  address	= {Eindhoven, The Netherlands},
  abstract	= {Pseudo-triangulations are interesting and flexible
		  generalizations of triangulations that have found their
		  place in computational geometry. Unlike triangulations,
		  pseudo-triangulations eluded a meaningful generalization to
		  higher dimensions so far. In this paper, we define
		  pseudo-simplices and pseudo-simplicial complexes in d-space
		  in a way consistent to pseudo-triangulations in the plane.
		  Flip operations in pseudo-complexes are specified, as
		  combinations of flips in pseudo-triangulations, and of
		  bistellar flips in simplicial complexes. Our results are
		  based on the concept of maximal locally convex functions on
		  polyhedral domains, that allows us to unify several
		  well-known structures, namely pseudo-triangulations,
		  constrained Delaunay triangulations, and regular simplicial
		  complexes.}
}

@InCollection{ak-vd-00,
  author	= {F. Aurenhammer and R. Klein},
  title		= {{V}oronoi diagrams},
  booktitle	= {Handbook of Computational Geometry, Chapter V},
  pages		= {201--290},
  publisher	= {Elsevier Science Publishing},
  year		= 2000,
  editor	= {J. Sack and G. Urrutia},
  note		= {[SFB Report F003-092, TU Graz, Austria, 1996]},
  abstract	= {The topic of this chapter -- Voronoi diagrams -- differs
		  from other areas of computational geometry, in that its
		  origin dates back to the 17th century. In his book on the
		  principles of philosophy, R.~Descartes' illustrations show
		  a decomposition of space into convex regions, whose
		  underlying idea seems to be that of a Voronoi diagram. This
		  concept has independently emerged, and proven useful, in
		  various fields of science. Different names particular to
		  the respective field have been used, such as {\em medial
		  axis transform\/} in biology and physiology, {\em
		  Wigner-Seitz zones\/} in chemistry and physics, {\em
		  domains of action\/} in crystallography, and {\em Thiessen
		  polygons\/} in metereology and geography. The
		  mathematicians Dirichlet and Voronoi were the first to
		  formally introduce this concept. The resulting structure
		  has been called {\em Dirichlet tessellation\/} or {\em
		  Voronoi diagram\/}, which has become its standard name
		  today. Voronoi was the first to consider the {\em dual\/}
		  of this structure, where any two point sites are connected
		  whose regions have a boundary in common. Later, Delaunay
		  obtained the same by defining that two point sites are
		  connected if and only if they lie on a circle whose
		  interior contains no other point site. After him, the dual
		  of the Voronoi diagram has been denoted {\em Delaunay
		  tessellation\/} or {\em Delaunay triangulation\/}. Besides
		  its applications in other fields of science, the Voronoi
		  diagram and its dual can be used for solving numerous, and
		  surprisingly different, geometric problems. Moreover, these
		  structures are very appealing, and a lot of research has
		  been devoted to their study (about one out of 16 papers in
		  computational geometry), ever since Shamos and Hoey
		  introduced them to the field. Within one chapter, we cannot
		  review all known results and applications. Instead, we are
		  trying to highlight the intrinsic potential of Voronoi
		  diagrams, that lies in its structural properties, in the
		  existence of efficient algorithms for its construction, and
		  in its adaptability.}
}

@Article{akkox-autmp-00a,
  author	= {F. Aurenhammer and N. Katoh and H. Kojima and M. Ohsaki
		  and Y.-F. Xu},
  title		= {Approximating uniform triangular meshes in polygons},
  journal	= {Theoretical Computer Science},
  year		= 2002,
  volume	= 289,
  pages		= {879--895},
  note		= {Special Issue. [SFB Report F003-159, TU Graz, Austria,
		  1999]},
  abstract	= {We consider the problem of triangulating a convex polygon
		  using $n$ Steiner points under the following optimality
		  criteria: (1) minimizing the overall edge length ratio, (2)
		  minimizing the maximum edge length, and (3) minimizing the
		  maximum triangle perimeter. We establish a relation of
		  these problems to a certain extreme packing problem. Based
		  on this relationship, we develop a heuristic producing
		  constant approximations for all the optimality criteria
		  above (provided $n$ is chosen sufficiently large). That is,
		  the produced triangular mesh is {\em uniform} in these
		  respects. The method is easy to implement and runs in
		  $O(n^2 \log n)$ time and $O(n)$ space. The observed runtime
		  is much less. Moreover, for criterion (1) the method works
		  -- within the same complexity and approximation bounds --
		  for {\em arbitrary} polygons with possible holes, and for
		  criteria (2) and (3) it does so for a large subclass.}
}

@InProceedings{akkox-autmp-00b,
  author	= {F. Aurenhammer and N. Katoh and H. Kojima and M. Ohsaki
		  and Y.-F. Xu},
  title		= {Approximating uniform triangular meshes in polygons},
  booktitle	= {Proc. $6^{th}$ Ann. Intl. Computing and Combinatorics
		  Conference, Lecture Notes in Computer Science},
  pages		= {23--33},
  year		= 2000,
  volume	= {1558},
  address	= {Sydney, Australia},
  publisher	= {Springer Verlag},
  abstract	= {We consider the problem of triangulating a convex polygon
		  using $n$ Steiner points under the following optimality
		  criteria: (1) minimizing the overall edge length ratio, (2)
		  minimizing the maximum edge length, and (3) minimizing the
		  maximum triangle perimeter. We establish a relation of
		  these problems to a certain extreme packing problem. Based
		  on this relationship, we develop a heuristic producing
		  constant approximations for all the optimality criteria
		  above (provided $n$ is chosen sufficiently large). That is,
		  the produced triangular mesh is {\em uniform} in these
		  respects. The method is easy to implement and runs in
		  $O(n^2 \log n)$ time and $O(n)$ space. The observed runtime
		  is much less. Moreover, for criterion (1) the method works
		  -- within the same complexity and approximation bounds --
		  for {\em arbitrary} polygons with possible holes, and for
		  criteria (2) and (3) it does so for a large subclass.}
}

@Article{aksb-isve-2002,
  author	= {K. Andrews and W. Kienreich and V. Sabol and J. Becker and
		  G. Droschl and F. Kappe and M. Granitzer and P. Auer and K.
		  Tochtermann},
  title		= {The {InfoSky} visual explorer: exploiting hierarchical
		  structure and document similarities},
  journal	= {Information Visualization},
  year		= {2002},
  volume	= {1},
  pages		= {166--181}
}

@InProceedings{aoss-nptcp-03,
  author	= {O. Aichholzer and D. Orden and F. Santos and B.
		  Speckmann},
  title		= {On the Number of Pseudo-Triangulations of Certain Point
		  Sets},
  booktitle	= {Proc. $15th$ Annual Canadian Conference on Computational
		  Geometry CCCG 2003},
  pages		= {141--144},
  year		= 2003,
  address	= {Halifax, Nova Scotia, Canada},
  abstract	= {We compute the exact number of pseudo-triangulations for
		  two prominent point sets, namely the so-called double
		  circle and the double chain. We also derive a new
		  asymptotic lower bound for the maximal number of
		  pseudo-triangulations which lies significantly above the
		  related bound for triangulations. }
}

@InProceedings{aoss-nptcp-04,
  author	= {O. Aichholzer and D. Orden and F. Santos and B.
		  Speckmann},
  title		= {On the Number of Pseudo-Triangulations of Certain Point
		  Sets},
  booktitle	= {Proc. $20^{th}$ European Workshop on Computational
		  Geometry EWCG '04},
  year		= 2004,
  pages		= {119--122},
  address	= {Sevilla, Spain},
  abstract	= {We compute the exact number of pseudo-triangulations for
		  two prominent point sets, namely the so-called double
		  circle and the double chain. We also derive a new
		  asymptotic lower bound for the maximal number of
		  pseudo-triangulations which lies significantly above the
		  related bound for triangulations. }
}

@InProceedings{appw-vdos-07,
  author	= {F. Aurenhammer and M. Peternell and H. Pottmann and J.
		  Wallner},
  title		= {Voronoi Diagrams for Oriented Spheres},
  booktitle	= {Proc. 4th Int. Symp. on Voronoi Diagrams in Science and
		  Engineering, ISVD'07},
  year		= 2007,
  pages		= {33--37},
  address	= {Pontypridd, UK},
  abstract	= {We consider finite sets of oriented spheres in (k-1)-space
		  and, by interpreting such spheres as points in k-space,
		  study the Voronoi diagrams they induce for several variants
		  of distance between spheres. We give bounds on the
		  combinatorial complexity of these diagrams in the plane and
		  in 3-space, and derive properties useful for constructing
		  them. Our results are motivated by applications to special
		  relativity theory.}
}

@InProceedings{ar-qdbsb-04,
  author	= {O. Aichholzer and K. Reinhardt},
  title		= {A quadratic distance bound on sliding between
		  crossing-free spanning trees - Extended Abstract},
  booktitle	= {Proc. $20^{th}$ European Workshop on Computational
		  Geometry EWCG '04},
  year		= 2004,
  pages		= {13--16},
  address	= {Sevilla, Spain},
  abstract	= {Let $S$ be a set of $n$ points in the plane and let
		  ${\mathcal T}_S$ be the set of all crossing-free spanning
		  trees of $S$. We show that any two trees in ${\mathcal
		  T}_S$ can be transformed into each other by $O(n^2)$ local
		  and constant-size edge slide operations. No polynomial
		  upper bound for this task has been known, but in~\cite{AAH}
		  a bound of $O(n^2 \log n)$ operations was conjectured.}
}

@InProceedings{arss-zppt-03,
  author	= {O. Aichholzer and G. Rote and B. Speckmann and I.
		  Streinu},
  title		= {The Zigzag Path of a Pseudo-Triangulation},
  booktitle	= {Lecture Notes in Computer Science 2748, Proc. 8th
		  International Workshop on Algorithms and Data Structures
		  (WADS)},
  volume	= {2748},
  pages		= {377--389},
  year		= 2003,
  abstract	= {We define the zigzag path of a pseudo-triangulation, a
		  concept generalizing the path of a triangulation of a point
		  set. The pseudo-tri\-an\-gu\-la\-tion zigzag path allows us
		  to use divide-and-conquer type of approaches for suitable
		  (i.e., decomposable) problems on
		  pseudo-tri\-an\-gu\-la\-tions. For this we provide an
		  algorithm that enumerates all pseudo-triangulation zigzag
		  paths (of all pseudo-triangulations of a given point set
		  with respect to a given line) in $O(n^2)$ time per path and
		  $O(n^2)$ space, where $n$ is the number of points. We
		  illustrate applications of our scheme which include a novel
		  algorithm to count the number of pseudo-triangulations of a
		  point set. }
}

@InProceedings{as-fvd-90,
  author	= {F. Aurenhammer and G. Stoeckl},
  title		= {Fenster - {V}oronoi {D}iagramme ({A}bstract)},
  booktitle	= {Tagungsband DMV Jubilaeumstagung},
  pages		= 52,
  year		= 1990,
  address	= {Bremen, Germany},
  abstract	= {In the peeper's Voronoi diagram for $n$ sites, any point
		  in the plane belongs to the region of the closest site
		  visible from it. Visibility is constrained to a segment on
		  a line avoiding the convex hull of the sites. We show that
		  the peeper's Voronoi diagram attains a size of
		  $\Theta(n^{2})$ in the worst case, and that it can be
		  computed in $O(n^{2})$ time and space.}
}

@Article{as-pvd-91,
  author	= {F. Aurenhammer and G. Stoeckl},
  title		= {On the peeper's {V}oronoi diagram},
  journal	= {SIGACT News},
  year		= 1991,
  volume	= 22,
  number	= 4,
  pages		= {50--59},
  note		= {[IIG-Report-Series 264, TU Graz, Austria, 1988]},
  abstract	= {In the peeper's Voronoi diagram for $n$ sites, any point
		  in the plane belongs to the region of the closest site
		  visible from it. Visibility is constrained to a segment on
		  a line avoiding the convex hull of the sites. We show that
		  the peeper's Voronoi diagram attains a size of
		  $\Theta(n^{2})$ in the worst case, and that it can be
		  computed in $O(n^{2})$ time and space.}
}

@InProceedings{as-solri-91,
  author	= {F. Aurenhammer and O. Schwarzkopf},
  title		= {A simple on-line randomized incremental algorithm for
		  computing higher order {V}oronoi diagrams},
  booktitle	= {Proc. $7^{th}$ Ann. ACM Symp. Computational Geometry},
  pages		= {142--151},
  year		= 1991,
  address	= {North Conway, U.S.A.},
  abstract	= {We present a simple algorithm for maintaining order-$k$
		  Voronoi diagrams in the plane. By using a duality transform
		  that is of interest in its own right, we show that the
		  insertion or deletion of a site involves little more than
		  the construction of a single convex hull in three-space. In
		  particular, the order-$k$ Voronoi diagram for $n$ sites can
		  be computed in time $O(nk^{2} \log n + nk \log^{3} n)$ and
		  optimal space $O(k(n-k))$ by an on-line randomized
		  incremental algorithm. The time bound can be improved by a
		  logarithmic factor without losing much simplicity. For $k
		  \geq \log^{2} n$, this is optimal for a randomized
		  incremental construction; we show that the expected number
		  of structural changes during the construction is $\Theta
		  (nk^{2})$. Finally, by going back to primal space, we
		  obtain a dynamic data structure that supports $k$-nearest
		  neighbor queries, insertions, and deletions in a planar set
		  of sites. The structure promises easy implementation,
		  exhibits a satisfactory expected performance, and occupies
		  no more storage than the current order-$k$ Voronoi diagram.}
}

@Article{as-solri-92,
  author	= {F. Aurenhammer and O. Schwarzkopf},
  title		= {A simple on-line randomized incremental algorithm for
		  computing higher order {V}oronoi diagrams},
  journal	= {Int'l Journal of Computational Geometry \& Applications},
  year		= 1992,
  volume	= 2,
  pages		= {363--381},
  note		= {Special Issue. [Report B 91-02, FU Berlin, Germany,
		  1991]},
  abstract	= {We present a simple algorithm for maintaining order-$k$
		  Voronoi diagrams in the plane. By using a duality transform
		  that is of interest in its own right, we show that the
		  insertion or deletion of a site involves little more than
		  the construction of a single convex hull in three-space. In
		  particular, the order-$k$ Voronoi diagram for $n$ sites can
		  be computed in time $O(nk^{2} \log n + nk \log^{3} n)$ and
		  optimal space $O(k(n-k))$ by an on-line randomized
		  incremental algorithm. The time bound can be improved by a
		  logarithmic factor without losing much simplicity. For $k
		  \geq \log^{2} n$, this is optimal for a randomized
		  incremental construction; we show that the expected number
		  of structural changes during the construction is $\Theta
		  (nk^{2})$. Finally, by going back to primal space, we
		  obtain a dynamic data structure that supports $k$-nearest
		  neighbor queries, insertions, and deletions in a planar set
		  of sites. The structure promises easy implementation,
		  exhibits a satisfactory expected performance, and occupies
		  no more storage than the current order-$k$ Voronoi diagram.}
}

@InProceedings{as-sslro-92a,
  author	= {F. Aurenhammer and G. Stoeckl},
  title		= {Searching for segments with largest relative overlap},
  booktitle	= {Proc. $15^{th}$ IFIP Conf. System Modelling and
		  Optimization, Lecture Notes in Control and Information
		  Sciences},
  pages		= {77--84},
  year		= 1992,
  volume	= 180,
  address	= {Zuerich, Switzerland},
  publisher	= {Springer Verlag},
  abstract	= {Let $S$ be a set of $n$ possibly intersecting line
		  segments on the $x$-axis. A data structure is developed
		  that -- for an arbitrary query segment $\sigma$ -- reports
		  in $O(\log)$ time a segment in $S$ which yields the largest
		  relative overlap with $\sigma$. The structure needs $O(n
		  \log)$ time and $O(n)$ space for construction. These bounds
		  are asymptotically optimal.}
}

@Article{as-sslro-92b,
  author	= {F. Aurenhammer and G. Stoeckl},
  title		= {Searching for segments with largest relative overlap},
  journal	= {Information Processing Letters},
  year		= 1992,
  volume	= 41,
  pages		= {103--108},
  note		= {[Report B 91-10, FU Berlin, Germany, 1991]},
  abstract	= {Let $S$ be a set of $n$ possibly intersecting line
		  segments on the $x$-axis. A data structure is developed
		  that -- for an arbitrary query segment $\sigma$ -- reports
		  in $O(\log)$ time a segment in $S$ which yields the largest
		  relative overlap with $\sigma$. The structure needs $O(n
		  \log)$ time and $O(n)$ space for construction. These bounds
		  are asymptotically optimal.}
}

@InProceedings{ass-ppt-02,
  author	= {O. Aichholzer and B. Speckmann and I. Streinu},
  title		= {The Path of a Pseudo-Triangulation},
  booktitle	= {Abstracts of the DIMACS Workshop on Computational Geometry
		  2002},
  pages		= {2},
  year		= 2002,
  address	= {Piscataway (NJ), USA},
  abstract	= {We define the path of a pseudo-triangulation, a data
		  structure generalizing the path of a triangulation of a
		  point set. This structure allows us to use
		  divide-and-conquer type of approaches for suitable (i.e.
		  decomposable) problems on pseudo-triangulations. We
		  illustrate this method by presenting a novel algorithm that
		  counts the number of pseudo-triangulations of a point
		  set.}
}

@InProceedings{asw-popfp-91,
  author	= {F. Aurenhammer and G. Stoeckl and E. Welzl},
  title		= {The post-office problem for fuzzy point sets},
  booktitle	= {Proc. $7^{th}$ Workshop on Computational Geometry CG '91,
		  Lecture Notes in Computer Science},
  pages		= {1--11},
  year		= 1991,
  volume	= 553,
  address	= {Bern, Switzerland},
  publisher	= {Springer Verlag},
  note		= {[Report B 91-07, FU Berlin, Germany, 1991]},
  abstract	= {The post-office problem for $n$ point sites in the plane
		  (determine which site is closest to a later specified query
		  point) is generalized to the situation when the residence
		  of each site is uncertain and it is described via uniform
		  distribution within a disk. Two probabilistic concepts of
		  neighborhood, -- expected closest site and probably closest
		  site -- are discussed and the resulting Voronoi diagrams
		  are investigated from a combinatorial and computational
		  point of view.}
}

@InCollection{ax-ot-07,
  author	= {F. Aurenhammer and Y.-F.Xu},
  title		= {Optimal triangulations},
  booktitle	= {Encyclopedia of Optimization, Second Edition},
  publisher	= {Springer},
  editor	= {C.A.Floudas, P.M.Pardalos},
  year		= 2008,
  pages		= {2757--2764},
  abstract	= {A {\em triangulation} of a given set $S$ of $n$ points in
		  the plane is a maximal set of non-crossing line segments
		  spanned by $S$. The problem of automatically generating
		  {\em optimal triangulations} for a point set $S$ has been a
		  subject of research since decades. As the number of
		  different triangulations of $S$ is an exponential function
		  of $n$, enumerating all possible triangulations and
		  selecting an optimal one (exhaustive search) is too
		  time-consuming even for small $n$. In fact, constructing
		  optimal triangulations in polynomial time is a challenging
		  task. Results on optimizing {\em combinatorial} properties
		  of triangulations, such as their degree or connectivity,
		  are rare. Most optimization criteria for which efficient
		  algorithms are known concern {\em geometric} properties of
		  the edges and triangles. The present survey article is
		  devoted to this topic.}
}

@InCollection{ax-ot-99,
  author	= {F. Aurenhammer and Y.-F.Xu},
  title		= {Optimal triangulations},
  booktitle	= {Encyclopedia of Optimization},
  publisher	= {Kluwer Academic Publishing},
  editor	= {C.A.Floudas, P.M.Pardalos},
  year		= 2000,
  pages		= {160--166},
  volume	= 4,
  note		= {[SFB Report F003-099, Tu Graz, Austria, 1998]},
  abstract	= {A {\em triangulation} of a given set $S$ of $n$ points in
		  the Euclidean plane is a maximal set of non-crossing line
		  segments spanned by $S$. The problem of automatically
		  generating {\em optimal triangulations} for a point set $S$
		  has been a subject of research since decades. As the number
		  of different triangulations of $S$ is an exponential
		  function of $n$, enumerating all possible triangulations
		  and selecting an optimal one (exhaustive search) is too
		  time-consuming even for small $n$. In fact, constructing
		  optimal triangulations in polynomial time is a challenging
		  task. Results on optimizing {\em combinatorial} properties
		  of triangulations, such as their degree or connectivity,
		  are rare. Most optimization criteria for which efficient
		  algorithms are known concern {\em geometric} properties of
		  the edges and triangles. The present survey article is
		  devoted to this topic.}
}

@InProceedings{cikm02_collabfiltering,
  author	= {Yu, Kai and Xu, Xiaowei and Schwaighofer, Anton and Tresp,
		  Volker and Kriegel, Hans-Peter},
  title		= {Removing Redundancy and Inconsistency in Memory-based
		  Collaborative Filtering},
  booktitle	= {Proceedings of the 11th International Conference on
		  Information and Knowledge Management CIKM02},
  year		= 2002,
  publisher	= {ACM},
  pages		= {52--59},
  abstract	= {The application range of memory-based collaborative
		  filtering (CF) is limited due to CF's high memory
		  consumption and long runtime. The approach presented in
		  this paper removes redundant and inconsistent instances
		  (users) from the data. Our work shows that a satisfactory
		  accuracy can be achieved by using only a small portion of
		  the original data set, thereby alleviating the storage and
		  runtime cost of the CF algorithm. In our approach, we
		  consider instance selection as the problem of selecting
		  informative data that increase the \textit{a posteriori}
		  probability of the optimal model. We evaluate the empirical
		  performance of our approach on two realworld data sets and
		  attain very promising results. Data size and prediction
		  time are significantly reduced, while the prediction
		  accuracy is on a par with results achieved by using the
		  complete database.},
  aschwaig_label= 9
}

@InProceedings{dap-ssbi-10,
  author	= {M. Demuth and F. Aurenhammer and A. Pinz},
  title		= {Straight skeletons for binary shapes},
  booktitle	= {$3^{rd}$ Workshop on non-rigid shape analysis and
		  deformable image alignment (NORDIA'10)},
  year		= 2010,
  address	= {San Francisco, USA},
  abstract	= {This paper reviews the concept of straight skeletons,
		  which is well known in computational geometry, and applies
		  it to binary shapes that are used in vision-based shape and
		  object recognition. We devise a novel algorithm for
		  computing discrete straight skeletons from binary input
		  images, which is based on a polygonal approximation of the
		  input shape and a hybrid method that combines continuous
		  and discrete geometry. In our experiments, we analyze the
		  potential of straight skeletons in shape recognition, by
		  comparing their performance with medial-axis based shock
		  graphs on the Kimia shape databases. Our discrete straight
		  skeleton algorithm is not only outperforming typical
		  skeleton algorithms in terms of computational complexity,
		  it also delivers surprisingly good results in its
		  straightforward application to shape recognition.}
}

@InProceedings{haa-nrmwt-97,
  author	= {R. Hainz and O. Aichholzer and F. Aurenhammer},
  title		= {New results on minimum-weight triangulations and the {LMT}
		  skeleton},
  booktitle	= {Proc. $13^{th}$ European Workshop on Computational
		  Geometry CG '97},
  pages		= {4--6},
  year		= 1997,
  address	= {Wuerzburg, Germany},
  abstract	= {Let $P$ be a simple polygon in the plane and let $MWT(P)$
		  be a minimum-weight triangulation of $P$. We prove that the
		  $\beta$-skeleton of $P$ is a subset of $MWT(P)$ for all
		  values $\beta$ > $\sqrt{\frac{4}{3}}$ provided $P$ is
		  convex or near-convex. This settles the question of
		  tightness of this bound for a special case and gives
		  evidence for its validity in the general point set case.\\
		  We further disprove the conjecture that the so-called
		  $LMT$-skeleton coincides with the intersection of all
		  locally minimal triangulations, $LMT(P)$, even for convex
		  polygons $P$. We introduce an improved $LMT$-skeleton
		  algorithm which, for simple polygons $P$, exactly computes
		  $LMT(P)$, and thus a larger subgraph of $MWT(P)$. The
		  algorithm achieves the same in the general point set case
		  provided the connectedness of the improved $LMT$-skeleton,
		  which is given in allmost all practical instances.}
}

@InProceedings{icann01_bcsvm,
  author	= {Schwaighofer, A. and Tresp, V.},
  title		= {The {B}ayesian Committee Support Vector Machine},
  booktitle	= {Artificial Neural Networks -- ICANN 2001},
  editor	= {Dorffner, G. and Bischof, H. and Hornik, K.},
  series	= lncs # {2130},
  pages		= {411--417},
  year		= 2001,
  publisher	= {Springer Verlag},
  abstract	= {Empirical evidence indicates that the training time for
		  the support vector machine (SVM) scales to the square of
		  the number of training data points. In this paper, we
		  introduce the Bayesian committee support vector machine
		  (BC-SVM) and achieve an algorithm for training the SVM
		  which scales linearly in the number of training data
		  points. We verify the good performance of the BC-SVM using
		  several data sets.},
  aschwaig_label= 2
}

@InProceedings{icann01_kernels,
  author	= {Tresp, V. and Schwaighofer, A.},
  title		= {Scalable Kernel Systems},
  booktitle	= {Artificial Neural Networks -- ICANN 2001},
  year		= 2001,
  editor	= {Dorffner, G. and Bischof, H. and Hornik, K.},
  series	= lncs # {2130},
  pages		= {285--291},
  publisher	= {Springer Verlag},
  abstract	= {Kernel-based systems are currently very popular approaches
		  to supervised learning. Unfortunately, the computational
		  load for training kernel-based systems increases
		  drastically with the number of training data points.
		  Recently, a number of approximate methods for scaling
		  kernel-based systems to large data sets have been
		  introduced. In this paper we investigate the relationship
		  between three of those approaches and compare their
		  performances experimentally.},
  aschwaig_label= 3
}

@MastersThesis{k-ktep-99,
  author	= {H. Krasser},
  title		= {Kompatible Triangulierungen ebener Punktmengen},
  note		= {(in German)},
  school	= {Institute for Theoretical Computer Science, Graz
		  University of Technology, Austria},
  year		= 1999,
  month		= {June},
  keywords	= {computational geometry, triangulations, compatible,
		  isomorphic},
  abstract	= {Triangulations are very important in computational
		  geometry, since a lot of algorithms make use of this data
		  structure. The subject of this thesis is the special
		  problem of compatible triangulations. The original
		  motivation to investigate this problem comes from
		  cartography. Two point sets in the plane are called
		  compatible if isomorphic triangulations exist. First the
		  structure of the convex hulls of compatible triangulations
		  is analyzed in the case of edge-compatibility. Then it is
		  shown that triangle-compatibility simplifies the situation.
		  A connection to lambda-matrices is also given. If two point
		  sets have the same lambda-matrices then every triangulation
		  of one point set admits a compatible triangulation of the
		  other point set. At last the main conjecture on compatible
		  triangulations, namely that every two point sets of same
		  cardinality in general position that have an identical
		  number of points on the convex hulls admit compatible
		  triangulations, is proved for some special cases. }
}

@PhDThesis{k-otpsp-03,
  author	= {H. Krasser},
  title		= {Order Types of Point Sets in the Plane},
  school	= {Institute for Theoretical Computer Science, Graz
		  University of Technology, Austria},
  year		= 2003,
  month		= {October},
  keywords	= {computational geometry, combinatorial geometry, order
		  types, point sets, pseudoline arrangements, rectilinear
		  crossing number, triangulation, pseudo-triangulation},
  abstract	= {Order types are a means to characterize the combinatorial
		  properties of a finite point set in the plane. In
		  particular, the crossing properties of all straight-line
		  segments spanned by a point configuration are reflected by
		  its order type. We establish a complete and reliable data
		  base for all different order types of size up to $11$. To
		  the best of our knowledge, such a project has not been
		  carried out before, not even for point sets of smaller
		  size. We discuss several applications of this data base and
		  related techniques to prominent problems in computational
		  and combinatorial geometry. These include problems on
		  crossing-free graphs like triangulations or simple
		  polygonalizations, but also general crossing problems like
		  the rectilinear crossing number.}
}

@InProceedings{nips02_approxgp,
  author	= {Schwaighofer, Anton and Tresp, Volker},
  title		= {Transductive and Inductive Methods for Approximate
		  {G}aussian Process Regression},
  booktitle	= nips # { 15},
  year		= 2003,
  editor	= {Becker, Suzanna and Thrun, Sebastian and Obermayer,
		  Klaus},
  publisher	= {MIT Press},
  abstract	= {Gaussian process regression allows a simple analytical
		  treatment of exact Bayesian inference and has been found to
		  provide good performance, yet scales badly with the number
		  of training data. In this paper we compare several
		  approaches towards scaling Gaussian processes regression to
		  large data sets: the subset of representers method, the
		  reduced rank approximation, online Gaussian processes, and
		  the Bayesian committee machine. Furthermore we provide
		  theoretical insight into some of our experimental results.
		  We found that subset of representers methods can give good
		  and particularly fast predictions for data sets with high
		  and medium noise levels. On complex low noise data sets,
		  the Bayesian committee machine achieves significantly
		  better accuracy, yet at a higher computational cost.},
  aschwaig_label= 6
}

@InProceedings{nips02_rheuma,
  author	= {Schwaighofer, Anton and Tresp, Volker and Mayer, Peter and
		  Scheel, Alexander K. and Mueller, Gerhard A. },
  title		= {The {RA} Scanner: Prediction of Rheumatoid Joint
		  Inflammation Based on Laser Imaging},
  booktitle	= nips # { 15},
  year		= 2003,
  editor	= {Becker, Suzanna and Thrun, Sebastian and Obermayer,
		  Klaus},
  publisher	= {MIT Press},
  abstract	= {We describe the RA scanner, a novel system for the
		  examination of patients suffering from rheumatoid
		  arthritis. The RA scanner is based on a novel laser-based
		  imaging technique which is sensitive to the optical
		  characteristics of finger joint tissue. Based on the laser
		  images, finger joints are classified according to whether
		  the inflammatory status has improved or worsened. To
		  perform the classification task, various linear and
		  kernel-based systems were implemented and their
		  performances were compared. Special emphasis was put on
		  measures to reliably perform parameter tuning and
		  evaluation, since only a very small data set was available.
		  Based on the results presented in this paper, it was
		  concluded that the RA scanner permits a reliable
		  classification of pathological finger joints, thus paving
		  the way for a further development from prototype to product
		  stage.},
  aschwaig_label= 7
}

@InProceedings{nips03_gplocation,
  author	= {Schwaighofer, Anton and Grigora{\c{s}}, Marian and Tresp,
		  Volker and Hoffmann, Clemens},
  title		= {{GPPS}: A {G}aussian Process Positioning System for
		  Cellular Networks},
  booktitle	= nips # { 16},
  editor	= {Thrun, Sebastian and Saul, Lawrence and Schoelkopf,
		  Bernhard},
  year		= 2004,
  abstract	= {In this article, we present a novel approach to solving
		  the localization problem in cellular networks. The goal is
		  to estimate a mobile user's position, based on measurements
		  of the signal strengths received from network base
		  stations. Our solution works by building Gaussian process
		  models for the distribution of signal strengths, as
		  obtained in a series of calibration measurements. In the
		  localization stage, the user's position can be estimated by
		  maximizing the likelihood of received signal strengths with
		  respect to the position. We investigate the accuracy of the
		  proposed approach on data obtained within a large indoor
		  cellular network.},
  aschwaig_label= 13
}

@Article{omics_mining,
  author	= {Dejori, Mathaeus and Schwaighofer, Anton and Tresp, Volker
		  and Stetter, Martin},
  title		= {Mining Functional Modules in Genetic Networks with
		  Decomposable Graphical Models},
  journal	= {OMICS A Journal of Integrative Biology},
  year		= 2004,
  note		= {Accepted for publication},
  aschwaig_label= 14
}

@Article{tbe_rheuma,
  author	= {Schwaighofer, A. and Tresp, V. and Mayer, P. and Scheel,
		  A. and Reuss-Borst, M. and Krause, A. and Mesecke-von
		  Rheinbaben, I. and Rost, H.},
  title		= {Prediction of Rheumatoid Joint Inflammation Based on Laser
		  Imaging Using Linear and Kernel-Based Classifiers},
  journal	= {IEEE Transactions on Biomedical Engineering},
  year		= 2002,
  note		= {Accepted for publication},
  abstract	= {We describe a novel system for the examination of patients
		  suffering from rheumatoid arthritis. Basis of this system
		  is a laser imaging technique which is sensitive to the
		  optical characteristics of finger joint tissue. From the
		  laser images acquired at baseline and followup, finger
		  joints can automatically be classified according to whether
		  the inflammatory status has improved or worsened. To
		  perform the classification task, various linear and
		  kernel-based systems were implemented and their
		  performances were compared. From the results presented in
		  this paper, we concluded that the laser-based imaging
		  permits a reliable classification of pathological finger
		  joints, making it a sensitive method for detecting
		  arthritic changes.},
  aschwaig_label= 4
}

@TechReport{techrep_nystrom,
  author	= {Williams, Christopher K.I. and Rasmussen, Carl Edward and
		  Schwaighofer, Anton and Tresp, Volker},
  title		= {Observations on the {N}ystroem Method for {G}aussian
		  Process Prediction},
  institution	= {Available from the authors' web pages},
  month		= may,
  year		= 2002,
  aschwaig_label= 8
}

@Article{telematik_fingerprint,
  author	= {Schwaighofer, Anton},
  title		= {Sorting it out: Machine Learning and Fingerprints},
  journal	= {Telematik},
  year		= 2002,
  volume	= 8,
  number	= 1,
  pages		= {18--20},
  abstract	= {Machine learning concepts can find applications in many
		  domains. We describe here one such application to the
		  problem of fingerprint verification. In biometric
		  verification using fingerprints, one often has to handle
		  large archives of fingerprint images. When verifying an
		  individual fingerprint, it is important to have a fast
		  method at hand that selects suitable candidate images out
		  of a large archive. We present an approach to sort a
		  fingerprint archive according to similarity with a given
		  fingerprint, inspired by an opto-electronic device called
		  the wedge-ring-detector.},
  aschwaig_label= 5
}

@Article{tkde_collabfiltering,
  author	= {Yu, Kai and Schwaighofer, Anton and Tresp, Volker and Xu,
		  Xiaowei and Kriegel, Hans-Peter},
  title		= {Probabilistic Memory Based Collaborative Filtering:
		  Learning Individual and Social Preferences},
  journal	= {IEEE Transactions on Knowledge and Data Engineering
		  (Special issue on mining and searching the web)},
  year		= 2004,
  volume	= 16,
  number	= 1,
  pages		= {56--69},
  abstract	= {Memory-based collaborative filtering (CF) has been
		  extensively studied in the literature and has proven to be
		  successful in various types of personalized recommender
		  systems. In this paper we develop a probabilistic framework
		  for memory-based CF (PMCF). While this framework has clear
		  links with classical memory-based CF, it allows us to find
		  principled solutions to known problems of CF-based
		  recommender systems. In particular, we show that a
		  probabilistic active learning method can be used to
		  actively query the user, thereby solving the ``new user
		  problem'' . Furthermore, the probabilistic framework allows
		  us to reduce the computational cost of memory-based CF by
		  working on a carefully selected subset of user profiles,
		  while retaining high accuracy. We report experimental
		  results based on two real world data sets, which
		  demonstrate that our proposed PMCF framework allows an
		  accurate and efficient prediction of user preferences.},
  aschwaig_label= 10
}

@InProceedings{uai03_collabensemble,
  author	= {Yu, Kai and Schwaighofer, Anton and Tresp, Volker and Ma,
		  Wei-Ying and Zhang, HongJiang},
  title		= {Collaborative Ensemble Learning: Combining Collaborative
		  and Content-Based Information Filtering via Hierarchical
		  {B}ayes},
  booktitle	= {Uncertainty in Artificial Intelligence: Proceedings of the
		  19th Conference (UAI-2003)},
  editor	= {Meek, Christopher and Kj{\ae}rulff, Uffe},
  publisher	= {Morgan Kaufmann},
  year		= 2003,
  pages		= {616--623},
  abstract	= {Collaborative filtering (CF) and content-based filtering
		  (CBF) have widely been used in information filtering
		  applications, both approaches having their individual
		  strengths and weaknesses. This paper proposes a novel
		  probabilistic framework to unify CF and CBF, named
		  collaborative ensemble learning. Based on content based
		  probabilistic models for each user's preferences (the CBF
		  idea), it combines a society of users' preferences to
		  predict an active user's preferences (the CF idea). While
		  retaining an intuitive explanation, the combination scheme
		  can be interpreted as a hierarchical Bayesian approach in
		  which a common prior distribution is learned from related
		  experiments. It does not require a global training stage
		  and thus can incrementally incorporate new data. We report
		  results based on two data sets, the Reuters-21578 text data
		  set and a data base of user opionions on art images. For
		  both data sets, collaborative ensemble achieved excellent
		  performance in terms of recommendation accuracy. In
		  addition to recommendation engines, collaborative ensemble
		  learning is applicable to problems typically solved via
		  classical hierarchical Bayes, like multisensor fusion and
		  multitask learning.},
  aschwaig_label= 11
}
