%---------------------------------------------------------------------------
@techreport{ECS-CSG-1-94,
author      = {{The Computer Systems Group}},
title       = {{Computer Systems Research at Edinburgh}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-1-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-1-94.ps.gz},
abstract    = {
We introduce a new series of technical reports produced by the
Computer Systems Group within the Computer Science Department at the
University of Edinburgh. We consider the ethos of the group and its
relationship with the wider subject. Some highlights of past "systems"
research in the department are reviewed and the current interests of
group members are surveyed.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-2-94,
author      = {Martin, P.},
title       = {{The Formal Specification in Z of Task Migration on
                the Testbeed Multicomputer}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-2-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-2-94.ps.gz},
abstract    = {
This report introduces a message-passing multicomputer called the
`Testbed' and describes its facilities for transparent task migration
between processors. A specification in the formal language Z is given
for the key operating system components which support migration.
Finally, rigorous arguments are presented which verify that task
migration is correct and safe. The contributions of this report are an
extended specification showing the application of Z to a real system
and a detailed demonstration of the verification of safety and
correctness properties. Conclusions are drawn about the utility of
formal methods in general.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-3-94,
author      = {Martin, P.},
title       = {{The Performance Profiling of a Load Balancing 
                Multicomputer}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-3-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-3-94.ps.gz},
abstract    = {
This report introduces a message-passing multicomputer called the
`Testbed' and describes the operating system and hybrid monitoring
support for load balancing. A series of experiments are reported in
which detailed and accurate performance figures are established for
the functions associated with task migration, thus parameterising some
key properties of the Testbed. A number of different performance
metrics are compared in terms of their costs versus their utility for
load balancing. Finally, a sample load balancing strategy is outlined
for the Testbed and speedup results obtained for a range of
applications.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-4-94,
author      = {Rojas-Mujica, I.},
title       = {{Comparison of three SNP Packages}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jun,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-4-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-4-94.ps.gz},
abstract    = {
For the appropiate analysis and simulation of various types of Petri
nets, modelling real systems or applications, the assistance of
computer packages or tools is indispensable. This paper provides an
brief introduction to the use of Stochastic Petri Nets packages,
specifically GreatSPN1.6, DSPNexpress1.2 and SPNPv3.Initially a review
of the main concepts employed in the analysis of Stochastic Timed
Petri Nets is given. It is expected that the reader will have a
general knowledge of Petri Nets and that this would act as a revision
and as the introduction to rather more complex concepts. The idea of
this section is to supply a theoretical background to the facilities
offered by the packages that are analysed in this paper. For each
package we describe its main features, what it offers and how its
implemented. As a guide for the selection of which package is more
appropiate for the readers particular needs a comparison section
between the packages is offered. The information supplied for each
package reviewed, has come from both the experience gained using the
packages and from the existing documentation for each one.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-5-94,
author      = {Harris, T. and Topham, N.},
title       = {{The Use of Caching in Decoupled Multiprocessors
               with Shared Memory}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = oct,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-5-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-5-94.ps.gz},
abstract    = {
In the following we evaluate the costs and benefits of using a cache
memory with a decoupled architecture supporting shared memory in both
the uniprocessor and multiprocessor cases. Firstly we identify the
performance bottleneck of such architectures, which we define as Loss
of Decoupling costs. We show that in both uniprocessors and
multiprocessor machines with high latency such costs can greatly
effect performance. We then assess the ability of cache to reduce loss
of decoupling costs in both uniprocessors and multiprocessors. Through
use of graphical tools we provide an intuition as to the behaviour of
such decoupled machines. In multiprocessors we define the target model
of shared memory and introduce various coherency schemes to implement
the model. Each coherency scheme is then evaluated experimentally. We
show that hardware coherence schemes can improve the performance of
such architectures, though the relationship between hit rate and
performance is substantially different than in the non-decoupled case.
Our results are based on discrete-event simulations which take as
input address traces from various scientific applications.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-6-94,
author      = {Donatelli, S. and Hillston, J. and Ribaudo, M.},
title       = {{A Comparison of Performance Evaluation Process
                Algebra and Generalized Stochastic Petri Nets}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = nov,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-6-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-6-94.ps.gz},
abstract    = {
Generalized Stochastic Petri Nets (GSPN) and Performance Evaluation
Process Algebra (PEPA) are two formalisms that can be used to study in
a single environment both qualitative and quantitative behaviour of
systems.
This paper presents a comparison of the two formalisms in terms of the
facilities that they provide to the modeller, considering both the
definition and the analysis of the performance model.
Our goal is to provide a better understanding of both formalisms, and
to prepare a fertile ground for exchanging ideas and techniques
between the two. To illustrate similarities and differences, we make
the different issues more concrete by means of a running example.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-7-94,
author      = {Gilmore, S. and Hillston, J. and Holton, R. and
               Rettelbach, M.},
title       = {{Specifications in Stochastic Process Algebra for a
                Robot Control Problem}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = dec,
year        = 1994,
type        = {technical report},
number      = {ECS-CSG-7-94},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-7-94.ps.gz},
abstract    = {
We present a novel approach to specification of dynamic systems. This
approach, a stochastic extension of process algebra, facilitates
quantitative, or performance, analysis, in addition to qualitative
analysis. For unreliable systems this integrated approach encourages
the investigation of the impact of functional characteristics on the
performance of the system. Throughout the paper details of the
stochastic process algebra are made concrete via an example: a robot
control problem.
Two specifications are presented of this problem. The first, an
idealisation, does not represent the possibility of failures. The
second models both failures and recoveries. Each is solved to obtain
performance measures for the system.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-8-95,
author      = {Pooley, R.},
title       = {{Integrating Behavioural and Simulation Modelling}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = mar,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-8-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-8-95.ps.gz},
abstract    = {
Discrete event simulation has grown up as a practical technique for
estimating the quantitative behaviour of systems, where direct
measurement is undesirable or impractical. It is also used to
understand the detailed functional behaviour of such systems. Its
theory is largely that of experimental science, centering on
statistical approaches to validating the measures generated by such
models, rather than on the verification of their detailed behaviour.
On the other hand, much work has been done on understanding and
proving functional properties of systems, using techniques of formal
specification and concurrency modelling. This paper presents an
approach to understanding the correctness of the behaviour of discrete
event simulation models, using a technique from the concurrency world,
Milner's Calculus of Communicating Systems (CCS) and to deriving
behavioural properties of such models without resorting to simulation.
It is shown that a common framework based on the process view of
models can be constructed,As a basis for this framework, a
hierarchical graphical modelling language (Extended Activity Diagrams)
is developed. This language is shown to map onto both the major
constructs of the DEMOS discrete event simulation language and their
equivalent CCS models. A graphically driven tool based on such a
framework is presented, which generates models to use both simulation
to answer performance questions (what is the throughput under a
certain load) and functional techniques to answer behavioural
questions (will the system behave as expected under certain
assumptions). An example of the application of this approach to a
typical model is presented.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-9-95,
author      = {Heywood, T. and Leopold, C.},
title       = {{Dynamic Randomized Simulation of Hierarchical PRAMs
                on Meshes}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-9-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-9-95.ps.gz},
abstract    = {
The Hierarchical PRAM (H-PRAM) model is a dynamically partitionable
PRAM, which charges for communication and synchronization, and allows
parallel algorithms to abstractly represent general locality. In this
paper we show that the H-PRAM can be implemented efficiently on a
two-dimensional mesh. We use the Peano indexing scheme to
hierarchically partition the mesh. Multiple sub-PRAMs of the H-PRAM
are simulated on irregular sub-meshes. For an H-PRAM program of cost T,
the overall CRCW H-PRAM simulation runs in time constant in T with
high probability. The simulation is dynamic, i.e. it does not depend
on prior knowledge of a program's specific hierarchical configuration,
which may be data dependent.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-10-95,
author      = {Chochia, G. and Cole, M. and Heywood, T.},
title       = {{Implementing the Hierarchical PRAM on the 2D Mesh:
                Analysis and Experiments}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-10-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-10-95.ps.gz},
abstract    = {
We investigate aspects of the performance of the EREW instance of the
Hierarchical PRAM (H-PRAM) model, a recursively partitionable PRAM, on
the 2D mesh architecture via analysis and simulation experiments.
Since one of the ideas behind the H-PRAM is to systematically exploit
locality in order to negate the need for expensive communication
hardware and thus promote cost-effective scalability, our design
decisions are based on minimizing implementation costs. The Peano
indexing scheme is used as a simple and natural means of allowing the
dynamic, recursive partitioning of the mesh into arbitrarily-sized
sub-meshes, as required by the H-PRAM.  We show that for any sub-mesh
the ratio of the largest manhattan distance between two nodes of the
sub-mesh to that of the square mesh with an identical number of
processors is at most 3/2, thereby demonstrating the locality
preserving properties of the Peano scheme for arbitrary partitions of
the mesh. We provide matching analytical and experimental evidence
that the routing required to efficiently implement the H-PRAM with
this scheme can be implemented cheaply and effectively.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-11-95,
author      = {Cusack, S. and Pooley, R.},
title       = {{Simulation Experiments with Protocol Interactions in
                ATM Networks}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jun,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-11-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-11-95.ps.gz},
abstract    = {
This paper outlines ongoing work in performance evaluation of ATM
networks through the use of simulation. This work is particularly
aimed at estimating the effects at the application level of higher
level protocol interactions. Current modelling of ATM networks has
been concentrated at the switch level. An objective of this work is to
broaden the context by examining the network performance and
characteristics at a level typically seen by network management. An
object-oriented methodology and the techniques of discrete event
simulation are being investigated as a means of providing a framework
in which to conduct these experiments.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-12-95,
author      = {Pooley, R.},
title       = {{A Survey of Performance Analysis Tools in Europe}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jun,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-12-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-12-95.ps.gz},
pdf-url     = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-12-95.pdf},
abstract    = {
This report contains a short survey for those interested in currently
available tools for performance evaluation of computer systems and
networks.  In particular it presents a survey of such tools which
originate from commercial and academic organisations in Europe.  Since
this is a rapidly evolving field, it also presents pointers to new
techniques which are likely to be incorporated into usable tools in
the near future.  After a brief definition of the term
performance analysis the paper considers the techniques available to
someone wishing to perform such an analysis and the tools which can be
used to support such work.  Techniques are divided into measurement
and workload estimation, queueing network modelling, Petri net
modelling, use of process algrebras and formal protocol languages,
simulation, performability and integrated environments.  Under each of
these headings a small number of the most representative and widely
available tools are presented.  Where tools offer similar
functionality, a brief comparison is offered.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-13-95,
author      = {Pooley, R.},
title       = {{Performance Evaluation of Computer Systems}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jun,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-13-95},
abstract-url = {http://www.dcs.ed.ac.uk/csg/reports/CSG-13-95.abstract},
abstract    = {
This report contains a brief introduction to the field of Computer
Performance Evaluation.  It is aimed at those working in the field of
Computer Science, who are considering this area for the first time.
It is very introductory in nature, but offers a short reading list for
further investigation.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-14-95,
author      = {Ganesh, A.J.},
title       = {{Large Deviations of the Sojourn Time for Queues in
                Series}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jul,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-14-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-14-95.ps.gz},
abstract    = {
We consider an open queueing network consisting of an arbitrary number
of queues in series. We assume that the arrival process into the first
queue and the service processes at the individual queues are jointly
stationary and ergodic, and that the mean inter-arrival time exceeds
the mean service time at each of the queues. Starting from Lindley's
recursion for the waiting time, we obtain a simple expression for the
total delay (sojourn time) in the system. Under some mild additional
assumptions, which are satisfied by many commonly used models, we show
that the delay distribution has an exponentially decaying tail and
compute the exact decay rate. We also find the most likely path
leading to the build-up of large delays. Such a result is of relevance
to communication networks, where it is often necessary to guarantee
bounds on the probability of large delays. Such bounds are part of the
specification of the quality of service desired by the network user.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-15-95,
author      = {Candlin, R.},
title       = {{The POSIE Project: Studies to Support the Design of
                Operating Systems for Multicomputers}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jul,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-15-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-15-95.ps.gz},
abstract    = {
The POSIE Project was concerned with a number of topics which are
relevant for the soundly-based design of operating systems for
distributed memory parallel machines. These topics include: the design
of a hardware monitor for the accurate measurement of operating system
costs; the development of a formal specification for an operating
system that supports process migration correctly; and a study of the
ways in which the characteristics of parallel programs influence
performance.
The project involved a large number of staff and students from the
Department of Computer Science, and this report gives a brief history
of the project, and an overview of what was achieved.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-16-95,
author      = {Zurek, Th. and Thanisch, P.},
title       = {{Optimization Techniques for Parallel Linear Recursive
                Query Processing}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jul,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-16-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-16-95.ps.gz},
abstract    = {
Query optimization for sequential execution of non-recursive queries
has reached a high level of sophistication in commercial DBMS. The
successful application of parallel processing for the evaluation of
recursive queries will require a query optimizer of comparable
sophistication. The groundwork for creating this new breed of query
optimizer will consist of a combination of theoretical insight and
empirical investigation. Restricting our attention to linear recursive
queries, we illustrate this process by developing a family of query
processing strategies and, through experiments on a parallel computer,
obtaining the basic information needed for an optimizer's heuristics.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-17-95,
author      = {Boeres, C. and Chochia, G. and Thanisch, P.},
title       = {{On the Scope and Applicability of the ETF Algorithm}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = aug,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-17-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-17-95.ps.gz},
abstract    = {
Superficially, the Earliest Task First (ETF) heuristic is
attractive because it models heterogeneous messages passing through a
heterogeneous network. On closer inspection, however, this is
precisely the set of circumstances that can cause ETF to produce
seriously sub-optimal schedules. In this paper we analyze the scope of
applicability of ETF. We show that ETF has a good performance if
messages are short and the links are fast and a poor performance
otherwise. For the first application we choose the Diamond DAG with
unit execution time for each task and the multiprocessor system in the
form of the fully connected network. We show that ETF partitions the
DAG into lines each of which is scheduled on the same processor. The
analysis reveals that if the communication times between pairs of
adjacent tasks in a precedence relation are all less than or equal to
unit then the schedule is optimal. If the communication time is equal
to the processing time needed to evaluate a row then the completion
time is O(pn) times more than the optimal one for an n \Theta n
Diamond DAG. For the second application, we choose the join DAG
evaluated by two connected processors.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-18-95,
author      = {Chochia, G. and Cole, M. and Heywood, T.},
title       = {{Lower Bounds on Average Time for Random Destination
                Mesh Routing and Their Utility as Performance
                Predictors for PRAM Simulation}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = oct,
year        = 1995,
type        = {technical report},
number      = {ECS-CSG-18-95},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-18-95.ps.gz},
abstract    = {
This paper presents lower bounds on the expected time for random
destination routing on a mesh, valid for any routing scheme, queueing
discipline and queue size. We show that the lower bounds are
applicable to probabilistic simulation of the P processor EREW PRAM
with shared memory M on a P processor mesh with distributed memory
and consider two cases: where the number of packets q per processor is
one, which corresponds to PRAM simulation with P = P, and where q AE
1, which corresponds to PRAM simulation with parallel slackness.
Experimental results are given showing that the bounds give good
analytical predictions of the actual performance of both random
destination routing and practical probabilistic PRAM simulation on
meshes. The experiments are carried out on a mesh with small fixed
queues and memory randomized by pseudo-random hash functions.
Simulating PRAM memory accesses with random strides show that \Theta
(ln P= ln ln P)-universal hash functions perform better than linear,
2-universal hash functions.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-19-96,
author      = {Reddy, M.},
title       = {{A Measure for Perceived Detail in Computer-Generated
                Images}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jan,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-19-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-19-96.ps.gz},
abstract    = {
This report will present a procedure for assessing the degree of
visual detail which a user can perceive in an arbitrary
computer-generated image. This will involve describing an image in
terms of its component spatial frequencies (c/deg); a measure which is
commonly employed in the field of visual perception to characterise
the efficacy of the human visual system. The text will present a brief
introduction to this visual metric, and then demonstrate how a
full-colour computer-generated image can be described in terms of this
metric. The approach advocated here is based upon an image
segmentation algorithm: Fourier techniques will be considered, and
shown to be inappropriate for this particular application.
Keywords: Computer graphics, visual perception, visual acuity, spatial
frequency.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-20-96,
author      = {Zurek, Th.},
title       = {{Parallel Temporal Nested-Loop Joins}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jan,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-20-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-20-96.ps.gz},
abstract    = {
In this report we present a framework for parallel temporal joins. We
focus on temporal intersection as the most general temporal join
condition. A basic algorithm is given that consists of a partition and
a joining stage. In the partition stage tuples have to be replicated
if they intersect with more than one partition range.  This causes a
significant overhead - around 70% in the case of our modest workload.
The joining stage can employ any sequential join technique. The basic
algorithm is enhanced through two possible optimisations that reduce
the overhead imposed by replication.
The algorithm and its optimisations are evaluated on top of a
performance model. We describe the model and give details in the
appendix. The evaluation shows that both optimisations together
decrease the basic costs significantly. Furthermore we can give an
idea of the quantitative impact of replication overhead in parallel
temporal join processing. Our (modest) workload caused a share of
around 70% of the total costs; higher values can be expected in
reality.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-21-96,
author      = {Rojas-Mujica, I.},
title       = {{Compositional Construction of SWN Models}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = feb,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-21-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-21-96.ps.gz},
abstract    = {
This report presents a method for the compositional construction of
Stochastic Petri Net models. The method is defined over Stochastic
Wellformed Nets in order to take advantage of the state space
reduction properties of this formalism. The set of composition
operations is based on the operators of Stochastic Process Algebra,
augmented with operations that reflect the different types of
synchronisation supported by Petri nets. Several examples are
presented to illustrate the use of the method. These are developed
following a set of guidelines for model construction.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-22-96,
author      = {Ibbett, R. and Chochia, G. and Coe, P.S. and Cole, M.I.
               and Heywood, P.E. and Heywood, T. and Pooley, R.J. and
               Thanisch, P. and Topham, N.P.},
title       = {{Algorithms, Architectures and Models of Computation}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = mar,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-22-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-22-96.ps.gz},
abstract    = {
The ALgorithms, Architectures and Models of computation (ALAMO)
project at the University of Edinburgh aims to investigate the
scalability and efficiency with which the Hierarchical PRAM model of
parallel computation may be implemented on realistic parallel
architectures. This investigation is being performed using a
programming language H-FORK together with HASE, a tool for the
hierarchical design and simulation of computer architectures. This
paper outlines the background to the project and the motivation for
it.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-23-96,
author      = {Ganesh, A.J.},
title       = {{Bias Correction in Effective Bandwidth Estimation}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-23-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-23-96.ps.gz},
abstract    = {
Call admission in ATM networks involves a trade-off between ensuring
an adequate quality of service to users and exploiting the scale
efficiencies of statistical multiplexing. Achieving a good trade-off
requires some knowledge of the source traffic. Its effective bandwidth
has been proposed as a measure that captures characteristics which are
relevant to quality of service provisioning. The effective bandwidth
of a source is not known a priori , but needs to be estimated from an
observation of its output. We show that direct estimators that have
been proposed for this purpose are biased when the source traffic is
autocorrelated. By explicitly computing the bias for auto-regressive
and Markov sources, we devise a bias correction scheme that does not
require knowledge of the model parameters. This is achieved by
exploiting a scaling property of the bias that is insensitive to model
parameters, and that has the same form for both auto-regressive and
Markov sources. This leads us to conjecture that the scaling property
may be valid in greater generality and can be used to obtain unbiased
effective bandwidth estimates for real traffic. Use of our bias
correction technique enables us to obtain accurate estimates of
effective bandwidths using relatively short block lengths. The latter
is important both because the variance of the estimator increases with
the block length, and because real traffic may well be non-stationary,
requiring that estimates be obtained from short data records.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-24-96,
author      = {Ganesh, A.J. and O'Connell, N.},
title       = {{Linear geodesics are not generally preserved by
                a FIFO queue}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = may,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-24-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-24-96.ps.gz},
abstract    = {
Large deviations techniques have proved to be very useful in
characterizing the tail of queue length distributions, for queues with
very general arrival and service processes. Extending these ideas to
networks of queues requires the characterization of the large
deviations behaviour at the output of a queue in terms of the
behaviour of the input and service processes. We show in this paper
that the output process does not, in general, possess a desirable
property that is usually assumed for the input process. This property
is the convexity of its large deviations rate function, or
equivalently, the linear geodesic property of its sample paths. The
lack of this property implies that a simple, inductive approach to
characterizing the tail of queue length distributions in networks is
not feasible in general.
Keywords: Queueing networks, large deviations, multiclass queues.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-25-96,
author      = {Chochia, G. and Cole, M. and Heywood, T.},
title       = {{Synchronizing Arbitrary Processor Groups in
                Dynamically Partitioned 2D Meshes}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jul,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-25-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-25-96.ps.gz},
abstract    = {
A general purpose synchronization mechanism for a parallel computer
should allow arbitrary, data-dependent, dynamically partitioned groups
of processors to remain internally synchronized, while proceeding
asynchronously with respect to other groups. We present an algorithm
which can support such a scheme. The algorithm constructs binary
synchronization trees for the sub-groups, given a group of processors
and a f0; 1g label for each processor, and is valid for any network.
We provide a general complexity analysis in terms of operations on the
synchronization trees which is then instantiated with respect to the n
\Theta n processor 2D mesh architecture. We show that the algorithm
constructs a synchronization tree for any subgroup of s processors in
O(n log s) parallel communication steps with high probability. We
present lower bounds on achievable performance based on the mesh
indexing scheme used: row/column major indexing schemes require \Omega
(n log n) parallel communication steps in the worst case, whereas the
recursive Hilbert indexing scheme requires \Omega (n plog n) parallel
communication steps. Experimental results are given validating the
analysis. Our algorithm has applications in implementations of PRAMs
(e.g. conditional instructions) and of nested data parallelism (or
mixed data/task parallelism) on distributed processor networks.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-26-96,
author      = {{Khalid M. Al-Tawil} and Ibbett, R.N.},
title       = {{Fault Tolerance of Hypercube-based Multicomputers:
                A Survey}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = sep,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-26-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-26-96.ps.gz},
abstract    = {
The problem of tolerating faulty processors or links in hypercubes has
been studied by many researchers, either by using spares or by
reconfiguration. Massively parallel computers, using thousands of
processors, will be the future trend for producing tremendous
computational power. However, in the current technology, if one
processor fails, the entire system may fail. A major drawback of
hypercubes is that a single processor failure may destroy the whole
network. The existence of a large number of components in such systems
makes them subject to failures. As the probability of any one or more
processors failing in such a complex system is large, building some
fault-tolerance feature into them becomes extremely important. Fault
tolerance in highly parallel computers is important for achieving
high-performance reliable computing. This manuscript is mainly a
survey of fault tolerance and related issues of hypercube-based
multicomputers.
Keywords: Fault Tolerance, Multicomputers, Hypercubes,
Reconfiguration, Broadcasting}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-27-96,
author      = {{Kann-Jang Yang} and Pooley, R.},
title       = {{Process Modelling to Support Object-Oriented Software
                Production}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = nov,
year        = 1996,
type        = {technical report},
number      = {ECS-CSG-27-96},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-27-96.ps.gz},
abstract    = {
Recently, the concepts of software process modelling and
object-oriented methodology have been widely discussed in the
literature to tackle the problems of software development. In this
article, we will concentrate on the method that uses formal
specification techniques to model the software process. PASTA (Process
and Artifact State Transition Abstraction) is a bridge between
guidance and automation of the software development process. By using
PASTA to model the Booch method, a double advantage can be obtained in
software development. Furthermore, the rapid advancement of
workstation and communication technologies has accelerated the spread
of the distributed development paradigm for software development. A
process-centred software environment with these advances could result
in a shortening of the development cycle and potentially an
improvement in productivity and quality.
Keywords: software process, process modelling, object-oriented
methodology, software factory.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-28-97,
author      = {Knafla, N.},
title       = {{A Prefetching Technique for Object-Oriented Databases}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jan,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-28-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-28-97.ps.gz},
abstract    = {
We present a new prefetching technique for objectoriented databases
which exploits the availability of multiprocessor client workstations.
The prefetching information is obtained from the object relationships
on the database pages and is stored in a Prefetch Object Table. This
prefetching algorithm is implemented usin g multithreading. In the
results we show the theoretical and empirical benefits of prefetching.
The benchmark tests show that multithreaded prefetching can improve
performance significantly for applications where the object access is
reasonably predictable.
Keywords: prefetching, distribution, object-oriented databases,
performance analys is, multithreading, application access pattern,
storage management.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-29-97,
author      = {Fernandes, M. and Llosa, J. and Topham, N.},
title       = {{Using Queues for Register File Organization in 
                VLIW Architectures}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = feb,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-29-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-29-97.ps.gz},
abstract    = {
Software pipelining is an effective technique for increasing the
through put of loops in superscalar or VLIW machines. However,
software pipelining generat es high register pressure, which in some
cases requires the introduction of spill code into the schedule. This
report shows that large multi-ported register files p resent
significant problems in the construction of scalable VLIW systems. In
an at tempt to address this problem we investigate the possibilities
for VLIW architectu res in which part of the register file is replaced
by queues. We believe that this organization has distict advantages in
terms hardware complexity, silicon area, i nstruction name space, and
scalability. Queues also represent a natural mechanism for
communication between clusters of functional units in a partitioned
VLIW syste m. In this report we present an experimental evaluation of
the machine resources r equired to support modulo scheduling under a
variety of VLIW register file configu rations. The results obtained
suggests that the use of queues is a feasible altern ative to global
register files.
Keywords: Software pipelining, instruction-level parallelism}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-30-97,
author      = {Reddy, M.},
title       = {{The Development and Evaluation of a Model of
                Visual Acuity for Computer-Generated Imagery}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = feb,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-30-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-30-97.ps.gz},
abstract    = {
This paper presents a model for human visual acuity which accommodates
the effects of peripheral vision and target velocity. The model is
based upon work from the field of visual perception in order to
provide an accur ate and principled result. The model is evaluated
through a number of psychophysic al experiments and subsequently
refined for the genus of stimuli commonly found in computer graphics
applications. With this work, and the companion paper Reddy (1996),
computer graphics developers gain a reliable method to predict the
ability of an average user to perceive detail at any point in a
computer-generated scene.  
Keywords: contrast sensitivity, psychophysics, spatial frequency,
threshold vision, visual acuity.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-31-97,
author      = {Cole, M.},
title       = {{Dividing and Conquering}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = mar,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-31-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-31-97.ps.gz},
abstract    = {
We suggest that the components of the well known divide-and-conquer
paradigm can be profitably presented as independent constructs in a
skeletal parallel programming model. We investigate this proposal in
the context of a rendition of Batcher's bitonic sorting algorithm in
which the nested parallel structure of the program is neatly
abstracted from the low level detail of a flat presentation. We show
that a variant of the conquer construct can make the presentation
neater still, and discuss the implications for the design of skeletal
models.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-32-97,
author      = {Jones, G.P and Topham, N.},
title       = {{Simplifying Hardware for Out of Order Execution using the Decoupling Paradigm}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = sep,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-32-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-32-97.ps.gz},
abstract    = {
Future hardware and software technology will try to provide improved
performance by extracting higher levels of parallelism.  However the
cost of a main memory access - in terms of missed instruction issue
slots - increases with faster processors and greater issue widths. For
this reason latency hiding technology remains one of the most
important parts of high performance processor designs.

In this paper we investigate the behaviour of data prefetching on an
access decoupled machine and a superscalar machine. Access decoupling
is a latency hiding technique that partitions a program into two
separate {\em instruction streams}\/ to aggressively prefetch data.
Superscalar architectures can support data prefetching through
out-of-order execution, non-blocking loads and lock-up free caches.
In this paper we investigate if there are benefits to using the
decoupling paradigm given that an out-of-order (o-o-o) superscalar
architecture could in principle prefetch to the same degree as an
access decoupled machine.

We have found that for large {\em issue width} \/the access decoupled
machine can hide memory latency more effectively than a single
instruction window o-o-o superscalar architecture.  For realistic
window sizes, our results show that to achieve the same performance as
an access decoupled machine our o-o-o superscalar machine requires an
instruction window 2.5 to 5 times larger.

Given that window issue logic is critical to processor clock speeds
and is dependent on window sizes, architectures that reduce window
logic complexity will be of interest to future designers.  Our
findings demonstrate that an access decoupled machine offers the
benefits of effectively hiding memory latency whilst reducing the
complexity of window issue logic.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-33-97,
author      = {Jones, G.P and Topham, N.},
title       = {{Design Issues for Latency Hiding on an Access Decoupled Machine}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = nov,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-33-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-33-97.ps.gz},
abstract    = {Future software and hardware technologies will try to
provide improved performance by extracting higher levels of
parallelism. However, the cost of main memory access
- in terms of missed instruction slots - increases
with faster processors and greater issue widths. For
this reason latency hiding technology remains one of
the most important parts of high performance
processor designs. In this paper we investigate a
latency hiding technique known as Access Decoupling
which partitions a program into two separate
instruction streams in order to aggressively prefetch
data.

We justify a renewed interest in Access Decoupling in
two ways. Firstly as a latency hiding technique and
secondly as a solution to the problem of hardware
complexity in large issue width, out-of-order
superscalar architectures. We show that in comparison
to a single instruction stream architecture Access
Decoupling is marginally more effective at hiding
memory latency and capable of achieving higher
performance through its simpler design.

After providing our justification for renewed
interest in the decoupling paradigm we quantify the
performance impact of different hardware/software
design issueson Access Decoupled Machines. We
consider the effect of restrictions imposed by data
dependency analysis, renaming, memory reordering,
operation reordering issue width and synchronisation
points on IPC and latency hiding effectiveness.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-34-97,
author      = {Fernandes, M. M. and Llosa, J. and Topham, N.},
title       = {{Extending a VLIW Architecture Model}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = dec,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-34-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-34-97.ps.gz},
abstract    = {
This technical report presents further developments on a VLIW architecture 
model previously reported. A distinctive characteristic of the architecture 
is the use of register files organized by means of queues, which results in 
a number of advantages over conventional schemes, but also requires the 
development of specific compiling and hardware features.
We have investigated a scheme based on copy operations to deal with data 
values to be  consumed more than once during loop execution. Experiments with
loop unrolling were also performed in order to optimize both loop 
execution and the use of machine resources. 
A simple partitioning algorithm has been implemented to perform
some experiments with a clustered architecture, an organization we understand 
as being essential to design and implement a  wide-issue machine. 
We also report a preliminary discussion on some hardware options to 
implement a queue register file.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-35-97,
author      = {Knafla, N.},
title       = {{Predicting Future Page Access by Analysing Object Relationships}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = dec,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-35-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-35-97.ps.gz},
abstract    = {
In this report we present a new approach to predicting page
access probability by considering the structure of the relationships
between objects. Database objects and their relationships to
other objects are modelled by a discrete-time Markov Chain. We
present two approaches to compute the page access probability:
(a) Using Hitting Times and Absorption Probabilities
and (b) Using the Chapman-Kolmogorov Equations.  If the probability of
a page is higher than a threshold defined by cost/benefit parameters
then the page is a candidate for prefetching. To determine the
prefetching threshold we consider various cost parameters to compare
the benefit of a correct prefetch with the cost of an incorrect
prefetch.
}}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-36-97,
author      = {Reddy, M.},
title       = {{The Effects of Low Frame Rate on a Measure for User
                Performance in Virtual Environments}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = dec,
year        = 1997,
type        = {technical report},
number      = {ECS-CSG-36-97},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-36-97.ps.gz},
abstract    = {
This report investigates the effect of low frame rate on human
performance in a desktop virtual environment. This is done using an
objective measure of users' ability to perform a simple heading task
at various frame rates (2.3-14.2 Hz). Two principal experiments are
presented. The first experiment shows that for a drop in frame rate
from 11.5 Hz to 2.3 Hz, users' accuracy and time to complete the
task degraded significantly. The second experiment reveals a
continuous, asymptotic relationship between frame rate and
performance for the chosen task. At low frame rates (up to 10-15 Hz)
there is a sharp improvement in performance as frame rate increases.
After around 15 Hz this increase is substantially less rapid. The
results provide evidence for reinforcing that a minimum frame rate
of around 15 Hz is necessary for virtual environments, but also that
further increases in frame rate will continue to yield greater
performance levels, albeit at a much reduced rate.
}}%---------------------------------------------------------------------------
@techreport{ECS-CSG-37-98,
author      = {Topham, N. and Gonzalez, A.},
title       = {{Randomized Cache Placement for Eliminating Conflicts}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = jan,
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-37-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-37-98.ps.gz},
abstract    = {
Applications with regular patterns of memory access can experience
high levels of cache conflict misses. In SMP systems using SPMD
programming models the level of conflict misses can be increased
significantly by the data transpositions required for parallelization.
Furthermore, techniques such as blocking which are introduced within a
single thread to improve locality, can result in yet more conflict
misses. The tension between minimizing cache conflicts and the 
other transformations needed for efficient parallelization leads to
complex optimization problems for parallelizing compilers.

This paper begins with a survey and quantitative evaluation of the 
existing proposals for conflict-resistant cache architectures. We show
that the introduction of a pseudo-random element into the
cache index function provides significant performance benefits. In
effect one can eliminate repetitive conflict misses and produce a
cache where miss ratio depends solely on working set behaviour. We
show empirically that one particular pseudo-random indexing function,
when used in conjunction with address prediction, yields the best
overall performance.

In many systems the processor clock period is closely linked to the
critical path through the first-level cache. In this paper we consider
the impact of pseudo-random cache indexing on processor cycle times
and present practical solutions to some of the major implementation
issues for this type of cache.

We present results from detailed simulations of a superscalar
out-of-order processor executing the Spec95 benchmarks, as well as
from cache simulations of individual loop kernels to illustrate
specific effects. We present measurements of Instructions committed
Per Cycle (IPC) when comparing the performance of different cache 
architectures on whole-program benchmarks such as the Spec95 suite.
In addition, miss ratio measurements are presented for some loop
kernels to highlight the extra conflicts introduced by loop
transformations and the elimination of these misses by our proposed
cache.
}}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-38-98,
author      = {Howell F.W. and Ibbett, R.N.},
title       = {{Evaluation of Multiprocessor Interconnection Networks}},
institution = {Dept. of Computer Science, Edinburgh University},
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-38-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-38-98.ps.gz},
abstract    = {
This report describes the work carried out under the EMIN project to
set up a testbed for simulating multiprocessor networks.  All levels
from low level hardware to the software interfaces affect performance,
and so the initial simulation testbed provided an MPI interface on top
of a cycle level simulator. The networks modelled included a crossbar
and the Cray T3D network. Meaningful simulations at this level of
detail proved infeasible, however, and an alternative approach was to
use microbenchmarking, of both shared memory and message passing
network primitives, as a means of characterising network performance
in a way which is meaningful to programmers. This led to a refined
simulation testbed which cleanly separates workload models from
network models, using an interface based on the microbenchmarking
work. In a further development, a web version of the testbed was
developed and the value of this approach to modelling is evaluated, in
particular the accessibility of the simulation models and the
importance of visualisation.
}}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-39-98,
author      = {Williams, L.M.},
title       = {{HASE: Entity Linkage and Abstraction}},
institution = {Dept. of Computer Science, Edinburgh University},
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-39-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-39-98.ps.gz},
abstract    = {
}}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-40-98,
author      = {Pooley, R. and Stevens, P.},
title       = {{ Software Reengineering Patterns}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = mar,
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-40-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-40-98.ps.gz},
abstract    = {
The problem of reengineering of legacy systems, in the widest sense, is
widely recognised as one of the most significant challenges facing software
engineers. So-called legacy systems are normally, but not necessarily,
large systems built in an era before encapsulation and componentisation
were regarded as fundamental tenets of design. Through a gradual process of
accretion and change, they have become devoid of useful structure. This
makes them hard, expensive or impossible to modify in order to meet changes
in the business processes. Legacy systems, whilst often essential to the
running of an organisation, also inhibit change in that organisation. The
problems of legacy systems are not limited to any one kind of organisation:
large corporations and SMEs both suffer. Moreover, there seems no reason to
be confident that today's new systems are not also tomorrow's legacy
systems. The problem of reengineering legacy systems is probably here to
stay. 

In this paper we introduce the idea of software reengineering patterns,
which adapt the ideas of design patterns to identify lessons in successful
reengineering projects and to make these lessons available to new projects.
This is done in the context of component based reengineering, which has
been the focus of considerable hope in the reengineering community, but
which has delivered limited successes so far. These ideas are developed in
terms of some introductory examples taken from real projects.


}}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-41-98,
author      = {Coe, P. and Ibbett, R.N. and Rafferty, N. and Williams, L.},
title       = {{ HASE: An Environment for Hardware/Software Codesign}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = mar,
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-41-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-41-98.ps.gz},
abstract    = {
Hardware/software codesign has recently become an explicit
topic in computer research circles, although it has been an implicit
part of many projects for several decades. Tools are required to aid
designers to follow codesign practices and methodologies, enabling
software and hardware designers to work within a unified environment
when creating a system.  The Hierarchical Architecture design and
Simulation Environment, HASE which allows rapid development and
exploration of computer architectures encompassing both hardware and
software, will be discussed as a tool to be used as a basis for such
an environment.  Example systems are also presented to highlight the
features of HASE.}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-43-98,
author      = {Knafla, N.},
title       = {{Page versus Object Prefetching: A Performance Evaluation}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = aug,
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-43-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-43-98.ps.gz},
abstract    = {
In this report we compare the performance of a prefetching page server
system with a prefetching object server system.  We simulate the
object access pattern by assigning transition probabilities to the
object relationships.  According to the transition probabilities we
compute the access probability of pages and objects. We designed
several prefetching techniques for a prefetching page server and a
prefetching object server. We compared the performance of the
prefetching techniques in a simulation.
}
}
%---------------------------------------------------------------------------
@techreport{ECS-CSG-44-98,
author      = {Wang, F. and McKenzie,  R.A.},
title       = {{Virtual Life in Virtual Environments}},
institution = {Dept. of Computer Science, Edinburgh University},
month       = sep,
year        = 1998,
type        = {technical report},
number      = {ECS-CSG-44-98},
url         = {http://www.dcs.ed.ac.uk/csg/reports/ECS-CSG-44-98.ps.gz},
abstract    = {
Virtual Environment is at the heart of Virtual Reality. High fidelity
environment can become inhabited by virtual lives so that human users
will feel immersed in a ``real'' world. With the unlimited application
of VR, {\em Virtual Life} becomes a new and promising area. In the view
of no clear definition and explanation on the notion of Virtual Life, this
paper has tried to define and discuss it from a systematic view.
For a true feeling of presence, virtual life should have a visual model
for realistic visual shape and appearance, and a mental model for
believable behaviour. This mental model should be autonomous to make
decisions by itself, adaptive to promote its structure for better
maintenance,
and interactive to external changes, especially to human user's actions,
by characteristic activities. Although virtual life derives from the
real life, it is not just a simple copy of the real one. It has also its own
features since its living sphere is a simulated one. The study of Virtual
Life utilizes knowledge from various disciplines and intertwines with
many other areas in computer science, including computer graphics,
artificial life, autonomous agents and artificial intelligence. This
paper reviews related areas and work and expects more attention to be
paid on Virtual Life, a new field which is beginning to take shape.
}
}
%---------------------------------------------------------------------------
