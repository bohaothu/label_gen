@inproceedings{2009:CA:1545006.1545037,
 title = {Cover Art},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {C1--},
 url = {http://dx.doi.org/10.1109/CGO.2009.50},
 doi = {http://dx.doi.org/10.1109/CGO.2009.50},
 acmid = {1545037},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:TPI:1545006.1545038,
 title = {Title Page i},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {i--},
 url = {http://dx.doi.org/10.1109/CGO.2009.1},
 doi = {http://dx.doi.org/10.1109/CGO.2009.1},
 acmid = {1545038},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:TPI:1545006.1545039,
 title = {Title Page ii},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {ii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.49},
 doi = {http://dx.doi.org/10.1109/CGO.2009.49},
 acmid = {1545039},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:TPI:1545006.1545040,
 title = {Title Page iii},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {iii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.2},
 doi = {http://dx.doi.org/10.1109/CGO.2009.2},
 acmid = {1545040},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:CP:1545006.1545041,
 title = {Copyright Page},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {iv--},
 url = {http://dx.doi.org/10.1109/CGO.2009.3},
 doi = {http://dx.doi.org/10.1109/CGO.2009.3},
 acmid = {1545041},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:MGC:1545006.1545042,
 title = {Message from General Co-chairs},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {viii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.4},
 doi = {http://dx.doi.org/10.1109/CGO.2009.4},
 acmid = {1545042},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:MPC:1545006.1545043,
 title = {Message from the Program Chair},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {ix--},
 url = {http://dx.doi.org/10.1109/CGO.2009.39},
 doi = {http://dx.doi.org/10.1109/CGO.2009.39},
 acmid = {1545043},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:AR:1545006.1545044,
 title = {Additional Reviewers},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {xii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.7},
 doi = {http://dx.doi.org/10.1109/CGO.2009.7},
 acmid = {1545044},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:SPO:1545006.1545045,
 title = {Sponsors},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {xiii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.40},
 doi = {http://dx.doi.org/10.1109/CGO.2009.40},
 acmid = {1545045},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:EGP:1545006.1545046,
 title = {An Evolution of General Purpose Processing: Reconfigurable Logic Computing},
 abstract = {The historical improvements in the performance of general purpose processors have long provided opportunities for application innovation. Word processing, spreadsheets, desktop publishing, networking and various game genres are just some of the many applications that have arisen because of the increasing capabilities and the versatility of general purpose processors. Key to these innovations is the fact that general purpose processors do not predefine the applications that they are going to run. Currently, the capabilities of individual general purpose processors are encountering challenges, such as power limits, and are not scaling as rapidly as they once were. As a consequence, a variety of approaches are being employed to address this situation, including multiprocessors, vector extensions and special purpose accelerators. While each of these techniques extends a processor's compute capabilities each sacrifices generality in one way or another.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {xxii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.38},
 doi = {http://dx.doi.org/10.1109/CGO.2009.38},
 acmid = {1545046},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Reconfigurable logic, architecture, FPGAs},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:NGC:1545006.1545047,
 title = {The Next Generation of Compilers},
 abstract = {Over the past decade, production compilers for general-purpose processors have adopted a number of major technologies emerging from compiler research, including SSA-based optimization, pointer analysis, profile-guided optimization, link-time cross-module optimization, automatic vectorization, and just-in-time compilation with adaptive optimization for dynamic languages. These features are here to stay for the foreseeable future. So what major new features could emerge from compiler research over the next decade?First, just-in-time and dynamic optimization will be extended to static languages, such as C, C++, and Fortran. This has already happened for graphics applications, as in the MacOS X OpenGL library and the AMD ATI compiler, and is now being adopted for general-purpose multicore platforms such as the RapidMind Multicore Development Platform.Second, and perhaps most predictably, compilers will play a major role in tackling the multicore programming challenge. This does not mean that automatic parallelization will come back from the dead. Rather compiler support for parallel programming will take two forms: optimization and code generation for explicitly parallel programs; and interactive, potentially optimistic, parallelization technology to support semi-automatic porting of existing code to explicitly parallel programming models.Third, compilers will increasingly be responsible for enhancing or enforcing safety and reliability properties for programs. The last few years have seen new language and compiler techniques (e.g. in the Cyclone, CCured, and SAFECode projects) that guarantee complete memory safety and sound operational semantics even for C and C++ programs. There is no longer any excuse for production C/C++ compilers not to provide these capabilities, at least as an option for security-sensitive software, including all privileged software. Furthermore, these capabilities can be deployed via a typed virtual machine that enables more powerful security and reliability techniques than with native machine code.Fourth, compilers will increasingly incorporate more sophisticated auto-tuning strategies for exploring optimization sequences, or even arbitrary code sequences for key kernels. This is one of the major sources of unexploited performance improvements with existing compiler technology.Finally, compilers will adopt speculative optimizations in order to compensate for the constraints imposed by conservative static analysis. Recent architecture research has led to novel hardware mechanisms that can make such speculation efficient and the ball is in the compiler community's court to invent new ways to exploit this hardware support for more powerful, traditional and non-traditional, optimizations.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {xxiii--},
 url = {http://dx.doi.org/10.1109/CGO.2009.37},
 doi = {http://dx.doi.org/10.1109/CGO.2009.37},
 acmid = {1545047},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Bronevetsky:2009:CSD:1545006.1545049,
 author = {Bronevetsky, Greg},
 title = {Communication-Sensitive Static Dataflow for Parallel Message Passing Applications},
 abstract = {Message passing is a very popular style of parallel programming, used in a wide variety of applications and supported by many APIs, such as BSD sockets, MPI and PVM. Its importance has motivated significant amounts of research on optimization and debugging techniques for such applications. Although this work has produced impressive results, it has also failed to fulfill its full potential. The reason is that while prior work has focused on runtime techniques, there has been very little work on compiler analyses that understand the properties of parallel message passing applications and use this information to improve application performance and quality of debuggers.This paper presents a novel compiler analysis framework that extends dataflow to parallel message passing applications on arbitrary numbers of processes. It works on an extended control-flow graph that includes all possible inter-process interactions of any numbers of processes. This enables dataflow analyses built on top of this framework to incorporate information about the application's parallel behavior and communication topology. The parallel dataflow framework can be instantiated with a variety of specific dataflow analyses as well as abstractions that can tune the accuracy of communication topology detection against its cost.The proposed framework bridges the gap between prior work on parallel runtime systems and sequential dataflow analyses, enabling new transformations, runtime optimizations and bug detection tools that require knowledge of the application's communication topology. We instantiate this framework with two different symbolic analyses and show how these analyses can detect different types of communication patterns, which enables the use of dataflow analyses on a wide variety of real applications.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {1--12},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.32},
 doi = {http://dx.doi.org/10.1109/CGO.2009.32},
 acmid = {1545049},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {message-passing, compiler analysis, static analysis, parallel processing, multi-core},
} 

@inproceedings{Spear:2009:RMO:1545006.1545048,
 author = {Spear, Michael F. and Michael, Maged M. and Scott, Michael L. and Wu, Peng},
 title = {Reducing Memory Ordering Overheads in Software Transactional Memory},
 abstract = {Most research into high-performance software transactional memory (STM) assumes that transactions will run on a processor with a relatively strict memory model, such as Total Store Ordering (TSO). To execute these algorithms correctly on processors with relaxed memory models, explicit fence instructions may be required on every transactional access, and neither the processor nor the compiler may be able to safely reorder transactional reads. The overheads of fence instructions and read serialization are a significant but unstudied source of latency for STM, with impact on the tradeoffs among different STM systems and on the optimizations that may be possible for any given system. Straightforward ports of STM runtimes from strict to relaxed machines may fail to realize the latter's performance potential. We explore the implementation of STM for machines with relaxed memory consistency using two recent high-performance STM systems. We propose compiler optimizations that can safely eliminate many fence instructions. Using these techniques, we obtain a reduction of up to 89\% in the number of fences, and 20\% in per-transaction latency, for common transactional benchmarks.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {13--24},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.30},
 doi = {http://dx.doi.org/10.1109/CGO.2009.30},
 acmid = {1545048},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Software Transactional Memory, Memory Fences},
} 

@inproceedings{Duan:2009:DEP:1545006.1545050,
 author = {Duan, Yuelu and Feng, Xiaobing and Wang, Lei and Zhang, Chao and Yew, Pen-Chung},
 title = {Detecting and Eliminating Potential Violations of Sequential Consistency for Concurrent C/C++ Programs},
 abstract = {When a concurrent shared-memory program written with a sequential consistency (SC) model is run on a machine implemented with a relaxed consistency (RC) model, it could cause SC violations that are very hard to debug. To avoid such violations, programmers need to provide explicit synchronizations or insert fence instructions. In this paper, we propose a scheme to detect and eliminate potential SC violations by combining Shasha/Snir's conflict graph and delay set theory with existing data race detection techniques. For each execution, we generate a race graph, which contains the improperly synchronized conflict accesses, called race accesses, and race cycles formed with those accesses. As a race cycle would probably lead to a non-sequential-consistent execution, we call it a potential violation of sequential consistency (PVSC) bug. We then compute the race delays of race cycles, and suggest programmers to insert fences into source code to eliminate PVSC bugs. We further convert a race graph into a PC race graph, and improves cycle detection and race delay computation to O(n^2), where n is the number of race access instructions. We evaluate our approach with the SPLASH-2 benchmarks, two large real-world applications (MySQL and Apache), and several multi-threaded Cilk programs. The results show that (1) the proposed approach could effec-tively detect PVSC bugs in real-world applications with good scalability; (2) it retains most of the performance of the concurrent program after inserting required fence instructions, with less than 6.3\% performance loss; and (3) the additional cost of our approach over traditional race detection techniques is quite low, with 3.3\% on average.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {25--34},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.29},
 doi = {http://dx.doi.org/10.1109/CGO.2009.29},
 acmid = {1545050},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Sequential consistency, data race detection, delay set, fence, relaxed memory model},
} 

@inproceedings{Yu:2009:ERN:1545006.1545051,
 author = {Yu, Jing and Garzaran, Maria Jesus and Snir, Marc},
 title = {ESoftCheck: Removal of Non-vital Checks for Fault Tolerance},
 abstract = {As semiconductor technology scales into the deep submicron regime the occurrence of transient or soft errors will increase. This will require new approaches to error detection. Software checking approaches are attractive because they require little hardware modification and can be easily adjusted to fit different reliability and performance requirements. Unfortunately, software checking adds a significant performance overhead.In this paper we present ESoftCheck, a set of compiler optimization techniques to determine which are the vital checks, that is, the minimum number of checks that are necessary to detect an error and roll back to a correct program state. ESoftCheck identifies the vital checks on platforms where registers are hardware-protected with parity or ECC, when there are redundant checks and when checks appear in loops. ESoftCheck also provides knobs to trade reliability for performance based on the support for recovery and the degree of trustiness of the operations. Our experimental results on a Pentium 4 show that ESoftCheck can obtain 27.1\% performance improvement without losing fault coverage.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {35--46},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.14},
 doi = {http://dx.doi.org/10.1109/CGO.2009.14},
 acmid = {1545051},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {ESoftCheck, non-vital checks, fault tolerance},
} 

@inproceedings{Zhang:2009:ATD:1545006.1545052,
 author = {Zhang, Xiangyu and Navabi, Armand and Jagannathan, Suresh},
 title = {Alchemist: A Transparent Dependence Distance Profiling Infrastructure},
 abstract = {Effectively migrating sequential applications to take advantage of parallelism available on multicore platforms is a well-recognized challenge. This paper addresses important aspects of this issue by proposing a novel profiling technique to automatically detect available concurrency in C programs. The profiler, called Alchemist, operates completely transparently to applications, and identifies constructs at various levels of granularity (e.g., loops, procedures, and conditional statements) as candidates for asynchronous execution. Various dependences including read-after-write (RAW), write-after-read (WAR), and write-after-write (WAW), are detected between a construct and its continuation, the execution following the completion of the construct. The time-ordered \em distance between program points forming a dependence gives a measure of the effectiveness of parallelizing that construct, as well as identifying the transformations necessary to facilitate such parallelization. Using the notion of post-dominance, our profiling algorithm builds an execution index tree at run-time. This tree is used to differentiate among multiple instances of the same static construct, and leads to improved accuracy in the computed profile, useful to better identify constructs that are amenable to parallelization. Performance results indicate that the profiles generated by Alchemist pinpoint strong candidates for parallelization, and can help significantly ease the burden of application migration to multicore environments.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {47--58},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.15},
 doi = {http://dx.doi.org/10.1109/CGO.2009.15},
 acmid = {1545052},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {execution indexing, parallelization, profiling, program dependence},
} 

@inproceedings{Berube:2009:WRM:1545006.1545057,
 author = {Berube, Paul and Amaral, Jose Nelson and Ho, Rayson and Silvera, Raul},
 title = {Workload Reduction for Multi-input Feedback-Directed Optimization},
 abstract = {Feedback-directed optimization is an effective technique to improve program performance, but it may result in program performance and compiler behavior that is sensitive to both the selection of inputs used for training and the actual input in each run of the program. Cross-validation over a workload of inputs can address the input-sensitivity problem, but introduces the need to select a representative workload of minimal size from the population of available inputs. We present a compiler-centric clustering methodology to group similar inputs so that redundant inputs can be eliminated from the training workload. Input similarity is determined based on the compile-time code transformations made by the compiler after training separately on each input. Differences between inputs are weighted by a performance metric based on cross-validation in order to account for code transformation differences that have little impact on performance. We introduce the CrossError metric that allows the exploration of correlations between transformations based on the results of clustering. The methodology is applied to several SPEC benchmark programs, and illustrated using selected case studies.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {59--69},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.23},
 doi = {http://dx.doi.org/10.1109/CGO.2009.23},
 acmid = {1545057},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {compilers, workload reduction, clustering, feedback-directed optimization},
} 

@inproceedings{Roy:2009:PKP:1545006.1545058,
 author = {Roy, Subhajit and Srikant, Y. N.},
 title = {Profiling k-Iteration Paths: A Generalization of the Ball-Larus Profiling Algorithm},
 abstract = {The Ball-Larus path-profiling algorithm is an efficient technique to collect acyclic path frequencies of a program. However, longer paths \&#8212; those extending across loop iterations \&#8212; describe the runtime behaviour of programs better. We generalize the Ball-Larus profiling algorithm for profiling k-iteration paths \&#8212; paths that can span up to to k iterations of a loop. We show that it is possible to number such k-iteration paths perfectly, thus allowing for an efficient profiling algorithm for such longer paths. We also describe a scheme for mixed-mode profiling: profiling different parts of a procedure with different path lengths. Experimental results show that k-iteration profiling is realistic.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {70--80},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.11},
 doi = {http://dx.doi.org/10.1109/CGO.2009.11},
 acmid = {1545058},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Leather:2009:AFG:1545006.1545059,
 author = {Leather, Hugh and Bonilla, Edwin and O'Boyle, Michael},
 title = {Automatic Feature Generation for Machine Learning Based Optimizing Compilation},
 abstract = {Recent work has shown that machine learning can automate and in some cases outperform hand crafted compiler optimizations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This paper develops a novel mechanism to automatically find those features which most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modeling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCC's hard-coded heuristic achieves only 3\% of the maximum performance available, while a state of the art machine learning approach with hand-coded features obtains 59\%. Our feature generation technique is able to achieve 76\% of the maximum available speedup, outperforming existing approaches.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {81--91},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.21},
 doi = {http://dx.doi.org/10.1109/CGO.2009.21},
 acmid = {1545059},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Mao:2009:CLD:1545006.1545060,
 author = {Mao, Feng and Shen, Xipeng},
 title = {Cross-Input Learning and Discriminative Prediction in Evolvable Virtual Machines},
 abstract = {Modern languages like Java and C# rely on dynamic optimizations in virtual machines for better performance. Current dynamic optimizations are reactive. Their performance is constrained by the dependence on runtime sampling and the partial knowledge of the execution. This work tackles the problems by developing a set of techniques that make a virtual machine evolve across production runs. The virtual machine incrementally learns the relation between program inputs and optimization strategies so that it proactively predicts the optimizations suitable for a new run. The prediction is discriminative, guarded by confidence measurement through dynamic self-evaluation. We employ an enriched extensible specification language to resolve the complexities in program inputs. These techniques, implemented in Jikes RVM, produce significant performance improvement on a set of Java applications.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {92--101},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.10},
 doi = {http://dx.doi.org/10.1109/CGO.2009.10},
 acmid = {1545060},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Cross-Input Learning, Java Virtual Machine, Evolvable Computing, Adaptive Optimization, Input-Centric Optimization, Discriminative Prediction},
} 

@inproceedings{Voronenko:2009:CGG:1545006.1545061,
 author = {Voronenko, Yevgen and de Mesmay, Fr\'{e}d\'{e}ric and P\"{u}schel, Markus},
 title = {Computer Generation of General Size Linear Transform Libraries},
 abstract = {The development of high-performance libraries has become extraordinarily difficult due to multiple processor cores, vector instruction sets, and deep memory hierarchies. Often, the library has to be reimplemented and reoptimized, when a new platform is released. In this paper we show how to automatically generate general input-size libraries for the domain of linear transforms. The input to our generator is a formal specification of the transform and the recursive algorithms the library should use; the output is a library that supports general input size, is vectorized and multithreaded, provides an adaptation mechanism for the memory hierarchy, and has excellent performance, comparable to or better than the best human-written libraries. Further, we show that our library generator enables various customizations; one example is the generation of Java libraries.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {102--113},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.33},
 doi = {http://dx.doi.org/10.1109/CGO.2009.33},
 acmid = {1545061},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Linear transform, discrete Fourier transform, DFT, domain-specific language, program generation, automatic performance tuning, multithreading, SIMD vector instructions},
} 

@inproceedings{Boissinot:2009:ROT:1545006.1545063,
 author = {Boissinot, Benoit and Darte, Alain and Rastello, Fabrice and de Dinechin, Benoit Dupont and Guillon, Christophe},
 title = {Revisiting Out-of-SSA Translation for Correctness, Code Quality and Efficiency},
 abstract = {Static single assignment (SSA) form is an intermediate program representation in which many code optimizations can be performed with fast and easy-to-implement algorithms. However, some of these optimizations create situations where the SSA variables arising from the same original variable now have overlapping live ranges. This complicates the translation out of SSA code into standard code. There are three issues to consider: correctness, code quality (elimination of copies), and algorithm efficiency (speed and memory footprint). Briggs et al. proposed patches to correct the initial approach of Cytron et al. A cleaner and more general approach was proposed by Sreedhar et al., along with techniques to reduce the number of generated copies. We propose a new approach based on coalescing and a precise view of interferences, in which correctness and optimizations are separated. Our approach is provably correct and simpler to implement, with no patches or particular cases as in previous solutions, while reducing the number of generated copies. Also, experiments with SPEC CINT2000 show that it is 2x faster and 10x less memory-consuming than the Method III of Sreedhar et al., which makes it suitable for just-in-time compilation.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {114--125},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.19},
 doi = {http://dx.doi.org/10.1109/CGO.2009.19},
 acmid = {1545063},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {SSA form, Compilers, JIT-compilation},
} 

@inproceedings{Pereira:2009:WPD:1545006.1545062,
 author = {Pereira, Fernando Magno Quintao and Berlin, Daniel},
 title = {Wave Propagation and Deep Propagation for Pointer Analysis},
 abstract = {This paper describes two new algorithms for solving inclusion based points-to analysis. The first algorithm, the Wave Propagation Method, is a modified version of an early technique presented by Pearce et al; however, it greatly improves on the running time of its predecessor. The second algorithm, the Deep Propagation Method, is a more light-weighted analysis, that requires less memory. We have compared these algorithms with three state-of-the-art techniques by Hardekopf-Lin, Heintze-Tardieu and Pearce-Kelly-Hankin. Our experiments show that Deep Propagation has the best average execution time across a suite of 17 well-known benchmarks, the lowest memory requirements in absolute numbers, and the fastest absolute times for benchmarks under 100,000 lines of code. The memory-hungry Wave Propagation has the fastest absolute running times in a memory rich execution environment, matching the speed of the best known points-to analysis algorithms in large benchmarks.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {126--135},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.9},
 doi = {http://dx.doi.org/10.1109/CGO.2009.9},
 acmid = {1545062},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Pointer analysis, Inclusion based, Context insensitive, Cycle detection},
} 

@inproceedings{Lokuciejewski:2009:FPS:1545006.1545064,
 author = {Lokuciejewski, Paul and Cordes, Daniel and Falk, Heiko and Marwedel, Peter},
 title = {A Fast and Precise Static Loop Analysis Based on Abstract Interpretation, Program Slicing and Polytope Models},
 abstract = {A static loop analysis is a program analysis computing loop iteration counts. This information is crucial for different fields of applications. In the domain of compilers, the knowledge about loop iterations can be exploited for aggressive loop optimizations like Loop Unrolling. A loop analyzer also provides static information about code execution frequencies which can assist feedback-directed optimizations. Another prominent application is the static worst-case execution time (WCET) analysis which relies on a safe approximation of loop iteration counts.In this paper, we propose a framework for a static loop analysis based on Abstract Interpretation, a theory of a sound approximation of program semantics. To accelerate the analysis, we preprocess the analyzed code using Program Slicing, a technique that removes statements irrelevant forthe loop analysis. In addition, we introduce a novel polytope-based loop evaluation that further significantly reduces the analysis time. The efficiency of our loop analyzer is evaluated on a large number of benchmarks. Results show that 99\% of the considered loops could be successfully analyzed in an acceptable amount of time. This study points out that our methodology is best suited for real-world problems.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {136--146},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.17},
 doi = {http://dx.doi.org/10.1109/CGO.2009.17},
 acmid = {1545064},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {loop analysis, static program analysis, abstract interpretation, WCET analysis},
} 

@inproceedings{Baev:2009:TRR:1545006.1545065,
 author = {Baev, Ivan D.},
 title = {Techniques for Region-Based Register Allocation},
 abstract = {Register allocation is an important component of every compiler and numerous studies have investigated ways to improve allocation quality or reduce allocation time. However, techniques proposed to reduce register allocation time tend to have a detrimental impact on application run-time. In this paper we propose three enhancements to region-based register allocation that not only provide scalable allocation times across multiple applications, but also improve application run-time. We have developed a register pressure based model to determine when using multiple regions is profitable, proposed the use of different regions for each register class, and designed a new region formation algorithm. Our implementation in HP-UX C/C++/Fortran production compilers led to 29\% allocation time and 1.4\% run-time improvements over a global allocator on the SPEC2006FP suite of benchmarks.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {147--156},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.31},
 doi = {http://dx.doi.org/10.1109/CGO.2009.31},
 acmid = {1545065},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {register allocation, region-based compilation},
} 

@inproceedings{Kelsey:2009:FTS:1545006.1545066,
 author = {Kelsey, Kirk and Bai, Tongxin and Ding, Chen and Zhang, Chengliang},
 title = {Fast Track: A Software System for Speculative Program Optimization},
 abstract = {Fast track is a software speculation system that enables unsafe optimization of sequential code. It speculatively runs optimized code to improve performance and then checks the correctness of the speculative code by running the original program on multiple processors. We present the interface design and system implementation for Fast Track. It lets a programmer or a pro\&#64257;ling tool mark fast-track code regions and uses a run-time system to manage the parallel execution of the speculative process and its checking processes and ensures the correct display of program outputs. The core of the run-time system is a novel concurrent algorithm that balances exploitable parallelism and available processors when the fast track is too slow or too fast. The programming interface closely affects the run-time support. Our system permits both explicit and implicit end markers for speculatively optimized code regions as well as extensions that allow the use of multiple tracks and user de\&#64257;ned correctness checking. We discuss the possible uses of speculative optimization and demonstrate the effectiveness of our prototype system by examples of unsafe semantic optimization and a general system for fast memory-safety checking, which is able to reduce the checking time by factors between 2 and 7 for large sequential code on a 8-CPU system.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {157--168},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.18},
 doi = {http://dx.doi.org/10.1109/CGO.2009.18},
 acmid = {1545066},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Mars:2009:SBO:1545006.1545068,
 author = {Mars, Jason and Hundt, Robert},
 title = {Scenario Based Optimization: A Framework for Statically Enabling Online Optimizations},
 abstract = {Online optimization allows the continuous restructuring and adaptation of an executing application using live information about its execution environment. The further advancement of performance monitoring hardware presents new opportunities for online optimization techniques. While managed runtime environments lend themselves nicely to online and dynamic optimizations, such techniques remain difficult to successfully achieve at the binary level. Binary level Dynamic optimizers introduce virtual layers which at the binary level produces overhead that is often prohibitive. Another challenge comes from the lack of source level information that can significantly aid optimization.In this paper we present a new static/dynamic collaborative approach to online optimization that takes advantage of the strength of static optimization and the adaptive nature of dynamic optimization techniques. We call this hybrid optimization framework Scenario Based Optimization (SBO). Statically we multiversion and specialize functions for different dynamic scenarios that can be identified by monitoring micro-architectural events. Using these events to infer the current scenario, we dynamically reroute execution to the relevant code tuned to that scenario. We have implemented our static SBO infrastructure in GCC 4.3.1 and designed our Dynamic Introspection Engine using the Perfmon2 infrastructure. To demonstrate the effectiveness of our Scenario Based Optimization framework we have designed an SBO optimization we call the Online Aiding and Abetting of Aggressive Optimizations (OAAAO). Using SPEC2006 this optimization shows a speedup of 7\% to 10\% for a number of benchmarks and in our best case (h264ref) a 17\% speedup over native execution compiled at the -O2 optimization level.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {169--179},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.24},
 doi = {http://dx.doi.org/10.1109/CGO.2009.24},
 acmid = {1545068},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Scenario based optimization, dynamic optimization, online optimization, compilation techniques},
} 

@inproceedings{Li:2009:EMD:1545006.1545067,
 author = {Li, Jianjun and Wu, Chenggang and Hsu, Wei-Chung},
 title = {An Evaluation of Misaligned Data Access Handling Mechanisms in Dynamic Binary Translation Systems},
 abstract = {Binary translation (BT) has been an important approach to migrate application software across instruction set architectures (ISAs). Some architectures, such as X86, allow misaligned data accesses (MDAs), while most modern architectures have the alignment restriction that requires data to be aligned in memory on natural boundaries. In a binary translation system, where the source ISA allows MDA and the target ISA does not, memory operations must be carefully translated to satisfy the alignment restriction. Naive translation will cause frequent misaligned data access traps to occur at runtime on the target machine, and severely slow down the migrated application.This paper evaluates different approaches in handling MDA in binary translation systems. It also proposes a new mechanism to deal with MDAs. Measurements based on SPEC CPU2000 and CPU2006 benchmark show that the proposed approach can significantly outperform existing methods.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {180--189},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.22},
 doi = {http://dx.doi.org/10.1109/CGO.2009.22},
 acmid = {1545067},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {optimization, misaligned memory access, binary translation},
} 

@inproceedings{Cuthbertson:2009:PAH:1545006.1545069,
 author = {Cuthbertson, John and Viswanathan, Sandhya and Bobrovsky, Konstantin and Astapchuk, Alexander and Srinivasan, Eric Kaczmarek.  Uma},
 title = {A Practical Approach to Hardware Performance Monitoring Based Dynamic Optimizations in a Production JVM},
 abstract = {While the concept of online profile directed dynamic optimizations using hardware performance monitoring unit (PMU) data is not new, it has seen fairly limited or no use in commercial JVMs. The main reason behind this fact is the set of significant challenges involved in (1) obtaining low overhead and usable profiling support from the underlying platform (2) the complexity of filtering, interpreting and using precise PMU events online in a JVM environment (3) demonstrating the total runtime benefit of PMU data based optimizations above and beyond regular online profile based optimizations. In this paper we address all three challenges by presenting a practical framework for PMU data collection and use within a high performance product JVM on a highly scalable server platform. Our experiments with JavaTM workloads using the SunTM HotspotTM JDK 1.6 JVM on the Intel \&#174; Itanium \&#174; platform indicate that the hardware data collection overhead (less than 0.5\%) is not as significant as the challenge of extracting the precise information for optimization purposes. We demonstrate the feasibility of mapping the instruction IP address based hardware event information to the runtime components as well as the JIT server compiler internal data structures for use in optimizations within a dynamic environment. We also evaluated the additional performance potential of optimizations such as object co-location during garbage collection and global instruction scheduling in the JIT compiler with the use of PMU generated load latency information. Experimental results show performance improvements of up to 14\% with an average of 2.2\% across select Java server benchmarks such as SPECjbb2005[16], SPECjvm2008[17] and Dacapo[18]. These benefits were observed over and above those provided by profile guided server JVM optimizations in the absence of hardware PMU data.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {190--199},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.13},
 doi = {http://dx.doi.org/10.1109/CGO.2009.13},
 acmid = {1545069},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Optimization, Java, Hardware Performance Monitoring},
} 

@inproceedings{Udupa:2009:SPE:1545006.1545070,
 author = {Udupa, Abhishek and Govindarajan, R. and Thazhuthaveetil, Matthew J.},
 title = {Software Pipelined Execution of Stream Programs on GPUs},
 abstract = {The StreamIt programming model has been proposed to exploit parallelism in streaming applications on general purpose multi-core architectures. This model allows programmers to specify the structure of a program as a set of filters that act upon data, and a set of communication channels between them. The StreamIt graphs describe task, data and pipeline parallelism which can be exploited on modern Graphics Processing Units (GPUs), as they support abundant parallelism in hardware. In this paper, we describe the challenges in mapping StreamIt to GPUs and propose an efficient technique to software pipeline the execution of stream programs on GPUs. We formulate this problem --- both scheduling and assignment of filters to processors --- as an efficient Integer Linear Program (ILP), which is then solved using ILP solvers. We also describe a novel buffer layout technique for GPUs which facilitates exploiting the high memory bandwidth available in GPUs. The proposed scheduling utilizes both the scalar units in GPU, to exploit data parallelism, and multiprocessors, to exploit task and pipeline parallelism. Further it takes into consideration the synchronization and bandwidth limitations of GPUs, and yields speedups between 1.87X and 36.83X over a single threaded CPU.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {200--209},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.20},
 doi = {http://dx.doi.org/10.1109/CGO.2009.20},
 acmid = {1545070},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {CUDA, GPU Programming, Software Pipelining, Stream Programming},
} 

@inproceedings{Choi:2009:SCR:1545006.1545071,
 author = {Choi, Yoonseo and Lin, Yuan and Chong, Nathan and Mahlke, Scott and Mudge, Trevor},
 title = {Stream Compilation for Real-Time Embedded Multicore Systems},
 abstract = {Multicore systems have not only become ubiquitous in the desktop and server worlds, but are also becoming the standard in the embedded space. Multicore offers programability and flexibility over traditional ASIC solutions. However, many of the advantages of switching to multicore hinge on the assumption that software development is simpler and less costly than hardware development. However, the design and development of correct, high-performance, multi-threaded programs is a difficult challenge for most programmers. Stream programming is one model that has wide applicability in the multimedia, signal processing, and networking domains. Streaming is convenient for developers because it separates the creation of actors, or functions that operate on packets of data, from the flow of data through the system. However, stream compilers are generally ineffective for embedded systems because they do not handle strict resource or timing constraints. Specifically, real-time deadlines and memory size limitations are not handled by conventional stream partitioning and scheduling techniques. This paper introducesthe SPIR compiler that orchestrates the execution of streaming applications with strict memory and timing constraints. Software defined radio or SDR is chosen as the application space to illustrate the effectiveness of the compiler for mapping applications onto the IBM Cell platform.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {210--220},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.27},
 doi = {http://dx.doi.org/10.1109/CGO.2009.27},
 acmid = {1545071},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Multicore, streaming applications},
} 

@inproceedings{Serrano:2009:BAC:1545006.1545072,
 author = {Serrano, Mauricio and Zhuang, Xiaotong},
 title = {Building Approximate Calling Context from Partial Call Traces},
 abstract = {We present an approach for building calling context information useful for program understanding, performance analysis and optimizations. Our approach exploits a lightweight profiling mechanism providing partial call traces. The goal is to reconstruct calling context information as accurately as possible, and to help the user navigate through it.We propose three steps to merge partial call traces into a smaller number of partial calling context trees. We intend to minimize errors such that the final partial contexts represent actual components of the real calling context tree with a very high probability. The first step concatenates call traces based on their common sequences. The second step converts call traces into partial calling context trees, and the last step merges partial context trees through maximal matching. To gauge how well the merged trees represent the full calling context tree, several criteria are presented. Our results indicate that call traces are successfully merged into a small number of large calling context trees. The merged trees are highly accurate.We have also developed a semi-automatic tool to navigate across partial calling context trees for program understanding and performance analysis purposes. Our results for several Java benchmarks show that our merging strategies exhibit a maximum 1\% inaccuracy when compared to the exact solution.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {221--230},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.12},
 doi = {http://dx.doi.org/10.1109/CGO.2009.12},
 acmid = {1545072},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Calling Context Tree, Call Context Analysis, Program Understanding},
} 

@inproceedings{Collin:2009:TDC:1545006.1545073,
 author = {Collin, Mikael and Brorsson, Mats},
 title = {Two-Level Dictionary Code Compression: A New Scheme to Improve Instruction Code Density of Embedded Applications},
 abstract = {Dictionary code compression is a technique which has been studied as a method to reduce the energy consumed in the instruction fetch path of processors. Instructions or instruction sequences in the code are replaced with short code words. These code words are later used to index a dictionary which contains the original uncompressed instruction or an entire sequence. In this paper, we present a new method which improves on code density compared to previously published dictionary methods. It uses a two-level dictionary design and is capable of handling compression of both individual instructions and code sequences of 2-16 instructions. The two dictionaries are in separate pipeline stages and work together to decompress sequences and instructions. The impact on storage size for the dictionaries is rather small as the sequences in the dictionary are stored as individually compressed instructions, instead of normal instructions. Compared to previous dictionary code compression methods we achieve improved dynamic compression rate, potential for better performance with reasonable static compression rate and with still small dictionary size suitable for context switching.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {231--242},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.16},
 doi = {http://dx.doi.org/10.1109/CGO.2009.16},
 acmid = {1545073},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Dictionary code compression, code density optimization, code generation},
} 

@inproceedings{Schaeckeler:2009:PAR:1545006.1545074,
 author = {Schaeckeler, Stefan and Shang, Weijia},
 title = {Procedural Abstraction with Reverse Prefix Trees},
 abstract = {For memory constrained environments like embedded systems, optimization for size is often as important as, if not more important than, optimization for execution speed. A common technique for compacting code is procedural abstraction. Equivalent code fragments are identified and abstracted into a procedure. The standard algorithm for identifying these fragments is based on suffix trees. We propose in this paper the calculation of suffix trees over the program text not in the common top-down fashion, but reversed, i.e. bottom-up. With this simple modification, not only equivalent fragments can be identified, but also fragments equivalent to (possibly often differently long) suffixes of the longest fragments. A longest fragment is then abstracted, and all fragments are replaced by procedure calls to their corresponding start instruction somewhere in the abstracted procedure. This allows us to harvest more and longer fragments than with standard suffix trees, improving code size reductions on average by 8.277\% over standard suffix trees.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {243--253},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.25},
 doi = {http://dx.doi.org/10.1109/CGO.2009.25},
 acmid = {1545074},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {embedded systems, code compaction, code size reduction, post-pass optimization, procedural abstraction, suffix tree, reverse prefix tree, program visualization},
} 

@inproceedings{Moseley:2009:OPA:1545006.1545075,
 author = {Moseley, Tipp and Grunwald, Dirk and Peri, Ramesh},
 title = {OptiScope: Performance Accountability for Optimizing Compilers},
 abstract = {Compilers employ many aggressive code transformations to achieve highly optimized code. However, because of complex target architectures and unpredictable optimization interactions, these transformations may not always be beneficial. Current analysis methods measure performance at the application level and ignore optimization effects at the function and loop level. To better measure and understand these effects, we present OptiScope, a compiler independent tool to identify performance opportunities by comparing programs built with different compilers or optimization flags. The analysis includes hundreds of different metrics and uses a novel loop correlation technique for binary programs (produced from the same source by different compilers) to isolate measurements to specific regions. We present several case studies using OptiScope to identify key differences between different compiler suites, versions, and target architectures. The examples demonstrate performance improvement opportunities between 32.5\\% to 893\\% on select regions of SPEC 2006 benchmarks.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {254--264},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2009.26},
 doi = {http://dx.doi.org/10.1109/CGO.2009.26},
 acmid = {1545075},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Liu:2009:PCS:1545006.1545076,
 author = {Liu, Lixia and Rus, Silvius},
 title = {Perflint: A Context Sensitive Performance Advisor for C++ Programs},
 abstract = {We present perflint, a new industrial strength open source analysis tool that identifies suboptimal use patterns of the C++ standard library. Simply by recompiling and running on a representative input set, programmers receive context sensitive performance advice on their use of standard library data structures and algorithms. Our solution consists of collecting traces of relevant library operations and state during program execution, and then recognizing patterns for which there is a faster alternative, based on a model made of performance guarantees in the C++ language standard and machine knowledge. perflint has already found hundreds of suboptimal patterns in a set of large C++ benchmarks. In one case, following the advice and changing one line of code resulted in 17\% program run time reduction.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {265--274},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2009.36},
 doi = {http://dx.doi.org/10.1109/CGO.2009.36},
 acmid = {1545076},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {advisor, C++, STL},
} 

@inproceedings{Kumar:2009:TDD:1545006.1545077,
 author = {Kumar, Naveen and Childers, Bruce and Soffa, Mary Lou},
 title = {Transparent Debugging of Dynamically Optimized Code},
 abstract = {Debugging programs at the source level is essential in the software development cycle. With the growing importance of dynamic optimization, there is a clear need for debugging support in the presence of runtime code transformation. This paper presents a framework, called DeDoc, and lightweight techniques that allow debugging at the source level for programs that have been transformed by a trace-based binary dynamic optimizer. Our techniquesprovide full transparency and hide from the user the effect of dynamic optimizations on code statements and data values. We describe and evaluate an implementation of DeDoc and its techniques that interface a dynamic optimizer with a native debugger. Our experimental results indicate that DeDoc is able to report over 96\% of values, that are otherwise not reportable due to code transformations, and incurs less than 1\% performance overhead.},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {275--286},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2009.28},
 doi = {http://dx.doi.org/10.1109/CGO.2009.28},
 acmid = {1545077},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Dynamic optimization, software dynamic translation, debugging, binary translation},
} 

@inproceedings{2009:AI:1545006.1545078,
 title = {Author Index},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {287--},
 url = {http://dx.doi.org/10.1109/CGO.2009.34},
 doi = {http://dx.doi.org/10.1109/CGO.2009.34},
 acmid = {1545078},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2009:RP:1545006.1545079,
 title = {Roster Page},
 abstract = {},
 booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization},
 series = {CGO '09},
 year = {2009},
 isbn = {978-0-7695-3576-0},
 pages = {288--},
 url = {http://dx.doi.org/10.1109/CGO.2009.35},
 doi = {http://dx.doi.org/10.1109/CGO.2009.35},
 acmid = {1545079},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Zhuang:2008:PFP:1356058.1356060,
 author = {Zhuang, Xiaotong and Kim, Suhyun and Serrano, Mauri io and Choi, Jong-Deok},
 title = {Perfdiff: a framework for performance difference analysis in a virtual machine environment},
 abstract = {Although applications running on virtual machines, such as Java, can achieve platform independence, performance evaluation and analysis becomes difficult due to extra intermediate layers and the dynamic nature of virtual execution environment. We present a framework for analyzing performance across multiple runs of a program, possibly in dramatically different execution environments. Our framework is based upon our prior lightweight instrumentation technique for building a calling context tree (CCT) of methods at runtime. We first represent each run of a program by a CCT, annotating its edges and nodes with various performance attributes such as call counts or elapsed times. We then identify components of the CCTs that are topologically identical but with significant performance-attribute differences. Next, the topological differences of two CCTs are identified, while ignoring the performance attributes. Finally, we identify the differences in both topology and performance attributes that can be fed back to the software developers or performance analyzers for further scrutiny. We have applied our methodology to a number of well-known Java benchmarks and a large J2EE application, using call counters as the performance attribute. Our results indicate that this approach can efficiently and effectively relate differences to a small percentage of nodes on the CCT. We present an iterative framework for program analysis, where topological changes are performed to identify differences in CCTs. For most of the test programs, applying a few topological changes such as deletion, addition, and renaming of nodes - are needed to make any two CCTs from the same program identical, whereas less than 2\% of performance-attribute changes are needed to achieve a 90\% overlap of any two CCTs in performance attributes, after the two CCTs are topologically matched. We have applied our framework to identify subtle configuration differences for complex server applications.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {4--13},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356060},
 doi = {http://doi.acm.org/10.1145/1356058.1356060},
 acmid = {1356060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance comparison, virtual machine},
} 

@inproceedings{Wimmer:2008:AAI:1356058.1356061,
 author = {Wimmer, Christian and M\"{o}ssenb\"{o}ck, Hanspeter},
 title = {Automatic array inlining in java virtual machines},
 abstract = {Array inlining expands the concepts of object inlining to arrays. Groups of objects and arrays that reference each other are placed consecutively in memory so that their relative offsets are fixed, i.e. they are colocated. This allows memory loads to be replaced by address arithmetic, which reduces the costs of field and array accesses. We implemented this optimization for Sun Microsystems' Java HotSpot VM. The optimization is performed automatically and requires no actions on the part of the programmer. Arrays are frequently used for the implementation of dynamic data structures. Therefore, the length of arrays often varies, and fields referencing such arrays have to be changed whenever the array is reallocated. We present an efficient code pattern that detects these changes and allows the optimized access of such array fields. It is integrated into the array bounds check. We also claim that inlining array element objects into an array is not possible without a global data flow analysis. The evaluation shows that our dynamic approach can optimize frequently accessed fields with a reasonable low compilation and analysis overhead. The peak performance of SPECjvm98 is improved by 10\% on average, with a maximum of 25\%.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {14--23},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356061},
 doi = {http://doi.acm.org/10.1145/1356058.1356061},
 acmid = {1356061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {array inlining, garbage collection, java, just-in-time compilation, object inlining, optimization, performance},
} 

@inproceedings{Gu:2008:PAR:1356058.1356062,
 author = {Gu, Dayong and Verbrugge, Clark},
 title = {Phase-based adaptive recompilation in a JVM},
 abstract = {Modern JIT compilers often employ multi-level recompilation strategies as a means of ensuring the most used code is also the most highly optimized, balancing optimization costs and expected future performance. Accurate selection of code to compile and level of optimization to apply is thus important to performance. In this paper we investigate the effect of an improved recompilation strategy for a Java virtual machine. Our design makes use of a lightweight, low-level profiling mechanism to detect high-level, variable length phases in program execution. Phases are then used to guide adaptive recompilation choices, improving performance. We develop both an offline implementation based on trace data and a self-contained online version. Our offline study shows an average speedup of 8.7\% and up to 21\%, and our online system achieves an average speedup of 4.4\%, up to 18\%. We subject our results to extensive analysis and show that our design achieves good overall performance with high consistency despite the existence of many complex and interacting factors in such an environment.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {24--34},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1356058.1356062},
 doi = {http://doi.acm.org/10.1145/1356058.1356062},
 acmid = {1356062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive optimization, hardware counters, java, runtime technique, virtual machine},
} 

@inproceedings{Boissinot:2008:FLC:1356058.1356064,
 author = {Boissinot, Benoit and Hack, Sebastian and Grund, Daniel and Dupont de Dine hin, Beno\^{\i}t and Rastello, Fabri e},
 title = {Fast liveness checking for ssa-form programs},
 abstract = {Liveness analysis is an important analysis in optimizing compilers. Liveness information is used in several optimizations and is mandatory during the code-generation phase. Two drawbacks of conventional liveness analyses are that their computations are fairly expensive and their results are easily invalidated by program transformations. We present a method to check liveness of variables that overcomes both obstacles. The major advantage of the proposed method is that the analysis result survives all program transformations except for changes in the control-flow graph. For common program sizes our technique is faster and consumes less memory than conventional data-flow approaches. Thereby, we heavily make use of SSA-form properties, which allow us to completely circumvent data-flow equation solving. We evaluate the competitiveness of our approach in an industrial strength compiler. Our measurements use the integer part of the SPEC2000 benchmarks and investigate the liveness analysis used by the SSA destruction pass. We compare the net time spent in liveness computations of our implementation against the one provided by that compiler. The results show that in the vast majority of cases our algorithm, while providing the same quality of information, needs less time: an average speed-up of 16\%.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {35--44},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356064},
 doi = {http://doi.acm.org/10.1145/1356058.1356064},
 acmid = {1356064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilers, dominance, jit-compilation, liveness analysis, ssa form},
} 

@inproceedings{Koes:2008:NIS:1356058.1356065,
 author = {Koes, David Ryan and Goldstein, Seth Copen},
 title = {Near-optimal instruction selection on dags},
 abstract = {Instruction selection is a key component of code generation. High quality instruction selection is of particular importance in the embedded space where complex instruction sets are common and code size is a prime concern. Although instruction selection on tree expressions is a well understood and easily solved problem, instruction selection on directed acyclic graphs is NP-complete. In this paper we present NOLTIS, a near-optimal, linear time instruction selection algorithm for DAG expressions. NOLTIS is easy to implement, fast, and effective with a demonstrated average code size improvement of 5.1\% compared to the traditional tree decomposition and tiling approach.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {45--54},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356065},
 doi = {http://doi.acm.org/10.1145/1356058.1356065},
 acmid = {1356065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instruction selection},
} 

@inproceedings{Thakur:2008:CPD:1356058.1356066,
 author = {Thakur, Aditya and Govindarajan, R.},
 title = {Comprehensive path-sensitive data-flow analysis},
 abstract = {Data-flow analysis is an integral part of any aggressive optimizing compiler. We propose a framework for improving the precision of data-flow analysis in the presence of complex control-flow. We initially perform data-flow analysis to determine those control-flow merges which cause the loss in data-flow analysis precision. The control-flow graph of the program is then restructured such that performing data-flow analysis on the resulting restructured graph gives more precise results. The proposed framework is both simple, involving the familiar notion of product automata, and also general, since it is applicable to any forward data-flow analysis. Apart from proving that our restructuring process is correct, we also show that restructuring is effective in that it necessarily leads to more optimization opportunities. Furthermore, the framework handles the trade-off between the increase in data-flow precision and the code size increase inherent in the restructuring. We show that determining an optimal restructuring is NP-hard, and propose and evaluate a greedy strategy. The framework has been implemented in the Scale research compiler, and instantiated for the specific problem of Constant Propagation. On the SPECINT 2000 benchmark suite we observe an average speedup of 4\% in the running times over Wegman-Zadeck conditional constant propagation algorithm and 2\% over a purely path profile guided approach.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {55--63},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1356058.1356066},
 doi = {http://doi.acm.org/10.1145/1356058.1356066},
 acmid = {1356066},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code duplication, data-flow analysis, destructive merge, path-sensitive, precision, restructuring, split graph},
} 

@inproceedings{Salverda:2008:ACP:1356058.1356068,
 author = {Salverda, Pierre and Tu ker, Charles and Zilles, Craig},
 title = {Accurate critical path prediction via random trace construction},
 abstract = {We present a new approach to performing program analysis through profile-guided random generation of instruction traces. Using hardware support available in commercial processors, we profile the behavior of individual instructions. Then, in conjunction with the program binary, we use that information to fabricate short (1,000-instruction) traces by randomly evaluating branches in proportion to their profiled behavior. We demonstrate our technique in the context of critical path analysis, showing it can achieve the same accuracy as a hardware critical path predictor, but with lower hardware requirements. Key to achieving this accuracy is correctly identifying memory dependences in the fabricated trace, for which purpose we use a form of abstract interpretation to identify aliasing store-load pairs without explicitly profiling them. We also demonstrate that our approach is very tolerant of the quality of profile information available.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {64--73},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356068},
 doi = {http://doi.acm.org/10.1145/1356058.1356068},
 acmid = {1356068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instruction criticality, profiling, trace fabrication},
} 

@inproceedings{Saxena:2008:EFB:1356058.1356069,
 author = {Saxena, Prateek and Sekar, R and Puranik, Varun},
 title = {Efficient fine-grained binary instrumentationwith applications to taint-tracking},
 abstract = {Fine-grained binary instrumentations, such as those for taint-tracking, have become very popular in computer security due to their applications in exploit detection, sandboxing, malware analysis, etc. However, practical application of taint-tracking has been limited by high performance overheads. For instance, previous software based techniques for taint-tracking on binary code have typically slowed down programs by a factor of 3 or more. In contrast, source-code based techniques have achieved better performance using high level optimizations. Unfortunately, these optimizations are difficult to perform on binaries since much of the high level program structure required by such static analyses is lost during the compilation process. In this paper, we address this challenge by developing static techniques that can recover some of the higher level structure from x86 binaries. Our new static analysis enables effective optimizations, which are applied in the context of taint tracking. As a result, we achieve a substantial reduction in performance overheads as compared to previous works.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {74--83},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356069},
 doi = {http://doi.acm.org/10.1145/1356058.1356069},
 acmid = {1356069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary analysis/rewriting, information flow, taint tracking},
} 

@inproceedings{Lee:2008:BRA:1356058.1356070,
 author = {Lee, Edward and Zilles, Craig},
 title = {Branch-on-random},
 abstract = {We propose a new instruction, branch-on-random, that is like a standard conditional branch, except rather than specifying the condition on which the branch should be taken, it specifies a frequency at which the branch should be taken. We show that branch-on-random is useful for reducing the overhead of program instrumentation, via sampling. Specifically, branch-on-random provides an order-of-magnitude reduction in execution time overhead compared to previously proposed software-only frameworks for instrumentation sampling. Furthermore, we demonstrate that branch-on-random can be cleanly architected and implemented simply and efficiently. For simple processors, we estimate that branch-on-random can be implemented with 20 bits of state and less than 100 gates; for aggressive superscalars, this grows to less than 100 bits of state and at most a few hundred gates.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {84--93},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356070},
 doi = {http://doi.acm.org/10.1145/1356058.1356070},
 acmid = {1356070},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch, instrumentation, lfsr, profiling, pseudo-random, sampling},
} 

@inproceedings{Ketterlin:2008:PTC:1356058.1356071,
 author = {Ketterlin, Alain and Clauss, Philippe},
 title = {Prediction and trace compression of data access addresses through nested loop recognition},
 abstract = {This paper describes an algorithm that takes a trace (i.e., a sequence of numbers or vectors of numbers) as input, and from that produces a sequence of loop nests that, when run, produces exactly the original sequence. The input format is suitable for any kind of program execution trace, and the output conforms to standard models of loop nests. The first, most obvious, use of such an algorithm is for program behavior modeling for any measured quantity (memory accesses, number of cache misses, etc.). Finding loops amounts to detecting periodic behavior and provides an explanatory model. The second application is trace compression, i.e., storing the loop nests instead of the original trace. Decompression consists of running the loops, which is easy and fast. A third application is value prediction. Since the algorithm forms loops while reading input, it is able to extrapolate the loop under construction to predict further incoming values. Throughout the paper, we provide examples that explain our algorithms. Moreover, we evaluate trace compression and value prediction on a subset of the SPEC2000 benchmarks.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {94--103},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356071},
 doi = {http://doi.acm.org/10.1145/1356058.1356071},
 acmid = {1356071},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data access, nested loop recognition, trace analysis, trace compression, value prediction},
} 

@inproceedings{Winkel:2008:LSP:1356058.1356073,
 author = {Winkel, Sebastian and Krishnaiyer, Rakesh and Sampson, Robyn},
 title = {Latency-tolerant software pipelining in a production compiler},
 abstract = {In this paper we investigate the benefit of scheduling non-critical loads for a higher latency during software pipelining. "Non-critical" denotes those loads that have sufficient slack in the cyclic data dependence graph so that increasing the scheduling distance to their first use can only increase the number of stages of the software pipeline, but should not increase the lengths of the individual stages, the initiation interval (II). The associated cost is in many cases negligible, but the memory stall reduction due to improved latency coverage and load clustering in the schedule can be considerable. We first analyze benefit and cost in theory and then present how we have implemented latency-tolerant pipelining experimentally in the Intel Itanium\&#174; product compiler. A key component of the technique is the preselection of likely long-latency loads that is integrated into prefetching heuristics in the high-level optimizer. Only when applied selectively based on these prefetcher hints, the optimization gives the full benefit also without trip-count information from dynamic profiles. Experimental results show gains of up to 14\%, with an average of 2.2\%, in a wide range of SPEC\&#174; CPU2000 and CPU2006 benchmarks. These gains were realized on top of best-performing compiler options typically used for SPEC submissions.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {104--113},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356073},
 doi = {http://doi.acm.org/10.1145/1356058.1356073},
 acmid = {1356073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compiler, epic, itanium, latency-tolerant scheduling, load clustering, memory latency, memory-level parallelism, modulo scheduling, prefetching, software pipelining},
} 

@inproceedings{Raman:2008:PDS:1356058.1356074,
 author = {Raman, Easwaran and Ottoni, Guilherme and Raman, Arun and Bridges, Matthew J. and August, David I.},
 title = {Parallel-stage decoupled software pipelining},
 abstract = {In recent years, the microprocessor industry has embraced chip multiprocessors (CMPs), also known as multi-core architectures, as the dominant design paradigm. For existing and new applications to make effective use of CMPs, it is desirable that compilers automatically extract thread-level parallelism from single-threaded applications. DOALL is a popular automatic technique for loop-level parallelization employed successfully in the domains of scientific and numeric computing. While DOALL generally scales well with the number of iterations of the loop, its applicability is limited by the presence of loop-carried dependences. A parallelization technique with greater applicability is decoupled software pipelining (DSWP), which parallelizes loops even in the presence of loop-carried dependences. However, the scalability of DSWP is limited by the size of the loop body and the number of recurrences it contains, which are usually smaller than the loop iteration count. This work proposes a novel non-speculative compiler parallelization technique called parallel-stage decoupled software pipelining (PS-DSWP). The goal of PS-DSWP is to combine the applicability of DSWP with the scalability of DOALL parallelization. A key insight of PS-DSWP is that, after isolating the recurrences in their own stages in DSWP, portions of the loop suitable for DOALL parallelization may be exposed. PS-DSWP extends DSWP to benefit from these opportunities, utilizing multiple threads to execute the same stage of a DSWPed loop in parallel. This paper describes the PS-DSWP transformation in detail and discusses its implementation in a research compiler. PS-DSWP produces an average speedup of 114\% (up to a maximum of 155\%) with 6 threads on loops from a set of 5 applications. Our experiments also demonstrate that PS-DSWP achieves better scalability with the number of threads than DSWP.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {114--123},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356074},
 doi = {http://doi.acm.org/10.1145/1356058.1356074},
 acmid = {1356074},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, doall, dswp, multi-core architectures, pipelined parallelism, tlp},
} 

@inproceedings{Fan:2008:MSH:1356058.1356075,
 author = {Fan, Kevin and Park, Hyun hul and Kudlur, Manjunath and Mahlke, S ott},
 title = {Modulo scheduling for highly customized datapaths to increase hardware reusability},
 abstract = {In the embedded domain, custom hardware in the form of ASICs is often used to implement critical parts of applications when performance and energy efficiency goals cannot be met with software implementations on a general purpose processor or DSP. The downsides of using ASICs include high non-recurring engineering costs, inability to accommodate changes in the application after production, and inability to reuse hardware for new applications. However, by allowing a degree of post-programmability, the hardware can retain high performance and energy efficiency while increasing flexibility and reusability. The difficulty with programmable custom hardware lies in mapping new applications onto an existing datapath that is both sparse and irregular. This paper proposes a constraint-driven modulo scheduler that maps software-pipelineable loops onto programmable loop accelerator hardware. The scheduler is able to target accelerators with widely varying levels of datapath functional capability and connectivity, and thus, varying degrees of programmability. The paper investigates the ability of the scheduler to map new loops onto existing hardware, which depends on both the degree of programmability of the hardware as well as the similarity of the new loop to the original loop for which the hardware was designed.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {124--133},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356075},
 doi = {http://doi.acm.org/10.1145/1356058.1356075},
 acmid = {1356075},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop accelerator, modulo scheduling, programmable asic},
} 

@inproceedings{Sundaresan:2008:RRV:1356058.1356077,
 author = {Sundaresan, Vijay and Stoodley, Mark and Ramarao, Pramod},
 title = {Removing redundancy via exception check motion},
 abstract = {Partial redundancy elimination aims to reduce the number of times an expression is computed more than once. The traditional Lazy Code Motion (LCM) algorithm formulated by Knoop, Ruthing and Steffen, through its reliance on unordered bit vectors, is severely limited in its ability to remove redundancy when precise exception semantics are required because bit vectors cannot express the order of exception checks. This paper describes our new PRE algorithm Exception Check Motion that uses the LCM algorithm to treat and optimize exception checks in a similar way to any other expression. Unlike earlier techniques that can remove only the compare instruction of a partially redundant exception check, our solution can eliminate both the compare and trap instructions without any run time code patching or expensive recovery operations. Since it is the trap instructions that restrict subsequent code motions, our technique gives downstream optimizations more flexibility to improve the performance of the resulting code once the partially redundant checks are eliminated. Our analysis has been implemented in the IBM\&#174; Testarossa (TR) just-in-time (JIT) compiler in the IBM Developer Kit for Java Release 5.0 as part of the J9 Virtual Machine. We measure performance improvements up to 7.6\% and averaging 2.5\% across 22 SPEC and DaCapo benchmarks on 4-way IBM pSeries (PowerPC) hardware.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356077},
 doi = {http://doi.acm.org/10.1145/1356058.1356077},
 acmid = {1356077},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, just-in-time compilation, partial redundancy elimination.},
} 

@inproceedings{Murphy:2008:FCM:1356058.1356078,
 author = {Murphy, Brian R. and Menon, Vijay and Schneider, Florian T. and Shpeisman, Tatiana and Adl-Tabatabai, Ali-Reza},
 title = {Fault-safe code motion for type-safe languages},
 abstract = {Compilers for Java and other type-safe languages have historically worked to overcome overheads and constraints imposed by runtime safety checks and precise exception semantics. We instead exploit these safety properties to perform code motion optimizations that are even more aggressive than those possible in unsafe languages such as C++. We present a novel framework for speculative motion of dangerous (potentially faulting) instructions in safe, object-oriented languages such as Java and C#. Unlike earlier work, our approach requires no hardware or operating system support. We leverage the properties already provided by a safe language to define fault safety, a more precise notion of safety that guarantees that a dangerous operation (e.g., a memory load) will not fault at a given program point. We illustrate how typical code motion optimizations are easily adapted to exploit our safety framework. First, we modify the standard SSAPRE partial redundancy elimination (PRE) algorithm to use fault safety, rather than the traditional down safety property. Our modified algorithm better exploits profile information by inserting of dangerous instructions on new paths when it is profitable and provably safe. Second, we extend an instruction trace scheduler to use fault safety to safely schedule load instructions across branches to better tolerate memory latency and to more compactly target instruction slots. We implemented these optimizations in StarJIT, a dynamic compiler, and show performance benefits of up to 10\% on a set of standard Java benchmarks.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {144--154},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1356058.1356078},
 doi = {http://doi.acm.org/10.1145/1356058.1356078},
 acmid = {1356078},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code motion, intermediate representations, partial redundancy elimination, safe code motion, safety dependences, scheduling, speculative code motion},
} 

@inproceedings{Chen:2008:PIR:1356058.1356079,
 author = {Chen, Tong and Zhang, Tao and Sura, Zehra and Tallada, Mar Gonzales},
 title = {Prefetching irregular references for software cache on cell},
 abstract = {The IBM Single Source Research Compiler for the Cell processor (the SSC Research Compiler) was developed to manage the complexity of programming the heterogeneous multicore Cell processor. The compiler accepts conventional source programs as input, and automatically generates binaries that execute on both the PPU and SPU cores available on a Cell chip. The compiler uses a software cache and direct buffers to manage data in the small local memory of SPUs. However, irregular references, such as a[ind[i]], often become performance bottle-necks. These references are accessed through software cache, usually with high miss rates. To solve this problem, we propose a method to prefetch irregular references accessed through a software cache that is built upon hardware such as Cell. This method includes code transformation in the compiler and a runtime library component for the software cache. Our design simplifies the synchronization required when prefetching into software cache, overlaps DMA operations for misses, and avoids frequent context switching to the miss handler. It also minimizes the cache pollution caused by prefetching, by looking both forwards and backwards through the sequence of addresses to be prefetched. We evaluated our prefetching method using the NAS benchmarks. We found that when applicable, our prefetching can improve the performance of some benchmarks by 2 times on average, and by close to 4 times in the best case. We also present data to show the impact of different configurations and optimizations when prefetching in a software cache.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {155--164},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356079},
 doi = {http://doi.acm.org/10.1145/1356058.1356079},
 acmid = {1356079},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DMA, prefetch, software cache},
} 

@inproceedings{Hoste:2008:CCO:1356058.1356080,
 author = {Hoste, Kenneth and Eeckhout, Lieven},
 title = {Cole: compiler optimization level exploration},
 abstract = {Modern compilers implement a large number of optimizations which all interact in complex ways, and which all have a different impact on code quality, compilation time, code size, energy consumption, etc. For this reason, compilers typically provide a limited number of standard optimization levels, such as -O1, -O2, -O3 and -Os, that combine various optimizations providing a number of trade-offs between multiple objective functions (such as code quality, compilation time and code size). The construction of these optimization levels, i.e., choosing which optimizations to activate at each level, is a manual process typically done using high-level heuristics based on the compiler developer's experience. This paper proposes COLE, Compiler Optimization Level Exploration, a framework for automatically finding Pareto optimal optimization levels through multi-objective evolutionary searching. Our experimental results using GCC and the SPEC CPU benchmarks show that the automatic construction of optimization levels is feasible in practice, and in addition, yields better optimization levels than GCC's manually derived (-Os, -O1, -O2 and -O3) optimization levels, as well as the optimization levels obtained through random sampling. We also demonstrate that COLE can be used to gain insight into the effectiveness of compiler optimizations as well as to better understand a benchmark's sensitivity to compiler optimizations.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {165--174},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356080},
 doi = {http://doi.acm.org/10.1145/1356058.1356080},
 acmid = {1356080},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler optimization, multi-objective search},
} 

@inproceedings{Raman:2008:SSP:1356058.1356082,
 author = {Raman, Easwaran and Va hharajani, Neil and Rangan, Ram and August, David I.},
 title = {Spice: speculative parallel iteration chunk execution},
 abstract = {The recent trend in the processor industry of packing multiple processor cores in a chip has increased the importance of automatic techniques for extracting thread level parallelism. A promising approach for extracting thread level parallelism in general purpose applications is to apply memory alias or value speculation to break dependences amongst threads and executes them concurrently. In this work, we present a speculative parallelization technique called Speculative Parallel Iteration Chunk execution (Spice) which relies on a novel software-only value prediction mechanism. Our value prediction technique predicts the loop live-ins of only a few iterations of a given loop, enabling speculative threads to start from those iterations. It also increases the probability of successful speculation by only predicting that the values will be used as live-ins in some future iterations of the loop. These twin properties enable our value prediction scheme to have high prediction accuracies while exposing significant coarse-grained thread-level parallelism. Spice has been implemented as an automatic transformation in a research compiler. The technique results in up to 157\% speedup (101\% on average) with 4 threads.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356082},
 doi = {http://doi.acm.org/10.1145/1356058.1356082},
 acmid = {1356082},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic paralleization, multicore architectures, speculative parallelization, thread level parallelism, value speculation},
} 

@inproceedings{Zhao:2008:PPP:1356058.1356083,
 author = {Zhao, Qin and Cutcutache, Ioana and Wong, Weng-Fai},
 title = {Pipa: pipelined profiling and analysis on multi-core systems},
 abstract = {Dynamic instrumentation systems are gaining popularity as means of constructing customized program profiling and analysis tools. However, dynamic instrumentation based analysis tools still suffer from performance problems. The overhead of such systems can be broken down into two components - the overhead of dynamic instrumentation and the time consumed in the user-defined analysis tools. While important progress has been made in reducing the performance penalty of the dynamic instrumentation itself, less attention has been paid to the user-defined component. In this paper, we present PiPA - Pipelined Profiling and Analysis, which is a novel technique for parallelizing dynamic program profiling and analysis by taking advantage of multi-core systems. We implemented a prototype of PiPA using the dynamic instrumentation system DynamoRIO. Our experiments show that PiPA is able to speed up the overall profiling and analysis tasks significantly. Compared to the more than 100x slowdown of Cachegrind and the 32x slowdown of Pin dcache, we achieved a mere 10.5x slowdown on an 8-core system.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {185--194},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356083},
 doi = {http://doi.acm.org/10.1145/1356058.1356083},
 acmid = {1356083},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analysis, dynamic instrumentation, multi-core systems, parallel cache simulation, pipelining, profiling},
} 

@inproceedings{Ryoo:2008:POS:1356058.1356084,
 author = {Ryoo, Shane and Rodrigues, Christopher I. and Stone, Sam S. and Baghsorkhi, Sara S. and Ueng, Sain-Zee and Stratton, John A. and Hwu, Wen-mei W.},
 title = {Program optimization space pruning for a multithreaded gpu},
 abstract = {Program optimization for highly-parallel systems has historically been considered an art, with experts doing much of the performance tuning by hand. With the introduction of inexpensive, single-chip, massively parallel platforms, more developers will be creating highly-parallel applications for these platforms, who lack the substantial experience and knowledge needed to maximize their performance. This creates a need for more structured optimization methods with means to estimate their performance effects. Furthermore these methods need to be understandable by most programmers. This paper shows the complexity involved in optimizing applications for one such system and one relatively simple methodology for reducing the workload involved in the optimization process. This work is based on one such highly-parallel system, the GeForce 8800 GTX using CUDA. Its flexible allocation of resources to threads allows it to extract performance from a range of applications with varying resource requirements, but places new demands on developers who seek to maximize an application's performance. We show how optimizations interact with the architecture in complex ways, initially prompting an inspection of the entire configuration space to find the optimal configuration. Even for a seemingly simple application such as matrix multiplication, the optimal configuration can be unexpected. We then present metrics derived from static code that capture the first-order factors of performance. We demonstrate how these metrics can be used to prune many optimization configurations, down to those that lie on a Pareto-optimal curve. This reduces the optimization space by as much as 98\% and still finds the optimal configuration for each of the studied applications.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {195--204},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1356058.1356084},
 doi = {http://doi.acm.org/10.1145/1356058.1356084},
 acmid = {1356084},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gpgpu, optimization, parallel computing},
} 

@inproceedings{Hampton:2008:CVA:1356058.1356085,
 author = {Hampton, Mark and Asanovic, Krste},
 title = {Compiling for vector-thread architectures},
 abstract = {Vector-thread (VT) architectures exploit multiple forms of parallelism simultaneously. This paper describes a compiler for the Scale VT architecture, which takes advantage of the VT features. We focus on compiling loops, and show how the compiler can transform code that poses difficulties for traditional vector or VLIW processors, such as loops with internal control flow or cross-iteration dependences, while still taking advantage of features not supported by multithreaded designs, such as vector memory instructions. We evaluate the compiler using several embedded benchmarks and show that we can obtain substantial speedups over a single-issue, in-order scalar machine.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {205--215},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1356058.1356085},
 doi = {http://doi.acm.org/10.1145/1356058.1356085},
 acmid = {1356085},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compilers, vector processors},
} 

@inproceedings{Sarkar:2008:COP:1356058.1356087,
 author = {Sarkar, Vivek},
 title = {Code optimization of parallel programs: evolutionary vs. revolutionary approaches},
 abstract = {Code optimization has a rich history that dates back over half a century. Over the years, it has contributed deep innovations to address challenges posed by new computer system and programming language features. Examples of the former include optimizations for improved register utilization, instruction-level parallelism, vector parallelism, multiprocessor parallelism and memory hierarchy utilization. Examples of the latter include optimizations for procedural, object-oriented, functional and domain-specific languages as well as dynamic optimization for managed runtimes. These optimizations have contributed significantly to programmer productivity by reducing the effort that programmers need to spend on hand-implementing code optimizations and by enabling code to be more portable, especially as programming models and computer architectures change. While compiler frameworks are often able to incorporate new code optimizations in an evolutionary manner, there have been notable periods in the history of compilers when more revolutionary changes were necessary. Examples of such paradigm shifts in the history of compilers include interprocedural whole program analysis, coloring-based register allocation, static single assignment form, array dependence analysis, pointer alias analysis, loop transformations, adaptive profile-directed optimizations, and dynamic compilation. The revolutionary nature of these shifts is evidenced by the fact that production-strength optimization frameworks (especially those in industry) had to be rewritten from scratch or significantly modified to support the new capabilities. In this talk, we claim that the current multicore trend in the computer industry is forcing a new paradigm shift in compilers to address the challenge of code optimization of parallel programs, regardless of whether the parallelism is implicit or explicit in the programming model. All computers --- embedded, mainstream, and high-end --- are now being built from multicore processors with little or no increase in clock speed per core. This trend poses multiple challenges for compilers for future systems as the number of cores per socket continues to grow, and the cores become more heterogeneous. In addition, compilers have to keep pace with emerging parallel programming models embodied in a proliferation of new libraries and new languages. To substantiate our claim, we examine the historical foundations of code optimization including intermediate representations (IR's), abstract execution models, legality and cost analyses of IR transformations and show that they are all deeply entrenched in the von Neumann model of sequential computing. We discuss ongoing evolutionary efforts to support optimization of parallel programs in the context of existing compiler frameworks, and their inherent limitations for the long term. We then outline what a revolutionary approach will entail, and identify where its underlying paradigm shifts are likely to lie. We provide examples of past research that are likely to influence future directions in code optimization of parallel programs such as program dependence graphs, partitioning and scheduling of lightweight parallelism, synchronization optimizations, communication optimizations, transactional memory optimizations, code generation for heterogeneous accelerators, impact of memory models on code optimization, and general forms of data and computation alignment. Finally, we briefly describe the approach to code optimization of parallel programs being taken in the Habanero Multicore Software Research project at Rice University.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1356058.1356087},
 doi = {http://doi.acm.org/10.1145/1356058.1356087},
 acmid = {1356087},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code optimization, multicore processors, parallel programs},
} 

@inproceedings{Rubin:2008:ICC:1356058.1356088,
 author = {Rubin, Norm},
 title = {Issues and challenges in compiling for graphics processors},
 abstract = {Graphics has been one of the best success stories of parallel processing. Using a unique combination of specialized hardware and aspecialized programming model, game developers routinely write high performance code using millions of threads. Each Generation of graphic processors (GPU's) delivers higher performance and is more programmable then the last. Unlike CPU's, these processors are designed from the beginning to run highly parallel programs and fit a different programming model. We would claim that a GPU is much more like a traditional supercomputer then a desktop processor. The programming model for graphics, called the graphics pipeline, is quite different from any of the CPU models. We will include a quick overview of this model. We believe that the Graphics programming model for utilizing the GPU will continue to be radically different from the CPU programming model for some time to come. The key feature that makes games perform well (at least to compiler writers) is that games are shipped as byte code and JIT compiled each time the game is started. The main focus of this talk is the AMD provided JIT compiler. We will describe some of the design decisions made in the JIT compiler and give some statistics on how well they worked. For instance we will explain what the compiler does with 256 thousand registers and why it would like more. Finally we will highlight some of the open research problems raised by graphics hardware.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {2--2},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1356058.1356088},
 doi = {http://doi.acm.org/10.1145/1356058.1356088},
 acmid = {1356088},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilers, graphics},
} 

@inproceedings{Griesemer:2008:PDD:1356058.1356089,
 author = {Griesemer, Robert},
 title = {Parallelism by design: data analysis with sawzall},
 abstract = {Very large data sets - telephone call records, network logs, high-resolution satellite images, or web document repositories - are not easily analyzed using traditional database techniques. They may be simply too large, grow too fast, or may not fit well in a database schema. They tend to span multiple disks and machines. On the other hand, these large data sets often have a flat and regular structure that permits distributed filtering and aggregation. We present a system and language for such analyses*. Altering phase, in which a query is expressed using the procedural programming language Sawzall, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The language constructs and execution model of Sawzall have been devised to enable parallel execution without the need for complex dependency analysis. Even with our fairly traditional implementation of the Sawzall execution engine we observe nearly perfect scalability as we add more machines. *Joint work with Sean Dorward, Rob Pike, and Sean Quinlan.},
 booktitle = {Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '08},
 year = {2008},
 isbn = {978-1-59593-978-4},
 location = {Boston, MA, USA},
 pages = {3--3},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1356058.1356089},
 doi = {http://doi.acm.org/10.1145/1356058.1356089},
 acmid = {1356089},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {sawzall},
} 

@inproceedings{Zorn:2010:PDL:1772954.1772955,
 author = {Zorn, Benjamin},
 title = {Performance is dead, long live performance!},
 abstract = {In a world of social networking, security attacks, and hot mobile phones, the importance of application performance appears to have dimin-ished. My own research agenda has shifted from looking at the performance of memory allocation to building runtime systems that are more resilient to data corruption and security attacks. In my talk, I will outline a number of areas where code-generation and runtime tech-niques can be successfully applied to areas for purposes other than performance, such as fault tolerance, reliability, and security. Along the way, I will consider such questions as "Does it really matter if this corruption was caused by a software or hardware error?" and "Is it okay to let a malicious person allocate arbitrary data on my heap?". Despite these other opportunities, the importance of performance in modern applications remains undiminished, and current hardware trends place an increasing burden on software to provide needed performance boosts. In concluding, I will suggest several important trends that I believe will help define the next 10 years of code generation and optimization research.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1772954.1772955},
 doi = {http://doi.acm.org/10.1145/1772954.1772955},
 acmid = {1772955},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation},
} 

@inproceedings{Patil:2010:PFD:1772954.1772958,
 author = {Patil, Harish and Pereira, Cristiano and Stallcup, Mack and Lueck, Gregory and Cownie, James},
 title = {PinPlay: a framework for deterministic replay and reproducible analysis of parallel programs},
 abstract = {Analysis of parallel programs is hard mainly because their behavior changes from run to run. We present an execution capture and deterministic replay system that enables repeatable analysis of parallel programs. Our goal is to provide an easy-to-use framework for capturing, deterministically replaying, and analyzing execution of large programs with reasonable runtime and disk usage. Our system, called PinPlay, is based on the popular Pin dynamic instrumentation system hence is very easy to use. PinPlay extends the capability of Pin-based analysis by providing a tool for capturing one execution instance of a program (as log files called pinballs) and by allowing Pin-based tools to run off the captured execution. Most Pintools can be trivially modified to work off pinballs thus doing their usual analysis but with a guaranteed repeatability. Furthermore, the capture/replay works across operating systems (Windows to Linux) as the pinball format is independent of the operating system. We have used PinPlay to analyze and deterministically debug large parallel programs running trillions of instructions. This paper describes the design of PinPlay and its applications for analyses such as simulation point selection, tracing, and debugging.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772958},
 doi = {http://doi.acm.org/10.1145/1772954.1772958},
 acmid = {1772958},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deterministic replay, dynamic program analysis, repeatable simulation point selection, reproducible debugging},
} 

@inproceedings{Borin:2010:TTA:1772954.1772959,
 author = {Borin, Edson and Wu, Youfeng and Wang, Cheng and Liu, Wei and Breternitz,Jr., Mauricio and Hu, Shiliang and Natanzon, Esfir and Rotem, Shai and Rosner, Roni},
 title = {TAO: two-level atomicity for dynamic binary optimizations},
 abstract = {Dynamic binary translation is a key component of Hardware/Software (HW/SW) co-design, which is an enabling technology for processor microarchitecture innovation. There are two well-known dynamic binary optimization techniques based on atomic execution support. Frame-based optimizations leverage processor pipeline support to enable atomic execution of hot traces. Region level optimizations employ transactional-memory-like atomicity support to aggressively optimize large regions of code. In this paper we propose a two-level atomic optimization scheme which not only overcomes the limitations of the two approaches, but also boosts the benefits of the two approaches effectively. Our experiment shows that the combined approach can achieve a total of 21.5\% performance improvement over an aggressive out-of-order baseline machine and improve the performance over the frame-based approach by an additional 5.3\%.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772959},
 doi = {http://doi.acm.org/10.1145/1772954.1772959},
 acmid = {1772959},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atomic execution, dynamic binary optimization, hardware/software co-design, large region optimization},
} 

@inproceedings{Zhao:2010:UES:1772954.1772960,
 author = {Zhao, Qin and Bruening, Derek and Amarasinghe, Saman},
 title = {Umbra: efficient and scalable memory shadowing},
 abstract = {Shadow value tools use metadata to track properties of application data at the granularity of individual machine instructions. These tools provide effective means of monitoring and analyzing the runtime behavior of applications. However, the high runtime overhead stemming from fine-grained monitoring often limits the use of such tools. Furthermore, 64-bit architectures pose a new challenge to the building of efficient memory shadowing tools. Current tools are not able to efficiently monitor the full 64-bit address space due to limitations in their shadow metadata translation. This paper presents an efficient and scalable memory shadowing framework called Umbra. Employing a novel translation scheme, Umbra supports efficient mapping from application data to shadow metadata for both 32-bit and 64-bit applications. Umbra's translation scheme does not rely on any platform features and is not restricted to any specific shadow memory size. We also present several mapping optimizations and general dynamic instrumentation techniques that substantially reduce runtime overhead, and demonstrate their effectiveness on a real-world shadow value tool. We show that shadow memory translation overhead can be reduced to just 133\% on average.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {22--31},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772960},
 doi = {http://doi.acm.org/10.1145/1772954.1772960},
 acmid = {1772960},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic optimization, shadow memory},
} 

@inproceedings{Price:2010:LPT:1772954.1772961,
 author = {Price, Graham and Vachharajani, Manish},
 title = {Large program trace analysis and compression with ZDDs},
 abstract = {Prior work has shown that reduced, ordered, binary decision diagrams (BDDs) can be a powerful tool for program trace analysis and visualization. Unfortunately, it can take hours or days to encode large traces as BDDs. Further, techniques used to improve BDD performance are inapplicable to large dynamic program traces. This paper explores the use of ZDDs for compressing dynamic trace data. Prior work has show that ZDDs can represent sparse data sets with less memory compared to BDDs. This paper demonstrates that (1) ZDDs do indeed provide greater compression for sets of dynamic traces (25\% smaller than BDDs on average), (2) with proper tuning, ZDDs encode sets of dynamic trace data over 9x faster than BDDs, and (3) ZDDs can be used for all prior applications of BDDs for trace analysis and visualization.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {32--41},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772961},
 doi = {http://doi.acm.org/10.1145/1772954.1772961},
 acmid = {1772961},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallel programming, trace compression},
} 

@inproceedings{Chen:2010:THE:1772954.1772963,
 author = {Chen, Dehao and Vachharajani, Neil and Hundt, Robert and Liao, Shih-wei and Ramasamy, Vinodha and Yuan, Paul and Chen, Wenguang and Zheng, Weimin},
 title = {Taming hardware event samples for FDO compilation},
 abstract = {Feedback-directed optimization (FDO) is effective in improving application runtime performance, but has not been widely adopted due to the tedious dual-compilation model, the difficulties in generating representative training data sets, and the high runtime overhead of profile collection. The use of hardware-event sampling to generate estimated edge profiles overcomes these drawbacks. Yet, hardware event samples are typically not precise at the instruction or basic-block granularity. These inaccuracies lead to missed performance when compared to instrumentation-based FDO@. In this paper, we use multiple hardware event profiles and supervised learning techniques to generate heuristics for improved precision of basic-block-level sample profiles, and to further improve the smoothing algorithms used to construct edge profiles. We demonstrate that sampling-based FDO can achieve an average of 78\% of the performance gains obtained using instrumentation-based exact edge profiles for SPEC2000 benchmarks, matching or beating instrumentation-based FDO in many cases. The overhead of collection is only 0.74\% on average, while compiler based instrumentation incurs 6.8\%-53.5\% overhead (and 10x overhead on an industrial web search application), and dynamic instrumentation incurs 28.6\%-1639.2\% overhead.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {42--52},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1772954.1772963},
 doi = {http://doi.acm.org/10.1145/1772954.1772963},
 acmid = {1772963},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feedback-directed optimization, performance counters, sampling profile},
} 

@inproceedings{Li:2010:LFC:1772954.1772964,
 author = {Li, David Xinliang and Ashok, Raksit and Hundt, Robert},
 title = {Lightweight feedback-directed cross-module optimization},
 abstract = {Cross-module inter-procedural compiler optimization (IPO) and Feedback-Directed Optimization (FDO) are two important compiler techniques delivering solid performance gains. The combination of IPO and FDO delivers peak performance, but also multiplies both techniques' usability problems. In this paper, we present LIPO, a novel static IPO framework, which integrates IPO and FDO. Compared to existing approaches, LIPO no longer requires writing of the compiler's intermediate representation, eliminates the link-time inter-procedural optimization phase entirely, and minimizes code re-generation overhead, thus improving scalability by an order of magnitude. Compared to an FDO baseline, and without further specific tuning, LIPO improves performance of SPEC2006 INT by 2.5\%, and of SPEC2000 INT by 4.4\%, with up to 23\% for one benchmarks. We confirm our scalability results on a set of large industrial applications, demonstrating 2.9\% performance improvements on average. Compile time overhead for full builds is less than 30\%, incremental builds take a few seconds on average, and storage requirements increase by only 24\%, all compared to the FDO baseline.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {53--61},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772964},
 doi = {http://doi.acm.org/10.1145/1772954.1772964},
 acmid = {1772964},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cross-module, feedback-directed, inter-procedural, optimization},
} 

@inproceedings{Hoste:2010:AJC:1772954.1772965,
 author = {Hoste, Kenneth and Georges, Andy and Eeckhout, Lieven},
 title = {Automated just-in-time compiler tuning},
 abstract = {Managed runtime systems, such as a Java virtual machine (JVM), are complex pieces of software with many interacting components. The Just-In-Time (JIT) compiler is at the core of the virtual machine, however, tuning the compiler for optimum performance is a challenging task. There are (i) many compiler optimizations and options, (ii) there may be multiple optimization levels (e.g., -O0, -O1, -O2), each with a specific optimization plan consisting of a collection of optimizations, (iii) the Adaptive Optimization System (AOS) that decides which method to optimize to which optimization level requires fine-tuning, and (iv) the effectiveness of the optimizations depends on the application as well as on the hardware platform. Current practice is to manually tune the JIT compiler which is both tedious and very time-consuming, and in addition may lead to suboptimal performance. This paper proposes automated tuning of the JIT compiler through multi-objective evolutionary search. The proposed framework (i) identifies optimization plans that are Pareto-optimal in terms of compilation time and code quality, (ii) assigns these plans to optimization levels, and (iii) fine-tunes the AOS accordingly. The key benefit of our framework is that it automates the entire exploration process, which enables tuning the JIT compiler for a given hardware platform and/or application at very low cost. By automatically tuning Jikes RVM using our framework for average performance across the DaCapo and SPECjvm98 benchmark suites, we achieve similar performance to the hand-tuned default Jikes RVM. When optimizing the JIT compiler for individual benchmarks, we achieve statistically significant speedups for most benchmarks, up to 40\% for startup and up to 19\% for steady-state performance. We also show that tuning the JIT compiler for a new hardware platform can yield significantly better performance compared to using a JIT compiler that was tuned for another platform.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {62--72},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1772954.1772965},
 doi = {http://doi.acm.org/10.1145/1772954.1772965},
 acmid = {1772965},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler tuning, evolutionary search, java virtual machine (JVM), just-in-time (JIT) compiler, machine learning},
} 

@inproceedings{Jung:2010:HJC:1772954.1772966,
 author = {Jung, Dong-Heon and Moon, Soo-Mook and Oh, Hyeong-Seok},
 title = {Hybrid Java compilation and optimization for digital TV software platform},
 abstract = {The Java software platform for the interactive digital TV (DTV) is composed of the system/middleware classes statically installed on the DTV set-top box and the xlet classes dynamically downloaded from the TV stations, where xlets are executed only when the TV viewer initiates the interaction. In order to achieve high performance on this dual-component, user-initiated system, existing just-in-time compilation is not enough, but idle-time compilation and optimization as well as ahead-of-time compilation are also needed, requiring a hybrid compilation and optimization environment. We constructed such a hybrid environment for a commercial DTV software platform and experimented with real, on-air xlet applications. Our experimental results show that the proposed hybrid environment can improve the DTV Java performance by as much as an average of 150\%, compared to the JITC-only environment.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {73--81},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772966},
 doi = {http://doi.acm.org/10.1145/1772954.1772966},
 acmid = {1772966},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ahead-of-time compiler, digital TV java, idle-time compiler, just-in-time compiler, xlets},
} 

@inproceedings{Srisa-an:2010:SCC:1772954.1772968,
 author = {Srisa-an, Witawas and Cohen, Myra B. and Shang, Yu and Soundararaj, Mithuna},
 title = {A self-adjusting code cache manager to balance start-up time and memory usage},
 abstract = {In virtual machines for embedded devices that use just-in-time compilation, the management of the code cache can significantly impact performance in terms of both memory usage and start-up time. Although improving memory usage has been a common focus for system designers, start-up time is often overlooked. In systems with constrained resources, however, these two performance metrics are often at odds and must be considered together. In this paper, we present an adaptive self-adjusting code cache manager to improve performance with respect to both start-up time and memory usage. It balances these concerns by detecting changes in method compilation rates, resizing the cache after each pitching event. We conduct experiments to validate our proposed system and quantify the impacts that different code cache management techniques have on memory usage and start-up time through two oracle systems. Our results show that the proposed algorithm yields nearly the same start-up times as a hand-tuned oracle and shorter execution times than those of the SSCLI in eight out of ten applications. It also has lower memory usage over time in all but one application.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {82--91},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772968},
 doi = {http://doi.acm.org/10.1145/1772954.1772968},
 acmid = {1772968},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, code cache, embedded systems},
} 

@inproceedings{Wang:2010:IHM:1772954.1772969,
 author = {Wang, Zhenjiang and Wu, Chenggang and Yew, Pen-Chung},
 title = {On improving heap memory layout by dynamic pool allocation},
 abstract = {Dynamic memory allocation is widely used in modern programs. General-purpose heap allocators often focus more on reducing their run-time overhead and memory space utilization, but less on exploiting the characteristics of their allocated heap objects. This paper presents a lightweight dynamic optimizer, named Dynamic Pool Allocation (DPA), which aims to exploit the affinity of the allocated heap objects and improve their layout at run-time. DPA uses an adaptive partial call chain with heuristics to aggregate affinitive heap objects into dedicated memory regions, called memory pools. We examine the factors that could affect the effectiveness of such layout. We have implemented DPA and measured its performance on several SPEC CPU 2000 and 2006 benchmarks that use extensive heap objects. Evaluations show that it could achieve an average speed up of 12.1\% and 10.8\% on two x86 commodity machines respectively using GCC -O3, and up to 82.2\% for some benchmarks.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {92--100},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772969},
 doi = {http://doi.acm.org/10.1145/1772954.1772969},
 acmid = {1772969},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive partial call chain, data layout, dynamic optimization, pool allocation},
} 

@inproceedings{Gottschlich:2010:EST:1772954.1772970,
 author = {Gottschlich, Justin E. and Vachharajani, Manish and Siek, Jeremy G.},
 title = {An efficient software transactional memory using commit-time invalidation},
 abstract = {To improve the performance of transactional memory (TM), researchers have found many eager and lazy optimizations for conflict detection</i>, the process of determining if transactions can commit. Despite these optimizations, nearly all TMs perform one aspect of lazy conflict detection in the same manner to preserve serializability. That is, they perform commit-time validation</i>, where a transaction is checked for conflicts with previously committed transactions during its commit phase. While commit-time validation is efficient for workloads that exhibit limited contention, it can limit transaction throughput for contending workloads. This paper presents an efficient implementation of commit-time invalidation</i>, a strategy where transactions resolve their conflicts with in-flight (uncommitted) transactions before</i> they commit. Commit-time invalidation supplies the contention manager (CM) with data that is unavailable through commit-time validation, allowing the CM to make decisions that increase transaction throughput. Commit-time invalidation also requires notably fewer operations than commit-time validation for memory-intensive transactions, uses zero commit-time operations for dynamically detected read-only transactions, and guarantees full opacity for any transaction in O(N) time, an improvement over incremental validation's O(N<sup>2</sup>) time. Our experimental results show that for contending workloads, our efficient commit-time invalidating software TM (STM) is up to 3 x faster than TL2, a state-of-the-art validating STM.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {101--110},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772970},
 doi = {http://doi.acm.org/10.1145/1772954.1772970},
 acmid = {1772970},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {commit-time invalidation, software transactional memory},
} 

@inproceedings{Stratton:2010:ECF:1772954.1772971,
 author = {Stratton, John A. and Grover, Vinod and Marathe, Jaydeep and Aarts, Bastiaan and Murphy, Mike and Hu, Ziang and Hwu, Wen-mei W.},
 title = {Efficient compilation of fine-grained SPMD-threaded programs for multicore CPUs},
 abstract = {In this paper we describe techniques for compiling fine-grained SPMD-threaded programs, expressed in programming models such as OpenCL or CUDA, to multicore execution platforms. Programs developed for manycore processors typically express finer thread-level parallelism than is appropriate for multicore platforms. We describe options for implementing fine-grained threading in software, and find that reasonable restrictions on the synchronization model enable significant optimizations and performance improvements over a baseline approach. We evaluate these techniques in a production-level compiler and runtime for the CUDA programming model targeting modern CPUs. Applications tested with our tool often showed performance parity with the compiled C version of the application for single-thread performance. With modest coarse-grained multithreading typical of today's CPU architectures, an average of 3.4x speedup on 4 processors was observed across the test applications.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {111--119},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772971},
 doi = {http://doi.acm.org/10.1145/1772954.1772971},
 acmid = {1772971},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU, CUDA, SPMD, multicore},
} 

@inproceedings{Newburn:2010:LTS:1772954.1772956,
 author = {Newburn, Chris C.J.},
 title = {There are at least two sides to every heterogeneous system},
 abstract = {Since there are at least two sides to every heterogeneous system, optimizing for heterogeneous systems is inherently an exercise in manag-ing complexity, balanced trade-offs and layering. Efforts to make the hardware simple may result in software complexity, unless there's an abstracting software layer involved. Different customer-driven usage models make it challenging to offer a layered but consistent pro-gramming model, a cost-effective set of performance features and a flexibly-capable systems software stack. And often, the very reasons why heterogeneous systems exist drives them to change over time, making them difficult to target from a code generation perspective. As a company that provides hardware systems, compilers, systems software infrastructure and services, one of Intel's research and devel-opment focuses is on optimizing for heterogeneous systems, such as a mix of IA multi-cores architectures that are used for both graphics and throughput computing. This talk addresses some of the challenges we've encountered in that space, and offers some potential directions. Primary among the case studies used in this talk is a dynamic compiler that uses Intel's Ct technology, which strives to make it easier for programmers to specify what data-parallel work needs to be accomplished, and manages extracting parallelism from the application and making use of it on multicore and many-core Intel architectures optimized for throughput computing. The set of issues that will be ad-dressed include how to specify parallelism, safety and debugging, software infrastructure and compiler architecture, and achieving performance on heterogeneous systems.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {120--120},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1772954.1772956},
 doi = {http://doi.acm.org/10.1145/1772954.1772956},
 acmid = {1772956},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation},
} 

@inproceedings{Huang:2010:DSP:1772954.1772973,
 author = {Huang, Jialu and Raman, Arun and Jablin, Thomas B. and Zhang, Yun and Hung, Tzu-Han and August, David I.},
 title = {Decoupled software pipelining creates parallelization opportunities},
 abstract = {Decoupled Software Pipelining (DSWP) is one approach to automatically extract threads from loops. It partitions loops into long-running threads that communicate in a pipelined manner via inter-core queues. This work recognizes that DSWP can also be an enabling transformation for other loop parallelization techniques. This use of DSWP, called DSWP+, splits a loop into new loops with dependence patterns amenable to parallelization using techniques that were originally either inapplicable or poorly-performing. By parallelizing each stage of the DSWP+ pipeline using (potentially) different techniques, not only is the benefit of DSWP increased, but the applicability and performance of other parallelization techniques are enhanced. This paper evaluates DSWP+ as an enabling framework for other transformations by applying it in conjunction with DOALL, LOCALWRITE, and SpecDOALL to individual stages of the pipeline. This paper demonstrates significant performance gains on a commodity 8-core multicore machine running a variety of codes transformed with DSWP+.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772973},
 doi = {http://doi.acm.org/10.1145/1772954.1772973},
 acmid = {1772973},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DSWP, enabling transformation, multicore, parallelization, speculation},
} 

@inproceedings{SuBkraut:2010:PCF:1772954.1772974,
 author = {S\"{u}\sskraut, Martin and Knauth, Thomas and Weigert, Stefan and Schiffel, Ute and Meinhold, Martin and Fetzer, Christof},
 title = {Prospect: a compiler framework for speculative parallelization},
 abstract = {Making efficient use of modern multi-core and future many-core CPUs is a major challenge. We describe a new compiler-based platform, Prospect, that supports the parallelization of sequential applications. The underlying approach is a generalization of an existing approach to parallelize runtime checks. The basic idea is to generate two variants of the application: (1) a fast variant having bare bone functionality, and (2) a slow variant with extra functionality. The fast variant is executed sequentially. Its execution is divided into epochs. Each epoch is re-executed by an executor using the slow variant. The approach scales by running the executors on multiple cores in parallel to each other and to the fast variant. We have implemented the Prospect framework to evaluate this approach. Prospect allows custom plug-ins for generating the fast and slow variants. With the help of our novel StackLifter, a process can switch between the fast variant and the slow variant during runtime at arbitrary positions.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {131--140},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772974},
 doi = {http://doi.acm.org/10.1145/1772954.1772974},
 acmid = {1772974},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assertions, bounds checker, parallelization, speculation, stack translation},
} 

@inproceedings{Han:2010:SPP:1772954.1772975,
 author = {Han, Liang and Liu, Wei and Tuck, James M.},
 title = {Speculative parallelization of partial reduction variables},
 abstract = {Reduction variables are an important class of cross-thread dependence that can be parallelized by exploiting the associativity and commutativity of their operation. In this paper, we define a class of shared variables called partial reduction variables (PRV). These variables either cannot be proven to be reductions or they violate the requirements of a reduction variable in some way. We describe an algorithm that allows the compiler to detect PRVs, and we also discuss the necessary requirements to parallelize detected PRVs. Based on these requirements, we propose an implementation in a TLS system to parallelize PRVs that works by a combination of techniques at compile time and in the hardware. The compiler transforms the variable under the assumption that the reduction-like behavior proven statically will hold true at runtime. However, if a thread reads or updates the shared variable as a result of an alias or unlikely control path, a lightweight hardware mechanism will detect the access and synchronize it to ensure correct execution. We implement our compiler analysis and transformation in GCC, and analyze its potential on the SPEC CPU 2000 benchmarks.We find that supporting PRVs provides up to 46\% performance gain over a highly optimized TLS system and on average 10.7\% performance improvement.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772975},
 doi = {http://doi.acm.org/10.1145/1772954.1772975},
 acmid = {1772975},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-core architecture, parallelization, reduction variables, thread-level speculation},
} 

@inproceedings{Canedo:2010:APS:1772954.1772976,
 author = {Canedo, Arquimedes and Yoshizawa, Takeo and Komatsu, Hideaki},
 title = {Automatic parallelization of simulink applications},
 abstract = {The parallelization of Simulink applications is currently a responsibility of the system designer and the superscalar execution of the processors. State-of-the-art Simulink compilers excel at producing reliable and production-quality embedded code, but fail to exploit the natural concurrency available in the programs and to effectively use modern multi-core architectures. The reason may be that many Simulink applications are replete with loop-carried dependencies that inhibit most parallel computing techniques and compiler transformations. In this paper, we introduce the concept of strands that allow the data dependencies to be broken while preserving the original semantics of the Simulink program. Our fully automatic compiler transformations create a concurrent representation of the program, and thread-level parallelism for multi-core systems is planned and orchestrated. To improve single processor performance, we also exploit fine grain (equation-level) parallelism by level-order scheduling inside each thread. Our strand transformation has been implemented as an automatic transformation in a proprietary compiler and with a realistic aeronautic model executed in two processors leads to an up to 1.98 times speedup over uniprocessor execution, while the existing manual parallelization method achieves a 1.75 times speedup.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {151--159},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772976},
 doi = {http://doi.acm.org/10.1145/1772954.1772976},
 acmid = {1772976},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, coarse grain dataflow, compilers, equation-level parallelism, multi-core, simulink, strands},
} 

@inproceedings{Odaira:2010:CCG:1772954.1772978,
 author = {Odaira, Rei and Nakaike, Takuya and Inagaki, Tatsushi and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Coloring-based coalescing for graph coloring register allocation},
 abstract = {Graph coloring register allocation tries to minimize the total cost of spilled live ranges of variables. Live-range splitting and coalescing are often performed before the coloring to further reduce the total cost. Coalescing of split live ranges, called sub-ranges, can decrease the total cost by lowering the interference degrees of their common interference neighbors. However, it can also increase the total cost because the coalesced sub-ranges can become uncolorable. In this paper, we propose coloring-based coalescing, which first performs trial coloring and next coalesces all copyrelated sub-ranges that were assigned the same color. The coalesced graph is then colored again with the graph coloring register allocation. The rationale is that coalescing of differently colored sub-ranges could result in spilling because there are some interference neighbors that prevent them from being assigned the same color. Experiments on Java programs show that the combination of live-range splitting and coloring-based coalescing reduces the static spill cost by more than 6\% on average, comparing to the baseline coloring without splitting. In contrast, well-known iterated and optimistic coalescing algorithms, when combined with splitting, increase the cost by more than 20\%. Coloring-based coalescing improves the execution time by up to 15\% and 3\% on average, while the existing algorithms improve by up to 12\% and 1\% on average.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {160--169},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772978},
 doi = {http://doi.acm.org/10.1145/1772954.1772978},
 acmid = {1772978},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {register allocation, register coalescing},
} 

@inproceedings{Wimmer:2010:LSR:1772954.1772979,
 author = {Wimmer, Christian and Franz, Michael},
 title = {Linear scan register allocation on SSA form},
 abstract = {The linear scan algorithm for register allocation provides a good register assignment with a low compilation overhead and is thus frequently used for just-in-time compilers. Although most of these compilers use static single assignment (SSA) form, the algorithm has not yet been applied on SSA form, i.e., SSA form is usually deconstructed before register allocation. However, the structural properties of SSA form can be used to simplify the algorithm. With only one definition per variable, lifetime intervals (the main data structure) can be constructed without data flow analysis. During allocation, some tests of interval intersection can be skipped because SSA form guarantees non-intersection. Finally, deconstruction of SSA form after register allocation can be integrated into the resolution phase of the register allocator without much additional code. We modified the linear scan register allocator of the Java HotSpot client compiler so that it operates on SSA form. The evaluation shows that our simpler and faster version generates equally good or slightly better machine code.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772979},
 doi = {http://doi.acm.org/10.1145/1772954.1772979},
 acmid = {1772979},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, SSA form, SSA form deconstruction, just-in-time compilation, lifetime analysis, linear scan, register allocation},
} 

@inproceedings{Edler von Koch:2010:IIS:1772954.1772980,
 author = {Edler von Koch, Tobias J.K. and B\"{o}hm, Igor and Franke, Bj\"{o}rn},
 title = {Integrated instruction selection and register allocation for compact code generation exploiting freeform mixing of 16- and 32-bit instructions},
 abstract = {For memory constrained embedded systems code size is at least as important as performance. One way of increasing code density is to exploit compact instruction formats, e.g. ARM Thumb, where the processor either operates in standard or compact instruction mode. The ARCompact ISA considered in this paper is different in that it allows freeform mixing of 16- and 32-bit instructions without a mode switch. Compact 16-bit instructions can be used anywhere in the code given that additional register constraints are satisfied. In this paper we present an integrated instruction selection and register allocation methodology and develop two approaches for mixed-mode code generation: a simple opportunistic scheme and a more advanced feedback-guided instruction selection scheme. We have implemented a code generator targeting the ARCompact ISA and evaluated its effectiveness against the ARC750D embedded processor and the EEMBC benchmark suite. On average, we achieve a code size reduction of 16.7\% across all benchmarks whilst at the same time improving performance by on average 17.7\%.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {180--189},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772980},
 doi = {http://doi.acm.org/10.1145/1772954.1772980},
 acmid = {1772980},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ARCompact, code size, dual instruction set architecture, instruction selection, register allocation, variable-length instructions},
} 

@inproceedings{Yuki:2010:ACT:1772954.1772982,
 author = {Yuki, Tomofumi and Renganarayanan, Lakshminarayanan and Rajopadhye, Sanjay and Anderson, Charles and Eichenberger, Alexandre E. and O'Brien, Kevin},
 title = {Automatic creation of tile size selection models},
 abstract = {Tiling is a widely used loop transformation for exposing/exploiting parallelism and data locality. Effective use of tiling requires selection and tuning of the tile sizes. This is usually achieved by hand-crafting tile size selection (TSS) models that characterize the performance of the tiled program as a function of tile sizes. The best tile sizes are selected by either directly using the TSS model or by using the TSS model together with an empirical search. Hand-crafting accurate TSS models is hard, and adapting them to different architecture/compiler, or even keeping them up-to-date with respect to the evolution of a single compiler is often just as hard. Instead of hand-crafting TSS models, can we automatically learn or create them? In this paper, we show that for a specific class of programs fairly accurate TSS models can be automatically created by using a combination of simple program features, synthetic kernels, and standard machine learning techniques. The automatic TSS model generation scheme can also be directly used for adapting the model and/or keeping it up-to-date. We evaluate our scheme on six different architecture-compiler combinations (chosen from three different architectures and four different compilers). The models learned by our method have consistently shown near-optimal performance (within 5\% of the optimal on average) across all architecture-compiler combinations.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772982},
 doi = {http://doi.acm.org/10.1145/1772954.1772982},
 acmid = {1772982},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {machine learning, neural network, performance modeling, tiling},
} 

@inproceedings{Baskaran:2010:PTR:1772954.1772983,
 author = {Baskaran, Muthu Manikandan and Hartono, Albert and Tavarageri, Sanket and Henretty, Thomas and Ramanujam, J. and Sadayappan, P.},
 title = {Parameterized tiling revisited},
 abstract = {Tiling, a key transformation for optimizing programs, has been widely studied in literature. Parameterized tiled code is important for auto-tuning systems since they often execute a large number of runs with dynamically varied tile sizes. Previous work on tiled code generation has addressed parameterized tiling for the sequential context, and the parallel case with fixed compile-time constants for tile sizes. In this paper, we revisit the problem of generating tiled code using parametric tile sizes. We develop a systematic approach to formulate tiling transformations through manipulation of linear inequalities and develop a novel approach to overcoming the fundamental obstacle faced by previous approaches regarding generation of parallel parameterized tiled code. To the best of our knowledge, the approach proposed in this paper is the first compile-time solution to the problem of parallel parameterized code generation for affine imperfectly nested loops. Experimental results demonstrate the effectiveness of the implemented system.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {200--209},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772983},
 doi = {http://doi.acm.org/10.1145/1772954.1772983},
 acmid = {1772983},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, compile-time optimization, tiling},
} 

@inproceedings{Wei:2010:MCR:1772954.1772984,
 author = {Wei, Haitao and Yu, Junqing and Yu, Huafei and Gao, Guang R.},
 title = {Minimizing communication in rate-optimal software pipelining for stream programs},
 abstract = {Stream programming model has been productively applied to a number of important applications domains. Software pipelining is an important code scheduling technique for stream programs. However, the multi-/many-core evolution has presented a new dimension of challenges: that is while searching a best software pipelining schedule how to ensure the communications between processing cores are also minimized? In this paper, we proposed a new solution methodology to address the above problem. Our main contributions include the following. A unified formulation has been proposed that combines the requirement of both rate-optimal software pipelining and the minimization of inter-core communication overhead. This formulation has been developed based on a synchronized dataflow graph model, and is expressed as an integer linear programming problem. A solution testbed has been implemented for the proposed problem formulation on the IBM Cell architecture. This has been realized by extending the Brook stream programming environment with our software pipelining support -- named DFBrook. An experimental study has been conducted to verify the effectiveness of the proposed solution. And a comparison of other scheduling methods has demon-strated the performance superiority of our proposed method.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {210--217},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1772954.1772984},
 doi = {http://doi.acm.org/10.1145/1772954.1772984},
 acmid = {1772984},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell processor, dfbrook, multi-core, software pipelining, stream programs},
} 

@inproceedings{Yu:2010:LLM:1772954.1772985,
 author = {Yu, Hongtao and Xue, Jingling and Huo, Wei and Feng, Xiaobing and Zhang, Zhaoqing},
 title = {Level by level: making flow- and context-sensitive pointer analysis scalable for millions of lines of code},
 abstract = {We present a practical and scalable method for flow- and context-sensitive (FSCS) pointer analysis for C programs. Our method analyzes the pointers in a program level by level in terms of their points-to levels, allowing the points-to relations of the pointers at a particular level to be discovered based on the points-to relations of the pointers at this level and higher levels. This level-by-level strategy can enhance the scalability of the FSCS pointer analysis in two fundamental ways, by enabling (1) fast and accurate flow-sensitive analysis on full sparse SSA form using a flow-insensitive algorithm and (2) fast and accurate context-sensitive analysis using a full transfer function and a meet function for each procedure. Our level-by-level algorithm, LevPA, gives rises to (1) a precise and compact SSA representation for subsequent program analysis and optimization tasks and (2) a flow- and context-sensitive MAY/MUST mod (modification) set and read set for each procedure. Our preliminary results show that LevPA can analyze some programs with over a million lines of C code in minutes, faster than the state-of-the-art FSCS methods.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {218--229},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1772954.1772985},
 doi = {http://doi.acm.org/10.1145/1772954.1772985},
 acmid = {1772985},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alias analysis, pointer analysis},
} 

@inproceedings{Linderman:2010:TPO:1772954.1772987,
 author = {Linderman, Michael D. and Ho, Matthew and Dill, David L. and Meng, Teresa H. and Nolan, Garry P.},
 title = {Towards program optimization through automated analysis of numerical precision},
 abstract = {Reducing the arithmetic precision of a computation has real performance implications, including increased speed, decreased power consumption, and a smaller memory footprint. For some architectures, e.g., GPUs, there can be such a large performance difference that using reduced precision is effectively a requirement. The trade-off is that the accuracy of the computation will be compromised. In this paper we describe a proof assistant and associated static analysis techniques for efficiently bounding numerical and precision-related errors. The programmer/compiler can use these bounds to numerically verify and optimize an application for different input and machine configurations. We present several case study applications that demonstrate the effectiveness of these techniques and the performance benefits that can be achieved with rigorous precision analysis.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {230--237},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1772954.1772987},
 doi = {http://doi.acm.org/10.1145/1772954.1772987},
 acmid = {1772987},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fixed-point numbers, floating-point numbers, numerical precision, static error analysis},
} 

@inproceedings{Stephenson:2010:SRP:1772954.1772988,
 author = {Stephenson, Mark William and Rangan, Ram and Yashchin, Emmanuel and Van Hensbergen, Eric},
 title = {Statistically regulating program behavior via mainstream computing},
 abstract = {We introduce mainstream computing, a collaborative system that dynamically checks a program--via runtime assertion checks--to ensure that it is running according to expectation. Rather than enforcing strict, statically-defined assertions, our system allows users to run with a set of assertions that are statistically guaranteed to fail at a rate bounded by a user-defined probability, p<sub>fail</sub>. For example, a user can request a set of assertions that will fail at most 0.5\% of the times the application is invoked. Users who believe their usage of an application is mainstream can use relatively large settings for p<sub>fail</sub>. Higher values of p<sub>fail</sub> provide stricter regulation of the application which likely enhances security, but will also inhibit some legitimate program behaviors; in contrast, program behavior is unregulated when p<sub>fail</sub> = 0, leaving the user vulnerable to attack. We show that our prototype is able to detect denial of service attacks, integer overflows, frees of uninitialized memory, boundary violations, and an injection attack. In addition we perform experiments with a mainstream computing system designed to protect against soft errors.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {238--247},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772988},
 doi = {http://doi.acm.org/10.1145/1772954.1772988},
 acmid = {1772988},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative community, mainstream computing},
} 

@inproceedings{Jiang:2010:ESC:1772954.1772989,
 author = {Jiang, Yunlian and Zhang, Eddy Z. and Tian, Kai and Mao, Feng and Gethers, Malcom and Shen, Xipeng and Gao, Yaoqing},
 title = {Exploiting statistical correlations for proactive prediction of program behaviors},
 abstract = {This paper presents a finding and a technique on program behavior prediction. The finding is that surprisingly strong statistical correlations exist among the behaviors of different program components (e.g., loops) and among different types of program level behaviors (e.g., loop trip-counts versus data values). Furthermore, the correlations can be beneficially exploited: They help resolve the proactivity-adaptivity dilemma faced by existing program behavior predictions, making it possible to gain the strengths of both approaches--the large scope and earliness of offline-profiling--based predictions, and the cross-input adaptivity of runtime sampling-based predictions. The main technique contributed by this paper centers on a new concept, seminal behaviors. Enlightened by the existence of strong correlations among program behaviors, we propose a regression based framework to automatically identify a small set of behaviors that can lead to accurate prediction of other behaviors in a program. We call these seminal behaviors. By applying statistical learning techniques, the framework constructs predictive models that map from seminal behaviors to other behaviors, enabling proactive and cross-input adaptive prediction of program behaviors. The prediction helps a commercial compiler, the IBM XL C compiler, generate code that runs up to 45\% faster (5\%-13\% on average), demonstrating the large potential of correlation-based techniques for program optimizations.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {248--256},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772989},
 doi = {http://doi.acm.org/10.1145/1772954.1772989},
 acmid = {1772989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {correlation, program behavior},
} 

@inproceedings{Mars:2010:CAE:1772954.1772991,
 author = {Mars, Jason and Vachharajani, Neil and Hundt, Robert and Soffa, Mary Lou},
 title = {Contention aware execution: online contention detection and response},
 abstract = {Cross-core application interference due to contention for shared on-chip and off-chip resources pose a significant challenge to providing application level quality of service (QoS) guarantees on commodity multicore micro-architectures. Unexpected cross-core interference is especially problematic when considering latency-sensitive applications that are present in the web service data center application domains, such as web-search. The commonly used solution is to simply disallow the co-location of latency-sensitive applications and throughput-oriented batch applications on a single chip, leaving much of the processing capabilities of multicore micro-architectures underutilized. In this work we present a Contention Aware Execution Runtime (CAER) environment that provides a lightweight runtime solution that minimizes cross-core interference due to contention, while maximizing utilization. CAER leverages the ubiquitous performance monitoring capabilities present in current multicore processors to infer and respond to contention and requires no added hardware support. We present the design and implementation of the CAER environment, two separate contention detection heuristics, and approaches to respond to contention online. We evaluate our solution using the SPEC2006 benchmark suite. Our experiments show that when allowing co-location with CAER, as opposed to disallowing co-location, we are able to increase the utilization of the multicore CPU by 58\% on average. Meanwhile CAER brings the overhead due to allowing co-location from 17\% down to just 4\% on average.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {257--265},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1772954.1772991},
 doi = {http://doi.acm.org/10.1145/1772954.1772991},
 acmid = {1772991},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {contention, cross-core interference, dynamic techniques, execution runtimes, multicore, online adaptation},
} 

@inproceedings{Wang:2010:ATC:1772954.1772992,
 author = {Wang, Lei and Cui, Huimin and Duan, Yuelu and Lu, Fang and Feng, Xiaobing and Yew, Pen-Chung},
 title = {An adaptive task creation strategy for work-stealing scheduling},
 abstract = {Work-stealing is a key technique in many multi-threading programming languages to get good load balancing. The current work-stealing techniques have a high implementation overhead in some applications and require a large amount of memory space for data copying to assure correctness. They also cannot handle many application programs that have an unbalanced call tree or have no definitive working sets. In this paper, we propose a new adaptive task creation strategy, called AdaptiveTC, which supports effective work-stealing schemes and also handles the above mentioned problems effectively. As shown in some experimental results, AdaptiveTC runs 2.71x faster than Cilk and 1.72x faster than Tascell for the 16-queen problem with 8 threads.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {266--277},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1772954.1772992},
 doi = {http://doi.acm.org/10.1145/1772954.1772992},
 acmid = {1772992},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive, backtracking search, task granularity, work-stealing},
} 

@inproceedings{Williams:2010:DID:1772954.1772993,
 author = {Williams, Kevin and McCandless, Jason and Gregg, David},
 title = {Dynamic interpretation for dynamic scripting languages},
 abstract = {Dynamic scripting languages offer programmers increased flexibility by allowing properties of programs to be defined at run-time. Typically, program execution begins with an interpreter where type checks are implemented using conditional statements. Recent JIT compilers have begun removing run-time checks by specializing native code to program properties discovered at JIT time. This paper presents a novel intermediate representation for scripting languages that explicitly encodes types of variables. The dynamic representation is a flow graph, where each node is a specialized virtual instruction and each edge directs program flow based on control and type changes in the program. The interpreter thus performs specialized execution of whole programs. We present techniques for the efficient interpretation of our representation showing speed-ups of greater than 2x over static interpretation, with an average speed-up of approximately 1.3x.},
 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization},
 series = {CGO '10},
 year = {2010},
 isbn = {978-1-60558-635-9},
 location = {Toronto, Ontario, Canada},
 pages = {278--287},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772954.1772993},
 doi = {http://doi.acm.org/10.1145/1772954.1772993},
 acmid = {1772993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic languages, interpreter optimization, type specialization},
} 

@inproceedings{2007:MGC:1251974.1252521,
 title = {Message from the General Chairs},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {x--},
 url = {http://dx.doi.org/10.1109/CGO.2007.23},
 doi = {http://dx.doi.org/10.1109/CGO.2007.23},
 acmid = {1252521},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2007:MPC:1251974.1252522,
 title = {Message from the Program Chairs},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {xi--},
 url = {http://dx.doi.org/10.1109/CGO.2007.24},
 doi = {http://dx.doi.org/10.1109/CGO.2007.24},
 acmid = {1252522},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2007:OC:1251974.1252523,
 title = {Organizing Committee},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {xii--},
 url = {http://dx.doi.org/10.1109/CGO.2007.27},
 doi = {http://dx.doi.org/10.1109/CGO.2007.27},
 acmid = {1252523},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2007:REV:1251974.1252524,
 title = {Reviewers},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {xiv--},
 url = {http://dx.doi.org/10.1109/CGO.2007.33},
 doi = {http://dx.doi.org/10.1109/CGO.2007.33},
 acmid = {1252524},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2007:CS:1251974.1252525,
 title = {Corporate Sponsors},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {xv--},
 url = {http://dx.doi.org/10.1109/CGO.2007.8},
 doi = {http://dx.doi.org/10.1109/CGO.2007.8},
 acmid = {1252525},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Buck:2007:GCP:1251974.1252526,
 author = {Buck, Ian},
 title = {GPU Computing: Programming a Massively Parallel Processor},
 abstract = {Many researchers have observed that general purpose computing with programmable graphics hardware (GPUs) has shown promise to solve many of the world's compute intensive problems, many orders of magnitude faster the conventional CPUs. The challenge has been working within the constraints of a graphics programming environment and limited language support to leverage this huge performance potential. GPU computing with CUDA is a new approach to computing where hundreds of on-chip processor cores simultaneously communicate and cooperate to solve complex computing problems, transforming the GPU into a massively parallel processor. The NVIDIA C-compiler for the GPU provides a complete development environment that gives developers the tools they need to solve new problems in computation-intensive applications such as product design, data analysis, technical computing, and game physics. In this talk, I will provide a description of how CUDA can solve compute intensive problems and highlight the challenges when compiling parallel programs for GPUs including the differences between graphics shaders vs. CUDA applications.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {17--},
 url = {http://dx.doi.org/10.1109/CGO.2007.13},
 doi = {http://dx.doi.org/10.1109/CGO.2007.13},
 acmid = {1252526},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Fang:2007:PPE:1251974.1252527,
 author = {Fang, Jesse},
 title = {Parallel Programming Environment: A Key to Translating Tera-Scale Platforms into a Big Success},
 abstract = {Moore's Law will continue to increase the number of transistors on die for a couple of decades, as silicon technology moves from 65nm today to 45nm, 32 nm and 22nm in the future. Since the power and thermal constraints increase with frequency, multi-core or many-core will be the way of the future microprocessor.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {18--},
 url = {http://dx.doi.org/10.1109/CGO.2007.28},
 doi = {http://dx.doi.org/10.1109/CGO.2007.28},
 acmid = {1252527},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Dice:2007:UTS:1251974.1252528,
 author = {Dice, Dave and Shavit, Nir},
 title = {Understanding Tradeoffs in Software Transactional Memory},
 abstract = {There has been a flurry of recent work on the design of high performance software and hybrid hardware/software transactional memories (STMs and HyTMs). This paper reexamines the design decisions behind several of these stateof- the-art algorithms, adopting some ideas, rejecting others, all in an attempt to make STMs faster. We created the transactional locking (TL) framework of STM algorithms and used it to conduct a range of comparisons of the performance of non-blocking, lock-based, and Hybrid STM algorithms versus fine-grained hand-crafted ones. We were able to make several illuminating observations regarding lock acquisition order, the interaction of STMs with memory management schemes, and the role of overheads and abort rates in STM performance.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {21--33},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.38},
 doi = {http://dx.doi.org/10.1109/CGO.2007.38},
 acmid = {1252528},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wang:2007:CGO:1251974.1252529,
 author = {Wang, Cheng and Chen, Wei-Yu and Wu, Youfeng and Saha, Bratin and Adl-Tabatabai, Ali-Reza},
 title = {Code Generation and Optimization for Transactional Memory Constructs in an Unmanaged Language},
 abstract = {Transactional memory offers significant advantages for concurrency control compared to locks. This paper presents the design and implementation of transactional memory constructs in an unmanaged language. Unmanaged languages pose a unique set of challenges to transactional memory constructs - for example, lack of type and memory safety, use of function pointers, aliasing of local variables, and others. This paper describes novel compiler and runtime mechanisms that address these challenges and optimize the performance of transactions in an unmanaged environment. We have implemented these mechanisms in a production-quality C compiler and a high-performance software transactional memory runtime. We measure the effectiveness of these optimizations and compare the performance of lock-based versus transaction-based programming on a set of concurrent data structures and the SPLASH-2 benchmark suite. On a 16 processor SMP system, the transaction-based version of the SPLASH-2 benchmarks scales much better than the coarse-grain locking version and performs comparably to the fine-grain locking version. Compiler optimizations significantly reduce the overheads of transactional memory so that, on a single thread, the transaction-based version incurs only about 6.4\% overhead compared to the lock-based version for the SPLASH-2 benchmark suite. Thus, our system is the first to demonstrate that transactions integrate well with an unmanaged language, and can perform as well as fine-grain locking while providing the programming ease of coarse-grain locking even on an unmanaged environment.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {34--48},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/CGO.2007.4},
 doi = {http://dx.doi.org/10.1109/CGO.2007.4},
 acmid = {1252529},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kotzmann:2007:RSO:1251974.1252531,
 author = {Kotzmann, Thomas and Mossenbock, Hanspeter},
 title = {Run-Time Support for Optimizations Based on Escape Analysis},
 abstract = {The JavaTM programming language does not allow the programmer to influence memory management. An object is usually allocated on the heap and deallocated by the garbage collector when it is not referenced any longer. Under certain conditions, the virtual machine can allocate objects on the stack or eliminate their allocation via scalar replacement. However, even if the dynamic compiler guarantees that the conditions are fulfilled, the optimizations require support by the run-time environment. We implemented a new escape analysis algorithm for Sun Microsystems' Java HotSpotTM VM. The results are used to replace objects with scalar variables, to allocate objects on the stack, and to remove synchronization. This paper deals with the representation of optimized objects in the debugging information and with reallocation and garbage collection support for a safe execution of optimized methods. Assignments to fields of parameters that can refer to both stack and heap objects are associated with an extended write barrier which skips card marking for stack objects. The traversal of objects during garbage collection uses a wrapper that abstracts from stack objects and presents their pointer fields as root pointers to the garbage collector. When a previously compiled and currently executing method must be continued in the interpreter because dynamic class loading invalidates the machine code, execution is suspended and compiler optimizations are undone. Scalar-replaced objects are reallocated on the heap lazily when control returns to the invalidated method, whereas stack-allocated objects must be reallocated immediately before program execution resumes. After reallocation, objects for which synchronization was removed are relocked.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {49--60},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2007.34},
 doi = {http://dx.doi.org/10.1109/CGO.2007.34},
 acmid = {1252531},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hiser:2007:EIB:1251974.1252530,
 author = {Hiser, Jason D. and Williams, Daniel and Hu, Wei and Davidson, Jack W. and Mars, Jason and Childers, Bruce R.},
 title = {Evaluating Indirect Branch Handling Mechanisms in Software Dynamic Translation Systems},
 abstract = {Software Dynamic Translation (SDT) systems are used for program instrumentation, dynamic optimization, security, intrusion detection, and many other uses. As noted by many researchers, a major source of SDT overhead is the execution of code which is needed to translate an indirect branch's target address into the address of the translated destination block. This paper discusses the sources of indirect branch (IB) overhead in SDT systems and evaluates several techniques for overhead reduction. Measurements using SPEC CPU2000 show that the appropriate choice and configuration of IB translation mechanisms can significantly reduce the IB handling overhead. In addition, cross-architecture evaluation of IB handling mechanisms reveals that the most efficient implementation and configuration can be highly dependent on the implementation of the underlying architecture.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {61--73},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.10},
 doi = {http://dx.doi.org/10.1109/CGO.2007.10},
 acmid = {1252530},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Reddi:2007:PCC:1251974.1252532,
 author = {Reddi, Vijay Janapa and Connors, Dan and Cohn, Robert and Smith, Michael D.},
 title = {Persistent Code Caching: Exploiting Code Reuse Across Executions and Applications},
 abstract = {Run-time compilation systems are challenged with the task of translating a program's instruction stream while maintaining low overhead. While software managed code caches are utilized to amortize translation costs, they are ineffective for programs with short run times or large amounts of cold code. Such program characteristics are prevalent in real-life computing environments, ranging from Graphical User Interface (GUI) programs to large-scale applications such as database management systems. Persistent code caching addresses these issues. It is described and evaluated in an industry-strength dynamic binary instrumentation system - Pin. The proposed approach improves the intra-execution model of code reuse by storing and reusing translations across executions, thereby achieving inter-execution persistence. Dynamically linked programs leverage inter-application persistence by using persistent translations of library code generated by other programs. New translations discovered across executions are automatically accumulated into the persistent code caches, thereby improving performance over time. Inter-execution persistence improves the performance of GUI applications by nearly 90\%, while inter-application persistence achieves a 59\% improvement. In more specialized uses, the SPEC2K INT benchmark suite experiences a 26\% improvement under dynamic binary instrumentation. Finally, a 400\% speedup is achieved in translating the Oracle database in a regression testing environment.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {74--88},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/CGO.2007.29},
 doi = {http://dx.doi.org/10.1109/CGO.2007.29},
 acmid = {1252532},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Codina:2007:VCS:1251974.1252533,
 author = {Codina, Josep M. and Sanchez, Jesus and Gonzalez, Antonio},
 title = {Virtual Cluster Scheduling Through the Scheduling Graph},
 abstract = {This paper presents an instruction scheduling and cluster assignment approach for clustered processors. The proposed technique makes use of a novel representation named the scheduling graph which describes all possible schedules. A powerful deduction process is applied to this graph, reducing at each step the set of possible schedules. In contrast to traditional list scheduling techniques, the proposed scheme tries to establish relations among instructions rather than assigning each instruction to a particular cycle. The main advantage is that wrong or poor schedules can be anticipated and discarded earlier. In addition, cluster assignment of instructions is performed using another novel concept called virtual clusters, which define sets of instructions that must execute in the same cluster. These clusters are managed during the deduction process to identify incompatibilities among instructions. The mapping of virtual to physical clusters is postponed until the scheduling of the instructions has finalized. The advantages this novel approach features include: (1) accurate scheduling information when assigning, and, (2) accurate information of the cluster assignment constraints imposed by scheduling decisions. We have implemented and evaluated the proposed scheme with superblocks extracted from SpecInt95 and MediaBench. The results show that this approach produces better schedules than the previous state-ofthe- art. Speed-ups are up to 15\%, with average speedups ranging from 2.5\% (2-Clusters) to 9.5\% (4- Clusters).},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {89--101},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.39},
 doi = {http://dx.doi.org/10.1109/CGO.2007.39},
 acmid = {1252533},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Bouchez:2007:CRC:1251974.1252534,
 author = {Bouchez, Florent and Darte, Alain and Rastello, Fabrice},
 title = {On the Complexity of Register Coalescing},
 abstract = {Memory transfers are becoming more important to optimize, for both performance and power consumption. With this goal in mind, new register allocation schemes are developed, which revisit not only the spilling problem but also the coalescing problem. Indeed, a more aggressive strategy to avoid load/store instructions may increase the constraints to suppress (coalesce) move instructions. This paper is devoted to the complexity of the coalescing phase, in particular in the light of recent developments on the SSA form. We distinguish several optimizations that occur in coalescing heuristics: a) aggressive coalescing removes as many moves as possible, regardless of the colorability of the resulting interference graph; b) conservative coalescing removes as many moves as possible while keeping the colorability of the graph; c) incremental conservative coalescing removes one particular move while keeping the colorability of the graph; d) optimistic coalescing coalesces moves aggressively, then gives up about as few moves as possible so that the graph becomes colorable again. We almost completely classify the NP-completeness of these problems, discussing also on the structure of the interference graph: arbitrary, chordal, or k-colorable in a greedy fashion. We believe that such a study is a necessary step for designing new coalescing strategies.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {102--114},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.26},
 doi = {http://dx.doi.org/10.1109/CGO.2007.26},
 acmid = {1252534},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Birkbeck:2007:DAA:1251974.1252535,
 author = {Birkbeck, Neil and Levesque, Jonathan and Amaral, Jose Nelson},
 title = {A Dimension Abstraction Approach to Vectorization in Matlab},
 abstract = {Matlab is a matrix-processing language that offers very efficient built-in operations for data organized in arrays. However Matlab operation is slow when the program accesses data through interpreted loops. Often during the development of a Matlab application writing loop-based code is more intuitive than crafting the data organization into arrays. Furthermore, many Matlab users do not command the linear algebra expertise necessary to write efficient code. Thus loop-based Matlab coding is a fairly common practice. This paper presents a tool that automatically converts loop-based Matlab code into equivalent array-based form and built-in Matlab constructs. Arraybased code is produced by checking the input and output dimensions of equations within loops, and by transposing terms when necessary to generate correct code. This paper also describes an extensible loop pattern database that allows user-defined patterns to be discovered and replaced by more efficient Matlab routines that perform the same computation. The safe conversion of loop-based into more efficient array-based code is made possible by the introduction of a new abstract representation for dimensions.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {115--130},
 numpages = {16},
 url = {http://dx.doi.org/10.1109/CGO.2007.1},
 doi = {http://dx.doi.org/10.1109/CGO.2007.1},
 acmid = {1252535},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Vaswani:2007:MSE:1251974.1252536,
 author = {Vaswani, Kapil and Thazhuthaveetil, Matthew J. and Srikant, Y. N. and Joseph, P. J.},
 title = {Microarchitecture Sensitive Empirical Models for Compiler Optimizations},
 abstract = {This paper proposes the use of empirical modeling techniques for building microarchitecture sensitive models for compiler optimizations. The models we build relate program performance to settings of compiler optimization flags, associated heuristics and key microarchitectural parameters. Unlike traditional analytical modeling methods, this relationship is learned entirely from data obtained by measuring performance at a small number of carefully selected compiler/microarchitecture configurations. We evaluate three different learning techniques in this context viz. linear regression, adaptive regression splines and radial basis function networks. We use the generated models to a) predict program performance at arbitrary compiler/microarchitecture configurations, b) quantify the significance of complex interactions between optimizations and the microarchitecture, and c) efficiently search for 'optimal' settings of optimization flags and heuristics for any given microarchitectural configuration. Our evaluation using benchmarks from the SPEC CPU2000 suits suggests that accurate models (\le 5\% average error in prediction) can be generated using a reasonable number of simulations. We also find that using compiler settings prescribed by a model-based search can improve program performance by as much as 19\% (with an average of 9.5\%) over highly optimized binaries.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {131--143},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.25},
 doi = {http://dx.doi.org/10.1109/CGO.2007.25},
 acmid = {1252536},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Pouchet:2007:IOP:1251974.1252537,
 author = {Pouchet, Louis-Noel and Bastoul, Cedric and Cohen, Albert and Vasilache, Nicolas},
 title = {Iterative Optimization in the Polyhedral Model: Part I, One-Dimensional Time},
 abstract = {Emerging microprocessors offer unprecedented parallel computing capabilities and deeper memory hierarchies, increasing the importance of loop transformations in optimizing compilers. Because compiler heuristics rely on simplistic performance models, and because they are bound to a limited set of transformations sequences, they only uncover a fraction of the peak performance on typical benchmarks. Iterative optimization is a maturing framework to address these limitations, but so far, it was not successfully applied complex loop transformation sequences because of the combinatorics of the optimization search space. We focus on the class of loop transformation which can be expressed as one-dimensional affine schedules. We define a systematic exploration method to enumerate the space of all legal, distinct transformations in this class. This method is based on an upstream characterization, as opposed to state-of-the-art downstream filtering approaches. Our results demonstrate orders of magnitude improvements in the size of the search space and in the convergence speed of a dedicated iterative optimization heuristic.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {144--156},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.21},
 doi = {http://dx.doi.org/10.1109/CGO.2007.21},
 acmid = {1252537},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kulkarni:2007:EHO:1251974.1252538,
 author = {Kulkarni, Prasad A. and Whalley, David B. and Tyson, Gary S.},
 title = {Evaluating Heuristic Optimization Phase Order Search Algorithms},
 abstract = {Program-specific or function-specific optimization phase sequences are universally accepted to achieve better overall performance than any fixed optimization phase ordering. A number of heuristic phase order space search algorithms have been devised to find customized phase orderings achieving high performance for each function. However, to make this approach of iterative compilation more widely accepted and deployed in mainstream compilers, it is essential to modify existing algorithms, or develop new ones that find near-optimal solutions quickly. As a step in this direction, in this paper we attempt to identify and understand the important properties of some commonly employed heuristic search methods by using information collected during an exhaustive exploration of the phase order search space. We compare the performance obtained by each algorithm with all others, as well as with the optimal phase ordering performance. Finally, we show how we can use the features of the phase order space to improve existing algorithms as well as devise new, and better performing search algorithms.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {157--169},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.9},
 doi = {http://dx.doi.org/10.1109/CGO.2007.9},
 acmid = {1252538},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Barthou:2007:LOU:1251974.1252539,
 author = {Barthou, Denis and Donadio, Sebastien and Carribault, Patrick and Duchateau, Alexandre and Jalby, William},
 title = {Loop Optimization using Hierarchical Compilation and Kernel Decomposition},
 abstract = {The increasing complexity of hardware features for re- cent processors makes high performance code genera- tion very challenging. In particular, several optimiza- tion targets have to be pursued simultaneously (minimizing L1/L2/L3/TLB misses and maximizing instruction level par- allelism). Very often, these optimization goals impose dif- ferent and contradictory constraints on the transformations to be applied. We propose a new hierarchical compilation approach for the generation of high performance code relying on the use of state-of-the-art compilers. This approach is not application-dependent and do not require any assembly hand-coding. It relies on the decomposition of the origi- nal loop nest into simpler kernels, typically 1D to 2D loops, much simpler to optimize. We successfully applied this approach to optimize dense matrix muliply primitives (not only for the square case but to the more general rectangular cases) and convolution. The performance of the optimized codes on Itanium 2 and Pentium 4 architectures outperforms ATLAS and in most cases, matches hand-tuned vendor libraries (e.g. MKL).},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {170--184},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/CGO.2007.22},
 doi = {http://dx.doi.org/10.1109/CGO.2007.22},
 acmid = {1252539},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Cavazos:2007:RSG:1251974.1252540,
 author = {Cavazos, John and Fursin, Grigori and Agakov, Felix and Bonilla, Edwin and O'Boyle, Michael F. P. and Temam, Olivier},
 title = {Rapidly Selecting Good Compiler Optimizations using Performance Counters},
 abstract = {Applying the right compiler optimizations to a particular program can have a significant impact on program performance. Due to the non-linear interaction of compiler optimizations, however, determining the best setting is nontrivial. There have been several proposed techniques that search the space of compiler options to find good solutions; however such approaches can be expensive. This paper proposes a different approach using performance counters as a means of determining good compiler optimization settings. This is achieved by learning a model off-line which can then be used to determine good settings for any new program. We show that such an approach outperforms the state-ofthe- art and is two orders of magnitude faster on average. Furthermore, we show that our performance counter-based approach outperforms techniques based on static code features. Using our technique we achieve a 17\% improvement over the highest optimization setting of the commercial PathScale EKOPath 2.3.1 optimizing compiler on the SPEC benchmark suite on a recent AMD Athlon 64 3700+ platform.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {185--197},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.32},
 doi = {http://dx.doi.org/10.1109/CGO.2007.32},
 acmid = {1252540},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Moseley:2007:SPH:1251974.1252541,
 author = {Moseley, Tipp and Shye, Alex and Reddi, Vijay Janapa and Grunwald, Dirk and Peri, Ramesh},
 title = {Shadow Profiling: Hiding Instrumentation Costs with Parallelism},
 abstract = {In profiling, a tradeoff exists between information and overhead. For example, hardware-sampling profilers incur negligible overhead, but the information they collect is consequently very coarse. Other profilers use instrumentation tools to gather temporal traces such as path profiles and hot memory streams, but they have high overhead. Runtime and feedback-directed compilation systems need detailed information to aggressively optimize, but the cost of gathering profiles can outweigh the benefits. Shadow profiling is a novel method for sampling long traces of instrumented code in parallel with normal execution, taking advantage of the trend of increasing numbers of cores. Each instrumented sample can be many millions of instructions in length. The primary goal is to incur negligible overhead, yet attain profile information that is nearly as accurate as a perfect profile. The profiler requires no modifications to the operating system or hardware, and is tunable to allow for greater coverage or lower overhead. We evaluate the performance and accuracy of this new profiling technique for two common types of instrumentation-based profiles: interprocedural path profiling and value profiling. Overall, profiles collected using the shadow profiling framework are 94\% accurate versus perfect value profiles, while incurring less than 1\% overhead. Consequently, this technique increases the viability of dynamic and continuous optimization systems by hiding the high overhead of instrumentation and enabling the online collection of many types of profiles that were previously too costly.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {198--208},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2007.35},
 doi = {http://dx.doi.org/10.1109/CGO.2007.35},
 acmid = {1252541},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wallace:2007:SPD:1251974.1252542,
 author = {Wallace, Steven and Hazelwood, Kim},
 title = {SuperPin: Parallelizing Dynamic Instrumentation for Real-Time Performance},
 abstract = {Dynamic instrumentation systems have proven to be extremely valuable for program introspection, architectural simulation, and bug detection. Yet a major drawback of modern instrumentation systems is that the instrumented applications often execute several orders of magnitude slower than native application performance. In this paper, we present a novel approach to dynamic instrumentation where several non-overlapping slices of an application are launched as separate instrumentation threads and executed in parallel in order to approach real-time performance. A direct implementation of our technique in the Pin dynamic instrumentation system results in dramatic speedups for various instrumentation tasks - often resulting in orderof- magnitude performance improvements. Our implementation is available as part of the Pin distribution, which has been downloaded over 10,000 times since its release.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {209--220},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2007.37},
 doi = {http://dx.doi.org/10.1109/CGO.2007.37},
 acmid = {1252542},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Fulton:2007:CTR:1251974.1252543,
 author = {Fulton, Mike and Stoodley, Mark},
 title = {Compilation Techniques for Real-Time Java Programs},
 abstract = {In this paper, we introduce the IBM\&#174; WebSphere\&#174; Real Time product, which incorporates a virtual machine that is fully Java\&#153; compliant as well as compliant with the Real-Time Specification for Java (RTSJ). We describe IBM's real-time Java enhancements, particularly in the area of our Testarossa (TR) ahead-of-time (AOT) compiler, our TR just-in-time (JIT) compiler, and our Metronome[2] deterministic Garbage Collector (GC). The main focus of this paper is on the various techniques employed by the TR compilers to optimize and regulate the performance of code running in a real-time Java environment through a simple Java source code example. Through the example, we highlight the additional checks required to provide a conformant RTSJ implementation as well as the performance issues with ahead-of-time code generation and the overheads required to support Metronome. We show how these checks are implemented in a production JVM, and then report the cost of the real-time changes in practice for the example as well as the SPECjvm98 benchmark suite, SPECjbb2000, and SPECjbb2005.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {221--231},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2007.5},
 doi = {http://dx.doi.org/10.1109/CGO.2007.5},
 acmid = {1252543},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Ozturk:2007:CVL:1251974.1252545,
 author = {Ozturk, O. and Chen, G. and Kandemir, M. and Karakoy, M.},
 title = {Compiler-Directed Variable Latency Aware SPM Management to CopeWith Timing Problems},
 abstract = {As a result of process parameter variations, a large variability in circuit delay occurs in scaled technologies. This delay or latency variation problem is particularly pressing for memory components due to the minimum sized transistors used to build them. Current memory design techniques mostly cope with such variations by adopting a worst-case design option, which simply assumes all memory locations are operated under the worst possible latency, whereas in reality some memory locations could be much faster than the others. Note that, assuming any other latency value other than the worst-case latency for all memory locations uniformly can lead to reliability problems, since the data may not be ready when the assumed latency has passed. Instead of operating under the worst-case design option, this paper proposes and experimentally evaluates a compilerdriven approach that operates an on-chip scratch-pad memory (SPM) assuming different latencies for the different SPM lines. Our goal is to reduce execution cycles without creating any reliability problems due to variations in access latencies. The proposed scheme achieves its goal by evaluating the reuse of different data items and adopting a reuse and latency aware data-to-SPM placement. It also employs data migration within SPM when it helps to cut down the number of execution cycles further. We also discuss an alternate scheme that can reduce latency of select SPM locations by controlling a circuit level mechanism in software to further improve performance. We implemented our approach within an optimizing compiler and tested its effectiveness through extensive simulations. Our experiments with twelve embedded application codes show that the proposed approach performs much better than the worst-case based design paradigm (16.2\% improvement on the average) and comes close (within 5.7\%) to an hypothetical bestcase design (i.e., one with no process variation) where every SPM locations uniformly have low latency.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {232--243},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2007.6},
 doi = {http://dx.doi.org/10.1109/CGO.2007.6},
 acmid = {1252545},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wang:2007:CSR:1251974.1252544,
 author = {Wang, Cheng and Kim, Ho-seop and Wu, Youfeng and Ying, Victor},
 title = {Compiler-Managed Software-based Redundant Multi-Threading for Transient Fault Detection},
 abstract = {As transistors become increasingly smaller and faster with tighter noise margins, modern processors are becoming increasingly more susceptible to transient hardware faults. Existing Hardware-based Redundant Multi-Threading (HRMT) approaches rely mostly on special-purpose hardware to replicate the program into redundant execution threads and compare their computation results. In this paper, we present a Software-based Redundant Multi-Threading (SRMT) approach for transient fault detection. Our SRMT technique uses compiler to automatically generate redundant threads so they can run on general-purpose chip multi-processors (CMPs). We exploit high-level program information available at compile time to optimize data communication between redundant threads. Furthermore, our software-based technique provides flexible program execution environment where the legacy binary codes and the reliability-enhanced codes can co-exist in a mix-and-match fashion, depending on the desired level of reliability and software compatibility. Our experimental results show that compiler analysis and optimization techniques can reduce data communication requirement by up to 88\% of HRMT. With general-purpose intra-chip communication mechanisms in CMP machine, SRMT overhead can be as low as 19\%. Moreover, SRMT technique achieves error coverage rates of 99.98\% and 99.6\% for SPEC CPU2000 integer and floating-point benchmarks, respectively. These results demonstrate the competitiveness of SRMT to HRMT approaches.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {244--258},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/CGO.2007.7},
 doi = {http://dx.doi.org/10.1109/CGO.2007.7},
 acmid = {1252544},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Dreweke:2007:GPA:1251974.1252546,
 author = {Dreweke, A. and Worlein, M. and Fischer, I. and Schell, D. and Meinl, Th. and Philippsen, M.},
 title = {Graph-Based Procedural Abstraction},
 abstract = {Procedural abstraction (PA) extracts duplicate code segments into a newly created method and hence reduces code size. For embedded micro computers the amount of memory is still limited so code reduction is an important issue. This paper presents a novel approach to PA, that is especially targeted towards embedded systems. Earlier approaches of PA are blind with respect to code reordering, i.e., two code segments with the same semantic effect but with different instruction orders were not detected as candidates for PA. Instead of instruction sequences, in our approach the data flow graphs of basic blocks are considered. Compared to known PA techniques more than twice the number of instructions can be saved on a set of binaries, by detecting frequently appearing graph fragments with a graph mining tool based on the well known gSpan algorithm. The detection and extraction of graph fragments is not as straight forward as extracting sequential code fragments. NP-complete graph operations and special rules to decide which parts can be abstracted are needed. However, this effort pays off as smaller sizes significantly reduce costs on mass-produced embedded systems.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {259--270},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2007.14},
 doi = {http://dx.doi.org/10.1109/CGO.2007.14},
 acmid = {1252546},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Raman:2007:SLO:1251974.1252547,
 author = {Raman, Easwaran and Hundt, Robert and Mannarswamy, Sandya},
 title = {Structure Layout Optimization for Multithreaded Programs},
 abstract = {Structure layout optimizations seek to improve runtime performance by improving data locality and reuse. The structure layout heuristics for single-threaded benchmarks differ from those for multi-threaded applications running on multiprocessor machines, where the effects of false sharing need to be taken into account. In this paper we propose a technique for structure layout transformations for multithreaded applications that optimizes both for improved spatial locality and reduced false sharing, simultaneously. We develop a semi-automatic tool that produces actual structure layouts for multi-threaded programs and outputs the key factors contributing to the layout decisions. We apply this tool on the HP-UX kernel and demonstrate the effects of these transformations for a variety of already highly hand-tuned key structures with different set of properties. We show that na¨\&#253;ve heuristics can result in massive performance degradations on such a highly tuned application, while our technique generally avoids those pitfalls. The improved structures produced by our tool improve performance by up to 3.2\% over a highly tuned baseline.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {271--282},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2007.36},
 doi = {http://dx.doi.org/10.1109/CGO.2007.36},
 acmid = {1252547},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{He:2007:CCO:1251974.1252548,
 author = {He, Haifeng and Trimble, John and Perianayagam, Somu and Debray, Saumya and Andrews, Gregory},
 title = {Code Compaction of an Operating System Kernel},
 abstract = {General-purpose operating systems, such as Linux, are increasingly being used in embedded systems. Computational resources are usually limited, and embedded processors often have a limited amount of memory. This makes code size especially important. This paper describes techniques for automatically reducing the memory footprint of general-purpose operating systems on embedded platforms. The problem is complicated by the fact that kernel code tends to be quite different from ordinary application code, including the presence of a significant amount of hand-written assembly code, multiple entry points, implicit control flow paths involving interrupt handlers, and frequent indirect control flow via function pointers. We use a novel "approximate decompilation" technique to apply source-level program analysis to hand-written assembly code. A prototype implementation of our ideas on an Intel x86 platform, applied to a Linux kernel that has been configured to exclude unnecessary code, obtains a code size reduction of close to 24\%.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {283--298},
 numpages = {16},
 url = {http://dx.doi.org/10.1109/CGO.2007.3},
 doi = {http://dx.doi.org/10.1109/CGO.2007.3},
 acmid = {1252548},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhao:2007:UMI:1251974.1252550,
 author = {Zhao, Qin and Rabbah, Rodric and Amarasinghe, Saman and Rudolph, Larry and Wong, Weng-Fai},
 title = {Ubiquitous memory introspection},
 abstract = {Modern memory systems play a critical role in the performance of applications, but a detailed understanding of the application behavior in the memory system is not trivial to attain. It requires time consuming simulations and detailed modeling of the memory hierarchy, often using long address traces. It is increasingly possible to access hardware performance counters to count relevant events in the memory system, but the measurements are coarse-grained and better suited for performance summaries than providing instruction level feedback. The availability of a low cost, online, and accurate methodology for deriving finegrained memory behavior profiles can prove extremely useful for runtime analysis and optimization of programs. This paper presents a new methodology for Ubiquitous Memory Introspection (UMI). It is an online and lightweight methodology that uses fast mini-simulations to analyze short memory access traces recorded from frequently executed code regions. The simulations provide profiling results at varying granularities, down to that of a single instruction or address. UMI naturally complements runtime optimizations and enables new opportunities for online memory specific optimizations. We present a prototype runtime system implementing UMI. The prototype has an average runtime overhead of 14\%. This overhead is only 1\% more than a state of the art binary instrumentation tool. We used 32 benchmarks, including the full suite of SPEC CPU2000 benchmarks, for evaluation. We show that the mini-simulations accurately reflect the cache performance of two existing memory systems, an Intel Pentium 4 and an AMD Athlon MP (K7). We also demonstrate that UMI predicts delinquent load instructions with an 88\% rate of accuracy for applications with a relatively high number of cache misses, and 61\% overall. The online profiling results are used at runtime to implement a simple software prefetching strategy that achieves an overall speedup of 64\% in the best case.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {299--311},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.12},
 doi = {http://dx.doi.org/10.1109/CGO.2007.12},
 acmid = {1252550},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Dai:2007:PEC:1251974.1252549,
 author = {Dai, Jinquan and Li, Long and Huang, Bo},
 title = {Pipelined Execution of Critical Sections Using Software-Controlled Caching in Network Processors},
 abstract = {To keep up with the explosive internet packet processing demands, modern network processors (NPs) employ a highly parallel, multi-threaded and multi-core architecture. In such a parallel paradigm, accesses to the shared variables in the external memory (and the associated memory latency) are contained in the critical sections, so that they can be executed atomically and sequentially by different threads in the network processor. In this paper, we present a novel program transformation that is used in the Intel Auto-partitioning C Compiler for IXP to exploit the inherent finer-grained parallelism of those critical sections, using the software-controlled caching mechanism available in the NPs. Consequently, those critical sections can be executed in a pipelined fashion by different threads, thereby effectively hiding the memory latency and improving the performance of network applications. Experimental results show that the proposed transformation provides impressive speedup (up-to 9.94 and scalability (up-to 80 threads) of the performance for the real-world network application (a l0Gbps Efhernet Core/Metro Router).},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {312--324},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.30},
 doi = {http://dx.doi.org/10.1109/CGO.2007.30},
 acmid = {1252549},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Grzegorczyk:2007:IVH:1251974.1252551,
 author = {Grzegorczyk, Chris and Soman, Sunil and Krintz, Chandra and Wolski, Rich},
 title = {Isla Vista Heap Sizing: Using Feedback to Avoid Paging},
 abstract = {Managed runtime environments (MREs) employ garbage collection (GC) for automatic memory manage- ment. However, GC induces pressure on the virtual memory (VM) manager, since it may touch pages that are not related to the working set of the application. Paging due to GC can significantly hurt performance, even when the application's working set fits into physical memory. We present a feedback-directed heap resizing mechanism to avoid GC-induced paging, using information from the operating system (OS). We avoid costly GCs when there is physical memory available, and trade off GC for pag- ing when memory is constrained Our mechanism is simple and uses allocation stall events during GC alone to trig- ger heap resizing, without user participation or OS kernel modification. Our system enables significant performance improvements when real memory is restricted and similar to, or better performance than, the current state-of-the-art MRE, when memory is unconstrained.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {325--340},
 numpages = {16},
 url = {http://dx.doi.org/10.1109/CGO.2007.20},
 doi = {http://dx.doi.org/10.1109/CGO.2007.20},
 acmid = {1252551},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hormati:2007:ENA:1251974.1252552,
 author = {Hormati, Amir and Clark, Nathan and Mahlke, Scott},
 title = {Exploiting Narrow Accelerators with Data-Centric Subgraph Mapping},
 abstract = {The demand for high performance has driven acyclic computation accelerators into extensive use in modern embedded and desktop architectures. Accelerators that are ideal from a software perspective, are difficult or impossible to integrate in many modern architectures, though, due to area and timing requirements. This reality is coupled with the observation that many application domains under-utilize accelerator hardware, because of the narrow data they operate on and the nature of their computation. In this work, we take advantage of these facts to design accelerators capable of executing in modern architectures by narrowing datapath width and reducing interconnect. Novel compiler techniques are developed in order to generate highquality code for the reduced-cost accelerators and prevent performance loss to the extent possible. First, data width profiling is used to statistically determine how wide program data will be at run time. This information is used by the subgraph mapping algorithm to optimally select subgraphs for execution on targeted narrow accelerators. Overall, our data-centric compilation techniques achieve on average 6.5\%, and up to 12\%, speed up over previous subgraph mapping algorithms for 8-bit accelerators. We also show that, with appropriate compiler support, the increase in the total number of execution cycles in reduced-interconnect accelerators is less than 1\% of the fully-connected accelerator.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {341--353},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.11},
 doi = {http://dx.doi.org/10.1109/CGO.2007.11},
 acmid = {1252552},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Aleta:2007:HCV:1251974.1252553,
 author = {Aleta, Alex and Codina, Josep M. and Gonzalez, Antonio and Kaeli, David},
 title = {Heterogeneous Clustered VLIW Microarchitectures},
 abstract = {Increasing performance, while at the same time reducing power consumption, is a major design tradeoff in current microprocessors. In this paper, we investigate the potential of using a heterogeneous clustered VLIW microarchitecture. In the proposed microarchitecture, each cluster, the interconnection network and the supporting memory hierarchy can run at different frequencies and voltages. Some of the clusters can then be configured to be performanceoriented and run at high frequency, while the other clusters can be configured to be low-power-oriented and run at lower frequencies, thus reducing overall consumption. For this heterogeneous design to be effective, we need to select the most suitable frequencies and voltages for each component. We propose a scheme to choose these parameters based on a model that estimates the energy consumption and the execution time of floating-point codes at compile time. Finally, we present a modulo scheduling technique based on graph partitioning that exploits the opportunities presented on heterogeneous clustered microarchitectures. Results show that the Energy-Delay2 product (ED2) can be significantly reduced by 15\% on average for a microarchitecture with 4-clusters and by as much as 35\% for selected programs.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {354--366},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2007.15},
 doi = {http://dx.doi.org/10.1109/CGO.2007.15},
 acmid = {1252553},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kim:2007:PCS:1251974.1252554,
 author = {Kim, Hyesoon and Joao, Jos\'{e} A. and Mutlu, Onur and Patt, Yale N.},
 title = {Profile-assisted Compiler Support for Dynamic Predication in Diverge-Merge Processors},
 abstract = {Dynamic predication has been proposed to reduce the branch misprediction penalty due to hard-to-predict branch instructions. A recently proposed dynamic predication architecture, the diverge-merge processor (DMP), provides large performance improvements by dynamically predicating a large set of complex control-flow graphs that result in branch mispredictions. DMP requires significant support from a profiling compiler to determine which branch instructions and control-flow structures can be dynamically predicated. However, previous work on dynamic predication did not extensively examine the tradeoffs involved in profiling and code generation for dynamic predication architectures. This paper describes compiler support for obtaining high performance in the diverge-merge processor. We describe new profile-driven algorithms and heuristics to select branch instructions that are suitable and profitable for dynamic predication. We also develop a new profile-based analytical cost-benefit model to estimate, at compiletime, the performance benefits of the dynamic predication of different types of control-flow structures including complex hammocks and loops. Our evaluations show that DMP can provide 20.4\% average performance improvement over a conventional processor on SPEC integer benchmarks with our optimized compiler algorithms, whereas the average performance improvement of the best-performing alternative simple compiler algorithm is 4.5\%. We also find that, with the proposed algorithms, DMP performance is not significantly affected by the differences in profile- and run-time input data sets.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {367--378},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2007.31},
 doi = {http://dx.doi.org/10.1109/CGO.2007.31},
 acmid = {1252554},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{2007:AI:1251974.1252555,
 title = {Author Index},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {379--},
 url = {http://dx.doi.org/10.1109/CGO.2007.2},
 doi = {http://dx.doi.org/10.1109/CGO.2007.2},
 acmid = {1252555},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2005:MGC:1048922.1048966,
 title = {Message from the General Co-Chairs},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {.09--ix},
 url = {http://dx.doi.org/10.1109/CGO.2005.20},
 doi = {http://dx.doi.org/10.1109/CGO.2005.20},
 acmid = {1048966},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2005:MPC:1048922.1048967,
 title = {Message from the Program Chair},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {.10--x},
 url = {http://dx.doi.org/10.1109/CGO.2005.21},
 doi = {http://dx.doi.org/10.1109/CGO.2005.21},
 acmid = {1048967},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2005:COM:1048922.1048968,
 title = {Committees},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {.11--xiii},
 url = {http://dx.doi.org/10.1109/CGO.2005.11},
 doi = {http://dx.doi.org/10.1109/CGO.2005.11},
 acmid = {1048968},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2005:REV:1048922.1048969,
 title = {Reviewers},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {.14--xiv},
 url = {http://dx.doi.org/10.1109/CGO.2005.31},
 doi = {http://dx.doi.org/10.1109/CGO.2005.31},
 acmid = {1048969},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2005:CS:1048922.1048970,
 title = {Corporate Sponsors},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {.15--xv},
 url = {http://dx.doi.org/10.1109/CGO.2005.15},
 doi = {http://dx.doi.org/10.1109/CGO.2005.15},
 acmid = {1048970},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Hind:2005:VML:1048922.1048971,
 author = {Hind, Michael},
 title = {Virtual Machine Learning: Thinking like a Computer Architect},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {11--11},
 numpages = {1},
 url = {http://dx.doi.org/10.1109/CGO.2005.37},
 doi = {http://dx.doi.org/10.1109/CGO.2005.37},
 acmid = {1048971},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Berndl:2005:CTF:1048922.1048973,
 author = {Berndl, Marc and Vitale, Benjamin and Zaleski, Mathew and Brown, Angela Demke},
 title = {Context Threading: A Flexible and Efficient Dispatch Technique for Virtual Machine Interpreters},
 abstract = {Direct-threaded interpreters use indirect branches to dispatch bytecodes, but deeply-pipelined architectures rely on branch prediction for performance. Due to the poor correlation between the virtual program's control flow and the hardware program counter, which we call the context problem, direct threading's indirect branches are poorly predicted by the hardware, limiting performance. Our dispatch technique, context threading, improves branch prediction and performance by aligning hardware and virtual machine state. Linear virtual instructions are dispatched with native calls and returns, aligning the hardware and virtual PC. Thus, sequential control flow is predicted by the hardware return stack. We convert virtual branching instructions to native branches, mobilizing the hardware's branch prediction resources. We evaluate the impact of context threading on both branch prediction and performance using interpreters for Java and OCaml on the Pentium and PowerPC architectures. On the Pentium IV, our technique reduces mean mispredicted branches by 95\%. On the PowerPC, it reduces mean branch stall cycles by 75\% for OCaml and 82\% for Java. Due to reduced branch hazards, context threading reduces mean execution time by 25\% for Java and by 19\% and 37\% for OCaml on the P4 and PPC970, respectively. We also combine context threading with a conservative inlining technique and find its performance comparable to that of selective inlining.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {15--26},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.14},
 doi = {http://dx.doi.org/10.1109/CGO.2005.14},
 acmid = {1048973},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Stoodley:2005:ARR:1048922.1048972,
 author = {Stoodley, Mark and Sundaresan, Vijay},
 title = {Automatically Reducing Repetitive Synchronization with a Just-in-Time Compiler for Java},
 abstract = {
},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {27--36},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CGO.2005.7},
 doi = {http://dx.doi.org/10.1109/CGO.2005.7},
 acmid = {1048972},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Nandivada:2005:CCM:1048922.1048974,
 author = {Nandivada, V. Krishna and Detlefs, David},
 title = {Compile-Time Concurrent Marking Write Barrier Removal},
 abstract = {Garbage collectors incorporating concurrent marking to cope with large live data sets and stringent pause time constraints have become common in recent years. The snapshot-at-the-beginning style of concurrent marking has several advantages over the incremental update alternative, but one main disadvantage: it requires the mutator to execute a significantly more expensive write barrier. This paper demonstrates that a large fraction of these write barriers are unnecessary, and may be eliminated by static analysis.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {37--48},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.12},
 doi = {http://dx.doi.org/10.1109/CGO.2005.12},
 acmid = {1048974},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Arnold:2005:CEH:1048922.1048975,
 author = {Arnold, Matthew and Grove, David},
 title = {Collecting and Exploiting High-Accuracy Call Graph Profiles in Virtual Machines},
 abstract = {Due to the high dynamic frequency of virtual method calls in typical object-oriented programs, feedback-directed devirtualization and inlining is one of the most important optimizations performed by high-performance virtual machines. A critical input to effective feedback-directed inlining is an accurate dynamic call graph. In a virtual machine, the dynamic call graph is computed online during program execution. Therefore, to maximize overall system performance, the profiling mechanism must strike a balance between profile accuracy, the speed at which the profile becomes available to the optimizer, and profiling overhead. This paper introduces a new low-overhead sampling-based technique that rapidly converges on a high-accuracy dynamic call graph. We have implemented the technique in two high-performance virtual machines: Jikes RVMand J9. We empirically assess our profiling technique by reporting on the accuracy of the dynamic call graphs it computes and by demonstrating that increasing the accuracy of the dynamic call graph results in more effective feedback-directed inlining.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {51--62},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.9},
 doi = {http://dx.doi.org/10.1109/CGO.2005.9},
 acmid = {1048975},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hu:2005:EAC:1048922.1048976,
 author = {Hu, Shiwen and Valluri, Madhavi and John, Lizy Kurian},
 title = {Effective Adaptive Computing Environment Management via Dynamic Optimization},
 abstract = {To minimize the surging power consumption of microprocessors, adaptive computing environments (ACEs) where microarchitectural resources can be dynamically tuned to match a program's runtime requirement and characteristics are becoming increasingly common. Adaptive computing environments usually have multiple configurable hardware units, necessitating exploration of a large number of combinatorial configurations in order to identify the most energy-efficient configuration. In this paper, we propose a scheme for efficient management of multiple configurable units, utilizing the inherent capabilities of dynamic optimization systems. Most dynamic optimizers typically detect dominant code regions (hotspots). We develop an ACE management scheme where hotpot boundaries are used for phase detection and adaptation. Since hotspots are of variable sizes and are often nested, program phase behavior which is hierarchical in nature is automatically captured in this technique. To demonstrate the usefulness and effectiveness of our framework, we use the proposed framework to dynamically adapt the sizes of L1 data and L2 caches that have different reconfiguration latencies and overheads. Our technique reduces L1D and L2 cache energy consumption by 47\% and 58\%, while a popular previously proposed technique only achieves reduction of 32\% and 52\% respectively.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {63--73},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2005.17},
 doi = {http://dx.doi.org/10.1109/CGO.2005.17},
 acmid = {1048976},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Bruening:2005:MCB:1048922.1048977,
 author = {Bruening, Derek and Amarasinghe, Saman},
 title = {Maintaining Consistency and Bounding Capacity of Software Code Caches},
 abstract = {Software code caches are becoming ubiquitous, in dynamic optimizers, runtime tool platforms, dynamic translators, fast simulators and emulators, and dynamic compilers. Caching frequently executed fragments of code provides significant performance boosts, reducing the overhead of translation and emulation and meeting or exceeding native performance in dynamic optimizers. One disadvantage of caching, memory expansion, can sometimes be ignoredwhen executing a single application. However, as optimizers and translators are applied more and more in production systems, the memory expansion from running multiple applications simultaneously becomes problematic. A second drawback to caching is the addedrequirement of maintaining consistency between the code cache and the original code. On architectures like IA-32 that do not require explicit application actions when modifying code, detecting code changes is challenging. Again, consistency can be ignored for certain sets of applications, but as caching systems scale up to executing large, modern, complex programs, consistency becomes critical. This paper presents efficient schemes for keeping a software code cache consistent and for dynamically bounding code cache size to match the current working set of the application. These schemes are evaluated in the DynamoRIO runtime code manipulation system, and operate on stock hardware in the presence of multiple threads and dynamic behavior, including dynamically-loaded, generated, and even modified code.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {74--85},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.19},
 doi = {http://dx.doi.org/10.1109/CGO.2005.19},
 acmid = {1048977},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Das:2005:PRO:1048922.1048978,
 author = {Das, Abhinav and Lu, Jiwei and Chen, Howard and Kim, Jinpyo and Yew, Pen-Chung and Hsu, Wei-Chung and Chen, Dong-Yuan},
 title = {Performance of Runtime Optimization on BLAST},
 abstract = {Optimization of a real world application BLAST is used to demonstrate the limitations of static and profile-guided optimizations and to highlight the potential of runtime optimization systems. We analyze the performance profile of this application to determine performance bottlenecks and evaluate the effect of aggressive compiler optimizations on BLAST. We find that applying common optimizations (e.g. O3) can degrade performance. Profile guided optimizations do not show much improvement across the board, as current implementations do not address critical performance bottlenecks in BLAST. In some cases, these optimizations lower performance significantly due to unexpected secondary effects of aggressive optimizations. We also apply runtime optimization to BLAST using the ADORE framework. ADORE is able to detect performance bottlenecks and deploy optimizations resulting in performance gains up to 58\% on some queries using data cache prefetching.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {86--96},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2005.25},
 doi = {http://dx.doi.org/10.1109/CGO.2005.25},
 acmid = {1048978},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Li:2005:OSG:1048922.1048979,
 author = {Li, Xiaoming and Garzaran, Maria Jesus and Padua, David},
 title = {Optimizing Sorting with Genetic Algorithms},
 abstract = {The growing complexity of modern processors has made the generation of highly efficient code increasingly difficult. Manual code generation is very time consuming, but it is often the only choice since the code generated by today's compiler technology often has much lower performance than the best hand-tuned codes. A promising code generation strategy, implemented by systems like ATLAS, FFTW, and SPIRAL, uses empirical search to find the parameter values of the implementation, such as the tile size and instruction schedules, that deliver near-optimal performance for a particular machine. However, this approach has only proven successful on scientific codes whose performance does not depend on the input data. In this paper we study machine learning techniques to extend empirical search to the generation of sorting routines, whose performance depends on the input characteristics and the architecture of the target machine. We build on a previous study that selects a "pure" sorting algorithm at the outset of the computation as a function of the standard deviation. The approach discussed in this paper uses genetic algorithms and a classifier system to build hierarchically-organized hybrid sorting algorithms capable of adapting to the input data. Our results show that such algorithms generated using the approach presented in this paper are quite effective at taking into account the complex interactions between architectural and input data characteristics and that the resulting code performs significantly better than conventional sorting implementations and the code generated by our earlier study. In particular, the routines generated using our approach perform better than all the commercial libraries that we tried including IBM ESSL, INTEL MKL and the C++ STL. The best algorithm we have been able to generate is on the average 26\%and 62\% faster than the IBM ESSL in an IBM Power 3 and IBM Power 4, respectively.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {99--110},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.24},
 doi = {http://dx.doi.org/10.1109/CGO.2005.24},
 acmid = {1048979},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chen:2005:CMG:1048922.1048980,
 author = {Chen, Chun and Cha Jacqueline and Hall, Mary},
 title = {Combining Models and Guided Empirical Search to Optimize for Multiple Levels of the Memory Hierarchy},
 abstract = {This paper describes an algorithm for simultaneously optimizing across multiple levels of the memory hierarchy for dense-matrix computations. Our approach combines compiler models and heuristics with guided empirical search to take advantage of their complementary strengths. The models and heuristics limit the search to a small number of candidate implementations, and the empirical results provide the most accurate information to the compiler to select among candidates and tune optimization parameter values. We have developed an initial implementation and applied this approach to two case studies, Matrix Multiply and Jacobi Relaxation. For Matrix Multiply, our results on two architectures, SGI R10000 and Sun UltraSparc IIe, outperform the native compiler, and either outperform or achieve comparable performance as the ATLAS self-tuning library and the hand-tuned vendor BLAS library. Jacobi results also substantially outperform the native compilers.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {111--122},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.10},
 doi = {http://dx.doi.org/10.1109/CGO.2005.10},
 acmid = {1048980},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Stephenson:2005:PUF:1048922.1048981,
 author = {Stephenson, Mark and Amarasinghe, Saman},
 title = {Predicting Unroll Factors Using Supervised Classification},
 abstract = {Compilers base many critical decisions on abstracted architectural models. While recent research has shown that modeling is effective for some compiler problems, building accurate models requires a great deal of human time and effort. This paper describes how machine learning techniques can be leveraged to help compiler writers model complex systems. Because learning techniques can effectively make sense of high dimensional spaces, they can be a valuable tool for clarifying and discerning complex decision boundaries. In this work we focus on loop unrolling, a well-known optimization for exposing instruction level parallelism. Using the Open Research Compiler as a testbed, we demonstrate how one can use supervised learning techniques to determine the appropriateness of loop unrolling. We use more than 2,500 loops - drawn from 72 benchmarks - to train two different learning algorithms to predict unroll factors (i.e., the amount by which to unroll a loop) for any novel loop. The technique correctly predicts the unroll factor for 65\% of the loops in our dataset, which leads to a 5\% overall improvement for the SPEC 2000 benchmark suite (9\% for the SPEC 2000 floating point benchmarks).},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {123--134},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.29},
 doi = {http://dx.doi.org/10.1109/CGO.2005.29},
 acmid = {1048981},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Amarasinghe:2005:MCP:1048922.1048982,
 author = {Amarasinghe, Saman},
 title = {Multicores from the Compiler's Perspective: A Blessing or a Curse?},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {137--137},
 numpages = {1},
 url = {http://dx.doi.org/10.1109/CGO.2005.22},
 doi = {http://dx.doi.org/10.1109/CGO.2005.22},
 acmid = {1048982},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chen:2005:OAC:1048922.1048984,
 author = {Chen, Guilin and Kandemir, Mahmut},
 title = {Optimizing Address Code Generation for Array-Intensive DSP Applications},
 abstract = {The application code size is a critical design factor for many embedded systems. Unfortunately, most available compilers optimize primarily for speed of execution rather than code density. As a result, the compiler-generated code can be much larger than necessary. In particular, in the DSP domain, the past research found that optimizing address code generation can be very important since address code can account for over 50\% of all program bits. This paper presents a compiler-directed scheme to minimize the number of instructions to be generated to manipulate address registers found in DSP architectures. As opposed to most of the prior techniques that attempt to reduce the number of such instructions through careful address register assignment, this paper proposes modifying loop access patterns in array-intensive signal processing applications. In addition, it demonstrates how the proposed scheme can cooperate with a data layout optimizer for increasing its benefits further. We also discuss how optimizations that target effective address code generation can conflict with data locality-enhancing transformations. We evaluate the proposed approach using twelve array-intensive embedded applications. Our experimental results indicate that the proposed approach not only leads to significant reductions in code size but also outperforms prior efforts on reducing code size of array-intensive DSP applications.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {141--152},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.23},
 doi = {http://dx.doi.org/10.1109/CGO.2005.23},
 acmid = {1048984},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wu:2005:ESC:1048922.1048985,
 author = {Wu, Peng and Eichenberger, Alexandre E. and Wang, Amy},
 title = {Efficient SIMD Code Generation for Runtime Alignment and Length Conversion},
 abstract = {When generating codes for today's multimedia extensions, one of the major challenges is to deal with memory alignment issues. While hand programming still yields best performing SIMD codes, it is both time consuming and error prone. Compiler technology has greatly improved, including techniques that simdize loops with misaligned accesses by automatically rearranging mis-aligned memory streams in registers. Current techniques are applicable to runtime alignments, but they aggressively reduce the alignment overhead only when all alignments are known at compile time. This paper presents two major enhancements to the state of the art, improving both performance and coverage. First, we propose a novel technique to simdize loops with runtime alignment nearly as efficiently as those with compile-time misalignment. Runtime alignment is pervasive in real applications because it is either part of the algorithms, or it is an artifact of the compiler's inability to extract accurate alignment information from complex applications. Second, we incorporate length conversion operations, e.g., conversions between data of different sizes, into the alignment handling framework. Length conversions are pervasive in multimedia applications where mixed integer types are often used. Supporting length conversion can greatly improve the coverage of simdizable loops. Experimental results indicate that our runtime alignment technique achieves a 19\% to 32\% speedup increase over prior art for a benchmark stressing the impact of misaligned data. We also demonstrate speedup factors of up to 8.11 for real benchmarks over sequential execution.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {153--164},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.18},
 doi = {http://dx.doi.org/10.1109/CGO.2005.18},
 acmid = {1048985},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Shin:2005:SPP:1048922.1048983,
 author = {Shin, Jaewook and Hall, Mary and Cha Jacqueline},
 title = {Superword-Level Parallelism in the Presence of Control Flow},
 abstract = {In this paper, we describe how to extend the concept of superword-level parallelization (SLP), used for multimedia extension architectures, so that it can be applied in the presence of control flow constructs. Superword-level parallelization involves identifying scalar instructions in a large basic block that perform the same operation, and, if dependences do not prevent it, combining them into a superword operation on a multi-word object. A key insight is that we can use techniques related to optimizations for architectures supporting predicated execution, even for multimedia ISAs that do not provide hardware predication. We derive large basic blocks with predicated instructions to which SLP can be applied. We describe how to minimize overheads for superword predicates and re-introduce control flow for scalar operations. We discuss other extensions to SLP to address common features of real multimedia codes. We present automatically-generated performance results on 8 multimedia codes to demonstrate the power of this approach. We observe speedups ranging from 1.97X to 15.07X as compared to both sequential execution and SLP alone.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {165--175},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2005.33},
 doi = {http://dx.doi.org/10.1109/CGO.2005.33},
 acmid = {1048983},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Ravindran:2005:CMD:1048922.1048986,
 author = {Ravindran, Rajiv A. and Nagarkar, Pracheeti D. and Dasika, Ganesh S. and Marsman, Eric D. and Senger, Robert M. and Mahlke, Scott A. and Brown, Richard B.},
 title = {Compiler Managed Dynamic Instruction Placement in a Low-Power Code Cache},
 abstract = {Modern embedded microprocessors use low power on-chip memories called scratch-pad memories to store frequently executed instructions and data. Unlike traditional caches, scratch-pad memories lack the complex tag checking and comparison logic, thereby proving to be efficient in area and power. In this work, we focus on exploiting scratch-pad memories for storing hot code segments within an application. Static placement techniques focus on placing the most frequently executed portions of programs into the scratch-pad. However, static schemes are inherently limited by not allowing the contents of the scratch-pad memory to change at run time. In a large fraction of applications, the instruction memory footprints exceed the scratch-pad memory size, thereby limiting the usefulness of the scratch-pad. We propose a compiler managed dynamic placement algorithm, wherein multiple hot code sequences, or traces, are overlapped with each other in the scratch-pad memory at different points in time during execution. Special copy instructions are provided to copy the traces into the scratch-pad memory at run-time. Using a power estimate, the compiler initially selects the most frequent traces in an application for relocation into the scratch-pad memory. Through iterative code motion and redundancy elimination, copy instructions are inserted in infrequently executed regions of the code. For a 64-byte code cache, the compiler managed dynamic placement achieves an average of 64\% energy improvement over the static solution in a low-power embedded microcontroller.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {179--190},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.13},
 doi = {http://dx.doi.org/10.1109/CGO.2005.13},
 acmid = {1048986},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Nagpurkar:2005:PRP:1048922.1048987,
 author = {Nagpurkar, Priya and Krintz, Chandra and Sherwood, Timothy},
 title = {Phase-Aware Remote Profiling},
 abstract = {Recent advances in networking and embedded device technology have made the vision of ubiquitous computing a reality; users can access the Internet's vast offerings any-time and anywhere. Moreover, battery-powered devices such as personal digital assistants and web-enabled mobile phones have successfully emerged as new access points to the world's digital infrastructure. This ubiquity offers a new opportunity for software developers: users can now participate in the software development, optimization, and evolution process while they use their software. Such participation requires effective techniques for gathering profile information from remote, resource-constrained devices. Further, these techniques must be unobtrusive and transparent to the user; profiles must be gathered using minimal computation, communication, and power. Toward this end, we present a flexible hardware-software scheme for efficient remote profiling. We rely on the extraction of meta information from executing programs in the form of phases, and then use this information to guide intelligent online sampling and to manage the communication of those samples. Our results indicate that phase-based remote profiling can reduce the communication, computation, and energy consumption overheads by 50-75\% over random and periodic sampling.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {191--202},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.26},
 doi = {http://dx.doi.org/10.1109/CGO.2005.26},
 acmid = {1048987},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Bond:2005:PPP:1048922.1048988,
 author = {Bond, Michael D. and McKinley, Kathryn S.},
 title = {Practical Path Profiling for Dynamic Optimizers},
 abstract = {Modern processors are hungry for instructions. To satisfy them, compilers need to find and optimize execution paths across multiple basic blocks. Path profiles provide this context, but their high overhead has so far limited their use by dynamic compilers. We present new techniques for low overhead online practical path profiling (PPP). Following targeted path profiling (TPP), PPP uses an edge profile to simplify path profile instrumentation (profile-guided profiling). PPP improves over prior work by (1) reducing the amount of profiling instrumentation on cold paths and paths that the edge profile predicts well and (2) reducing the cost of the remaining instrumentation. Experiments in an ahead-of-time compiler perform edge profile-guided inlining and unrolling prior to path profiling instrumentation. These transformations are faithful to staged optimization, and create longer, harder to predict paths. We introduce the branch-flow metric to measure path flow as a function of branch decisions, rather than weighting all paths equally as in prior work. On SPEC2000, PPP maintains high accuracy and coverage, but has only 5\% overhead on average (ranging from -3\% to 13\%), making it appealing for use by dynamic compilers.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {205--216},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.28},
 doi = {http://dx.doi.org/10.1109/CGO.2005.28},
 acmid = {1048988},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Vaswani:2005:PHP:1048922.1048989,
 author = {Vaswani, Kapil and Thazhuthaveetil, Matthew J. and Srikant, Y. N.},
 title = {A Programmable Hardware Path Profiler},
 abstract = {For aggressive path-based program optimizations to be profitable in cost-sensitive environments, accurate path profiles must be available at low overheads. In this paper, we propose a low-overhead, non-intrusive hardware path profiling scheme that can be programmed to detect several types of paths including acyclic, intra-procedural paths, paths for a Whole Program Path and extended paths. The profiler consists of a path stack, which detects paths and generates a sequence of path descriptors using branch information from the processor pipeline, and a hot path table that collects a profile of hot paths for later use by a program optimizer. With assistance from the processor's event detection logic, our profiler can track a host of architectural metrics along paths, enabling context-sensitive performance monitoring and bottleneck analysis. We illustrate the utility of our scheme by associating paths with a power metric that estimates power consumption in the cache hierarchy caused by instructions along the path. Experiments using programs from the SPEC CPU2000 benchmark suite show that our path profiler, occupying 7KB of hardware real-estate, collects accurate path profiles (average overlap of 88\% with a perfect profile) at negligible execution time overheads (0.6\% on average).},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {217--228},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.3},
 doi = {http://dx.doi.org/10.1109/CGO.2005.3},
 acmid = {1048989},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Burtscher:2005:AGH:1048922.1048990,
 author = {Burtscher, Martin and Sam, Nana B.},
 title = {Automatic Generation of High-Performance Trace Compressors},
 abstract = {Program execution traces are frequently used in industry and academia. Yet, most trace-compression algorithms have to be re-implemented every time the trace format is changed, which takes time, is error prone, and often results in inefficient solutions. This paper describes and evaluates TCgen, a tool that automatically generates portable, customized, high-performance trace compressors. All the user has to do is provide a description of the trace format and select one or more predictors to compress the fields in the trace records. TCgen translates this specification into C source code and optimizes it for the specified trace format and predictor algorithms. On average, the generated code is faster and compresses better than the six other compression algorithms we have tested. For example, a comparison with SBC, one of the best trace-compression algorithms in the current literature, shows that TCgen's synthesized code compresses SPECcpu2000 address traces 23\% more, decompresses them 24\% faster, and compresses them 1029\% faster.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {229--240},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.6},
 doi = {http://dx.doi.org/10.1109/CGO.2005.6},
 acmid = {1048990},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Reis:2005:SSI:1048922.1048991,
 author = {Reis, George A. and Chang, Jonathan and Vachharajani, Neil and Rangan, Ram and August, David I.},
 title = {SWIFT: Software Implemented Fault Tolerance},
 abstract = {To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates. However, these advances make processors more susceptible to transient faults that can affect correctness. While reliable systems typically employ hardware techniques to address soft-errors, software techniques can provide a lower-cost and more flexible alternative. This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT. SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs. SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism. We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost. Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51\% average speedup.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {243--254},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.34},
 doi = {http://dx.doi.org/10.1109/CGO.2005.34},
 acmid = {1048991},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhang:2005:BIS:1048922.1048992,
 author = {Zhang, Tao and Zhuang, Xiaotong and Pande, Santosh},
 title = {Building Intrusion-Tolerant Secure Software},
 abstract = {In this work, we develop a secret sharing based compiler solution to achieve confidentiality, integrity and availability (intrusion tolerance) of critical data together, rather than tackling them one by one as in previous approaches. Under our scheme, some critical data values are automatically identified by the compiler, whereas some others are specified by the user. The compiler generates code for scattering/assembling and verifying of those critical data values using secret sharing scheme. In this way, we achieve data confidentiality and integrity. We also provide mechanisms to gracefully recover upon data tampering, achieving intrusion tolerance. The implementation of our secret sharing scheme is carefully crafted to achieve low overhead. We further propose several compiler optimizations such as secret-sharing-aware register allocation, rematerialization etc. to reduce the cost of secret sharing further, making our scheme a practical solution in a high performance system.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {255--266},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.8},
 doi = {http://dx.doi.org/10.1109/CGO.2005.8},
 acmid = {1048992},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Koes:2005:PRA:1048922.1048993,
 author = {Koes, David and Goldstein, Seth Copen},
 title = {A Progressive Register Allocator for Irregular Architectures},
 abstract = {Register allocation is one of the most important optimizations a compiler performs. Conventional graph-coloring based register allocators are fast and do well on regular, RISC-like, architectures, but perform poorly on irregular, CISC-like, architectures with few registers and non-orthogonal instruction sets. At the other extreme, optimal register allocators based on integer linear programming are capable of fully modeling and exploiting the peculiarities of irregular architectures but do not scale well. We introduce the idea of a progressive allocator. A progressive allocator finds an initial allocation of quality comparable to a conventional allocator, but as more time is allowed for computation the quality of the allocation approaches optimal. This paper presents a progressive register allocator which uses a multi-commodity network flow model to elegantly represent the intricacies of irregular architectures. We evaluate our allocator as a substitute for gcc's local register allocation pass.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {269--280},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.4},
 doi = {http://dx.doi.org/10.1109/CGO.2005.4},
 acmid = {1048993},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Dai:2005:GCF:1048922.1048994,
 author = {Dai, Xiaoru and Zhai, Antonia and Hsu, Wei-Chung and Yew, Pen-Chung},
 title = {A General Compiler Framework for Speculative Optimizations Using Data Speculative Code Motion},
 abstract = {Data speculative optimization refers to code transformations that allow load and store instructions to be moved across potentially dependent memory operations. Existing research work on data speculative optimizations has mainly focused on individual code transformation. The required speculative analysis that identifies data speculative optimization opportunitiesand the required recovery code generation that guarantees the correctness of their execution are handled separately for each optimization. This paper proposes a new compiler framework to facilitate the design and implementation of general data speculative optimizations such as dead store elimination, redundancy elimination, copy propagation, and code scheduling. This framework allows different data speculative optimizations to share the followings: (i) a speculative analysis mechanism to identify data speculative optimization opportunities by ignoring low probability data dependences from optimizations, and (ii) a recovery code generation mechanism to guarantee the correctness of the data speculative optimizations.The proposed recovery code generation is based on Data Speculative Code Motion (DSCM) that uses code motion to facilitate a desired transformation. Based on the position of the moved instruction, recovery code can be generated accordingly. The proposed framework greatly simplifies the task of incorporating data speculation into non-speculative optimizations by sharing the recovery code generation and the speculative analysis. We have implemented the proposed framework in the ORC 2.1 compiler and demonstrated its effectiveness on SPEC2000 benchmark programs.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {280--290},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2005.1},
 doi = {http://dx.doi.org/10.1109/CGO.2005.1},
 acmid = {1048994},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Guo:2005:PAL:1048922.1048995,
 author = {Guo, Bolei and Bridges, Matthew J. and Triantafyllis, Spyridon and Ottoni, Guilherme and Raman, Easwaran and August, David I.},
 title = {Practical and Accurate Low-Level Pointer Analysis},
 abstract = {Pointer analysis is traditionally performed once, early in the compilation process, upon an intermediate representation (IR) with source-code semantics. However, performing pointer analysis only once at this level imposes a phase-ordering constraint, causing alias information to become stale after subsequent code transformations. Moreover, high-level pointer analysis cannot be used at link time or run time, where the source code is unavailable. This paper advocates performing pointer analysis on a low-level intermediate representation. We present the first context-sensitive and partially flow-sensitive points-to analysis designed to operate at the assembly level. As we will demonstrate, low-level pointer analysis can be as accurate as high-level analysis. Additionally, our low-level pointer analysis also enables a quantitative comparison of propagating high-level pointer analysis results through subsequent code transformations, versus recomputing them at the low level. We show that, for C programs, the former practice is considerably less accurate than the latter.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {291--302},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.27},
 doi = {http://dx.doi.org/10.1109/CGO.2005.27},
 acmid = {1048995},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zilles:2005:RTC:1048922.1048996,
 author = {Zilles, Craig and Neelakantam, Naveen},
 title = {Reactive Techniques for Controlling Software Speculation},
 abstract = {Aggressive software speculation holds significant potential, because it enables program transformations to reduce the program's critical path. Like any form of speculation, however, the key to software speculation is employing it only where it is likely to succeed. While mechanisms for controlling hardware speculation (e.g., saturating counters updated after each instance) are well understood, these techniques do not translate directly to software techniques because changing a speculation requires changing the code. As it stands, the dominant software speculation control technique, non-reactive profile-guided optimization, lacks the robustness to support aggressive speculation. The primary thesis of this paper is that software speculation can be made to be robust by adding a reactive controller that can dynamically adjust the speculation. We make two primary observations about such systems: 1) reactive control systems can select behaviors on which to speculate with performance that equals or exceeds self-training, and 2) such control systems are remarkably latency tolerant. Although reactivity is required, it can be done at a low frequency; latencies of hundreds of thousands, or even millions of cycles, can be tolerated for most actions. Together these two characteristics imply that robust aggressive software speculation is a realistic goal.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {305--316},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2005.30},
 doi = {http://dx.doi.org/10.1109/CGO.2005.30},
 acmid = {1048996},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhao:2005:MFA:1048922.1048997,
 author = {Zhao, Min and Childers, Bruce R. and Soffa, Mary Lou},
 title = {A Model-Based Framework: An Approach for Profit-Driven Optimization},
 abstract = {Although optimizations have been applied for a number of years to improve the performance of software, problems that have been long-standing remain, which include knowing what optimizations to apply and how to apply them. To systematically tackle these problems, we need to understand the properties of optimizations. In our current research, we are investigating the profitability property, which is useful for determining the benefit of applying an optimization. Due to the high cost of applying optimizations and then experimentally evaluating their profitability, we use an analytic model framework for predicting the profitability of optimizations. In this paper, we target scalar optimizations, and in particular, describe framework instances for Partial Redundancy Elimination (PRE) and Loop Invariant Code Motion (LICM). We implemented the framework for both optimizations and compare profit-driven PRE and LICM with a heuristic-driven approach. Our experiments demonstrate that a model-based approach is effective and efficient in that it can accurately predict the profitability of optimizations with low overhead. By predicting the profitability using models, we can selectively apply optimizations. The model-based approach does not require tuning of parameters used in heuristic approaches and works well across different code contexts and optimizations.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {317--327},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2005.2},
 doi = {http://dx.doi.org/10.1109/CGO.2005.2},
 acmid = {1048997},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Odaira:2005:SPH:1048922.1048998,
 author = {Odaira, Rei and Hiraki, Kei},
 title = {Sentinel PRE: Hoisting beyond Exception Dependency with Dynamic Deoptimization},
 abstract = {Many excepting instructions cannot be removed by existing Partial Redundancy Elimination (PRE) algorithms because the ordering constraints must be preserved between the excepting instructions, which we call exception dependencies. In this work, we propose Sentinel PRE, a PRE algorithm that overcomes exception dependencies and retains program semantics. Sentinel PRE first hoists excepting instructions without considering exception dependencies, and then detects exception reordering by fast analysis. If an exception occurs at a reordered instruction, it deoptimizes the code into the one before hoisting. Since we rarely encounter exceptions in real programs, the optimized code is executed in almost all cases. We implemented Sentinel PRE in a Java just-in-time compiler and conducted experiments. The results show 9.0\% performance improvement in the LU program in the Java Grande Forum Benchmark Suite.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {328--338},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2005.32},
 doi = {http://dx.doi.org/10.1109/CGO.2005.32},
 acmid = {1048998},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{2005:AI:1048922.1048999,
 title = {Author Index},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization},
 series = {CGO '05},
 year = {2005},
 isbn = {0-7695-2298-X},
 pages = {339--339},
 numpages = {1},
 url = {http://dx.doi.org/10.1109/CGO.2005.5},
 doi = {http://dx.doi.org/10.1109/CGO.2005.5},
 acmid = {1048999},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:ISC:1121992.1122381,
 title = {International Symposium on Code Generation and Optimization - Title Page},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.01--iii},
 url = {http://dx.doi.org/10.1109/CGO.2006.21},
 doi = {http://dx.doi.org/10.1109/CGO.2006.21},
 acmid = {1122381},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:ISC:1121992.1122382,
 title = {International Symposium on Code Generation and Optimization - Copyright},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.04--},
 url = {http://dx.doi.org/10.1109/CGO.2006.18},
 doi = {http://dx.doi.org/10.1109/CGO.2006.18},
 acmid = {1122382},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:MGC:1121992.1122383,
 title = {Message from the General Co-Chairs},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.10--},
 url = {http://dx.doi.org/10.1109/CGO.2006.23},
 doi = {http://dx.doi.org/10.1109/CGO.2006.23},
 acmid = {1122383},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:MPC:1121992.1122384,
 title = {Message from the Program Chair},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.11--},
 url = {http://dx.doi.org/10.1109/CGO.2006.24},
 doi = {http://dx.doi.org/10.1109/CGO.2006.24},
 acmid = {1122384},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:COM:1121992.1122385,
 title = {Committees},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.12--},
 url = {http://dx.doi.org/10.1109/CGO.2006.7},
 doi = {http://dx.doi.org/10.1109/CGO.2006.7},
 acmid = {1122385},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:REV:1121992.1122386,
 title = {Reviewers},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.14--},
 url = {http://dx.doi.org/10.1109/CGO.2006.31},
 doi = {http://dx.doi.org/10.1109/CGO.2006.31},
 acmid = {1122386},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:CS:1121992.1122387,
 title = {Corporate Sponsors},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {.15--},
 url = {http://dx.doi.org/10.1109/CGO.2006.12},
 doi = {http://dx.doi.org/10.1109/CGO.2006.12},
 acmid = {1122387},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Hazelwood:2006:CIC:1121992.1122388,
 author = {Hazelwood, Kim and Cohn, Robert},
 title = {A Cross-Architectural Interface for Code Cache Manipulation},
 abstract = {Software code caches help amortize the overhead of dynamic binary transformation by enabling reuse of transformed code. Since code caches contain a potentiallyaltered copy of every instruction that executes, run-time access to a code cache can be a very powerful opportunity. Unfortunately, current research infrastructures lack the ability to model and direct code caching, and as a result, past code cache investigations have required access to the source code of the binary transformation system. This paper presents a code cache-aware interface to the Pin dynamic instrumentation system. While a program executes, our interface allows a user to inspect the code cache, receive callbacks when key events occur, and manipulate the code cache contents at will. We demonstrate the utility of this interface on four architectures (IA32, EM64T, IPF, XScale) and present several tools written using our API. These tools include a self-modifying code handler, a two-phase instrumentation analyzer, a code cache visualizer, and custom code cache replacement policies. We also show that tools written using our interface have comparable performance to direct, source-level implementations. Both our interface and sample open-source tools that utilize the interface have been incorporated into the standard distribution of the Pin dynamic instrumentation engine, which has been downloaded over 5,000 times in 18 months.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {17--27},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.3},
 doi = {http://dx.doi.org/10.1109/CGO.2006.3},
 acmid = {1122388},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Bruening:2006:TSC:1121992.1122389,
 author = {Bruening, Derek and Kiriansky, Vladimir and Garnett, Timothy and Banerji, Sanjeev},
 title = {Thread-Shared Software Code Caches},
 abstract = {Software code caches are increasingly being used to amortize the runtime overhead of dynamic optimizers, simulators, emulators, dynamic translators, dynamic compilers, and other tools. Despite the now-widespread use of code caches, techniques for efficiently sharing them across multiple threads have not been fully explored. Some systems simply do not support threads, while others resort to thread-private code caches. Although thread-private caches are much simpler to manage, synchronize, and provide scratch space for, they simply do not scale when applied to many-threaded programs. Thread-shared code caches are needed to target server applications, which employ hundreds of worker threads all performing similar tasks. Yet, those systems that do share their code caches often have bruteforce, inefficient solutions to the challenges of concurrent code cache access: a single global lock on runtime system code and suspension of all threads for any cache management action. This limits the possibilities for cache design and has performance problems with applications that require frequent cache invalidations to maintain cache consistency. In this paper, we discuss the design choices when building thread-shared code caches and enumerate the difficulties of thread-local storage, synchronization, trace building, in-cache lookup tables, and cache eviction. We present efficient solutions to these problems that both scale well and do not require thread suspension. We evaluate our results in DynamoRIO, an industrial-strength dynamic binary translation system, on real-world server applications. On these applications our thread-shared caches use an order of magnitude less memory and improve throughput by up to four times compared to threadprivate caches.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {28--38},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.36},
 doi = {http://dx.doi.org/10.1109/CGO.2006.36},
 acmid = {1122389},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Cooper:2006:TGR:1121992.1122391,
 author = {Cooper, Keith D. and Dasgupta, Anshuman},
 title = {Tailoring Graph-coloring Register Allocation For Runtime Compilation},
 abstract = {Just-in-time compilers are invoked during application execution and therefore need to ensure fast compilation times. Consequently, runtime compiler designers are averse to implementing compile-time intensive optimization algorithms. Instead, they tend to select faster but less effective transformations. In this paper, we explore this trade-off for an important optimization - global register allocation. We present a graph-coloring register allocator that has been redesigned for runtime compilation. Compared to Chaitin- Briggs [7], a standard graph-coloring technique, the reformulated algorithm requires considerably less allocation time and produces allocations that are only marginally worse than those of Chaitin-Briggs. Our experimental results indicate that the allocator performs better than the linear-scan and Chaitin-Briggs allocators on most benchmarks in a runtime compilation environment. By increasing allocation efficiency and preserving optimization quality, the presented algorithm increases the suitability and profitability of a graph-coloring register allocation strategy for a runtime compiler.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {39--49},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.35},
 doi = {http://dx.doi.org/10.1109/CGO.2006.35},
 acmid = {1122391},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhang:2006:SPE:1121992.1122392,
 author = {Zhang, Weifeng and Calder, Brad and Tullsen, Dean M.},
 title = {A Self-Repairing Prefetcher in an Event-Driven Dynamic Optimization Framework},
 abstract = {Software prefetching has been demonstrated as a powerful technique to tolerate long load latencies. However, to be effective, prefetching must target the most critical (frequently missing) loads, and prefetch them sufficiently far in advance. This is difficult to do correctly with a static optimizer, because locality characteristics and cache latencies vary across data inputs and across different machines. This paper presents a mechanism that dynamically inserts prefetch instructions into frequently executed hot traces. Hot traces are dynamically analyzed to identify delinquent loads and the appropriate prefetch distance for those loads. Those prefetches are then inserted into the hot trace. The low overhead of the event-driven dynamic optimization system allows the optimizer to continuously monitor the performance of the software prefetches. This is done to find an accurate and stable prefetch distance and to adapt to changes in program behavior using what we call Self- Repairing prefetching. Relative to the baseline hardware stride prefetching, we find a total 23\% improvement when we use the self-repairing mechanism to adaptively discover the best prefetch distance for each load, which is 12\% better performance than dynamic prefetching techniques without adaptive repairing.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {50--64},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/CGO.2006.4},
 doi = {http://dx.doi.org/10.1109/CGO.2006.4},
 acmid = {1122392},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chen:2006:JJB:1121992.1122393,
 author = {Chen, Miaobo and Goldenberg, Shalom and Srinivas, Suresh and Ushakov, Valery and Wang, Young and Zhang, Qi and Lin, Eric and Zach, Yoav},
 title = {Java JNI Bridge: A Framework for Mixed Native ISA Execution},
 abstract = {Managed Runtime Environments (MRTEs) such as the Java platform promise a cross platform "Write Once, Deploy Anywhere" mechanism. However, MRTE applications that contain native method calls are not seamlessly portable across platforms. In this paper, we describe a new approach to transparently run Java applications containing native method calls to one ISA (Instruction Set Architecture) on a different ISA's Java platform. This approach operates within the same operating system process that executes the application but with the support of a dynamic translator. This paper describes the technical challenges and solutions of such an in-process implementation within a production Java Virtual Machine (JVM). These include interfacing the JVM with a dynamic translator to support native calls to a different ISA, pursuing a JVM-independent implementation, enhancing the dynamic translator to support shared libraries in addition to executables, marshalling arguments across ISA boundaries, and providing full support for all Java features such as multi-threading. The paper presents performance results for an end-user application, showing our approach to be 2-3 times faster than other approaches.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {65--75},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.22},
 doi = {http://dx.doi.org/10.1109/CGO.2006.22},
 acmid = {1122393},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Venstermans:2006:SJO:1121992.1122394,
 author = {Venstermans, Kris and Eeckhout, Lieven and Bosschere, Koen De},
 title = {Space-Efficient 64-bit Java Objects through Selective Typed Virtual Addressing},
 abstract = {Memory performance is an important design issue for contemporary systems given the ever increasing memory gap. This paper proposes a space-efficient Java object model for reducing the memory consumption of 64-bit Java virtual machines. We propose Selective Typed Virtual Addressing (STVA) which uses typed virtual addressing (TVA) or implicit typing for reducing the header of 64-bit Java objects. The idea behind TVA is to encode the object's type in the object's virtual address. In other words, all objects of a given type are allocated in a contiguous memory segment. As such, the type information can be removed from the object's header which reduces the number of allocated bytes per object. Whenever type information is needed for the given object, masking is applied to the object's virtual address. Unlike previous work on implicit typing, we apply TVA to a selected number of frequently allocated and/or long-lived object types. This limits the amount of memory fragmentation. We implement STVA in the 64-bit version of the Jikes RVM on an AIX IBM platform and compare its performance against a traditional VM implementation without STVA using a multitude of Java benchmarks. We conclude that STVA reduces memory consumption by on average 15.5\% (and up to 39\% for some benchmarks). In terms of performance, STVA generally does not affect performance, however for some benchmarks we observe statistically significant performance speedups, up to 24\%.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {76--86},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.34},
 doi = {http://dx.doi.org/10.1109/CGO.2006.34},
 acmid = {1122394},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Sundaresan:2006:EMD:1121992.1122395,
 author = {Sundaresan, Vijay and Maier, Daryl and Ramarao, Pramod and Stoodley, Mark},
 title = {Experiences with Multi-threading and Dynamic Class Loading in a Java Just-In-Time Compiler},
 abstract = {In this paper, we describe the techniques that have been implemented in the IBM TestaRossa (TR) Just-in-Time(JIT) compiler to safely perform aggressive code patching and collect accurate profiles in the context of a Java application employing multiple threads and dynamic class loading and unloading. Previous work in these areas either did not account for the synchronization cost of safety or dynamic class loading/unloading effects in a heavily multithreaded program or did not consider how different patching techniques may be required for different platforms where instruction cache coherence guarantees vary. We evaluate the space and time overhead to make our profiling framework correct, showing that privatizing the profiling variables to achieve correctness impacts execution time only minimally but it can grow the stack frames for profiled methods by less than 15\% on average for the SPECjvm98 and SPECjbb2000 benchmarks. Since methods are profiled for only a brief time and the stack frames themselves are not large, we do not consider this growth to be prohibitive. The techniques reported in this paper are implemented in the 1.5.0 release of the IBM Developer Kit for Java targeting 12 different processor-operating system platforms.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {87--97},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.16},
 doi = {http://dx.doi.org/10.1109/CGO.2006.16},
 acmid = {1122395},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Su:2006:DCH:1121992.1122396,
 author = {Su, Lixin and Lipasti, Mikko H.},
 title = {Dynamic Class Hierarchy Mutation},
 abstract = {Class hierarchies in object-oriented programs are used to capture various attributes of the underlying objects they represent, allowing programmers to encapsulate common attributes in base classes while distributing private attributes in lower-level derived classes. In essence, the semantics of the class hierarchy elegantly capture some of the possible states that a particular object can assume. However, class hierarchies are often poorly designed or evolve in ways that fail to fully capture the stateful behavior of objects. This paper proposes an automated approach for detecting stateful class attributes, and then mutating the class hierarchy dynamically to capture such behavior by creating implicit derived classes that can be specialized for specific object states. Our scheme captures both run-time static behavior, which could have been captured by the programmer by restructuring the class hierarchy at the source level; as well as run-time variant behavior, which cannot be captured using source code transformations. In the latter case, objects transition from one state to another and are dynamically mutated from a derived class to a peer derived class corresponding to the object's new state. These class hierarchy mutations create new opportunities for conventional optimizations such as constant propagation, function specialization, and dead code elimination. For our benchmark set, which includes two versions of SPECjbb, we measure speedups of 1.9\% to 31.4\% within our Jikes-based prototype implementation, with negligible increases in compilation overhead and object code size.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {98--110},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.14},
 doi = {http://dx.doi.org/10.1109/CGO.2006.14},
 acmid = {1122396},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Nagpurkar:2006:OPD:1121992.1122397,
 author = {Nagpurkar, Priya and Krintz, Chandra and Hind, Michael and Sweeney, Peter F. and Rajan, V. T.},
 title = {Online Phase Detection Algorithms},
 abstract = {Today's virtual machines (VMs) dynamically optimize an application as it is executing, often employing optimizations that are specialized for the current execution profile. An online phase detector determines when an executing program is in a stable period of program execution (a phase) or is in transition. A VM using an online phase detector can apply specialized optimizations during a phase or reconsider optimization decisions between phases. Unfortunately, extant approaches to detecting phase behavior rely on either offline profiling, hardware support, or are targeted toward a particular optimization. In this work, we focus on the enabling technology of online phase detection. More specifically, we contribute (a) a novel framework for online phase detection, (b) multiple instantiations of the framework that produce novel online phase detection algorithms, (c) a novel client- and machine-independent baseline methodology for evaluating the accuracy of an online phase detector, (d) a metric to compare online detectors to this baseline, and (e) a detailed empirical evaluation, using Java applications, of the accuracy of the numerous phase detectors.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {111--123},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.26},
 doi = {http://dx.doi.org/10.1109/CGO.2006.26},
 acmid = {1122397},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Das:2006:RML:1121992.1122398,
 author = {Das, Abhinav and Lu, Jiwei and Hsu, Wei-Chung},
 title = {Region Monitoring for Local Phase Detection in Dynamic Optimization Systems},
 abstract = {Dynamic optimization relies on phase detection for two important functions (1) To detect change in code working set and (2) To detect change in performance characteristics that can affect optimization strategy. Current prototype runtime optimization systems [12][13] compare aggregate metrics like CPI over fixed time intervals to detect a change in working set and a change in performance. While simple and cost-effective, these metrics are sensitive to sampling rate and interval size. A phase detection scheme that computes performance metrics by aggregating the performance of individually optimized regions can be misled by some regions impacting aggregate metrics adversely. In this paper, we investigate the benefits and limitations of using aggregate metrics for phase detection, which we call Global Phase Detection (GPD). We present a new model to detect change in working set and propose that the scope of phase detection be limited to within the candidate regions for optimization. By associating phase detection to individual regions we can isolate the effects of regions that are inherently unstable. This approach, which we call Local Phase Detection (LPD), shows improved performance on several benchmarks even when global phase detection is not able to detect stable phases.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {124--134},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.39},
 doi = {http://dx.doi.org/10.1109/CGO.2006.39},
 acmid = {1122398},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Lau:2006:SSP:1121992.1122399,
 author = {Lau, Jeremy and Perelman, Erez and Calder, Brad},
 title = {Selecting Software Phase Markers with Code Structure Analysis},
 abstract = {Most programs are repetitive, where similar behavior can be seen at different execution times. Algorithms have been proposed that automatically group similar portions of a program's execution into phases, where samples of execution in the same phase have homogeneous behavior and similar resource requirements. In this paper, we present an automated profiling approach to identify code locations whose executions correlate with phase changes. These "software phase markers" can be used to easily detect phase changes across different inputs to a program without hardware support. Our approach builds a combined hierarchical procedure call and loop graph to represent a program's execution, where each edge also tracks the max, average, and standard deviation in hierarchical execution variability on paths from that edge. We search this annotated call-loop graph for instructions in the binary that accurately identify the start of unique stable behaviors across different inputs. We show that our phase markers can be used to accurately partition execution into units of repeating homogeneous behavior by counting execution cycles and data cache hits. We also compare the use of our software markers to prior work on guiding data cache reconfiguration using datareuse markers. Finally, we show that the phase markers can be used to partition the program's execution at code transitions to pick accurately simulation points for SimPoint. When simulation points are defined in terms of phase markers, they can potentially be re-used across inputs, compiler optimizations, and different instruction set architectures for the same source code.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {135--146},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.32},
 doi = {http://dx.doi.org/10.1109/CGO.2006.32},
 acmid = {1122399},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Mysore:2006:POA:1121992.1122400,
 author = {Mysore, Shashidhar and Agrawal, Banit and Sherwood, Timothy and Shrivastava, Nisheeth and Suri, Subhash},
 title = {Profiling over Adaptive Ranges},
 abstract = {Modern computer systems are called on to deal with billions of events every second, whether they are instructions executed, memory locations accessed, or packets forwarded. This presents a serious challenge to those who seek to quantify, analyze, or optimize such systems, because important trends and behaviors may easily be lost in a sea of data. We present Range Adaptive Profiling (RAP) as a new and general purpose profiling method capable of hierarchically classifying streams of data efficiently in hardware. Through the use of RAP, events in an input stream are dynamically classified into increasingly precise categories based on the frequency with which they occur. The more important a class, or range of events, the more precisely it is quantified. Despite the dynamic nature of our technique, we build upon tight theoretic bounds covering both worst-case error as well as the requiredmemory. In the limit, it is known that error and the memory bounds can be independent of the stream size, and grow only linearly with the level of precision desired. Significantly, we expose the critical constants in these algorithms and through careful engineering, algorithm re-design, and use of heuristics, we show how a high performance profile system can be implemented for Range Adaptive Profiling. RAP can be used on various profiles such as PCs, load values, and memory addresses, and has a broad range of uses, from hot-region profiling to quantifying cache miss value locality. We propose two methods of implementation, one in software and the other with specialized hardware, and we show that with just 8k bytes of memory range profiles can be gathered with an average accuracy of 98\%.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {147--158},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.30},
 doi = {http://dx.doi.org/10.1109/CGO.2006.30},
 acmid = {1122400},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kim:2006:DIB:1121992.1122401,
 author = {Kim, Hyesoon and Suleman, M. Aater and Mutlu, Onur and Patt, Yale N.},
 title = {2D-Profiling: Detecting Input-Dependent Branches with a Single Input Data Set},
 abstract = {Static compilers use profiling to predict run-time program behavior. Generally, this requires multiple input sets to capture wide variations in run-time behavior. This is expensive in terms of resources and compilation time. We introduce a new mechanism, 2D-profiling, which profiles with only one input set and predicts whether the result of the profile would change significantly across multiple input sets. We use 2D-profiling to predict whether a branch's prediction accuracy varies across input sets. The key insight is that if the prediction accuracy of an individual branch varies significantly over a profiling run with one input set, then it is more likely that the prediction accuracy of that branch varies across input sets. We evaluate 2D-profiling with the SPEC CPU 2000 integer benchmarks and show that it can identify input-dependent branches accurately.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {159--172},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/CGO.2006.1},
 doi = {http://dx.doi.org/10.1109/CGO.2006.1},
 acmid = {1122401},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wentzlaff:2006:CVA:1121992.1122402,
 author = {Wentzlaff, David and Agarwal, Anant},
 title = {Constructing Virtual Architectures on a Tiled Processor},
 abstract = {As the amount of available silicon resources on one chip increases, we have seen the advent of ever increasing parallel resources integrated on-chip. Many architectures use these resources as individually controllable, parallel processing elements. While such architectures excel at parallel applications, they seldom support legacy single-threaded applications. In this work, we propose using parallel resources to facilitate execution of legacy codes with acceptable performance on parallel architectures containing a drastically different instruction set through the use of an all software parallel dynamic binary translation engine. This engine spatially implements different portions of a superscalar processor across distinct parallel elements thus exploiting the pipeline parallelism inherent in a superscalar. This virtual microarchitecture facilitates changing the allocation of silicon resources between different superscalar units in software which is not possible when special purpose physical resources are built. We propose building dynamically reconfigurable architectures that inspect the current virtual machine configuration along with the dynamic instruction stream and change the configuration to best suit the program's needs at runtime. An x86 to Raw parallel translation engine was built in which tiles dedicated to translation can be traded for tiles dedicated to the memory system as an example of dynamic reconfiguration.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {173--184},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.11},
 doi = {http://dx.doi.org/10.1109/CGO.2006.11},
 acmid = {1122402},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Smith:2006:CEA:1121992.1122404,
 author = {Smith, Aaron and Gibson, Jon and Maher, Bertrand and Nethercote, Nick and Yoder, Bill and Burger, Doug and McKinle, Kathryn S. and Burrill, Jim},
 title = {Compiling for EDGE Architectures},
 abstract = {Explicit Data Graph Execution (EDGE) architectures offer the possibility of high instruction-level parallelism with energy efficiency. In EDGE architectures, the compiler breaks a program into a sequence of structured blocks that the hardware executes atomically. The instructions within each block communicate directly, instead of communicating through shared registers. The TRIPS EDGE architecture imposes restrictions on its blocks to simplify the microarchitecture: each TRIPS block has at most 128 instructions, issues at most 32 loads and/or stores, and executes at most 32 register bank reads and 32 writes. To detect block completion, each TRIPS block must produce a constant number of outputs (stores and register writes) and a branch decision. The goal of the TRIPS compiler is to produce TRIPS blocks full of useful instructions while enforcing these constraints. This paper describes a set of compiler algorithms that meet these sometimes conflicting goals, including an algorithm that assigns load and store identifiers to maximize the number of loads and stores within a block. We demonstrate the correctness of these algorithms in simulation on SPEC2000, EEMBC, and microbenchmarks extracted from SPEC2000 and others. We measure speedup in cycles over an Alpha 21264 on microbenchmarks.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {185--195},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.10},
 doi = {http://dx.doi.org/10.1109/CGO.2006.10},
 acmid = {1122404},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Liao:2006:DCT:1121992.1122405,
 author = {Liao, Shih-wei and Du, Zhaohui and Wu, Gansha and Lueh, Guei-Yuan},
 title = {Data and Computation Transformations for Brook Streaming Applications on Multiprocessors},
 abstract = {Multicore processors are about to become prevalent in the PC world. Meanwhile, over 90\% of the computing cycles are estimated to be consumed by streaming media applications [24]. Although stream programming exposes parallelism naturally, we found that achieving high performance on multiprocessors is challenging. Therefore, we develop a parallel compiler for the Brook streaming language with aggressive data and computation transformations. First, we formulate fifteen Brook stream operators in terms of systems of inequalities. Our compiler optimizes the modeled operators to improve memory footprint and performance. Second, the stream computation including both kernels and operators is mapped to the affine partitioning model by modeling each kernel as an implicit loop nest over stream elements. Note that our general abstraction is not limited to Brook. Our modeling and transformations yield high performance on uniprocessors as well. The geometric mean of speedups is 4.7 on ten streaming applications on a Xeon. On multiprocessors, we show that exploiting the standard intra-kernel data parallelism is inferior to our general modeling. The former yields a speedup of 1.5 for ten applications on a 4-way Xeon, while the latter achieves a speedup of 6.4 over the same baseline. We show that our compiler effectively reduces memory footprint, exploits parallelism, and circumvents phase-ordering issues.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {196--207},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.13},
 doi = {http://dx.doi.org/10.1109/CGO.2006.13},
 acmid = {1122405},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chu:2006:CDP:1121992.1122406,
 author = {Chu, Michael L. and Mahlke, Scott A.},
 title = {Compiler-directed Data Partitioning for Multicluster Processors},
 abstract = {Multicluster architectures overcome the scaling problem of centralized resources by distributing the datapath, register file, and memory subsystem across multiple clusters connected by a communication network. Traditional compiler partitioning algorithms focus solely on distributing operations across the clusters to maximize instruction-level parallelism. The distribution of data objects is generally ignored. In this work, we examine explicit partitioning of data objects and its affects on operation partitioning. The partitioning of data objects must consider several factors: object size, access frequency/pattern, and dependence patterns between operations that manipulate the objects. This work proposes a compiler-directed approach to synergistically partition both data objects and computation across multiple clusters. First, a global view of the application determines the interaction between data memory objects and their associated computation. Next, data objects are partitioned across multiple clusters with knowledge of the associated computation required by the application. Finally, the resulting distribution of the data objects is relayed to a region-level computation partitioner, which carefully places computation operations in a performance- centric manner.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {208--220},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.9},
 doi = {http://dx.doi.org/10.1109/CGO.2006.9},
 acmid = {1122406},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chakrabarti:2006:IAB:1121992.1122407,
 author = {Chakrabarti, Dhruva R. and Liu, Shin-Ming},
 title = {Inline Analysis: Beyond Selection Heuristics},
 abstract = {Research on procedure inlining has mainly focused on heuristics that decide whether inlining a particular call-site maximizes application performance. However, other equally important aspects of inline analysis such as call-site analysis order, indirect effects of inlining, and selection of the most profitable version of a procedure warrant more attention. This paper evaluates a number of different sequences in which call-sites are examined for inlining and shows that choosing the correct order is crucial to obtaining the best run-time performance. We then present a novel, work-list-based, and updated sequence that achieves the best results. While applying cross-module inline analysis on large applications with thousands of files and millions of lines of code, we separate the analysis from the transformation phase and allow the former to work solely on summary information in order to reduce compile-time and memory consumption. A focus of this paper is to enumerate the summaries that our compiler maintains, present a technique to compute the goodness factor on which the work-list sequence is based, and describe methods to continuously update the summaries as and when a call-site is accepted for inlining. We then introduce inline specialization, a new technique that facilitates inlining into call chains selectively. The power of inline specialization lies in its ability to choose the most profitable version of the called procedure without having to maintain multiple versions at any point of time. We discuss implementation of these techniques in the HPUX Itanium production compiler and present experimental results showing that a dynamic work-list based analysis order, comprehensive summary updates, and inline specialization significantly improve performance of applications.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {221--232},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.17},
 doi = {http://dx.doi.org/10.1109/CGO.2006.17},
 acmid = {1122407},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hundt:2006:PSL:1121992.1122408,
 author = {Hundt, Robert and Mannarswamy, Sandya and Chakrabarti, Dhruva},
 title = {Practical Structure Layout Optimization and Advice},
 abstract = {With the delta between processor clock frequency and memory latency ever increasing and with the standard locality improving transformations maturing, compilers increasingly seek to modify an application's data layout to improve spatial and temporal locality and to reduce cache miss and page fault penalties. In this paper we describe a practical implementation of the data layout optimizations Structure Splitting, Structure Peeling, Structure Field Reordering and Dead Field Removal, both for profile and non-profile based compilations. We demonstrate significant performance gains, but find that automatic transformations fail for a relatively high number of record types because of legality violations or profitability constraints. Additionally, we find a class of desirable transformations for which the framework cannot provide satisfying results. To address this issue we complement the automatic transformations with an advisory tool. We reuse the compiler analysis done for automatic transformation and correlate its results with peformance data collected during runtime for structure fields, such as data cache misses and latencies. We then use the compiler as a pefomtance analysis and reporting tool and provide insight into how to layout structure types more eficiently.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {233--244},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.29},
 doi = {http://dx.doi.org/10.1109/CGO.2006.29},
 acmid = {1122408},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Lupo:2006:PRA:1121992.1122409,
 author = {Lupo, Christopher and Wilken, Kent D.},
 title = {Post Register Allocation Spill Code Optimization},
 abstract = {A highly optimized register allocator should provide an efficient placement of save/restore code for procedures that contain calls. This paper presents a new approach to placing callee-saved save and restore instructions that generalizes Chow's shrink-wrapping technique[6]. An efficient, profile-guided, hierarchical spill code placement algorithm is used to analyze the structure of a procedure to calculate the minimum dynamic execution count locations to place callee-saved save and restore code. The algorithm is implemented in the Gnu Compiler Collection and has been tested on the SPEC CPU2000 Integer Benchmark suite. Results show that the technique reduces the number of dynamic load and store instructions by 15\% compared to saving and restoring at procedure entry and exit while Chow's shrink-wrapping technique reduces dynamic load and store instructions by only 1\% compared to saving and restoring at procedure entry and exit. The dynamic number of calleesaved save and restore instructions inserted with this new approach is never greater than the number produced by Chow's shrink-wrapping technique or the placement at procedure entry and exit.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {245--255},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.28},
 doi = {http://dx.doi.org/10.1109/CGO.2006.28},
 acmid = {1122409},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Son:2006:CAR:1121992.1122410,
 author = {Son, Seung Woo and Chen, Guangyu and Kandemir, Mahmut},
 title = {A Compiler-Guided Approach for Reducing Disk Power Consumption by Exploiting Disk Access Locality},
 abstract = {Power consumption of large servers and clusters has recently been a popular research topic, since this issue is important from both technical and environmental viewpoints. The prior research proposed disk power management as one of the important ways of reducing overall power of a large system and considered both hardware-based and softwareguided disk power reduction schemes. One of the common characteristics of the previously proposed approaches to disk power reduction is that they work with a given disk access pattern. In comparison, the goal of the approach proposed in this paper is to restructure application code using an optimizing compiler so that disk idle periods are lengthened. This in turn allows the underlying disk power management scheme to be more effective since such schemes usually prefer the long idle periods over the short ones. Our approach targets at large scientific applications that operate on disk-resident arrays using nested loops and exhibit regular data access patterns. To test the effectiveness of the proposed approach, we implemented it within an optimizing compiler and performed experiments with six data-intensive applications that manipulate disk-resident data. Our experimental analysis shows that the proposed approach is very successful in practice and reduces the total disk energy consumption on average by 18.17\%, as compared to an execution without any disk power management, and by 11.55\%, as compared to an execution that employs disks with low-power capabilities without our code restructuring approach.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {256--268},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.2},
 doi = {http://dx.doi.org/10.1109/CGO.2006.2},
 acmid = {1122410},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Li:2006:ODB:1121992.1122411,
 author = {Li, Jianhui and Zhang, Qi and Xu, Shu and Huang, Bo},
 title = {Optimizing Dynamic Binary Translation for SIMD Instructions},
 abstract = {Dynamic binary translation technology allows a program written for one architecture to be executed on a second architecture without recompiling the source code. Effective dynamic binary translation of SIMD (Single Instruction Multiple Data) instructions has become more and more important as SIMD extensions have gained popularity among general-purpose CPUs within the last decade. Many SIMD extensions allow an SIMD1 register to hold data of different types at different times. Supporting multiple data types within the same register complicates the task of a dynamic translator, which may or may not be able to determine the type of the register at translation time. We propose the SIMD data type tracking algorithm to translate the SIMD instructions and three algorithms to further optimize the translation. Our results show that the three optimizing algorithms give overall 3.89\% performance improvement for SPEC2K INT benchmarks and 6.61\% for SPEC2K FP benchmarks.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {269--280},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.27},
 doi = {http://dx.doi.org/10.1109/CGO.2006.27},
 acmid = {1122411},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Nuzman:2006:MA:1121992.1122403,
 author = {Nuzman, Dorit and Henderson, Richard},
 title = {Multi-platform Auto-vectorization},
 abstract = {The recent proliferation of the Single Instruction Multiple Data (SIMD) model has lead to a wide variety of implementations. These have been incorporated into many platforms, from gaming machines and DSPs to general purpose architectures. In this paper we present an automatic vectorizer as implemented in GCC, the most multi-targetable compiler available today. We discuss the considerations involved in developing a multi-platform vectorization technology, and demonstrate how our vectorization scheme is suited to a variety of SIMD architectures. Experiments on four different SIMD platforms demonstrate that our automatic vectorization scheme is able to efficiently support individual platforms, achieving significant speedups on key kernels.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {281--294},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/CGO.2006.25},
 doi = {http://dx.doi.org/10.1109/CGO.2006.25},
 acmid = {1122403},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Agakov:2006:UML:1121992.1122412,
 author = {Agakov, F. and Bonilla, E. and Cavazos, J. and Franke, B. and Fursin, G. and O'Boyle, M. F. P. and Thomson, J. and Toussaint, M. and Williams, C. K. I.},
 title = {Using Machine Learning to Focus Iterative Optimization},
 abstract = {Iterative compiler optimization has been shown to outperform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper develops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an independent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C6713 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magnitude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {295--305},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/CGO.2006.37},
 doi = {http://dx.doi.org/10.1109/CGO.2006.37},
 acmid = {1122412},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kulkarni:2006:EOP:1121992.1122413,
 author = {Kulkarni, Prasad A. and Whalley, David B. and Tyson, Gary S. and Davidson, Jack W.},
 title = {Exhaustive Optimization Phase Order Space Exploration},
 abstract = {The phase-ordering problem is a long standing issue for compiler writers. Most optimizing compilers typically have numerous different code-improving phases, many of which can be applied in any order. These phases interact by enabling or disabling opportunities for other optimization phases to be applied. As a result, varying the order of applying optimization phases to a program can produce different code, with potentially significant performance variation amongst them. Complicating this problem further is the fact that there is no universal optimization phase order that will produce the best code, since the best phase order depends on the function being compiled, the compiler, and the target architecture characteristics. Moreover, finding the optimal optimization sequence for even a single function is hard as the space of attempted optimization phase sequences is huge and the interactions between different optimizations are poorly understood. Most previous studies performed to search for the most effective optimization phase sequence assume the optimization phase order search space to be extremely large, and hence consider exhaustive exploration of this space infeasible. In this paper we show that even though the attempted search space is extremely large, with careful and aggressive pruning it is possible to limit the actual search space with no loss of information so that it can be completely evaluated in a matter of minutes or a few hours for most functions. We were able to exhaustively enumerate all the possible function instances that can be produced by different phase orderings performed by our compiler for more than 98\% of the functions in our benchmark suite. In this paper we describe the algorithm we used to make exhaustive search of the optimization phase order space possible. We then analyze this space to automatically calculate relationships between different phases. Finally, we show that the results of this analysis can be used to reduce the compilation time for a conventional batch compiler.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {306--318},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.15},
 doi = {http://dx.doi.org/10.1109/CGO.2006.15},
 acmid = {1122413},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Pan:2006:FEO:1121992.1122414,
 author = {Pan, Zhelong and Eigenmann, Rudolf},
 title = {Fast and Effective Orchestration of Compiler Optimizations for Automatic Performance Tuning},
 abstract = {Although compile-time optimizations generally improve program performance, degradations caused by individual techniques are to be expected. One promising research direction to overcome this problem is the development of dynamic, feedback-directed optimization orchestration algorithms, which automatically search for the combination of optimization techniques that achieves the best program performance. The challenge is to develop an orchestration algorithm that finds, in an exponential search space, a solution that is close to the best, in acceptable time. In this paper, we build such a fast and effective algorithm, called Combined Elimination (CE). The key advance of CE over existing techniques is that it takes the least tuning time (57\% of the closest alternative), while achieving the same program performance. We conduct the experiments on both a Pentium IV machine and a SPARC II machine, by measuring performance of SPEC CPU2000 benchmarks under a large set of 38 GCC compiler options. Furthermore, through orchestrating a small set of optimizations causing the most degradation, we show that the performance achieved by CE is close to the upper bound obtained by an exhaustive search algorithm. The gap is less than 0.2\% on average.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {319--332},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/CGO.2006.38},
 doi = {http://dx.doi.org/10.1109/CGO.2006.38},
 acmid = {1122414},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Borin:2006:STC:1121992.1122415,
 author = {Borin, Edson and Wang, Cheng and Wu, Youfeng and Araujo, Guido},
 title = {Software-Based Transparent and Comprehensive Control-Flow Error Detection},
 abstract = {Shrinking microprocessor feature size and growing transistor density may increase the soft-error rates to unacceptable levels in the near future. While reliable systems typically employ hardware techniques to address soft-errors, software-based techniques can provide a less expensive and more flexible alternative. This paper presents a control-flow error classification and proposes two new software-based comprehensive control-flow error detection techniques. The new techniques are better than the previous ones in the sense that they detect errors in all the branch-error categories. We implemented the techniques in our dynamic binary translator so that the techniques can be applied to existing x86 binaries transparently. We compared our new techniques with the previous ones and we show that our methods cover more errors while has similar performance overhead.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {333--345},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.33},
 doi = {http://dx.doi.org/10.1109/CGO.2006.33},
 acmid = {1122415},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhang:2006:COR:1121992.1122416,
 author = {Zhang, Tao and Zhuang, Xiaotong and Pande, Santosh},
 title = {Compiler Optimizations to Reduce Security Overhead},
 abstract = {In this work, we present several compiler optimizations to reduce the overhead due to software protection. We first propose an aggressive rematerialization algorithm which attempts to maximally realize non-trusted values from other trusted values thereby avoiding the security cost for those non-trusted values. We further propose a compiler technique to utilize the secure storage in our machine model efficiently. To optimize the security cost on data that has to be stored in non-trusted storage, we propose a data grouping technique. Security operations can be performed over the group of data instead of over each piece separately. We show an interesting application of the data grouping technique to reduce the security cost. We test the effectiveness of our optimizations on a recently proposed software protection scheme that involves large overhead. Our results show that the above optimizations are effective and reduce the security overhead significantly.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {346--357},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/CGO.2006.8},
 doi = {http://dx.doi.org/10.1109/CGO.2006.8},
 acmid = {1122416},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Nanda:2006:BBI:1121992.1122417,
 author = {Nanda, Susanta and Li, Wei and Lam, Lap-Chung and Chiueh, Tzi-cker},
 title = {BIRD: Binary Interpretation using Runtime Disassembly},
 abstract = {The majority of security vulnerabilities published in the literature are due to software bugs. Many researchers have developed program transformation and analysis techniques to automatically detect or eliminate such vulnerabilities. So far, most of them cannot be applied to commercially distributed applications on the Windows/x86 platform, because it is almost impossible to disassemble a binary file with 100\% accuracy and coverage on that platform. This paper presents the design, implementation, and evaluation of a binary analysis and instrumentation infrastructure for the Windows/x86 platform called BIRD (Binary Interpretation using Runtime Disassembly), which provides two services to developers of security-enhancing program transformation tools: converting binary code into assembly language instructions for further analysis, and inserting instrumentation code at specific places of a given binary without affecting its execution semantics. Instead of requiring a high-fidelity instruction set architectural emulator, BIRD combines static disassembly with an on-demand dynamic disassembly approach to guarantee that each instruction in a binary file is analyzed or transformed before it is executed. It takes 12 student months to develop the first BIRD prototype, which can successfully work for all applications in Microsoft Office suite as well as Internet Explorer and IIS web server, including all DLLs that they use. Moreover, the additional throughput penalty of the BIRD prototype on production server applications such as Apache, IIS, and BIND is uniformly below 4\%.},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {358--370},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/CGO.2006.6},
 doi = {http://dx.doi.org/10.1109/CGO.2006.6},
 acmid = {1122417},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{2006:AI:1121992.1122390,
 title = {Author Index},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {371--},
 url = {http://dx.doi.org/10.1109/CGO.2006.5},
 doi = {http://dx.doi.org/10.1109/CGO.2006.5},
 acmid = {1122390},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2006:ISC:1121992.1122380,
 title = {International Symposium on Code Generation and Optimization - Front Cover},
 abstract = {},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '06},
 year = {2006},
 isbn = {0-7695-2499-0},
 pages = {371.1--},
 url = {http://dx.doi.org/10.1109/CGO.2006.19},
 doi = {http://dx.doi.org/10.1109/CGO.2006.19},
 acmid = {1122380},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:MGC:977395.977650,
 title = {Message from the General Chair},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.09--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977650},
 acmid = {977650},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:MPC:977395.977653,
 title = {Message from the Program Chair},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.10--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977653},
 acmid = {977653},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:CC:977395.977649,
 title = {Committee Chairs},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.11--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977649},
 acmid = {977649},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:SC:977395.977651,
 title = {Steering Committee},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.12--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977651},
 acmid = {977651},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:PC:977395.977654,
 title = {Program Committee},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.13--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977654},
 acmid = {977654},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:ER:977395.977655,
 title = {External Reviewers},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.14--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977655},
 acmid = {977655},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2004:CS:977395.977652,
 title = {Corporate Supporters},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {.15--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977652},
 acmid = {977652},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Luk:2004:IPO:977395.977666,
 author = {Luk, Chi-Keung and Muth, Robert and Patil, Harish and Cohn, Robert and Lowney, Geoff},
 title = {Ispike: A Post-link Optimizer for the Intel\&\#174;Itanium\&\#174;Architecture},
 abstract = {Ispike is post-link optimizer developed for theIntel\&#174;Itanium Processor Family (IPF) processors.TheIPF architecture poses both opportunities and challenges topost-link optimizations.IPF offers a rich set of performancecounters to collect detailed profile information at a low cost,which is essential to post-link optimization being practical.At the same time, the prediction and bundling features onIPF make post-link code transformation more challengingthan on other architectures.In Ispike, we have implementedoptimizations like code layout, instruction prefetching, datalayout, and data prefetching that exploit the IPF advantages,and strategies that cope with the IPF-specific challenges.Using SPEC CINT2000 as benchmarks, we showthat Ispike improves performance by as much as 40\% on theItanium\&#174;2 processor, with average improvement of 8.5\%and 9.9\% over executables generated by the Intel\&#174;Electroncompiler and by the Gcc compiler, respectively.We alsodemonstrate that statistical profiles collected via IPF performancecounters and complete profiles collected via instrumentationproduce equal performance benefit, but theprofiling overhead is significantly lower for performancecounters.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {15--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977666},
 acmid = {977666},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kim:2004:PEP:977395.977665,
 author = {Kim, Dongkeun and Liao, Steve Shih-wei and Wang, Perry H. and Cuvillo, Juan del and Tian, Xinmin and Zou, Xiang and Wang, Hong and Yeung, Donald and Girkar, Milind and Shen, John P.},
 title = {Physical Experimentation with Prefetching Helper Threads on Intel's Hyper-Threaded Processors},
 abstract = {Pre-execution techniques have received much attention as aneffective way of prefetching cache blocks to tolerate the ever-increasingmemory latency. A number of pre-execution techniquesbased on hardware, compiler, or both have been proposed andstudied extensively by researchers. They report promising resultson simulators that model a Simultaneous Multithreading (SMT)processor. In this paper, we apply the helper threading idea ona real multithreaded machine, i.e., Intel Pentium 4 processor withHyper-Threading Technology, and show that indeed it can providewall-clock speedup on real silicon. To achieve further performanceimprovements via helper threads, we investigate threehelper threading scenarios that are driven by automated compilerinfrastructure, and identify several key challenges and opportunitiesfor novel hardware and software optimizations. Our studyshows a program behavior changes dynamically during execution.In addition, the organizations of certain critical hardware structuresin the hyper-threaded processors are either shared or partitionedin the multi-threading mode and thus, the tradeoffs regardingresource contention can be intricate. Therefore, it is essentialto judiciously invoke helper threads by adapting to the dynamicprogram behavior so that we can alleviate potential performancedegradation due to resource contention. Moreover, since adaptingto the dynamic behavior requires frequent thread synchronization,having light-weight thread synchronization mechanisms is important.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {27--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977665},
 acmid = {977665},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhai:2004:COM:977395.977667,
 author = {Zhai, Antonia and Colohan, Christopher B. and Steffan, J. Gregory and Mowry, Todd C.},
 title = {Compiler Optimization of Memory-Resident Value Communication Between Speculative Threads},
 abstract = {Efficient inter-thread value communication is essential for improving performance in Thread-Level Speculation (TLS). Although several mechanisms for improving value communication using hardware support have been proposed, there is relatively little work onexploiting the potential of compiler optimization.Building on recent research on compiler optimization of scalar value communication between speculative threads, we propose compiler techniques for the optimization of memory-resident values.In TLS, data dependences through memory-resident values aretracked by the underlying hardware and preserved by reexecutingany speculative thread that violates a dependence; however, reexecution incurs a large performance penalty and should be usedonly to resolve data dependences that are infrequent. In contrast,value communication for frequently-occurring data dependencesmust be very efficient.In this paper, we propose using the compiler to first identifyfrequently-occurring memory-resident data dependences, then insert synchronization for communicating values to preserve thesedependences. We find that by synchronizing frequently-occurringdata dependences we can significantly improve the efficiency ofparallel execution. A comparison between compiler-inserted andhardware-inserted memory synchronization reveals that the two techniques are complementary, with each technique benefitting different benchmarks.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {39--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977667},
 acmid = {977667},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Dupre:2004:VQB:977395.977672,
 author = {Dupr\'{e}, Michael and Drach, Nathalie and Temam, Olivier},
 title = {VHC: Quickly Building an Optimizer for Complex Embedded Architectures},
 abstract = {To meet the high demand for powerful embedded processors,VLIW architectures are increasingly complex (e.g.,multiple clusters), and moreover, they now run increasinglysophisticated control-intensive applications. As a result, developingarchitecture-specific compiler optimizations is becomingboth increasingly critical and complex, while time-to-market constraints remain very tight.In this article, we present a novel program optimizationapproach, called the Virtual Hardware Compiler (VHC),that can perform as well as static compiler optimizations,but which requires far less compiler development effort,even for complex VLIW architectures and complex targetapplications. The principle is to augment the target processorsimulator with superscalar-like features, observe howthe target program is dynamically optimized during execution,and deduce an optimized binary for the static VLIWarchitecture. Developing an architecture-specific optimizerthen amounts to modifying the processor simulator whichis very fast compared to adapting static compiler optimizationsto an architecture. We also show that a VHC-optimizedbinary trained on a number of data sets performs as wellas a statically-optimized binary on other test data sets. Theonly drawback of the approach is a largely increased compilationtime, which is often acceptable for embedded applicationsand devices. Using the Texas Instruments C62 VLIWprocessor and the associated compiler, we experimentallyshow that this approach performs as well as static compileroptimizations for a much lower research and developmenteffort. Using a single-core C60 and a dual-core clusteredC62 processors, we also show that the same approach canbe used for efficiently retargeting binary programs within afamily of processors.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {53--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977672},
 acmid = {977672},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Moon:2004:SFS:977395.977668,
 author = {Moon, Sungdo and Li, Xinliang D. and Hundt, Robert and Chakrabarti, Dhruva R. and Lozano, Luis A. and Srinivasan, Uma and Liu, Shin-Ming},
 title = {SYZYGY - A Framework for Scalable Cross-Module IPO},
 abstract = {Performing analysis across module boundariesfor an entire program is important for exploitingseveral runtime performance opportunities. However,due to scalability problems in existing full-programanalysis frameworks, such performance opportunitiesare only realized by paying tremendous compile-timecosts. Alternative solutions, such as partialcompilations or user assertions, are complicated orunsafe and as a result, not many commercialapplications are compiled today with cross-moduleoptimizations.This paper presents SYZYGY, a practicalframework for performing efficient, scalable,interprocedural optimizations. The framework isimplemented in the HP-UX Itanium\&#174; compilers andwe have successfully compiled many very largeapplications consisting of millions of lines of code. Weachieved performance improvements of up to 40\%over optimization level two and compilation timeimprovements in the order of 100\% and morecompared to a previous approach.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {65--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977668},
 acmid = {977668},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Lattner:2004:LCF:977395.977673,
 author = {Lattner, Chris and Adve, Vikram},
 title = {LLVM: A Compilation Framework for Lifelong Program Analysis \& Transformation},
 abstract = {This paper describes LLVM (Low Level Virtual Machine),a compiler framework designed to support transparent, lifelongprogram analysis and transformation for arbitrary programs,by providing high-level information to compilertransformations at compile-time, link-time, run-time, and inidle time between runs.LLVM defines a common, low-levelcode representation in Static Single Assignment (SSA) form,with several novel features: a simple, language-independenttype-system that exposes the primitives commonly used toimplement high-level language features; an instruction fortyped address arithmetic; and a simple mechanism that canbe used to implement the exception handling features ofhigh-level languages (and setjmp/longjmp in C) uniformlyand efficiently.The LLVM compiler framework and coderepresentation together provide a combination of key capabilitiesthat are important for practical, lifelong analysis andtransformation of programs.To our knowledge, no existingcompilation approach provides all these capabilities.We describethe design of the LLVM representation and compilerframework, and evaluate the design in three ways: (a) thesize and effectiveness of the representation, including thetype information it provides; (b) compiler performance forseveral interprocedural problems; and (c) illustrative examplesof the benefits LLVM provides for several challengingcompiler problems.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {75--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977673},
 acmid = {977673},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hazelwood:2004:ECC:977395.977664,
 author = {Hazelwood, Kim and Smith, James E.},
 title = {Exploring Code Cache Eviction Granularities in Dynamic Optimization Systems},
 abstract = {Dynamic optimization systems store optimized or translatedcode in a software-managed code cache in order tomaximize reuse of transformed code. Code caches storesuperblocks that are not fixed in size, may contain linksto other superblocks, and carry a high replacement overhead.These additional constraints reduce the effectivenessof conventional hardware-based cache management policies.In this paper, we explore code cache managementpolicies that evict large blocks of code from the code cache,thus avoiding the bookkeeping overhead of managing singlecache blocks. Through a combined simulation and analyticalstudy of cache management overheads, we show thatemploying a medium-grained FIFO eviction policy resultsin an effective balance of cache management complexityand cache miss rates. Under high cache pressure the choiceof medium granularity translates into a significant reductionin overall execution time versus both coarse and finegranularities.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {89--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977664},
 acmid = {977664},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Adl-Tabatabai:2004:IJI:977395.977662,
 author = {Adl-Tabatabai, Ali-Reza and Bharadwaj, Jay and Cierniak, Michal and Eng, Marsha and Fang, Jesse and Lewis, Brian T. and Murphy, Brian R. and Stichnoth, James M.},
 title = {Improving 64-Bit Java IPF Performance by Compressing Heap References},
 abstract = {64-bit processor architectures like the Intel\&#174; Itanium\&#174;Processor Family are designed for large applicationsthat need large memory addresses.When runningapplications that fit within a 32-bit address space, 64-bitCPUs are at a disadvantage compared to 32-bit CPUsbecause of the larger memory footprints for their data.This results in worse cache and TLB utilization, and consequentlylower performance because of increased missratios.This paper considers software techniques for virtualmachines that allow 32-bit pointers to be used on 64-bitCPUs for managed runtime applications that do notneed the full 64-bit address space.We describe ourpointer compression techniques and discuss our experienceimplementating these for Java1 applications.In addition,we give performance results with our techniques forboth the SPEC JVM98 and SPEC JBB2000 benchmarks.We demonstrate a 12\% performance improvement onSPEC JBB2000 and a reduction in the number of garbagecollections required for a given heap size.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {100--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977662},
 acmid = {977662},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Li:2004:DTS:977395.977663,
 author = {Li, Xiaoming and Garzar\'{a}n, Mar\'{\i}a Jes\'{u}s and Padua, David},
 title = {A Dynamically Tuned Sorting Library},
 abstract = {Empirical search is a strategy used during the installation oflibrary generators such as ATLAS, FFTW, and SPIRAL to identify the algorithm or the version of an algorithm that delivers thebest performance. In the past, empirical search has been appliedalmost exclusively to scientific problems. In this paper, we discuss the application of empirical search to sorting, which is oneof the best understood symbolic computing problems. When contrasted with the dense numerical computations of ATLAS, FFTW,and SPIRAL, sorting presents a new challenge, namely that the relative performance of the algorithms depend not only on the characteristics of the target machine and the size of the input data but also on the distribution of values in the input data set.Empirical search is applied in the study reported here as partof a sorting library generator. The resulting routines dynamicallyadapt to the characteristics of the input data by selecting the bestsorting algorithm from a small set of alternatives. To generate therun time selection mechanism our generator makes use of machinelearning to predict the best algorithm as a function of the characteristics of the input data set and the performance of the differentalgorithms on the target machine. This prediction is based on thedata obtained through empirical search at installation time.Our results show that our approach is quite effective. Whensorting data inputs of 12M keys with various standard deviations,our adaptive approach selected the best algorithm for all the inputdata sets and all platforms that we tried in our experiments. Thewrong decision could have introduced a performance degradationof up to 133\%, with an average value of 44\%.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {111--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977663},
 acmid = {977663},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Canal:2004:SO:977395.977647,
 author = {Canal, Ramon and Gonz\'{a}lez, Antonio and Smith, James E.},
 title = {Software-Controlled Operand-Gating},
 abstract = {Operand gating is a technique for improving processorenergy efficiency by gating off sections of the data paththat are unneeded by short-precision (narrow) operands.A method for implementing software-controlled powergating is proposed and evaluated. The instruction setarchitecture (ISA) is enhanced to include opcodes thatspecify operand widths (if not already included in the ISA).A compiler or a binary translator uses statically availableinformation to determine initial value ranges. Thetechnique is enhanced through a profile-based analysisthat results in the specialization of certain code regions fora given value range. After the analysis, instructionopcodes are assigned using the minimum required width.To evaluate this technique the Alpha instruction set isenhanced to include opcodes for 8, 16, and 32 bitoperands. Applying the proposed software technique to theSpecInt95 benchmarks results in energy-delay2savings of 14\%. When combined with previously proposed hardware-based techniques, the energy-delay2 benefit is 28\%.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {125--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977647},
 acmid = {977647},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Almog:2004:SDO:977395.977648,
 author = {Almog, Yoav and Rosner, Roni and Schwartz, Naftali and Schmorak, Ari},
 title = {Specialized Dynamic Optimizations for High-Performance Energy-Efficient Microarchitecture},
 abstract = {We study several major characteristics of dynamic optimizationwithin the PARROT power-aware, trace-cache-basedmicroarchitectural framework. We investigate thebenefit of providing optimizations which although tightlycoupled with the microarchitecture in substance are decoupledin time.The tight coupling in substance provides the potentialfor tailoring optimizations for microarchitecture in amanner impossible or impractical not only for traditionalstatic compilers but even for a JIT. We show that the contributionof common, generic optimizations to processorperformance and energy efficiency may be more thandoubled by creating a more intimate correlation betweenhardware specifics and the optimizer. In particular, dynamicoptimizations can profit greatly from hardwaresupporting fused and SIMDified operations.At the same time, the decoupling in time allows optimizationsto be arbitrarily aggressive without significantperformance loss. We demonstrate that requiring up to512 repetitions before a trace is optimized sacrifices almostno performance or efficiency as compared withlower thresholds. These results confirm the feasibility ofenergy efficient hardware implementation of an aggressiveoptimizer.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {137--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977648},
 acmid = {977648},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Smelyanskiy:2004:PPM:977395.977658,
 author = {Smelyanskiy, Mikhail and Mahlke, Scott and Davidson, Edward S.},
 title = {Probabilistic Predicate-Aware Modulo Scheduling},
 abstract = {Predicated execution enables the removal of branches by convertingsegments of branching code into sequences of conditional operations.An important side effect of this transformation is that thecompiler must unconditionally assign resources to predicated operations.However, a resource is only put to productive use whenthe predicate associated with an operation evaluates to True. To reducethis superfluous commitment of resources, we propose probabilisticpredicate-aware scheduling to assign multiple operationsto the same resource at the same time, thereby over-subscribing itsuse. Assignment is performed in a probabilistic manner using acombination of predicate profile information and predicate analysisaimed at maximizing the benefits of over-subscription in viewof the expected degree of conflict. Conflicts occur when two ormore operations assigned to the same resource have their predicatesevaluate to True. A predicate-aware VLIW processor pipeline detectssuch conflicts, recovers, and correctly executes the conflictingoperations. By increasing the effective throughput of a fixed setof resources, probabilistic predicate-aware scheduling provided anaverage of 20\% performance gain in our evaluations on a 4-issueprocessor, and 8\% gain on a 6-issue processor.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {151--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977658},
 acmid = {977658},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Rong:2004:SSP:977395.977657,
 author = {Rong, Hongbo and Tang, Zhizhong and Govindarajan, R. and Douillet, Alban and Gao, Guang R.},
 title = {Single-Dimension Software Pipelining for Multi-Dimensional Loops},
 abstract = {Traditionally, software pipelining is applied either to theinnermost loop of a given loop nest or from the innermostloop to outer loops. In this paper, we propose a three-stepapproach, called Single-dimension Software Pipelining(SSP), to software pipeline a loop nest at an arbitraryloop level.The first step identifies the most profitable loop level forsoftware pipelining in terms of initiation rate or data reusepotential. The second step simplifies the multi-dimensionaldata-dependence graph (DDG) into a 1-dimensional DDGand constructs a 1-dimensional schedule for the selectedloop level. The third step derives a simple mapping functionwhich specifies the schedule time for the operations of themulti-dimensional loop, based on the 1-dimensional schedule.We prove that the SSP method is correct and at least asefficient as other modulo scheduling methods.We establish the feasibility and correctness of our approachby implementing it on the IA-64 architecture. Experimentalresults on a small number of loops show significantperformance improvements over existing modulo schedulingmethods that software pipeline a loop nest from the innermostloop.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {163--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977657},
 acmid = {977657},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Rong:2004:CGS:977395.977656,
 author = {Rong, Hongbo and Douillet, Alban and Govindarajan, R. and Gao, Guang R.},
 title = {Code Generation for Single-Dimension Software Pipelining of Multi-Dimensional Loops},
 abstract = {Traditionally, software pipelining is applied either to theinnermost loop of a given loop nest or from the innermostloop to the outer loops. In a companion paper, we proposeda scheduling method, called Single-dimension SoftwarePipelining (SSP), to software pipeline a multi-dimensionalloop nest at an arbitrary loop level.In this paper, we describe our solution to SSP code generation.In contrast to traditional software pipelining, SSPhandles two distinct repetitive patterns, and thus requiresnew code generation algorithms. Further, these two distinctrepetitive patterns complicate register assignment and requiretwo levels of register renaming. As rotating registerssupport renaming at only one level, our solution is based ona combination of dynamic register renaming (using rotatingregisters) and static register renaming (using code replication).Finally, code size increase, an even more important issuefor SSP than for traditional software-pipelining, is alsoaddressed. Optimizations are proposed to reduce code sizewithout significant performance degradation.We first present a code generation scheme and subsequentlyimplement it for the IA-64 architecture, making effectiveuse of rotating registers and predicated execution.We present some initial experimental results, which demonstratenot only the feasibility and correctness of our codegeneration scheme, but also its code quality.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {175--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977656},
 acmid = {977656},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Winkel:2004:EPP:977395.977669,
 author = {Winkel, Sebastian},
 title = {Exploring the Performance Potential of Itanium\&\#174; Processors with ILP-based Scheduling},
 abstract = {HP and Intel's Itanium Processor Family (IPF) isconsidered as one of the most challenging processorarchitectures to generate code for.During global instructionscheduling, the compiler must balance the useof strongly interdependent techniques like code motion,speculation and prediction.A too conservative applicationof these features can lead to empty executionslots, contrary to the EPIC philosophy.But overuse cancause resource shortage which spoils the benefit.We tackle this problem using integer linear programming(ILP), a proven standard optimization method.Our ILP model comprises global, partial-ready code motionwith automated generation of compensation codeas well as vital predication.The ILP approach can - withsome restrictions - resolve the interdependences betweenthese decisions and deliver the global optimum.This promises a speedup for compute-intensive applicationsas well as some theoretically funded insightsinto the potential of the architecture.Experiments with several hot functions from theSPEC benchmarks show substantial improvements:Our postpass optimizer reduces the schedule lengthsproduced by Intel's compiler by about 20-40\%.The resultingspeedup of the routines is 16\% on average.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {189--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977669},
 acmid = {977669},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kudlur:2004:FFL:977395.977671,
 author = {Kudlur, Manjunath and Fan, Kevin and Chu, Michael and Ravindran, Rajiv and Clark, Nathan and Mahlke, Scott},
 title = {FLASH: Foresighted Latency-Aware Scheduling Heuristic for Processors with Customized Datapaths},
 abstract = {Application-specific instruction set processors (ASIPs)have the potential to meet the challenging cost, performance,and power goals of future embedded processors bycustomizing the hardware to suit an application. A centralproblem is creating compilers that are capable of dealingwith the heterogeneous and non-uniform hardware createdby the customization process. The processor datapath providesan effective area to customize, but specialized datapathsoften have non-uniform connectivity between the functionunits, making the effective latency of a function unitdependent on the consuming operation. Traditional instructionschedulers break down in this environment due to theirlocally greedy nature of binding the best choice for a singleoperation even though that choice may be poor due toa lack of communication paths. To effectively schedule withnon-uniform connectivity, we propose a foresighted latency-awarescheduling heuristic (FLASH) that performs lookaheadacross future scheduling steps to estimate the effectsof a potential binding. FLASH combines a set of lookaheadheuristics to achieve effective foresight with low compile-timeoverhead.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {201--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977671},
 acmid = {977671},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hu:2004:UDB:977395.977670,
 author = {Hu, Shiliang and Smith, James E.},
 title = {Using Dynamic Binary Translation to Fuse Dependent Instructions},
 abstract = {Instruction scheduling hardware can be simplifiedand easily pipelined if pairs of dependent instructionsare fused so they share a single instruction schedulingslot. We study an implementation of the x86 ISA thatdynamically translates x86 code to an underlying ISAthat supports instruction fusing. A microarchitecturethat is co-designed with the fused instruction set completesthe implementation.In this paper, we focus on the dynamic binarytranslator for such a co-designed x86 virtual machine.The dynamic binary translator first cracks x86 instructionsbelonging to hot superblocks into RISC-stylemicro-operations, and then uses heuristics to fuse togetherpairs of dependent micro-operations.Experimental results with SPEC2000 integer benchmarksdemonstrate that: (1) the fused ISA with dynamicbinary translation reduces the number of schedulingdecisions by about 30\% versus a conventionalimplementation that uses hardware cracking into RISCmicro-operations; (2) an instruction scheduling slotneeds only hold two source register fields even thoughit may hold two instructions; (3) translations generatedin the proposed ISA consume about 30\% less storagethan a corresponding fixed-length RISC-style ISA.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {213--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977670},
 acmid = {977670},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wu:2004:AIP:977395.977661,
 author = {Wu, Youfeng and Breternitz, Mauricio and Quek, Justin and Etzion, Orna and Fang, Jesse},
 title = {The Accuracy of Initial Prediction in Two-Phase Dynamic Binary Translators},
 abstract = {Dynamic binary translators use a two-phase approachto identify and optimize frequently executed codedynamically. In the first step (profiling phase), blocks ofcode are interpreted or quickly translated to collectexecution frequency information for the blocks. In thesecond phase (optimization phase), frequently executedblocks are grouped into regions and advancedoptimizations are applied on them.This approach implicitly assumes that the initialprofile of each block is representative of the blockthroughout its lifetime. This study investigates the abilityof the initial profile to predict the average programbehavior. We compare the predicted behavior of varyinglengths of the initial execution with the average programbehavior for the whole program execution, and use theprediction from the training input as the reference. Ourresult indicates that, for the SPEC2000 benchmarks, evenvery short initial profiles have comparable predictionaccuracy to the traditional profile-guided optimizationsusing the training input, although the initial profile isinadequate for predicting loop trip count information forsome integer programs and several benchmarks canbenefit from phase-awareness during dynamic binarytranslation.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {227--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977661},
 acmid = {977661},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Joshi:2004:TPP:977395.977660,
 author = {Joshi, Rahul and Bond, Michael D. and Zilles, Craig},
 title = {Targeted Path Profiling: Lower Overhead Path Profiling for Staged Dynamic Optimization Systems},
 abstract = {In this paper, we present a technique for reducing theoverhead of collecting path profiles in the context of a dynamicoptimizer. The key idea to our approach, called TargetedPath Profiling (TPP), is to use an edge profile to simplifythe collection of a path profile. This notion of profile-guidedprofiling is a natural fit for dynamic optimizers,which typically optimize the code in a series of stages.TPP is an extension to the Ball-Larus Efficient Path Profilingalgorithm. Its increased efficiency comes from twosources: (i) reducing the number of potential paths by notenumerating paths with cold edges, allowing array accessesto be substituted for more expensive hash table lookups, and(ii) not instrumenting regions where paths can be unambiguouslyderived from an edge profile. Our results suggestthat on average the overhead of profile collection can be reducedby half (SPEC95) to almost two-thirds (SPEC2000)relative to the Ball-Larus algorithm with minimal impact onthe information collected.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {239--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977660},
 acmid = {977660},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Tallam:2004:EPP:977395.977659,
 author = {Tallam, Sriraman and Zhang, Xiangyu and Gupta, Rajiv},
 title = {Extending Path Profiling across Loop Backedges and Procedure Boundaries},
 abstract = {Since their introduction, path profiles have been used toguide the application of aggressive code optimizations andperforming instruction scheduling. However, for optimizationand scheduling, it is often desirable to obtain frequencycounts of paths that extend across loop iterations and crossprocedure boundaries. These longer paths, referred to asinteresting paths in this paper, account for over 75\% of theflow in a subset of SPEC benchmarks. Although the frequencycounts of interesting paths can be estimated frompath profiles, the degree of imprecision of these estimates isvery high. We extend Ball Larus (BL) paths to create slightlylonger overlapping paths and develop an instrumentationalgorithm to collect their frequencies. While these pathsare slightly longer than BL paths, they enable very preciseestimation of frequencies of potentially much longer interestingpaths. Our experiments show that the average cost ofcollecting frequencies of overlapping paths is 86.8\% whichis 4.2 times that of BL paths. However, while the averageimprecision in estimated total flow of interesting paths derivedfrom BL path frequencies ranges from -38 \% to +138\%, the average imprecision in flow estimates derived fromoverlapping path frequencies ranges only from -4\% to +8\%.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {251--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977659},
 acmid = {977659},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {path profiles, overlapping path profiles, profileguided optimization, and instruction scheduling},
} 

@inproceedings{Rastello:2004:OTO:977395.977678,
 author = {Rastello, F. and Ferri\`{e}re, F. de and Guillon, C.},
 title = {Optimizing Translation Out of SSA Using Renaming Constraints},
 abstract = {Static Single Assignment form is an intermediate representationthat usesinstructions to merge values ateach confluent point of the control flow graph. instructionsare not machine instructions and must be renamedback to move instructions when translating out of SSAform.Without a coalescing algorithm, the out of SSAtranslation generates many move instructions.Leung andGeorge use a SSA form for programs represented as native machine instructions, including the use of machinededicated registers.For this purpose, they handlerenaming constraints thanks to a pinning mechanism.Pinningarguments and their corresponding definitionto a common resource is also a very attractive techniquefor coalescing variables.In this paper, extending thisidea, we propose a method to reduce the -related copiesduring the out of SSA translation, thanks to a pinning-basedcoalescing algorithm that is aware of renaming constraints.We implemented our algorithm in the STMicro-electronicsLinear Assembly Optimizer.Our experimentsshow interesting results when comparing to the existingapproaches of Leung and George, Sreedhar etal., and Appel and George for register coalescing.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {265--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977678},
 acmid = {977678},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Ding:2004:CSR:977395.977679,
 author = {Ding, Yonghua and Li, Zhiyuan},
 title = {A Compiler Scheme for Reusing Intermediate Computation Results},
 abstract = {Recent research has shown that programs often exhibitvalue locality. Such locality occurs when a code segment,although executed repeatedly in the program, takes only asmall number of different values as input and, naturally,generates a small number of different outputs. It is potentiallybeneficial to replace such a code segment by a tablewhich records the computation results for the previous inputs.When the program execution re-enters the code segmentwith a repeated input, its computation can be simplifiedto a table look-up. In this paper, we discuss a compilerscheme to identify code segments which are good candidatesfor computation reuse. We discuss the conditions underwhich the table look-up costs less than repeating theexecution, and we perform profiling to identify candidateswhich have many repeated inputs at run time. Comparedto previous work, this scheme requires no special hardwaresupport and is therefore particularly useful for resourceconstrained systems such as handheld computing devices.We implement our scheme and its supporting analysesin GCC. We experiment with several multimedia benchmarksand the GNU Go game by executing them on a handheldcomputing device. The results show the scheme to improvethe performance and to reduce the energy consumptionquite substantially for these programs.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {279--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977679},
 acmid = {977679},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{So:2004:CDL:977395.977674,
 author = {So, Byoungro and Hall, Mary W. and Ziegler, Heidi E.},
 title = {Custom Data Layout for Memory Parallelism},
 abstract = {In this paper, we describe a generalized approach toderiving a custom data layout in multiple memory banksfor array-based computations, to facilitate high-bandwidthparallel memory accesses in modern architectures wheremultiple memory banks can simultaneously feed one ormore functional units. We do not use a fixed data layout,but rather select application-specific layouts according toaccess patterns in the code. A unique feature of this approachis its flexibility in the presence of code reorderingtransformations, such as the loop nest transformations commonlyapplied to array-based computations. We have implementedthis algorithm in the DEFACTO system, a designenvironment for automatically mapping C programsto hardware implementations for FPGA-based systems. Wepresent experimental results for five multimedia kernels thatdemonstrate the benefits of this approach. Our results showthat custom data layout yields results as good as, or betterthan, naive or fixed cyclic layouts, and is significantly betterfor certain access patterns and in the presence of codereordering transformations. When used in conjunction withunrolling loops in a nest to expose instruction-level parallelism,we observe greater than a 75\% reduction in the numberof memory access cycles and speedups ranging from3.96 to 46.7 for 8 memories, as compared to using a singlememory with no unrolling.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {291--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977674},
 acmid = {977674},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Panait:2004:SID:977395.977676,
 author = {Panait, Vlad-Mihai and Sasturkar, Amit and Wong, Weng-Fai},
 title = {Static Identification of Delinquent Loads},
 abstract = {The effective use of processor caches is crucial to theperformance of applications. It has been shown that cachemisses are not evenly distributed throughout a program.In applications running on RISC-style processors, a smallnumber of delinquent load instructions are responsible formost of the cache misses. Identification of delinquent loadsis the key to the success of many cache optimization andprefetching techniques. In this paper, we propose a methodfor identifying delinquent loads that can be implemented atcompile time. Our experiments over eighteen benchmarksfrom the SPEC suite shows that our proposed scheme is stableacross benchmarks, inputs, and cache structures, identifyingan average of 10\% of the total number of loads in thebenchmarks we tested that account for over 90\% of all datacache misses. As far as we know, this is the first time a techniquefor static delinquent load identification with such alevel of precision and coverage has been reported. Whilecomparable techniques can also identify load instructionsthat cover 90\% of all data cache misses, they do so by selectingover 50\% of all load instructions in the code, resultingin a high number of false positives. If basic block profilingis used in conjunction with our heuristic, then our resultsshow that it is possible to pin down just 1.3\% of theload instructions that account for 82\% of all data cachemisses.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {303--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977676},
 acmid = {977676},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Wu:2004:EMA:977395.977677,
 author = {Wu, Qiang and Pyatakov, Artem and Spiridonov, Alexey and Raman, Easwaran and Clark, Douglas W. and August, David I.},
 title = {Exposing Memory Access Regularities Using Object-Relative Memory Profiling},
 abstract = {Memory profiling is the process of characterizing a program's memorybehavior by observing and recording its response to specific inputsets. Relevant aspects of the program's memory behavior maythen be used to guide memory optimizations in an aggressively optimizingcompiler. In general, memory access behavior has eludedmeaningful characterization because of confounding artifacts frommemory allocators, linker data layout, and OS memory management.Since these artifacts may change from run to run, memoryaccess patterns may appear different in each run even for the sameinput set. Worse, regular memory access behavior such as linkedlist traversals appear to have no structure.In this paper we present object-relative translation and decompositiontechniques to eliminate these artifacts and to expose previouslyobscured memory access patterns. To demonstrate the potential ofthese ideas, we implement two different memory profilers targetedat different sets of applications. These profilers outperform the existingones in terms of profile size and useful information per byteof data. The first profiler is a lossless profiler, called WHOMP,which uses object-relativity to achieve a 22\% better compressionthan the previously best known scheme. The second profiler, calledLEAP, uses lossy compression to get highly compact profiles whileproviding useful information to the targeted applications. LEAPcorrectly characterizes the memory alias rates for 56\% more instructionpairs than the previously best known scheme with a practicalrunning time.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {315--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977677},
 acmid = {977677},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{2004:AI:977395.977675,
 title = {Author Index},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '04},
 year = {2004},
 isbn = {0-7695-2102-9},
 location = {Palo Alto, California},
 pages = {325--},
 url = {http://portal.acm.org/citation.cfm?id=977395.977675},
 acmid = {977675},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:MGC:776261.793100,
 title = {Message from the General Co-Chairs},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.09--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793100},
 acmid = {793100},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:MPC:776261.793103,
 title = {Message from the Program Chair},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.10--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793103},
 acmid = {793103},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:CC:776261.793102,
 title = {Committee Chairs},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.11--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793102},
 acmid = {793102},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:SC:776261.793104,
 title = {Steering Committee},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.12--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793104},
 acmid = {793104},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:PC:776261.793098,
 title = {Program Committee},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.13--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793098},
 acmid = {793098},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:ER:776261.793099,
 title = {External Reviewers},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.14--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793099},
 acmid = {793099},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{2003:CS:776261.793101,
 title = {Corporate Supporters},
 abstract = {},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {.15--},
 url = {http://portal.acm.org/citation.cfm?id=776261.793101},
 acmid = {793101},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
key = {{$\!\!$}} ,
} 

@inproceedings{Dehnert:2003:TCM:776261.776263,
 author = {Dehnert, James C. and Grant, Brian K. and Banning, John P. and Johnson, Richard and Kistler, Thomas and Klaiber, Alexander and Mattson, Jim},
 title = {The Transmeta Code Morphing\&trade; Software: using speculation, recovery, and adaptive retranslation to address real-life challenges},
 abstract = {Transmeta's Crusoe microprocessor is a full, system-level implementation of the x86 architecture, comprising a native VLIW microprocessor with a software layer, the <b>Code Morphing Software</b> (CMS), that combines an interpreter, dynamic binary translator, optimizer, and runtime system. In its general structure, CMS resembles other binary translation systems described in the literature, but it is unique in several respects. The wide range of PC workloads that CMS must handle gracefully in real-life operation, plus the need for full system-level x86 compatibility, expose several issues that have received little or no attention in previous literature, such as exceptions and interrupts, I/O, DMA, and self-modifying code. In this paper we discuss some of the challenges raised by these issues, and present the techniques developed in Crusoe and CMS to meet those challenges. The key to these solutions is the Crusoe paradigm of aggressive speculation, recovery to a consistent x86 state using unique hardware commit-and-rollback support, and adaptive retranslation when exceptions occur too often to be handled efficiently by interpretation.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {15--24},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776263},
 acmid = {776263},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {binary translation, dynamic optimization, dynamic translation, emulation, self-modifying code, speculation},
} 

@inproceedings{Kim:2003:DBT:776261.776264,
 author = {Kim, Ho-Seop and Smith, James E.},
 title = {Dynamic binary translation for accumulator-oriented architectures},
 abstract = {A dynamic binary translation system for a co-designed virtual machine is described and evaluated. The underlying hardware directly executes an accumulator-oriented instruction set that exposes instruction dependence chains (strands) to a distributed microarchitecture containing a simple instruction pipeline. To support conventional program binaries, a source instruction set (Alpha in our study) is dynamically translated to the target accumulator instruction set. The binary translator identifies chains of inter-instruction dependences and assigns them to dependence-carrying accumulators. Because the underlying superscalar microarchitecture is capable of dynamic instruction scheduling, the binary translation system does not perform aggressive optimizations or re-schedule code; this significantly reduces binary translation overhead.Detailed timing simulation of the dynamically translated code running on an accumulator-based distributed microarchitecture shows the overall system is capable of achieving similar performance to an ideal out-of-order superscalar processor, ignoring the significant clock frequency advantages that the accumulator-based hardware is likely to have. As part of the study, we evaluate an instruction set modification that simplifies precise trap implementation. This approach significantly reduces the number of instructions required for register state copying, thereby improving performance. We also observe that translation chaining methods can have substantial impact on the performance, and we evaluate a number of chaining methods.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {25--35},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=776261.776264},
 acmid = {776264},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Scott:2003:RRS:776261.776265,
 author = {Scott, K. and Kumar, N. and Velusamy, S. and Childers, B. and Davidson, J. W. and Soffa, M. L.},
 title = {Retargetable and reconfigurable software dynamic translation},
 abstract = {Software dynamic translation (SDT) is a technology that permits the modification of an executing program's instructions. In recent years, SDT has received increased attention, from both industry and academia, as a feasible and effective approach to solving a variety of significant problems. Despite this increased attention, the task of initiating a new project in software dynamic translation remains a difficult one. To address this concern, and in particular, to promote the adoption of SDT technology into an even wider range of applications, we have implemented Strata, a cross-platform infrastructure for building software dynamic translators. This paper describes Strata's architecture, our experience retargeting it to three different processors, and our use of Strata to build two novel SDT systems---one for safe execution of untrusted binaries and one for fast prototyping of architectural simulators.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {36--47},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776265},
 acmid = {776265},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Kamin:2003:JRC:776261.776266,
 author = {Kamin, Sam and Clausen, Lars and Jarvis, Ava},
 title = {Jumbo: run-time code generation for Java and its applications},
 abstract = {Run-time code generation is a well-known technique for improving the efficiency of programs by exploiting dynamic information. Unfortunately, the difficulty of constructing run-time code-generators has hampered their widespread use. We describe Jumbo, a tool for easily creating run-time code generators for Java. Jumbo is a compiler for a two-level version of Java, where programs can contain quoted code fragments. The Jumbo API allows the code fragments to be combined at run-time and then executed. We illustrate Jumbo with several examples that show significant speed-ups over similar code written in plain Java, and argue further that Jumbo is a generalized software component system.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {48--56},
 numpages = {9},
 url = {http://portal.acm.org/citation.cfm?id=776261.776266},
 acmid = {776266},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Java, run-time code generation},
} 

@inproceedings{McFarling:2003:RO:776261.776268,
 author = {McFarling, Scott},
 title = {Reality-based optimization},
 abstract = {Profile-based optimization has been studied extensively. Numerous papers and real systems have shown substantial improvements. However, most of these papers have been limited to either branch prediction or instruction cache performance. Also, most of these papers have looked at small applications with a limited number of testing and training scenarios.In this paper, we look at real use of large real-world desktop applications. We also assume memory consumption and disk performance are the primary metrics of interest. For this domain, we show that it is very difficult to get adequate coverage of large applications even with an extensive collection of training scenarios. We propose instead to augment traditional scenarios with data derived from real use. We show that this methodology allows us to reduce memory pressure by 29\% and disk reads by 33\% compared to traditional approaches.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {59--68},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776268},
 acmid = {776268},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Krintz:2003:COO:776261.776269,
 author = {Krintz, Chandra},
 title = {Coupling on-line and off-line profile information to improve program performance},
 abstract = {In this paper, we describe a novel execution environment for Java programs that substantially improves execution performance by incorporating both on-line and off-line profile information to guide dynamic optimization. By using both types of profile collection techniques, we are able to exploit the strengths of each constituent approach: profile accuracy and low overhead.Such coupling also reduces the negative impact of these approaches when each is used in isolation. On-line profiling introduces overhead for dynamic instrumentation, measurement, and decision making. Off-line profile information can be inaccurate when program inputs for execution and optimization differ from those used for profiling. To combat these drawbacks and to achieve the benefits from both online and off-line profiling, we developed a dynamic compilation system (based on JikesRVM) that makes use of both. As a result, we are able improve Java program performance by 9\% on average, for the programs studied.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {69--78},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776269},
 acmid = {776269},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chen:2003:DTS:776261.776270,
 author = {Chen, Howard and Hsu, Wei-Chung and Lu, Jiwei and Yew, Pen-Chung and Chen, Dong-Yuan},
 title = {Dynamic trace selection using performance monitoring hardware sampling},
 abstract = {Optimizing programs at run-time provides opportunities to apply aggressive optimizations to programs based on information that was not available at compile time. At run time, programs can be adapted to better exploit architectural features, optimize the use of dynamic libraries, and simplify code based on run-time constants.Our profiling system provides a framework for collecting information required for performing run-time optimization. We sample the performance hardware registers available on an ltanium processor, and select a set of code that is likely to lead to important performance-events. We gather distribution information about the performance-events we wish to monitor, and test our traces by estimating the ability for dynamic patching of a program to execute run-time generated traces.Our results show that we are able to capture 58\% of execution time across various SPEC2000 integer benchmarks using our profile and patching techniques on a relatively small number of frequently executed execution paths. Our profiling and detection system overhead increases execution time by only 2--4\%.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {79--90},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776270},
 acmid = {776270},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Cai:2003:OES:776261.776271,
 author = {Cai, Qiong and Xue, Jingling},
 title = {Optimal and efficient speculation-based partial redundancy elimination},
 abstract = {Existing profile-guided partial redundancy elimination (PRE) methods use speculation to enable the removal of partial redundancies along more frequently executed paths at the expense of introducing additional expression evaluations along less frequently executed paths. While being capable of minimizing the number of expression evaluations in some cases, they are, in general, not computationally optimal in achieving this objective. In addition, the experimental results for their effectiveness are mostly missing.This work addresses the following three problems: (1) Is the computational optimality of speculative PRE solvable in polynomial time ? (2) Is edge profiling --- less costly than path profiling --- sufficient to guarantee the computational optimality? (3) Is the optimal algorithm (if one exists) lightweight enough to be used efficiently in a dynamic compiler? In this paper, we provide positive answers to the first two problems and promising results to the third.We present an algorithm that analyzes edge insertion points based on an edge profile. Our algorithm guarantees optimally that the total number of computations for an expression in the transformed code is always minimized with respect to the edge profile given. This implies that edge profiling, which is less costly than path profiling, is sufficient to guarantee this optimality. The key in the development of our algorithm lies in the removal of some non-essential edges (and consequently, all resulting non-essential nodes) from a flow graph so that the problem of finding an optimal code motion is reduced to one of finding a minimal cut in the reduced (flow) graph thus obtained. We have implemented our algorithm in lntel's Open Runtime Platform (ORP). Our preliminary results over a number of Java benchmarks show that our algorithm is lightweight and can be potentially a practical component in a dynamic compiler. As a result, our algorithm can also be profitably employed in a profile-guided static compiler, in which compilation cost can often be sacrificed for code efficiency.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {91--102},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776271},
 acmid = {776271},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Collard:2003:OPC:776261.776273,
 author = {Collard, Jean-Francois and Lavery, Daniel},
 title = {Optimizations to prevent cache penalties for the Intel\&reg; Itanium\&reg; 2 Processor},
 abstract = {This paper describes scheduling optimizations in the Intel\&reg; Itanium\&reg; compiler to prevent cache penalties due to various micro-architectural effects on the Itanium 2 processor. This paper does not try to improve cache hit rates but to avoid penalties, which probably all processors have in one form or another, even in the case of cache hits. These optimizations make use of sophisticated methods for disambiguation of memory references, and this paper examines the performance improvement obtained by integrating these methods into the cache optimizations.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {105--114},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776273},
 acmid = {776273},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Settle:2003:OII:776261.776274,
 author = {Settle, Alex and Connors, Daniel A. and Hoflehner, Gerolf and Lavery, Dan},
 title = {Optimization for the Intel\&reg; Itanium\&reg; architecture register stack},
 abstract = {The Intel\&reg; Itanium\&reg; architecture contains a number of innovative compiler-controllable features designed to exploit instruction level parallelism. New code generation and optimization techniques are critical to the application of these features to improve processor performance. For instance, the Itanium\&reg; architecture provides a compiler-controllable virtual register stack to reduce the penalty of memory accesses associated with procedure calls. The ltanium\&reg; Register Stack Engine (RSE) transparently manages the register stack and saves and restores physical registers to and from memory as needed. Existing code generation techniques for the register stack aggressively allocate virtual registers without regard to the register pressure on different control-flow paths. As such, applications with large data sets may stress the RSE, and cause substantial execution delays due to the high number of register saves and restores. Since the Itanium\&reg; architecture is developed around Explicitly Parallel Instruction Computing (EPIC) concepts, solutions to increasing the register stack efficiency favor code generation techniques rather than hardware approaches.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {115--124},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776274},
 acmid = {776274},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Lin:2003:SRP:776261.776275,
 author = {Lin, Jin and Chen, Tong and Hsu, Wei-Chung and Yew, Pen-Chung},
 title = {Speculative register promotion using Advanced Load Address Table (ALAT)},
 abstract = {The pervasive use of pointers with complicated patterns in C programs often constrains compiler alias analysis to yield conservative register allocation and promotion. Speculative register promotion with hardware support has the potential to more aggressively promote memory references into registers in the presence of aliases. This paper studies the use of the Advanced Load Address Table (ALAT), a data speculation feature defined in the IA-64 architecture, for speculative register promotion. An algorithm for speculative register promotion based on partial redundancy elimination is presented. The algorithm is implemented in Intel's Open Research Compiler (ORC). Experiments on SPEC CPU2000 benchmark programs are conducted to show that speculative register promotion can improve performance of some benchmarks by 1\% to 7\%.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {125--134},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776275},
 acmid = {776275},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Thomas:2003:IMF:776261.776276,
 author = {Thomas, James W.},
 title = {Inlining of mathematical functions in HP-UX for Itanium\&reg; 2},
 abstract = {HP-UX compilers inline mathematical functions for Itanium Processor Family (IPF) systems to improve throughput 4X-8X versus external library calls, achieving speeds comparable to highly tuned vector functions, without requiring the user to code for a vector interface and without sacrificing accuracy or edge-case behaviors. This paper highlights IPF architectural features that support implementation of high-performance, high-quality math functions for inlining. It discusses strategies for utilizing the features and developing inlineable sequences on a large scale, and it presents requisite compiler features and language extensions. Also, this paper describes compiler mechanisms that produce inlineable code and inline it.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {135--144},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776276},
 acmid = {776276},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Spadini:2003:IQS:776261.776278,
 author = {Spadini, Francesco and Fahs, Brian and Patel, Sanjay and Lumetta, Steven S.},
 title = {Improving quasi-dynamic schedules through region slip},
 abstract = {Modern processors perform dynamic scheduling to achieve better utilization of execution resources. A schedule created at run-time is often better than one created at compile-time as it can dynamically adapt to specific events encountered at execution-time. In this paper, we examine some fundamental impediments to effective static scheduling. More specifically, we examine the question of why schedules generated quasi-dynamically by a low-level run-time optimizer and executed on a statically scheduled machine perform worse than using a dynamically-scheduled approach. We observe that such schedules suffer because of region boundaries and a skewed distribution of parallelism towards the beginning of a region. To overcome these limitations, we investigate a new concept, region slip, in which the schedules of different statically-scheduled regions can be interleaved in the processor issue queue to reduce the region boundary effects that cause empty issue slots.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {149--158},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776278},
 acmid = {776278},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Inagaki:2003:IPS:776261.776279,
 author = {Inagaki, Tatsushi and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Integrated prepass scheduling for a Java Just-In-Time compiler on the IA-64 architecture},
 abstract = {We present a new integrated prepass scheduling (IPS) algorithm for a Java Just-In-Time (JIT) compiler, which integrates register minimization into list scheduling. We use backtracking in the list scheduling when we have used up all the available registers. To reduce the overhead of backtracking, we incrementally maintain a set of candidate instructions for undoing scheduling. To maximize the ILP after undoing scheduling, we select an instruction chain with the smallest increase in the total execution time. We implemented our new algorithm in a production-level Java JIT compiler for the Intel Itanium processor. The experiment showed that, compared to the best known algorithm by Govindarajan et al., our IPS algorithm improved the performance by up to +1.8\% while it reduced the compilation time for IPS by 58\% on average.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {159--168},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776279},
 acmid = {776279},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Smelyanskiy:2003:PST:776261.776280,
 author = {Smelyanskiy, Mikhail and Mahlke, Scott A. and Davidson, Edward S. and Lee, Hsien-Hsin S.},
 title = {Predicate-aware scheduling: a technique for reducing resource constraints},
 abstract = {Predicated execution enables the removal of branches wherein segments of branching code are converted into straight-line segments of conditional operations. An important, but generally ignored side effect of this transformation is that the compiler must assign distinct resources to all the predicated operations at a given time to ensure that those resources are available at run-time. However, a resource is only put to productive use when the predicates associated with its operations evaluate to True. We propose predicate-aware scheduling to reduce the superfluous commitment of resources to operations whose predicates evaluate to False at run-time. The central idea is to assign multiple operations to the same resource at the same time, thereby oversubscribing its use. This assignment is intelligently performed to ensure that no two operations simultaneously assigned to the same resource will have both of their predicates evaluate to True. Thus, no resource is dynamically oversubscribed. The overall effect of predicate aware scheduling is to use resources more efficiently, thereby increasing performance when resource constraints are a bottleneck.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {169--178},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776280},
 acmid = {776280},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {VLIW processor, instruction scheduling, predicate analysis, predicated execution, resource utilization, software pipelining},
} 

@inproceedings{Chuang:2003:PLI:776261.776281,
 author = {Chuang, Weihaw and Calder, Brad and Ferrante, Jeanne},
 title = {Phi-Predication for light-weight if-conversion},
 abstract = {Predicated execution can eliminate hard to predict branches and help to enable instruction level parallelism. Many current predication variants exist where the result update is conditional based upon the outcome of the guarding predicate. However, conditional writing of a register creates a naming problem for an out-of-order processor, and can stall the issuing of instructions. This problem arises from potential multiple predicated definitions reaching a use, which is unresolved until the prior predicate values are computed.In this paper we focus on a light-weight form of predication, Phi-Predication, where all predicated instructions write a result value to their register regardless of the predicate value (i.e. even if it is false). Therefore, the predicate does not guard the writing of the result register; it instead acts as a form of selection between two input registers. This eliminates the naming problem for an out-of-order processor. Our Phi-Predicated</i> ISA is derived from the predicated features of the Multiflow ISA, with extensions to efficiently predicate complex control flow. Our compiler modifications also expand upon prior techniques to provide efficient code generation. We examine the use of Phi-Predication for an in-order and out-of-order architecture and compare its performance to using select-op and IA64 ISA predication.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {179--190},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776281},
 acmid = {776281},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Gibert:2003:LST:776261.776283,
 author = {Gibert, Enric and S\'{a}nchez, Jes\'{u}s and Gonz\'{a}lez, Antonio},
 title = {Local scheduling techniques for memory coherence in a clustered VLIW processor with a distributed data cache},
 abstract = {Clustering is a common technique to deal with wire delays. Fully-distributed architectures, where the register file, the functional units and the cache memory are partitioned, are particularly effective to deal with these constraints and besides they are very scalable. However, the distribution of the data cache introduces a new problem: memory instructions may reach the cache in an order different to the sequential program order, thus possibly violating its contents. In this paper two local scheduling mechanisms that guarantee the serialization of aliased memory instructions are proposed and evaluated: the construction of memory dependent chains (MDC solution), and two transformations (store replication and load-store synchronization) applied to the original Data Dependence Graph (DDGT solution). These solutions do not require any extra hardware.The proposed scheduling techniques are evaluated for a word-interleaved cache clustered VLIW processor (although these techniques can also be used for any other distributed cache configuration). Results for the Mediabench benchmark suite demonstrate the effectiveness of such techniques. In particular, the DDGT solution increases the proportion of local accesses by 16\% compared to MDC, and stall time is reduced by 32\% since load instructions can be freely scheduled in any cluster. However, the MDC solution reduces compute time and it often outperforms the former. Finally the impact of both techniques on an architecture with Attraction Buffers is studied and evaluated.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {193--203},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=776261.776283},
 acmid = {776283},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Triantafyllis:2003:COE:776261.776284,
 author = {Triantafyllis, Spyridon and Vachharajani, Manish and Vachharajani, Neil and August, David I.},
 title = {Compiler optimization-space exploration},
 abstract = {To meet the demands of modern architectures, optimizing compilers must incorporate an ever larger number of increasingly complex transformation algorithms. Since code transformations may often degrade performance or interfere with subsequent transformations, compilers employ predictive heuristics to guide optimizations by predicting their effects a priori. Unfortunately, the unpredictability of optimization interaction and the irregularity of today's wide-issue machines severely limit the accuracy of these heuristics. As a result, compiler writers may temper high variance optimization with overly conservative heuristics or may exclude these optimizations entirely. While this process results in a compiler capable of generating good average code quality across the target benchmark set, it is at the cost of missed optimization opportunities in individual code segments.To replace predictive heuristics, researchers have proposed compilers which explore many optimization options, selecting the best one a posteriori</i>. Unfortunately, these existing iterative compilation</i> techniques are not practical for reasons of compile time and applicability. In this paper, we present the Optimization-Space Exploration (OSE) compiler organization, the first practical iterative compilation strategy applicable to optimizations in general-purpose compilers. Instead of replacing predictive heuristics, OSE uses the compiler writer's knowledge encoded in the heuristics to select a small number of promising optimization alternatives for a given code segment. Compile time is limited by evaluating only these alternatives for hot code segments using a general compiletime performance estimator. An OSE-enhanced version of lntel's highly-tuned, aggressively optimizing production compiler for IA-64 yields a significant performance improvement, more than 20\% in some cases, on Itanium for SPEC codes.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {204--215},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776284},
 acmid = {776284},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Budiu:2003:OMA:776261.776285,
 author = {Budiu, Mihai and Goldstein, Seth C.},
 title = {Optimizing memory accesses for spatial computation},
 abstract = {In this paper we present the internal representation and optimizations used by the CASH compiler for improving the memory parallelism of pointer-based programs. CASH uses an SSA-based representation for memory, which compactly summarizes both control-flow-and dependence information.In CASH, memory optimization is a four-step process: (1)first an initial, relatively coarse, representation of memory dependences is built; (2) next, unnecessary memory dependences are removed using dependence tests; (3) third, redundant memory operations are removed (4)finally, parallelism is increased by pipelining memory accesses in loops. While the first three steps above are very general, the loop pipelining transformations are particularly applicable for spatial computation, which is the primary target of CASH.The redundant memory removal optimizations presented are: load/store hoisting (subsuming partial redundancy elimination and common-subexpression elimination), load-after-store removal, store-before-store removal (dead store removal) and loop-invariant load motion.One of our loop pipelining transformations is a new form of loop parallelization, called loop decoupling</i>. This transformation separates independent memory accesses within a loop body into several independent loops, which are allowed dynamically to slip with respect to each other. A new computational primitive, a token generator</i> is used to dynamically control the amount of slip, allowing maximum freedom, while guaranteeing that no memory dependences are violated.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {216--227},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776285},
 acmid = {776285},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Haber:2003:OOC:776261.776286,
 author = {Haber, Gadi and Klausner, Moshe and Eisenberg, Vadim and Mendelson, Bilha and Gurevich, Maxim},
 title = {Optimization opportunities created by global data reordering},
 abstract = {Memory access has proven to be one of the bottlenecks in modern architectures. Improving memory locality and eliminating the amount of memory access can help release this bottleneck. We present a method for link-time profile-based optimization by reordering the global data of the program and modifying its code accordingly. The proposed optimization reorders the entire global data of the program, according to a representative execution rate of each instruction (or basic block) in the code. The data reordering is done in a way that enables the replacement of frequently-executed Load instructions, which reference the global data, with fast Add Immediate instructions. In addition, it tries to improve the global data locality and to reduce the total size of the global data area. The optimization was implemented into FDPR (Feedback Directed Program Restructuring), a post-link optimizer, which is part of the IBM AIX operating system for the IBM pSeries servers. Our results on SPECint2000 show a significant improvement of up to 11\% (average 3\%) in execution time, along with up to 97.9\% (average 83\%) reduction in memory references to the global variables via the global data access mechanism of the program.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {228--237},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776286},
 acmid = {776286},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Fink:2003:DIE:776261.776288,
 author = {Fink, Stephen J. and Qian, Feng},
 title = {Design, implementation and evaluation of adaptive recompilation with on-stack replacement},
 abstract = {Modern virtual machines often maintain multiple compiled versions of a method. An on-stack replacement (OSR) mechanism enables a virtual machine to transfer execution between compiled versions, even while a method runs. Relying on this mechanism, the system can exploit powerful techniques to reduce compile time and code space, dynamically de-optimize code, and invalidate speculative optimizations.This paper presents a new, simple, mostly compiler-independent mechanism to transfer execution into compiled code. Additionally, we present enhancements to an analytic model for recompilation to exploit OSR for more aggressive optimization. We have implemented these techniques in Jikes RVM and present a comprehensive evaluation, including a study of fully automatic, online, profile-driven deferred compilation.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {241--252},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776288},
 acmid = {776288},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Hazelwood:2003:AOC:776261.776289,
 author = {Hazelwood, Kim and Grove, David},
 title = {Adaptive online context-sensitive inlining},
 abstract = {As current trends in software development move toward more complex object-oriented programming, inlining has become a vital optimization that provides substantial performance improvements to C++ and Java programs. Yet, the aggressiveness of the inlining algorithm must be carefully monitored to effectively balance performance and code size. The state-of-the-art is to use profile information (associated with call edges) to guide inlining decisions. In the presence of virtual method calls, profile information for one call edge may not be sufficient for making effectual inlining decisions. Therefore, we explore the use of profiling data with additional levels of context sensitivity. In addition to exploring fixed levels of context sensitivity, we explore several adaptive schemes that attempt to find the ideal degree of context sensitivity for each call site. Our techniques are evaluated on the basis of runtime performance, code size and dynamic compilation time. On average, we found that with minimal impact on performance (+/-1\%) context sensitivity can enable 10\% reductions in compiled code space and compile time. Performance on individual programs varied from \&minus;4.2\% to 5.3\% while reductions in compile time and code space of up to 33.0\% and 56.7\% respectively were obtained.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {253--264},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776289},
 acmid = {776289},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Bruening:2003:IAD:776261.776290,
 author = {Bruening, Derek and Garnett, Timothy and Amarasinghe, Saman},
 title = {An infrastructure for adaptive dynamic optimization},
 abstract = {Dynamic optimization is emerging as a promising approach to overcome many of the obstacles of traditional static compilation. But while there are a number of compiler infrastructures for developing static optimizations, there are very few for developing dynamic optimizations. We present a framework for implementing dynamic analyses and optimizations. We provide an interface for building external modules, or clients, for the DynamoRlO dynamic code modification system. This interface abstracts away many low-level details of the DynamoRlO runtime system while exposing a simple and powerful, yet efficient and lightweight, API. This is achieved by restricting optimization units to linear streams of code and using adaptive levels of detail for representing instructions. The interface is not restricted to optimization and can be used for instrumentation, profiling, dynamic translation, etc.To demonstrate the usefulness and effectiveness of our framework, we implemented several optimizations. These improve the performance of some applications by as much as 40\% relative to native execution. The average speedup relative to base DynamoRlO performance is 12\%.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {265--275},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=776261.776290},
 acmid = {776290},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Berndl:2003:DPT:776261.776291,
 author = {Berndl, Marc and Hendren, Laurie},
 title = {Dynamic profiling and trace cache generation},
 abstract = {Dynamic program optimization is increasingly important for achieving good runtime performance. A key issue is how to select which code to optimize. One approach is to dynamically detect traces, long sequences of instructions spanning multiple methods, which are likely to execute to completion. Traces are easy to optimize and have been shown to be a good unit for optimization.This paper reports on a new approach for dynamically detecting, creating and storing traces in a Java virtual machine. We first describe four important criteria for a successful trace strategy: good instruction stream coverage, low dispatch rate, cache stability, and optimizability of traces. We then present our approach based on branch correlation graphs. A branch correlation graph stores information about the correlation between pairs of branches, as well as additional state information.We present the complete design for an efficient implementation of the system, including a detailed discussion of the trace cache and profiling mechanisms. We have implemented an experimental framework to measure the traces generated by our approach in a direct-threaded Java VM (SableVM) and we present experimental results to show that the traces we generate meet the design criteria.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {276--285},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776291},
 acmid = {776291},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Marathe:2003:MTD:776261.776293,
 author = {Marathe, Jaydeep and Mueller, Frank and Mohan, Tushar and de Supinski, Bronis R. and McKee, Sally A. and Yoo, Andy},
 title = {METRIC: tracking down inefficiencies in the memory hierarchy via binary rewriting},
 abstract = {In this paper, we present METRIC, an environment for determining memory inefficiencies by examining data traces. METRIC is designed to alter the performance behavior of applications that are mostly constrained by their latency to resolve memory references. We make four primary contributions in this paper. First, we present methods to extract partial data traces from running applications by observing their memory behavior via dynamic binary rewriting. Second, we present a methodology to represent partial data traces in constant space for regular references through a novel technique for online compression of reference streams. Third, we employ offline cache simulation to derive indications about memory performance bottlenecks from partial data traces. By exploiting summarized memory metrics, by-reference metrics as well as cache evictor information, we can pin-point the sources of performance problems. Fourth, we demonstrate the ability to derive opportunities for optimizations and assess their benefits in several experiments resulting in up to 40\% lower miss ratios.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {289--300},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776293},
 acmid = {776293},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Chen:2003:TTE:776261.776294,
 author = {Chen, Michael and Olukotun, Kunle},
 title = {TEST: a tracer for extracting speculative threads},
 abstract = {Thread-level speculation (TLS) allows sequential programs to be arbitrarily decomposed into threads that can be safely executed in parallel. A key challenge for TLS processors is choosing thread decompositions that speedup the program. Current techniques for identifying decompositions have practical limitations in real systems. Traditional parallelizing compilers do not work effectively on most integer programs, and software profiling slows down program execution too much for real-time analysis.Tracer for Extracting Speculative Threads (TEST) is hardware support that analyzes sequential program execution to estimate performance of possible thread decompositions. This hardware is used in a dynamic parallelization system that automatically transforms unmodified, sequential Java programs to run on TLS processors. In this system, the best thread decompositions found by TEST are dynamically recompiled to run speculatively. This paper describes the analysis performed by TEST and presents simulation results demonstrating its effectiveness on real programs. Estimates are also provided that show the tracer requires minimal hardware additions to our speculative chipmultiprocessor (\&lt; 1\% of the total transistor count) and causes only minor slowdowns to programs during analysis (3--25\%).},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {301--312},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776294},
 acmid = {776294},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Drinic:2003:COC:776261.776296,
 author = {Drini\'{c}, Milenko and Kirovski, Darko and Vo, Hoi},
 title = {Code optimization for code compression},
 abstract = {With the emergence of software delivery platforms such as Microsoft's .NET, reduced size of transmitted binaries has become a very important system parameter strongly affecting system performance. In this paper, we present two novel pre-processing steps for code compression that explore program binaries' syntax and semantics to achieve superior compression ratios. The first preprocessing step involves heuristic partitioning of a program binary into streams with high auto-correlation. The second preprocessing step uses code optimization via instruction rescheduling in order to improve prediction probabilities for a given com pression engine. We have developed three heuristics for instruction rescheduling that explore tradeoffs of the solution quality versus algorithm run-time. The pre-processing steps are integrated with the generic paradigm of prediction partial matching (PPM) which is the fundament of our compression codec. The compression algorithm is implemented for x86 binaries and tested on several large Microsoft applications. Binaries compressed using our compression codec are 18--24\% smaller than those compressed using the best available off-the-shelf compressor.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {315--324},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776296},
 acmid = {776296},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Zhang:2003:HPS:776261.776297,
 author = {Zhang, Xiangyu and Gupta, Rajiv},
 title = {Hiding program slices for software security},
 abstract = {Given the high cost of producing software, development of technology for prevention of software piracy is important for the software industry. In this paper we present a novel approach for preventing the creation of unauthorized copies of software. Our approach splits software modules into open</i> and hidden</i> components. The open components are installed (executed) on an unsecure machine while the hidden components are installed (executed) on a secure machine. We assume that while open components can be stolen, to obtain a fully functioning copy of the software, the hidden components must be recovered. We describe an algorithm that constructs hidden components by slicing the original software components. We argue that recovery of hidden components constructed through slicing, in order to obtain a fully functioning copy of the software, is a complex task. We further develop security analysis to capture the complexity of recovering hidden components. Finally we apply our technique to several large Java programs to study the complexity of recovering constructed hidden components and to measure the runtime overhead introduced by splitting of software into open and hidden components.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {325--336},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=776261.776297},
 acmid = {776297},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{Eckstein:2003:AMS:776261.776298,
 author = {Eckstein, Erik and Scholz, Bernhard},
 title = {Addressing mode selection},
 abstract = {Many processor architectures provide a set of addressing modes in their address generation units. For example DSPs (digital signal processors) have powerful addressing modes for efficiently implementing numerical algorithms. Typical addressing modes of DSPs are auto post-modification and indexing for address registers. The selection of the optimal addressing modes in the means of minimal code size and minimal execution time depends on many parameters and is NP complete in general.In this work we present a new approach for solving the addressing mode selection (AMS) problem. We provide a method for modeling the target architecture's addressing modes as cost functions for a partitioned boolean quadratic optimization problem (PBQP). For solving the PBQP we present an efficient and effect way to implement large matrices for modeling the cost model.We have integrated the addressing mode selection with the Atair C-Compiler for the uPD7705x DSP from NEC. In our experiments we show that the addressing mode selection can be optimally solved for almost all benchmark programs and the compile-time overhead of the address mode selection is within acceptable bounds for a production DSP of compiler.},
 booktitle = {Proceedings of the international symposium on Code generation and optimization: feedback-directed and runtime optimization},
 series = {CGO '03},
 year = {2003},
 isbn = {0-7695-1913-X},
 location = {San Francisco, California},
 pages = {337--346},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=776261.776298},
 acmid = {776298},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

