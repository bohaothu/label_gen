@inproceedings{Lelarge:2009:ECE:1555349.1555351,
 author = {Lelarge, Marc},
 title = {Efficient control of epidemics over random networks},
 abstract = {Motivated by the modeling of the spread of viruses or epidemics with coordination among agents, we introduce a new model generalizing both the basic contact model and the bootstrap percolation. We analyze this percolated threshold model when the underlying network is a random graph with fixed degree distribution. Our main results unify many results in the random graphs literature. In particular, we provide a necessary and sufficient condition under which a single node can trigger a large cascade. Then we quantify the possible impact of an attacker against a degree based vaccination and an acquaintance vaccination. We define a security metric allowing to compare the different vaccinations. The acquaintance vaccination requires no knowledge of the node degrees or any other global information and is shown to be much more efficient than the uniform vaccination in all cases.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555351},
 doi = {http://doi.acm.org/10.1145/1555349.1555351},
 acmid = {1555351},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {epidemics, random graphs, vaccination},
} 

@inproceedings{Pathak:2009:BSC:1555349.1555352,
 author = {Pathak, Abhinav and Qian, Feng and Hu, Y. Charlie and Mao, Z. Morley and Ranjan, Supranamaya},
 title = {Botnet spam campaigns can be long lasting: evidence, implications, and analysis},
 abstract = {Accurately identifying spam campaigns launched by a large number of bots in a botnet allows for accurate spam campaign signature generation and hence is critical to defeating spamming botnets. The straight-forward approach of clustering all spam containing the same label such as an URL into a campaign can be easily defeated by techniques such as simple obfuscations of URLs. In this paper, we perform a comprehensive study of content-agnostic characteristics of spam campaigns, e.g.</i> duration and source-network distribution of spammers, in order to ascertain whether and how they can assist the simple label-based clustering methods in identifying campaigns and generating campaign signatures. In particular, from a five-month trace collected by a relay sinkhole, we manually identified and then analyzed seven URL-based botnet spam campaigns consisting of 52 million spam messages sent over 2.09 million SMTP connections originated from over 150,000 non-proxy spamming hosts and destined to about 200,000 end domains. Our analysis shows that the spam campaigns, when observed from large destination domains, exhibit durations far longer than the five-day period as reported in a recent study. We analyze the implications of this finding on spam campaign signature generation. We further study other characteristics of these long-lasting campaigns. Our analysis reveals several new findings regarding workload distribution, sending patterns, and coordination among the spamming machines.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555352},
 doi = {http://doi.acm.org/10.1145/1555349.1555352},
 acmid = {1555352},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {botnet, burstiness, distributedness, open relay, spam campaign},
} 

@inproceedings{Torres:2009:IUB:1555349.1555353,
 author = {Torres, Ruben D. and Hajjat, Mohammad Y. and Rao, Sanjay G. and Mellia, Marco and Munafo, Maurizio M.},
 title = {Inferring undesirable behavior from P2P traffic analysis},
 abstract = {While peer-to-peer (P2P) systems have emerged in popularity in recent years, their large-scale and complexity make them difficult to reason about. In this paper, we argue that systematic analysis of traffic characteristics of P2P systems can reveal a wealth of information about their behavior, and highlight potential undesirable activities that such systems may exhibit. As a first step to this end, we present an offline and semi-automated approach to detect undesirable behavior. Our analysis is applied on real traffic traces collected from a Point-of-Presence (PoP) of a national-wide ISP in which over 70\% of the total traffic is due to eMule [19], a popular P2P file-sharing system. Flow-level measurements are aggregated into "samples" referring to the activity of each host during a time interval. We then employ a clustering technique to automatically and coarsely identify similar behavior across samples, and extensively use domain knowledge to interpret and analyze the resulting clusters. Our analysis shows several examples of undesirable behavior including evidence of DDoS attacks exploiting live P2P clients, significant amounts of unwanted traffic that may harm network performance, and instances where the performance of participating peers may be subverted due to maliciously deployed servers. Identification of such patterns can benefit network operators, P2P system developers, and actual end-users.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555353},
 doi = {http://doi.acm.org/10.1145/1555349.1555353},
 acmid = {1555353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {P2P, measurement},
} 

@inproceedings{Anand:2009:RNT:1555349.1555355,
 author = {Anand, Ashok and Muthukrishnan, Chitra and Akella, Aditya and Ramjee, Ramachandran},
 title = {Redundancy in network traffic: findings and implications},
 abstract = {A large amount of popular content is transferred repeatedly across network links in the Internet. In recent years, protocol-independent redundancy elimination</i>, which can remove duplicate strings from within arbitrary network flows, has emerged as a powerful technique to improve the efficiency of network links in the face of repeated data. Many vendors offer such redundancy elimination middleboxes to improve the effective bandwidth of enterprise, data center and ISP links alike. In this paper, we conduct a large scale trace-driven study of protocol independent redundancy elimination mechanisms, driven by several terabytes of packet payload traces collected at 12 distinct network locations, including the access link of a large US-based university and of 11 enterprise networks of different sizes. Based on extensive analysis, we present a number of findings on the benefits and fundamental design issues in redundancy elimination systems. Two of our key findings are (1) A new redundancy elimination algorithm based on Winnowing that outperforms the widely-used Rabin fingerprint-based algorithm by 5-10\% on most traces and by as much as 35\% in some traces. (2) A surprising finding that 75-90\% of middlebox's bandwidth savings in our enterprise traces is due to redundant byte-strings from within each client's traffic, implying that pushing redundancy elimination capability to the end hosts, i.e. an end-to-end redundancy elimination solution</i>, could obtain most of the middlebox's bandwidth savings.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555355},
 doi = {http://doi.acm.org/10.1145/1555349.1555355},
 acmid = {1555355},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {traffic engineering, traffic redundancy},
} 

@inproceedings{Jin:2009:UCN:1555349.1555356,
 author = {Jin, Yu and Sharafuddin, Esam and Zhang, Zhi-Li},
 title = {Unveiling core network-wide communication patterns through application traffic activity graph decomposition},
 abstract = {As Internet communications and applications become more complex,operating, managing and securing networks have become increasingly challenging tasks. There are urgent demands for more sophisticated techniques for understanding and analyzing the behavioral characteristics of network traffic. In this paper, we study the network traffic behaviors using traffic activity graphs (TAGs), which capture the interactions among hosts engaging in certain types of communications and their collective behavior. TAGs derived from real network traffic are large, sparse, yet seemingly complex and richly connected, therefore difficult to visualize and comprehend. In order to analyze and characterize these TAGs, we propose a novel statistical traffic graph decomposition technique based on orthogonal nonnegative matrix tri-factorization (tNMF) to decompose and extract the core host interaction patterns and other structural properties. Using the real network traffic traces, we demonstrate that our tNMF-based graph decomposition technique produces meaningful and interpretable results. It enables us to characterize and quantify the key structural properties of large and sparse TAGs associated with various applications, and study their formation and evolution.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555356},
 doi = {http://doi.acm.org/10.1145/1555349.1555356},
 acmid = {1555356},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, application classification, graph decomposition, nonnegative matrix factorization, traffic graph},
} 

@inproceedings{Ramasubramanian:2009:TIL:1555349.1555357,
 author = {Ramasubramanian, Venugopalan and Malkhi, Dahlia and Kuhn, Fabian and Balakrishnan, Mahesh and Gupta, Archit and Akella, Aditya},
 title = {On the treeness of internet latency and bandwidth},
 abstract = {Existing empirical studies of Internet structure and path properties indicate that the Internet is tree-like. This work quantifies the degree to which at least two important Internet measures--latency and bandwidth--approximate tree metrics. We evaluate our ability to model end-to-end measures using tree embeddings by actually building tree representations. In addition to being simple and intuitive models, these trees provide a range of commonly-required functionality beyond serving as an analytical tool. The contributions of our study are twofold. First, we investigate the ability to portray the inherent hierarchical structure of the Internet using the most pure and compact topology, trees. Second, we evaluate the ability of our compact representation to facilitate many natural tasks, such as the selection of servers with short latency or high bandwidth from a client. Experiments show that these tasks can be done with high degree of success and modest overhead.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555357},
 doi = {http://doi.acm.org/10.1145/1555349.1555357},
 acmid = {1555357},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bandwidth, internet topology, latency, sequoia, tree embedding},
} 

@inproceedings{Meiners:2009:TTA:1555349.1555359,
 author = {Meiners, Chad R. and Liu, Alex X. and Torng, Eric},
 title = {Topological transformation approaches to optimizing TCAM-based packet classification systems},
 abstract = {Several range reencoding schemes have been proposed to mitigate the effect of range expansion and the limitations of small capacity, large power consumption, and high heat generation of TCAM-based packet classification systems. However, they all disregard the semantics of classifiers and therefore miss significant opportunities for space compression. In this paper, we propose new approaches to range reencoding by taking into account classifier semantics. Fundamentally different from prior work, we view reencoding as a topological transformation process from one colored hyperrectangle to another where the color is the decision associated with a given packet. We present two orthogonal, yet composable, reencoding approaches, domain compression and prefix alignment. Our techniques significantly outperform all previous reencoding techniques. In comparison with the state-of-the-art results, our experimental results show that our techniques achieve at least 7 times more space reduction in terms of TCAM space for an encoded classifier and at least 3 times more space reduction in terms of TCAM space for a reencoded classifier and its transformers.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555359},
 doi = {http://doi.acm.org/10.1145/1555349.1555359},
 acmid = {1555359},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, range encoding, tcam},
} 

@inproceedings{Shen:2009:RPA:1555349.1555360,
 author = {Shen, Kai and Stewart, Christopher and Li, Chuanpeng and Li, Xin},
 title = {Reference-driven performance anomaly identification},
 abstract = {Complex system software allows a variety of execution conditions on system configurations and workload properties. This paper explores a principled use of reference executions--those of similar execution conditions from the target--to help identify the symptoms and causes of performance anomalies. First, to identify anomaly symptoms, we construct change profiles that probabilistically characterize expected performance deviations between target and reference executions. By synthesizing several single-parameter change profiles, we can scalably identify anomalous reference-to-target changes in a complex system with multiple execution parameters. Second, to narrow the scope of anomaly root cause analysis, we filter anomaly-related low-level system metrics as those that manifest very differently between target and reference executions. Our anomaly identification approach requires little expert knowledge or detailed models on system internals and consequently it can be easily deployed. Using empirical case studies on the Linux I/O subsystem and a J2EE-based distributed online service, we demonstrate our approach's effectiveness in identifying performance anomalies over a wide range of execution conditions as well as multiple system software versions. In particular, we discovered five previously unknown performance anomaly causes in the Linux 2.6.23 kernel. Additionally, our preliminary results suggest that online anomaly detection and system reconfiguration may help evade performance anomalies in complex online systems.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555360},
 doi = {http://doi.acm.org/10.1145/1555349.1555360},
 acmid = {1555360},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating system, performance anomaly},
} 

@inproceedings{Gupta:2009:NWS:1555349.1555361,
 author = {Gupta, Gagan R. and Sanghavi, Sujay and Shroff, Ness B.},
 title = {Node weighted scheduling},
 abstract = {This paper proposes a new class of online policies for scheduling in input-buffered crossbar switches. Given an initial configuration of packets at the input buffers, these policies drain all packets in the system in the minimal amount of time provided that there are no further arrivals. These policies are also throughput optimal for a large class of arrival processes which satisfy strong-law of large numbers. We show that it is possible for policies in our class to be throughput optimal even if they are not constrained to be maximal in every time slot. Most algorithms for switch scheduling take an edge based approach; in contrast, we focus on scheduling (a large enough set of) the most congested ports. This alternate approach allows for lower-complexity algorithms, and also requires a non-standard technique to prove throughput-optimality. One algorithm in our class, Maximum Vertex-weighted Matching (MVM) has worst-case complexity similar to Max-size Matching, and in simulations shows slightly better delay performance than Max-(edge)weighted-Matching (MWM).},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555361},
 doi = {http://doi.acm.org/10.1145/1555349.1555361},
 acmid = {1555361},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bipartite matching, fluid limits, scheduling, switch},
} 

@inproceedings{Chaintreau:2009:AGS:1555349.1555363,
 author = {Chaintreau, Augustin and Le Boudec, Jean-Yves and Ristanovic, Nikodin},
 title = {The age of gossip: spatial mean field regime},
 abstract = {Disseminating a piece of information, or updates for a piece of information, has been shown to benefit greatly from simple randomized procedures, sometimes referred to as gossiping, or epidemic algorithms. Similarly, in a network where mobile nodes occasionally receive updated content from a base station, gossiping using opportunistic contacts allows for recent updates to be efficiently maintained, for a large number of nodes. In this case, however, gossiping depends on node mobility. For this reason, we introduce a new gossip model, with mobile nodes moving between different classes</i> that can represent locations or states, which determine gossiping behavior of the nodes. Here we prove that, when the number of mobile nodes becomes large, the age of the latest updates received by mobile nodes approaches a deterministic mean-field regime. More precisely, we show that the occupancy measure of the process constructed, with the ages defined above, converges to a deterministic limit that can be entirely characterized by differential equations. This major simplification allows us to characterize how mobility, source inputs and gossiping influence the age distribution for low and high ages. It also leads to a scalable numerical evaluation of the performance of mobile update systems, which we validate (using a trace of 500 taxicabs) and use to propose infrastructure deployment.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555363},
 doi = {http://doi.acm.org/10.1145/1555349.1555363},
 acmid = {1555363},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic content, epidemic, gossip, infrastructure deployment, mean field, updates},
} 

@inproceedings{Bodas:2009:SMW:1555349.1555364,
 author = {Bodas, Shreeshankar and Shakkottai, Sanjay and Ying, Lei and Srikant, R.},
 title = {Scheduling in multi-channel wireless networks: rate function optimality in the small-buffer regime},
 abstract = {We consider the problem of designing scheduling algorithms for the downlink of cellular wireless networks where bandwidth is partitioned into tens to hundreds of parallel channels, each of which can be allocated to a possibly different user in each time slot. We prove that a class of algorithms called Iterated Longest Queues First (iLQF) algorithms achieves the smallest buffer overflow probability in an appropriate large deviations sense. The class of iLQF algorithms is quite different from the class of max-weight policies which have been studied extensively in the literature, and it achieves much better performance in the regimes studied in this paper.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555364},
 doi = {http://doi.acm.org/10.1145/1555349.1555364},
 acmid = {1555364},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay optimality, large deviations, scheduling algorithm},
} 

@inproceedings{Rajagopalan:2009:NAT:1555349.1555365,
 author = {Rajagopalan, Shreevatsa and Shah, Devavrat and Shin, Jinwoo},
 title = {Network adiabatic theorem: an efficient randomized protocol for contention resolution},
 abstract = {The popularity of Aloha</i>-like algorithms for resolution of contention between multiple entities accessing common resources is due to their extreme simplicity and distributed nature. Example applications of such algorithms include Ethernet and recently emerging wireless multi-access networks. Despite a long and exciting history of more than four decades, the question of designing an algorithm that is essentially</i> as simple and distributed as Aloha while being efficient has remained unresolved. In this paper, we resolve this question successfully for a network of queues where contention is modeled through independent-set constraints over the network graph. The work by Tassiulas and Ephremides (1992) suggests that an algorithm that schedules queues so that the summation of `weight' of scheduled queues is maximized, subject to constraints, is efficient. However, implementing such an algorithm using Aloha-like mechanism has remained a mystery. We design such an algorithm building upon a Metropolis-Hastings sampling mechanism along with selection of `weight' as an appropriate function of the queue-size. The key ingredient in establishing the efficiency of the algorithm is a novel adiabatic</i>-like theorem for the underlying queueing network, which may be of general interest in the context of dynamical systems.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555365},
 doi = {http://doi.acm.org/10.1145/1555349.1555365},
 acmid = {1555365},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aloha, markov chain, mixing time, wireless multi-access},
} 

@inproceedings{Sharma:2009:DDC:1555349.1555367,
 author = {Sharma, Abhishek B. and Golubchik, Leana and Govindan, Ramesh and Neely, Michael J.},
 title = {Dynamic data compression in multi-hop wireless networks},
 abstract = {Data compression can save energy and increase network capacity in wireless sensor networks. However, the decision of whether and when to compress data can depend upon platform hardware, topology, wireless channel conditions, and application data rates. Using Lyapunov optimization theory, we design an algorithm called SEEC that makes joint compression and transmission decisions with the goal of minimizing energy consumption. A practical distributed variant, DSEEC, is able to achieve more than 30\% energy savings and adapts seamlessly across a wide range of conditions, without explicitly taking topology, application data rates, and link quality changes into account.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555367},
 doi = {http://doi.acm.org/10.1145/1555349.1555367},
 acmid = {1555367},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy efficiency, stochatic network optimization},
} 

@inproceedings{Gandhi:2009:OPA:1555349.1555368,
 author = {Gandhi, Anshul and Harchol-Balter, Mor and Das, Rajarshi and Lefurgy, Charles},
 title = {Optimal power allocation in server farms},
 abstract = {Server farms today consume more than 1.5\% of the total electricity in the U.S. at a cost of nearly \$4.5 billion. Given the rising cost of energy, many industries are now seeking solutions for how to best make use of their available power. An important question which arises in this context is how to distribute available power among servers in a server farm so as to get maximum performance. By giving more power to a server, one can get higher server frequency (speed). Hence it is commonly believed that, for a given power budget, performance can be maximized by operating servers at their highest power levels. However, it is also conceivable that one might prefer to run servers at their lowest power levels, which allows more servers to be turned on for a given power budget. To fully understand the effect of power allocation on performance in a server farm with a fixed power budget, we introduce a queueing theoretic model, which allows us to predict the optimal power allocation in a variety of scenarios. Results are verified via extensive experiments on an IBM BladeCenter. We find that the optimal power allocation varies for different scenarios. In particular, it is not always optimal to run servers at their maximum power levels. There are scenarios where it might be optimal to run servers at their lowest power levels or at some intermediate power levels. Our analysis shows that the optimal power allocation is non-obvious and depends on many factors such as the power-to-frequency relationship in the processors, the arrival rate of jobs, the maximum server frequency, the lowest attainable server frequency and the server farm configuration. Furthermore, our theoretical model allows us to explore more general settings than we can implement, including arbitrarily large server farms and different power-to-frequency curves. Importantly, we show that the optimal power allocation can significantly improve server farm performance, by a factor of typically 1.4 and as much as a factor of 5 in some cases.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555368},
 doi = {http://doi.acm.org/10.1145/1555349.1555368},
 acmid = {1555368},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data center, power management, power-to-frequency, response time, server farm},
} 

@inproceedings{Coskun:2009:EIJ:1555349.1555369,
 author = {Coskun, Ayse K. and Strong, Richard and Tullsen, Dean M. and Simunic Rosing, Tajana},
 title = {Evaluating the impact of job scheduling and power management on processor lifetime for chip multiprocessors},
 abstract = {Temperature-induced reliability issues are among the major challenges for multicore architectures. Thermal hot spots and thermal cycles combine to degrade reliability. This research presents new reliability-aware job scheduling and power management approaches for chip multiprocessors. Accurate evaluation of these policies requires a novel simulation framework that can capture architecture-level effects over tens of seconds or longer, while also capturing thermal interactions among cores resulting from dynamic scheduling policies. Using this framework and a set of new thermal management policies, this work shows that techniques that offer similar performance, energy, and even peak temperature can differ significantly in their effects on the expected processor lifetime.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555369},
 doi = {http://doi.acm.org/10.1145/1555349.1555369},
 acmid = {1555369},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, reliability, simulation methodology, thermal management},
} 

@inproceedings{Chen:2009:UIC:1555349.1555371,
 author = {Chen, Feng and Koufaty, David A. and Zhang, Xiaodong},
 title = {Understanding intrinsic characteristics and system implications of flash memory based solid state drives},
 abstract = {Flash Memory based Solid State Drive (SSD) has been called a "pivotal technology" that could revolutionize data storage systems. Since SSD shares a common interface with the traditional hard disk drive (HDD), both physically and logically, an effective integration of SSD into the storage hierarchy is very important. However, details of SSD hardware implementations tend to be hidden behind such narrow interfaces. In fact, since sophisticated algorithms are usually, of necessity, adopted in SSD controller firmware, more complex performance dynamics are to be expected in SSD than in HDD systems. Most existing literature or product specifications on SSD just provide high-level descriptions and standard performance data, such as bandwidth and latency. In order to gain insight into the unique performance characteristics of SSD, we have conducted intensive experiments and measurements on different types of state-of-the-art SSDs, from low-end to high-end products. We have observed several unexpected performance issues and uncertain behavior of SSDs, which have not been reported in the literature. For example, we found that fragmentation could seriously impact performance -- by a factor of over 14 times on a recently announced SSD. Moreover, contrary to the common belief that accesses to SSD are uncorrelated with access patterns, we found a strong correlation between performance and the randomness of data accesses, for both reads and writes. In the worst case, average latency could increase by a factor of 89 and bandwidth could drop to only 0.025MB/sec. Our study reveals several unanticipated aspects in the performance dynamics of SSD technology that must be addressed by system designers and data-intensive application users in order to effectively place it in the storage hierarchy.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555371},
 doi = {http://doi.acm.org/10.1145/1555349.1555371},
 acmid = {1555371},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flash memory, hard disk drive, solid state drive},
} 

@inproceedings{Schroeder:2009:DEW:1555349.1555372,
 author = {Schroeder, Bianca and Pinheiro, Eduardo and Weber, Wolf-Dietrich},
 title = {DRAM errors in the wild: a large-scale field study},
 abstract = {Errors in dynamic random access memory (DRAM) are a common form of hardware failure in modern compute clusters. Failures are costly both in terms of hardware replacement costs and service disruption. While a large body of work exists on DRAM in laboratory conditions, little has been reported on real DRAM failures in large production clusters. In this paper, we analyze measurements of memory errors in a large fleet of commodity servers over a period of 2.5 years. The collected data covers multiple vendors, DRAM capacities and technologies, and comprises many millions of DIMM days. The goal of this paper is to answer questions such as the following: How common are memory errors in practice? What are their statistical properties? How are they affected by external factors, such as temperature and utilization, and by chip-specific factors, such as chip density, memory technology and DIMM age? We find that DRAM error behavior in the field differs in many key aspects from commonly held assumptions. For example, we observe DRAM error rates that are orders of magnitude higher than previously reported, with 25,000 to 70,000 errors per billion device hours per Mbit and more than 8\% of DIMMs affected by errors per year. We provide strong evidence that memory errors are dominated by hard errors, rather than soft errors, which previous work suspects to be the dominant error mode. We find that temperature, known to strongly impact DIMM error rates in lab conditions, has a surprisingly small effect on error behavior in the field, when taking all other factors into account. Finally, unlike commonly feared, we don't observe any indication that newer generations of DIMMs have worse error behavior.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555372},
 doi = {http://doi.acm.org/10.1145/1555349.1555372},
 acmid = {1555372},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data corruption, dimm, dram, dram reliability, ecc, empirical study, hard error, large-scale systems, memory, soft error},
} 

@inproceedings{Mi:2009:RUI:1555349.1555373,
 author = {Mi, Ningfang and Riska, Alma and Li, Xin and Smirni, Evgenia and Riedel, Erik},
 title = {Restrained utilization of idleness for transparent scheduling of background tasks},
 abstract = {A common practice in system design is to treat features intended to enhance performance and reliability as low priority tasks by scheduling them during idle periods, with the goal to keep these features transparent to the user. In this paper, we present an algorithmic framework that determines the schedulability of non-preemptable low priority tasks in storage systems. The framework estimates when</i> and for how long</i> idle times can be utilized by low priority background tasks, without violating pre-defined performance targets of user foreground tasks. The estimation is based on monitored system information that includes the histogram of idle times. This histogram captures accurately important statistical characteristics of the complex demands of the foreground activity. The robustness and the effectiveness of the proposed framework is corroborated via extensive trace driven simulations under a wide range of system conditions and background activities, and via experimentation on a Linux kernel 2.6.22 prototype.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555373},
 doi = {http://doi.acm.org/10.1145/1555349.1555373},
 acmid = {1555373},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuous data histogram, idleness, low priority work, performance guarantee},
} 

@inproceedings{Wang:2009:NBM:1555349.1555375,
 author = {Wang, Yi and Schapira, Michael and Rexford, Jennifer},
 title = {Neighbor-specific BGP: more flexible routing policies while improving global stability},
 abstract = {The Border Gateway Protocol (BGP) offers network administrators considerable flexibility in controlling how traffic flows through their networks. However, the interaction between routing policies in different Autonomous Systems (ASes) can lead to protocol oscillation. The best-known sufficient conditions of BGP global routing stability impose restrictions on the kinds of local routing policies individual ASes can safely implement. In this paper, we present neighbor-specific BGP</i> (NS-BGP), a modest extension to BGP that enables a much wider range of local policies without compromising global stability. Whereas a conventional BGP-speaking router selects a single "best" route (for each destination prefix), NS-BGP allows a router to customize the route selection on behalf of each neighbor. For example, one neighbor may prefer the shortest route, another the most secure route, and yet another the least expensive route. Surprisingly, we prove that the much more flexible NS-BGP is guaranteed to be stable under much less restrictive conditions on how routers "rank" the candidate routes. We also show that it is safe to deploy NS-BGP incrementally, as a routing system with a partial deployment of NS-BGP is guaranteed to be stable, even in the presence of failure and other topology changes. In addition to our theoretical results, we also describe how NS-BGP can be deployed by individual ASes independently without changes to the BGP message format or collaboration from neighboring ASes.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555375},
 doi = {http://doi.acm.org/10.1145/1555349.1555375},
 acmid = {1555375},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bgp, flexibility, policy, routing, stability},
} 

@inproceedings{Laoutaris:2009:DTB:1555349.1555376,
 author = {Laoutaris, Nikolaos and Smaragdakis, Georgios and Rodriguez, Pablo and Sundaram, Ravi},
 title = {Delay tolerant bulk data transfers on the internet},
 abstract = {Many emerging scientific and industrial applications require transferring multiple Tbytes of data on a daily basis. Examples include pushing scientific data from particle accelerators/colliders to laboratories around the world, synchronizing data-centers across continents, and replicating collections of high definition videos from events taking place at different time-zones. A key property of all above applications is their ability to tolerate delivery delays ranging from a few hours to a few days. Such Delay Tolerant Bulk</i> (DTB) data are currently being serviced mostly by the postal system using hard drives and DVDs, or by expensive dedicated networks. In this work we propose transmitting such data through commercial ISPs by taking advantage of already-paid-for off-peak bandwidth resulting from diurnal traffic patterns and percentile pricing. We show that between sender-receiver pairs with small time-zone difference, simple source scheduling policies are able to take advantage of most of the existing off-peak capacity. When the time-zone difference increases, taking advantage of the full capacity requires performing store-and-forward through intermediate storage nodes. We present an extensive evaluation of the two options based on traffic data from 200+ links of a large transit provider with PoPs at three continents. Our results indicate that there exists huge potential for performing multi Tbyte transfers on a daily basis at little or no additional cost.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {229--238},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1555349.1555376},
 doi = {http://doi.acm.org/10.1145/1555349.1555376},
 acmid = {1555376},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {95-percentile pricing, content distribution, delay tolerant networks},
} 

@inproceedings{Jiang:2009:CCD:1555349.1555377,
 author = {Jiang, Wenjie and Zhang-Shen, Rui and Rexford, Jennifer and Chiang, Mung},
 title = {Cooperative content distribution and traffic engineering in an ISP network},
 abstract = {Traditionally, Internet Service Providers (ISPs) make profit by providing Internet connectivity, while content providers (CPs) play the more lucrative role of delivering content to users. As network connectivity is increasingly a commodity, ISPs have a strong incentive to offer content to their subscribers by deploying their own content distribution infrastructure. Providing content services in an ISP network presents new opportunities for coordination between traffic engineering (to select efficient routes for the traffic) and server selection (to match servers with subscribers). In this work, we develop a mathematical framework that considers three models with an increasing amount of cooperation between the ISP and the CP. We show that separating server selection and traffic engineering leads to sub-optimal equilibria, even when the CP is given accurate and timely information about the ISP's network in a partial cooperation. More surprisingly, extra visibility may result in a less efficient outcome and such performance degradation can be unbounded. Leveraging ideas from cooperative game theory, we propose an architecture based on the concept of Nash bargaining solution. Simulations on realistic backbone topologies are performed to quantify the performance differences among the three models. Our results apply both when a network provider attempts to provide content, and when separate ISP and CP entities wish to cooperate. This study is a step toward a systematic understanding of the interactions between those who provide and operate networks and those who generate and distribute content.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555377},
 doi = {http://doi.acm.org/10.1145/1555349.1555377},
 acmid = {1555377},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, game, internet, traffic engineering},
} 

@inproceedings{Cohen:2009:LDS:1555349.1555379,
 author = {Cohen, Edith and Kaplan, Haim},
 title = {Leveraging discarded samples for tighter estimation of multiple-set aggregates},
 abstract = {Many datasets, including market basket data, text or hypertext documents, and events recorded in different locations or time periods, can be modeled as a collection of sets over a ground set of keys. Common queries over such data, including similarity or association rules are represented as the weight or selectivity of keys that satisfy some selection predicate defined over keys' attributes and memberships in particular sets. On massive data sets, exact computation of such aggregates can be inefficient or infeasible, and therefore, approximate queries are processed over sketches of the sets. Sketches based on coordinated random samples are scalable and flexible and well suited for many applications. Queries are resolved by producing a sketch of the union of sets used in the predicate from the sketches of these sets and then applying an estimator to this union-sketch. We derive novel tighter (unbiased) estimators that leverage sampled keys that are present in the union of applicable sketches but excluded from the union sketch. We establish analytically that our estimators dominate estimators applied to the union-sketch for all queries and data sets</i>. Empirical evaluation on synthetic and real data reveals that on typical applications we can expect a 25\% 4 fold reduction in estimation error.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {251--262},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555379},
 doi = {http://doi.acm.org/10.1145/1555349.1555379},
 acmid = {1555379},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate query processing, bottom-k sampling, coordinated sampling, multiple-set aggregates, order sampling, sketches},
} 

@inproceedings{Loiseau:2009:MLE:1555349.1555380,
 author = {Loiseau, Patrick and Gon\c{c}alves, Paulo and Girard, St\'{e}phane and Forbes, Florence and Vicat-Blanc Primet, Pascale},
 title = {Maximum likelihood estimation of the flow size distribution tail index from sampled packet data},
 abstract = {In the context of network traffic analysis, we address the problem of estimating the tail index of flow (or more generally of any group) size distribution from the observation of a sampled population of packets (individuals). We give an exhaustive bibliography of the existing methods and show the relations between them. The main contribution of this work is then to propose a new method to estimate the tail index from sampled data, based on the resolution of the maximum likelihood problem. To assess the performance of our method, we present a full performance evaluation based on numerical simulations, and also on a real traffic trace corresponding to internet traffic recently acquired.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {263--274},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555380},
 doi = {http://doi.acm.org/10.1145/1555349.1555380},
 acmid = {1555380},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {expectation-maximization algorithm, heavy-tailed distribution, maximum likelihood estimation, network monitoring, packet sampling, traffic measurement},
} 

@inproceedings{Qiu:2009:MCP:1555349.1555381,
 author = {Qiu, Tongqing and Ge, Zihui and Lee, Seungjoon and Wang, Jia and Zhao, Qi and Xu, Jun},
 title = {Modeling channel popularity dynamics in a large IPTV system},
 abstract = {Understanding the channel popularity or content popularity is an important step in the workload characterization for modern information distribution systems (e.g., World Wide Web, peer-to-peer file-sharing systems, video-on-demand systems). In this paper, we focus on analyzing the channel popularity in the context of Internet Protocol Television (IPTV). In particular, we aim at capturing two important aspects of channel popularity - the distribution and temporal dynamics of the channel popularity. We conduct in-depth analysis on channel popularity on a large collection of user channel access data from a nation-wide commercial IPTV network. Based on the findings in our analysis, we choose a stochastic model that finds good matches in all attributes of interest with respect to the channel popularity. Furthermore, we propose a method to identify subsets of user population with inherently different channel interest. By tracking the change of population mixtures among different user classes, we extend our model to a multi-class population model, which enables us to capture the moderate diurnal popularity patterns exhibited in some channels. We also validate our channel popularity model using real user channel access data from commercial IPTV network.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {275--286},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555381},
 doi = {http://doi.acm.org/10.1145/1555349.1555381},
 acmid = {1555381},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IPTV, channel popularity, modeling, network measurement},
} 

@inproceedings{Harchol-Balter:2009:SRT:1555349.1555383,
 author = {Harchol-Balter, Mor and Scheller-Wolf, Alan and Young, Andrew R.},
 title = {Surprising results on task assignment in server farms with high-variability workloads},
 abstract = {This paper investigates the performance of task assignment policies for server farms, as the variability of job sizes (service demands) approaches infinity. Our results reveal that some common wisdoms regarding task assignment are flawed. The Size-Interval-Task-Assignment policy (SITA), which assigns each server a unique size range, was heretofore thought of by some as the panacea for dealing with high-variability job-size distributions. We show SITA to be inferior to the much simpler greedy policy, Least-Work-Left (LWL), for certain common job-size distributions, including many modal, hyperexponential, and Pareto distributions. We also define regimes where SITA's performance is superior, and prove simple closed-form bounds on its performance for the above-mentioned distributions.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {287--298},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555383},
 doi = {http://doi.acm.org/10.1145/1555349.1555383},
 acmid = {1555383},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high variability, load balancing, m/g/k, pareto, server farm, sita, size-interval, task assignment},
} 

@inproceedings{Sandholm:2009:MOU:1555349.1555384,
 author = {Sandholm, Thomas and Lai, Kevin},
 title = {MapReduce optimization using regulated dynamic prioritization},
 abstract = {We present a system for allocating resources in shared data and compute clusters that improves MapReduce job scheduling in three ways. First, the system uses regulated and user-assigned priorities to offer different service levels to jobs and users over time. Second, the system dynamically adjusts resource allocations to fit the requirements of different job stages. Finally, the system automatically detects and eliminates bottlenecks within a job. We show experimentally using real applications that users can optimize not only job execution time but also the cost-benefit ratio or prioritization efficiency of a job using these three strategies. Our approach relies on a proportional share mechanism that continuously allocates virtual machine resources. Our experimental results show a 11-31\% improvement in completion time and 4-187\% improvement in prioritization efficiency for different classes of MapReduce jobs. We further show that delay intolerant users gain even more from our system.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555384},
 doi = {http://doi.acm.org/10.1145/1555349.1555384},
 acmid = {1555384},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disc, mapreduce, proportional share, resource allocation, workflow optimization},
} 

@inproceedings{Gupta:2009:SAC:1555349.1555385,
 author = {Gupta, Varun and Harchol-Balter, Mor},
 title = {Self-adaptive admission control policies for resource-sharing systems},
 abstract = {We consider the problem of admission control in resource sharing systems, such as web servers and transaction processing systems, when the job size distribution has high variability, with the aim of minimizing the mean response time. It is well known that in such resource sharing systems, as the number of tasks concurrently sharing the resource is increased, the server throughput initially increases, due to more efficient utilization of resources, but starts falling beyond a certain point, due to resource contention and thrashing. Most admission control mechanisms solve this problem by imposing a fixed upper bound on the number of concurrent transactions allowed into the system, called the Multi-Programming-Limit (MPL), and making the arrivals which find the server full queue up. Almost always, the MPL is chosen to be the point that maximizes server efficiency. In this paper we abstract such resource sharing systems as a Processor Sharing (PS) server with state-dependent service rate and a First-Come-First-Served (FCFS) queue, and we analyze the performance of this model from a queueing theoretic perspective. We start by showing that, counter to the common wisdom, the peak efficiency point is not always optimal for minimizing the mean response time. Instead, significant performance gains can be obtained by running the system at less than the peak efficiency. We provide a simple expression for the static MPL that achieves near-optimal mean response time for general distributions. Next we present two traffic-oblivious dynamic admission control policies that adjust the MPL based on the instantaneous queue length while also taking into account the variability of the job size distribution. The structure of our admission control policies is a mixture of fluid control when the number of jobs in the system is high, with a stochastic component when the system is near-empty. We show via simulations that our dynamic policies are much more robust to unknown traffic intensities and burstiness in the arrival process than imposing a static MPL.},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555385},
 doi = {http://doi.acm.org/10.1145/1555349.1555385},
 acmid = {1555385},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {2-moment approximation, admission control, multiprogramming limit, optimal stochastic control, processor sharing, traffic-oblivious policy},
} 

@inproceedings{Kadayif:2007:MID:1254882.1254884,
 author = {Kadayif, Ismail and Kandemir, Mahmut},
 title = {Modeling and improving data cache reliability: 1},
 abstract = {Soft errors arising from energetic particle strikes pose a significant reliability concern for computing systems, especially for those running in noisy environments. Technology scaling and aggressive leakage control mechanisms make the problem caused by these transient errors even more severe. Therefore, it is very important to employ reliability enhancing mechanisms in processor/memory designs to protect them against soft errors. To do so, we first need to model soft errors, and then study cost/reliability tradeoffs among various reliability enhancing techniques based on the model so that system requirements could be met. Since cache memories take the largest fraction of on-chip real estate today and their share is expected to continue to grow in future designs, they are more vulnerable to soft errors, as compared to many other components of a computing system. In this paper, we first focus on a soft error model for L1 data caches, and then explore different reliability enhancing mechanisms. More specifically, we define a metric called AVFC (Architectural Vulnerability Factor for Caches), which represents the probability with which a fault in the cache can be visible in the final output of the program. Based on this model, we then propose three architectural schemes for improving reliability in the existence of soft errors. Our first scheme prevents an error from propagating to the lower levels in the memory hierarchy by not forwarding the unmodified data words of a dirty cache block to the L2 cache when the dirty block is to be replaced. The second scheme proposed selectively invalidates cache blocks to reduce their vulnerable periods, decreasing their chances of catching any soft errors. Based on the AVFC metric, our experimental results show that these two schemes are very effective in alleviating soft errors in the L1 data cache. Specifically, by using our first scheme, it is possible to improve the AVFC metric by 32\% without any performance loss. On the other hand, the second scheme enhances the AVFC metric between 60\% and 97\%, at the cost of a performance degradation which varies from 0\% to 21.3\%, depending on how aggressively the cache blocks are invalidated. To reduce the performance overhead caused by cache block invalidation, we also propose a third scheme which tries to bring a fresh copy of the invalidated block into the cache via prefetching. Our experimental results indicate that, this scheme can reduce the performance overheads to less than 1\% for all applications in our experimental suite, at the cost of giving up a tolerable portion of the reliability enhancement the second scheme achieves.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {12--},
 url = {http://doi.acm.org/10.1145/1254882.1254884},
 doi = {http://doi.acm.org/10.1145/1254882.1254884},
 acmid = {1254884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data caches, data integrity, reliability, soft errors, vulnerability factors},
} 

@article{Kadayif:2007:MID:1269899.1254884,
 author = {Kadayif, Ismail and Kandemir, Mahmut},
 title = {Modeling and improving data cache reliability: 1},
 abstract = {Soft errors arising from energetic particle strikes pose a significant reliability concern for computing systems, especially for those running in noisy environments. Technology scaling and aggressive leakage control mechanisms make the problem caused by these transient errors even more severe. Therefore, it is very important to employ reliability enhancing mechanisms in processor/memory designs to protect them against soft errors. To do so, we first need to model soft errors, and then study cost/reliability tradeoffs among various reliability enhancing techniques based on the model so that system requirements could be met. Since cache memories take the largest fraction of on-chip real estate today and their share is expected to continue to grow in future designs, they are more vulnerable to soft errors, as compared to many other components of a computing system. In this paper, we first focus on a soft error model for L1 data caches, and then explore different reliability enhancing mechanisms. More specifically, we define a metric called AVFC (Architectural Vulnerability Factor for Caches), which represents the probability with which a fault in the cache can be visible in the final output of the program. Based on this model, we then propose three architectural schemes for improving reliability in the existence of soft errors. Our first scheme prevents an error from propagating to the lower levels in the memory hierarchy by not forwarding the unmodified data words of a dirty cache block to the L2 cache when the dirty block is to be replaced. The second scheme proposed selectively invalidates cache blocks to reduce their vulnerable periods, decreasing their chances of catching any soft errors. Based on the AVFC metric, our experimental results show that these two schemes are very effective in alleviating soft errors in the L1 data cache. Specifically, by using our first scheme, it is possible to improve the AVFC metric by 32\% without any performance loss. On the other hand, the second scheme enhances the AVFC metric between 60\% and 97\%, at the cost of a performance degradation which varies from 0\% to 21.3\%, depending on how aggressively the cache blocks are invalidated. To reduce the performance overhead caused by cache block invalidation, we also propose a third scheme which tries to bring a fresh copy of the invalidated block into the cache via prefetching. Our experimental results indicate that, this scheme can reduce the performance overheads to less than 1\% for all applications in our experimental suite, at the cost of giving up a tolerable portion of the reliability enhancement the second scheme achieves.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {12--},
 url = {http://doi.acm.org/10.1145/1269899.1254884},
 doi = {http://doi.acm.org/10.1145/1269899.1254884},
 acmid = {1254884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data caches, data integrity, reliability, soft errors, vulnerability factors},
} 

@inproceedings{Gulati:2007:PAC:1254882.1254885,
 author = {Gulati, Ajay and Merchant, Arif and Varman, Peter J.},
 title = {pClock: an arrival curve based approach for QoS guarantees in shared storage systems},
 abstract = {Storage consolidation is becoming an attractive paradigm for data organization because of the economies of sharing and the ease of centralized management. However, sharing of resources is viable only if applications can be isolated from each other. This work targets the problem of providing performance guarantees to an application irrespective of the behavior of other workloads. Application requirements are represented in terms of the average throughput, latency and maximum burst size. Most earlier schemes only do weighted bandwidth allocation; schemes that provide control of latency either cannot handle bursts or penalize applications for their own prior behavior, such as using spare capacity. Our algorithm p</i>Clock is based on arrival curves that intuitively capture the bandwidth and burst requirements of applications. We show analytically that an application following its arrival curve never misses its deadline. We have implemented p</i>Clock both in DiskSim and as a module in the Linux kernel 2.6. Our evaluation shows three important features of p</i>Clock: (1) benefits over existing algorithms; (2) efficient performance isolation and burst handling; and (3) the ability to allocate spare capacity to either speed up some applications or to a background utility, such as backup. p</i>Clock can be efficiently implemented in a system without much overhead.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254885},
 doi = {http://doi.acm.org/10.1145/1254882.1254885},
 acmid = {1254885},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS, burst handling, fair scheduling, real time guarantees, resource allocation, storage performance virtualization},
} 

@article{Gulati:2007:PAC:1269899.1254885,
 author = {Gulati, Ajay and Merchant, Arif and Varman, Peter J.},
 title = {pClock: an arrival curve based approach for QoS guarantees in shared storage systems},
 abstract = {Storage consolidation is becoming an attractive paradigm for data organization because of the economies of sharing and the ease of centralized management. However, sharing of resources is viable only if applications can be isolated from each other. This work targets the problem of providing performance guarantees to an application irrespective of the behavior of other workloads. Application requirements are represented in terms of the average throughput, latency and maximum burst size. Most earlier schemes only do weighted bandwidth allocation; schemes that provide control of latency either cannot handle bursts or penalize applications for their own prior behavior, such as using spare capacity. Our algorithm p</i>Clock is based on arrival curves that intuitively capture the bandwidth and burst requirements of applications. We show analytically that an application following its arrival curve never misses its deadline. We have implemented p</i>Clock both in DiskSim and as a module in the Linux kernel 2.6. Our evaluation shows three important features of p</i>Clock: (1) benefits over existing algorithms; (2) efficient performance isolation and burst handling; and (3) the ability to allocate spare capacity to either speed up some applications or to a background utility, such as backup. p</i>Clock can be efficiently implemented in a system without much overhead.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254885},
 doi = {http://doi.acm.org/10.1145/1269899.1254885},
 acmid = {1254885},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS, burst handling, fair scheduling, real time guarantees, resource allocation, storage performance virtualization},
} 

@article{Iyer:2007:QPA:1269899.1254886,
 author = {Iyer, Ravi and Zhao, Li and Guo, Fei and Illikkal, Ramesh and Makineni, Srihari and Newell, Don and Solihin, Yan and Hsu, Lisa and Reinhardt, Steve},
 title = {QoS policies and architecture for cache/memory in CMP platforms},
 abstract = {As we enter the era of CMP platforms with multiple threads/cores on the die, the diversity of the simultaneous workloads running on them is expected to increase. The rapid deployment of virtualization as a means to consolidate workloads on to a single platform is a prime example of this trend. In such scenarios, the quality of service (QoS) that each individual workload gets from the platform can widely vary depending on the behavior of the simultaneously running workloads. While the number of cores assigned to each workload can be controlled, there is no hardware or software support in today's platforms to control allocation of platform resources such as cache space and memory bandwidth to individual workloads. In this paper, we propose a QoS-enabled memory architecture for CMP platforms that addresses this problem. The QoS-enabled memory architecture enables more cache resources (i.e. space) and memory resources (i.e. bandwidth) for high priority applications based on guidance from the operating environment. The architecture also allows dynamic resource reassignment during run-time to further optimize the performance of the high priority application with minimal degradation to low priority. To achieve these goals, we will describe the hardware/software support required in the platform as well as the operating environment (O/S and virtual machine monitor). Our evaluation framework consists of detailed platform simulation models and a QoS-enabled version of Linux. Based on evaluation experiments, we show the effectiveness of a QoS-enabled architecture and summarize key findings/trade-offs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254886},
 doi = {http://doi.acm.org/10.1145/1269899.1254886},
 acmid = {1254886},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, QoS, cache/memory, performance, quality of service, resource sharing priniciples, service level agreements},
} 

@inproceedings{Iyer:2007:QPA:1254882.1254886,
 author = {Iyer, Ravi and Zhao, Li and Guo, Fei and Illikkal, Ramesh and Makineni, Srihari and Newell, Don and Solihin, Yan and Hsu, Lisa and Reinhardt, Steve},
 title = {QoS policies and architecture for cache/memory in CMP platforms},
 abstract = {As we enter the era of CMP platforms with multiple threads/cores on the die, the diversity of the simultaneous workloads running on them is expected to increase. The rapid deployment of virtualization as a means to consolidate workloads on to a single platform is a prime example of this trend. In such scenarios, the quality of service (QoS) that each individual workload gets from the platform can widely vary depending on the behavior of the simultaneously running workloads. While the number of cores assigned to each workload can be controlled, there is no hardware or software support in today's platforms to control allocation of platform resources such as cache space and memory bandwidth to individual workloads. In this paper, we propose a QoS-enabled memory architecture for CMP platforms that addresses this problem. The QoS-enabled memory architecture enables more cache resources (i.e. space) and memory resources (i.e. bandwidth) for high priority applications based on guidance from the operating environment. The architecture also allows dynamic resource reassignment during run-time to further optimize the performance of the high priority application with minimal degradation to low priority. To achieve these goals, we will describe the hardware/software support required in the platform as well as the operating environment (O/S and virtual machine monitor). Our evaluation framework consists of detailed platform simulation models and a QoS-enabled version of Linux. Based on evaluation experiments, we show the effectiveness of a QoS-enabled architecture and summarize key findings/trade-offs.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254886},
 doi = {http://doi.acm.org/10.1145/1254882.1254886},
 acmid = {1254886},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, QoS, cache/memory, performance, quality of service, resource sharing priniciples, service level agreements},
} 

@inproceedings{Mesnier:2007:MRF:1254882.1254887,
 author = {Mesnier, Michael P. and Wachs, Matthew and Sambasivan, Raja R. and Zheng, Alice X. and Ganger, Gregory R.},
 title = {Modeling the relative fitness of storage},
 abstract = {Relative fitness is a new black-box approach to modeling the performance of storage devices. In contrast with an absolute model that predicts the performance of a workload on a given storage device, a relative fitness model predicts performance differences</i> between a pair of devices. There are two primary advantages to this approach. First, because are lative fitness model is constructed for a device pair, the application-device feedback of a closed workload can be captured (e.g., how the I/O arrival rate changes as the workload moves from device A to device B). Second, a relative fitness model allows performance and resource utilization to be used in place of workload characteristics. This is beneficial when workload characteristics are difficult to obtain or concisely express (e.g., rather than describe the spatio-temporal characteristics of a workload, one could use the observed cache behavior of device A to help predict the performance of B. This paper describes the steps necessary to build a relative fitness model, with an approach that is general enough to be used with any black-box modeling technique. We compare relative fitness models and absolute models across a variety of workloads and storage devices. On average, relative fitness models predict bandwidth and throughput within 10-20\% and can reduce prediction error by as much as a factor of two when compared to absolute models.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254887},
 doi = {http://doi.acm.org/10.1145/1254882.1254887},
 acmid = {1254887},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CART, black-box, modeling, storage},
} 

@article{Mesnier:2007:MRF:1269899.1254887,
 author = {Mesnier, Michael P. and Wachs, Matthew and Sambasivan, Raja R. and Zheng, Alice X. and Ganger, Gregory R.},
 title = {Modeling the relative fitness of storage},
 abstract = {Relative fitness is a new black-box approach to modeling the performance of storage devices. In contrast with an absolute model that predicts the performance of a workload on a given storage device, a relative fitness model predicts performance differences</i> between a pair of devices. There are two primary advantages to this approach. First, because are lative fitness model is constructed for a device pair, the application-device feedback of a closed workload can be captured (e.g., how the I/O arrival rate changes as the workload moves from device A to device B). Second, a relative fitness model allows performance and resource utilization to be used in place of workload characteristics. This is beneficial when workload characteristics are difficult to obtain or concisely express (e.g., rather than describe the spatio-temporal characteristics of a workload, one could use the observed cache behavior of device A to help predict the performance of B. This paper describes the steps necessary to build a relative fitness model, with an approach that is general enough to be used with any black-box modeling technique. We compare relative fitness models and absolute models across a variety of workloads and storage devices. On average, relative fitness models predict bandwidth and throughput within 10-20\% and can reduce prediction error by as much as a factor of two when compared to absolute models.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254887},
 doi = {http://doi.acm.org/10.1145/1269899.1254887},
 acmid = {1254887},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CART, black-box, modeling, storage},
} 

@article{Wen:2007:FFI:1269899.1254889,
 author = {Wen, Zhihua and Triukose, Sipat and Rabinovich, Michael},
 title = {Facilitating focused internet measurements},
 abstract = {This paper describes our implementation of and initial experiences with DipZoom (for "Deep Internet Performance Zoom"), a novel approach to provide focused, on-demand Internet measurements. Unlike existing approaches that face a difficult challenge of building a measurement platform with sufficiently diverse measurements and measuring hosts, DipZoom implements a matchmaking service instead, which uses P2P concepts to bring together experimenters in need of measurements with external measurement providers. DipZoom offers the following two main contributions. First,since it is just a facilitator for an open community of participants, it promises unprecedented availability of diverse measurements and measuring points. Second, it can be used as a veneer over existing measurement platforms, automating the planning and execution of complex measurements.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254889},
 doi = {http://doi.acm.org/10.1145/1269899.1254889},
 acmid = {1254889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet measurement infrastructures, network measurements, peer-to-peer systems},
} 

@inproceedings{Wen:2007:FFI:1254882.1254889,
 author = {Wen, Zhihua and Triukose, Sipat and Rabinovich, Michael},
 title = {Facilitating focused internet measurements},
 abstract = {This paper describes our implementation of and initial experiences with DipZoom (for "Deep Internet Performance Zoom"), a novel approach to provide focused, on-demand Internet measurements. Unlike existing approaches that face a difficult challenge of building a measurement platform with sufficiently diverse measurements and measuring hosts, DipZoom implements a matchmaking service instead, which uses P2P concepts to bring together experimenters in need of measurements with external measurement providers. DipZoom offers the following two main contributions. First,since it is just a facilitator for an open community of participants, it promises unprecedented availability of diverse measurements and measuring points. Second, it can be used as a veneer over existing measurement platforms, automating the planning and execution of complex measurements.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254889},
 doi = {http://doi.acm.org/10.1145/1254882.1254889},
 acmid = {1254889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet measurement infrastructures, network measurements, peer-to-peer systems},
} 

@article{Huang:2007:DND:1269899.1254890,
 author = {Huang, Yiyi and Feamster, Nick and Lakhina, Anukool and Xu, Jim (Jun)},
 title = {Diagnosing network disruptions with network-wide analysis},
 abstract = {To maintain high availability in the face of changing network conditions, network operators must quickly detect, identify, and react to events that cause network disruptions. One way to accomplish this goal is to monitor routing dynamics, by analyzing routing update streams collected from routers. Existing monitoring approaches typically treat streams of routing updates from different routers as independent signals, and report only the "loud" events (i.e., events that involve large volume of routing messages). In this paper, we examine BGP routing data from all routers in the Abilene backbone for six months and correlate them with a catalog of all known disruptions to its nodes and links. We find that many important events are not loud enough to be detected from a single stream. Instead, they become detectable only when multiple BGP update streams are simultaneously examined. This is because routing updates exhibit network-wide</i> dependencies. This paper proposes using network-wide analysis of routing information to diagnose (i.e., detect and identify) network disruptions. To detect network disruptions, we apply a multivariate analysis technique on dynamic routing information, (i.e., update traffic from all the Abilene routers) and find that this technique can detect every reported disruption to nodes and links within the network with a low rate of false alarms. To identify the type of disruption, we jointly analyze both the network-wide static configuration and details in the dynamic routing updates; we find that our method can correctly explain the scenario that caused the disruption. Although much work remains to make network-wide analysis of routing data operationally practical, our results illustrate the importance and potential of such an approach.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254890},
 doi = {http://doi.acm.org/10.1145/1269899.1254890},
 acmid = {1254890},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, network management, statistical inference},
} 

@inproceedings{Huang:2007:DND:1254882.1254890,
 author = {Huang, Yiyi and Feamster, Nick and Lakhina, Anukool and Xu, Jim (Jun)},
 title = {Diagnosing network disruptions with network-wide analysis},
 abstract = {To maintain high availability in the face of changing network conditions, network operators must quickly detect, identify, and react to events that cause network disruptions. One way to accomplish this goal is to monitor routing dynamics, by analyzing routing update streams collected from routers. Existing monitoring approaches typically treat streams of routing updates from different routers as independent signals, and report only the "loud" events (i.e., events that involve large volume of routing messages). In this paper, we examine BGP routing data from all routers in the Abilene backbone for six months and correlate them with a catalog of all known disruptions to its nodes and links. We find that many important events are not loud enough to be detected from a single stream. Instead, they become detectable only when multiple BGP update streams are simultaneously examined. This is because routing updates exhibit network-wide</i> dependencies. This paper proposes using network-wide analysis of routing information to diagnose (i.e., detect and identify) network disruptions. To detect network disruptions, we apply a multivariate analysis technique on dynamic routing information, (i.e., update traffic from all the Abilene routers) and find that this technique can detect every reported disruption to nodes and links within the network with a low rate of false alarms. To identify the type of disruption, we jointly analyze both the network-wide static configuration and details in the dynamic routing updates; we find that our method can correctly explain the scenario that caused the disruption. Although much work remains to make network-wide analysis of routing data operationally practical, our results illustrate the importance and potential of such an approach.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254890},
 doi = {http://doi.acm.org/10.1145/1254882.1254890},
 acmid = {1254890},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, network management, statistical inference},
} 

@inproceedings{Pucha:2007:UND:1254882.1254891,
 author = {Pucha, Himabindu and Zhang, Ying and Mao, Z. Morley and Hu, Y. Charlie},
 title = {Understanding network delay changes caused by routing events},
 abstract = {Network delays and delay variations are two of the most important network performance metrics directly impacting real-time applications such as voice over IP and time-critical financial transactions. This importance is illustrated by past work on understanding the delay constancy of Internet paths and recent work on predicting network delays using virtual coordinate systems. Merely understanding currently observed delays is insufficient, as network performance can degrade not only due to traffic variability but also as a result of routing changes. Unfortunately this latter effect so far has been ignored in understanding and predicting delay related performance metrics of Internet paths. Our work is the first to address this short coming by systematically analyzing changes in network delays and jitter of a diverse and comprehensive set of Internet paths. Using empirical measurements, we illustrate that routing changes can result in roundtrip delay increase of converged paths by more than 1 second. Surprisingly, intradomain routing changes can also cause such large delay increase. Given these observations, we develop a framework to analyze in detail the impact of routing changes on network delays between end-hosts. Using topology information and properties associated with routing changes, we explain the causes for observed delay fluctuations and more importantly identify routing changes that lead to predictable effects on delay-related metrics. Using our framework, we study the predictability of delay and jitter changes in response to both passively observed interdomain and actively measured intradomain routing changes.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254891},
 doi = {http://doi.acm.org/10.1145/1254882.1254891},
 acmid = {1254891},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network delay changes, network jitter changes, routing dynamics, routing events},
} 

@article{Pucha:2007:UND:1269899.1254891,
 author = {Pucha, Himabindu and Zhang, Ying and Mao, Z. Morley and Hu, Y. Charlie},
 title = {Understanding network delay changes caused by routing events},
 abstract = {Network delays and delay variations are two of the most important network performance metrics directly impacting real-time applications such as voice over IP and time-critical financial transactions. This importance is illustrated by past work on understanding the delay constancy of Internet paths and recent work on predicting network delays using virtual coordinate systems. Merely understanding currently observed delays is insufficient, as network performance can degrade not only due to traffic variability but also as a result of routing changes. Unfortunately this latter effect so far has been ignored in understanding and predicting delay related performance metrics of Internet paths. Our work is the first to address this short coming by systematically analyzing changes in network delays and jitter of a diverse and comprehensive set of Internet paths. Using empirical measurements, we illustrate that routing changes can result in roundtrip delay increase of converged paths by more than 1 second. Surprisingly, intradomain routing changes can also cause such large delay increase. Given these observations, we develop a framework to analyze in detail the impact of routing changes on network delays between end-hosts. Using topology information and properties associated with routing changes, we explain the causes for observed delay fluctuations and more importantly identify routing changes that lead to predictable effects on delay-related metrics. Using our framework, we study the predictability of delay and jitter changes in response to both passively observed interdomain and actively measured intradomain routing changes.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254891},
 doi = {http://doi.acm.org/10.1145/1269899.1254891},
 acmid = {1254891},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network delay changes, network jitter changes, routing dynamics, routing events},
} 

@inproceedings{Kashyap:2007:TRS:1254882.1254893,
 author = {Kashyap, Abhishek and Sengupta, Sudipta and Bhatia, Randeep and Kodialam, M.},
 title = {Two-phase routing, scheduling and power control for wireless mesh networks with variable traffic},
 abstract = {We consider the problem of joint routing, scheduling and transmission power assignment in multi-hop wireless mesh networks with unknown traffic. We assume the traffic is unknown, but the traffic matrix, which specifies the traffic load between every source-destination pair in the network, always lies inside a polytope defined by hose</i> model constraints. The objective is to minimize the maximum of the total transmission power in the network over all traffic matrices in a given polytope. We propose efficient algorithms that compute a two-phase routing, schedule and power assignment, and prove the solution to be 3-approximation with respect to an optimal two-phase routing, scheduling and power assignment. We show via extensive simulations that the proposed algorithm has good performance at its worst operating traffic compared to an algorithm optimized for that traffic.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254893},
 doi = {http://doi.acm.org/10.1145/1254882.1254893},
 acmid = {1254893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {power control, scheduling, two-phase routing, variable traffic, wireless mesh networks},
} 

@article{Kashyap:2007:TRS:1269899.1254893,
 author = {Kashyap, Abhishek and Sengupta, Sudipta and Bhatia, Randeep and Kodialam, M.},
 title = {Two-phase routing, scheduling and power control for wireless mesh networks with variable traffic},
 abstract = {We consider the problem of joint routing, scheduling and transmission power assignment in multi-hop wireless mesh networks with unknown traffic. We assume the traffic is unknown, but the traffic matrix, which specifies the traffic load between every source-destination pair in the network, always lies inside a polytope defined by hose</i> model constraints. The objective is to minimize the maximum of the total transmission power in the network over all traffic matrices in a given polytope. We propose efficient algorithms that compute a two-phase routing, schedule and power assignment, and prove the solution to be 3-approximation with respect to an optimal two-phase routing, scheduling and power assignment. We show via extensive simulations that the proposed algorithm has good performance at its worst operating traffic compared to an algorithm optimized for that traffic.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254893},
 doi = {http://doi.acm.org/10.1145/1269899.1254893},
 acmid = {1254893},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {power control, scheduling, two-phase routing, variable traffic, wireless mesh networks},
} 

@article{Mirza:2007:MLA:1269899.1254894,
 author = {Mirza, Mariyam and Sommers, Joel and Barford, Paul and Zhu, Xiaojin},
 title = {A machine learning approach to TCP throughput prediction},
 abstract = {TCP throughput prediction</i> is an important capability in wide area overlay and multi-homed networks where multiple paths may exist between data sources and receivers. In this paper we describe a new, lightweight method for TCP throughput prediction that can generate accurate forecasts for a broad range of file sizes and path conditions. Our method is based on Support Vector Regression modeling that uses a combination of prior file transfers and measurements of simple path properties. We calibrate and evaluate the capabilities of our throughput predictor in an extensive set of lab-based experiments where ground truth can be established for path properties using highly accurate passive measurements. We report the performance for our method in the ideal case of using our passive path property measurements over a range of test configurations. Our results show that for bulk transfers in heavy traffic, TCP throughput is predicted within 10\% of the actual value 87\% of the time, representing nearly a 3-fold improvement in accuracy over prior history-based methods. In the same lab environment, we assess our method using less accurate active probe measurements of path properties, and show that predictions can be made within 10\% of the actual value nearly 50\% of the time over a range of file sizes and traffic conditions. This result represents approximately a 60\% improvement over history-based methods with a much lower impact on end-to-end paths. Finally, we implement our predictor in a tool called PathPerf</i> and test it in experiments conducted on wide area paths. The results demonstrate that PathPerf</i> predicts TCP through put accurately over a variety of paths.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254894},
 doi = {http://doi.acm.org/10.1145/1269899.1254894},
 acmid = {1254894},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP throughput prediction, active measurements, machine learning, support vector regression},
} 

@inproceedings{Mirza:2007:MLA:1254882.1254894,
 author = {Mirza, Mariyam and Sommers, Joel and Barford, Paul and Zhu, Xiaojin},
 title = {A machine learning approach to TCP throughput prediction},
 abstract = {TCP throughput prediction</i> is an important capability in wide area overlay and multi-homed networks where multiple paths may exist between data sources and receivers. In this paper we describe a new, lightweight method for TCP throughput prediction that can generate accurate forecasts for a broad range of file sizes and path conditions. Our method is based on Support Vector Regression modeling that uses a combination of prior file transfers and measurements of simple path properties. We calibrate and evaluate the capabilities of our throughput predictor in an extensive set of lab-based experiments where ground truth can be established for path properties using highly accurate passive measurements. We report the performance for our method in the ideal case of using our passive path property measurements over a range of test configurations. Our results show that for bulk transfers in heavy traffic, TCP throughput is predicted within 10\% of the actual value 87\% of the time, representing nearly a 3-fold improvement in accuracy over prior history-based methods. In the same lab environment, we assess our method using less accurate active probe measurements of path properties, and show that predictions can be made within 10\% of the actual value nearly 50\% of the time over a range of file sizes and traffic conditions. This result represents approximately a 60\% improvement over history-based methods with a much lower impact on end-to-end paths. Finally, we implement our predictor in a tool called PathPerf</i> and test it in experiments conducted on wide area paths. The results demonstrate that PathPerf</i> predicts TCP through put accurately over a variety of paths.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254894},
 doi = {http://doi.acm.org/10.1145/1254882.1254894},
 acmid = {1254894},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP throughput prediction, active measurements, machine learning, support vector regression},
} 

@inproceedings{Ringberg:2007:SPT:1254882.1254895,
 author = {Ringberg, Haakon and Soule, Augustin and Rexford, Jennifer and Diot, Christophe},
 title = {Sensitivity of PCA for traffic anomaly detection},
 abstract = {Detecting anomalous traffic is a crucial part of managing IP networks. In recent years, network-wide anomaly detection based on Principal Component Analysis (PCA) has emerged as a powerful method for detecting a wide variety of anomalies. We show that tuning PCA to operate effectively in practice is difficult and requires more robust techniques than have been presented thus far. We analyze a week of network-wide traffic measurements from two IP backbones (Abilene and Geant) across three different traffic aggregations (ingress routers, OD flows, and input links), and conduct a detailed inspection of the feature time series for each suspected anomaly. Our study identifies and evaluates four main challenges of using PCA to detect traffic anomalies: (i) the false positive rate is very sensitive to small differences in the number of principal components in the normal subspace, (ii) the effectiveness of PCA is sensitive to the level of aggregation of the traffic measurements, (iii) a large anomaly may in advertently pollute the normal subspace, (iv) correctly identifying which flow triggered the anomaly detector is an inherently challenging problem.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254895},
 doi = {http://doi.acm.org/10.1145/1254882.1254895},
 acmid = {1254895},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network traffic analysis, principal component analysis, traffic engineering},
} 

@article{Ringberg:2007:SPT:1269899.1254895,
 author = {Ringberg, Haakon and Soule, Augustin and Rexford, Jennifer and Diot, Christophe},
 title = {Sensitivity of PCA for traffic anomaly detection},
 abstract = {Detecting anomalous traffic is a crucial part of managing IP networks. In recent years, network-wide anomaly detection based on Principal Component Analysis (PCA) has emerged as a powerful method for detecting a wide variety of anomalies. We show that tuning PCA to operate effectively in practice is difficult and requires more robust techniques than have been presented thus far. We analyze a week of network-wide traffic measurements from two IP backbones (Abilene and Geant) across three different traffic aggregations (ingress routers, OD flows, and input links), and conduct a detailed inspection of the feature time series for each suspected anomaly. Our study identifies and evaluates four main challenges of using PCA to detect traffic anomalies: (i) the false positive rate is very sensitive to small differences in the number of principal components in the normal subspace, (ii) the effectiveness of PCA is sensitive to the level of aggregation of the traffic measurements, (iii) a large anomaly may in advertently pollute the normal subspace, (iv) correctly identifying which flow triggered the anomaly detector is an inherently challenging problem.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254895},
 doi = {http://doi.acm.org/10.1145/1269899.1254895},
 acmid = {1254895},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network traffic analysis, principal component analysis, traffic engineering},
} 

@inproceedings{Lee:2007:BCS:1254882.1254896,
 author = {Lee, Seungjoon and Levin, Dave and Gopalakrishnan, Vijay and Bhattacharjee, Bobby},
 title = {Backbone construction in selfish wireless networks},
 abstract = {We present a protocol to construct routing backbones in wireless networks composed of selfish participants. Backbones are inherently cooperative, so constructing them in selfish environments is particularly difficult; participants want a backbone to exist (soothers relay their packets) but do not want to join the backbone (so they do not have to relay packets for others). We model the wireless backbone as a public good and use impatience as an incentive for cooperation. To determine if and when to donate to this public good, each participant calculates how patient it should be in obtaining the public good. We quantify patience using the Volunteer's Timing Dilemma (VTD), which we extend to general multihop network settings. Using our generalized VTD analysis, each node individually computes as its dominant strategy the amount of time to wait before joining the backbone. We evaluate our protocol using both simulations and an implementation. Our results show that, even though participants in our system deliberately wait before volunteering, a backbone is formed quickly. Further, the quality of the backbone (such as the size and resulting network lifetime) is comparable to that of existing backbone protocols that assume altruistic behavior.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254896},
 doi = {http://doi.acm.org/10.1145/1254882.1254896},
 acmid = {1254896},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {incentives, public good, selfish network, volunteer's dilemma, wireless backbone},
} 

@article{Lee:2007:BCS:1269899.1254896,
 author = {Lee, Seungjoon and Levin, Dave and Gopalakrishnan, Vijay and Bhattacharjee, Bobby},
 title = {Backbone construction in selfish wireless networks},
 abstract = {We present a protocol to construct routing backbones in wireless networks composed of selfish participants. Backbones are inherently cooperative, so constructing them in selfish environments is particularly difficult; participants want a backbone to exist (soothers relay their packets) but do not want to join the backbone (so they do not have to relay packets for others). We model the wireless backbone as a public good and use impatience as an incentive for cooperation. To determine if and when to donate to this public good, each participant calculates how patient it should be in obtaining the public good. We quantify patience using the Volunteer's Timing Dilemma (VTD), which we extend to general multihop network settings. Using our generalized VTD analysis, each node individually computes as its dominant strategy the amount of time to wait before joining the backbone. We evaluate our protocol using both simulations and an implementation. Our results show that, even though participants in our system deliberately wait before volunteering, a backbone is formed quickly. Further, the quality of the backbone (such as the size and resulting network lifetime) is comparable to that of existing backbone protocols that assume altruistic behavior.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254896},
 doi = {http://doi.acm.org/10.1145/1269899.1254896},
 acmid = {1254896},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {incentives, public good, selfish network, volunteer's dilemma, wireless backbone},
} 

@inproceedings{Xia:2007:SFQ:1254882.1254898,
 author = {Xia, Cathy H. and Liu, Zhen and Towsley, Don and Lelarge, Marc},
 title = {Scalability of fork/join queueing networks with blocking},
 abstract = {This paper investigates how the through put of a general fork-join queueing network with blocking behaves as the number of nodes increases to infinity while the processing speed and buffer space of each node stay unchanged. The problem is motivated by applications arising from distributed systems and computer networks. One example is large-scale distributed stream processing systems where TCP is used as the transport protocol for data transfer in between processing components. Other examples include reliable multicast in overlay networks, and reliable data transfer in ad hoc networks. Using an analytical approach, the paper establishes bounds on the asymptotic throughput of such a network. For a subclass of networks which are balanced, we obtain sufficient conditions under which the network stays scalable in the sense that the throughput is lower bounded by a positive constant as the network size increases. Necessary conditions of throughput scalability are derived for general networks. The special class of series-parallel networks is then studied in greater detail, where the asymptotic behavior of the throughput is characterized.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254898},
 doi = {http://doi.acm.org/10.1145/1254882.1254898},
 acmid = {1254898},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymptotic analysis, blocking, fork and join, queueing networks, scalability, throughput},
} 

@article{Xia:2007:SFQ:1269899.1254898,
 author = {Xia, Cathy H. and Liu, Zhen and Towsley, Don and Lelarge, Marc},
 title = {Scalability of fork/join queueing networks with blocking},
 abstract = {This paper investigates how the through put of a general fork-join queueing network with blocking behaves as the number of nodes increases to infinity while the processing speed and buffer space of each node stay unchanged. The problem is motivated by applications arising from distributed systems and computer networks. One example is large-scale distributed stream processing systems where TCP is used as the transport protocol for data transfer in between processing components. Other examples include reliable multicast in overlay networks, and reliable data transfer in ad hoc networks. Using an analytical approach, the paper establishes bounds on the asymptotic throughput of such a network. For a subclass of networks which are balanced, we obtain sufficient conditions under which the network stays scalable in the sense that the throughput is lower bounded by a positive constant as the network size increases. Necessary conditions of throughput scalability are derived for general networks. The special class of series-parallel networks is then studied in greater detail, where the asymptotic behavior of the throughput is characterized.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254898},
 doi = {http://doi.acm.org/10.1145/1269899.1254898},
 acmid = {1254898},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymptotic analysis, blocking, fork and join, queueing networks, scalability, throughput},
} 

@inproceedings{Osogami:2007:OSC:1254882.1254899,
 author = {Osogami, Takayuki and Kato, Sei},
 title = {Optimizing system configurations quickly by guessing at the performance},
 abstract = {The performance of a Web system can be greatly improved by tuning its configuration parameters. However, finding the optimal configuration has been a time-consuming task due to the long measurement time needed to evaluate the performance of a given configuration. We propose an algorithm, which we refer to as Quick Optimization via Guessing (QOG), that quickly selects one of nearly best configurations with high probability. The key ideas in QOG are (i) the measurement of a configuration is terminated as soon as the configuration is found to be suboptimal, and (ii) the performance of a configuration is guessed at based on the measured similar configurations, so that the better configurations are more likely to be measured before the others. If the performance of a good configuration has been measured, a poor configuration will be quickly found to be suboptimal with short measurement time. We apply QOG to optimizing the configuration of a real Web system, and find that QOG can drastically reduce the total measurement time needed to select the best configuration. Our experiments also illuminate several interesting properties of QOG specifically when it is applied to optimizing Web systems.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254899},
 doi = {http://doi.acm.org/10.1145/1254882.1254899},
 acmid = {1254899},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {configuration parameters, performance optimization, ranking and selection, regression, web system},
} 

@article{Osogami:2007:OSC:1269899.1254899,
 author = {Osogami, Takayuki and Kato, Sei},
 title = {Optimizing system configurations quickly by guessing at the performance},
 abstract = {The performance of a Web system can be greatly improved by tuning its configuration parameters. However, finding the optimal configuration has been a time-consuming task due to the long measurement time needed to evaluate the performance of a given configuration. We propose an algorithm, which we refer to as Quick Optimization via Guessing (QOG), that quickly selects one of nearly best configurations with high probability. The key ideas in QOG are (i) the measurement of a configuration is terminated as soon as the configuration is found to be suboptimal, and (ii) the performance of a configuration is guessed at based on the measured similar configurations, so that the better configurations are more likely to be measured before the others. If the performance of a good configuration has been measured, a poor configuration will be quickly found to be suboptimal with short measurement time. We apply QOG to optimizing the configuration of a real Web system, and find that QOG can drastically reduce the total measurement time needed to select the best configuration. Our experiments also illuminate several interesting properties of QOG specifically when it is applied to optimizing Web systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254899},
 doi = {http://doi.acm.org/10.1145/1269899.1254899},
 acmid = {1254899},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {configuration parameters, performance optimization, ranking and selection, regression, web system},
} 

@article{Wang:2007:SSR:1269899.1254900,
 author = {Wang, Zhe and Dong, Wei and Josephson, William and Lv, Qin and Charikar, Moses and Li, Kai},
 title = {Sizing sketches: a rank-based analysis for similarity search},
 abstract = {Sketches are compact data structures that can be used to estimate properties of the original data in building large-scale search engines and data analysis systems. Recent theoretical and experimental studies have shown that sketches constructed from feature vectors using randomized projections can effectively approximate L1 distance on the feature vectors with the Hamming distance on their sketches. Furthermore, such sketches can achieve good filtering accuracy while reducing the metadata space requirement and speeding up similarity searches by an order of magnitude. However, it is not clear how to choose the size of the sketches since it depends ondata type, dataset size, and desired filtering quality. In real systems designs, it is necessary to understand how to choose sketch size without the dataset, or at least without the whole datase. This paper presents an analytical model and experimental results to help system designers make such design decisions. We present arank-based filtering model that describes the relationship between sketch size and data set size based on the dataset distance distribution. Our experimental results with several datasets including images, audio, and 3D shapes show that the model yields good, conservative predictions. We show that the parameters of the model can be set with a small sample data set and the resulting model can make good predictions for a large dataset. We illustrate how to apply the approach with a concrete example.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254900},
 doi = {http://doi.acm.org/10.1145/1269899.1254900},
 acmid = {1254900},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feature-rich data, similarity search, sketch},
} 

@inproceedings{Wang:2007:SSR:1254882.1254900,
 author = {Wang, Zhe and Dong, Wei and Josephson, William and Lv, Qin and Charikar, Moses and Li, Kai},
 title = {Sizing sketches: a rank-based analysis for similarity search},
 abstract = {Sketches are compact data structures that can be used to estimate properties of the original data in building large-scale search engines and data analysis systems. Recent theoretical and experimental studies have shown that sketches constructed from feature vectors using randomized projections can effectively approximate L1 distance on the feature vectors with the Hamming distance on their sketches. Furthermore, such sketches can achieve good filtering accuracy while reducing the metadata space requirement and speeding up similarity searches by an order of magnitude. However, it is not clear how to choose the size of the sketches since it depends ondata type, dataset size, and desired filtering quality. In real systems designs, it is necessary to understand how to choose sketch size without the dataset, or at least without the whole datase. This paper presents an analytical model and experimental results to help system designers make such design decisions. We present arank-based filtering model that describes the relationship between sketch size and data set size based on the dataset distance distribution. Our experimental results with several datasets including images, audio, and 3D shapes show that the model yields good, conservative predictions. We show that the parameters of the model can be set with a small sample data set and the resulting model can make good predictions for a large dataset. We illustrate how to apply the approach with a concrete example.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254900},
 doi = {http://doi.acm.org/10.1145/1254882.1254900},
 acmid = {1254900},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feature-rich data, similarity search, sketch},
} 

@article{Park:2007:MET:1269899.1254902,
 author = {Park, Soyeon and Jiang, Weihang and Zhou, Yuanyuan and Adve, Sarita},
 title = {Managing energy-performance tradeoffs for multithreaded applications on multiprocessor architectures},
 abstract = {In modern computers, non-performance metrics such as energy consumption have become increasingly important, requiring tradeoff with performance. A recent work has proposed performance-guaranteed energy management, but it is designed specifically for sequential applications and cannot be used to a large class of multithreaded applications running on high end computers and data servers. To address the above problem, this paper makes the first attempt to provide performance-guaranteed energy management for multithreaded applications on multiprocessor architectures. We first conduct a comprehensive study on the effects of energy adaptation on thread synchronizations and show that a multithreaded application suffers from not only local slowdowns due to energy adaptation, but also significant slowdowns propagated from other threads because of synchronization. Based on these findings, we design three Synchronization-Aware (SA) algorithms, LWT (Lock Waiting Time-based), CSL (Critical Section Length-based) and ODP (Operation Delay Propagation-based) algorithms, to estimate the energy adaptation-induced slowdowns on each thread. The local slowdowns are then combined across multiple threads via three aggregation methods (MAX, AVG and SUM) to estimate the overall application slowdown. We evaluate our methods using a large multithreaded commercial application, IBM DB2 with industrial-strength online transaction processing (OLTP) workloads, and six SPLASH parallel scientific applications. Our experimental results show that LWT combined with the MAX aggregation method not only controls the performance slow down within the specified limits but also conserves the most energy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254902},
 doi = {http://doi.acm.org/10.1145/1269899.1254902},
 acmid = {1254902},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy and performance tradeoffs, low power design, memory energy management, multithreaded applications},
} 

@inproceedings{Park:2007:MET:1254882.1254902,
 author = {Park, Soyeon and Jiang, Weihang and Zhou, Yuanyuan and Adve, Sarita},
 title = {Managing energy-performance tradeoffs for multithreaded applications on multiprocessor architectures},
 abstract = {In modern computers, non-performance metrics such as energy consumption have become increasingly important, requiring tradeoff with performance. A recent work has proposed performance-guaranteed energy management, but it is designed specifically for sequential applications and cannot be used to a large class of multithreaded applications running on high end computers and data servers. To address the above problem, this paper makes the first attempt to provide performance-guaranteed energy management for multithreaded applications on multiprocessor architectures. We first conduct a comprehensive study on the effects of energy adaptation on thread synchronizations and show that a multithreaded application suffers from not only local slowdowns due to energy adaptation, but also significant slowdowns propagated from other threads because of synchronization. Based on these findings, we design three Synchronization-Aware (SA) algorithms, LWT (Lock Waiting Time-based), CSL (Critical Section Length-based) and ODP (Operation Delay Propagation-based) algorithms, to estimate the energy adaptation-induced slowdowns on each thread. The local slowdowns are then combined across multiple threads via three aggregation methods (MAX, AVG and SUM) to estimate the overall application slowdown. We evaluate our methods using a large multithreaded commercial application, IBM DB2 with industrial-strength online transaction processing (OLTP) workloads, and six SPLASH parallel scientific applications. Our experimental results show that LWT combined with the MAX aggregation method not only controls the performance slow down within the specified limits but also conserves the most energy.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254902},
 doi = {http://doi.acm.org/10.1145/1254882.1254902},
 acmid = {1254902},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy and performance tradeoffs, low power design, memory energy management, multithreaded applications},
} 

@article{Cvetkovski:2007:AAC:1269899.1254903,
 author = {Cvetkovski, Andrej},
 title = {An algorithm for approximate counting using limited memory resources},
 abstract = {This paper describes a randomized algorithm for approximate counting that preserves the same modest memory requirements of log(log n) bits per counter as the approximate counting algorithm introduced in the seminal paper of R. Morris (1978), and in addition, is characterized by (i) lower expected number of memory accesses and (ii) lower standard error on more than 99 percent of its counting range. An exact analysis of the relevant statistical properties of the algorithm is carried out. Performance evaluation via simulations is also provided to validate the presented theory. Given its properties, the presented algorithm is suitable as a basic building block of data streaming applications having a large number of simultaneous counters and/or operating at very high speeds. As such, it is applicable to a wide range of measurement and monitoring operations, including performance monitoring of communication hardware, measurements for optimization in large database systems, and gathering statistics for data compression.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1269899.1254903},
 doi = {http://doi.acm.org/10.1145/1269899.1254903},
 acmid = {1254903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate counting, data streaming, network monitoring},
} 

@inproceedings{Cvetkovski:2007:AAC:1254882.1254903,
 author = {Cvetkovski, Andrej},
 title = {An algorithm for approximate counting using limited memory resources},
 abstract = {This paper describes a randomized algorithm for approximate counting that preserves the same modest memory requirements of log(log n) bits per counter as the approximate counting algorithm introduced in the seminal paper of R. Morris (1978), and in addition, is characterized by (i) lower expected number of memory accesses and (ii) lower standard error on more than 99 percent of its counting range. An exact analysis of the relevant statistical properties of the algorithm is carried out. Performance evaluation via simulations is also provided to validate the presented theory. Given its properties, the presented algorithm is suitable as a basic building block of data streaming applications having a large number of simultaneous counters and/or operating at very high speeds. As such, it is applicable to a wide range of measurement and monitoring operations, including performance monitoring of communication hardware, measurements for optimization in large database systems, and gathering statistics for data compression.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1254882.1254903},
 doi = {http://doi.acm.org/10.1145/1254882.1254903},
 acmid = {1254903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate counting, data streaming, network monitoring},
} 

@article{Lee:2007:SDN:1269899.1254904,
 author = {Lee, Eric S. and Whalen, Thom},
 title = {Synthetic designs: a new form of true experimental design for use in information systems development},
 abstract = {Computer scientists and software engineers seldom rely on using experimental methods despite frequent calls to do so. The problem may lie with the shortcomings of traditional experimental methods. We introduce a new form of experimental designs, synthetic designs, which address these shortcomings. Compared with classical experimental designs (between-subjects, within-subjects, and matched-subjects), synthetic designs can offer substantial reductions in sample sizes, cost, time and effort expended, increased statistical power, and fewer threats to validity (internal, external, and statistical conclusion). This new design is a variation of within-subjects design in which each system user serves in only a single treatment condition. System performance scores for all other treatment conditions are derived synthetically without repeated testing of each subject. This design, though not applicable in all situations, can be used in the development and testing of some computer systems provided that user behavior is unaffected by the version of computer system being used. We justify synthetic designs on three grounds: this design has been used successfully in the development of computerized mug shot systems, showing marked advantages over traditional designs; a detailed comparison with traditional designs showing their advantages on 17 of the 18 criteria considered; and an assessment showing these designs satisfy all the requirements of true experiments (albeit in a novel way).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254904},
 doi = {http://doi.acm.org/10.1145/1269899.1254904},
 acmid = {1254904},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {experimental designs, synthetic experimental designs},
} 

@inproceedings{Lee:2007:SDN:1254882.1254904,
 author = {Lee, Eric S. and Whalen, Thom},
 title = {Synthetic designs: a new form of true experimental design for use in information systems development},
 abstract = {Computer scientists and software engineers seldom rely on using experimental methods despite frequent calls to do so. The problem may lie with the shortcomings of traditional experimental methods. We introduce a new form of experimental designs, synthetic designs, which address these shortcomings. Compared with classical experimental designs (between-subjects, within-subjects, and matched-subjects), synthetic designs can offer substantial reductions in sample sizes, cost, time and effort expended, increased statistical power, and fewer threats to validity (internal, external, and statistical conclusion). This new design is a variation of within-subjects design in which each system user serves in only a single treatment condition. System performance scores for all other treatment conditions are derived synthetically without repeated testing of each subject. This design, though not applicable in all situations, can be used in the development and testing of some computer systems provided that user behavior is unaffected by the version of computer system being used. We justify synthetic designs on three grounds: this design has been used successfully in the development of computerized mug shot systems, showing marked advantages over traditional designs; a detailed comparison with traditional designs showing their advantages on 17 of the 18 criteria considered; and an assessment showing these designs satisfy all the requirements of true experiments (albeit in a novel way).},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254904},
 doi = {http://doi.acm.org/10.1145/1254882.1254904},
 acmid = {1254904},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {experimental designs, synthetic experimental designs},
} 

@inproceedings{Feng:2007:PUP:1254882.1254906,
 author = {Feng, Hanhua and Misra, Vishal and Rubenstein, Dan},
 title = {PBS: a unified priority-based scheduler},
 abstract = {Blind scheduling policies schedule tasks without knowledge of the tasks' remaining processing times. Existing blind policies, such as FCFS, PS, and LAS, have proven useful in network and operating system applications, but each policy has a separate, vastly differing description, leading to separate and distinct implementations. This paper presents the design and implementation of a configurable blind scheduler that contains a continuous, tunable parameter. By merely changing the value of this parameter, the scheduler's policy exactly emulates or closely approximates several existing standard policies. Other settings enable policies whose behavior is a hybrid of these standards. We demonstrate the practical benefits of such a configurable</i> scheduler by implementing it into the Linux operating system. We show that we can emulate the behavior of Linux's existing, more complex scheduler with a single (hybrid) setting of the parameter. We also show, using synthetic workloads, that the best value for the tunable parameter is not unique, but depends on distribution of the size of tasks arriving to the system. Finally, we use our formulation of the configurable scheduler to contrast the behavior of various blind schedulers by exploring how various properties of the scheduler change as we vary our scheduler's tunable parameter.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {203--214},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254906},
 doi = {http://doi.acm.org/10.1145/1254882.1254906},
 acmid = {1254906},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS, LAS, PBS, linux, queueing systems, scheduling},
} 

@article{Feng:2007:PUP:1269899.1254906,
 author = {Feng, Hanhua and Misra, Vishal and Rubenstein, Dan},
 title = {PBS: a unified priority-based scheduler},
 abstract = {Blind scheduling policies schedule tasks without knowledge of the tasks' remaining processing times. Existing blind policies, such as FCFS, PS, and LAS, have proven useful in network and operating system applications, but each policy has a separate, vastly differing description, leading to separate and distinct implementations. This paper presents the design and implementation of a configurable blind scheduler that contains a continuous, tunable parameter. By merely changing the value of this parameter, the scheduler's policy exactly emulates or closely approximates several existing standard policies. Other settings enable policies whose behavior is a hybrid of these standards. We demonstrate the practical benefits of such a configurable</i> scheduler by implementing it into the Linux operating system. We show that we can emulate the behavior of Linux's existing, more complex scheduler with a single (hybrid) setting of the parameter. We also show, using synthetic workloads, that the best value for the tunable parameter is not unique, but depends on distribution of the size of tasks arriving to the system. Finally, we use our formulation of the configurable scheduler to contrast the behavior of various blind schedulers by exploring how various properties of the scheduler change as we vary our scheduler's tunable parameter.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {203--214},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254906},
 doi = {http://doi.acm.org/10.1145/1269899.1254906},
 acmid = {1254906},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS, LAS, PBS, linux, queueing systems, scheduling},
} 

@article{Jelenkovic:2007:ASC:1269899.1254907,
 author = {Jelenkovic, Predrag R. and Kang, Xiaozhu and Tan, Jian},
 title = {Adaptive and scalable comparison scheduling},
 abstract = {The Shortest Remaining Processing Time (SRPT) scheduling disciplineis optimal and its superior performance, compared with the policies that do not use the knowledge of job sizes, can be quantified using mean-value analysis as well as our new a symptotic distribution allimits for the relatively smaller heavy-tailed jobs. However, the main difficulty in implementing SRPT in large practical systems, e.g., Web servers, is that its complexity grows with the number of jobs in the queue. Hence, in order to lower the complexity, it is natural to approximate SRPT by grouping the arrivals into a fixed (small) number of classes containing jobs of approximately equal size and then serve the classes of smaller jobs with higher priorities. In this paper, we design a novel adaptive grouping mechanism based on relative size comparison of a newly arriving job to the preceding m</i> arrivals. Specifically, if the newly arriving job is smallerthan k</i> and larger than m-k</i> of the previous m</i> jobs, it isrouted into class k</i>. The excellent performance of this mechanism,even for a small number of classes m</i>+1, is demonstrated using both the asymptotic queueing analysis under heavy tails and extensive simulations. We also discuss refinements of the comparison grouping mechanism that improve the accuracy of job classification at the expense of a small additional complexity.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {215--226},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254907},
 doi = {http://doi.acm.org/10.1145/1269899.1254907},
 acmid = {1254907},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, adaptive thresholds, comparison scheduling, scalability},
} 

@inproceedings{Jelenkovic:2007:ASC:1254882.1254907,
 author = {Jelenkovic, Predrag R. and Kang, Xiaozhu and Tan, Jian},
 title = {Adaptive and scalable comparison scheduling},
 abstract = {The Shortest Remaining Processing Time (SRPT) scheduling disciplineis optimal and its superior performance, compared with the policies that do not use the knowledge of job sizes, can be quantified using mean-value analysis as well as our new a symptotic distribution allimits for the relatively smaller heavy-tailed jobs. However, the main difficulty in implementing SRPT in large practical systems, e.g., Web servers, is that its complexity grows with the number of jobs in the queue. Hence, in order to lower the complexity, it is natural to approximate SRPT by grouping the arrivals into a fixed (small) number of classes containing jobs of approximately equal size and then serve the classes of smaller jobs with higher priorities. In this paper, we design a novel adaptive grouping mechanism based on relative size comparison of a newly arriving job to the preceding m</i> arrivals. Specifically, if the newly arriving job is smallerthan k</i> and larger than m-k</i> of the previous m</i> jobs, it isrouted into class k</i>. The excellent performance of this mechanism,even for a small number of classes m</i>+1, is demonstrated using both the asymptotic queueing analysis under heavy tails and extensive simulations. We also discuss refinements of the comparison grouping mechanism that improve the accuracy of job classification at the expense of a small additional complexity.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {215--226},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254907},
 doi = {http://doi.acm.org/10.1145/1254882.1254907},
 acmid = {1254907},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, adaptive thresholds, comparison scheduling, scalability},
} 

@article{Bhadra:2007:OCP:1269899.1254909,
 author = {Bhadra, Sandeep and Lu, Yingdong and Squillante, Mark S.},
 title = {Optimal capacity planning in stochastic loss networks with time-varying workloads},
 abstract = {We consider a capacity planning optimization problem in a general theoretical framework that extends the classical Erlang loss modeland related stochastic loss networks to support time-varying workloads. The time horizon consists of a sequence of coarse time intervals, each of which involves a stochastic loss network under a fixed multi-class workload that can change in a general manner from one interval to the next. The optimization problem consists of determining the capacities for each time interval that maximize a utility function over the entire time horizon, finite or infinite, where rewards gained from servicing customers are offset by penalties associated with deploying capacities in an interval and with changing capacities among intervals. We derive a state-dependent optimal policy within the context of a particular limiting regime of the optimization problem, and we prove this solution to be a symptotically optimal. Then, under fairly mild conditions, we prove that a similar structural property holds for the optimal solution of the original stochastic optimization problem, and we show how the optimal capacities comprising this solution can be efficiently computed.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {227--238},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254909},
 doi = {http://doi.acm.org/10.1145/1269899.1254909},
 acmid = {1254909},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymptotic optimality, capacity planning, erlang fixed-point approximation, erlang loss formula, stochastic dynamic programming, stochastic loss networks, time-varying workloads},
} 

@inproceedings{Bhadra:2007:OCP:1254882.1254909,
 author = {Bhadra, Sandeep and Lu, Yingdong and Squillante, Mark S.},
 title = {Optimal capacity planning in stochastic loss networks with time-varying workloads},
 abstract = {We consider a capacity planning optimization problem in a general theoretical framework that extends the classical Erlang loss modeland related stochastic loss networks to support time-varying workloads. The time horizon consists of a sequence of coarse time intervals, each of which involves a stochastic loss network under a fixed multi-class workload that can change in a general manner from one interval to the next. The optimization problem consists of determining the capacities for each time interval that maximize a utility function over the entire time horizon, finite or infinite, where rewards gained from servicing customers are offset by penalties associated with deploying capacities in an interval and with changing capacities among intervals. We derive a state-dependent optimal policy within the context of a particular limiting regime of the optimization problem, and we prove this solution to be a symptotically optimal. Then, under fairly mild conditions, we prove that a similar structural property holds for the optimal solution of the original stochastic optimization problem, and we show how the optimal capacities comprising this solution can be efficiently computed.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {227--238},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254909},
 doi = {http://doi.acm.org/10.1145/1254882.1254909},
 acmid = {1254909},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymptotic optimality, capacity planning, erlang fixed-point approximation, erlang loss formula, stochastic dynamic programming, stochastic loss networks, time-varying workloads},
} 

@article{Liu:2007:FSD:1269899.1254910,
 author = {Liu, Jiaping and Prouti\`{e}re, Alexandre and Yi, Yung and Chiang, Mung and Poor, H. Vincent},
 title = {Flow-level stability of data networks with non-convex and time-varying rate regions},
 abstract = {In this paper we characterize flow-level stochastic stability for networks with non-convex or time-varying rate regions underresource allocation based on utility maximization. Similar to prior works on flow-level stability, we consider exogenous data arrivals with finite workloads. However, to model many realistic situations, the rate region, which constrains the feasibility of resource allocation, may be either non-convex or time-varying. When the rate region is fixed but non-convex, we derive sufficient and necessary conditions for stability, which coincide when the set of allocated rate vectors has continuous contours. When the rate region is time-varying according to some stationary, ergodic process, we derive the precise stability region. In both cases,the size of the stability region depends on the resource allocation policy, in particular, on the fairness parameter in \&#8733;-fair utility maximization. This is in sharp contrast with the substantial existing literature on stability under fixed and convex rate regions, in which the stability region coincides with the rate region for many utility-based resource allocation schemes, independently of the value of the fairness parameter. We further investigate the tradeoff between fairness and stability when rate region is non-convex or time-varying. Numerical examples of both wired and wireless networks are provided to illustrate the new stability regions and tradeoffs proved in the paper.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254910},
 doi = {http://doi.acm.org/10.1145/1269899.1254910},
 acmid = {1254910},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fairness, network utility maximization, resource allocation, stability},
} 

@inproceedings{Liu:2007:FSD:1254882.1254910,
 author = {Liu, Jiaping and Prouti\`{e}re, Alexandre and Yi, Yung and Chiang, Mung and Poor, H. Vincent},
 title = {Flow-level stability of data networks with non-convex and time-varying rate regions},
 abstract = {In this paper we characterize flow-level stochastic stability for networks with non-convex or time-varying rate regions underresource allocation based on utility maximization. Similar to prior works on flow-level stability, we consider exogenous data arrivals with finite workloads. However, to model many realistic situations, the rate region, which constrains the feasibility of resource allocation, may be either non-convex or time-varying. When the rate region is fixed but non-convex, we derive sufficient and necessary conditions for stability, which coincide when the set of allocated rate vectors has continuous contours. When the rate region is time-varying according to some stationary, ergodic process, we derive the precise stability region. In both cases,the size of the stability region depends on the resource allocation policy, in particular, on the fairness parameter in \&#8733;-fair utility maximization. This is in sharp contrast with the substantial existing literature on stability under fixed and convex rate regions, in which the stability region coincides with the rate region for many utility-based resource allocation schemes, independently of the value of the fairness parameter. We further investigate the tradeoff between fairness and stability when rate region is non-convex or time-varying. Numerical examples of both wired and wireless networks are provided to illustrate the new stability regions and tradeoffs proved in the paper.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254910},
 doi = {http://doi.acm.org/10.1145/1254882.1254910},
 acmid = {1254910},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fairness, network utility maximization, resource allocation, stability},
} 

@article{Smirni:2007:FDP:1269899.1254912,
 author = {Smirni, Evgenia and Darema, Frederica and Greenberg, Albert and Hoisie, Adolfy and Towsley, Don},
 title = {Future directions in performance evaluation research},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {251--252},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254912},
 doi = {http://doi.acm.org/10.1145/1269899.1254912},
 acmid = {1254912},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smirni:2007:FDP:1254882.1254912,
 author = {Smirni, Evgenia and Darema, Frederica and Greenberg, Albert and Hoisie, Adolfy and Towsley, Don},
 title = {Future directions in performance evaluation research},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {251--252},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254912},
 doi = {http://doi.acm.org/10.1145/1254882.1254912},
 acmid = {1254912},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dong:2007:WSP:1269899.1254914,
 author = {Dong, Qunfeng and Banerjee, Suman and Wang, Jia and Agrawal, Dheeraj},
 title = {Wire speed packet classification without tcams: a few more registers (and a bit of logic) are enough},
 abstract = {Packet classification is the foundation of many Internet functions such as QoS and security. A long thread of research has proposed efficient software-based solutions to this problem. Such software solutions are attractive because they require cheap memory systems for implementation, thus bringing down the overall cost of the system. In contrast, hardware-based solutions use more expensive memory systems, e.g., TCAMs, but are often preferred by router vendors for their faster classification speeds. The goal of this paper is to find a "best-of-both-worlds" solution -- a solution that incurs the cost of a software-based system and has the speed of a hardware-based one. Our proposed solution, called smart rule cache</i> achieves this goal by using minimal hardware -- a few additional registers -- to cache evolving</i> rules which preserve classification semantics, and additional logic to match incoming packets to these rules. Using real traffic traces and real rule sets from a tier-1 ISP, we show such a setup is sufficient to achieve very high hit ratios for fast classification in hardware. Cache miss ratios are 2 \&#8764; 4 orders of magnitude lower than flow cache schemes. Given its low cost and good performance, we believe our solution may create significant impact on current industry practice.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254914},
 doi = {http://doi.acm.org/10.1145/1269899.1254914},
 acmid = {1254914},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, rule cache, rule evolution},
} 

@inproceedings{Dong:2007:WSP:1254882.1254914,
 author = {Dong, Qunfeng and Banerjee, Suman and Wang, Jia and Agrawal, Dheeraj},
 title = {Wire speed packet classification without tcams: a few more registers (and a bit of logic) are enough},
 abstract = {Packet classification is the foundation of many Internet functions such as QoS and security. A long thread of research has proposed efficient software-based solutions to this problem. Such software solutions are attractive because they require cheap memory systems for implementation, thus bringing down the overall cost of the system. In contrast, hardware-based solutions use more expensive memory systems, e.g., TCAMs, but are often preferred by router vendors for their faster classification speeds. The goal of this paper is to find a "best-of-both-worlds" solution -- a solution that incurs the cost of a software-based system and has the speed of a hardware-based one. Our proposed solution, called smart rule cache</i> achieves this goal by using minimal hardware -- a few additional registers -- to cache evolving</i> rules which preserve classification semantics, and additional logic to match incoming packets to these rules. Using real traffic traces and real rule sets from a tier-1 ISP, we show such a setup is sufficient to achieve very high hit ratios for fast classification in hardware. Cache miss ratios are 2 \&#8764; 4 orders of magnitude lower than flow cache schemes. Given its low cost and good performance, we believe our solution may create significant impact on current industry practice.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254914},
 doi = {http://doi.acm.org/10.1145/1254882.1254914},
 acmid = {1254914},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, rule cache, rule evolution},
} 

@article{Hirzel:2007:DLO:1269899.1254915,
 author = {Hirzel, Martin},
 title = {Data layouts for object-oriented programs},
 abstract = {Object-oriented programs rely heavily on objects and pointers, making them vulnerable to slow downs from cache and TLB misses. The cache and TLB behavior depends on the data layout of objects in memory. There are many possible data layouts with different impacts on performance, but it is not known which perform better. This paper presents a novel framework for evaluating data layouts. The framework both makes implementing many layouts easy, andenables performance measurements of real programs using a product Java virtual machine on stock hardware. This is achieved by sorting objects during copying garbage collection; outside of garbage collection, program performance is solely determined by the data layout that the sort key implements. This paper surveys and evaluates 10 common data layouts with 32 realistic bench mark programs running on 3 different hardware configurations. The results confirm the importance of data layouts for program performance, and show that almost all layouts yield the best performance for some programs and the worst performance for others.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254915},
 doi = {http://doi.acm.org/10.1145/1269899.1254915},
 acmid = {1254915},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GC, TLB, cache, data layout, data placement, hardware performance counters, memory subsystem, spatial locality},
} 

@inproceedings{Hirzel:2007:DLO:1254882.1254915,
 author = {Hirzel, Martin},
 title = {Data layouts for object-oriented programs},
 abstract = {Object-oriented programs rely heavily on objects and pointers, making them vulnerable to slow downs from cache and TLB misses. The cache and TLB behavior depends on the data layout of objects in memory. There are many possible data layouts with different impacts on performance, but it is not known which perform better. This paper presents a novel framework for evaluating data layouts. The framework both makes implementing many layouts easy, andenables performance measurements of real programs using a product Java virtual machine on stock hardware. This is achieved by sorting objects during copying garbage collection; outside of garbage collection, program performance is solely determined by the data layout that the sort key implements. This paper surveys and evaluates 10 common data layouts with 32 realistic bench mark programs running on 3 different hardware configurations. The results confirm the importance of data layouts for program performance, and show that almost all layouts yield the best performance for some programs and the worst performance for others.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254915},
 doi = {http://doi.acm.org/10.1145/1254882.1254915},
 acmid = {1254915},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GC, TLB, cache, data layout, data placement, hardware performance counters, memory subsystem, spatial locality},
} 

@article{Hao:2007:BHA:1269899.1254916,
 author = {Hao, Fang and Kodialam, Murali and Lakshman, T. V.},
 title = {Building high accuracy bloom filters using partitioned hashing},
 abstract = {The growing importance of operations such as packet-content inspection, packet classification based on non-IP headers, maintaining flow-state, etc. has led to increased interest in the networking applications of Bloom filters. This is because Bloom filters provide a relatively easy method for hardware implementation of set-membership queries. However, the tradeoff is that Bloom filters only provide a probabilistic test and membership queries can result in false positives. Ideally, we would like this false positive probability to be very low. The main contribution of this paper is a method for significantly reducing this false positive probability in comparison to existing schemes. This is done by developing a partitioned hashing</i> method which results in a choice of hash functions that set far fewer bits in the Bloom filter bit vector than would be the case otherwise. This lower fill factor of the bit vector translates to a much lower false positive probability. We show experimentally that this improved choice can result in as much as a ten-fold increase in accuracy over standard Bloom filters. We also show that the scheme performs much better than other proposed schemes for improving Bloom filters.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254916},
 doi = {http://doi.acm.org/10.1145/1269899.1254916},
 acmid = {1254916},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bloom filter, hashing},
} 

@inproceedings{Hao:2007:BHA:1254882.1254916,
 author = {Hao, Fang and Kodialam, Murali and Lakshman, T. V.},
 title = {Building high accuracy bloom filters using partitioned hashing},
 abstract = {The growing importance of operations such as packet-content inspection, packet classification based on non-IP headers, maintaining flow-state, etc. has led to increased interest in the networking applications of Bloom filters. This is because Bloom filters provide a relatively easy method for hardware implementation of set-membership queries. However, the tradeoff is that Bloom filters only provide a probabilistic test and membership queries can result in false positives. Ideally, we would like this false positive probability to be very low. The main contribution of this paper is a method for significantly reducing this false positive probability in comparison to existing schemes. This is done by developing a partitioned hashing</i> method which results in a choice of hash functions that set far fewer bits in the Bloom filter bit vector than would be the case otherwise. This lower fill factor of the bit vector translates to a much lower false positive probability. We show experimentally that this improved choice can result in as much as a ten-fold increase in accuracy over standard Bloom filters. We also show that the scheme performs much better than other proposed schemes for improving Bloom filters.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254916},
 doi = {http://doi.acm.org/10.1145/1254882.1254916},
 acmid = {1254916},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bloom filter, hashing},
} 

@inproceedings{Bairavasundaram:2007:ALS:1254882.1254917,
 author = {Bairavasundaram, Lakshmi N. and Goodson, Garth R. and Pasupathy, Shankar and Schindler, Jiri},
 title = {An analysis of latent sector errors in disk drives},
 abstract = {The reliability measures in today's disk drive-based storage systems focus predominantly on protecting against complete disk failures. Previous disk reliability studies have analyzed empirical data in an attempt to better understand and predict disk failure rates. Yet, very little is known about the incidence of latent sector errors i.e., errors that go undetected until the corresponding disk sectors are accessed. Our study analyzes data collected from production storage systems over 32 months across 1.53 million disks (both nearline and enterprise class). We analyze factors that impact latent sector errors, observe trends, and explore their implications on the design of reliability mechanisms in storage systems. To the best of our knowledge, this is the first study of such large scale our sample size is at least anorder of magnitude larger than previously published studies and the first one to focus specifically on latent sector errors and their implications on the design and reliability of storage systems.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254917},
 doi = {http://doi.acm.org/10.1145/1254882.1254917},
 acmid = {1254917},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MTTDL, disk drive reliability, latent sector errors},
} 

@article{Bairavasundaram:2007:ALS:1269899.1254917,
 author = {Bairavasundaram, Lakshmi N. and Goodson, Garth R. and Pasupathy, Shankar and Schindler, Jiri},
 title = {An analysis of latent sector errors in disk drives},
 abstract = {The reliability measures in today's disk drive-based storage systems focus predominantly on protecting against complete disk failures. Previous disk reliability studies have analyzed empirical data in an attempt to better understand and predict disk failure rates. Yet, very little is known about the incidence of latent sector errors i.e., errors that go undetected until the corresponding disk sectors are accessed. Our study analyzes data collected from production storage systems over 32 months across 1.53 million disks (both nearline and enterprise class). We analyze factors that impact latent sector errors, observe trends, and explore their implications on the design of reliability mechanisms in storage systems. To the best of our knowledge, this is the first study of such large scale our sample size is at least anorder of magnitude larger than previously published studies and the first one to focus specifically on latent sector errors and their implications on the design and reliability of storage systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254917},
 doi = {http://doi.acm.org/10.1145/1269899.1254917},
 acmid = {1254917},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MTTDL, disk drive reliability, latent sector errors},
} 

@inproceedings{Legout:2007:CSI:1254882.1254919,
 author = {Legout, Arnaud and Liogkas, Nikitas and Kohler, Eddie and Zhang, Lixia},
 title = {Clustering and sharing incentives in BitTorrent systems},
 abstract = {Peer-to-peer protocols play an increasingly instrumental role in Internet content distribution. It is therefore important to gain a complete understanding of how these protocols behave in practice and how their operating parameters affect overall system performance. This paper presents the first detailed experimental investigation of the peer selection strategy in the popular BitTorrent protocol. By observing more than 40 nodes in instrumented private torrents, we validate three protocol properties that, though believed to hold, have not been previously demonstrated experimentally: the clustering of similar-bandwidth peers, the effectiveness of BitTorrent's sharing incentives, and the peers' high uplink utilization. In addition, we observe that BitTorrent's modified choking algorithmin seed state provides uniform service to all peers, and that an underprovisioned initial seed leads to absence of peer clustering and less effective sharing incentives. Based on our results, we provide guidelines for seed provisioning by content providers, and discuss a tracker protocol extension that addresses an identified limitation of the protocol.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254919},
 doi = {http://doi.acm.org/10.1145/1254882.1254919},
 acmid = {1254919},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BitTorrent, choking algorithm, clustering, incentives, seed provisioning},
} 

@article{Legout:2007:CSI:1269899.1254919,
 author = {Legout, Arnaud and Liogkas, Nikitas and Kohler, Eddie and Zhang, Lixia},
 title = {Clustering and sharing incentives in BitTorrent systems},
 abstract = {Peer-to-peer protocols play an increasingly instrumental role in Internet content distribution. It is therefore important to gain a complete understanding of how these protocols behave in practice and how their operating parameters affect overall system performance. This paper presents the first detailed experimental investigation of the peer selection strategy in the popular BitTorrent protocol. By observing more than 40 nodes in instrumented private torrents, we validate three protocol properties that, though believed to hold, have not been previously demonstrated experimentally: the clustering of similar-bandwidth peers, the effectiveness of BitTorrent's sharing incentives, and the peers' high uplink utilization. In addition, we observe that BitTorrent's modified choking algorithmin seed state provides uniform service to all peers, and that an underprovisioned initial seed leads to absence of peer clustering and less effective sharing incentives. Based on our results, we provide guidelines for seed provisioning by content providers, and discuss a tracker protocol extension that addresses an identified limitation of the protocol.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254919},
 doi = {http://doi.acm.org/10.1145/1269899.1254919},
 acmid = {1254919},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BitTorrent, choking algorithm, clustering, incentives, seed provisioning},
} 

@article{Sanghavi:2007:DLS:1269899.1254920,
 author = {Sanghavi, Sujay and Bui, Loc and Srikant, R.},
 title = {Distributed link scheduling with constant overhead},
 abstract = {This paper proposes a new class of simple, distributed algorithms for scheduling in wireless networks. The algorithms generate new schedules in a distributed manner via simple local changes to existing schedules. The class is parameterized by integers k</i>\geq 1. We show that algorithm k</i> of our class achieves k</i>/(k</i>+2) of the capacity region, for every k</i>\geq 1. . The algorithms have small and constant worst-case overheads: in particular, algorithm k</i> generates a new schedule using (a) time less than 4k</i>+2 round-trip times between neighboring nodes in the network, and (b) at most three control transmissions by any given node, for any k</i>. The control signals are explicitly specified, and face the same interference effects as normal data transmissions. Our class of distributed wireless scheduling algorithms are the first ones guaranteed to achieve any fixed fraction of the capacity region while using small and constant overheads that do not scale with network size. The parameter k</i> explicitly captures the tradeoff between control overhead and scheduler throughput performance and provides a tuning knob protocol designers can use to harness this trade-off in practice.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {313--324},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254920},
 doi = {http://doi.acm.org/10.1145/1269899.1254920},
 acmid = {1254920},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {matchings, primary interference, scheduling, wireless networks},
} 

@inproceedings{Sanghavi:2007:DLS:1254882.1254920,
 author = {Sanghavi, Sujay and Bui, Loc and Srikant, R.},
 title = {Distributed link scheduling with constant overhead},
 abstract = {This paper proposes a new class of simple, distributed algorithms for scheduling in wireless networks. The algorithms generate new schedules in a distributed manner via simple local changes to existing schedules. The class is parameterized by integers k</i>\geq 1. We show that algorithm k</i> of our class achieves k</i>/(k</i>+2) of the capacity region, for every k</i>\geq 1. . The algorithms have small and constant worst-case overheads: in particular, algorithm k</i> generates a new schedule using (a) time less than 4k</i>+2 round-trip times between neighboring nodes in the network, and (b) at most three control transmissions by any given node, for any k</i>. The control signals are explicitly specified, and face the same interference effects as normal data transmissions. Our class of distributed wireless scheduling algorithms are the first ones guaranteed to achieve any fixed fraction of the capacity region while using small and constant overheads that do not scale with network size. The parameter k</i> explicitly captures the tradeoff between control overhead and scheduler throughput performance and provides a tuning knob protocol designers can use to harness this trade-off in practice.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {313--324},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254920},
 doi = {http://doi.acm.org/10.1145/1254882.1254920},
 acmid = {1254920},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {matchings, primary interference, scheduling, wireless networks},
} 

@inproceedings{Rajendran:2007:TBC:1254882.1254921,
 author = {Rajendran, Raj Kumar and Misra, Vishal and Rubenstein, Dan},
 title = {Theoretical bounds on control-plane self-monitoring in routing protocols},
 abstract = {The distributed routing protocols in use today promise to operate correctly only if all nodes implement the protocol faithfully. A small insignificant set of nodes have, in the past, brought an entire network to a standstill by reporting incorrect route information. The damage caused by these erroneous reports, in some instances, could have been contained since incorrect route reports sometimes reveal themselves as inconsistencies in the state-information of correctly functioning nodes. By checking for such inconsitencies and taking preventive action, such as disregarding selected route-reports, a correctly functioning node could have limited the damage caused by the malfunctioning nodes. Our theoretical study attempts to understand when a correctly functioning node can, by analysing its routing-state, detect that some node is misimplementing route selection. We present a methodology, called Strong-Detection that helps answer the question. We then apply Strong-Detection to three classes of routing protocols: distance-vector, path-vector, and link-state. For each class, we derive low-complexity self-monitoring algorithms that take as input the routing state and output whether any detectable anomalies exist. We then use these algorithms to compare and contrast the self-monitoring power ofthese different classes of protocols in relation to the complexity of the irrouting-state.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {325--336},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254921},
 doi = {http://doi.acm.org/10.1145/1254882.1254921},
 acmid = {1254921},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distance vector, misconfiguration, rogue node, routing protocols},
} 

@article{Rajendran:2007:TBC:1269899.1254921,
 author = {Rajendran, Raj Kumar and Misra, Vishal and Rubenstein, Dan},
 title = {Theoretical bounds on control-plane self-monitoring in routing protocols},
 abstract = {The distributed routing protocols in use today promise to operate correctly only if all nodes implement the protocol faithfully. A small insignificant set of nodes have, in the past, brought an entire network to a standstill by reporting incorrect route information. The damage caused by these erroneous reports, in some instances, could have been contained since incorrect route reports sometimes reveal themselves as inconsistencies in the state-information of correctly functioning nodes. By checking for such inconsitencies and taking preventive action, such as disregarding selected route-reports, a correctly functioning node could have limited the damage caused by the malfunctioning nodes. Our theoretical study attempts to understand when a correctly functioning node can, by analysing its routing-state, detect that some node is misimplementing route selection. We present a methodology, called Strong-Detection that helps answer the question. We then apply Strong-Detection to three classes of routing protocols: distance-vector, path-vector, and link-state. For each class, we derive low-complexity self-monitoring algorithms that take as input the routing state and output whether any detectable anomalies exist. We then use these algorithms to compare and contrast the self-monitoring power ofthese different classes of protocols in relation to the complexity of the irrouting-state.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {325--336},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254921},
 doi = {http://doi.acm.org/10.1145/1269899.1254921},
 acmid = {1254921},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distance vector, misconfiguration, rogue node, routing protocols},
} 

@article{Yuan:2007:ORF:1269899.1254922,
 author = {Yuan, Xin and Nienaber, Wickus and Duan, Zhenhai and Melhem, Rami},
 title = {Oblivious routing for fat-tree based system area networks with uncertain traffic demands},
 abstract = {Fat-tree based system area networks have been widely adopted in high performance computing clusters. In such systems, the routing is often deterministic and the traffic demand is usually uncertain and changing. In this paper, we study routing performance on fat-tree based system area networks with deterministic routing under the assumption that the traffic demand is uncertain. The performance of a routing algorithm under uncertain traffic demands is characterized by the oblivious performance</i> ratio that bounds the relative performance of the routing algorithm and the optimal routing algorithm for any given traffic demand. We consider both single path routing where the traffic between each source-destination pair follows one path, and multi-path routing where multiple paths can be used for the traffic between a source-destination pair. We derive lower bounds of the oblivious performance ratio of any single path routing scheme for fat-tree topologies and develop single path oblivious routing schemes that achieve the optimal oblivious performance ratio for commonly used fat-tree topologies. These oblivious routing schemes provide the best performance guarantees among all single path routing algorithms under uncertain traffic demands. For multi-path routing, we show that it is possible to obtain a scheme that is optimal for any traffic demand (an oblivious performance ratio of 1) on the fat-tree topology. These results quantitatively demonstrate that single path routing cannot guarantee high routing performance while multi-path routing is very effective in balancing network loads on the fat-tree topology.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1269899.1254922},
 doi = {http://doi.acm.org/10.1145/1269899.1254922},
 acmid = {1254922},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fat-tree, oblivious routing, system area networks},
} 

@inproceedings{Yuan:2007:ORF:1254882.1254922,
 author = {Yuan, Xin and Nienaber, Wickus and Duan, Zhenhai and Melhem, Rami},
 title = {Oblivious routing for fat-tree based system area networks with uncertain traffic demands},
 abstract = {Fat-tree based system area networks have been widely adopted in high performance computing clusters. In such systems, the routing is often deterministic and the traffic demand is usually uncertain and changing. In this paper, we study routing performance on fat-tree based system area networks with deterministic routing under the assumption that the traffic demand is uncertain. The performance of a routing algorithm under uncertain traffic demands is characterized by the oblivious performance</i> ratio that bounds the relative performance of the routing algorithm and the optimal routing algorithm for any given traffic demand. We consider both single path routing where the traffic between each source-destination pair follows one path, and multi-path routing where multiple paths can be used for the traffic between a source-destination pair. We derive lower bounds of the oblivious performance ratio of any single path routing scheme for fat-tree topologies and develop single path oblivious routing schemes that achieve the optimal oblivious performance ratio for commonly used fat-tree topologies. These oblivious routing schemes provide the best performance guarantees among all single path routing algorithms under uncertain traffic demands. For multi-path routing, we show that it is possible to obtain a scheme that is optimal for any traffic demand (an oblivious performance ratio of 1) on the fat-tree topology. These results quantitatively demonstrate that single path routing cannot guarantee high routing performance while multi-path routing is very effective in balancing network loads on the fat-tree topology.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1254882.1254922},
 doi = {http://doi.acm.org/10.1145/1254882.1254922},
 acmid = {1254922},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fat-tree, oblivious routing, system area networks},
} 

@article{Nahum:2007:ESS:1269899.1254924,
 author = {Nahum, Erich M. and Tracey, John and Wright, Charles P.},
 title = {Evaluating SIP server performance},
 abstract = {SIP is a protocol of growing importance, with uses for VoIP, instant messaging, presence, and more. However, its performance is not well-studied or understood. In this extended abstract we overview our experimental evaluation of common SIP server scenarios using open-source SIP software such as OpenSER and SIP prunning on Linux. We show performance varies greatly depending on the server scenario and how the protocol is used. Depending on the configuration, through put can vary from hundreds to thousands of operations per second. For example, we observe that the choice of stateless vs. stateful proxying, using TCP rather than UDP, or including MD5-based authentication can each can affect performance by a factor of 2-4. We also provide kernel and application profiles using Oprofile that help explain and illustrate processing costs. Finally, we provide a simple fix for transaction-stateful proxying that improves performance by a factor of 10. Full details can be found in our accompanying technical report.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {349--350},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254924},
 doi = {http://doi.acm.org/10.1145/1269899.1254924},
 acmid = {1254924},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIP, experimental evaluation, performance, server},
} 

@inproceedings{Nahum:2007:ESS:1254882.1254924,
 author = {Nahum, Erich M. and Tracey, John and Wright, Charles P.},
 title = {Evaluating SIP server performance},
 abstract = {SIP is a protocol of growing importance, with uses for VoIP, instant messaging, presence, and more. However, its performance is not well-studied or understood. In this extended abstract we overview our experimental evaluation of common SIP server scenarios using open-source SIP software such as OpenSER and SIP prunning on Linux. We show performance varies greatly depending on the server scenario and how the protocol is used. Depending on the configuration, through put can vary from hundreds to thousands of operations per second. For example, we observe that the choice of stateless vs. stateful proxying, using TCP rather than UDP, or including MD5-based authentication can each can affect performance by a factor of 2-4. We also provide kernel and application profiles using Oprofile that help explain and illustrate processing costs. Finally, we provide a simple fix for transaction-stateful proxying that improves performance by a factor of 10. Full details can be found in our accompanying technical report.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {349--350},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254924},
 doi = {http://doi.acm.org/10.1145/1254882.1254924},
 acmid = {1254924},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIP, experimental evaluation, performance, server},
} 

@inproceedings{Puzak:2007:PS:1254882.1254925,
 author = {Puzak, Thomas R. and Hartstein, Allan and Srinivasan, Viji and Emma, Philip and Nadas, Arthur},
 title = {Pipeline spectroscopy},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {351--352},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254925},
 doi = {http://doi.acm.org/10.1145/1254882.1254925},
 acmid = {1254925},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, convex combination, cost of a miss, probability transition matrix},
} 

@article{Puzak:2007:PS:1269899.1254925,
 author = {Puzak, Thomas R. and Hartstein, Allan and Srinivasan, Viji and Emma, Philip and Nadas, Arthur},
 title = {Pipeline spectroscopy},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {351--352},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254925},
 doi = {http://doi.acm.org/10.1145/1269899.1254925},
 acmid = {1254925},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, convex combination, cost of a miss, probability transition matrix},
} 

@article{Cohen:2007:BSB:1269899.1254926,
 author = {Cohen, Edith and Kaplan, Haim},
 title = {Bottom-k sketches: better and more efficient estimation of aggregates},
 abstract = {A Bottom-k sketch</i> is a summary of a set of items with nonnegative weights. Each such summary allows us to compute approximate aggregates over the set of items. Bottom-k</i> sketches are obtained by associating with each item in a ground set an independent random rank drawn from a probability distribution that depends on the weight of the item. For each subset of interest, the bottom-k</i> sketch is the set of the k</i> minimum ranked items and their ranks. Bottom-k</i> sketches have numerous applications. We develop and analyze data structures and estimators for bottom-k</i> sketches to facilitate their deployment. We develop novel estimators and algorithms that show that they are a superior alternative to other sketching methods in both efficiency of obtaining the sketches and the accuracy of the estimates derived from the sketches.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {353--354},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254926},
 doi = {http://doi.acm.org/10.1145/1269899.1254926},
 acmid = {1254926},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate query processing, bottom-k, sampling, sketches},
} 

@inproceedings{Cohen:2007:BSB:1254882.1254926,
 author = {Cohen, Edith and Kaplan, Haim},
 title = {Bottom-k sketches: better and more efficient estimation of aggregates},
 abstract = {A Bottom-k sketch</i> is a summary of a set of items with nonnegative weights. Each such summary allows us to compute approximate aggregates over the set of items. Bottom-k</i> sketches are obtained by associating with each item in a ground set an independent random rank drawn from a probability distribution that depends on the weight of the item. For each subset of interest, the bottom-k</i> sketch is the set of the k</i> minimum ranked items and their ranks. Bottom-k</i> sketches have numerous applications. We develop and analyze data structures and estimators for bottom-k</i> sketches to facilitate their deployment. We develop novel estimators and algorithms that show that they are a superior alternative to other sketching methods in both efficiency of obtaining the sketches and the accuracy of the estimates derived from the sketches.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {353--354},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254926},
 doi = {http://doi.acm.org/10.1145/1254882.1254926},
 acmid = {1254926},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate query processing, bottom-k, sampling, sketches},
} 

@article{Gu:2007:GEM:1269899.1254927,
 author = {Gu, Yu and Breslau, Lee and Duffield, Nick G. and Sen, Subhabrata},
 title = {GRE encapsulated multicast probing: a scalable technique for measuring one-way loss},
 abstract = {We develop techniques for estimating one-way loss from a measurement host to network routers which exploit commonly implemented features on commercial routers and do not require any new router capabilities. The work addressesthe problem of scalably performing one-way loss measurements across specific network paths.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {355--356},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254927},
 doi = {http://doi.acm.org/10.1145/1269899.1254927},
 acmid = {1254927},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {measurement, monitoring, multicast, one-way loss, performance},
} 

@inproceedings{Gu:2007:GEM:1254882.1254927,
 author = {Gu, Yu and Breslau, Lee and Duffield, Nick G. and Sen, Subhabrata},
 title = {GRE encapsulated multicast probing: a scalable technique for measuring one-way loss},
 abstract = {We develop techniques for estimating one-way loss from a measurement host to network routers which exploit commonly implemented features on commercial routers and do not require any new router capabilities. The work addressesthe problem of scalably performing one-way loss measurements across specific network paths.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {355--356},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254927},
 doi = {http://doi.acm.org/10.1145/1254882.1254927},
 acmid = {1254927},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {measurement, monitoring, multicast, one-way loss, performance},
} 

@inproceedings{Mirkovic:2007:SRD:1254882.1254928,
 author = {Mirkovic, Jelena and Hussain, Alefiya and Willson, Brett and Fahmy, Sonia and Yao, Wei-Min and Reiher, Peter and Schwab, Stephen and Thomas, Roshan},
 title = {When is service really denied?: a user-centric dos metric},
 abstract = {Denial-of-service (DoS) research community lacks accurate metrics to evaluate an attack's impact on network services, its severity and the effectiveness of a potential defense. We propose several DoS impact metrics that measure the quality of service experienced by end users during an attack, and compare these measurements to application-specific thresholds. Our metrics are ideal for testbed experimentation, since necessary traffic parameters are extracted from packet traces gathered during an experiment.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {357--358},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254928},
 doi = {http://doi.acm.org/10.1145/1254882.1254928},
 acmid = {1254928},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {denial of service, measurement, metrics},
} 

@article{Mirkovic:2007:SRD:1269899.1254928,
 author = {Mirkovic, Jelena and Hussain, Alefiya and Willson, Brett and Fahmy, Sonia and Yao, Wei-Min and Reiher, Peter and Schwab, Stephen and Thomas, Roshan},
 title = {When is service really denied?: a user-centric dos metric},
 abstract = {Denial-of-service (DoS) research community lacks accurate metrics to evaluate an attack's impact on network services, its severity and the effectiveness of a potential defense. We propose several DoS impact metrics that measure the quality of service experienced by end users during an attack, and compare these measurements to application-specific thresholds. Our metrics are ideal for testbed experimentation, since necessary traffic parameters are extracted from packet traces gathered during an experiment.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {357--358},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254928},
 doi = {http://doi.acm.org/10.1145/1269899.1254928},
 acmid = {1254928},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {denial of service, measurement, metrics},
} 

@article{Guo:2007:IMT:1269899.1254929,
 author = {Guo, Lei and Tan, Enhua and Chen, Songqing and Xiao, Zhen and Zhang, Xiaodong},
 title = {Does internet media traffic really follow Zipf-like distribution?},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {359--360},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254929},
 doi = {http://doi.acm.org/10.1145/1269899.1254929},
 acmid = {1254929},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Zipf-like, media, stretched exponential},
} 

@inproceedings{Guo:2007:IMT:1254882.1254929,
 author = {Guo, Lei and Tan, Enhua and Chen, Songqing and Xiao, Zhen and Zhang, Xiaodong},
 title = {Does internet media traffic really follow Zipf-like distribution?},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {359--360},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254929},
 doi = {http://doi.acm.org/10.1145/1254882.1254929},
 acmid = {1254929},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Zipf-like, media, stretched exponential},
} 

@inproceedings{Kejariwal:2007:CCS:1254882.1254930,
 author = {Kejariwal, Arun and Hoflehner, Gerolf F. and Desai, Darshan and Lavery, Daniel M. and Nicolau, Alexandru and Veidenbaum, Alexander V.},
 title = {Comparative characterization of SPEC CPU2000 and CPU2006 on Itanium\&\#174; architecture},
 abstract = {Recently SPEC1 released the next generation of its CPU benchmark, widely used by compiler writers and architects for measuring processor performance. This calls for characterization of the applications in SPEC CPU2006 to guide the design of future microprocessors. In addition, it necessitates assessing the change in the characteristics of the applications from one suite to another. Although similar studies using the retired SPEC CPU benchmark suites have been done in the past, to the best of our knowledge, a thorough characterization of CPU2006 and its comparison with CPU2000 has not been done so far. In this paper, we present the above; specifically, we analyze IPC (instructions per cycle), L1, L2 data cache misses and branch prediction, especially in CPU2006.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {361--362},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254930},
 doi = {http://doi.acm.org/10.1145/1254882.1254930},
 acmid = {1254930},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SPEC CPU benchmarks, branch prediction, caches, performance evaluation},
} 

@article{Kejariwal:2007:CCS:1269899.1254930,
 author = {Kejariwal, Arun and Hoflehner, Gerolf F. and Desai, Darshan and Lavery, Daniel M. and Nicolau, Alexandru and Veidenbaum, Alexander V.},
 title = {Comparative characterization of SPEC CPU2000 and CPU2006 on Itanium\&\#174; architecture},
 abstract = {Recently SPEC1 released the next generation of its CPU benchmark, widely used by compiler writers and architects for measuring processor performance. This calls for characterization of the applications in SPEC CPU2006 to guide the design of future microprocessors. In addition, it necessitates assessing the change in the characteristics of the applications from one suite to another. Although similar studies using the retired SPEC CPU benchmark suites have been done in the past, to the best of our knowledge, a thorough characterization of CPU2006 and its comparison with CPU2000 has not been done so far. In this paper, we present the above; specifically, we analyze IPC (instructions per cycle), L1, L2 data cache misses and branch prediction, especially in CPU2006.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {361--362},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254930},
 doi = {http://doi.acm.org/10.1145/1269899.1254930},
 acmid = {1254930},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SPEC CPU benchmarks, branch prediction, caches, performance evaluation},
} 

@inproceedings{Lin:2007:PRT:1254882.1254931,
 author = {Lin, Bin and Mallik, Arindam and Dinda, Peter A. and Memik, Gokhan and Dick, Robert P.},
 title = {Power reduction through measurement and modeling of users and CPUs: summary},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {363--364},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254931},
 doi = {http://doi.acm.org/10.1145/1254882.1254931},
 acmid = {1254931},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling (DVFS), process-driven voltage scaling (PDVS), user-driven frequency scaling (UDFS)},
} 

@article{Lin:2007:PRT:1269899.1254931,
 author = {Lin, Bin and Mallik, Arindam and Dinda, Peter A. and Memik, Gokhan and Dick, Robert P.},
 title = {Power reduction through measurement and modeling of users and CPUs: summary},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {363--364},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254931},
 doi = {http://doi.acm.org/10.1145/1269899.1254931},
 acmid = {1254931},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling (DVFS), process-driven voltage scaling (PDVS), user-driven frequency scaling (UDFS)},
} 

@article{Wang:2007:GRI:1269899.1254932,
 author = {Wang, Chong and Byers, John W.},
 title = {Generating representative ISP topologies from first-principles},
 abstract = {Understanding and modeling the factors that underlie the growth and evolution of network topologies are basic questions that impinge upon capacity planning, forecasting, and protocol research. Early topology generation work focused on generating network-wide connectivity maps, either at the AS-level or the router-level, typically with an eye towards reproducing abstract properties of observed topologies. But recently, advocates of an alternative "first-principles" approach question the feasibility of realizing representative topologies with simple generative models that do not explicitly incorporate real-world constraints, such as the relative costs of router configurations, into the model. Our work synthesizes these two lines by designing a topology generation mechanism that incorporates first-principles constraints. Our goal is more modest than that of constructing an Internet-wide topology: we aim to generate representative topologies for single ISPs. However, our methods also go well beyond previous work, as we annotate these topologies with representative capacity and latency information. Taking only demand for network services over a given region as input, we propose a natural cost model for building and interconnecting PoPs and formulate the resulting optimization problem faced by an ISP. We devise hill-climbing heuristics for this problem and demonstrate that the solutions we obtain are quantitatively similar to those in measured router-level ISP topologies, with respect to both topological properties and fault-tolerance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254932},
 doi = {http://doi.acm.org/10.1145/1269899.1254932},
 acmid = {1254932},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network design, network topology modeling, optimization},
} 

@inproceedings{Wang:2007:GRI:1254882.1254932,
 author = {Wang, Chong and Byers, John W.},
 title = {Generating representative ISP topologies from first-principles},
 abstract = {Understanding and modeling the factors that underlie the growth and evolution of network topologies are basic questions that impinge upon capacity planning, forecasting, and protocol research. Early topology generation work focused on generating network-wide connectivity maps, either at the AS-level or the router-level, typically with an eye towards reproducing abstract properties of observed topologies. But recently, advocates of an alternative "first-principles" approach question the feasibility of realizing representative topologies with simple generative models that do not explicitly incorporate real-world constraints, such as the relative costs of router configurations, into the model. Our work synthesizes these two lines by designing a topology generation mechanism that incorporates first-principles constraints. Our goal is more modest than that of constructing an Internet-wide topology: we aim to generate representative topologies for single ISPs. However, our methods also go well beyond previous work, as we annotate these topologies with representative capacity and latency information. Taking only demand for network services over a given region as input, we propose a natural cost model for building and interconnecting PoPs and formulate the resulting optimization problem faced by an ISP. We devise hill-climbing heuristics for this problem and demonstrate that the solutions we obtain are quantitatively similar to those in measured router-level ISP topologies, with respect to both topological properties and fault-tolerance.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254932},
 doi = {http://doi.acm.org/10.1145/1254882.1254932},
 acmid = {1254932},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network design, network topology modeling, optimization},
} 

@article{Bissias:2007:BDL:1269899.1254933,
 author = {Bissias, George Dean and Levine, Brian Neil and Rosenberg, Arnold},
 title = {Bounding damage from link destruction, with application to the internet},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {367--368},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254933},
 doi = {http://doi.acm.org/10.1145/1269899.1254933},
 acmid = {1254933},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph partitioning, spectral graph theory, vulnerability},
} 

@inproceedings{Bissias:2007:BDL:1254882.1254933,
 author = {Bissias, George Dean and Levine, Brian Neil and Rosenberg, Arnold},
 title = {Bounding damage from link destruction, with application to the internet},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {367--368},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254933},
 doi = {http://doi.acm.org/10.1145/1254882.1254933},
 acmid = {1254933},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph partitioning, spectral graph theory, vulnerability},
} 

@article{Erman:2007:SNT:1269899.1254934,
 author = {Erman, Jeffrey and Mahanti, Anirban and Arlitt, Martin and Cohen, Ira and Williamson, Carey},
 title = {Semi-supervised network traffic classification},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {369--370},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254934},
 doi = {http://doi.acm.org/10.1145/1269899.1254934},
 acmid = {1254934},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {semi-supervised learning, traffic classification},
} 

@inproceedings{Erman:2007:SNT:1254882.1254934,
 author = {Erman, Jeffrey and Mahanti, Anirban and Arlitt, Martin and Cohen, Ira and Williamson, Carey},
 title = {Semi-supervised network traffic classification},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {369--370},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254934},
 doi = {http://doi.acm.org/10.1145/1254882.1254934},
 acmid = {1254934},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {semi-supervised learning, traffic classification},
} 

@article{Mi:2007:EMI:1269899.1254935,
 author = {Mi, Ningfang and Riska, Alma and Zhang, Qi and Smirni, Evgenia and Riedel, Erik},
 title = {Efficient management of idleness in systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {371--372},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254935},
 doi = {http://doi.acm.org/10.1145/1269899.1254935},
 acmid = {1254935},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {foreground/background scheduling, storage systems},
} 

@inproceedings{Mi:2007:EMI:1254882.1254935,
 author = {Mi, Ningfang and Riska, Alma and Zhang, Qi and Smirni, Evgenia and Riedel, Erik},
 title = {Efficient management of idleness in systems},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {371--372},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254935},
 doi = {http://doi.acm.org/10.1145/1254882.1254935},
 acmid = {1254935},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {foreground/background scheduling, storage systems},
} 

@article{de Jager:2007:AIS:1269899.1254936,
 author = {de Jager, Douglas V. and Bradley, Jeremy T.},
 title = {Asynchronous iterative solution for state-based performance metrics},
 abstract = {Solution of large sparse fixed-point problems, Mline over x = over x and Mline over x + line over b = over x, may be seen as underpinning many important performance-analysis calculations. These calculations include steady-state, passage-time and transient-time calculations in discrete-time Markov chains, continuous-time Markov chains and semi-Markov chains. In recent years, much work has been done to extend the application of asynchronous iterative fixed-point solution methods to many different contexts. This work has been motivated by the potential for faster solution, more efficient use of the communication channel and/or access to memory, and simplification of task management and programming. In this paper, we present theoretical developments which allow us to extend theapplication of asynchronous iterative solution methods to solve for the key performance metrics mentioned above-such that we may employ the full breadth of Chazan and Miranker's classes of asynchronous iterations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254936},
 doi = {http://doi.acm.org/10.1145/1269899.1254936},
 acmid = {1254936},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Perron-Frobenius, asynchonous iterations, dominant eigenvectors, matrix-vector splitting, performance analysis},
} 

@inproceedings{de Jager:2007:AIS:1254882.1254936,
 author = {de Jager, Douglas V. and Bradley, Jeremy T.},
 title = {Asynchronous iterative solution for state-based performance metrics},
 abstract = {Solution of large sparse fixed-point problems, Mline over x = over x and Mline over x + line over b = over x, may be seen as underpinning many important performance-analysis calculations. These calculations include steady-state, passage-time and transient-time calculations in discrete-time Markov chains, continuous-time Markov chains and semi-Markov chains. In recent years, much work has been done to extend the application of asynchronous iterative fixed-point solution methods to many different contexts. This work has been motivated by the potential for faster solution, more efficient use of the communication channel and/or access to memory, and simplification of task management and programming. In this paper, we present theoretical developments which allow us to extend theapplication of asynchronous iterative solution methods to solve for the key performance metrics mentioned above-such that we may employ the full breadth of Chazan and Miranker's classes of asynchronous iterations.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254936},
 doi = {http://doi.acm.org/10.1145/1254882.1254936},
 acmid = {1254936},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Perron-Frobenius, asynchonous iterations, dominant eigenvectors, matrix-vector splitting, performance analysis},
} 

@inproceedings{Hoste:2007:ACP:1254882.1254937,
 author = {Hoste, Kenneth and Eeckhout, Lieven and Blockeel, Hendrik},
 title = {Analyzing commercial processor performance numbers for predicting performance of applications of interest},
 abstract = {Current practice in benchmarking commercial computer systems is to run a number of industry-standard benchmarks and to report performance numbers. The huge amount of machines and the large number of benchmarks for which performance numbers are published make it hard to observe clear performance trends though. In addition, these performance numbers for specific benchmarks do not provide insight into how applications of interest that are not part of the benchmark suite would perform on those machines. In this work we build a methodology for analyzing published commercial machine performance data sets. We apply statistical data analysis techniques, more in particular principal components analysis and cluster analysis, to reduce the amount of information to a manageable amount to facilitate its understanding. Visualizing SPEC CPU2000 performance numbers for 26 benchmarks and 1000+ machines in just a few graphs gives insight into how commercial machines compare against each other.In this work we build a methodology for analyzing published commercial machine performance data sets. We apply statistical data analysis techniques, more in particular principal components analysis and cluster analysis, to reduce the amount of information to a manageable amount to facilitate its understanding. Visualizing SPEC CPU2000 performance numbers for 26 benchmarks and 1000+ machines in just a few graphs gives insight into how commercial machines compare against each other. In addition, we provide a way of relating inherent program behavior to these performance numbers so that insights can be gained into how the observed performance trends relate to the behavioral characteristics of computer programs. This results in a methodology for the ubiquitous benchmarking problem of predicting performance of an application of interest based on its similarities with the benchmarks in a published industry-standard benchmark suite.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {375--376},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254937},
 doi = {http://doi.acm.org/10.1145/1254882.1254937},
 acmid = {1254937},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark similarity, performance analysis, performance prediction},
} 

@article{Hoste:2007:ACP:1269899.1254937,
 author = {Hoste, Kenneth and Eeckhout, Lieven and Blockeel, Hendrik},
 title = {Analyzing commercial processor performance numbers for predicting performance of applications of interest},
 abstract = {Current practice in benchmarking commercial computer systems is to run a number of industry-standard benchmarks and to report performance numbers. The huge amount of machines and the large number of benchmarks for which performance numbers are published make it hard to observe clear performance trends though. In addition, these performance numbers for specific benchmarks do not provide insight into how applications of interest that are not part of the benchmark suite would perform on those machines. In this work we build a methodology for analyzing published commercial machine performance data sets. We apply statistical data analysis techniques, more in particular principal components analysis and cluster analysis, to reduce the amount of information to a manageable amount to facilitate its understanding. Visualizing SPEC CPU2000 performance numbers for 26 benchmarks and 1000+ machines in just a few graphs gives insight into how commercial machines compare against each other.In this work we build a methodology for analyzing published commercial machine performance data sets. We apply statistical data analysis techniques, more in particular principal components analysis and cluster analysis, to reduce the amount of information to a manageable amount to facilitate its understanding. Visualizing SPEC CPU2000 performance numbers for 26 benchmarks and 1000+ machines in just a few graphs gives insight into how commercial machines compare against each other. In addition, we provide a way of relating inherent program behavior to these performance numbers so that insights can be gained into how the observed performance trends relate to the behavioral characteristics of computer programs. This results in a methodology for the ubiquitous benchmarking problem of predicting performance of an application of interest based on its similarities with the benchmarks in a published industry-standard benchmark suite.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {375--376},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254937},
 doi = {http://doi.acm.org/10.1145/1269899.1254937},
 acmid = {1254937},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark similarity, performance analysis, performance prediction},
} 

@inproceedings{He:2007:BSS:1254882.1254938,
 author = {He, Jiayue and Chaintreau, Augustin},
 title = {BRADO: scalable streaming through reconfigurable trees},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {377--378},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254938},
 doi = {http://doi.acm.org/10.1145/1254882.1254938},
 acmid = {1254938},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP tandem, application layer multicast, network overlays},
} 

@article{He:2007:BSS:1269899.1254938,
 author = {He, Jiayue and Chaintreau, Augustin},
 title = {BRADO: scalable streaming through reconfigurable trees},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {377--378},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254938},
 doi = {http://doi.acm.org/10.1145/1269899.1254938},
 acmid = {1254938},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP tandem, application layer multicast, network overlays},
} 

@inproceedings{Nurmi:2007:QQB:1254882.1254939,
 author = {Nurmi, Daniel Charles and Brevik, John and Wolski, Rich},
 title = {QBETS: queue bounds estimation from time series},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {379--380},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254939},
 doi = {http://doi.acm.org/10.1145/1254882.1254939},
 acmid = {1254939},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {batch scheduling, queue prediction, super-computing},
} 

@article{Nurmi:2007:QQB:1269899.1254939,
 author = {Nurmi, Daniel Charles and Brevik, John and Wolski, Rich},
 title = {QBETS: queue bounds estimation from time series},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {379--380},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254939},
 doi = {http://doi.acm.org/10.1145/1269899.1254939},
 acmid = {1254939},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {batch scheduling, queue prediction, super-computing},
} 

@article{Deng:2007:PDS:1269899.1254940,
 author = {Deng, Leiwen and Kuzmanovic, Aleksandar},
 title = {Pong: diagnosing spatio-temporal internet congestion properties},
 abstract = {The ability to accurately detect congestion events in the Internet and reveal their spatial (i.e.</i>, where they happen?) and temporal (i.e.</i>, how frequently they occur and how long they last?) properties would significantly improve our understanding of how the Internet operates. In this paper we present Pong</i>, a novel measurement tool capable of effectively diagnosing congestion events over short (e.g.</i>, ~100ms or longer) time-scales, and simultaneously locating congested points within a single hop on an end-to-end path at the granularity of a single link. Pong</i> (i</i>) uses queuing delay as indicative of congestion, and (ii</i>) strategically combines end-to-end probes with those targeted to intermediate nodes. Moreover, it (iii</i>) achieves high sampling frequency by sending probes to all intermediate nodes, including uncongested ones, (iv</i>) dramatically improves spatial detection granularity (i.e.</i>, from path segments to individual links), by using short-term congestion history, (v</i>) considerably enhances the measurement quality by adjusting the probing methodology (e.g.</i>, send 4-, 3-, or 2-packet probes) based on the observed path topology, and (vi</i>) deterministically detects moments of its own inaccuracy. We conduct a large-scale measurement study on over 23,000 Internet paths and present their spatial-temporal properties as inferred by Pong</i>.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {381--382},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254940},
 doi = {http://doi.acm.org/10.1145/1269899.1254940},
 acmid = {1254940},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Pong, coordinated probing},
} 

@inproceedings{Deng:2007:PDS:1254882.1254940,
 author = {Deng, Leiwen and Kuzmanovic, Aleksandar},
 title = {Pong: diagnosing spatio-temporal internet congestion properties},
 abstract = {The ability to accurately detect congestion events in the Internet and reveal their spatial (i.e.</i>, where they happen?) and temporal (i.e.</i>, how frequently they occur and how long they last?) properties would significantly improve our understanding of how the Internet operates. In this paper we present Pong</i>, a novel measurement tool capable of effectively diagnosing congestion events over short (e.g.</i>, ~100ms or longer) time-scales, and simultaneously locating congested points within a single hop on an end-to-end path at the granularity of a single link. Pong</i> (i</i>) uses queuing delay as indicative of congestion, and (ii</i>) strategically combines end-to-end probes with those targeted to intermediate nodes. Moreover, it (iii</i>) achieves high sampling frequency by sending probes to all intermediate nodes, including uncongested ones, (iv</i>) dramatically improves spatial detection granularity (i.e.</i>, from path segments to individual links), by using short-term congestion history, (v</i>) considerably enhances the measurement quality by adjusting the probing methodology (e.g.</i>, send 4-, 3-, or 2-packet probes) based on the observed path topology, and (vi</i>) deterministically detects moments of its own inaccuracy. We conduct a large-scale measurement study on over 23,000 Internet paths and present their spatial-temporal properties as inferred by Pong</i>.},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {381--382},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254940},
 doi = {http://doi.acm.org/10.1145/1254882.1254940},
 acmid = {1254940},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Pong, coordinated probing},
} 

@inproceedings{Aalto:2007:MDO:1254882.1254941,
 author = {Aalto, Samuli and Ayesta, Urtzi},
 title = {Mean delay optimization for the M/G/1 queue with pareto type service times},
 abstract = {},
 booktitle = {Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '07},
 year = {2007},
 isbn = {978-1-59593-639-4},
 location = {San Diego, California, USA},
 pages = {383--384},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1254882.1254941},
 doi = {http://doi.acm.org/10.1145/1254882.1254941},
 acmid = {1254941},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, gittins index, mean delay, pareto distribution, scheduling},
} 

@article{Aalto:2007:MDO:1269899.1254941,
 author = {Aalto, Samuli and Ayesta, Urtzi},
 title = {Mean delay optimization for the M/G/1 queue with pareto type service times},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {35},
 issue = {1},
 month = {June},
 year = {2007},
 issn = {0163-5999},
 pages = {383--384},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1269899.1254941},
 doi = {http://doi.acm.org/10.1145/1269899.1254941},
 acmid = {1254941},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, gittins index, mean delay, pareto distribution, scheduling},
} 

@inproceedings{Thereska:2010:PPM:1811039.1811041,
 author = {Thereska, Eno and Doebel, Bjoern and Zheng, Alice X. and Nobel, Peter},
 title = {Practical performance models for complex, popular applications},
 abstract = {Perhaps surprisingly, no practical performance models exist for popular (and complex) client applications such as Adobe's Creative Suite, Microsoft's Office and Visual Studio, Mozilla, Halo 3, etc. There is currently no tool that automatically answers program developers', IT administrators' and end-users' simple what-if questions like "what happens to the performance of my favorite application X if I upgrade from Windows Vista to Windows 7?". This paper describes our approach towards constructing practical, versatile performance models to address this problem. The goal is to have these models be useful for application developers to help expand application testing coverage and for IT administrators to assist with understanding the performance consequences of a software, hardware or configuration change. This paper's main contributions are in system building and performance modeling. We believe we have built applications that are easier to model because we have proactively instrumented them to export their state and associated metrics. This application-specific monitoring is always on and interesting data is collected from real, "in-the-wild" deployments. The models we are experimenting with are based on statistical techniques. They require no modifications to the OS or applications beyond the above instrumentation, and no explicit a priori model on how an OS or application should behave. We are in the process of learning from models we have constructed for several Microsoft products, including the Office suite, Visual Studio and Media Player. This paper presents preliminary findings from a large user deployment (several hundred thousand user sessions) of these applications that show the coverage and limitations of such models. These findings pushed us to move beyond averages/means and go into some depth into why client application performance has an inherently large variance.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811041},
 doi = {http://doi.acm.org/10.1145/1811039.1811041},
 acmid = {1811041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IT administrators, developers, performance variance, what-if},
} 

@article{Thereska:2010:PPM:1811099.1811041,
 author = {Thereska, Eno and Doebel, Bjoern and Zheng, Alice X. and Nobel, Peter},
 title = {Practical performance models for complex, popular applications},
 abstract = {Perhaps surprisingly, no practical performance models exist for popular (and complex) client applications such as Adobe's Creative Suite, Microsoft's Office and Visual Studio, Mozilla, Halo 3, etc. There is currently no tool that automatically answers program developers', IT administrators' and end-users' simple what-if questions like "what happens to the performance of my favorite application X if I upgrade from Windows Vista to Windows 7?". This paper describes our approach towards constructing practical, versatile performance models to address this problem. The goal is to have these models be useful for application developers to help expand application testing coverage and for IT administrators to assist with understanding the performance consequences of a software, hardware or configuration change. This paper's main contributions are in system building and performance modeling. We believe we have built applications that are easier to model because we have proactively instrumented them to export their state and associated metrics. This application-specific monitoring is always on and interesting data is collected from real, "in-the-wild" deployments. The models we are experimenting with are based on statistical techniques. They require no modifications to the OS or applications beyond the above instrumentation, and no explicit a priori model on how an OS or application should behave. We are in the process of learning from models we have constructed for several Microsoft products, including the Office suite, Visual Studio and Media Player. This paper presents preliminary findings from a large user deployment (several hundred thousand user sessions) of these applications that show the coverage and limitations of such models. These findings pushed us to move beyond averages/means and go into some depth into why client application performance has an inherently large variance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811041},
 doi = {http://doi.acm.org/10.1145/1811099.1811041},
 acmid = {1811041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IT administrators, developers, performance variance, what-if},
} 

@article{Gast:2010:MFM:1811099.1811042,
 author = {Gast, Nicolas and Bruno, Gaujal},
 title = {A mean field model of work stealing in large-scale systems},
 abstract = {In this paper, we consider a generic model of computational grids, seen as several clusters of homogeneous processors. In such systems, a key issue when designing efficient job allocation policies is to balance the workload over the different resources. We present a Markovian model for performance evaluation of such a policy, namely work stealing (idle processors steal work from others) in large-scale heterogeneous systems. Using mean field theory, we show that when the size of the system grows, it converges to a system of deterministic ordinary differential equations that allows one to compute the expectation of performance functions (such as average response times) as well as the distributions of these functions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811042},
 doi = {http://doi.acm.org/10.1145/1811099.1811042},
 acmid = {1811042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {grid computing, load balancing, mean field},
} 

@inproceedings{Gast:2010:MFM:1811039.1811042,
 author = {Gast, Nicolas and Bruno, Gaujal},
 title = {A mean field model of work stealing in large-scale systems},
 abstract = {In this paper, we consider a generic model of computational grids, seen as several clusters of homogeneous processors. In such systems, a key issue when designing efficient job allocation policies is to balance the workload over the different resources. We present a Markovian model for performance evaluation of such a policy, namely work stealing (idle processors steal work from others) in large-scale heterogeneous systems. Using mean field theory, we show that when the size of the system grows, it converges to a system of deterministic ordinary differential equations that allows one to compute the expectation of performance functions (such as average response times) as well as the distributions of these functions.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811042},
 doi = {http://doi.acm.org/10.1145/1811039.1811042},
 acmid = {1811042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {grid computing, load balancing, mean field},
} 

@article{Balsamo:2010:UAP:1811099.1811043,
 author = {Balsamo, Simonetta and Harrison, Peter G. and Marin, Andrea},
 title = {A unifying approach to product-forms in networks with finite capacity constraints},
 abstract = {In queueing networks with blocking, stations wishing to transmit customers to a full queue are blocked and need to take alternative action on completing a service. In general, product-forms, i.e. separable solutions for such a network's equilibrium state probabilities, do not exist but some product-forms have been obtained over the years in special cases, using a variety of techniques. We show that the Reversed Compound Agent Theorem (RCAT) can obtain these diverse results in a uniform way by its direct application, so unifying product-forms in networks with and without blocking. New product-forms are also constructed for a type of blocking we call `skipping', where a blocked station sends its output-customers to the queue after the one causing the blocking in that customer's path. Finally, we investigate a novel congestion management scheme for networks of finite-capacity queues in which a station with a full queue transmits signals that delete customers from upstream queues in order to reduce incoming traffic.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811043},
 doi = {http://doi.acm.org/10.1145/1811099.1811043},
 acmid = {1811043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {product-form solutions, queueing theory},
} 

@inproceedings{Balsamo:2010:UAP:1811039.1811043,
 author = {Balsamo, Simonetta and Harrison, Peter G. and Marin, Andrea},
 title = {A unifying approach to product-forms in networks with finite capacity constraints},
 abstract = {In queueing networks with blocking, stations wishing to transmit customers to a full queue are blocked and need to take alternative action on completing a service. In general, product-forms, i.e. separable solutions for such a network's equilibrium state probabilities, do not exist but some product-forms have been obtained over the years in special cases, using a variety of techniques. We show that the Reversed Compound Agent Theorem (RCAT) can obtain these diverse results in a uniform way by its direct application, so unifying product-forms in networks with and without blocking. New product-forms are also constructed for a type of blocking we call `skipping', where a blocked station sends its output-customers to the queue after the one causing the blocking in that customer's path. Finally, we investigate a novel congestion management scheme for networks of finite-capacity queues in which a station with a full queue transmits signals that delete customers from upstream queues in order to reduce incoming traffic.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811043},
 doi = {http://doi.acm.org/10.1145/1811039.1811043},
 acmid = {1811043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {product-form solutions, queueing theory},
} 

@inproceedings{Andrew:2010:OFR:1811039.1811044,
 author = {Andrew, Lachlan L.H. and Lin, Minghong and Wierman, Adam},
 title = {Optimality, fairness, and robustness in speed scaling designs},
 abstract = {This work examines fundamental tradeoffs incurred by a speed scaler seeking to minimize the sum of expected response time and energy use per job. We prove that a popular speed scaler is 2-competitive for this objective and no "natural" speed scaler can do better. Additionally, we prove that energy-proportional speed scaling works well for both Shortest Remaining Processing Time (SRPT) and Processor Sharing (PS) and we show that under both SRPT and PS, gated-static speed scaling is nearly optimal when the mean workload is known, but that dynamic speed scaling provides robustness against uncertain workloads. Finally, we prove that speed scaling magnifies unfairness under SRPT but that PS remains fair under speed scaling. These results show that these speed scalers can achieve any two, but only two, of optimality, fairness, and robustness.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811044},
 doi = {http://doi.acm.org/10.1145/1811039.1811044},
 acmid = {1811044},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {PS, SRPT, energy, fairness, robustness, scheduling, speed scaling},
} 

@article{Andrew:2010:OFR:1811099.1811044,
 author = {Andrew, Lachlan L.H. and Lin, Minghong and Wierman, Adam},
 title = {Optimality, fairness, and robustness in speed scaling designs},
 abstract = {This work examines fundamental tradeoffs incurred by a speed scaler seeking to minimize the sum of expected response time and energy use per job. We prove that a popular speed scaler is 2-competitive for this objective and no "natural" speed scaler can do better. Additionally, we prove that energy-proportional speed scaling works well for both Shortest Remaining Processing Time (SRPT) and Processor Sharing (PS) and we show that under both SRPT and PS, gated-static speed scaling is nearly optimal when the mean workload is known, but that dynamic speed scaling provides robustness against uncertain workloads. Finally, we prove that speed scaling magnifies unfairness under SRPT but that PS remains fair under speed scaling. These results show that these speed scalers can achieve any two, but only two, of optimality, fairness, and robustness.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811044},
 doi = {http://doi.acm.org/10.1145/1811099.1811044},
 acmid = {1811044},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {PS, SRPT, energy, fairness, robustness, scheduling, speed scaling},
} 

@inproceedings{Dong:2010:EEE:1811039.1811046,
 author = {Dong, Wei and Liu, Yunhao and Wu, Xiaofan and Gu, Lin and Chen, Chun},
 title = {Elon: enabling efficient and long-term reprogramming for wireless sensor networks},
 abstract = {We present a new mechanism called Elon for enabling efficient and long-term reprogramming in wireless sensor networks. Elon reduces the transferred code size significantly by introducing the concept of replaceable component. It avoids the cost of hardware reboot with a novel software reboot mechanism. Moreover, it significantly prolongs the reprogramming lifetime by avoiding flash writes for TelosB nodes. Experimental results show that Elon transfers up to 120--389 times less information than Deluge, and 18-42 times less information than Stream. The software reboot mechanism that Elon applies reduces the rebooting cost by 50.4\%-53.87\% in terms of beacon packets, and 56.83\% in terms of unsynchronized nodes. In addition, Elon prolongs the reprogramming lifetime by a factor of 2.3.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811046},
 doi = {http://doi.acm.org/10.1145/1811039.1811046},
 acmid = {1811046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {component, reboot, reprogramming, wireless sensor network},
} 

@article{Dong:2010:EEE:1811099.1811046,
 author = {Dong, Wei and Liu, Yunhao and Wu, Xiaofan and Gu, Lin and Chen, Chun},
 title = {Elon: enabling efficient and long-term reprogramming for wireless sensor networks},
 abstract = {We present a new mechanism called Elon for enabling efficient and long-term reprogramming in wireless sensor networks. Elon reduces the transferred code size significantly by introducing the concept of replaceable component. It avoids the cost of hardware reboot with a novel software reboot mechanism. Moreover, it significantly prolongs the reprogramming lifetime by avoiding flash writes for TelosB nodes. Experimental results show that Elon transfers up to 120--389 times less information than Deluge, and 18-42 times less information than Stream. The software reboot mechanism that Elon applies reduces the rebooting cost by 50.4\%-53.87\% in terms of beacon packets, and 56.83\% in terms of unsynchronized nodes. In addition, Elon prolongs the reprogramming lifetime by a factor of 2.3.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811046},
 doi = {http://doi.acm.org/10.1145/1811099.1811046},
 acmid = {1811046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {component, reboot, reprogramming, wireless sensor network},
} 

@inproceedings{Karbasi:2010:DSN:1811039.1811047,
 author = {Karbasi, Amin and Oh, Sewoong},
 title = {Distributed sensor network localization from local connectivity: performance analysis for the HOP-TERRAIN algorithm},
 abstract = {This paper addresses the problem of determining the node locations in ad-hoc sensor networks when only connectivity information is available. In previous work, we showed that the localization algorithm MDS-MAP proposed by Y. Shang et al. is able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r. The main limitation of MDS-MAP is the assumption that the available connectivity information is processed in a centralized way. In this work we investigate a practically important question whether similar performance guarantees can be obtained in a distributed setting. In particular, we analyze the performance of the HOP-TERRAIN algorithm proposed by C. Savarese et al. This algorithm can be seen as a distributed version of the MDS-MAP algorithm. More precisely, assume that the radio range r=o(1) and that the network consists of n sensors positioned randomly on a d-dimensional unit cube and d+1 anchors in general positions. We show that when only connectivity information is available, for every unknown node i, the Euclidean distance between the estimate x<sub>i</sub> and the correct position x<sub>i</sub> is bounded by ||x<sub>i</sub>-x<sub>i</sub>|| < r<sub>0</sub>/r + o(1), where r<sub>0</sub>=C<sub>d</sub> (log n/ n)<sup>(1/d)</sup> for some constant C<sub>d</sub> which only depends on d. Furthermore, we illustrate that a similar bound holds for the range-based model, when the approximate measurement for the distances is provided.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1811039.1811047},
 doi = {http://doi.acm.org/10.1145/1811039.1811047},
 acmid = {1811047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed, localization, sensor network},
} 

@article{Karbasi:2010:DSN:1811099.1811047,
 author = {Karbasi, Amin and Oh, Sewoong},
 title = {Distributed sensor network localization from local connectivity: performance analysis for the HOP-TERRAIN algorithm},
 abstract = {This paper addresses the problem of determining the node locations in ad-hoc sensor networks when only connectivity information is available. In previous work, we showed that the localization algorithm MDS-MAP proposed by Y. Shang et al. is able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r. The main limitation of MDS-MAP is the assumption that the available connectivity information is processed in a centralized way. In this work we investigate a practically important question whether similar performance guarantees can be obtained in a distributed setting. In particular, we analyze the performance of the HOP-TERRAIN algorithm proposed by C. Savarese et al. This algorithm can be seen as a distributed version of the MDS-MAP algorithm. More precisely, assume that the radio range r=o(1) and that the network consists of n sensors positioned randomly on a d-dimensional unit cube and d+1 anchors in general positions. We show that when only connectivity information is available, for every unknown node i, the Euclidean distance between the estimate x<sub>i</sub> and the correct position x<sub>i</sub> is bounded by ||x<sub>i</sub>-x<sub>i</sub>|| < r<sub>0</sub>/r + o(1), where r<sub>0</sub>=C<sub>d</sub> (log n/ n)<sup>(1/d)</sup> for some constant C<sub>d</sub> which only depends on d. Furthermore, we illustrate that a similar bound holds for the range-based model, when the approximate measurement for the distances is provided.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1811099.1811047},
 doi = {http://doi.acm.org/10.1145/1811099.1811047},
 acmid = {1811047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed, localization, sensor network},
} 

@article{Xu:2010:SPC:1811099.1811048,
 author = {Xu, Kuang and Dousse, Olivier and Thiran, Patrick},
 title = {Self-synchronizing properties of CSMA wireless multi-hop networks},
 abstract = {We show that CSMA is able to spontaneously synchronize transmissions in a wireless network with constant-size packets, and that this property can be used to devise efficient synchronized CSMA scheduling mechanisms without message passing. Using tools from queuing theory, we prove that for any connected wireless networks with arbitrary interference constraints, it is possible to implement self-synchronizing TDMA schedules without any explicit message passing or clock synchronization besides transmitting the original data packets, and the interaction can be fully local in that each node decides when to transmit next only by overhearing its neighbors' transmissions. We also provide a necessary and sufficient condition on the emergence of self-synchronization for a given TDMA schedule, and prove that such conditions for self-synchronization can be checked in a finite number of steps for a finite network topology.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {71--82},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811048},
 doi = {http://doi.acm.org/10.1145/1811099.1811048},
 acmid = {1811048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {scheduling algorithm, self-synchronization, stochastic recursive sequence},
} 

@inproceedings{Xu:2010:SPC:1811039.1811048,
 author = {Xu, Kuang and Dousse, Olivier and Thiran, Patrick},
 title = {Self-synchronizing properties of CSMA wireless multi-hop networks},
 abstract = {We show that CSMA is able to spontaneously synchronize transmissions in a wireless network with constant-size packets, and that this property can be used to devise efficient synchronized CSMA scheduling mechanisms without message passing. Using tools from queuing theory, we prove that for any connected wireless networks with arbitrary interference constraints, it is possible to implement self-synchronizing TDMA schedules without any explicit message passing or clock synchronization besides transmitting the original data packets, and the interaction can be fully local in that each node decides when to transmit next only by overhearing its neighbors' transmissions. We also provide a necessary and sufficient condition on the emergence of self-synchronization for a given TDMA schedule, and prove that such conditions for self-synchronization can be checked in a finite number of steps for a finite network topology.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {71--82},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811048},
 doi = {http://doi.acm.org/10.1145/1811039.1811048},
 acmid = {1811048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {scheduling algorithm, self-synchronization, stochastic recursive sequence},
} 

@inproceedings{Moallemi:2010:FDP:1811039.1811050,
 author = {Moallemi, Ciamac and Shah, Devavrat},
 title = {On the flow-level dynamics of a packet-switched network},
 abstract = {The packet is the fundamental unit of transportation in modern communication networks such as the Internet. Physical layer scheduling decisions are made at the level of packets, and packet-level models with exogenous arrival processes have long been employed to study network performance, as well as design scheduling policies that more efficiently utilize network resources. On the other hand, a user of the network is more concerned with end-to-end bandwidth, which is allocated through congestion control policies such as TCP. Utility-based flow-level models have played an important role in understanding congestion control protocols. In summary, these two classes of models have provided separate insights for flow-level and packet-level dynamics of a network. In this paper, we wish to study these two dynamics together. We propose a joint flow-level and packet-level stochastic model for the dynamics of a network, and an associated policy for congestion control and packet scheduling that is based on alpha-weighted policies from the literature. We provide a fluid analysis for the model that establishes the throughput optimality of the proposed policy, thus validating prior insights based on separate packet-level and flow-level models. By analyzing a critically scaled fluid model under the proposed policy, we provide constant factor performance bounds on the delay performance and characterize the invariant states of the system.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811050},
 doi = {http://doi.acm.org/10.1145/1811039.1811050},
 acmid = {1811050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion control, flow-level model, maximum weight, packet-level model, scheduling, utility maximizaiton},
} 

@article{Moallemi:2010:FDP:1811099.1811050,
 author = {Moallemi, Ciamac and Shah, Devavrat},
 title = {On the flow-level dynamics of a packet-switched network},
 abstract = {The packet is the fundamental unit of transportation in modern communication networks such as the Internet. Physical layer scheduling decisions are made at the level of packets, and packet-level models with exogenous arrival processes have long been employed to study network performance, as well as design scheduling policies that more efficiently utilize network resources. On the other hand, a user of the network is more concerned with end-to-end bandwidth, which is allocated through congestion control policies such as TCP. Utility-based flow-level models have played an important role in understanding congestion control protocols. In summary, these two classes of models have provided separate insights for flow-level and packet-level dynamics of a network. In this paper, we wish to study these two dynamics together. We propose a joint flow-level and packet-level stochastic model for the dynamics of a network, and an associated policy for congestion control and packet scheduling that is based on alpha-weighted policies from the literature. We provide a fluid analysis for the model that establishes the throughput optimality of the proposed policy, thus validating prior insights based on separate packet-level and flow-level models. By analyzing a critically scaled fluid model under the proposed policy, we provide constant factor performance bounds on the delay performance and characterize the invariant states of the system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811050},
 doi = {http://doi.acm.org/10.1145/1811099.1811050},
 acmid = {1811050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion control, flow-level model, maximum weight, packet-level model, scheduling, utility maximizaiton},
} 

@inproceedings{Godfrey:2010:ICD:1811039.1811051,
 author = {Godfrey, P. Brighten and Schapira, Michael and Zohar, Aviv and Shenker, Scott},
 title = {Incentive compatibility and dynamics of congestion control},
 abstract = {his paper studies under what conditions congestion control schemes can be both efficient, so that capacity is not wasted, and incentive compatible, so that each participant can maximize its utility by following the prescribed protocol. We show that both conditions can be achieved if routers run strict priority queueing (SPQ) or weighted fair queueing (WFQ) and end-hosts run any of a family of protocols which we call Probing Increase Educated Decrease (PIED). A natural question is whether incentive compatibility and efficiency are possible while avoiding the per-flow processing of WFQ. We partially address that question in the negative by showing that any policy satisfying a certain "locality" condition cannot guarantee both properties. Our results also have implication for convergence to some steady-state throughput for the flows. Even when senders transmit at a fixed rate (as in a UDP flow which does not react to congestion), feedback effects among the routers can result in complex dynamics which do not appear in the simple topologies studied in past work.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {95--106},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811051},
 doi = {http://doi.acm.org/10.1145/1811039.1811051},
 acmid = {1811051},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, congestion control, incentives, queueing},
} 

@article{Godfrey:2010:ICD:1811099.1811051,
 author = {Godfrey, P. Brighten and Schapira, Michael and Zohar, Aviv and Shenker, Scott},
 title = {Incentive compatibility and dynamics of congestion control},
 abstract = {his paper studies under what conditions congestion control schemes can be both efficient, so that capacity is not wasted, and incentive compatible, so that each participant can maximize its utility by following the prescribed protocol. We show that both conditions can be achieved if routers run strict priority queueing (SPQ) or weighted fair queueing (WFQ) and end-hosts run any of a family of protocols which we call Probing Increase Educated Decrease (PIED). A natural question is whether incentive compatibility and efficiency are possible while avoiding the per-flow processing of WFQ. We partially address that question in the negative by showing that any policy satisfying a certain "locality" condition cannot guarantee both properties. Our results also have implication for convergence to some steady-state throughput for the flows. Even when senders transmit at a fixed rate (as in a UDP flow which does not react to congestion), feedback effects among the routers can result in complex dynamics which do not appear in the simple topologies studied in past work.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {95--106},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811051},
 doi = {http://doi.acm.org/10.1145/1811099.1811051},
 acmid = {1811051},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, congestion control, incentives, queueing},
} 

@article{Shah:2010:DCG:1811099.1811052,
 author = {Shah, Devavrat and Shin, Jinwoo},
 title = {Dynamics in congestion games},
 abstract = {Game theoretic modeling and equilibrium analysis of congestion games have provided insights in the performance of Internet congestion control, road transportation networks, etc. Despite the long history, very little is known about their transient (non equilibrium) performance. In this paper, we are motivated to seek answers to questions such as how long does it take to reach equilibrium, when the system does operate near equilibrium in the presence of dynamics, e.g. nodes join or leave. , or the tradeoff between performance and the rate of dynamics. In this pursuit, we provide three contributions in this paper. First, a novel probabilistic model to capture realistic behaviors of agents allowing for the possibility of arbitrariness in conjunction with rationality. Second, evaluation of (a) time to converge to equilibrium under this behavior model and (b) distance to Nash equilibrium. Finally, determination of tradeoff between the rate of dynamics and quality of performance (distance to equilibrium) which leads to an interesting uncertainty principle. The novel technical ingredients involve analysis of logarithmic Sobolov constant of Markov process with time varying state space and methodically this should be of broader interest in the context of dynamical systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {107--118},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811052},
 doi = {http://doi.acm.org/10.1145/1811099.1811052},
 acmid = {1811052},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion game, logarithmic sobolov constant, logit-response},
} 

@inproceedings{Shah:2010:DCG:1811039.1811052,
 author = {Shah, Devavrat and Shin, Jinwoo},
 title = {Dynamics in congestion games},
 abstract = {Game theoretic modeling and equilibrium analysis of congestion games have provided insights in the performance of Internet congestion control, road transportation networks, etc. Despite the long history, very little is known about their transient (non equilibrium) performance. In this paper, we are motivated to seek answers to questions such as how long does it take to reach equilibrium, when the system does operate near equilibrium in the presence of dynamics, e.g. nodes join or leave. , or the tradeoff between performance and the rate of dynamics. In this pursuit, we provide three contributions in this paper. First, a novel probabilistic model to capture realistic behaviors of agents allowing for the possibility of arbitrariness in conjunction with rationality. Second, evaluation of (a) time to converge to equilibrium under this behavior model and (b) distance to Nash equilibrium. Finally, determination of tradeoff between the rate of dynamics and quality of performance (distance to equilibrium) which leads to an interesting uncertainty principle. The novel technical ingredients involve analysis of logarithmic Sobolov constant of Markov process with time varying state space and methodically this should be of broader interest in the context of dynamical systems.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {107--118},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811052},
 doi = {http://doi.acm.org/10.1145/1811039.1811052},
 acmid = {1811052},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion game, logarithmic sobolov constant, logit-response},
} 

@inproceedings{Xiang:2010:ORS:1811039.1811054,
 author = {Xiang, Liping and Xu, Yinlong and Lui, John C.S. and Chang, Qian},
 title = {Optimal recovery of single disk failure in RDP code storage systems},
 abstract = {Modern storage systems use thousands of inexpensive disks to meet the storage requirement of applications. To enhance the data availability, some form of redundancy is used. For example, conventional RAID-5 systems provide data availability for single disk failure only, while recent advanced coding techniques such as row-diagonal parity (RDP) can provide data availability with up to two disk failures. To reduce the probability of data unavailability, whenever a single disk fails, disk recovery (or rebuild) will be carried out. We show that conventional recovery scheme of RDP code for a single disk failure is inefficient and suboptimal. In this paper, we propose an optimal and efficient disk recovery scheme, Row-Diagonal Optimal Recovery (RDOR), for single disk failure of RDP code that has the following properties: (1) it is read optimal in the sense that it issues the smallest number of disk reads to recover the failed disk; (2) it has the load balancing property that all surviving disks will be subjected to the same amount of additional workload in rebuilding the failed disk. We carefully explore the design state space and theoretically show the optimality of RDOR. We carry out performance evaluation to quantify the merits of RDOR on some widely used disks.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {119--130},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811054},
 doi = {http://doi.acm.org/10.1145/1811039.1811054},
 acmid = {1811054},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RDP code, disk failure, raid recovery, recovery algorithm},
} 

@article{Xiang:2010:ORS:1811099.1811054,
 author = {Xiang, Liping and Xu, Yinlong and Lui, John C.S. and Chang, Qian},
 title = {Optimal recovery of single disk failure in RDP code storage systems},
 abstract = {Modern storage systems use thousands of inexpensive disks to meet the storage requirement of applications. To enhance the data availability, some form of redundancy is used. For example, conventional RAID-5 systems provide data availability for single disk failure only, while recent advanced coding techniques such as row-diagonal parity (RDP) can provide data availability with up to two disk failures. To reduce the probability of data unavailability, whenever a single disk fails, disk recovery (or rebuild) will be carried out. We show that conventional recovery scheme of RDP code for a single disk failure is inefficient and suboptimal. In this paper, we propose an optimal and efficient disk recovery scheme, Row-Diagonal Optimal Recovery (RDOR), for single disk failure of RDP code that has the following properties: (1) it is read optimal in the sense that it issues the smallest number of disk reads to recover the failed disk; (2) it has the load balancing property that all surviving disks will be subjected to the same amount of additional workload in rebuilding the failed disk. We carefully explore the design state space and theoretically show the optimality of RDOR. We carry out performance evaluation to quantify the merits of RDOR on some widely used disks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {119--130},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811054},
 doi = {http://doi.acm.org/10.1145/1811099.1811054},
 acmid = {1811054},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RDP code, disk failure, raid recovery, recovery algorithm},
} 

@inproceedings{Ghanbari:2010:QLR:1811039.1811055,
 author = {Ghanbari, Saeed and Soundararajan, Gokul and Amza, Cristiana},
 title = {A query language and runtime tool for evaluating behavior of multi-tier servers},
 abstract = {As modern multi-tier systems are becoming increasingly large and complex, it becomes more difficult for system analysts to understand the overall behavior of the system, and diagnose performance problems. To assist analysts inspect performance behavior, we introduce SelfTalk, a novel declarative language that allows analysts to query and understand the status of a large scale system. SelfTalk is sufficiently expressive to encode an analyst's high-level hypotheses about system invariants, normal correlations between system metrics, or other a priori derived performance models, such as, "I expect that the throughputs of interconnected system components are linearly correlated". Given a hypothesis, Dena, our runtime support system, instantiates and validates it using actual monitoring data within specific system configurations. We evaluate SelfTalk/Dena by posing several hypotheses about system behavior and querying Dena to validate system behavior in a multi-tier dynamic content server. We find that Dena automatically validates the system performance based on the pre-existing hypotheses and helps to diagnose system misbehavior.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {131--142},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811055},
 doi = {http://doi.acm.org/10.1145/1811039.1811055},
 acmid = {1811055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {expectation, hypothesis, management, performance models},
} 

@article{Ghanbari:2010:QLR:1811099.1811055,
 author = {Ghanbari, Saeed and Soundararajan, Gokul and Amza, Cristiana},
 title = {A query language and runtime tool for evaluating behavior of multi-tier servers},
 abstract = {As modern multi-tier systems are becoming increasingly large and complex, it becomes more difficult for system analysts to understand the overall behavior of the system, and diagnose performance problems. To assist analysts inspect performance behavior, we introduce SelfTalk, a novel declarative language that allows analysts to query and understand the status of a large scale system. SelfTalk is sufficiently expressive to encode an analyst's high-level hypotheses about system invariants, normal correlations between system metrics, or other a priori derived performance models, such as, "I expect that the throughputs of interconnected system components are linearly correlated". Given a hypothesis, Dena, our runtime support system, instantiates and validates it using actual monitoring data within specific system configurations. We evaluate SelfTalk/Dena by posing several hypotheses about system behavior and querying Dena to validate system behavior in a multi-tier dynamic content server. We find that Dena automatically validates the system performance based on the pre-existing hypotheses and helps to diagnose system misbehavior.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {131--142},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811055},
 doi = {http://doi.acm.org/10.1145/1811099.1811055},
 acmid = {1811055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {expectation, hypothesis, management, performance models},
} 

@article{Goel:2010:SSQ:1811099.1811056,
 author = {Goel, Ashish and Gupta, Pankaj},
 title = {Small subset queries and bloom filters using ternary associative memories, with applications},
 abstract = {Associative memories offer high levels of parallelism in matching a query against stored entries. We design and analyze an architecture which uses single</i> lookup into a Ternary Content Addressable Memory (TCAM) to solve the subset query problem for small sets, i.e., to check whether a given set (the query) contains (or alternately, is contained in) any one of a large collection of sets in a database. We use each TCAM entry as a small Ternary Bloom Filter (each 'bit' of which is one of 0,1,wildcard) to store one of the sets in the collection. Like Bloom filters, our architecture is susceptible to false positives. Since each TCAM entry is quite small, asymptotic analyses of Bloom filters do not directly apply. Surprisingly, we are able to show that the asymptotic false positive probability formula can be safely used if we penalize the small Bloom filter by taking away just one bit of storage and adding just half an extra set element before applying the formula. We believe that this analysis is independently interesting. The subset query problem has applications in databases, network intrusion detection, packet classification in Internet routers, and Information Retrieval. We demonstrate our architecture on one illustrative streaming application -- intrusion detection in network traffic. Be shingling (i.e., taking consecutive bytes of) the strings in the database, we can perform a single subset query and hence a single TCAM search, to skip many bytes in the stream. We evaluate our scheme on the open source CLAM anti-virus database, for worst-case</i> as well as random streams. Our architecture appears to be at least one order of magnitude faster than previous approaches. Since the individual Bloom filters must fit in a single TCAM entry (currently 72 to 576 bits), our solution applies only when each set is of a small cardinality. However, this is sufficient for many typical applications. Also, recent algorithms for the subset-query problem use a small-set version as a subroutine},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811056},
 doi = {http://doi.acm.org/10.1145/1811099.1811056},
 acmid = {1811056},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCAM, bloom filters, subset queries},
} 

@inproceedings{Goel:2010:SSQ:1811039.1811056,
 author = {Goel, Ashish and Gupta, Pankaj},
 title = {Small subset queries and bloom filters using ternary associative memories, with applications},
 abstract = {Associative memories offer high levels of parallelism in matching a query against stored entries. We design and analyze an architecture which uses single</i> lookup into a Ternary Content Addressable Memory (TCAM) to solve the subset query problem for small sets, i.e., to check whether a given set (the query) contains (or alternately, is contained in) any one of a large collection of sets in a database. We use each TCAM entry as a small Ternary Bloom Filter (each 'bit' of which is one of 0,1,wildcard) to store one of the sets in the collection. Like Bloom filters, our architecture is susceptible to false positives. Since each TCAM entry is quite small, asymptotic analyses of Bloom filters do not directly apply. Surprisingly, we are able to show that the asymptotic false positive probability formula can be safely used if we penalize the small Bloom filter by taking away just one bit of storage and adding just half an extra set element before applying the formula. We believe that this analysis is independently interesting. The subset query problem has applications in databases, network intrusion detection, packet classification in Internet routers, and Information Retrieval. We demonstrate our architecture on one illustrative streaming application -- intrusion detection in network traffic. Be shingling (i.e., taking consecutive bytes of) the strings in the database, we can perform a single subset query and hence a single TCAM search, to skip many bytes in the stream. We evaluate our scheme on the open source CLAM anti-virus database, for worst-case</i> as well as random streams. Our architecture appears to be at least one order of magnitude faster than previous approaches. Since the individual Bloom filters must fit in a single TCAM entry (currently 72 to 576 bits), our solution applies only when each set is of a small cardinality. However, this is sufficient for many typical applications. Also, recent algorithms for the subset-query problem use a small-set version as a subroutine},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811056},
 doi = {http://doi.acm.org/10.1145/1811039.1811056},
 acmid = {1811056},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCAM, bloom filters, subset queries},
} 

@article{Laadan:2010:TLA:1811099.1811057,
 author = {Laadan, Oren and Viennot, Nicolas and Nieh, Jason},
 title = {Transparent, lightweight application execution replay on commodity multiprocessor operating systems},
 abstract = {We present Scribe, the first system to provide transparent, low-overhead application record-replay and the ability to go live from replayed execution. Scribe introduces new lightweight operating system mechanisms, rendezvous and sync points, to efficiently record nondeterministic interactions such as related system calls, signals, and shared memory accesses. Rendezvous points make a partial ordering of execution based on system call dependencies sufficient for replay, avoiding the recording overhead of maintaining an exact execution ordering. Sync points convert asynchronous interactions that can occur at arbitrary times into synchronous events that are much easier to record and replay. We have implemented Scribe without changing, relinking, or recompiling applications, libraries, or operating system kernels, and without any specialized hardware support such as hardware performance counters. It works on commodity Linux operating systems, and commodity multi-core and multiprocessor hardware. Our results show for the first time that an operating system mechanism can correctly and transparently record and replay multi-process and multi-threaded applications on commodity multiprocessors. Scribe recording overhead is less than 2.5\% for server applications including Apache and MySQL, and less than 15\% for desktop applications including Firefox, Acrobat, OpenOffice, parallel kernel compilation, and movie playback.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811057},
 doi = {http://doi.acm.org/10.1145/1811099.1811057},
 acmid = {1811057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, fault-tolerance, record-replay, virtualization},
} 

@inproceedings{Laadan:2010:TLA:1811039.1811057,
 author = {Laadan, Oren and Viennot, Nicolas and Nieh, Jason},
 title = {Transparent, lightweight application execution replay on commodity multiprocessor operating systems},
 abstract = {We present Scribe, the first system to provide transparent, low-overhead application record-replay and the ability to go live from replayed execution. Scribe introduces new lightweight operating system mechanisms, rendezvous and sync points, to efficiently record nondeterministic interactions such as related system calls, signals, and shared memory accesses. Rendezvous points make a partial ordering of execution based on system call dependencies sufficient for replay, avoiding the recording overhead of maintaining an exact execution ordering. Sync points convert asynchronous interactions that can occur at arbitrary times into synchronous events that are much easier to record and replay. We have implemented Scribe without changing, relinking, or recompiling applications, libraries, or operating system kernels, and without any specialized hardware support such as hardware performance counters. It works on commodity Linux operating systems, and commodity multi-core and multiprocessor hardware. Our results show for the first time that an operating system mechanism can correctly and transparently record and replay multi-process and multi-threaded applications on commodity multiprocessors. Scribe recording overhead is less than 2.5\% for server applications including Apache and MySQL, and less than 15\% for desktop applications including Firefox, Acrobat, OpenOffice, parallel kernel compilation, and movie playback.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811057},
 doi = {http://doi.acm.org/10.1145/1811039.1811057},
 acmid = {1811057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, fault-tolerance, record-replay, virtualization},
} 

@article{Ni:2010:CSP:1811099.1811059,
 author = {Ni, Jian and Srikant, R. and Wu, Xinzhou},
 title = {Coloring spatial point processes with applications to peer discovery in large wireless networks},
 abstract = {In this paper, we study distributed channel assignment in wireless networks with applications to peer discovery in ad hoc wireless networks. We model channel assignment as a coloring problem for spatial point processes in which n nodes are located in a unit cube uniformly at random and each node is assigned one of K colors, where each color represents a channel. The objective is to maximize the spatial separation between nodes of the same color. In general, it is hard to derive the optimal coloring algorithm and therefore, we consider a natural greedy coloring algorithm, first proposed in [5]. We prove two key results: (i) with just a small number of colors when K is roughly of the order of log(n) loglog(n), the distance separation achieved by the greedy coloring algorithm asymptotically matches the optimal distance separation that can be achieved by an algorithm which is allowed to select the locations of the nodes but is allowed to use only one color, and (ii) when K = Omega(log(n)), the greedy coloring algorithm asymptotically achieves the best distance separation that can be achieved by an algorithm which is allowed to both optimally color and place nodes. The greedy coloring algorithm is also shown to dramatically outperform a simple random coloring algorithm. Moreover, the results continue to hold under node mobilities.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811059},
 doi = {http://doi.acm.org/10.1145/1811099.1811059},
 acmid = {1811059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel assignment, coloring algorithms, spatial point processes, wireless networks},
} 

@inproceedings{Ni:2010:CSP:1811039.1811059,
 author = {Ni, Jian and Srikant, R. and Wu, Xinzhou},
 title = {Coloring spatial point processes with applications to peer discovery in large wireless networks},
 abstract = {In this paper, we study distributed channel assignment in wireless networks with applications to peer discovery in ad hoc wireless networks. We model channel assignment as a coloring problem for spatial point processes in which n nodes are located in a unit cube uniformly at random and each node is assigned one of K colors, where each color represents a channel. The objective is to maximize the spatial separation between nodes of the same color. In general, it is hard to derive the optimal coloring algorithm and therefore, we consider a natural greedy coloring algorithm, first proposed in [5]. We prove two key results: (i) with just a small number of colors when K is roughly of the order of log(n) loglog(n), the distance separation achieved by the greedy coloring algorithm asymptotically matches the optimal distance separation that can be achieved by an algorithm which is allowed to select the locations of the nodes but is allowed to use only one color, and (ii) when K = Omega(log(n)), the greedy coloring algorithm asymptotically achieves the best distance separation that can be achieved by an algorithm which is allowed to both optimally color and place nodes. The greedy coloring algorithm is also shown to dramatically outperform a simple random coloring algorithm. Moreover, the results continue to hold under node mobilities.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {167--178},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811059},
 doi = {http://doi.acm.org/10.1145/1811039.1811059},
 acmid = {1811059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel assignment, coloring algorithms, spatial point processes, wireless networks},
} 

@inproceedings{van de Ven:2010:OTE:1811039.1811060,
 author = {van de Ven, Peter M. and Janssen, Augustus J.E.M. and van Leeuwaarden, Johan S.H.},
 title = {Optimal tradeoff between exposed and hidden nodes in large wireless networks},
 abstract = {Wireless networks equipped with the CSMA protocol are subject to collisions due to interference. For a given interference range we investigate the tradeoff between collisions (hidden nodes) and unused capacity (exposed nodes). We show that the sensing range that maximizes throughput critically depends on the activation rate of nodes. For infinite line networks, we prove the existence of a threshold: When the activation rate is below this threshold the optimal sensing range is small (to maximize spatial reuse). When the activation rate is above the threshold the optimal sensing range is just large enough to preclude all collisions. Simulations suggest that this threshold policy extends to more complex linear and non-linear topologies.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {179--190},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811060},
 doi = {http://doi.acm.org/10.1145/1811039.1811060},
 acmid = {1811060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {carrier-sensing range, exposed nodes, hidden nodes, markov processes, multi-access, throughput, wireless networks},
} 

@article{van de Ven:2010:OTE:1811099.1811060,
 author = {van de Ven, Peter M. and Janssen, Augustus J.E.M. and van Leeuwaarden, Johan S.H.},
 title = {Optimal tradeoff between exposed and hidden nodes in large wireless networks},
 abstract = {Wireless networks equipped with the CSMA protocol are subject to collisions due to interference. For a given interference range we investigate the tradeoff between collisions (hidden nodes) and unused capacity (exposed nodes). We show that the sensing range that maximizes throughput critically depends on the activation rate of nodes. For infinite line networks, we prove the existence of a threshold: When the activation rate is below this threshold the optimal sensing range is small (to maximize spatial reuse). When the activation rate is above the threshold the optimal sensing range is just large enough to preclude all collisions. Simulations suggest that this threshold policy extends to more complex linear and non-linear topologies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {179--190},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811060},
 doi = {http://doi.acm.org/10.1145/1811099.1811060},
 acmid = {1811060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {carrier-sensing range, exposed nodes, hidden nodes, markov processes, multi-access, throughput, wireless networks},
} 

@article{Liu:2010:SMW:1811099.1811061,
 author = {Liu, Shihuan and Ying, Lei and Srikant, R.},
 title = {Scheduling in multichannel wireless networks with flow-level dynamics},
 abstract = {This paper studies scheduling in multichannel wireless networks with flow-level dynamics. We consider a downlink network with a single base station, M channels (frequency bands), and multiple mobile users (flows). We also assume mobiles dynamically join the network to receive finite-size files and leave after downloading the complete files. A recent study [16] has shown that the MaxWeight algorithm fails to be throughput-optimal under this flow-level dynamics. The main contribution of this paper is the development of joint channel-assignment and workload-based scheduling algorithms for multichannel downlink networks with dynamic flow arrivals/departures. We prove that these algorithms are throughput-optimal. Our simulations further demonstrate that a hybrid channel-assignment and workload-based scheduling algorithm significantly improves the network performance (in terms of both file-transfer delay and blocking probability) compared to the existing algorithms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811061},
 doi = {http://doi.acm.org/10.1145/1811099.1811061},
 acmid = {1811061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flow-level dynamics, multichannel downlink network, wireless scheduling},
} 

@inproceedings{Liu:2010:SMW:1811039.1811061,
 author = {Liu, Shihuan and Ying, Lei and Srikant, R.},
 title = {Scheduling in multichannel wireless networks with flow-level dynamics},
 abstract = {This paper studies scheduling in multichannel wireless networks with flow-level dynamics. We consider a downlink network with a single base station, M channels (frequency bands), and multiple mobile users (flows). We also assume mobiles dynamically join the network to receive finite-size files and leave after downloading the complete files. A recent study [16] has shown that the MaxWeight algorithm fails to be throughput-optimal under this flow-level dynamics. The main contribution of this paper is the development of joint channel-assignment and workload-based scheduling algorithms for multichannel downlink networks with dynamic flow arrivals/departures. We prove that these algorithms are throughput-optimal. Our simulations further demonstrate that a hybrid channel-assignment and workload-based scheduling algorithm significantly improves the network performance (in terms of both file-transfer delay and blocking probability) compared to the existing algorithms.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811061},
 doi = {http://doi.acm.org/10.1145/1811039.1811061},
 acmid = {1811061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flow-level dynamics, multichannel downlink network, wireless scheduling},
} 

@article{Shah:2010:DSC:1811099.1811063,
 author = {Shah, Devavrat and Zaman, Tauhid},
 title = {Detecting sources of computer viruses in networks: theory and experiment},
 abstract = {We provide a systematic study of the problem of finding the source of a computer virus in a network. We model virus spreading in a network with a variant of the popular SIR model and then construct an estimator for the virus source. This estimator is based upon a novel combinatorial quantity which we term rumor centrality. We establish that this is an ML estimator for a class of graphs. We find the following surprising threshold phenomenon: on trees which grow faster than a line, the estimator always has non-trivial detection probability, whereas on trees that grow like a line, the detection probability will go to 0 as the network grows. Simulations performed on synthetic networks such as the popular small-world and scale-free networks, and on real networks such as an internet AS network and the U.S. electric power grid network, show that the estimator either finds the source exactly or within a few hops in different network topologies. We compare rumor centrality to another common network centrality notion known as distance centrality. We prove that on trees, the rumor center and distance center are equivalent, but on general networks, they may differ. Indeed, simulations show that rumor centrality outperforms distance centrality in finding virus sources in networks which are not tree-like.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {203--214},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811063},
 doi = {http://doi.acm.org/10.1145/1811099.1811063},
 acmid = {1811063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {epidemics, estimation},
} 

@inproceedings{Shah:2010:DSC:1811039.1811063,
 author = {Shah, Devavrat and Zaman, Tauhid},
 title = {Detecting sources of computer viruses in networks: theory and experiment},
 abstract = {We provide a systematic study of the problem of finding the source of a computer virus in a network. We model virus spreading in a network with a variant of the popular SIR model and then construct an estimator for the virus source. This estimator is based upon a novel combinatorial quantity which we term rumor centrality. We establish that this is an ML estimator for a class of graphs. We find the following surprising threshold phenomenon: on trees which grow faster than a line, the estimator always has non-trivial detection probability, whereas on trees that grow like a line, the detection probability will go to 0 as the network grows. Simulations performed on synthetic networks such as the popular small-world and scale-free networks, and on real networks such as an internet AS network and the U.S. electric power grid network, show that the estimator either finds the source exactly or within a few hops in different network topologies. We compare rumor centrality to another common network centrality notion known as distance centrality. We prove that on trees, the rumor center and distance center are equivalent, but on general networks, they may differ. Indeed, simulations show that rumor centrality outperforms distance centrality in finding virus sources in networks which are not tree-like.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {203--214},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811063},
 doi = {http://doi.acm.org/10.1145/1811039.1811063},
 acmid = {1811063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {epidemics, estimation},
} 

@inproceedings{Misra:2010:IPS:1811039.1811064,
 author = {Misra, Vishal and Ioannidis, Stratis and Chaintreau, Augustin and Massouli\'{e}, Laurent},
 title = {Incentivizing peer-assisted services: a fluid shapley value approach},
 abstract = {A new generation of content delivery networks for live streaming, video on demand, and software updates takes advantage of a peer-to-peer architecture to reduce their operating cost. In contrast with previous uncoordinated peer-to-peer schemes, users opt-in to dedicate part of the resources they own to help the content delivery, in exchange for receiving the same service at a reduced price. Such incentive mechanisms are appealing, as they simplify coordination and accounting. However, they also increase a user's expectation that she will receive a fair price for the resources she provides. Addressing this issue carefully is critical in ensuring that all interested parties--including the provider--are willing to participate in such a system, thereby guaranteeing its stability. In this paper, we take a cooperative game theory approach to identify the ideal incentive structure that follows the axioms formulated by Lloyd Shapley. This ensures that each player, be it the provider or a peer, receives an amount proportional to its contribution and bargaining power when entering the game. In general, the drawback of this ideal incentive structure is its computational complexity. However, we prove that as the number of peers receiving the service becomes large, the Shapley value received by each player approaches a fluid limit. This limit follows a simple closed form expression and can be computed in several scenarios of interest: by applying our technique, we show that several peer-assisted services, deployed on both wired and wireless networks, can benefit from important cost and energy savings with a proper incentive structure that follows simple compensation rules.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {215--226},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811064},
 doi = {http://doi.acm.org/10.1145/1811039.1811064},
 acmid = {1811064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooperative game theory, incentive mechanisms},
} 

@article{Misra:2010:IPS:1811099.1811064,
 author = {Misra, Vishal and Ioannidis, Stratis and Chaintreau, Augustin and Massouli\'{e}, Laurent},
 title = {Incentivizing peer-assisted services: a fluid shapley value approach},
 abstract = {A new generation of content delivery networks for live streaming, video on demand, and software updates takes advantage of a peer-to-peer architecture to reduce their operating cost. In contrast with previous uncoordinated peer-to-peer schemes, users opt-in to dedicate part of the resources they own to help the content delivery, in exchange for receiving the same service at a reduced price. Such incentive mechanisms are appealing, as they simplify coordination and accounting. However, they also increase a user's expectation that she will receive a fair price for the resources she provides. Addressing this issue carefully is critical in ensuring that all interested parties--including the provider--are willing to participate in such a system, thereby guaranteeing its stability. In this paper, we take a cooperative game theory approach to identify the ideal incentive structure that follows the axioms formulated by Lloyd Shapley. This ensures that each player, be it the provider or a peer, receives an amount proportional to its contribution and bargaining power when entering the game. In general, the drawback of this ideal incentive structure is its computational complexity. However, we prove that as the number of peers receiving the service becomes large, the Shapley value received by each player approaches a fluid limit. This limit follows a simple closed form expression and can be computed in several scenarios of interest: by applying our technique, we show that several peer-assisted services, deployed on both wired and wireless networks, can benefit from important cost and energy savings with a proper incentive structure that follows simple compensation rules.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {215--226},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811064},
 doi = {http://doi.acm.org/10.1145/1811099.1811064},
 acmid = {1811064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooperative game theory, incentive mechanisms},
} 

@inproceedings{Ma:2010:LPM:1811039.1811065,
 author = {Ma, Yadi and Banerjee, Suman and Lu, Shan and Estan, Cristian},
 title = {Leveraging parallelism for multi-dimensional packetclassification on software routers},
 abstract = {We present a software-based solution to the multi-dimensional packet classification problem which can operate at high line speeds, e.g., in excess of 10 Gbps, using high-end multi-core desktop platforms available today. Our solution, called Storm, leverages a common notion that a subset of rules are likely to be popular over short durations of time. By identifying a suitable set of popular rules one can significantly speed up existing software-based classification algorithms. A key aspect of our design is in partitioning processor resources into various relevant tasks, such as continuously computing the popular rules based on a sampled subset of traffic, fast classification for traffic that matches popular rules, dealing with packets that do not match the most popular rules, and traffic sampling. Our results show that by using a single 8-core Xeon processor desktop platform, it is possible to sustain classification rates of more than 15 Gbps for representative rule sets of size in excess of 5-dimensional 9000 rules, with no packet losses. This performance is significantly superior to a 8-way implementation of a state-of-the-art packet classification software system running on the same 8-core machine. Therefore, we believe that our design of packet classification functions can be a useful classification building block for RouteBricks-style designs, where a core router might be constructed as a mesh of regular desktop machines.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {227--238},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811065},
 doi = {http://doi.acm.org/10.1145/1811039.1811065},
 acmid = {1811065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, parallelism, storm},
} 

@article{Ma:2010:LPM:1811099.1811065,
 author = {Ma, Yadi and Banerjee, Suman and Lu, Shan and Estan, Cristian},
 title = {Leveraging parallelism for multi-dimensional packetclassification on software routers},
 abstract = {We present a software-based solution to the multi-dimensional packet classification problem which can operate at high line speeds, e.g., in excess of 10 Gbps, using high-end multi-core desktop platforms available today. Our solution, called Storm, leverages a common notion that a subset of rules are likely to be popular over short durations of time. By identifying a suitable set of popular rules one can significantly speed up existing software-based classification algorithms. A key aspect of our design is in partitioning processor resources into various relevant tasks, such as continuously computing the popular rules based on a sampled subset of traffic, fast classification for traffic that matches popular rules, dealing with packets that do not match the most popular rules, and traffic sampling. Our results show that by using a single 8-core Xeon processor desktop platform, it is possible to sustain classification rates of more than 15 Gbps for representative rule sets of size in excess of 5-dimensional 9000 rules, with no packet losses. This performance is significantly superior to a 8-way implementation of a state-of-the-art packet classification software system running on the same 8-core machine. Therefore, we believe that our design of packet classification functions can be a useful classification building block for RouteBricks-style designs, where a core router might be constructed as a mesh of regular desktop machines.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {227--238},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811065},
 doi = {http://doi.acm.org/10.1145/1811099.1811065},
 acmid = {1811065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, parallelism, storm},
} 

@inproceedings{Shah:2010:QP9:1811039.1811067,
 author = {Shah, Devavrat and Tsitsiklis, John N. and Zhong, Yuan},
 title = {Qualitative properties of \&\#945;-weighted scheduling policies},
 abstract = {We consider a switched network, a fairly general constrained queueing network model that has been used successfully to model the detailed packet-level dynamics in communication networks, such as input-queued switches and wireless networks. The main operational issue in this model is that of deciding which queues to serve, subject to certain constraints. In this paper, we study qualitative performance properties of the well known \&#945;-weighted scheduling policies. The stability, in the sense of positive recurrence, of these policies has been well understood. We establish exponential upper bounds on the tail of the steady-state distribution of the backlog. Along the way, we prove finiteness of the expected steady-state backlog when \&#945;<1, a property that was known only for \&#945; \&#8805; 1. Finally, we analyze the excursions of the maximum backlog over a finite time horizon for \&#945; \&#8805; 1. As a consequence, for \&#945; \&#8805; 1, we establish the full state space collapse property.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811067},
 doi = {http://doi.acm.org/10.1145/1811039.1811067},
 acmid = {1811067},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain, exponential bound, maximum weight-alpha, state space collapse, switched network},
} 

@article{Shah:2010:QP9:1811099.1811067,
 author = {Shah, Devavrat and Tsitsiklis, John N. and Zhong, Yuan},
 title = {Qualitative properties of \&\#945;-weighted scheduling policies},
 abstract = {We consider a switched network, a fairly general constrained queueing network model that has been used successfully to model the detailed packet-level dynamics in communication networks, such as input-queued switches and wireless networks. The main operational issue in this model is that of deciding which queues to serve, subject to certain constraints. In this paper, we study qualitative performance properties of the well known \&#945;-weighted scheduling policies. The stability, in the sense of positive recurrence, of these policies has been well understood. We establish exponential upper bounds on the tail of the steady-state distribution of the backlog. Along the way, we prove finiteness of the expected steady-state backlog when \&#945;<1, a property that was known only for \&#945; \&#8805; 1. Finally, we analyze the excursions of the maximum backlog over a finite time horizon for \&#945; \&#8805; 1. As a consequence, for \&#945; \&#8805; 1, we establish the full state space collapse property.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811067},
 doi = {http://doi.acm.org/10.1145/1811099.1811067},
 acmid = {1811067},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain, exponential bound, maximum weight-alpha, state space collapse, switched network},
} 

@inproceedings{Casale:2010:CMS:1811039.1811068,
 author = {Casale, Giuliano and Mi, Ningfang and Smirni, Evgenia},
 title = {CWS: a model-driven scheduling policy for correlated workloads},
 abstract = {We define CWS, a non-preemptive scheduling policy for workloads with correlated job sizes. CWS tackles the scheduling problem by inferring the expected sizes of upcoming jobs based on the structure of correlations and on the outcome of past scheduling decisions. Size prediction is achieved using a class of Hidden Markov Models (HMM) with continuous observation densities that describe job sizes. We show how the forward-backward algorithm of HMMs applies effectively in scheduling applications and how it can be used to derive closed-form expressions for size prediction. This is particularly simple to implement in the case of observation densities that are phase-type (PH-type) distributed, where existing fitting methods for Markovian point processes may also simplify the parameterization of the HMM workload model. Based on the job size predictions, CWS emulates size-based policies which favor short jobs, with accuracy depending mainly on the HMM used to parametrize the scheduling algorithm. Extensive simulation and analysis illustrate that CWS is competitive with policies that assume exact information about the workload.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {251--262},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811068},
 doi = {http://doi.acm.org/10.1145/1811039.1811068},
 acmid = {1811068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {correlated workload, model-driven scheduling, response time, stochastic sheduling},
} 

@article{Casale:2010:CMS:1811099.1811068,
 author = {Casale, Giuliano and Mi, Ningfang and Smirni, Evgenia},
 title = {CWS: a model-driven scheduling policy for correlated workloads},
 abstract = {We define CWS, a non-preemptive scheduling policy for workloads with correlated job sizes. CWS tackles the scheduling problem by inferring the expected sizes of upcoming jobs based on the structure of correlations and on the outcome of past scheduling decisions. Size prediction is achieved using a class of Hidden Markov Models (HMM) with continuous observation densities that describe job sizes. We show how the forward-backward algorithm of HMMs applies effectively in scheduling applications and how it can be used to derive closed-form expressions for size prediction. This is particularly simple to implement in the case of observation densities that are phase-type (PH-type) distributed, where existing fitting methods for Markovian point processes may also simplify the parameterization of the HMM workload model. Based on the job size predictions, CWS emulates size-based policies which favor short jobs, with accuracy depending mainly on the HMM used to parametrize the scheduling algorithm. Extensive simulation and analysis illustrate that CWS is competitive with policies that assume exact information about the workload.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {251--262},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811068},
 doi = {http://doi.acm.org/10.1145/1811099.1811068},
 acmid = {1811068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {correlated workload, model-driven scheduling, response time, stochastic sheduling},
} 

@inproceedings{Zheng:2010:RAU:1811039.1811069,
 author = {Zheng, Haoqiang and Nieh, Jason},
 title = {RSIO: automatic user interaction detection and scheduling},
 abstract = {We present RSIO, a processor scheduling framework for improving the response time of latency-sensitive applications by monitoring accesses to I/O channels and inferring when user interactions occur. RSIO automatically identifies processes involved in a user interaction and boosts their priorities at the time the interaction occurs to improve system response time. RSIO also detects processes indirectly involved in processing an interaction, automatically accounting for dependencies and boosting their priorities accordingly. RSIO works with existing schedulers and requires no application modifications to identify periods of latency-sensitive application activity. We have implemented RSIO in Linux and measured its effectiveness on microbenchmarks and real applications. Our results show that RSIO is easy to use and can provide substantial improvements in system performance for latency-sensitive applications.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {263--274},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811069},
 doi = {http://doi.acm.org/10.1145/1811039.1811069},
 acmid = {1811069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependencies, interactive applications, scheduling},
} 

@article{Zheng:2010:RAU:1811099.1811069,
 author = {Zheng, Haoqiang and Nieh, Jason},
 title = {RSIO: automatic user interaction detection and scheduling},
 abstract = {We present RSIO, a processor scheduling framework for improving the response time of latency-sensitive applications by monitoring accesses to I/O channels and inferring when user interactions occur. RSIO automatically identifies processes involved in a user interaction and boosts their priorities at the time the interaction occurs to improve system response time. RSIO also detects processes indirectly involved in processing an interaction, automatically accounting for dependencies and boosting their priorities accordingly. RSIO works with existing schedulers and requires no application modifications to identify periods of latency-sensitive application activity. We have implemented RSIO in Linux and measured its effectiveness on microbenchmarks and real applications. Our results show that RSIO is easy to use and can provide substantial improvements in system performance for latency-sensitive applications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {263--274},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811069},
 doi = {http://doi.acm.org/10.1145/1811099.1811069},
 acmid = {1811069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependencies, interactive applications, scheduling},
} 

@inproceedings{Bramson:2010:RLB:1811039.1811071,
 author = {Bramson, Maury and Lu, Yi and Prabhakar, Balaji},
 title = {Randomized load balancing with general service time distributions},
 abstract = {Randomized load balancing greatly improves the sharing of resources in a number of applications while being simple to implement. One model that has been extensively used to study randomized load balancing schemes is the supermarket model. In this model, jobs arrive according to a rate-n\&#955; Poisson process at a bank of n rate-1 exponential server queues. A notable result, due to Vvedenskaya et.al.</i> (1996), showed that when each arriving job is assigned to the shortest of d \&#8805; 2 randomly chosen queues, the equilibrium queue sizes decay doubly exponentially in the limit as n to \&#8734;. This is a substantial improvement over the case d=1, where queue sizes decay exponentially. The method of analysis used in the above paper and in the subsequent literature applies to jobs with exponential service time distributions and does not easily generalize. It is desirable to study load balancing models with more general, especially heavy-tailed, service time distributions since such service times occur widely in practice. This paper describes a modularized program for treating randomized load balancing problems with general service time distributions and service disciplines. The program relies on an ansatz</i> which asserts that any finite set of queues in a randomized load balancing scheme becomes independent as n to \&#8734;. This allows one to derive queue size distributions and other performance measures of interest. We establish the ansatz</i> when the service discipline is FIFO and the service time distribution has a decreasing hazard rate (this includes heavy-tailed service times). Assuming the ansatz</i>, we also obtain the following results: (i) as n to \&#8734;, the process of job arrivals at any fixed queue tends to a Poisson process whose rate depends on the size of the queue, (ii) when the service discipline at each server is processor sharing or LIFO with preemptive resume, the distribution of the number of jobs is insensitive to the service distribution, and (iii) the tail behavior of the queue-size distribution in terms of the service distribution for the FIFO service discipline.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {275--286},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811071},
 doi = {http://doi.acm.org/10.1145/1811039.1811071},
 acmid = {1811071},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymptotic independence, load balancing, randomized algorithms},
} 

@article{Bramson:2010:RLB:1811099.1811071,
 author = {Bramson, Maury and Lu, Yi and Prabhakar, Balaji},
 title = {Randomized load balancing with general service time distributions},
 abstract = {Randomized load balancing greatly improves the sharing of resources in a number of applications while being simple to implement. One model that has been extensively used to study randomized load balancing schemes is the supermarket model. In this model, jobs arrive according to a rate-n\&#955; Poisson process at a bank of n rate-1 exponential server queues. A notable result, due to Vvedenskaya et.al.</i> (1996), showed that when each arriving job is assigned to the shortest of d \&#8805; 2 randomly chosen queues, the equilibrium queue sizes decay doubly exponentially in the limit as n to \&#8734;. This is a substantial improvement over the case d=1, where queue sizes decay exponentially. The method of analysis used in the above paper and in the subsequent literature applies to jobs with exponential service time distributions and does not easily generalize. It is desirable to study load balancing models with more general, especially heavy-tailed, service time distributions since such service times occur widely in practice. This paper describes a modularized program for treating randomized load balancing problems with general service time distributions and service disciplines. The program relies on an ansatz</i> which asserts that any finite set of queues in a randomized load balancing scheme becomes independent as n to \&#8734;. This allows one to derive queue size distributions and other performance measures of interest. We establish the ansatz</i> when the service discipline is FIFO and the service time distribution has a decreasing hazard rate (this includes heavy-tailed service times). Assuming the ansatz</i>, we also obtain the following results: (i) as n to \&#8734;, the process of job arrivals at any fixed queue tends to a Poisson process whose rate depends on the size of the queue, (ii) when the service discipline at each server is processor sharing or LIFO with preemptive resume, the distribution of the number of jobs is insensitive to the service distribution, and (iii) the tail behavior of the queue-size distribution in terms of the service distribution for the FIFO service discipline.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {275--286},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811071},
 doi = {http://doi.acm.org/10.1145/1811099.1811071},
 acmid = {1811071},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymptotic independence, load balancing, randomized algorithms},
} 

@inproceedings{Ganesh:2010:LBV:1811039.1811072,
 author = {Ganesh, Ayalvadi and Lilienthal, Sarah and Manjunath, D. and Proutiere, Alexandre and Simatos, Florian},
 title = {Load balancing via random local search in closed and open systems},
 abstract = {In this paper, we analyze the performance of random load resampling and migration</i> strategies in parallel server systems. Clients initially attach to an arbitrary server, but may switch servers independently at random instants of time in an attempt to improve their service rate. This approach to load balancing contrasts with traditional approaches where clients make smart server selections upon arrival (e.g., Join-the-Shortest-Queue policy and variants thereof). Load resampling is particularly relevant in scenarios where clients cannot predict the load of a server before being actually attached to it. An important example is in wireless spectrum sharing where clients try to share a set of frequency bands in a distributed manner. We first analyze the natural Random Local Search (RLS)</i> strategy. Under this strategy, after sampling a new server randomly, clients only switch to it if their service rate is improved. In closed systems, where the client population is fixed, we derive tight estimates of the time it takes under RLS strategy to balance the load across servers. We then study open systems where clients arrive according to a random process and leave the system upon service completion. In this scenario, we analyze how client migrations within the system interact with the system dynamics induced by client arrivals and departures. We compare the load-aware RLS strategy to a load-oblivious strategy in which clients just randomly switch server without accounting for the server loads. Surprisingly, we show that both load-oblivious and load-aware strategies stabilize the system whenever this is at all possible. We further demonstrate, using large-system asymptotics, that the average client sojourn time under the load-oblivious strategy is not considerably reduced when clients apply smarter load-aware strategies.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {287--298},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811072},
 doi = {http://doi.acm.org/10.1145/1811039.1811072},
 acmid = {1811072},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mean field asymptotics, stability analysis},
} 

@article{Ganesh:2010:LBV:1811099.1811072,
 author = {Ganesh, Ayalvadi and Lilienthal, Sarah and Manjunath, D. and Proutiere, Alexandre and Simatos, Florian},
 title = {Load balancing via random local search in closed and open systems},
 abstract = {In this paper, we analyze the performance of random load resampling and migration</i> strategies in parallel server systems. Clients initially attach to an arbitrary server, but may switch servers independently at random instants of time in an attempt to improve their service rate. This approach to load balancing contrasts with traditional approaches where clients make smart server selections upon arrival (e.g., Join-the-Shortest-Queue policy and variants thereof). Load resampling is particularly relevant in scenarios where clients cannot predict the load of a server before being actually attached to it. An important example is in wireless spectrum sharing where clients try to share a set of frequency bands in a distributed manner. We first analyze the natural Random Local Search (RLS)</i> strategy. Under this strategy, after sampling a new server randomly, clients only switch to it if their service rate is improved. In closed systems, where the client population is fixed, we derive tight estimates of the time it takes under RLS strategy to balance the load across servers. We then study open systems where clients arrive according to a random process and leave the system upon service completion. In this scenario, we analyze how client migrations within the system interact with the system dynamics induced by client arrivals and departures. We compare the load-aware RLS strategy to a load-oblivious strategy in which clients just randomly switch server without accounting for the server loads. Surprisingly, we show that both load-oblivious and load-aware strategies stabilize the system whenever this is at all possible. We further demonstrate, using large-system asymptotics, that the average client sojourn time under the load-oblivious strategy is not considerably reduced when clients apply smarter load-aware strategies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {287--298},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811072},
 doi = {http://doi.acm.org/10.1145/1811099.1811072},
 acmid = {1811072},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mean field asymptotics, stability analysis},
} 

@inproceedings{Zhao:2010:UMF:1811039.1811073,
 author = {Zhao, Haiquan (Chuck) and Xia, Cathy H. and Liu, Zhen and Towsley, Don},
 title = {A unified modeling framework for distributed resource allocation of general fork and join processing networks},
 abstract = {This paper addresses the problem of distributed resource allocation in general fork and join processing networks. The problem is motivated by the complicated processing requirements arising from distributed data intensive computing. In such applications, the underlying data processing software consists of a rich set of semantics that include synchronous and asynchronous data fork and data join. The different types of semantics and processing requirements introduce complex interdependence between various data flows within the network. We study the distributed resource allocation problem in such systems with the goal of achieving the maximum total utility of output streams. Past research has dealt with networks with specific types of fork/join semantics, but none of them included all four types. We propose a novel modeling framework that can represent all combinations of fork and join semantics, and formulate the resource allocation problem as a convex optimization problem on this model. We propose a shadow-queue based decentralized iterative algorithm to solve the resource allocation problem. We show that the algorithm guarantees optimality and demonstrate through simulation that it can adapt quickly to dynamically changing environments.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811073},
 doi = {http://doi.acm.org/10.1145/1811039.1811073},
 acmid = {1811073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithm, fork and join networks, resource allocation},
} 

@article{Zhao:2010:UMF:1811099.1811073,
 author = {Zhao, Haiquan (Chuck) and Xia, Cathy H. and Liu, Zhen and Towsley, Don},
 title = {A unified modeling framework for distributed resource allocation of general fork and join processing networks},
 abstract = {This paper addresses the problem of distributed resource allocation in general fork and join processing networks. The problem is motivated by the complicated processing requirements arising from distributed data intensive computing. In such applications, the underlying data processing software consists of a rich set of semantics that include synchronous and asynchronous data fork and data join. The different types of semantics and processing requirements introduce complex interdependence between various data flows within the network. We study the distributed resource allocation problem in such systems with the goal of achieving the maximum total utility of output streams. Past research has dealt with networks with specific types of fork/join semantics, but none of them included all four types. We propose a novel modeling framework that can represent all combinations of fork and join semantics, and formulate the resource allocation problem as a convex optimization problem on this model. We propose a shadow-queue based decentralized iterative algorithm to solve the resource allocation problem. We show that the algorithm guarantees optimality and demonstrate through simulation that it can adapt quickly to dynamically changing environments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811073},
 doi = {http://doi.acm.org/10.1145/1811099.1811073},
 acmid = {1811073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithm, fork and join networks, resource allocation},
} 

@article{Ioannidis:2010:DCO:1811099.1811075,
 author = {Ioannidis, Stratis and Massoulie, Laurent and Chaintreau, Augustin},
 title = {Distributed caching over heterogeneous mobile networks},
 abstract = {Sharing content over a mobile network through opportunistic contacts has recently received considerable attention. In proposed scenarios, users store content they download in a local cache and share it with other users they meet, e.g., via Bluetooth or WiFi. The storage capacity of mobile devices is typically limited; therefore, identifying which content a user should store in her cache is a fundamental problem in the operation of any such content distribution system. In this work, we propose Psephos, a novel mechanism for determining the caching policy of each mobile user. Psephos is fully distributed: users compute their own policies individually, in the absence of a central authority. Moreover, it is designed for a heterogeneous environment, in which demand for content, access to resources, and mobility characteristics may vary across different users. Most importantly, the caching policies computed by our mechanism are optimal: we rigorously show that Psephos maximizes the system's social welfare. Our results are derived formally using techniques from stochastic approximation and convex optimization; to the best of our knowledge, our work is the first to address caching with heterogeneity in a fully distributed manner.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811075},
 doi = {http://doi.acm.org/10.1145/1811099.1811075},
 acmid = {1811075},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caching, content distribution, heterogeneity, opportunistic networks},
} 

@inproceedings{Ioannidis:2010:DCO:1811039.1811075,
 author = {Ioannidis, Stratis and Massoulie, Laurent and Chaintreau, Augustin},
 title = {Distributed caching over heterogeneous mobile networks},
 abstract = {Sharing content over a mobile network through opportunistic contacts has recently received considerable attention. In proposed scenarios, users store content they download in a local cache and share it with other users they meet, e.g., via Bluetooth or WiFi. The storage capacity of mobile devices is typically limited; therefore, identifying which content a user should store in her cache is a fundamental problem in the operation of any such content distribution system. In this work, we propose Psephos, a novel mechanism for determining the caching policy of each mobile user. Psephos is fully distributed: users compute their own policies individually, in the absence of a central authority. Moreover, it is designed for a heterogeneous environment, in which demand for content, access to resources, and mobility characteristics may vary across different users. Most importantly, the caching policies computed by our mechanism are optimal: we rigorously show that Psephos maximizes the system's social welfare. Our results are derived formally using techniques from stochastic approximation and convex optimization; to the best of our knowledge, our work is the first to address caching with heterogeneity in a fully distributed manner.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811075},
 doi = {http://doi.acm.org/10.1145/1811039.1811075},
 acmid = {1811075},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caching, content distribution, heterogeneity, opportunistic networks},
} 

@article{Antunes:2010:AFI:1811099.1811076,
 author = {Antunes, Nelson and Jacinto, Gon\c{c}alo and Pacheco, Ant\'{o}nio},
 title = {An analytical framework to infer multihop path reliability in MANETs},
 abstract = {Due to complexity and intractability reasons, most of the analytical studies on the reliability of communication paths in mobile ad hoc networks are based on the assumption of link independence. In this paper, an analytical framework is developed to characterize the random behavior of a multihop path and derive path metrics to characterize the reliability of paths. This is achieved through the modeling of a multihop path as a PDMP (piecewise deterministic Markov process). Two path based metrics are obtained as expectations of functionals of the process: the mean path duration and the path persistence. We show that these metrics are the unique solution of a set of integro-differential equations and provide a recursive scheme for their computation. Finally, numerical results illustrate the computation of the metrics; these results are compared with independent link approximation results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {323--332},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1811099.1811076},
 doi = {http://doi.acm.org/10.1145/1811099.1811076},
 acmid = {1811076},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile ad hoc networks, mobility, multihop path reliability, piecewise deterministic markov processes, random walk},
} 

@inproceedings{Antunes:2010:AFI:1811039.1811076,
 author = {Antunes, Nelson and Jacinto, Gon\c{c}alo and Pacheco, Ant\'{o}nio},
 title = {An analytical framework to infer multihop path reliability in MANETs},
 abstract = {Due to complexity and intractability reasons, most of the analytical studies on the reliability of communication paths in mobile ad hoc networks are based on the assumption of link independence. In this paper, an analytical framework is developed to characterize the random behavior of a multihop path and derive path metrics to characterize the reliability of paths. This is achieved through the modeling of a multihop path as a PDMP (piecewise deterministic Markov process). Two path based metrics are obtained as expectations of functionals of the process: the mean path duration and the path persistence. We show that these metrics are the unique solution of a set of integro-differential equations and provide a recursive scheme for their computation. Finally, numerical results illustrate the computation of the metrics; these results are compared with independent link approximation results.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {323--332},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1811039.1811076},
 doi = {http://doi.acm.org/10.1145/1811039.1811076},
 acmid = {1811076},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile ad hoc networks, mobility, multihop path reliability, piecewise deterministic markov processes, random walk},
} 

@inproceedings{Coffman:2010:CFD:1811039.1811077,
 author = {Coffman, Ed and Robert, Philippe and Simatos, Florian and Tarumi, Shuzo and Zussman, Gil},
 title = {Channel fragmentation in dynamic spectrum access systems: a theoretical study},
 abstract = {Dynamic Spectrum Access systems exploit temporarily available spectrum ('white spaces') and can spread transmissions over a number of non-contiguous sub-channels. Such methods are highly beneficial in terms of spectrum utilization. However, excessive fragmentation degrades performance and hence off-sets the benefits. Thus, there is a need to study these processes so as to determine how to ensure acceptable levels of fragmentation. Hence, we present experimental and analytical results derived from a mathematical model. We model a system operating at capacity serving requests for bandwidth by assigning a collection of gaps (sub-channels) with no limitations on the fragment size. Our main theoretical result shows that even if fragments can be arbitrarily small, the system does not degrade with time. Namely, the average total number of fragments remains bounded. Within the very difficult class of dynamic fragmentation models (including models of storage fragmentation), this result appears to be the first of its kind. Extensive experimental results describe behavior, at times unexpected, of fragmentation under different algorithms. Our model also applies to dynamic linked-list storage allocation, and provides a novel analysis in that domain. We prove that, interestingly, the 50\% rule of the classical (non-fragmented) allocation model carries over to our model. Overall, the paper provides insights into the potential behavior of practical fragmentation algorithms.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {333--344},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811039.1811077},
 doi = {http://doi.acm.org/10.1145/1811039.1811077},
 acmid = {1811077},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cognitive radio, dynamic spectrum access, ergodicity of markov chains, fragmentation},
} 

@article{Coffman:2010:CFD:1811099.1811077,
 author = {Coffman, Ed and Robert, Philippe and Simatos, Florian and Tarumi, Shuzo and Zussman, Gil},
 title = {Channel fragmentation in dynamic spectrum access systems: a theoretical study},
 abstract = {Dynamic Spectrum Access systems exploit temporarily available spectrum ('white spaces') and can spread transmissions over a number of non-contiguous sub-channels. Such methods are highly beneficial in terms of spectrum utilization. However, excessive fragmentation degrades performance and hence off-sets the benefits. Thus, there is a need to study these processes so as to determine how to ensure acceptable levels of fragmentation. Hence, we present experimental and analytical results derived from a mathematical model. We model a system operating at capacity serving requests for bandwidth by assigning a collection of gaps (sub-channels) with no limitations on the fragment size. Our main theoretical result shows that even if fragments can be arbitrarily small, the system does not degrade with time. Namely, the average total number of fragments remains bounded. Within the very difficult class of dynamic fragmentation models (including models of storage fragmentation), this result appears to be the first of its kind. Extensive experimental results describe behavior, at times unexpected, of fragmentation under different algorithms. Our model also applies to dynamic linked-list storage allocation, and provides a novel analysis in that domain. We prove that, interestingly, the 50\% rule of the classical (non-fragmented) allocation model carries over to our model. Overall, the paper provides insights into the potential behavior of practical fragmentation algorithms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {333--344},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1811099.1811077},
 doi = {http://doi.acm.org/10.1145/1811099.1811077},
 acmid = {1811077},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cognitive radio, dynamic spectrum access, ergodicity of markov chains, fragmentation},
} 

@article{Bermond:2010:DSA:1811099.1811079,
 author = {Bermond, Jean-Claude and Mazauric, Dorian and Misra, Vishal and Nain, Philippe},
 title = {A distributed scheduling algorithm for wireless networks with constant overhead and arbitrary binary interference},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {345--346},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811079},
 doi = {http://doi.acm.org/10.1145/1811099.1811079},
 acmid = {1811079},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithm, interference, stability, transmission scheduling, wireless network},
} 

@inproceedings{Bermond:2010:DSA:1811039.1811079,
 author = {Bermond, Jean-Claude and Mazauric, Dorian and Misra, Vishal and Nain, Philippe},
 title = {A distributed scheduling algorithm for wireless networks with constant overhead and arbitrary binary interference},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {345--346},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811079},
 doi = {http://doi.acm.org/10.1145/1811039.1811079},
 acmid = {1811079},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithm, interference, stability, transmission scheduling, wireless network},
} 

@inproceedings{Sagnol:2010:SCD:1811039.1811080,
 author = {Sagnol, Guillaume and Bouhtou, Mustapha and Gaubert, St\'{e}phane},
 title = {Successive c-optimal designs: a scalable technique to optimize the measurements on large networks},
 abstract = {We propose a new approach to optimize the deployment and the sampling rates of network monitoring tools, such as Netflow, on a large IP network. It reduces to solving a stochastic sequence of Second Order Cone Programs. We validate our approach with experiments relying on real data from a commercial network.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {347--348},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811080},
 doi = {http://doi.acm.org/10.1145/1811039.1811080},
 acmid = {1811080},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SOCP, c-optimality, netflow, optimal experimental design},
} 

@article{Sagnol:2010:SCD:1811099.1811080,
 author = {Sagnol, Guillaume and Bouhtou, Mustapha and Gaubert, St\'{e}phane},
 title = {Successive c-optimal designs: a scalable technique to optimize the measurements on large networks},
 abstract = {We propose a new approach to optimize the deployment and the sampling rates of network monitoring tools, such as Netflow, on a large IP network. It reduces to solving a stochastic sequence of Second Order Cone Programs. We validate our approach with experiments relying on real data from a commercial network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {347--348},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811080},
 doi = {http://doi.acm.org/10.1145/1811099.1811080},
 acmid = {1811080},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SOCP, c-optimality, netflow, optimal experimental design},
} 

@inproceedings{Cuevas:2010:DDB:1811039.1811081,
 author = {Cuevas, Rub\'{e}n and Laoutaris, Nikolaos and Yang, Xiaoyuan and Siganos, Georgos and Rodriguez, Pablo},
 title = {Deep diving into BitTorrent locality},
 abstract = {A substantial amount of work has recently gone into localizing BitTorrent traffic within an ISP in order to avoid excessive and often times unnecessary transit costs. In this work we aim to answer yet unanswered questions such as: what is the minimum and the maximum transit traffic reduction across hundreds of ISPs?, what are the win-win boundaries for ISPs and their users?, what is the maximum amount of transit traffic that can be localized without requiring fine-grained control of inter-AS overlay connections?, what is the impact to transit traffic from upgrades of residential broadband speeds?.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {349--350},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811081},
 doi = {http://doi.acm.org/10.1145/1811039.1811081},
 acmid = {1811081},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BitTorrent, locality, measurements},
} 

@article{Cuevas:2010:DDB:1811099.1811081,
 author = {Cuevas, Rub\'{e}n and Laoutaris, Nikolaos and Yang, Xiaoyuan and Siganos, Georgos and Rodriguez, Pablo},
 title = {Deep diving into BitTorrent locality},
 abstract = {A substantial amount of work has recently gone into localizing BitTorrent traffic within an ISP in order to avoid excessive and often times unnecessary transit costs. In this work we aim to answer yet unanswered questions such as: what is the minimum and the maximum transit traffic reduction across hundreds of ISPs?, what are the win-win boundaries for ISPs and their users?, what is the maximum amount of transit traffic that can be localized without requiring fine-grained control of inter-AS overlay connections?, what is the impact to transit traffic from upgrades of residential broadband speeds?.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {349--350},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811081},
 doi = {http://doi.acm.org/10.1145/1811099.1811081},
 acmid = {1811081},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BitTorrent, locality, measurements},
} 

@article{Jin:2010:IAN:1811099.1811082,
 author = {Jin, Yu and Duffield, Nick and Haffner, Patrick and Sen, Subhabrata and Zhang, Zhi-Li},
 title = {Inferring applications at the network layer using collective traffic statistics},
 abstract = {In this paper, we propose a novel technique for inferring the distribution of application classes present in the aggregated traffic flows between endpoints, which exploits both the statistics of the traffic flows, and the spatial distribution of those flows across the network. Our method employs a two-step supervised model, where the bootstrapping step provides initial (inaccurate) inference on the traffic application classes, and the graph-based calibration step adjusts the initial inference through the collective spatial traffic distribution. In evaluations using real traffic flow measurements from a large ISP, we show how our method can accurately classify application types within aggregate traffic between endpoints, even without the knowledge of ports and other traffic features. While the bootstrap estimate classifies the aggregates with 80\% accuracy, incorporating spatial distributions through calibration increases the accuracy to 92\%, i.e., roughly halving the number of errors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {351--352},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811082},
 doi = {http://doi.acm.org/10.1145/1811099.1811082},
 acmid = {1811082},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application identification, collective traffic statistics, graph-based calibration, two-step model},
} 

@inproceedings{Jin:2010:IAN:1811039.1811082,
 author = {Jin, Yu and Duffield, Nick and Haffner, Patrick and Sen, Subhabrata and Zhang, Zhi-Li},
 title = {Inferring applications at the network layer using collective traffic statistics},
 abstract = {In this paper, we propose a novel technique for inferring the distribution of application classes present in the aggregated traffic flows between endpoints, which exploits both the statistics of the traffic flows, and the spatial distribution of those flows across the network. Our method employs a two-step supervised model, where the bootstrapping step provides initial (inaccurate) inference on the traffic application classes, and the graph-based calibration step adjusts the initial inference through the collective spatial traffic distribution. In evaluations using real traffic flow measurements from a large ISP, we show how our method can accurately classify application types within aggregate traffic between endpoints, even without the knowledge of ports and other traffic features. While the bootstrap estimate classifies the aggregates with 80\% accuracy, incorporating spatial distributions through calibration increases the accuracy to 92\%, i.e., roughly halving the number of errors.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {351--352},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811082},
 doi = {http://doi.acm.org/10.1145/1811039.1811082},
 acmid = {1811082},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application identification, collective traffic statistics, graph-based calibration, two-step model},
} 

@inproceedings{Anselmi:2010:PAP:1811039.1811083,
 author = {Anselmi, Jonatha and Gaujal, Bruno},
 title = {The price of anarchy in parallel queues revisited},
 abstract = {We consider a network of parallel, non-observable queues and analyze the Price of Anarchy (PoA) from the new point of view where the router has the memory of previous dispatching choices. In the regime where the demands grow with the network size, we provide an upper bound on the PoA by means of convex programming. To study the impact of non-Bernoulli routers, we introduce the Price of Forgetting (PoF) and prove that it is bounded from above by two. Numerical experiments show that the benefit of having memory in the router is independent of the network size and heterogeneity, and monotonically depends on the network load only.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {353--354},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811083},
 doi = {http://doi.acm.org/10.1145/1811039.1811083},
 acmid = {1811083},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {convex programming, parallel queues, price of anarchy, price of forgetting},
} 

@article{Anselmi:2010:PAP:1811099.1811083,
 author = {Anselmi, Jonatha and Gaujal, Bruno},
 title = {The price of anarchy in parallel queues revisited},
 abstract = {We consider a network of parallel, non-observable queues and analyze the Price of Anarchy (PoA) from the new point of view where the router has the memory of previous dispatching choices. In the regime where the demands grow with the network size, we provide an upper bound on the PoA by means of convex programming. To study the impact of non-Bernoulli routers, we introduce the Price of Forgetting (PoF) and prove that it is bounded from above by two. Numerical experiments show that the benefit of having memory in the router is independent of the network size and heterogeneity, and monotonically depends on the network load only.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {353--354},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811083},
 doi = {http://doi.acm.org/10.1145/1811099.1811083},
 acmid = {1811083},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {convex programming, parallel queues, price of anarchy, price of forgetting},
} 

@article{Khouzani:2010:OPS:1811099.1811084,
 author = {Khouzani, M.H. R. and Sarkar, Saswati and Altman, Eitan},
 title = {Optimal propagation of security patches in mobile wireless networks: extended abstract},
 abstract = {Reliable security measures against outbreaks of malware is imperative to enable large scale proliferation of wireless technologies. Immunization and healing of the nodes through dissemination of security patches can counter the spread of a malware upon an epidemic outbreak. The distribution of patches however burdens the bandwidth which is scarce in wireless networks. The trade-offs between security risks and resource consumption can be attained by activating at any given time only fractions of dispatchers and dynamically selecting their packet transmission rates. We formulate the above trade-offs as an optimal control problem that seek to minimize the aggregate network costs that depend on security risks and resource consumed by the countermeasures. Using Pontryagin's maximum principle, we prove that the dynamic control strategies have simple structures. When the resource consumption cost is concave, optimal strategy is to use maximum resources for distribution of patches until a threshold time, upon which, the patching should halt. When the resource consumption cost is convex, the above transition is strict but continuous.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {355--356},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811084},
 doi = {http://doi.acm.org/10.1145/1811099.1811084},
 acmid = {1811084},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic patching, optimal control, security-performance trade-off},
} 

@inproceedings{Khouzani:2010:OPS:1811039.1811084,
 author = {Khouzani, M.H. R. and Sarkar, Saswati and Altman, Eitan},
 title = {Optimal propagation of security patches in mobile wireless networks: extended abstract},
 abstract = {Reliable security measures against outbreaks of malware is imperative to enable large scale proliferation of wireless technologies. Immunization and healing of the nodes through dissemination of security patches can counter the spread of a malware upon an epidemic outbreak. The distribution of patches however burdens the bandwidth which is scarce in wireless networks. The trade-offs between security risks and resource consumption can be attained by activating at any given time only fractions of dispatchers and dynamically selecting their packet transmission rates. We formulate the above trade-offs as an optimal control problem that seek to minimize the aggregate network costs that depend on security risks and resource consumed by the countermeasures. Using Pontryagin's maximum principle, we prove that the dynamic control strategies have simple structures. When the resource consumption cost is concave, optimal strategy is to use maximum resources for distribution of patches until a threshold time, upon which, the patching should halt. When the resource consumption cost is convex, the above transition is strict but continuous.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {355--356},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811084},
 doi = {http://doi.acm.org/10.1145/1811039.1811084},
 acmid = {1811084},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic patching, optimal control, security-performance trade-off},
} 

@inproceedings{Le:2010:MCE:1811039.1811085,
 author = {Le, Kien and Bilgir, Ozlem and Bianchini, Ricardo and Martonosi, Margaret and Nguyen, Thu D.},
 title = {Managing the cost, energy consumption, and carbon footprint of internet services},
 abstract = {The large amount of energy consumed by Internet services represents significant and fast-growing financial and environmental costs. This paper introduces a general, optimization-based framework and several request distribution policies that enable multi-data-center services to manage their brown energy consumption and leverage green energy, while respecting their service-level agreements (SLAs) and minimizing energy cost. Our policies can be used to abide by caps on brown energy consumption that might arise from various scenarios such as government imposed Kyoto-style carbon limits. Extensive simulations and real experiments show that our policies allow a service to trade off consumption and cost. For example, using our policies, a service can reduce brown energy consumption by 24\% for only a 10\% increase in cost, while still abiding by SLAs.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {357--358},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811085},
 doi = {http://doi.acm.org/10.1145/1811039.1811085},
 acmid = {1811085},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data center, energy cap, optimization, renewable energy, request distribution},
} 

@article{Le:2010:MCE:1811099.1811085,
 author = {Le, Kien and Bilgir, Ozlem and Bianchini, Ricardo and Martonosi, Margaret and Nguyen, Thu D.},
 title = {Managing the cost, energy consumption, and carbon footprint of internet services},
 abstract = {The large amount of energy consumed by Internet services represents significant and fast-growing financial and environmental costs. This paper introduces a general, optimization-based framework and several request distribution policies that enable multi-data-center services to manage their brown energy consumption and leverage green energy, while respecting their service-level agreements (SLAs) and minimizing energy cost. Our policies can be used to abide by caps on brown energy consumption that might arise from various scenarios such as government imposed Kyoto-style carbon limits. Extensive simulations and real experiments show that our policies allow a service to trade off consumption and cost. For example, using our policies, a service can reduce brown energy consumption by 24\% for only a 10\% increase in cost, while still abiding by SLAs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {357--358},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811085},
 doi = {http://doi.acm.org/10.1145/1811099.1811085},
 acmid = {1811085},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data center, energy cap, optimization, renewable energy, request distribution},
} 

@inproceedings{Mishra:2010:CPM:1811039.1811086,
 author = {Mishra, Asit K. and Srikantaiah, Shekhar and Kandemir, Mahmut and Das, Chita R.},
 title = {Coordinated power management of voltage islands in CMPs},
 abstract = {Multiple clock domain architectures have recently been proposed to alleviate the power problem in CMPs by having different frequency/voltage values assigned to each domain based on workload requirements. However, accurate allocation of power to these voltage/frequency islands based on time varying workload characteristics as well as controlling the power consumption at the provisioned power level is non-trivial. Toward this end, we propose a two-tier feedback-based control theoretic solution. Our first-tier consists of a global power manager that allocates power targets to individual islands based on the workload dynamics. The power consumptions of these islands are in turn controlled by a second-tier, consisting of local controllers that regulate island power using dynamic voltage and frequency scaling in response to workload requirements.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {359--360},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811086},
 doi = {http://doi.acm.org/10.1145/1811039.1811086},
 acmid = {1811086},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DVFs, GALs, chip multiprocessors (CMP), control theory},
} 

@article{Mishra:2010:CPM:1811099.1811086,
 author = {Mishra, Asit K. and Srikantaiah, Shekhar and Kandemir, Mahmut and Das, Chita R.},
 title = {Coordinated power management of voltage islands in CMPs},
 abstract = {Multiple clock domain architectures have recently been proposed to alleviate the power problem in CMPs by having different frequency/voltage values assigned to each domain based on workload requirements. However, accurate allocation of power to these voltage/frequency islands based on time varying workload characteristics as well as controlling the power consumption at the provisioned power level is non-trivial. Toward this end, we propose a two-tier feedback-based control theoretic solution. Our first-tier consists of a global power manager that allocates power targets to individual islands based on the workload dynamics. The power consumptions of these islands are in turn controlled by a second-tier, consisting of local controllers that regulate island power using dynamic voltage and frequency scaling in response to workload requirements.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {359--360},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811086},
 doi = {http://doi.acm.org/10.1145/1811099.1811086},
 acmid = {1811086},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DVFs, GALs, chip multiprocessors (CMP), control theory},
} 

@article{Nguyen:2010:RSA:1811099.1811087,
 author = {Nguyen, Hung X. and Roughan, Matthew},
 title = {Rigorous statistical analysis of internet loss measurements},
 abstract = {In this paper we present a rigorous technique for estimating confidence intervals of packet loss measurements. Our approach is motivated by simple observations that the loss process can be modelled as an alternating renewal process. We use this structure to build a Hidden Semi-Markov Model (HSMM) for the measurement process, and from this estimate both loss rates, and their confidence intervals. We use both simulations and a set of more than 18000 hours of real Internet measurements (between dedicated measurement hosts, PlanetLab hosts, web and DNS servers) to cross-validate our estimates, and show that they are significantly more accurate than any current alternative.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {361--362},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811087},
 doi = {http://doi.acm.org/10.1145/1811099.1811087},
 acmid = {1811087},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accuracy, loss rate, performance measurement},
} 

@inproceedings{Nguyen:2010:RSA:1811039.1811087,
 author = {Nguyen, Hung X. and Roughan, Matthew},
 title = {Rigorous statistical analysis of internet loss measurements},
 abstract = {In this paper we present a rigorous technique for estimating confidence intervals of packet loss measurements. Our approach is motivated by simple observations that the loss process can be modelled as an alternating renewal process. We use this structure to build a Hidden Semi-Markov Model (HSMM) for the measurement process, and from this estimate both loss rates, and their confidence intervals. We use both simulations and a set of more than 18000 hours of real Internet measurements (between dedicated measurement hosts, PlanetLab hosts, web and DNS servers) to cross-validate our estimates, and show that they are significantly more accurate than any current alternative.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {361--362},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811087},
 doi = {http://doi.acm.org/10.1145/1811039.1811087},
 acmid = {1811087},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accuracy, loss rate, performance measurement},
} 

@article{Osogami:2010:SOT:1811099.1811088,
 author = {Osogami, Takayuki and Raymond, Rudy},
 title = {Semidefinite optimization for transient analysis of queues},
 abstract = {We derive an upper bound on the tail distribution of the transient waiting time for the GI/GI/1 queue from a formulation of semidefinite programming (SDP). Our upper bounds are expressed in closed forms using the first two moments of the service time and the interarrival time. The upper bounds on the tail distributions are integrated to obtain the upper bounds on the corresponding expectations. We also extend the formulation of the SDP, using the higher moments of the service time and the interarrival time, and calculate upper bounds and lower bounds numerically.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {363--364},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811088},
 doi = {http://doi.acm.org/10.1145/1811099.1811088},
 acmid = {1811088},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bounds, duality, g/g/1 queue, moments, occupation measure, semidefinite programming, transient},
} 

@inproceedings{Osogami:2010:SOT:1811039.1811088,
 author = {Osogami, Takayuki and Raymond, Rudy},
 title = {Semidefinite optimization for transient analysis of queues},
 abstract = {We derive an upper bound on the tail distribution of the transient waiting time for the GI/GI/1 queue from a formulation of semidefinite programming (SDP). Our upper bounds are expressed in closed forms using the first two moments of the service time and the interarrival time. The upper bounds on the tail distributions are integrated to obtain the upper bounds on the corresponding expectations. We also extend the formulation of the SDP, using the higher moments of the service time and the interarrival time, and calculate upper bounds and lower bounds numerically.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {363--364},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811088},
 doi = {http://doi.acm.org/10.1145/1811039.1811088},
 acmid = {1811088},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bounds, duality, g/g/1 queue, moments, occupation measure, semidefinite programming, transient},
} 

@inproceedings{Park:2010:CCF:1811039.1811089,
 author = {Park, Dongchul and Debnath, Biplob and Du, David},
 title = {CFTL: a convertible flash translation layer adaptive to data access patterns},
 abstract = {The flash translation layer (FTL) is a software/hardware in terface inside NAND flash memory. Since FTL has a critical impact on the performance of NAND flash-based devices, a variety of FTL schemes have been proposed to improve their performance. In this paper, we propose a novel hybrid FTL scheme named Convertible Flash Translation Layer (CFTL). Unlike other existing FTLs using static address mapping schemes, CFTL is adaptive to data access patterns so that it can dynamically switch its mapping scheme to either a read-optimized or a write-optimized mapping scheme. In addition to this convertible scheme, we propose an efficient caching strategy to further improve the CFTL performance with only a simple hint. Consequently, both the convertible feature and the caching strategy empower CFTL to achieve good read performance as well as good write performance.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811089},
 doi = {http://doi.acm.org/10.1145/1811039.1811089},
 acmid = {1811089},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CFTL, FTL, flash memory, flash translation layer},
} 

@article{Park:2010:CCF:1811099.1811089,
 author = {Park, Dongchul and Debnath, Biplob and Du, David},
 title = {CFTL: a convertible flash translation layer adaptive to data access patterns},
 abstract = {The flash translation layer (FTL) is a software/hardware in terface inside NAND flash memory. Since FTL has a critical impact on the performance of NAND flash-based devices, a variety of FTL schemes have been proposed to improve their performance. In this paper, we propose a novel hybrid FTL scheme named Convertible Flash Translation Layer (CFTL). Unlike other existing FTLs using static address mapping schemes, CFTL is adaptive to data access patterns so that it can dynamically switch its mapping scheme to either a read-optimized or a write-optimized mapping scheme. In addition to this convertible scheme, we propose an efficient caching strategy to further improve the CFTL performance with only a simple hint. Consequently, both the convertible feature and the caching strategy empower CFTL to achieve good read performance as well as good write performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811089},
 doi = {http://doi.acm.org/10.1145/1811099.1811089},
 acmid = {1811089},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CFTL, FTL, flash memory, flash translation layer},
} 

@article{Qian:2010:CUS:1811099.1811090,
 author = {Qian, Feng and Pathak, Abhinav and Hu, Yu Charlie and Mao, Zhuoqing Morley and Xie, Yinglian},
 title = {A case for unsupervised-learning-based spam filtering},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {367--368},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811090},
 doi = {http://doi.acm.org/10.1145/1811099.1811090},
 acmid = {1811090},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {latent semantics analysis (LSA), spam campaign, spamcampaignassassin (SCA), unsupervised learning},
} 

@inproceedings{Qian:2010:CUS:1811039.1811090,
 author = {Qian, Feng and Pathak, Abhinav and Hu, Yu Charlie and Mao, Zhuoqing Morley and Xie, Yinglian},
 title = {A case for unsupervised-learning-based spam filtering},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {367--368},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811090},
 doi = {http://doi.acm.org/10.1145/1811039.1811090},
 acmid = {1811090},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {latent semantics analysis (LSA), spam campaign, spamcampaignassassin (SCA), unsupervised learning},
} 

@inproceedings{Rajagopalan:2010:DAD:1811039.1811091,
 author = {Rajagopalan, Shreevatsa and Shah, Devavrat},
 title = {Distributed averaging in dynamic networks},
 abstract = {Distributed averaging is a well-studied problem, and often a "prototype" for a class of fundamental questions arising in various disciplines. Previous work has considered the effect of dynamics in the network topology, in terms of changes in which communication links are present. Here, we analyze the other forms of dynamics, namely: changes in the values at the nodes, and nodes joining or leaving the network.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {369--370},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811091},
 doi = {http://doi.acm.org/10.1145/1811039.1811091},
 acmid = {1811091},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed averaging, distributed networks, dynamics, message-passing},
} 

@article{Rajagopalan:2010:DAD:1811099.1811091,
 author = {Rajagopalan, Shreevatsa and Shah, Devavrat},
 title = {Distributed averaging in dynamic networks},
 abstract = {Distributed averaging is a well-studied problem, and often a "prototype" for a class of fundamental questions arising in various disciplines. Previous work has considered the effect of dynamics in the network topology, in terms of changes in which communication links are present. Here, we analyze the other forms of dynamics, namely: changes in the values at the nodes, and nodes joining or leaving the network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {369--370},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811091},
 doi = {http://doi.acm.org/10.1145/1811099.1811091},
 acmid = {1811091},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed averaging, distributed networks, dynamics, message-passing},
} 

@inproceedings{Sarikaya:2010:PBP:1811039.1811092,
 author = {Sarikaya, Ruhi and Isci, Canturk and Buyuktosunoglu, Alper},
 title = {Program behavior prediction using a statistical metric model},
 abstract = {Adaptive computing systems rely on predictions of program behavior to understand and respond to the dynamically varying application characteristics. This study describes an accurate statistical workload metric modeling scheme for predicting program phases. Our evaluations demonstrate the superior performance of this predictor over existing predictors on a wide range of benchmarks. This prediction accuracy lends itself to improved power-performance trade-offs when applied to dynamic power management.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {371--372},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811092},
 doi = {http://doi.acm.org/10.1145/1811039.1811092},
 acmid = {1811092},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer architecture, system performance measurement, monitoring and forecasting, workload characterization},
} 

@article{Sarikaya:2010:PBP:1811099.1811092,
 author = {Sarikaya, Ruhi and Isci, Canturk and Buyuktosunoglu, Alper},
 title = {Program behavior prediction using a statistical metric model},
 abstract = {Adaptive computing systems rely on predictions of program behavior to understand and respond to the dynamically varying application characteristics. This study describes an accurate statistical workload metric modeling scheme for predicting program phases. Our evaluations demonstrate the superior performance of this predictor over existing predictors on a wide range of benchmarks. This prediction accuracy lends itself to improved power-performance trade-offs when applied to dynamic power management.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {371--372},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811092},
 doi = {http://doi.acm.org/10.1145/1811099.1811092},
 acmid = {1811092},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer architecture, system performance measurement, monitoring and forecasting, workload characterization},
} 

@article{Shah:2010:DOQ:1811099.1811093,
 author = {Shah, Devavrat and Shin, Jinwoo},
 title = {Delay optimal queue-based CSMA},
 abstract = {In the past year or so, an exciting progress has led to throughput optimal design of CSMA-based algorithms for wireless networks. However, such an algorithm suffers from very poor delay performance. A recent work suggests that it is impossible to design a CSMA-like simple algorithm that is throughput optimal and induces low delay for any wireless network. However, wireless networks arising in practice are formed by nodes placed, possibly arbitrarily, in some geographic area. In this paper, we propose a CSMA algorithm with per-node average-delay bounded by a constant, independent of the network size, when the network has geometry (precisely, polynomial growth structure) that is present in any</i> practical wireless network. Two novel features of our algorithm, crucial for its performance, are (a) choice of access probabilities as an appropriate function of queue-sizes, and (b) use of local network topological structures. Essentially, our algorithm is a queue-based CSMA with a minor difference that at each time instance a very small fraction of frozen</i> nodes do not execute CSMA. Somewhat surprisingly, appropriate selection of such frozen nodes, in a distributed manner, lead to the delay optimal performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811093},
 doi = {http://doi.acm.org/10.1145/1811099.1811093},
 acmid = {1811093},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aloha, markov chain, mixing time, wireless multi-access},
} 

@inproceedings{Shah:2010:DOQ:1811039.1811093,
 author = {Shah, Devavrat and Shin, Jinwoo},
 title = {Delay optimal queue-based CSMA},
 abstract = {In the past year or so, an exciting progress has led to throughput optimal design of CSMA-based algorithms for wireless networks. However, such an algorithm suffers from very poor delay performance. A recent work suggests that it is impossible to design a CSMA-like simple algorithm that is throughput optimal and induces low delay for any wireless network. However, wireless networks arising in practice are formed by nodes placed, possibly arbitrarily, in some geographic area. In this paper, we propose a CSMA algorithm with per-node average-delay bounded by a constant, independent of the network size, when the network has geometry (precisely, polynomial growth structure) that is present in any</i> practical wireless network. Two novel features of our algorithm, crucial for its performance, are (a) choice of access probabilities as an appropriate function of queue-sizes, and (b) use of local network topological structures. Essentially, our algorithm is a queue-based CSMA with a minor difference that at each time instance a very small fraction of frozen</i> nodes do not execute CSMA. Somewhat surprisingly, appropriate selection of such frozen nodes, in a distributed manner, lead to the delay optimal performance.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811093},
 doi = {http://doi.acm.org/10.1145/1811039.1811093},
 acmid = {1811093},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aloha, markov chain, mixing time, wireless multi-access},
} 

@article{Shye:2010:CMU:1811099.1811094,
 author = {Shye, Alex and Scholbrock, Benjamin and Memik, Gokhan and Dinda, Peter A.},
 title = {Characterizing and modeling user activity on smartphones: summary},
 abstract = {In this paper, we present a comprehensive analysis of real smartphone usage during a 6-month study of real user activity on the Android G1 smartphone. Our goal is to study the high-level characteristics of smartphone usage, and to understand the implications on optimizing smartphones, and their networks. Overall, we present 11 findings that cover general usage behavior, interaction with the battery, power consumption, network activity, frequently-run applications, and modeling usage states.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {375--376},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811094},
 doi = {http://doi.acm.org/10.1145/1811099.1811094},
 acmid = {1811094},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded systems, human factors},
} 

@inproceedings{Shye:2010:CMU:1811039.1811094,
 author = {Shye, Alex and Scholbrock, Benjamin and Memik, Gokhan and Dinda, Peter A.},
 title = {Characterizing and modeling user activity on smartphones: summary},
 abstract = {In this paper, we present a comprehensive analysis of real smartphone usage during a 6-month study of real user activity on the Android G1 smartphone. Our goal is to study the high-level characteristics of smartphone usage, and to understand the implications on optimizing smartphones, and their networks. Overall, we present 11 findings that cover general usage behavior, interaction with the battery, power consumption, network activity, frequently-run applications, and modeling usage states.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {375--376},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811094},
 doi = {http://doi.acm.org/10.1145/1811039.1811094},
 acmid = {1811094},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded systems, human factors},
} 

@inproceedings{Silveira:2010:DTA:1811039.1811095,
 author = {Silveira, Fernando and Diot, Christophe and Taft, Nina and Govindan, Ramesh},
 title = {Detecting traffic anomalies using an equilibrium property},
 abstract = {When many flows are multiplexed on a non-saturated link, their volume changes over short timescales tend to cancel each other out, making the average change across flows close to zero. This equilibrium property holds if the flows are nearly independent, and it is violated by traffic changes caused by several correlated flows. We exploit this empirical property to design a computationally simple anomaly detection method.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {377--378},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811095},
 doi = {http://doi.acm.org/10.1145/1811039.1811095},
 acmid = {1811095},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, statistical test},
} 

@article{Silveira:2010:DTA:1811099.1811095,
 author = {Silveira, Fernando and Diot, Christophe and Taft, Nina and Govindan, Ramesh},
 title = {Detecting traffic anomalies using an equilibrium property},
 abstract = {When many flows are multiplexed on a non-saturated link, their volume changes over short timescales tend to cancel each other out, making the average change across flows close to zero. This equilibrium property holds if the flows are nearly independent, and it is violated by traffic changes caused by several correlated flows. We exploit this empirical property to design a computationally simple anomaly detection method.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {377--378},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811095},
 doi = {http://doi.acm.org/10.1145/1811099.1811095},
 acmid = {1811095},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, statistical test},
} 

@article{Soundararajan:2010:CSE:1811099.1811096,
 author = {Soundararajan, Niranjan and Sivasubramaniam, Anand and Narayanan, Vijay},
 title = {Characterizing the soft error vulnerability of multicores running multithreaded applications},
 abstract = {Multicores have become the platform of choice across all market segments. Cost-effective protection against soft errors is important in these environments, due to the need to move to lower technology generations and the exploding number of transistors on a chip. While multicores offer the flexibility of varying the number of application threads and the number of cores on which they run, the reliability impact of choosing one configuration over another is unclear. Our study reveals that the reliability costs vary dramatically between configurations and being unaware could lead to a sub-optimal choice.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {379--380},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811096},
 doi = {http://doi.acm.org/10.1145/1811099.1811096},
 acmid = {1811096},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fit rate, multicore, soft errors},
} 

@inproceedings{Soundararajan:2010:CSE:1811039.1811096,
 author = {Soundararajan, Niranjan and Sivasubramaniam, Anand and Narayanan, Vijay},
 title = {Characterizing the soft error vulnerability of multicores running multithreaded applications},
 abstract = {Multicores have become the platform of choice across all market segments. Cost-effective protection against soft errors is important in these environments, due to the need to move to lower technology generations and the exploding number of transistors on a chip. While multicores offer the flexibility of varying the number of application threads and the number of cores on which they run, the reliability impact of choosing one configuration over another is unclear. Our study reveals that the reliability costs vary dramatically between configurations and being unaware could lead to a sub-optimal choice.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {379--380},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811096},
 doi = {http://doi.acm.org/10.1145/1811039.1811096},
 acmid = {1811096},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fit rate, multicore, soft errors},
} 

@inproceedings{Tan:2010:MMP:1811039.1811097,
 author = {Tan, Jian and Wei, Wei and Jiang, Bo and Shroff, Ness and Towsley, Don},
 title = {Can multipath mitigate power law delays?: effects of parallelism on tail performance},
 abstract = {},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {381--382},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811097},
 doi = {http://doi.acm.org/10.1145/1811039.1811097},
 acmid = {1811097},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multipath, parallelism, power law, redundant transmission, split transmission},
} 

@article{Tan:2010:MMP:1811099.1811097,
 author = {Tan, Jian and Wei, Wei and Jiang, Bo and Shroff, Ness and Towsley, Don},
 title = {Can multipath mitigate power law delays?: effects of parallelism on tail performance},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {381--382},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811097},
 doi = {http://doi.acm.org/10.1145/1811099.1811097},
 acmid = {1811097},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multipath, parallelism, power law, redundant transmission, split transmission},
} 

@article{Tomozei:2010:DUP:1811099.1811098,
 author = {Tomozei, Dan-Cristian and Massouli\'{e}, Laurent},
 title = {Distributed user profiling via spectral methods},
 abstract = {User profiling is a useful primitive for constructing personalized services, such as content recommendation. In the present work we investigate the feasibility of user profiling in a distributed setting, with no central authority and only local information exchanges between users. Our main contributions are: (i)~We propose a spectral clustering technique, and prove its ability to recover unknown user profiles with only few measures of affinity between users. (ii)~We develop distributed algorithms which achieve an embedding of users into a low-dimensional space, based on spectral transformation. These involve simple message passing among users, and provably converge to the desired embedding.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {38},
 issue = {1},
 month = {June},
 year = {2010},
 issn = {0163-5999},
 pages = {383--384},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811099.1811098},
 doi = {http://doi.acm.org/10.1145/1811099.1811098},
 acmid = {1811098},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, distributed spectral embedding, gossip},
} 

@inproceedings{Tomozei:2010:DUP:1811039.1811098,
 author = {Tomozei, Dan-Cristian and Massouli\'{e}, Laurent},
 title = {Distributed user profiling via spectral methods},
 abstract = {User profiling is a useful primitive for constructing personalized services, such as content recommendation. In the present work we investigate the feasibility of user profiling in a distributed setting, with no central authority and only local information exchanges between users. Our main contributions are: (i)~We propose a spectral clustering technique, and prove its ability to recover unknown user profiles with only few measures of affinity between users. (ii)~We develop distributed algorithms which achieve an embedding of users into a low-dimensional space, based on spectral transformation. These involve simple message passing among users, and provably converge to the desired embedding.},
 booktitle = {Proceedings of the ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '10},
 year = {2010},
 isbn = {978-1-4503-0038-4},
 location = {New York, New York, USA},
 pages = {383--384},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1811039.1811098},
 doi = {http://doi.acm.org/10.1145/1811039.1811098},
 acmid = {1811098},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustering, distributed spectral embedding, gossip},
} 

@inproceedings{Bordenave:2008:PRM:1375457.1375459,
 author = {Bordenave, Charles and McDonald, David and Proutiere, Alexandre},
 title = {Performance of random medium access control, an asymptotic approach},
 abstract = {Random Medium-Access-Control (MAC) algorithms have played an increasingly important role in the development of wired and wireless Local Area Networks (LANs) and yet the performance of even the simplest of these algorithms, such as slotted-Aloha, are still not clearly understood. In this paper we provide a general and accurate method to analyze networks where interfering users share a resource using random MAC algorithms. We show that this method is asymptotically exact when the number of users grows large, and explain why it also provides extremely accurate performance estimates even for small systems. We apply this analysis to solve two open problems: (a) We address the stability region of non-adaptive Aloha-like systems. Specifically, we consider a fixed number of buffered users receiving packets from independent exogenous processes and accessing the resource using Aloha-like algorithms. We provide an explicit expression to approximate the stability region of this system, and prove its accuracy. (b) We outline how to apply the analysis to predict the performance of adaptive MAC algorithms, such as the exponential back-off algorithm, in a system where saturated users interact through interference. In general, our analysis may be used to quantify how far from optimality the simple MAC algorithms used in LANs today are, and to determine if more complicated (e.g. queue-based) algorithms proposed in the literature could provide significant improvement in performance.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375459},
 doi = {http://doi.acm.org/10.1145/1375457.1375459},
 acmid = {1375459},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aloha/CSMA, exponential back-off, stability},
} 

@article{Bordenave:2008:PRM:1384529.1375459,
 author = {Bordenave, Charles and McDonald, David and Proutiere, Alexandre},
 title = {Performance of random medium access control, an asymptotic approach},
 abstract = {Random Medium-Access-Control (MAC) algorithms have played an increasingly important role in the development of wired and wireless Local Area Networks (LANs) and yet the performance of even the simplest of these algorithms, such as slotted-Aloha, are still not clearly understood. In this paper we provide a general and accurate method to analyze networks where interfering users share a resource using random MAC algorithms. We show that this method is asymptotically exact when the number of users grows large, and explain why it also provides extremely accurate performance estimates even for small systems. We apply this analysis to solve two open problems: (a) We address the stability region of non-adaptive Aloha-like systems. Specifically, we consider a fixed number of buffered users receiving packets from independent exogenous processes and accessing the resource using Aloha-like algorithms. We provide an explicit expression to approximate the stability region of this system, and prove its accuracy. (b) We outline how to apply the analysis to predict the performance of adaptive MAC algorithms, such as the exponential back-off algorithm, in a system where saturated users interact through interference. In general, our analysis may be used to quantify how far from optimality the simple MAC algorithms used in LANs today are, and to determine if more complicated (e.g. queue-based) algorithms proposed in the literature could provide significant improvement in performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375459},
 doi = {http://doi.acm.org/10.1145/1384529.1375459},
 acmid = {1375459},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aloha/CSMA, exponential back-off, stability},
} 

@article{Casale:2008:BAC:1384529.1375460,
 author = {Casale, Giuliano and Mi, Ningfang and Smirni, Evgenia},
 title = {Bound analysis of closed queueing networks with workload burstiness},
 abstract = {Burstiness and temporal dependence in service processes are often found in multi-tier architectures and storage devices and must be captured accurately in capacity planning models as these features are responsible of significant performance degradations. However, existing models and approximations for networks of first-come first-served (FCFS) queues with general independent (GI) service are unable to predict performance of systems with temporal dependence in workloads. To overcome this difficulty, we define and study a class of closed queueing networks where service times are represented by Markovian Arrival Processes (MAPs), a class of point processes that can model general distributions, but also temporal dependent features such as burstiness in service times. We call these models MAP queueing networks. We introduce provable upper and lower bounds for arbitrary performance indexes (e.g., throughput, response time, utilization) that we call Linear Reduction (LR) bounds. Numerical experiments indicate that LR bounds achieve a mean accuracy error of 2 percent. The result promotes LR bounds as a versatile and reliable bounding methodology of the performance of modern computer systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375460},
 doi = {http://doi.acm.org/10.1145/1384529.1375460},
 acmid = {1375460},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bound analysis, burstiness, closed systems, markovian arrival processes, nonrenewal service, queueing networks, temporal dependence},
} 

@inproceedings{Casale:2008:BAC:1375457.1375460,
 author = {Casale, Giuliano and Mi, Ningfang and Smirni, Evgenia},
 title = {Bound analysis of closed queueing networks with workload burstiness},
 abstract = {Burstiness and temporal dependence in service processes are often found in multi-tier architectures and storage devices and must be captured accurately in capacity planning models as these features are responsible of significant performance degradations. However, existing models and approximations for networks of first-come first-served (FCFS) queues with general independent (GI) service are unable to predict performance of systems with temporal dependence in workloads. To overcome this difficulty, we define and study a class of closed queueing networks where service times are represented by Markovian Arrival Processes (MAPs), a class of point processes that can model general distributions, but also temporal dependent features such as burstiness in service times. We call these models MAP queueing networks. We introduce provable upper and lower bounds for arbitrary performance indexes (e.g., throughput, response time, utilization) that we call Linear Reduction (LR) bounds. Numerical experiments indicate that LR bounds achieve a mean accuracy error of 2 percent. The result promotes LR bounds as a versatile and reliable bounding methodology of the performance of modern computer systems.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375460},
 doi = {http://doi.acm.org/10.1145/1375457.1375460},
 acmid = {1375460},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bound analysis, burstiness, closed systems, markovian arrival processes, nonrenewal service, queueing networks, temporal dependence},
} 

@article{Wierman:2008:SDI:1384529.1375461,
 author = {Wierman, Adam and Nuyens, Misja},
 title = {Scheduling despite inexact job-size information},
 abstract = {Motivated by the optimality of Shortest Remaining Processing Time (SRPT) for mean response time, in recent years many computer systems have used the heuristic of "favoring small jobs" in order to dramatically reduce user response times. However, rarely do computer systems have knowledge of exact remaining sizes. In this paper, we introduce the class of \&#949;-SMART policies, which formalizes the heuristic of "favoring small jobs" in a way that includes a wide range of policies that schedule using inexact job-size information. Examples of \&#949;-SMART policies include (i) policies that use exact size information, e.g., SRPT and PSJF, (ii) policies that use job-size estimates, and (iii) policies that use a finite number of size-based priority levels. For many \&#949;-SMART policies, e.g., SRPT with inexact job-size information, there are no analytic results available in the literature. In this work, we prove four main results: we derive upper and lower bounds on the mean response time, the mean slowdown, the response-time tail, and the conditional response time of \&#949;-SMART policies. In each case, the results explicitly characterize the tradeoff between the accuracy of the job-size information used to prioritize and the performance of the resulting policy. Thus, the results provide designers insight into how accurate job-size information must be in order to achieve desired performance guarantees.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375461},
 doi = {http://doi.acm.org/10.1145/1384529.1375461},
 acmid = {1375461},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, SMART, SRPT, job size estimates, queueing, response time, scheduling, shortest remaining processing time},
} 

@inproceedings{Wierman:2008:SDI:1375457.1375461,
 author = {Wierman, Adam and Nuyens, Misja},
 title = {Scheduling despite inexact job-size information},
 abstract = {Motivated by the optimality of Shortest Remaining Processing Time (SRPT) for mean response time, in recent years many computer systems have used the heuristic of "favoring small jobs" in order to dramatically reduce user response times. However, rarely do computer systems have knowledge of exact remaining sizes. In this paper, we introduce the class of \&#949;-SMART policies, which formalizes the heuristic of "favoring small jobs" in a way that includes a wide range of policies that schedule using inexact job-size information. Examples of \&#949;-SMART policies include (i) policies that use exact size information, e.g., SRPT and PSJF, (ii) policies that use job-size estimates, and (iii) policies that use a finite number of size-based priority levels. For many \&#949;-SMART policies, e.g., SRPT with inexact job-size information, there are no analytic results available in the literature. In this work, we prove four main results: we derive upper and lower bounds on the mean response time, the mean slowdown, the response-time tail, and the conditional response time of \&#949;-SMART policies. In each case, the results explicitly characterize the tradeoff between the accuracy of the job-size information used to prioritize and the performance of the resulting policy. Thus, the results provide designers insight into how accurate job-size information must be in order to achieve desired performance guarantees.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375461},
 doi = {http://doi.acm.org/10.1145/1375457.1375461},
 acmid = {1375461},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, SMART, SRPT, job size estimates, queueing, response time, scheduling, shortest remaining processing time},
} 

@inproceedings{Lelarge:2008:NED:1375457.1375463,
 author = {Lelarge, Marc and Bolot, Jean},
 title = {Network externalities and the deployment of security features and protocols in the internet},
 abstract = {Getting new security features and protocols to be widely adopted and deployed in the Internet has been a continuing challenge. There are several reasons for this, in particular economic reasons arising from the presence of network externalities. Indeed, like the Internet itself, the technologies to secure it exhibit network effects: their value to individual users changes as other users decide to adopt them or not. In particular, the benefits felt by early adopters of security solutions might fall significantly below the cost of adoption, making it difficult for those solutions to gain attraction and get deployed at a large scale. Our goal in this paper is to model and quantify the impact of such externalities on the adoptability and deployment of security features and protocols in the Internet. We study a network of interconnected agents, which are subject to epidemic risks such as those caused by propagating viruses and worms, and which can decide whether or not to invest some amount to deploy security solutions. Agents experience negative externalities from other agents, as the risks faced by an agent depend not only on the choices of that agent (whether or not to invest in self-protection), but also on those of the other agents. Expectations about choices made by other agents then influence investments in self-protection, resulting in a possibly suboptimal outcome overall. We present and solve an analytical model where the agents are connected according to a variety of network topologies. Borrowing ideas and techniques used in statistical physics, we derive analytic solutions for sparse random graphs, for which we obtain asymptotic results. We show that we can explicitly identify the impact of network externalities on the adoptability and deployment of security features. In other words, we identify both the economic and network properties that determine the adoption of security technologies. Therefore, we expect our results to provide useful guidance for the design of new economic mechanisms and for the development of network protocols likely to be deployed at a large scale.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375463},
 doi = {http://doi.acm.org/10.1145/1375457.1375463},
 acmid = {1375463},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cascading, economics, epidemics, game theory, price of anarchy, security},
} 

@article{Lelarge:2008:NED:1384529.1375463,
 author = {Lelarge, Marc and Bolot, Jean},
 title = {Network externalities and the deployment of security features and protocols in the internet},
 abstract = {Getting new security features and protocols to be widely adopted and deployed in the Internet has been a continuing challenge. There are several reasons for this, in particular economic reasons arising from the presence of network externalities. Indeed, like the Internet itself, the technologies to secure it exhibit network effects: their value to individual users changes as other users decide to adopt them or not. In particular, the benefits felt by early adopters of security solutions might fall significantly below the cost of adoption, making it difficult for those solutions to gain attraction and get deployed at a large scale. Our goal in this paper is to model and quantify the impact of such externalities on the adoptability and deployment of security features and protocols in the Internet. We study a network of interconnected agents, which are subject to epidemic risks such as those caused by propagating viruses and worms, and which can decide whether or not to invest some amount to deploy security solutions. Agents experience negative externalities from other agents, as the risks faced by an agent depend not only on the choices of that agent (whether or not to invest in self-protection), but also on those of the other agents. Expectations about choices made by other agents then influence investments in self-protection, resulting in a possibly suboptimal outcome overall. We present and solve an analytical model where the agents are connected according to a variety of network topologies. Borrowing ideas and techniques used in statistical physics, we derive analytic solutions for sparse random graphs, for which we obtain asymptotic results. We show that we can explicitly identify the impact of network externalities on the adoptability and deployment of security features. In other words, we identify both the economic and network properties that determine the adoption of security technologies. Therefore, we expect our results to provide useful guidance for the design of new economic mechanisms and for the development of network protocols likely to be deployed at a large scale.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375463},
 doi = {http://doi.acm.org/10.1145/1384529.1375463},
 acmid = {1375463},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cascading, economics, epidemics, game theory, price of anarchy, security},
} 

@article{Brosh:2008:DT:1384529.1375464,
 author = {Brosh, Eli and Baset, Salman Abdul and Rubenstein, Dan and Schulzrinne, Henning},
 title = {The delay-friendliness of TCP},
 abstract = {TCP has traditionally been considered unfriendly for real-time applications. Nonetheless, popular applications such as Skype use TCP since UDP packets cannot pass through many NATs and firewalls. Motivated by this observation, we study the delay performance of TCP for real-time media flows. We develop an analytical performance model for the delay of TCP. We use extensive experiments to validate the model and to evaluate the impact of various TCP mechanisms on its delay performance. Based on our results, we derive the working region for VoIP and live video streaming applications and provide guidelines for delay-friendly TCP settings. Our research indicates that simple application-level schemes, such as packet splitting and parallel connections, can reduce the delay of real-time TCP flows by as much as 30\% and 90\%, respectively.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375464},
 doi = {http://doi.acm.org/10.1145/1384529.1375464},
 acmid = {1375464},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TDP congestion control, VoIP, live video streaming, performance modeling},
} 

@inproceedings{Brosh:2008:DT:1375457.1375464,
 author = {Brosh, Eli and Baset, Salman Abdul and Rubenstein, Dan and Schulzrinne, Henning},
 title = {The delay-friendliness of TCP},
 abstract = {TCP has traditionally been considered unfriendly for real-time applications. Nonetheless, popular applications such as Skype use TCP since UDP packets cannot pass through many NATs and firewalls. Motivated by this observation, we study the delay performance of TCP for real-time media flows. We develop an analytical performance model for the delay of TCP. We use extensive experiments to validate the model and to evaluate the impact of various TCP mechanisms on its delay performance. Based on our results, we derive the working region for VoIP and live video streaming applications and provide guidelines for delay-friendly TCP settings. Our research indicates that simple application-level schemes, such as packet splitting and parallel connections, can reduce the delay of real-time TCP flows by as much as 30\% and 90\%, respectively.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375464},
 doi = {http://doi.acm.org/10.1145/1375457.1375464},
 acmid = {1375464},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TDP congestion control, VoIP, live video streaming, performance modeling},
} 

@article{Kim:2008:SVR:1384529.1375465,
 author = {Kim, Changhoon and Gerber, Alexandre and Lund, Carsten and Pei, Dan and Sen, Subhabrata},
 title = {Scalable VPN routing via relaying},
 abstract = {Enterprise customers are increasingly adopting MPLS (Multiprotocol Label Switching) VPN (Virtual Private Network) service that offers direct any-to-any reachability among the customer sites via a provider network. Unfortunately this direct reachability model makes the service provider's routing tables grow very large as the number of VPNs and the number of routes per customer increase. As a result, router memory in the provider's network has become a key bottleneck in provisioning new customers. This paper proposes Relaying, a scalable VPN routing architecture that the provider can implement simply by modifying the configuration of routers in the provider network, without requiring changes to the router hardware and software. Relaying substantially reduces the memory footprint of VPNs by choosing a small number of hub routers in each VPN that maintain full reachability information, and by allowing non-hub routers to reach other routers through a hub. Deploying Relaying in practice, however, poses a challenging optimization problem that involves minimizing router memory usage by having as few hubs as possible, while limiting the additional latency due to indirect delivery via a hub. We first investigate the fundamental tension between the two objectives and then develop algorithms to solve the optimization problem by leveraging some unique properties of VPNs, such as sparsity of traffic matrices and spatial locality of customer sites. Extensive evaluations using real traffic matrices, routing configurations, and VPN topologies demonstrate that Relaying is very promising and can reduce routing-table usage by up to 90\%, while increasing the additional distances traversed by traffic by only a few hundred miles, and the backbone bandwidth usage by less than 10\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375465},
 doi = {http://doi.acm.org/10.1145/1384529.1375465},
 acmid = {1375465},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VPN, measurement, optimization, routing},
} 

@inproceedings{Kim:2008:SVR:1375457.1375465,
 author = {Kim, Changhoon and Gerber, Alexandre and Lund, Carsten and Pei, Dan and Sen, Subhabrata},
 title = {Scalable VPN routing via relaying},
 abstract = {Enterprise customers are increasingly adopting MPLS (Multiprotocol Label Switching) VPN (Virtual Private Network) service that offers direct any-to-any reachability among the customer sites via a provider network. Unfortunately this direct reachability model makes the service provider's routing tables grow very large as the number of VPNs and the number of routes per customer increase. As a result, router memory in the provider's network has become a key bottleneck in provisioning new customers. This paper proposes Relaying, a scalable VPN routing architecture that the provider can implement simply by modifying the configuration of routers in the provider network, without requiring changes to the router hardware and software. Relaying substantially reduces the memory footprint of VPNs by choosing a small number of hub routers in each VPN that maintain full reachability information, and by allowing non-hub routers to reach other routers through a hub. Deploying Relaying in practice, however, poses a challenging optimization problem that involves minimizing router memory usage by having as few hubs as possible, while limiting the additional latency due to indirect delivery via a hub. We first investigate the fundamental tension between the two objectives and then develop algorithms to solve the optimization problem by leveraging some unique properties of VPNs, such as sparsity of traffic matrices and spatial locality of customer sites. Extensive evaluations using real traffic matrices, routing configurations, and VPN topologies demonstrate that Relaying is very promising and can reduce routing-table usage by up to 90\%, while increasing the additional distances traversed by traffic by only a few hundred miles, and the backbone bandwidth usage by less than 10\%.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375465},
 doi = {http://doi.acm.org/10.1145/1375457.1375465},
 acmid = {1375465},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VPN, measurement, optimization, routing},
} 

@article{Tschopp:2008:HRO:1384529.1375467,
 author = {Tschopp, Dominique and Diggavi, Suhas and Grossglauser, Matthias},
 title = {Hierarchical routing over dynamic wireless networks},
 abstract = {In dynamic networks the topology evolves and routes are maintained by frequent updates, consuming throughput available for data transmission. We ask whether there exist low-overhead schemes for these networks, that produce routes that are within a small constant factor (stretch) of the optimal route-length. This is studied by using the underlying geometric properties of the connectivity graph in wireless networks. For a class of models for wireless network that fulfill some mild conditions on the connectivity and on mobility over the time of interest, we can design distributed routing algorithm that maintain the routes over a changing topology. This scheme needs only node identities and integrates location service along with routing, therefore accounting for the complete overhead. We analyze the worst-case (conservative) overhead and route-quality (stretch) performance of this algorithm for the aforementioned class of models. Our algorithm allows constant stretch routing with a network wide control traffic overhead of O</i>(n</i> log<sup>2</sup> n</i>) bits per mobility time step (time-scale of topology change) translating to O</i>(log<sup>2</sup> n</i>) overhead per node (with high probability for wireless networks with such mobility model). We can reduce the maximum overhead per node by using a load-balancing technique at the cost of a slightly higher average overhead. Numerics show that these bounds are quite conservative.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375467},
 doi = {http://doi.acm.org/10.1145/1384529.1375467},
 acmid = {1375467},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {competitive analysis, distributed routing algorithms, geometric random graphs, wireless networks},
} 

@inproceedings{Tschopp:2008:HRO:1375457.1375467,
 author = {Tschopp, Dominique and Diggavi, Suhas and Grossglauser, Matthias},
 title = {Hierarchical routing over dynamic wireless networks},
 abstract = {In dynamic networks the topology evolves and routes are maintained by frequent updates, consuming throughput available for data transmission. We ask whether there exist low-overhead schemes for these networks, that produce routes that are within a small constant factor (stretch) of the optimal route-length. This is studied by using the underlying geometric properties of the connectivity graph in wireless networks. For a class of models for wireless network that fulfill some mild conditions on the connectivity and on mobility over the time of interest, we can design distributed routing algorithm that maintain the routes over a changing topology. This scheme needs only node identities and integrates location service along with routing, therefore accounting for the complete overhead. We analyze the worst-case (conservative) overhead and route-quality (stretch) performance of this algorithm for the aforementioned class of models. Our algorithm allows constant stretch routing with a network wide control traffic overhead of O</i>(n</i> log<sup>2</sup> n</i>) bits per mobility time step (time-scale of topology change) translating to O</i>(log<sup>2</sup> n</i>) overhead per node (with high probability for wireless networks with such mobility model). We can reduce the maximum overhead per node by using a load-balancing technique at the cost of a slightly higher average overhead. Numerics show that these bounds are quite conservative.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375467},
 doi = {http://doi.acm.org/10.1145/1375457.1375467},
 acmid = {1375467},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {competitive analysis, distributed routing algorithms, geometric random graphs, wireless networks},
} 

@article{Rayanchu:2008:LNC:1384529.1375468,
 author = {Rayanchu, Shravan and Sen, Sayandeep and Wu, Jianming and Banerjee, Suman and Sengupta, Sudipta},
 title = {Loss-aware network coding for unicast wireless sessions: design, implementation, and performance evaluation},
 abstract = {Local network coding is growing in prominence as a technique to facilitate greater capacity utilization in multi-hop wireless networks. A specific objective of such local network coding techniques has been to explicitly minimize the total number of transmissions needed to carry packets across each wireless hop. While such a strategy is certainly useful, we argue that in lossy wireless environments, a better use of local network coding is to provide higher levels of redundancy even at the cost of increasing the number of transmissions required to communicate the same information. In this paper we show that the design space for effective redundancy in local network coding is quite large, which makes optimal formulations of the problem hard to realize in practice. We present a detailed exploration of this design space and propose a suite of algorithms, called CLONE, that can lead to further throughput gains in multi-hop wireless scenarios. Through careful analysis, simulations, and detailed implementation on a real testbed, we show that some of our simplest CLONE algorithms can be efficiently implemented in today's wireless hardware to provide a factor of two improvement in throughput for example scenarios, while other, more effective, CLONE algorithms require additional advances in hardware processing speeds to be deployable in practice.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375468},
 doi = {http://doi.acm.org/10.1145/1384529.1375468},
 acmid = {1375468},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ieee 802.11, network coding, wireless networks},
} 

@inproceedings{Rayanchu:2008:LNC:1375457.1375468,
 author = {Rayanchu, Shravan and Sen, Sayandeep and Wu, Jianming and Banerjee, Suman and Sengupta, Sudipta},
 title = {Loss-aware network coding for unicast wireless sessions: design, implementation, and performance evaluation},
 abstract = {Local network coding is growing in prominence as a technique to facilitate greater capacity utilization in multi-hop wireless networks. A specific objective of such local network coding techniques has been to explicitly minimize the total number of transmissions needed to carry packets across each wireless hop. While such a strategy is certainly useful, we argue that in lossy wireless environments, a better use of local network coding is to provide higher levels of redundancy even at the cost of increasing the number of transmissions required to communicate the same information. In this paper we show that the design space for effective redundancy in local network coding is quite large, which makes optimal formulations of the problem hard to realize in practice. We present a detailed exploration of this design space and propose a suite of algorithms, called CLONE, that can lead to further throughput gains in multi-hop wireless scenarios. Through careful analysis, simulations, and detailed implementation on a real testbed, we show that some of our simplest CLONE algorithms can be efficiently implemented in today's wireless hardware to provide a factor of two improvement in throughput for example scenarios, while other, more effective, CLONE algorithms require additional advances in hardware processing speeds to be deployable in practice.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375468},
 doi = {http://doi.acm.org/10.1145/1375457.1375468},
 acmid = {1375468},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ieee 802.11, network coding, wireless networks},
} 

@article{Schmid:2008:EMV:1384529.1375469,
 author = {Schmid, Thomas and Charbiwala, Zainul and Friedman, Jonathan and Cho, Young H. and Srivastava, Mani B.},
 title = {Exploiting manufacturing variations for compensating environment-induced clock drift in time synchronization},
 abstract = {Time synchronization is an essential service in distributed computing and control systems. It is used to enable tasks such as synchronized data sampling and accurate time-of-flight estimation, which can be used to locate nodes. The deviation in nodes' knowledge of time and inter-node resynchronization rate are affected by three sources of time stamping errors: network wireless communication delays, platform hardware and software delays, and environment-dependent frequency drift characteristics of the clock source. The focus of this work is on the last source of error, the clock source, which becomes a bottleneck when either required time accuracy or available energy budget and bandwidth (and thus feasible resynchronization rate) are too stringent. Traditionally, this has required the use of expensive clock sources (such as temperature compensation using precise sensors and calibration models) that are not cost-effective in low-end wireless sensor nodes. Since the frequency of a crystal is a product of manufacturing and environmental parameters, we describe an approach that exploits the subtle manufacturing variation between a pair of inexpensive oscillators placed in close proximity to algorithmically compensate for the drift produced by the environment. The algorithm effectively uses the oscillators themselves as a sensor that can detect changes in frequency caused by a variety of environmental factors. We analyze the performance of our approach using behavioral models of crystal oscillators in our algorithm simulation. Then we apply the algorithm to an actual temperature dataset collected at the James Wildlife Reserve in Riverside County, California, and test the algorithms on a waveform generator based testbed. The result of our experiments show that the technique can effectively improve the frequency stability of an inexpensive uncompensated crystal 5 times with the potential for even higher gains in future implementations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375469},
 doi = {http://doi.acm.org/10.1145/1384529.1375469},
 acmid = {1375469},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clocks, emulation, oscillator, time synchronization},
} 

@inproceedings{Schmid:2008:EMV:1375457.1375469,
 author = {Schmid, Thomas and Charbiwala, Zainul and Friedman, Jonathan and Cho, Young H. and Srivastava, Mani B.},
 title = {Exploiting manufacturing variations for compensating environment-induced clock drift in time synchronization},
 abstract = {Time synchronization is an essential service in distributed computing and control systems. It is used to enable tasks such as synchronized data sampling and accurate time-of-flight estimation, which can be used to locate nodes. The deviation in nodes' knowledge of time and inter-node resynchronization rate are affected by three sources of time stamping errors: network wireless communication delays, platform hardware and software delays, and environment-dependent frequency drift characteristics of the clock source. The focus of this work is on the last source of error, the clock source, which becomes a bottleneck when either required time accuracy or available energy budget and bandwidth (and thus feasible resynchronization rate) are too stringent. Traditionally, this has required the use of expensive clock sources (such as temperature compensation using precise sensors and calibration models) that are not cost-effective in low-end wireless sensor nodes. Since the frequency of a crystal is a product of manufacturing and environmental parameters, we describe an approach that exploits the subtle manufacturing variation between a pair of inexpensive oscillators placed in close proximity to algorithmically compensate for the drift produced by the environment. The algorithm effectively uses the oscillators themselves as a sensor that can detect changes in frequency caused by a variety of environmental factors. We analyze the performance of our approach using behavioral models of crystal oscillators in our algorithm simulation. Then we apply the algorithm to an actual temperature dataset collected at the James Wildlife Reserve in Riverside County, California, and test the algorithms on a waveform generator based testbed. The result of our experiments show that the technique can effectively improve the frequency stability of an inexpensive uncompensated crystal 5 times with the potential for even higher gains in future implementations.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375469},
 doi = {http://doi.acm.org/10.1145/1375457.1375469},
 acmid = {1375469},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clocks, emulation, oscillator, time synchronization},
} 

@inproceedings{Cohen:2008:CEM:1375457.1375471,
 author = {Cohen, Edith and Duffield, Nick and Lund, Carsten and Thorup, Mikkel},
 title = {Confident estimation for multistage measurement sampling and aggregation},
 abstract = {Measurement, collection, and interpretation of network usage data commonly involves multiple stage of sampling and aggregation. Examples include sampling packets, aggregating them into flow statistics at a router, sampling and aggregation of usage records in a network data repository for reporting, query and archiving. Although unbiased estimates of packet, bytes and flows usage can be formed for each sampling operation, for many applications it is crucial to know the inherent estimation error. Previous work in this area has been limited mainly to analyzing the estimator variance for particular methods, e.g., independent packet sampling. However, the variance is of limited use for more general sampling methods, where the estimate may not be well approximated by a Gaussian distribution. This motivates our paper, in which we establish Chernoff bounds on the likelihood of estimation error in a general multistage combination of measurement sampling and aggregation. We derive the scale against which errors are measured, in terms of the constituent sampling and aggregation operations. In particular this enables us to obtain rigorous confidence intervals around any given estimate. We apply our method to a number of sampling schemes both in the literature and currently deployed, including sampling of packet sampled NetFlow records, Sample and Hold, and Flow Slicing. We obtain one particularly striking result in the first case: that for a range of parameterizations, packet sampling has no additional impact on the estimator confidence derived from our bound, beyond that already imposed by flow sampling.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375471},
 doi = {http://doi.acm.org/10.1145/1375457.1375471},
 acmid = {1375471},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {confidence intervals, estimation, network measurement, sampling},
} 

@article{Cohen:2008:CEM:1384529.1375471,
 author = {Cohen, Edith and Duffield, Nick and Lund, Carsten and Thorup, Mikkel},
 title = {Confident estimation for multistage measurement sampling and aggregation},
 abstract = {Measurement, collection, and interpretation of network usage data commonly involves multiple stage of sampling and aggregation. Examples include sampling packets, aggregating them into flow statistics at a router, sampling and aggregation of usage records in a network data repository for reporting, query and archiving. Although unbiased estimates of packet, bytes and flows usage can be formed for each sampling operation, for many applications it is crucial to know the inherent estimation error. Previous work in this area has been limited mainly to analyzing the estimator variance for particular methods, e.g., independent packet sampling. However, the variance is of limited use for more general sampling methods, where the estimate may not be well approximated by a Gaussian distribution. This motivates our paper, in which we establish Chernoff bounds on the likelihood of estimation error in a general multistage combination of measurement sampling and aggregation. We derive the scale against which errors are measured, in terms of the constituent sampling and aggregation operations. In particular this enables us to obtain rigorous confidence intervals around any given estimate. We apply our method to a number of sampling schemes both in the literature and currently deployed, including sampling of packet sampled NetFlow records, Sample and Hold, and Flow Slicing. We obtain one particularly striking result in the first case: that for a range of parameterizations, packet sampling has no additional impact on the estimator confidence derived from our bound, beyond that already imposed by flow sampling.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375471},
 doi = {http://doi.acm.org/10.1145/1384529.1375471},
 acmid = {1375471},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {confidence intervals, estimation, network measurement, sampling},
} 

@inproceedings{Lu:2008:CBN:1375457.1375472,
 author = {Lu, Yi and Montanari, Andrea and Prabhakar, Balaji and Dharmapurikar, Sarang and Kabbani, Abdul},
 title = {Counter braids: a novel counter architecture for per-flow measurement},
 abstract = {Fine-grained network measurement requires routers and switches to update large arrays of counters at very high link speed (e.g. 40 Gbps). A naive algorithm needs an infeasible amount of SRAM to store both the counters and a flow-to-counter association rule, so that arriving packets can update corresponding counters at link speed. This has made accurate per-flow measurement complex and expensive, and motivated approximate methods that detect and measure only the large flows. This paper revisits the problem of accurate per-flow measurement. We present a counter architecture, called Counter Braids, inspired by sparse random graph codes. In a nutshell, Counter Braids "compresses while counting". It solves the central problems (counter space and flow-to-counter association) of per-flow measurement by "braiding" a hierarchy of counters with random graphs. Braiding results in drastic space reduction by sharing counters among flows; and using random graphs generated on-the-fly with hash functions avoids the storage of flow-to-counter association. The Counter Braids architecture is optimal (albeit with a complex decoder) as it achieves the maximum compression rate asymptotically. For implementation, we present a low-complexity message passing decoding algorithm, which can recover flow sizes with essentially zero error. Evaluation on Internet traces demonstrates that almost all flow sizes are recovered exactly with only a few bits of counter space per flow.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375472},
 doi = {http://doi.acm.org/10.1145/1375457.1375472},
 acmid = {1375472},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {message passing algorithms, network measurement, statistic counters},
} 

@article{Lu:2008:CBN:1384529.1375472,
 author = {Lu, Yi and Montanari, Andrea and Prabhakar, Balaji and Dharmapurikar, Sarang and Kabbani, Abdul},
 title = {Counter braids: a novel counter architecture for per-flow measurement},
 abstract = {Fine-grained network measurement requires routers and switches to update large arrays of counters at very high link speed (e.g. 40 Gbps). A naive algorithm needs an infeasible amount of SRAM to store both the counters and a flow-to-counter association rule, so that arriving packets can update corresponding counters at link speed. This has made accurate per-flow measurement complex and expensive, and motivated approximate methods that detect and measure only the large flows. This paper revisits the problem of accurate per-flow measurement. We present a counter architecture, called Counter Braids, inspired by sparse random graph codes. In a nutshell, Counter Braids "compresses while counting". It solves the central problems (counter space and flow-to-counter association) of per-flow measurement by "braiding" a hierarchy of counters with random graphs. Braiding results in drastic space reduction by sharing counters among flows; and using random graphs generated on-the-fly with hash functions avoids the storage of flow-to-counter association. The Counter Braids architecture is optimal (albeit with a complex decoder) as it achieves the maximum compression rate asymptotically. For implementation, we present a low-complexity message passing decoding algorithm, which can recover flow sizes with essentially zero error. Evaluation on Internet traces demonstrates that almost all flow sizes are recovered exactly with only a few bits of counter space per flow.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375472},
 doi = {http://doi.acm.org/10.1145/1384529.1375472},
 acmid = {1375472},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {message passing algorithms, network measurement, statistic counters},
} 

@inproceedings{Anandkumar:2008:TSB:1375457.1375473,
 author = {Anandkumar, Animashree and Bisdikian, Chatschik and Agrawal, Dakshi},
 title = {Tracking in a spaghetti bowl: monitoring transactions using footprints},
 abstract = {The problem of tracking end-to-end service-level transactions in the absence of instrumentation support is considered. The transaction instances progress through a state-transition model and generate time-stamped footprints on entering each state in the model. The goal is to track individual transactions using these footprints even when the footprints may not contain any tokens uniquely identifying the transaction instances that generated them. Assuming a semi-Markov process model for state transitions, the transaction instances are tracked probabilistically by matching them to the available footprints according to the maximum likelihood (ML) criterion. Under the ML-rule, for a two-state system, it is shown that the probability that all the instances are matched correctly is minimized when the transition times are i.i.d. exponentially distributed. When the transition times are i.i.d. distributed, the ML-rule reduces to a minimum weight bipartite matching and reduces further to a first-in first-out match for a special class of distributions. For a multi-state model with an acyclic state transition digraph, a constructive proof shows that the ML-rule reduces to splicing the results of independent matching of many bipartite systems.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375473},
 doi = {http://doi.acm.org/10.1145/1375457.1375473},
 acmid = {1375473},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bipartite matching, maximum-likelihood tracking, semi-markov process, transaction monitoring},
} 

@article{Anandkumar:2008:TSB:1384529.1375473,
 author = {Anandkumar, Animashree and Bisdikian, Chatschik and Agrawal, Dakshi},
 title = {Tracking in a spaghetti bowl: monitoring transactions using footprints},
 abstract = {The problem of tracking end-to-end service-level transactions in the absence of instrumentation support is considered. The transaction instances progress through a state-transition model and generate time-stamped footprints on entering each state in the model. The goal is to track individual transactions using these footprints even when the footprints may not contain any tokens uniquely identifying the transaction instances that generated them. Assuming a semi-Markov process model for state transitions, the transaction instances are tracked probabilistically by matching them to the available footprints according to the maximum likelihood (ML) criterion. Under the ML-rule, for a two-state system, it is shown that the probability that all the instances are matched correctly is minimized when the transition times are i.i.d. exponentially distributed. When the transition times are i.i.d. distributed, the ML-rule reduces to a minimum weight bipartite matching and reduces further to a first-in first-out match for a special class of distributions. For a multi-state model with an acyclic state transition digraph, a constructive proof shows that the ML-rule reduces to splicing the results of independent matching of many bipartite systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375473},
 doi = {http://doi.acm.org/10.1145/1384529.1375473},
 acmid = {1375473},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bipartite matching, maximum-likelihood tracking, semi-markov process, transaction monitoring},
} 

@article{Singhal:2008:OSS:1384529.1375474,
 author = {Singhal, Harsh and Michailidis, George},
 title = {Optimal sampling in state space models with applications to network monitoring},
 abstract = {Advances in networking technology have enabled network engineers to use sampled data from routers to estimate network flow volumes and track them over time. However, low sampling rates result in large noise in traffic volume estimates. We propose to combine data on individual flows obtained from sampling with highly aggregate data obtained from SNMP measurements (similar to those used in network tomography) for the tracking problem at hand. Specifically, we introduce a linearized state space model for the estimation of network traffic flow volumes from combined SNMP and sampled data. Further, we formulate the problem of obtaining optimal sampling rates under router resource constraints as an experiment design problem. Theoretically it corresponds to the problem of optimal design for estimation of conditional means for state space models and we present the associated convex programs for a simple approach to it. The usefulness of the approach in the context of network monitoring is illustrated through an extensive numerical study.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375474},
 doi = {http://doi.acm.org/10.1145/1384529.1375474},
 acmid = {1375474},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet traffic matrix estimation, kalman filtering, optimal design of experiments, state space models},
} 

@inproceedings{Singhal:2008:OSS:1375457.1375474,
 author = {Singhal, Harsh and Michailidis, George},
 title = {Optimal sampling in state space models with applications to network monitoring},
 abstract = {Advances in networking technology have enabled network engineers to use sampled data from routers to estimate network flow volumes and track them over time. However, low sampling rates result in large noise in traffic volume estimates. We propose to combine data on individual flows obtained from sampling with highly aggregate data obtained from SNMP measurements (similar to those used in network tomography) for the tracking problem at hand. Specifically, we introduce a linearized state space model for the estimation of network traffic flow volumes from combined SNMP and sampled data. Further, we formulate the problem of obtaining optimal sampling rates under router resource constraints as an experiment design problem. Theoretically it corresponds to the problem of optimal design for estimation of conditional means for state space models and we present the associated convex programs for a simple approach to it. The usefulness of the approach in the context of network monitoring is illustrated through an extensive numerical study.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375474},
 doi = {http://doi.acm.org/10.1145/1375457.1375474},
 acmid = {1375474},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet traffic matrix estimation, kalman filtering, optimal design of experiments, state space models},
} 

@inproceedings{Ioannidis:2008:DHP:1375457.1375476,
 author = {Ioannidis, Stratis and Marbach, Peter},
 title = {On the design of hybrid peer-to-peer systems},
 abstract = {In this paper, we consider hybrid peer-to-peer systems where users form an unstructured peer-to-peer network with the purpose of assisting a server in the distribution of data. We present a mathematical model that we use to analyze the scalability of hybrid peer-to-peer systems under two query propagation mechanisms: the random walk and the expanding ring. In particular, we characterize how the query load at the server, the load at peers as well as the query response time scale as the number of users in the peer-to-peer network increases. We show that, under a properly designed random walk propagation mechanism, hybrid peer-to-peer systems can support an unbounded number of users while requiring only bounded resources both at the server and at individual peers. This important result shows that hybrid peer-to-peer systems have excellent scalability properties. To the best of our knowledge, this is the first time that a theoretical study characterizing the scalability of such hybrid peer-to-peer systems has been presented. We illustrate our results through numerical studies.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375476},
 doi = {http://doi.acm.org/10.1145/1375457.1375476},
 acmid = {1375476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {peer-to-peer, scalability},
} 

@article{Ioannidis:2008:DHP:1384529.1375476,
 author = {Ioannidis, Stratis and Marbach, Peter},
 title = {On the design of hybrid peer-to-peer systems},
 abstract = {In this paper, we consider hybrid peer-to-peer systems where users form an unstructured peer-to-peer network with the purpose of assisting a server in the distribution of data. We present a mathematical model that we use to analyze the scalability of hybrid peer-to-peer systems under two query propagation mechanisms: the random walk and the expanding ring. In particular, we characterize how the query load at the server, the load at peers as well as the query response time scale as the number of users in the peer-to-peer network increases. We show that, under a properly designed random walk propagation mechanism, hybrid peer-to-peer systems can support an unbounded number of users while requiring only bounded resources both at the server and at individual peers. This important result shows that hybrid peer-to-peer systems have excellent scalability properties. To the best of our knowledge, this is the first time that a theoretical study characterizing the scalability of such hybrid peer-to-peer systems has been presented. We illustrate our results through numerical studies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375476},
 doi = {http://doi.acm.org/10.1145/1384529.1375476},
 acmid = {1375476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {peer-to-peer, scalability},
} 

@article{Chen:2008:UMP:1384529.1375477,
 author = {Chen, Minghua and Ponec, Miroslav and Sengupta, Sudipta and Li, Jin and Chou, Philip A.},
 title = {Utility maximization in peer-to-peer systems},
 abstract = {In this paper, we study the problem of utility maximization in P2P systems, in which aggregate application-specific utilities are maximized by running distributed algorithms on P2P nodes, which are constrained by their uplink capacities. This may be understood as extending Kelly's seminal framework from single-path unicast over general topology to multi-path multicast over P2P topology, with network coding allowed. For certain classes of popular P2P topologies, we show that routing along a linear number of trees per source can achieve the largest rate region that can be possibly obtained by (multi-source) network coding. This simplification result allows us to develop a new multi-tree routing formulation for the problem. Despite of the negative results in literature on applying Primal-dual algorithms to maximize utility under multi-path settings, we have been able to develop a Primal-dual distributed algorithm to maximize the aggregate utility under the multi-path routing environments. Utilizing our proposed sufficient condition, we show global exponential convergence of the Primal-dual algorithm to the optimal solution under different P2P communication scenarios we study. The algorithm can be implemented by utilizing only end-to-end delay measurements between P2P nodes; hence, it can be readily deployed on today's Internet. To support this claim, we have implemented the Primal-dual algorithm for use in a peer-assisted multi-party conferencing system and evaluated its performance through actual experiments on a LAN testbed and the Internet.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375477},
 doi = {http://doi.acm.org/10.1145/1384529.1375477},
 acmid = {1375477},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, multi-party video conferencing, multicast, peer-to-peer, streaming, utility maximization},
} 

@inproceedings{Chen:2008:UMP:1375457.1375477,
 author = {Chen, Minghua and Ponec, Miroslav and Sengupta, Sudipta and Li, Jin and Chou, Philip A.},
 title = {Utility maximization in peer-to-peer systems},
 abstract = {In this paper, we study the problem of utility maximization in P2P systems, in which aggregate application-specific utilities are maximized by running distributed algorithms on P2P nodes, which are constrained by their uplink capacities. This may be understood as extending Kelly's seminal framework from single-path unicast over general topology to multi-path multicast over P2P topology, with network coding allowed. For certain classes of popular P2P topologies, we show that routing along a linear number of trees per source can achieve the largest rate region that can be possibly obtained by (multi-source) network coding. This simplification result allows us to develop a new multi-tree routing formulation for the problem. Despite of the negative results in literature on applying Primal-dual algorithms to maximize utility under multi-path settings, we have been able to develop a Primal-dual distributed algorithm to maximize the aggregate utility under the multi-path routing environments. Utilizing our proposed sufficient condition, we show global exponential convergence of the Primal-dual algorithm to the optimal solution under different P2P communication scenarios we study. The algorithm can be implemented by utilizing only end-to-end delay measurements between P2P nodes; hence, it can be readily deployed on today's Internet. To support this claim, we have implemented the Primal-dual algorithm for use in a peer-assisted multi-party conferencing system and evaluated its performance through actual experiments on a LAN testbed and the Internet.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375477},
 doi = {http://doi.acm.org/10.1145/1375457.1375477},
 acmid = {1375477},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, multi-party video conferencing, multicast, peer-to-peer, streaming, utility maximization},
} 

@inproceedings{Simatos:2008:QSM:1375457.1375478,
 author = {Simatos, Florian and Robert, Philippe and Guillemin, Fabrice},
 title = {A queueing system for modeling a file sharing principle},
 abstract = {We investigate in this paper the performance of a simple file sharing principle. For this purpose, we consider a system composed of N peers becoming active at exponential random times; the system is initiated with only one server offering the desired file and the other peers after becoming active try to download it. Once the file has been downloaded by a peer, this one immediately becomes a server. To investigate the transient behavior of this file sharing system, we study the instant when the system shifts from a congested state where all servers available are saturated by incoming demands to a state where a growing number of servers are idle. In spite of its apparent simplicity, this queueing model (with a random number of servers) turns out to be quite difficult to analyze. A formulation in terms of an urn and ball model is proposed and corresponding scaling results are derived. These asymptotic results are then compared against simulations.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375478},
 doi = {http://doi.acm.org/10.1145/1375457.1375478},
 acmid = {1375478},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {file sharing, peer to peer, queueing systems, transient analysis of markov processes},
} 

@article{Simatos:2008:QSM:1384529.1375478,
 author = {Simatos, Florian and Robert, Philippe and Guillemin, Fabrice},
 title = {A queueing system for modeling a file sharing principle},
 abstract = {We investigate in this paper the performance of a simple file sharing principle. For this purpose, we consider a system composed of N peers becoming active at exponential random times; the system is initiated with only one server offering the desired file and the other peers after becoming active try to download it. Once the file has been downloaded by a peer, this one immediately becomes a server. To investigate the transient behavior of this file sharing system, we study the instant when the system shifts from a congested state where all servers available are saturated by incoming demands to a state where a growing number of servers are idle. In spite of its apparent simplicity, this queueing model (with a random number of servers) turns out to be quite difficult to analyze. A formulation in terms of an urn and ball model is proposed and corresponding scaling results are derived. These asymptotic results are then compared against simulations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375478},
 doi = {http://doi.acm.org/10.1145/1384529.1375478},
 acmid = {1375478},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {file sharing, peer to peer, queueing systems, transient analysis of markov processes},
} 

@article{Goldberg:2008:PMP:1384529.1375480,
 author = {Goldberg, Sharon and Xiao, David and Tromer, Eran and Barak, Boaz and Rexford, Jennifer},
 title = {Path-quality monitoring in the presence of adversaries},
 abstract = {Edge networks connected to the Internet need effective monitoring techniques to drive routing decisions and detect violations of Service Level Agreements (SLAs). However, existing measurement tools, like ping, traceroute, and trajectory sampling, are vulnerable to attacks that can make a path look better than it really is. In this paper, we design and analyze path-quality monitoring protocols that reliably raise an alarm when the packet-loss rate and delay exceed a threshold, even when an adversary tries to bias monitoring results by selectively delaying, dropping, modifying, injecting, or preferentially treating packets. Despite the strong threat model we consider in this paper, our protocols are efficient enough to run at line rate on high-speed routers. We present a secure sketching protocol for identifying when packet loss and delay degrade beyond a threshold. This protocol is extremely lightweight, requiring only 250-600 bytes of storage and periodic transmission of a comparably sized IP packet to monitor billions of packets. We also present secure sampling protocols that provide faster feedback and accurate round-trip delay estimates, at the expense of somewhat higher storage and communication costs. We prove that all our protocols satisfy a precise definition of secure path-quality monitoring and derive analytic expressions for the trade-off between statistical accuracy and system overhead. We also compare how our protocols perform in the client-server setting, when paths are asymmetric, and when packet marking is not permitted.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375480},
 doi = {http://doi.acm.org/10.1145/1384529.1375480},
 acmid = {1375480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cryptography, path-quality monitoring, sampling, sketching},
} 

@inproceedings{Goldberg:2008:PMP:1375457.1375480,
 author = {Goldberg, Sharon and Xiao, David and Tromer, Eran and Barak, Boaz and Rexford, Jennifer},
 title = {Path-quality monitoring in the presence of adversaries},
 abstract = {Edge networks connected to the Internet need effective monitoring techniques to drive routing decisions and detect violations of Service Level Agreements (SLAs). However, existing measurement tools, like ping, traceroute, and trajectory sampling, are vulnerable to attacks that can make a path look better than it really is. In this paper, we design and analyze path-quality monitoring protocols that reliably raise an alarm when the packet-loss rate and delay exceed a threshold, even when an adversary tries to bias monitoring results by selectively delaying, dropping, modifying, injecting, or preferentially treating packets. Despite the strong threat model we consider in this paper, our protocols are efficient enough to run at line rate on high-speed routers. We present a secure sketching protocol for identifying when packet loss and delay degrade beyond a threshold. This protocol is extremely lightweight, requiring only 250-600 bytes of storage and periodic transmission of a comparably sized IP packet to monitor billions of packets. We also present secure sampling protocols that provide faster feedback and accurate round-trip delay estimates, at the expense of somewhat higher storage and communication costs. We prove that all our protocols satisfy a precise definition of secure path-quality monitoring and derive analytic expressions for the trade-off between statistical accuracy and system overhead. We also compare how our protocols perform in the client-server setting, when paths are asymmetric, and when packet marking is not permitted.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375480},
 doi = {http://doi.acm.org/10.1145/1375457.1375480},
 acmid = {1375480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cryptography, path-quality monitoring, sampling, sketching},
} 

@article{Pedarsani:2008:DAS:1384529.1375481,
 author = {Pedarsani, Pedram and Figueiredo, Daniel R. and Grossglauser, Matthias},
 title = {Densification arising from sampling fixed graphs},
 abstract = {During the past decade, a number of different studies have identified several peculiar properties of networks that arise from a diverse universe, ranging from social to computer networks. A recently observed feature is known as network densification, which occurs when the number of edges grows much faster than the number of nodes, as the network evolves over time. This surprising phenomenon has been empirically validated in a variety of networks that emerge in the real world and mathematical models have been recently proposed to explain it. Leveraging on how real data is usually gathered and used, we propose a new model called Edge Sampling to explain how densification can arise. Our model is innovative, as we consider a fixed underlying graph and a process that discovers this graph by probabilistically sampling its edges. We show that this model possesses several interesting features, in particular, that edges and nodes discovered can exhibit densification. Moreover, when the node degree of the fixed underlying graph follows a heavy-tailed distribution, we show that the Edge Sampling model can yield power law densification, establishing an approximate relationship between the degree exponent and the densification exponent. The theoretical findings are supported by numerical evaluations of the model. Finally, we apply our model to real network data to evaluate its performance on capturing the previously observed densification. Our results indicate that edge sampling is indeed a plausible alternative explanation for the densification phenomenon that has been recently observed.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375481},
 doi = {http://doi.acm.org/10.1145/1384529.1375481},
 acmid = {1375481},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {densification, edge sampling, network modeling},
} 

@inproceedings{Pedarsani:2008:DAS:1375457.1375481,
 author = {Pedarsani, Pedram and Figueiredo, Daniel R. and Grossglauser, Matthias},
 title = {Densification arising from sampling fixed graphs},
 abstract = {During the past decade, a number of different studies have identified several peculiar properties of networks that arise from a diverse universe, ranging from social to computer networks. A recently observed feature is known as network densification, which occurs when the number of edges grows much faster than the number of nodes, as the network evolves over time. This surprising phenomenon has been empirically validated in a variety of networks that emerge in the real world and mathematical models have been recently proposed to explain it. Leveraging on how real data is usually gathered and used, we propose a new model called Edge Sampling to explain how densification can arise. Our model is innovative, as we consider a fixed underlying graph and a process that discovers this graph by probabilistically sampling its edges. We show that this model possesses several interesting features, in particular, that edges and nodes discovered can exhibit densification. Moreover, when the node degree of the fixed underlying graph follows a heavy-tailed distribution, we show that the Edge Sampling model can yield power law densification, establishing an approximate relationship between the degree exponent and the densification exponent. The theoretical findings are supported by numerical evaluations of the model. Finally, we apply our model to real network data to evaluate its performance on capturing the previously observed densification. Our results indicate that edge sampling is indeed a plausible alternative explanation for the densification phenomenon that has been recently observed.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375481},
 doi = {http://doi.acm.org/10.1145/1375457.1375481},
 acmid = {1375481},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {densification, edge sampling, network modeling},
} 

@article{Oliveira:2008:SEG:1384529.1375482,
 author = {Oliveira, Ricardo V. and Pei, Dan and Willinger, Walter and Zhang, Beichuan and Zhang, Lixia},
 title = {In search of the elusive ground truth: the internet's as-level connectivity structure},
 abstract = {Despite significant efforts to obtain an accurate picture of the Internet's actual connectivity structure at the level of individual autonomous systems (ASes), much has remained unknown in terms of the quality of the inferred AS maps that have been widely used by the research community. In this paper we assess the quality of the inferred Internet maps through case studies of a set of ASes. These case studies allow us to establish the ground truth of AS-level Internet connectivity between the set of ASes and their directly connected neighbors. They also enable a direct comparison between the ground truth and inferred topology maps and yield new insights into questions such as which parts of the actual topology are adequately captured by the inferred maps, and which parts are missing and why. This information is critical in assessing for what kinds of real-world networking problems the use of currently inferred AS maps or proposed AS topology models are, or are not, appropriate. More importantly, our newly gained insights also point to new directions towards building realistic and economically viable Internet topology maps.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375482},
 doi = {http://doi.acm.org/10.1145/1384529.1375482},
 acmid = {1375482},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, inter-domain routing, internet topology},
} 

@inproceedings{Oliveira:2008:SEG:1375457.1375482,
 author = {Oliveira, Ricardo V. and Pei, Dan and Willinger, Walter and Zhang, Beichuan and Zhang, Lixia},
 title = {In search of the elusive ground truth: the internet's as-level connectivity structure},
 abstract = {Despite significant efforts to obtain an accurate picture of the Internet's actual connectivity structure at the level of individual autonomous systems (ASes), much has remained unknown in terms of the quality of the inferred AS maps that have been widely used by the research community. In this paper we assess the quality of the inferred Internet maps through case studies of a set of ASes. These case studies allow us to establish the ground truth of AS-level Internet connectivity between the set of ASes and their directly connected neighbors. They also enable a direct comparison between the ground truth and inferred topology maps and yield new insights into questions such as which parts of the actual topology are adequately captured by the inferred maps, and which parts are missing and why. This information is critical in assessing for what kinds of real-world networking problems the use of currently inferred AS maps or proposed AS topology models are, or are not, appropriate. More importantly, our newly gained insights also point to new directions towards building realistic and economically viable Internet topology maps.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375482},
 doi = {http://doi.acm.org/10.1145/1375457.1375482},
 acmid = {1375482},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, inter-domain routing, internet topology},
} 

@inproceedings{Bao:2008:HPI:1375457.1375484,
 author = {Bao, Yungang and Chen, Mingyu and Ruan, Yuan and Liu, Li and Fan, Jianping and Yuan, Qingbo and Song, Bo and Xu, Jianwei},
 title = {HMTT: a platform independent full-system memory trace monitoring system},
 abstract = {Memory trace analysis is an important technology for architecture research, system software (i.e., OS, compiler) optimization, and application performance improvements. Many approaches have been used to track memory trace, such as simulation, binary instrumentation and hardware snooping. However, they usually have limitations of time, accuracy and capacity. In this paper we propose a platform independent memory trace monitoring system, which is able to track virtual memory reference trace of full systems (including OS, VMMs, libraries, and applications). The system adopts a DIMM-snooping mechanism that uses hardware boards plugged in DIMM slots to snoop. There are several advantages in this approach, such as fast, complete, undistorted, and portable. Three key techniques are proposed to address the system design challenges with this mechanism: (1) To keep up with memory speeds, the DDR protocol state machine is simplified, and large FIFOs are added between the state machine and the trace transmitting logic to handle burst memory accesses; (2) To reconstruct physical-tovirtual mapping and distinguish one process' address space from others, an OS kernel module, which collects page table information, and a synchronization mechanism, which synchronizes the page table information with the memory race, are developed; (3) To dump massive trace data, we employ a straightforward method to compress the trace and use Gigabit Ethernet and RAID to send and receive the compressed trace. We present our implementation of an initial monitoring system, named HMTT (Hyper Memory Trace Tracker). Using HMTT, we have observed that burst bandwidth utilization is much larger than average bandwidth utilization, by up to 5X in desktop applications. We have also confirmed that the stream memory accesses of many applications contribute even more than 40\% of L2 Cache misses and OS virtual memory management may decrease stream accesses in view of memory controller (or L2 Cache), by up to 30.2\%. Moreover, we have evaluated OS impact on memory performance in real systems. The evaluations and case studies show the feasibility and effectiveness of our proposed monitoring mechanism and techniques.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375484},
 doi = {http://doi.acm.org/10.1145/1375457.1375484},
 acmid = {1375484},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DIMM, HMTT, memory trace, real system},
} 

@article{Bao:2008:HPI:1384529.1375484,
 author = {Bao, Yungang and Chen, Mingyu and Ruan, Yuan and Liu, Li and Fan, Jianping and Yuan, Qingbo and Song, Bo and Xu, Jianwei},
 title = {HMTT: a platform independent full-system memory trace monitoring system},
 abstract = {Memory trace analysis is an important technology for architecture research, system software (i.e., OS, compiler) optimization, and application performance improvements. Many approaches have been used to track memory trace, such as simulation, binary instrumentation and hardware snooping. However, they usually have limitations of time, accuracy and capacity. In this paper we propose a platform independent memory trace monitoring system, which is able to track virtual memory reference trace of full systems (including OS, VMMs, libraries, and applications). The system adopts a DIMM-snooping mechanism that uses hardware boards plugged in DIMM slots to snoop. There are several advantages in this approach, such as fast, complete, undistorted, and portable. Three key techniques are proposed to address the system design challenges with this mechanism: (1) To keep up with memory speeds, the DDR protocol state machine is simplified, and large FIFOs are added between the state machine and the trace transmitting logic to handle burst memory accesses; (2) To reconstruct physical-tovirtual mapping and distinguish one process' address space from others, an OS kernel module, which collects page table information, and a synchronization mechanism, which synchronizes the page table information with the memory race, are developed; (3) To dump massive trace data, we employ a straightforward method to compress the trace and use Gigabit Ethernet and RAID to send and receive the compressed trace. We present our implementation of an initial monitoring system, named HMTT (Hyper Memory Trace Tracker). Using HMTT, we have observed that burst bandwidth utilization is much larger than average bandwidth utilization, by up to 5X in desktop applications. We have also confirmed that the stream memory accesses of many applications contribute even more than 40\% of L2 Cache misses and OS virtual memory management may decrease stream accesses in view of memory controller (or L2 Cache), by up to 30.2\%. Moreover, we have evaluated OS impact on memory performance in real systems. The evaluations and case studies show the feasibility and effectiveness of our proposed monitoring mechanism and techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375484},
 doi = {http://doi.acm.org/10.1145/1384529.1375484},
 acmid = {1375484},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DIMM, HMTT, memory trace, real system},
} 

@inproceedings{Iliadis:2008:DSV:1375457.1375485,
 author = {Iliadis, Ilias and Haas, Robert and Hu, Xiao-Yu and Eleftheriou, Evangelos},
 title = {Disk scrubbing versus intra-disk redundancy for high-reliability raid storage systems},
 abstract = {Two schemes proposed to cope with unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used disk scrubbing scheme, which operates by periodically accessing disk drives to detect media-related unrecoverable errors. These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme which uses a further level of redundancy inside each disk, in addition to the RAID redundancy across multiple disks. Analytic results are obtained assuming Poisson arrivals of random I/O requests. Our results demonstrate that the reliability improvement due to disk scrubbing depends on the scrubbing frequency and the workload of the system, and may not reach the reliability level achieved by a simple IPC-based intra-disk redundancy scheme, which is insensitive to the workload. In fact, the IPC-based intra-disk redundancy scheme achieves essentially the same reliability as that of a system operating without unrecoverable sector errors. For heavy workloads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intra-disk redundancy scheme.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375485},
 doi = {http://doi.acm.org/10.1145/1375457.1375485},
 acmid = {1375485},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MTTDL, RAID, reliability analysis, stochastic modeling, unrecoverable or latent sector errors},
} 

@article{Iliadis:2008:DSV:1384529.1375485,
 author = {Iliadis, Ilias and Haas, Robert and Hu, Xiao-Yu and Eleftheriou, Evangelos},
 title = {Disk scrubbing versus intra-disk redundancy for high-reliability raid storage systems},
 abstract = {Two schemes proposed to cope with unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used disk scrubbing scheme, which operates by periodically accessing disk drives to detect media-related unrecoverable errors. These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme which uses a further level of redundancy inside each disk, in addition to the RAID redundancy across multiple disks. Analytic results are obtained assuming Poisson arrivals of random I/O requests. Our results demonstrate that the reliability improvement due to disk scrubbing depends on the scrubbing frequency and the workload of the system, and may not reach the reliability level achieved by a simple IPC-based intra-disk redundancy scheme, which is insensitive to the workload. In fact, the IPC-based intra-disk redundancy scheme achieves essentially the same reliability as that of a system operating without unrecoverable sector errors. For heavy workloads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intra-disk redundancy scheme.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375485},
 doi = {http://doi.acm.org/10.1145/1384529.1375485},
 acmid = {1375485},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MTTDL, RAID, reliability analysis, stochastic modeling, unrecoverable or latent sector errors},
} 

@article{Thereska:2008:IRP:1384529.1375486,
 author = {Thereska, Eno and Ganger, Gregory R.},
 title = {Ironmodel: robust performance models in the wild},
 abstract = {Traditional performance models are too brittle to be relied on for continuous capacity planning and performance debugging in many computer systems. Simply put, a brittle model is often inaccurate and incorrect. We find two types of reasons why a model's prediction might diverge from the reality: (1) the underlying system might be misconfigured or buggy or (2) the model's assumptions might be incorrect. The extra effort of manually finding and fixing the source of these discrepancies, continuously, in both the system and model, is one reason why many system designers and administrators avoid using mathematical models altogether. Instead, they opt for simple, but often inaccurate, "rules-of-thumb". This paper describes IRONModel, a robust performance modeling architecture. Through studying performance anomalies encountered in an experimental cluster-based storage system, we analyze why and how models and actual system implementations get out-of-sync. Lessons learned from that study are incorporated into IRONModel. IRONModel leverages the redundancy of high-level system specifications described through models and low-level system implementation to localize many types of system-model inconsistencies. IRONModel can guide designers to the potential source of the discrepancy, and, if appropriate, can semi-automatically evolve the models to handle unanticipated inputs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375486},
 doi = {http://doi.acm.org/10.1145/1384529.1375486},
 acmid = {1375486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active probing, behavioral modeling, management, what-if},
} 

@inproceedings{Thereska:2008:IRP:1375457.1375486,
 author = {Thereska, Eno and Ganger, Gregory R.},
 title = {Ironmodel: robust performance models in the wild},
 abstract = {Traditional performance models are too brittle to be relied on for continuous capacity planning and performance debugging in many computer systems. Simply put, a brittle model is often inaccurate and incorrect. We find two types of reasons why a model's prediction might diverge from the reality: (1) the underlying system might be misconfigured or buggy or (2) the model's assumptions might be incorrect. The extra effort of manually finding and fixing the source of these discrepancies, continuously, in both the system and model, is one reason why many system designers and administrators avoid using mathematical models altogether. Instead, they opt for simple, but often inaccurate, "rules-of-thumb". This paper describes IRONModel, a robust performance modeling architecture. Through studying performance anomalies encountered in an experimental cluster-based storage system, we analyze why and how models and actual system implementations get out-of-sync. Lessons learned from that study are incorporated into IRONModel. IRONModel leverages the redundancy of high-level system specifications described through models and low-level system implementation to localize many types of system-model inconsistencies. IRONModel can guide designers to the potential source of the discrepancy, and, if appropriate, can semi-automatically evolve the models to handle unanticipated inputs.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375486},
 doi = {http://doi.acm.org/10.1145/1375457.1375486},
 acmid = {1375486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active probing, behavioral modeling, management, what-if},
} 

@article{Liu:2008:XFS:1384529.1375488,
 author = {Liu, Alex X. and Chen, Fei and Hwang, JeeHyun and Xie, Tao},
 title = {Xengine: a fast and scalable XACML policy evaluation engine},
 abstract = {XACML has become the de facto standard for specifying access control policies for various applications, especially web services. With the explosive growth of web applications deployed on the Internet, XACML policies grow rapidly in size and complexity, which leads to longer request processing time. This paper concerns the performance of request processing, which is a critical issue and so far has been overlooked by the research community. In this paper, we propose XEngine, a scheme for efficient XACML policy evaluation. XEngine first converts a textual XACML policy to a numerical policy. Second, it converts a numerical policy with complex structures to a numerical policy with a normalized structure. Third, it converts the normalized numerical policy to tree data structures for efficient processing of requests. To evaluate the performance of XEngine, we conducted extensive experiments on both real-life and synthetic XACML policies. The experimental results show that XEngine is orders of magnitude more efficient than Sun PDP, and the performance difference between XEngine and Sun PDP grows almost linearly with the number of rules in XACML policies. For XACML policies of small sizes (with hundreds of rules), XEngine is one to two orders of magnitude faster than the widely deployed Sun PDP. For XACML policies of large sizes (with thousands of rules), XEngine is three to four orders of magnitude faster than Sun PDP.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375488},
 doi = {http://doi.acm.org/10.1145/1384529.1375488},
 acmid = {1375488},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {XACML, access control, policy decision point (PDP), policy enforcement point (PEP), policy evaluation, web server},
} 

@inproceedings{Liu:2008:XFS:1375457.1375488,
 author = {Liu, Alex X. and Chen, Fei and Hwang, JeeHyun and Xie, Tao},
 title = {Xengine: a fast and scalable XACML policy evaluation engine},
 abstract = {XACML has become the de facto standard for specifying access control policies for various applications, especially web services. With the explosive growth of web applications deployed on the Internet, XACML policies grow rapidly in size and complexity, which leads to longer request processing time. This paper concerns the performance of request processing, which is a critical issue and so far has been overlooked by the research community. In this paper, we propose XEngine, a scheme for efficient XACML policy evaluation. XEngine first converts a textual XACML policy to a numerical policy. Second, it converts a numerical policy with complex structures to a numerical policy with a normalized structure. Third, it converts the normalized numerical policy to tree data structures for efficient processing of requests. To evaluate the performance of XEngine, we conducted extensive experiments on both real-life and synthetic XACML policies. The experimental results show that XEngine is orders of magnitude more efficient than Sun PDP, and the performance difference between XEngine and Sun PDP grows almost linearly with the number of rules in XACML policies. For XACML policies of small sizes (with hundreds of rules), XEngine is one to two orders of magnitude faster than the widely deployed Sun PDP. For XACML policies of large sizes (with thousands of rules), XEngine is three to four orders of magnitude faster than Sun PDP.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375488},
 doi = {http://doi.acm.org/10.1145/1375457.1375488},
 acmid = {1375488},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {XACML, access control, policy decision point (PDP), policy enforcement point (PEP), policy evaluation, web server},
} 

@inproceedings{Traeger:2008:DDA:1375457.1375489,
 author = {Traeger, Avishay and Deras, Ivan and Zadok, Erez},
 title = {DARC: dynamic analysis of root causes of latency distributions},
 abstract = {OSprof is a versatile, portable, and efficient profiling methodology based on the analysis of latency distributions. Although OSprof has offers several unique benefits and has been used to uncover several interesting performance problems, the latency distributions that it provides must be analyzed manually. These latency distributions are presented as histograms and contain distinct groups of data, called peaks, that characterize the overall behavior of the running code. By automating the analysis process, we make it easier to take advantage of OSprof's unique features. We have developed the Dynamic Analysis of Root Causes system (DARC), which finds root cause paths in a running program's call-graph using runtime latency analysis. A root cause path is a call-path that starts at a given function and includes the largest latency contributors to a given peak. These paths are the main causes for the high-level behavior that is represented as a peak in an OSprof histogram. DARC performs PID and call-path filtering to reduce overheads and perturbations, and can handle recursive and indirect calls. DARC can analyze preemptive behavior and asynchronous call-paths, and can also resume its analysis from a previous state, which is useful when analyzing short-running programs or specific phases of a program's execution. We present DARC and show its usefulness by analyzing behaviors that were observed in several interesting scenarios. We also show that DARC has negligible elapsed time overheads for normal use cases.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375489},
 doi = {http://doi.acm.org/10.1145/1375457.1375489},
 acmid = {1375489},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic instrumentation, root cause},
} 

@article{Traeger:2008:DDA:1384529.1375489,
 author = {Traeger, Avishay and Deras, Ivan and Zadok, Erez},
 title = {DARC: dynamic analysis of root causes of latency distributions},
 abstract = {OSprof is a versatile, portable, and efficient profiling methodology based on the analysis of latency distributions. Although OSprof has offers several unique benefits and has been used to uncover several interesting performance problems, the latency distributions that it provides must be analyzed manually. These latency distributions are presented as histograms and contain distinct groups of data, called peaks, that characterize the overall behavior of the running code. By automating the analysis process, we make it easier to take advantage of OSprof's unique features. We have developed the Dynamic Analysis of Root Causes system (DARC), which finds root cause paths in a running program's call-graph using runtime latency analysis. A root cause path is a call-path that starts at a given function and includes the largest latency contributors to a given peak. These paths are the main causes for the high-level behavior that is represented as a peak in an OSprof histogram. DARC performs PID and call-path filtering to reduce overheads and perturbations, and can handle recursive and indirect calls. DARC can analyze preemptive behavior and asynchronous call-paths, and can also resume its analysis from a previous state, which is useful when analyzing short-running programs or specific phases of a program's execution. We present DARC and show its usefulness by analyzing behaviors that were observed in several interesting scenarios. We also show that DARC has negligible elapsed time overheads for normal use cases.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {277--288},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375489},
 doi = {http://doi.acm.org/10.1145/1384529.1375489},
 acmid = {1375489},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic instrumentation, root cause},
} 

@article{Chaitanya:2008:QQM:1384529.1375490,
 author = {Chaitanya, Shiva and Urgaonkar, Bhuvan and Sivasubramaniam, Anand},
 title = {QDSL: a queuing model for systems with differential service levels},
 abstract = {A feature exhibited by many modern computing systems is their ability to improve the quality of output they generate for a given input by spending more computing resources on processing it. Often this improvement comes at the price of degraded performance in the form of reduced throughput or increased response time. We formulate QDSL, a class of constrained optimization problems defined in the context of a queueing server equipped with multiple levels of service. Solutions to QDSL provide rules for dynamically varying the service level to achieve desired trade-offs between output quality and performance. Our approach involves reducing restricted versions of such systems to Markov Decision Processes. We find two variants of such systems worth studying: (i) VarSL, in which a single request may be serviced using a combination of multiple levels during its lifetime and (ii) FixSL in which the service level may not change during the lifetime of a request. Our modeling indicates that optimal service level selection policies in these systems correspond to very simple rules that can be implemented very efficiently in realistic, online systems. We find our policies to be useful in two response-time-sensitive real-world systems: (i) qSecStore, an iSCSI-based secure storage system that has access to multiple encryption functions, and (ii) qPowServer, a server with DVFS-capable processor. As a representative result, in an instance of qSecStore serving disk requests derived from the well-regarded TPC-H traces, we are able to improve the fraction of requests using more reliable encryption functions by 40-60\%, while meeting performance targets. In a simulation of qPowServer employing realistic DVFS parameters, we are able to improve response times significantly while only violating specified server-wide power budgets by less than 5W.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375490},
 doi = {http://doi.acm.org/10.1145/1384529.1375490},
 acmid = {1375490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {differential service levels, dynamic voltage frequency scaling, markov decision process, secure storage},
} 

@inproceedings{Chaitanya:2008:QQM:1375457.1375490,
 author = {Chaitanya, Shiva and Urgaonkar, Bhuvan and Sivasubramaniam, Anand},
 title = {QDSL: a queuing model for systems with differential service levels},
 abstract = {A feature exhibited by many modern computing systems is their ability to improve the quality of output they generate for a given input by spending more computing resources on processing it. Often this improvement comes at the price of degraded performance in the form of reduced throughput or increased response time. We formulate QDSL, a class of constrained optimization problems defined in the context of a queueing server equipped with multiple levels of service. Solutions to QDSL provide rules for dynamically varying the service level to achieve desired trade-offs between output quality and performance. Our approach involves reducing restricted versions of such systems to Markov Decision Processes. We find two variants of such systems worth studying: (i) VarSL, in which a single request may be serviced using a combination of multiple levels during its lifetime and (ii) FixSL in which the service level may not change during the lifetime of a request. Our modeling indicates that optimal service level selection policies in these systems correspond to very simple rules that can be implemented very efficiently in realistic, online systems. We find our policies to be useful in two response-time-sensitive real-world systems: (i) qSecStore, an iSCSI-based secure storage system that has access to multiple encryption functions, and (ii) qPowServer, a server with DVFS-capable processor. As a representative result, in an instance of qSecStore serving disk requests derived from the well-regarded TPC-H traces, we are able to improve the fraction of requests using more reliable encryption functions by 40-60\%, while meeting performance targets. In a simulation of qPowServer employing realistic DVFS parameters, we are able to improve response times significantly while only violating specified server-wide power budgets by less than 5W.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {289--300},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375490},
 doi = {http://doi.acm.org/10.1145/1375457.1375490},
 acmid = {1375490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {differential service levels, dynamic voltage frequency scaling, markov decision process, secure storage},
} 

@inproceedings{Parvez:2008:ABP:1375457.1375492,
 author = {Parvez, Nadim and Williamson, Carey and Mahanti, Anirban and Carlsson, Niklas},
 title = {Analysis of bittorrent-like protocols for on-demand stored media streaming},
 abstract = {This paper develops analytic models that characterize the behavior of on-demand stored media content delivery using BitTorrent-like protocols. The models capture the effects of different piece selection policies, including Rarest-First and two variants of In-Order. Our models provide insight into transient and steady-state system behavior, and help explain the sluggishness of the system with strict In-Order streaming. We use the models to compare different retrieval policies across a wide range of system parameters, including peer arrival rate, upload/download bandwidth, and seed residence time. We also provide quantitative results on the startup delays and retrieval times for streaming media delivery. Our results provide insights into the optimal design of peer-to-peer networks for on-demand media streaming.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375492},
 doi = {http://doi.acm.org/10.1145/1375457.1375492},
 acmid = {1375492},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bittorrent, on-demand streaming, peer-to-peer systems},
} 

@article{Parvez:2008:ABP:1384529.1375492,
 author = {Parvez, Nadim and Williamson, Carey and Mahanti, Anirban and Carlsson, Niklas},
 title = {Analysis of bittorrent-like protocols for on-demand stored media streaming},
 abstract = {This paper develops analytic models that characterize the behavior of on-demand stored media content delivery using BitTorrent-like protocols. The models capture the effects of different piece selection policies, including Rarest-First and two variants of In-Order. Our models provide insight into transient and steady-state system behavior, and help explain the sluggishness of the system with strict In-Order streaming. We use the models to compare different retrieval policies across a wide range of system parameters, including peer arrival rate, upload/download bandwidth, and seed residence time. We also provide quantitative results on the startup delays and retrieval times for streaming media delivery. Our results provide insights into the optimal design of peer-to-peer networks for on-demand media streaming.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {301--312},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375492},
 doi = {http://doi.acm.org/10.1145/1384529.1375492},
 acmid = {1375492},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bittorrent, on-demand streaming, peer-to-peer systems},
} 

@inproceedings{Liu:2008:PBP:1375457.1375493,
 author = {Liu, Shao and Zhang-Shen, Rui and Jiang, Wenjie and Rexford, Jennifer and Chiang, Mung},
 title = {Performance bounds for peer-assisted live streaming},
 abstract = {Peer-assisted streaming is a promising way for service providers to offer high-quality IPTV to consumers at reasonable cost. In peer-assisted streaming, the peers exchange video chunks with one another, and receive additional data from the central server as needed. In this paper, we analyze how to provision resources for the streaming system, in terms of the server capacity, the video quality, and the depth of the distribution trees that deliver the content. We derive the performance bounds for minimum server load, maximum streaming rate, and minimum tree depth under different peer selection constraints. Furthermore, we show that our performance bounds are actually tight, by presenting algorithms for constructing trees that achieve our bounds.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {313--324},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375493},
 doi = {http://doi.acm.org/10.1145/1375457.1375493},
 acmid = {1375493},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IPTV, peer-to-peer, streaming, tree construction, video},
} 

@article{Liu:2008:PBP:1384529.1375493,
 author = {Liu, Shao and Zhang-Shen, Rui and Jiang, Wenjie and Rexford, Jennifer and Chiang, Mung},
 title = {Performance bounds for peer-assisted live streaming},
 abstract = {Peer-assisted streaming is a promising way for service providers to offer high-quality IPTV to consumers at reasonable cost. In peer-assisted streaming, the peers exchange video chunks with one another, and receive additional data from the central server as needed. In this paper, we analyze how to provision resources for the streaming system, in terms of the server capacity, the video quality, and the depth of the distribution trees that deliver the content. We derive the performance bounds for minimum server load, maximum streaming rate, and minimum tree depth under different peer selection constraints. Furthermore, we show that our performance bounds are actually tight, by presenting algorithms for constructing trees that achieve our bounds.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {313--324},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375493},
 doi = {http://doi.acm.org/10.1145/1384529.1375493},
 acmid = {1375493},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IPTV, peer-to-peer, streaming, tree construction, video},
} 

@inproceedings{Bonald:2008:ELS:1375457.1375494,
 author = {Bonald, Thomas and Massouli\'{e}, Laurent and Mathieu, Fabien and Perino, Diego and Twigg, Andrew},
 title = {Epidemic live streaming: optimal performance trade-offs},
 abstract = {Several peer-to-peer systems for live streaming have been recently deployed (e.g. CoolStreaming, PPLive, SopCast). These all rely on distributed, epidemic-style dissemination mechanisms. Despite their popularity, the fundamental performance trade-offs of such mechanisms are still poorly understood. In this paper we propose several results that contribute to the understanding of such trade-offs. Specifically, we prove that the so-called random peer, latest useful chunk mechanism can achieve dissemination at an optimal rate and within an optimal delay, up to an additive constant term. This qualitative result suggests that epidemic live streaming algorithms can achieve near-unbeatable rates and delays. Using mean-field approximations, we also derive recursive formulas for the diffusion function of two schemes referred to as latest blind chunk, random peer and latest blind chunk, random useful peer. Finally, we provide simulation results that validate the above theoretical results and allow us to compare the performance of various practically interesting diffusion schemes terms of delay, rate, and control overhead. In particular, we identify several peer/chunk selection algorithms that achieve near-optimal performance trade-offs. Moreover, we show that the control overhead needed to implement these algorithms may be reduced by restricting the neighborhood of each peer without substantial performance degradation.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {325--336},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375494},
 doi = {http://doi.acm.org/10.1145/1375457.1375494},
 acmid = {1375494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay optimality, epidemic diffusion, p2p live streaming},
} 

@article{Bonald:2008:ELS:1384529.1375494,
 author = {Bonald, Thomas and Massouli\'{e}, Laurent and Mathieu, Fabien and Perino, Diego and Twigg, Andrew},
 title = {Epidemic live streaming: optimal performance trade-offs},
 abstract = {Several peer-to-peer systems for live streaming have been recently deployed (e.g. CoolStreaming, PPLive, SopCast). These all rely on distributed, epidemic-style dissemination mechanisms. Despite their popularity, the fundamental performance trade-offs of such mechanisms are still poorly understood. In this paper we propose several results that contribute to the understanding of such trade-offs. Specifically, we prove that the so-called random peer, latest useful chunk mechanism can achieve dissemination at an optimal rate and within an optimal delay, up to an additive constant term. This qualitative result suggests that epidemic live streaming algorithms can achieve near-unbeatable rates and delays. Using mean-field approximations, we also derive recursive formulas for the diffusion function of two schemes referred to as latest blind chunk, random peer and latest blind chunk, random useful peer. Finally, we provide simulation results that validate the above theoretical results and allow us to compare the performance of various practically interesting diffusion schemes terms of delay, rate, and control overhead. In particular, we identify several peer/chunk selection algorithms that achieve near-optimal performance trade-offs. Moreover, we show that the control overhead needed to implement these algorithms may be reduced by restricting the neighborhood of each peer without substantial performance degradation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {325--336},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375494},
 doi = {http://doi.acm.org/10.1145/1384529.1375494},
 acmid = {1375494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay optimality, epidemic diffusion, p2p live streaming},
} 

@article{Lin:2008:STM:1384529.1375496,
 author = {Lin, Jiang and Zheng, Hongzhong and Zhu, Zhichun and Gorbatov, Eugene and David, Howard and Zhang, Zhao},
 title = {Software thermal management of dram memory for multicore systems},
 abstract = {Thermal management of DRAM memory has become a critical issue for server systems. We have done, to our best knowledge, the first study of software thermal management for memory subsystem on real machines. Two recently proposed DTM (Dynamic Thermal Management) policies have been improved and implemented in Linux OS and evaluated on two multicore servers, a Dell PowerEdge 1950 server and a customized Intel SR1500AL server testbed. The experimental results first confirm that a system-level memory DTM policy may significantly improve system performance and power efficiency, compared with existing memory bandwidth throttling scheme. A policy called DTM-ACG (Adaptive Core Gating) shows performance improvement comparable to that reported previously. The average performance improvements are 13.3\% and 7.2\% on the PowerEdge 1950 and the SR1500AL (vs. 16.3\% from the previous simulation-based study), respectively. We also have surprising findings that reveal the weakness of the previous study: the CPU heat dissipation and its impact on DRAM memories, which were ignored, are significant factors. We have observed that the second policy, called DTM-CDVFS (Coordinated Dynamic Voltage and Frequency Scaling), has much better performance than previously reported for this reason. The average improvements are 10.8\% and 15.3\% on the two machines (vs. 3.4\% from the previous study), respectively. It also significantly reduces the processor power by 15.5\% and energy by 22.7\% on average.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375496},
 doi = {http://doi.acm.org/10.1145/1384529.1375496},
 acmid = {1375496},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DRAM memories, thermal management},
} 

@inproceedings{Lin:2008:STM:1375457.1375496,
 author = {Lin, Jiang and Zheng, Hongzhong and Zhu, Zhichun and Gorbatov, Eugene and David, Howard and Zhang, Zhao},
 title = {Software thermal management of dram memory for multicore systems},
 abstract = {Thermal management of DRAM memory has become a critical issue for server systems. We have done, to our best knowledge, the first study of software thermal management for memory subsystem on real machines. Two recently proposed DTM (Dynamic Thermal Management) policies have been improved and implemented in Linux OS and evaluated on two multicore servers, a Dell PowerEdge 1950 server and a customized Intel SR1500AL server testbed. The experimental results first confirm that a system-level memory DTM policy may significantly improve system performance and power efficiency, compared with existing memory bandwidth throttling scheme. A policy called DTM-ACG (Adaptive Core Gating) shows performance improvement comparable to that reported previously. The average performance improvements are 13.3\% and 7.2\% on the PowerEdge 1950 and the SR1500AL (vs. 16.3\% from the previous simulation-based study), respectively. We also have surprising findings that reveal the weakness of the previous study: the CPU heat dissipation and its impact on DRAM memories, which were ignored, are significant factors. We have observed that the second policy, called DTM-CDVFS (Coordinated Dynamic Voltage and Frequency Scaling), has much better performance than previously reported for this reason. The average improvements are 10.8\% and 15.3\% on the two machines (vs. 3.4\% from the previous study), respectively. It also significantly reduces the processor power by 15.5\% and energy by 22.7\% on average.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {337--348},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375496},
 doi = {http://doi.acm.org/10.1145/1375457.1375496},
 acmid = {1375496},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DRAM memories, thermal management},
} 

@inproceedings{Menache:2008:NPC:1375457.1375497,
 author = {Menache, Ishai and Shimkin, Nahum},
 title = {Noncooperative power control and transmission scheduling in wireless collision channels},
 abstract = {We consider a wireless collision channel, shared by a finite number of mobile users who transmit to a common base station using a random access protocol. Mobiles are self-optimizing, and wish to minimize their individual average power investment subject to minimum-throughput demand. The channel state between each mobile and the base station is stochastically time-varying and is observed by the mobile prior to transmission. Given the current channel state, a mobile may decide whether to transmit or not, and to determine the transmission power in case of transmission. In this paper, we investigate the properties of the Nash equilibrium of the resulting game in multiuser networks. We characterize the best-response strategy of the mobile and show that it leads to a "water-filling"-like power allocation. Our equilibrium analysis then reveals that one of the possible equilibria is uniformly best for all mobiles. Furthermore, this equilibrium can be reached by a simple distributed mechanism that does not require specific information on other mobiles' actions. We then explore some additional characteristics of the distributed power control framework. Braess-like paradoxes are reported, where the use of multiple power levels can diminish system capacity and also lead to larger per-user power consumption, compared to the case where a single level only is permitted.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {349--358},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1375457.1375497},
 doi = {http://doi.acm.org/10.1145/1375457.1375497},
 acmid = {1375497},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel state information, non-cooperative multiple access, power efficient nash equilibrium, uplink collision channel, water-filling power allocation},
} 

@article{Menache:2008:NPC:1384529.1375497,
 author = {Menache, Ishai and Shimkin, Nahum},
 title = {Noncooperative power control and transmission scheduling in wireless collision channels},
 abstract = {We consider a wireless collision channel, shared by a finite number of mobile users who transmit to a common base station using a random access protocol. Mobiles are self-optimizing, and wish to minimize their individual average power investment subject to minimum-throughput demand. The channel state between each mobile and the base station is stochastically time-varying and is observed by the mobile prior to transmission. Given the current channel state, a mobile may decide whether to transmit or not, and to determine the transmission power in case of transmission. In this paper, we investigate the properties of the Nash equilibrium of the resulting game in multiuser networks. We characterize the best-response strategy of the mobile and show that it leads to a "water-filling"-like power allocation. Our equilibrium analysis then reveals that one of the possible equilibria is uniformly best for all mobiles. Furthermore, this equilibrium can be reached by a simple distributed mechanism that does not require specific information on other mobiles' actions. We then explore some additional characteristics of the distributed power control framework. Braess-like paradoxes are reported, where the use of multiple power levels can diminish system capacity and also lead to larger per-user power consumption, compared to the case where a single level only is permitted.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {349--358},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1384529.1375497},
 doi = {http://doi.acm.org/10.1145/1384529.1375497},
 acmid = {1375497},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel state information, non-cooperative multiple access, power efficient nash equilibrium, uplink collision channel, water-filling power allocation},
} 

@article{Kandemir:2008:SCC:1384529.1375498,
 author = {Kandemir, Mahmut and Ozturk, Ozcan},
 title = {Software-directed combined cpu/link voltage scaling fornoc-based cmps},
 abstract = {Network-on-Chip (NoC) based chip multiprocessors (CMPs) are expected to become more widespread in future, in both high performance scientific computing and low-end embedded computing. For many execution environments that employ these systems, reducing power consumption is an important goal. This paper presents a software approach for reducing power consumption in such systems through compiler-directed voltage/frequency scaling. The unique characteristic of this approach is that it scales the voltages and frequencies of select CPUs and communication links in a coordinated manner to maximize energy savings without degrading performance. Our approach has three important components. The first component is the identification of phases in the application. The next step is to determine the critical execution paths and slacks in each phase. For implementing these two components, our approach employs a novel parallel program representation. The last component of our approach is the assignment of voltages and frequencies to CPUs and communication links to maximize energy savings. We use integer linear programming (ILP) for this voltage/frequency assignment problem. To test our approach, we implemented it within a compilation framework and conducted experiments with applications from the SPEComp suite and SPECjbb. Our results show that the proposed combined CPU/link scaling is much more effective than scaling voltages of CPUs or communication links in isolation. In addition, we observed that the energy savings obtained are consistent across a wide range of values of our major simulation parameters such as the number of CPUs, the number of voltage/frequency levels, and the thread-to-CPU mapping.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375498},
 doi = {http://doi.acm.org/10.1145/1384529.1375498},
 acmid = {1375498},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, NoC, communication link, compiler, cpu, voltage scaling},
} 

@inproceedings{Kandemir:2008:SCC:1375457.1375498,
 author = {Kandemir, Mahmut and Ozturk, Ozcan},
 title = {Software-directed combined cpu/link voltage scaling fornoc-based cmps},
 abstract = {Network-on-Chip (NoC) based chip multiprocessors (CMPs) are expected to become more widespread in future, in both high performance scientific computing and low-end embedded computing. For many execution environments that employ these systems, reducing power consumption is an important goal. This paper presents a software approach for reducing power consumption in such systems through compiler-directed voltage/frequency scaling. The unique characteristic of this approach is that it scales the voltages and frequencies of select CPUs and communication links in a coordinated manner to maximize energy savings without degrading performance. Our approach has three important components. The first component is the identification of phases in the application. The next step is to determine the critical execution paths and slacks in each phase. For implementing these two components, our approach employs a novel parallel program representation. The last component of our approach is the assignment of voltages and frequencies to CPUs and communication links to maximize energy savings. We use integer linear programming (ILP) for this voltage/frequency assignment problem. To test our approach, we implemented it within a compilation framework and conducted experiments with applications from the SPEComp suite and SPECjbb. Our results show that the proposed combined CPU/link scaling is much more effective than scaling voltages of CPUs or communication links in isolation. In addition, we observed that the energy savings obtained are consistent across a wide range of values of our major simulation parameters such as the number of CPUs, the number of voltage/frequency levels, and the thread-to-CPU mapping.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {359--370},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375498},
 doi = {http://doi.acm.org/10.1145/1375457.1375498},
 acmid = {1375498},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, NoC, communication link, compiler, cpu, voltage scaling},
} 

@inproceedings{Crk:2008:IEM:1375457.1375499,
 author = {Crk, Igor and Bi, Mingsong and Gniady, Chris},
 title = {Interaction-aware energy management for wireless network cards},
 abstract = {Wireless Network Interface Cards (WNICs) are part of every portable device, where efficient energy management plays a significant role in extending the device's battery life. The goal of efficient energy management is to match the performance of the WNIC to the network activity shaped by a running application. In the case of interactive applications on mobile systems, network I/O is largely driven by user interactions. Current solutions either require application modifications or lack a sufficient context of execution that is crucial in making accurate and timely predictions. This paper proposes a range of user-interaction-aware mechanisms that utilize a novel approach of monitoring a user's interaction with applications through the capture and classification of mouse events. This approach yields considerable improvements in energy savings and delay reductions of the WNIC, while significantly improving the accuracy, timeliness, and computational overhead of predictions when compared to existing state-of-the-art solutions.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {371--382},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375499},
 doi = {http://doi.acm.org/10.1145/1375457.1375499},
 acmid = {1375499},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-awareness, energy management, interaction monitoring, prediction, resource management, wireless network cards},
} 

@article{Crk:2008:IEM:1384529.1375499,
 author = {Crk, Igor and Bi, Mingsong and Gniady, Chris},
 title = {Interaction-aware energy management for wireless network cards},
 abstract = {Wireless Network Interface Cards (WNICs) are part of every portable device, where efficient energy management plays a significant role in extending the device's battery life. The goal of efficient energy management is to match the performance of the WNIC to the network activity shaped by a running application. In the case of interactive applications on mobile systems, network I/O is largely driven by user interactions. Current solutions either require application modifications or lack a sufficient context of execution that is crucial in making accurate and timely predictions. This paper proposes a range of user-interaction-aware mechanisms that utilize a novel approach of monitoring a user's interaction with applications through the capture and classification of mouse events. This approach yields considerable improvements in energy savings and delay reductions of the WNIC, while significantly improving the accuracy, timeliness, and computational overhead of predictions when compared to existing state-of-the-art solutions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {371--382},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375499},
 doi = {http://doi.acm.org/10.1145/1384529.1375499},
 acmid = {1375499},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-awareness, energy management, interaction monitoring, prediction, resource management, wireless network cards},
} 

@article{Stanojevi:2008:FDE:1384529.1375501,
 author = {Stanojevi, Rade and Shorten, Robert},
 title = {Fully decentralized emulation of best-effort and processor sharing queues},
 abstract = {Control of large distributed cloud-based services is a challenging problem. The Distributed Rate Limiting (DRL) paradigm was recently proposed as a mechanism for tackling this problem. The heuristic nature of existing DRL solutions makes their behavior unpredictable and analytically untractable. In this paper we treat the DRL problem in a mathematical framework and propose two novel DRL algorithms that exhibit good and predictable performance. The first algorithm Cloud Control with Constant Probabilities (C3P) solves the DRL problem in best effort environments, emulating the behavior of a single best-effort queue in a fully distributed manner. The second problem we approach is the DRL in processor sharing environments. Our algorithm, Distributed Deficit Round Robin (D2R2), parameterized by parameter \&#945;, converges to a state that is, at most, O</i>(1/\&#945;) away from the exact emulation of centralized processor sharing queue. The convergence and stability properties are fully analyzed for both C3P and D2R2. Analytical results are validated empirically through a number of representative packet level simulations. The closed-form nature of our results allows simple design rules which, together with extremely low communication overhead, makes the presented algorithms practical and easy to deploy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {383--394},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375501},
 doi = {http://doi.acm.org/10.1145/1384529.1375501},
 acmid = {1375501},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CDN, cloud control, consensus agreement, rate limiting, stability and convergence},
} 

@inproceedings{Stanojevi:2008:FDE:1375457.1375501,
 author = {Stanojevi, Rade and Shorten, Robert},
 title = {Fully decentralized emulation of best-effort and processor sharing queues},
 abstract = {Control of large distributed cloud-based services is a challenging problem. The Distributed Rate Limiting (DRL) paradigm was recently proposed as a mechanism for tackling this problem. The heuristic nature of existing DRL solutions makes their behavior unpredictable and analytically untractable. In this paper we treat the DRL problem in a mathematical framework and propose two novel DRL algorithms that exhibit good and predictable performance. The first algorithm Cloud Control with Constant Probabilities (C3P) solves the DRL problem in best effort environments, emulating the behavior of a single best-effort queue in a fully distributed manner. The second problem we approach is the DRL in processor sharing environments. Our algorithm, Distributed Deficit Round Robin (D2R2), parameterized by parameter \&#945;, converges to a state that is, at most, O</i>(1/\&#945;) away from the exact emulation of centralized processor sharing queue. The convergence and stability properties are fully analyzed for both C3P and D2R2. Analytical results are validated empirically through a number of representative packet level simulations. The closed-form nature of our results allows simple design rules which, together with extremely low communication overhead, makes the presented algorithms practical and easy to deploy.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {383--394},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375501},
 doi = {http://doi.acm.org/10.1145/1375457.1375501},
 acmid = {1375501},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CDN, cloud control, consensus agreement, rate limiting, stability and convergence},
} 

@article{Jagabathula:2008:ODS:1384529.1375502,
 author = {Jagabathula, Srikanth and Shah, Devavrat},
 title = {Optimal delay scheduling in networks with arbitrary constraints},
 abstract = {We consider the problem of designing an online scheduling scheme for a multi-hop wireless packet network with arbitrary topology and operating under arbitrary scheduling constraints. The objective is to design a scheme that achieves high throughput and low delay simultaneously. We propose a scheduling scheme that - for networks operating under primary interference constraints - guarantees a per-flow end-to-end packet delay bound of <sup>5d</i></sup>j</i>/(1-\&#961;<sub>j</sub></i>), at a factor 5 loss of throughput, where d<sub>j</sub></i> is the path length (number of hops) of flow j</i> and \&#961;<sub>j</sub></i> is the effective loading along the route of flow j</i>. Clearly, d<sub>j</sub></i> is a universal lower bound on end-to-end packet delay for flow j</i>. Thus, our result is essentially optimal. To the best of our knowledge, our result is the first one to show that it is possible to achieve a per-flow end-to-end delay bound of O</i>(# of hops) in a constrained network. Designing such a scheme comprises two related subproblems: Global Scheduling and Local Scheduling. Global Scheduling involves determining the set of links that will be simultaneously active, without violating the scheduling constraints. While local scheduling involves determining the packets that will be transferred across active edges. We design a local scheduling scheme by adapting the Preemptive Last-In-First-Out (PL) scheme, applied for quasi-reversible continuous time networks, to an unconstrained discrete-time network. A global scheduling scheme will be obtained by using stable marriage algorithms to emulate the unconstrained network with the constrained wireless network. Our scheme can be easily extended to a network operating under general scheduling constraints, such as secondary interference constraints, with the same delay bound and a loss of throughput that depends on scheduling constraints through an intriguing "sub-graph covering" property.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {395--406},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375502},
 doi = {http://doi.acm.org/10.1145/1384529.1375502},
 acmid = {1375502},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay, scheduling algorithm, throughput},
} 

@inproceedings{Jagabathula:2008:ODS:1375457.1375502,
 author = {Jagabathula, Srikanth and Shah, Devavrat},
 title = {Optimal delay scheduling in networks with arbitrary constraints},
 abstract = {We consider the problem of designing an online scheduling scheme for a multi-hop wireless packet network with arbitrary topology and operating under arbitrary scheduling constraints. The objective is to design a scheme that achieves high throughput and low delay simultaneously. We propose a scheduling scheme that - for networks operating under primary interference constraints - guarantees a per-flow end-to-end packet delay bound of <sup>5d</i></sup>j</i>/(1-\&#961;<sub>j</sub></i>), at a factor 5 loss of throughput, where d<sub>j</sub></i> is the path length (number of hops) of flow j</i> and \&#961;<sub>j</sub></i> is the effective loading along the route of flow j</i>. Clearly, d<sub>j</sub></i> is a universal lower bound on end-to-end packet delay for flow j</i>. Thus, our result is essentially optimal. To the best of our knowledge, our result is the first one to show that it is possible to achieve a per-flow end-to-end delay bound of O</i>(# of hops) in a constrained network. Designing such a scheme comprises two related subproblems: Global Scheduling and Local Scheduling. Global Scheduling involves determining the set of links that will be simultaneously active, without violating the scheduling constraints. While local scheduling involves determining the packets that will be transferred across active edges. We design a local scheduling scheme by adapting the Preemptive Last-In-First-Out (PL) scheme, applied for quasi-reversible continuous time networks, to an unconstrained discrete-time network. A global scheduling scheme will be obtained by using stable marriage algorithms to emulate the unconstrained network with the constrained wireless network. Our scheme can be easily extended to a network operating under general scheduling constraints, such as secondary interference constraints, with the same delay bound and a loss of throughput that depends on scheduling constraints through an intriguing "sub-graph covering" property.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {395--406},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375502},
 doi = {http://doi.acm.org/10.1145/1375457.1375502},
 acmid = {1375502},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay, scheduling algorithm, throughput},
} 

@article{Jung:2008:RSL:1384529.1375503,
 author = {Jung, Kyomin and Lu, Yingdong and Shah, Devavrat and Sharma, Mayank and Squillante, Mark S.},
 title = {Revisiting stochastic loss networks: structures and algorithms},
 abstract = {This paper considers structural and algorithmic problems in stochastic loss networks. The very popular Erlang approximation can be shown to provide relatively poor performance estimates, especially for loss networks in the critically loaded regime. This paper proposes a novel algorithm for estimating the stationary loss probabilities in stochastic loss networks based on structural properties of the exact stationary distribution, which is shown to always converge, exponentially fast, to the asymptotically exact results. Using a variational characterization of the stationary distribution, an alternative proof is provided for an important result due to Kelly, which is simpler and may be of interest in its own right. This paper also determines structural properties of the inverse Erlang function characterizing the region of capacities that ensures offered traffic is served within a set of loss probabilities. Numerical experiments investigate various issues of both theoretical and practical interest.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {407--418},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375503},
 doi = {http://doi.acm.org/10.1145/1384529.1375503},
 acmid = {1375503},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {erlang loss formula and fixed-point approximation, loss networks, multidimensional stochastic processes, stochastic approximations},
} 

@inproceedings{Jung:2008:RSL:1375457.1375503,
 author = {Jung, Kyomin and Lu, Yingdong and Shah, Devavrat and Sharma, Mayank and Squillante, Mark S.},
 title = {Revisiting stochastic loss networks: structures and algorithms},
 abstract = {This paper considers structural and algorithmic problems in stochastic loss networks. The very popular Erlang approximation can be shown to provide relatively poor performance estimates, especially for loss networks in the critically loaded regime. This paper proposes a novel algorithm for estimating the stationary loss probabilities in stochastic loss networks based on structural properties of the exact stationary distribution, which is shown to always converge, exponentially fast, to the asymptotically exact results. Using a variational characterization of the stationary distribution, an alternative proof is provided for an important result due to Kelly, which is simpler and may be of interest in its own right. This paper also determines structural properties of the inverse Erlang function characterizing the region of capacities that ensures offered traffic is served within a set of loss probabilities. Numerical experiments investigate various issues of both theoretical and practical interest.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {407--418},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375503},
 doi = {http://doi.acm.org/10.1145/1375457.1375503},
 acmid = {1375503},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {erlang loss formula and fixed-point approximation, loss networks, multidimensional stochastic processes, stochastic approximations},
} 

@article{Bonald:2008:TCM:1384529.1375504,
 author = {Bonald, Thomas and Ibrahim, Ali and Roberts, James},
 title = {Traffic capacity of multi-cell WLANS},
 abstract = {Performance of WLANs has been extensively studied during the past few years. While the focus has mostly been on isolated cells, the coverage of WLANs is in practice most often realised through several cells. Cells using the same frequency channel typically interact through the exclusion region enforced by the RTS/CTS mechanism prior to the transmission of any packet. In this paper, we investigate the impact of this interaction on the overall network capacity under realistic dynamic traffic conditions. Specifically, we represent each cell as a queue and derive the stability condition of the corresponding coupled queuing system. This condition is then used to calculate the network capacity. To gain insight into the particular nature of interference in multi-cell WLANs, we apply our model to a number of simple network topologies and explicitly derive the capacity in several cases. The results notably show that the capacity gain obtained by using M frequency channels can grow significantly faster than M, the rate one might intuitively expect. In addition to stability results, we present an approximate model to derive the impact of network load on the mean transfer rate seen by the users.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {419--430},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1384529.1375504},
 doi = {http://doi.acm.org/10.1145/1384529.1375504},
 acmid = {1375504},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity, flow-level model, ieee 802.11, multi-cell WLAN, stability},
} 

@inproceedings{Bonald:2008:TCM:1375457.1375504,
 author = {Bonald, Thomas and Ibrahim, Ali and Roberts, James},
 title = {Traffic capacity of multi-cell WLANS},
 abstract = {Performance of WLANs has been extensively studied during the past few years. While the focus has mostly been on isolated cells, the coverage of WLANs is in practice most often realised through several cells. Cells using the same frequency channel typically interact through the exclusion region enforced by the RTS/CTS mechanism prior to the transmission of any packet. In this paper, we investigate the impact of this interaction on the overall network capacity under realistic dynamic traffic conditions. Specifically, we represent each cell as a queue and derive the stability condition of the corresponding coupled queuing system. This condition is then used to calculate the network capacity. To gain insight into the particular nature of interference in multi-cell WLANs, we apply our model to a number of simple network topologies and explicitly derive the capacity in several cases. The results notably show that the capacity gain obtained by using M frequency channels can grow significantly faster than M, the rate one might intuitively expect. In addition to stability results, we present an approximate model to derive the impact of network load on the mean transfer rate seen by the users.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {419--430},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1375457.1375504},
 doi = {http://doi.acm.org/10.1145/1375457.1375504},
 acmid = {1375504},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity, flow-level model, ieee 802.11, multi-cell WLAN, stability},
} 

@inproceedings{Reineke:2008:RCC:1375457.1375506,
 author = {Reineke, Jan and Grund, Daniel},
 title = {Relative competitiveness of cache replacement policies},
 abstract = {},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {431--432},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375506},
 doi = {http://doi.acm.org/10.1145/1375457.1375506},
 acmid = {1375506},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {WCET analysis, cache performance, predictability, replacement policy, worst-case execution time},
} 

@article{Reineke:2008:RCC:1384529.1375506,
 author = {Reineke, Jan and Grund, Daniel},
 title = {Relative competitiveness of cache replacement policies},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {431--432},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375506},
 doi = {http://doi.acm.org/10.1145/1384529.1375506},
 acmid = {1375506},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {WCET analysis, cache performance, predictability, replacement policy, worst-case execution time},
} 

@inproceedings{Wen:2008:NDE:1375457.1375507,
 author = {Wen, Zhihua and Rabinovich, Michael},
 title = {Network distance estimation with dynamic landmark triangles},
 abstract = {This paper describes an efficient and accurate approach to estimate the network distance between arbitrary Internet hosts. We use three landmark hosts forming a triangle in two-dimensional space to estimate the distance between arbitrary hosts with simple trigonometrical calculations. To improve the accuracy of estimation, we dynamically choose the "best" triangle for a given pair of hosts using a heuristic algorithm. Our experiments show that this approach achieves both lower computational and network probing cost over the classic landmarks-based approach while producing more accurate estimates.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {433--434},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375507},
 doi = {http://doi.acm.org/10.1145/1375457.1375507},
 acmid = {1375507},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network distance estimation},
} 

@article{Wen:2008:NDE:1384529.1375507,
 author = {Wen, Zhihua and Rabinovich, Michael},
 title = {Network distance estimation with dynamic landmark triangles},
 abstract = {This paper describes an efficient and accurate approach to estimate the network distance between arbitrary Internet hosts. We use three landmark hosts forming a triangle in two-dimensional space to estimate the distance between arbitrary hosts with simple trigonometrical calculations. To improve the accuracy of estimation, we dynamically choose the "best" triangle for a given pair of hosts using a heuristic algorithm. Our experiments show that this approach achieves both lower computational and network probing cost over the classic landmarks-based approach while producing more accurate estimates.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {433--434},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375507},
 doi = {http://doi.acm.org/10.1145/1384529.1375507},
 acmid = {1375507},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network distance estimation},
} 

@article{Yuksel:2008:CIB:1384529.1375508,
 author = {Yuksel, Murat and Ramakrishnan, Kadangode K. and Kalyanaraman, Shivkumar and Houle, Joseph D. and Sadhvani, Rita},
 title = {Class-of-service in ip backbones: informing the network neutrality debate},
 abstract = {The benefit of Class-of-Service (CoS) is an important topic in the "Network Neutrality" debate. Proponents of network neutrality suggest that over-provisioning is a viable alternative to CoS. We quantify the extra capacity requirement for an over-provisioned classless (i.e., best-effort) network compared to a CoS network providing the same delay or loss performance for premium traffic. We first develop a link model that quantifies this Required Extra Capacity (REC). For bursty and realistic traffic distributions, we find the REC using ns-2 simulation comparisons of the CoS and classless link cases. We use these link models to quantify the REC for realistic network topologies. We show that REC can be significant even when the proportion of premium traffic is small, a situation often considered benign for the over-provisioning alternative.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {435--436},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375508},
 doi = {http://doi.acm.org/10.1145/1384529.1375508},
 acmid = {1375508},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {class-of-service, economics, network neutrality, performance},
} 

@inproceedings{Yuksel:2008:CIB:1375457.1375508,
 author = {Yuksel, Murat and Ramakrishnan, Kadangode K. and Kalyanaraman, Shivkumar and Houle, Joseph D. and Sadhvani, Rita},
 title = {Class-of-service in ip backbones: informing the network neutrality debate},
 abstract = {The benefit of Class-of-Service (CoS) is an important topic in the "Network Neutrality" debate. Proponents of network neutrality suggest that over-provisioning is a viable alternative to CoS. We quantify the extra capacity requirement for an over-provisioned classless (i.e., best-effort) network compared to a CoS network providing the same delay or loss performance for premium traffic. We first develop a link model that quantifies this Required Extra Capacity (REC). For bursty and realistic traffic distributions, we find the REC using ns-2 simulation comparisons of the CoS and classless link cases. We use these link models to quantify the REC for realistic network topologies. We show that REC can be significant even when the proportion of premium traffic is small, a situation often considered benign for the over-provisioning alternative.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {435--436},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375508},
 doi = {http://doi.acm.org/10.1145/1375457.1375508},
 acmid = {1375508},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {class-of-service, economics, network neutrality, performance},
} 

@inproceedings{Dreger:2008:PRC:1375457.1375509,
 author = {Dreger, Holger and Feldmann, Anja and Paxson, Vern and Sommer, Robin},
 title = {Predicting the resource consumption of network intrusion detection systems},
 abstract = {When installing network intrusion detection systems (NIDSs), operators are faced with a large number of parameters and analysis options for tuning trade-offs between detection accuracy versus resource requirements. In this work we set out to assist this process by understanding and predicting the CPU and memory consumption of such systems.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {437--438},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375509},
 doi = {http://doi.acm.org/10.1145/1375457.1375509},
 acmid = {1375509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {NIDS, performance model},
} 

@article{Dreger:2008:PRC:1384529.1375509,
 author = {Dreger, Holger and Feldmann, Anja and Paxson, Vern and Sommer, Robin},
 title = {Predicting the resource consumption of network intrusion detection systems},
 abstract = {When installing network intrusion detection systems (NIDSs), operators are faced with a large number of parameters and analysis options for tuning trade-offs between detection accuracy versus resource requirements. In this work we set out to assist this process by understanding and predicting the CPU and memory consumption of such systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {437--438},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375509},
 doi = {http://doi.acm.org/10.1145/1384529.1375509},
 acmid = {1375509},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {NIDS, performance model},
} 

@article{Li:2008:EMM:1384529.1375510,
 author = {Li, Bin and Peng, Lu and Ramadass, Balachandran},
 title = {Efficient mart-aided modeling for microarchitecture design space exploration and performance prediction},
 abstract = {Computer architects usually evaluate new designs by cycle-accurate processor simulation. This approach provides detailed insight into processor performance, power consumption and complexity. However, only configurations in a subspace can be simulated in practice due to long simulation time and limited resource, leading to suboptimal conclusions which might not be applied in a larger design space. In this paper, we propose an automated performance prediction approach which employs state-of-the-art techniques from experiment design, machine learning and data mining. Our method not only produces highly accurate estimations for unsampled points in the design space, but also provides interpretation tools that help investigators to understand performance bottlenecks. According to our experiments, by sampling only 0.02\% of the full design space with about 15 millions points, the median percentage errors, based on 5000 independent test points, range from 0.32\% to 3.12\% in 12 benchmarks. Even for the worst-case performance, the percentage errors are within 7\% for 10 out of 12 benchmarks. In addition, the proposed model can also help architects to find important design parameters and performance bottlenecks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {439--440},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375510},
 doi = {http://doi.acm.org/10.1145/1384529.1375510},
 acmid = {1375510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MART-aided models, design space exploration, performance prediction},
} 

@inproceedings{Li:2008:EMM:1375457.1375510,
 author = {Li, Bin and Peng, Lu and Ramadass, Balachandran},
 title = {Efficient mart-aided modeling for microarchitecture design space exploration and performance prediction},
 abstract = {Computer architects usually evaluate new designs by cycle-accurate processor simulation. This approach provides detailed insight into processor performance, power consumption and complexity. However, only configurations in a subspace can be simulated in practice due to long simulation time and limited resource, leading to suboptimal conclusions which might not be applied in a larger design space. In this paper, we propose an automated performance prediction approach which employs state-of-the-art techniques from experiment design, machine learning and data mining. Our method not only produces highly accurate estimations for unsampled points in the design space, but also provides interpretation tools that help investigators to understand performance bottlenecks. According to our experiments, by sampling only 0.02\% of the full design space with about 15 millions points, the median percentage errors, based on 5000 independent test points, range from 0.32\% to 3.12\% in 12 benchmarks. Even for the worst-case performance, the percentage errors are within 7\% for 10 out of 12 benchmarks. In addition, the proposed model can also help architects to find important design parameters and performance bottlenecks.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {439--440},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375510},
 doi = {http://doi.acm.org/10.1145/1375457.1375510},
 acmid = {1375510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MART-aided models, design space exploration, performance prediction},
} 

@inproceedings{Balon:2008:CII:1375457.1375511,
 author = {Balon, Simon and Leduc, Guy},
 title = {Combined intra- and inter-domain traffic engineering using hot-potato aware link weights optimization},
 abstract = {A well-known approach to intradomain traffic engineering consists in finding the set of link weights that minimizes a network-wide objective function for a given intradomain traffic matrix. This approach is inadequate because it ignores a potential impact on interdomain routing due to hot-potato routing policies. This may result in changes in the intradomain traffic matrix that have not been anticipated by the link weights optimizer, possibly leading to degraded network performance. We propose a BGP-aware link weights optimization method that takes these hot-potato effects into account. This method uses the interdomain traffic matrix and other available BGP data, to extend the intradomain topology with external virtual nodes and links, on which all the well-tuned heuristics of a classical link weights optimizer can be applied. Our method can also optimize the traffic on the interdomain peering links.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {441--442},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375511},
 doi = {http://doi.acm.org/10.1145/1375457.1375511},
 acmid = {1375511},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bgp, hot-potato routing, igp, ospf, traffic engineering},
} 

@article{Balon:2008:CII:1384529.1375511,
 author = {Balon, Simon and Leduc, Guy},
 title = {Combined intra- and inter-domain traffic engineering using hot-potato aware link weights optimization},
 abstract = {A well-known approach to intradomain traffic engineering consists in finding the set of link weights that minimizes a network-wide objective function for a given intradomain traffic matrix. This approach is inadequate because it ignores a potential impact on interdomain routing due to hot-potato routing policies. This may result in changes in the intradomain traffic matrix that have not been anticipated by the link weights optimizer, possibly leading to degraded network performance. We propose a BGP-aware link weights optimization method that takes these hot-potato effects into account. This method uses the interdomain traffic matrix and other available BGP data, to extend the intradomain topology with external virtual nodes and links, on which all the well-tuned heuristics of a classical link weights optimizer can be applied. Our method can also optimize the traffic on the interdomain peering links.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {441--442},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375511},
 doi = {http://doi.acm.org/10.1145/1384529.1375511},
 acmid = {1375511},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bgp, hot-potato routing, igp, ospf, traffic engineering},
} 

@article{Anderson:2008:MDW:1384529.1375512,
 author = {Anderson, Eric W. and Phillips, Caleb T. and Bauer, Kevin S. and Grunwald, Dirk C. and Sicker, Douglas C.},
 title = {Modeling directionality in wireless networks: extended abstract},
 abstract = {The physical-layer models commonly used in current networking research only minimally address the interaction of directional antennas and radio propagation. This paper compares the models found in popular simulation tools with measurements taken across a variety of links in multiple environments. We find that the effects of antenna direction are significantly different from the models used by the common wireless network simulators. We propose a parametric model which better captures the effects of different propagation environments on directional antenna systems. We believe that adopting this model will allow more realistic simulation of protocols relying on directional antennas, supporting better design and more valid assessment of those protocols.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {443--444},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375512},
 doi = {http://doi.acm.org/10.1145/1384529.1375512},
 acmid = {1375512},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {antenna, directional, modeling, networking, propagation, wireless},
} 

@inproceedings{Anderson:2008:MDW:1375457.1375512,
 author = {Anderson, Eric W. and Phillips, Caleb T. and Bauer, Kevin S. and Grunwald, Dirk C. and Sicker, Douglas C.},
 title = {Modeling directionality in wireless networks: extended abstract},
 abstract = {The physical-layer models commonly used in current networking research only minimally address the interaction of directional antennas and radio propagation. This paper compares the models found in popular simulation tools with measurements taken across a variety of links in multiple environments. We find that the effects of antenna direction are significantly different from the models used by the common wireless network simulators. We propose a parametric model which better captures the effects of different propagation environments on directional antenna systems. We believe that adopting this model will allow more realistic simulation of protocols relying on directional antennas, supporting better design and more valid assessment of those protocols.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {443--444},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375512},
 doi = {http://doi.acm.org/10.1145/1375457.1375512},
 acmid = {1375512},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {antenna, directional, modeling, networking, propagation, wireless},
} 

@inproceedings{Bremler-Barr:2008:LIC:1375457.1375513,
 author = {Bremler-Barr, Anat and Hay, David and Hendler, Danny and Farber, Boris},
 title = {Layered interval codes for tcam-based classification},
 abstract = {},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {445--446},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375513},
 doi = {http://doi.acm.org/10.1145/1375457.1375513},
 acmid = {1375513},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCAM, classification},
} 

@article{Bremler-Barr:2008:LIC:1384529.1375513,
 author = {Bremler-Barr, Anat and Hay, David and Hendler, Danny and Farber, Boris},
 title = {Layered interval codes for tcam-based classification},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {445--446},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375513},
 doi = {http://doi.acm.org/10.1145/1384529.1375513},
 acmid = {1375513},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCAM, classification},
} 

@inproceedings{Ramabhadran:2008:DRD:1375457.1375514,
 author = {Ramabhadran, Sriram and Pasquale, Joseph},
 title = {Durability of replicated distributed storage systems},
 abstract = {We study the problem of guaranteeing data durability [2] in distributed storage systems based on replication. Our work is motivated by several several recent efforts [3, 5, 1] to build such systems in a peer-to-peer environment. The key features of this environment which make achieving durability difficult are (1) data lifetimes may be several orders of magnitude larger than the lifetimes of individual storage units, and (2) the system may have little or no control over the participation of these storage units in the system. We use a model-based approach to develop engineering principles for designing automated replication and repair mechanisms to implement durability in such systems.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {447--448},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375514},
 doi = {http://doi.acm.org/10.1145/1375457.1375514},
 acmid = {1375514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {durability, replication},
} 

@article{Ramabhadran:2008:DRD:1384529.1375514,
 author = {Ramabhadran, Sriram and Pasquale, Joseph},
 title = {Durability of replicated distributed storage systems},
 abstract = {We study the problem of guaranteeing data durability [2] in distributed storage systems based on replication. Our work is motivated by several several recent efforts [3, 5, 1] to build such systems in a peer-to-peer environment. The key features of this environment which make achieving durability difficult are (1) data lifetimes may be several orders of magnitude larger than the lifetimes of individual storage units, and (2) the system may have little or no control over the participation of these storage units in the system. We use a model-based approach to develop engineering principles for designing automated replication and repair mechanisms to implement durability in such systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {447--448},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375514},
 doi = {http://doi.acm.org/10.1145/1384529.1375514},
 acmid = {1375514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {durability, replication},
} 

@inproceedings{Li:2008:IEM:1375457.1375515,
 author = {Li, Feihui and Kandemir, Mahmut and Irwin, Mary J.},
 title = {Implementation and evaluation of a migration-based NUCA design for chip multiprocessors},
 abstract = {Chip Multiprocessors (CMPs) and Non-Uniform Cache Architectures (NUCAs) represent two emerging trends in computer architecture. Targeting future CMP based systems with NUCA type L2 caches, this paper proposes a novel data migration algorithm for parallel applications and evaluates it. The goal of this migration scheme is to determine a suitable location for each data block within a large L2 space at any given point during execution. A unique characteristic of the proposed scheme is that it models the problem of optimal data placement in the L2 cache space as a two dimensional post office placement problem, presents a practical architectural implementation of this model, and gives an evaluation of the proposed implementation.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {449--450},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375515},
 doi = {http://doi.acm.org/10.1145/1375457.1375515},
 acmid = {1375515},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, NUCA, data migration, post office placement problem},
} 

@article{Li:2008:IEM:1384529.1375515,
 author = {Li, Feihui and Kandemir, Mahmut and Irwin, Mary J.},
 title = {Implementation and evaluation of a migration-based NUCA design for chip multiprocessors},
 abstract = {Chip Multiprocessors (CMPs) and Non-Uniform Cache Architectures (NUCAs) represent two emerging trends in computer architecture. Targeting future CMP based systems with NUCA type L2 caches, this paper proposes a novel data migration algorithm for parallel applications and evaluates it. The goal of this migration scheme is to determine a suitable location for each data block within a large L2 space at any given point during execution. A unique characteristic of the proposed scheme is that it models the problem of optimal data placement in the L2 cache space as a two dimensional post office placement problem, presents a practical architectural implementation of this model, and gives an evaluation of the proposed implementation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {449--450},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375515},
 doi = {http://doi.acm.org/10.1145/1384529.1375515},
 acmid = {1375515},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CMP, NUCA, data migration, post office placement problem},
} 

@article{Alouf:2008:MQR:1384529.1375516,
 author = {Alouf, Sara and Altman, Eitan and Azad, Amar Prakash},
 title = {M/G/1 queue with repeated inhomogeneous vacations applied to ieee 802.16e power saving},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {451--452},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375516},
 doi = {http://doi.acm.org/10.1145/1384529.1375516},
 acmid = {1375516},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1 queue with repeated inhomogeneous vacations, constrained optimization, numerical analysis, power save mode, system response time},
} 

@inproceedings{Alouf:2008:MQR:1375457.1375516,
 author = {Alouf, Sara and Altman, Eitan and Azad, Amar Prakash},
 title = {M/G/1 queue with repeated inhomogeneous vacations applied to ieee 802.16e power saving},
 abstract = {},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {451--452},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375516},
 doi = {http://doi.acm.org/10.1145/1375457.1375516},
 acmid = {1375516},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1 queue with repeated inhomogeneous vacations, constrained optimization, numerical analysis, power save mode, system response time},
} 

@article{Seetharaman:2008:MIT:1384529.1375517,
 author = {Seetharaman, Srinivasan and Ammar, Mostafa H.},
 title = {Managing inter-domain traffic in the presence of bittorrent file-sharing},
 abstract = {Overlay routing operating in a selfish manner is known to cause undesired instability when it interacts with native layer routing. We observe similar selfish behavior with the BitTorrent protocol, where its performance-awareness causes it to constantly alter the routing decisions (peer and piece selection). This causes fluctuations in the load experienced by the underlying native network. By using real BitTorrent traces and a comprehensive simulation with different network characteristics, we show that BitTorrent systems easily disrupt the load balance across inter-domain links. Further, we find that existing native layer traffic management schemes suffer from several downsides and are not conducive to deployment. To resolve this dilemma, we propose two BitTorrent strategies that are effective in resolving the cross-layer conflict.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {453--454},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375517},
 doi = {http://doi.acm.org/10.1145/1384529.1375517},
 acmid = {1375517},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bittorrent, conflict, contention, cross-layer, traffic engineering, traffic management},
} 

@inproceedings{Seetharaman:2008:MIT:1375457.1375517,
 author = {Seetharaman, Srinivasan and Ammar, Mostafa H.},
 title = {Managing inter-domain traffic in the presence of bittorrent file-sharing},
 abstract = {Overlay routing operating in a selfish manner is known to cause undesired instability when it interacts with native layer routing. We observe similar selfish behavior with the BitTorrent protocol, where its performance-awareness causes it to constantly alter the routing decisions (peer and piece selection). This causes fluctuations in the load experienced by the underlying native network. By using real BitTorrent traces and a comprehensive simulation with different network characteristics, we show that BitTorrent systems easily disrupt the load balance across inter-domain links. Further, we find that existing native layer traffic management schemes suffer from several downsides and are not conducive to deployment. To resolve this dilemma, we propose two BitTorrent strategies that are effective in resolving the cross-layer conflict.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {453--454},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375517},
 doi = {http://doi.acm.org/10.1145/1375457.1375517},
 acmid = {1375517},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bittorrent, conflict, contention, cross-layer, traffic engineering, traffic management},
} 

@inproceedings{Mota-Garcia:2008:COE:1375457.1375518,
 author = {Mota-Garcia, Edmar and Hasimoto-Beltran, Rogelio},
 title = {Clock offset estimation using collaborative one-way transit time},
 abstract = {We propose a new collaborative clock offset estimation scheme between two nodes in the Internet using independent one-way offset estimations. Our proposal (different than current schemes in the literature) is intended to provide a fast and accurate clock offset estimation in approximately [Round-Trip Time (RTT)+40]ms. The scheme sends a group of 5 probes in the the forward and reverse paths, and models the One-way Transit Time (OTT) by a Gamma distribution (with parameters adapted to actual path condition) to estimate the minimum distribution value (or long-term minimum OTT value). End nodes exchange their corresponding minimum distribution values to get an improved final clock offset estimate, which takes into account the network path asymmetries. We show that our scheme provides a faster clock offset estimation with lower RMSE and superior stability than NTP and current NTP-like state of the art methodologies in the literature.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {455--456},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375518},
 doi = {http://doi.acm.org/10.1145/1375457.1375518},
 acmid = {1375518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clock offset estimation, one-way transit time},
} 

@article{Mota-Garcia:2008:COE:1384529.1375518,
 author = {Mota-Garcia, Edmar and Hasimoto-Beltran, Rogelio},
 title = {Clock offset estimation using collaborative one-way transit time},
 abstract = {We propose a new collaborative clock offset estimation scheme between two nodes in the Internet using independent one-way offset estimations. Our proposal (different than current schemes in the literature) is intended to provide a fast and accurate clock offset estimation in approximately [Round-Trip Time (RTT)+40]ms. The scheme sends a group of 5 probes in the the forward and reverse paths, and models the One-way Transit Time (OTT) by a Gamma distribution (with parameters adapted to actual path condition) to estimate the minimum distribution value (or long-term minimum OTT value). End nodes exchange their corresponding minimum distribution values to get an improved final clock offset estimate, which takes into account the network path asymmetries. We show that our scheme provides a faster clock offset estimation with lower RMSE and superior stability than NTP and current NTP-like state of the art methodologies in the literature.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {455--456},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375518},
 doi = {http://doi.acm.org/10.1145/1384529.1375518},
 acmid = {1375518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clock offset estimation, one-way transit time},
} 

@inproceedings{Gupta:2008:SQL:1375457.1375519,
 author = {Gupta, Gagan R. and Shroff, Ness B.},
 title = {Scheduling with queue length guarantees for shared resource systems},
 abstract = {We develop a class of schemes called GMWM that guarantee optimal throughput for queuing systems with arbitrary constraints on the set of jobs that can be served simultaneously. We obtain an analytical upper bound on the expected queue length. To further tighten the upper bound, we formulate it as a convex optimization problem. We also show that whenever the arrival process is stabilizable, the scheme is guaranteed to achieve an expected queue length that is no larger than the expected queue length of any stationary randomized policy.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {457--458},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375519},
 doi = {http://doi.acm.org/10.1145/1375457.1375519},
 acmid = {1375519},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lyapunov thery, scheduling},
} 

@article{Gupta:2008:SQL:1384529.1375519,
 author = {Gupta, Gagan R. and Shroff, Ness B.},
 title = {Scheduling with queue length guarantees for shared resource systems},
 abstract = {We develop a class of schemes called GMWM that guarantee optimal throughput for queuing systems with arbitrary constraints on the set of jobs that can be served simultaneously. We obtain an analytical upper bound on the expected queue length. To further tighten the upper bound, we formulate it as a convex optimization problem. We also show that whenever the arrival process is stabilizable, the scheme is guaranteed to achieve an expected queue length that is no larger than the expected queue length of any stationary randomized policy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {457--458},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375519},
 doi = {http://doi.acm.org/10.1145/1384529.1375519},
 acmid = {1375519},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lyapunov thery, scheduling},
} 

@article{Chen:2008:ECD:1384529.1375520,
 author = {Chen, Aiyou and Li, Li and Cao, Jin},
 title = {Estimating cardinality distributions in network traffic: extended abstract},
 abstract = {Information on network host connectivity patterns are important for network monitoring and traffic engineering. In this paper, an efficient streaming algorithm is proposed to estimate cardinality distributions including connectivity distributions, e.g. percent of hosts with any given number of distinct communicating peers or flows.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {459--460},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375520},
 doi = {http://doi.acm.org/10.1145/1384529.1375520},
 acmid = {1375520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cardinality distribution, streaming algorithm},
} 

@inproceedings{Chen:2008:ECD:1375457.1375520,
 author = {Chen, Aiyou and Li, Li and Cao, Jin},
 title = {Estimating cardinality distributions in network traffic: extended abstract},
 abstract = {Information on network host connectivity patterns are important for network monitoring and traffic engineering. In this paper, an efficient streaming algorithm is proposed to estimate cardinality distributions including connectivity distributions, e.g. percent of hosts with any given number of distinct communicating peers or flows.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {459--460},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375520},
 doi = {http://doi.acm.org/10.1145/1375457.1375520},
 acmid = {1375520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cardinality distribution, streaming algorithm},
} 

@inproceedings{Grit:2008:WFS:1375457.1375521,
 author = {Grit, Laura E. and Chase, Jeffrey S.},
 title = {Weighted fair sharing for dynamic virtual clusters},
 abstract = {In a shared server infrastructure, a scheduler controls how quantities of resources are shared over time in a fair manner across multiple, competing consumers. It should support wide (parallel) requests for variable-sized pool of resources, provide assurance of minimum resource allotment on demand, and give predictable assignments. Our approach integrates a fair queuing algorithm with a calendar scheduler. We present WINKS, a proportional share allocation policy that addresses the needs of shared server environments. It extends start-time fair queuing to support wide requests with backfill, advance reservations, dynamic cluster sizing, dynamic request sizing, and intra-flow request prioritization. It also preserves fairness properties across queue transformations and calendar operations needed to implement these extensions.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {461--462},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375521},
 doi = {http://doi.acm.org/10.1145/1375457.1375521},
 acmid = {1375521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster computing, fair sharing, proportional sharing, virtual computing, weighted fair queuing},
} 

@article{Grit:2008:WFS:1384529.1375521,
 author = {Grit, Laura E. and Chase, Jeffrey S.},
 title = {Weighted fair sharing for dynamic virtual clusters},
 abstract = {In a shared server infrastructure, a scheduler controls how quantities of resources are shared over time in a fair manner across multiple, competing consumers. It should support wide (parallel) requests for variable-sized pool of resources, provide assurance of minimum resource allotment on demand, and give predictable assignments. Our approach integrates a fair queuing algorithm with a calendar scheduler. We present WINKS, a proportional share allocation policy that addresses the needs of shared server environments. It extends start-time fair queuing to support wide requests with backfill, advance reservations, dynamic cluster sizing, dynamic request sizing, and intra-flow request prioritization. It also preserves fairness properties across queue transformations and calendar operations needed to implement these extensions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {461--462},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375521},
 doi = {http://doi.acm.org/10.1145/1384529.1375521},
 acmid = {1375521},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster computing, fair sharing, proportional sharing, virtual computing, weighted fair queuing},
} 

@article{Sundaram:2008:ETT:1384529.1375522,
 author = {Sundaram, Vasumathi and Chandra, Abhishek and Weissman, Jon},
 title = {Exploring the throughput-fairness tradeoff of deadline scheduling in heterogeneous computing environments},
 abstract = {The scalability and computing power of large-scale computational platforms has made them attractive for hosting compute-intensive time-critical applications. Many of these applications are composed of computational tasks that require specific deadlines to be met for successful completion. In this paper, we show that combining redundant scheduling with deadline-based scheduling in these systems leads to a fundamental tradeoff between throughput and fairness. We propose a new scheduling algorithm called Limited Resource Earliest Deadline (LRED) that couples redundant scheduling with deadline-driven scheduling in a flexible way by using a simple tunable parameter to exploit this tradeoff. Our evaluation of LRED shows that LRED provides a powerful mechanism to achieve desired throughput or fairness under high loads and low timeliness environments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {463--464},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375522},
 doi = {http://doi.acm.org/10.1145/1384529.1375522},
 acmid = {1375522},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deadline, fairness, throughput},
} 

@inproceedings{Sundaram:2008:ETT:1375457.1375522,
 author = {Sundaram, Vasumathi and Chandra, Abhishek and Weissman, Jon},
 title = {Exploring the throughput-fairness tradeoff of deadline scheduling in heterogeneous computing environments},
 abstract = {The scalability and computing power of large-scale computational platforms has made them attractive for hosting compute-intensive time-critical applications. Many of these applications are composed of computational tasks that require specific deadlines to be met for successful completion. In this paper, we show that combining redundant scheduling with deadline-based scheduling in these systems leads to a fundamental tradeoff between throughput and fairness. We propose a new scheduling algorithm called Limited Resource Earliest Deadline (LRED) that couples redundant scheduling with deadline-driven scheduling in a flexible way by using a simple tunable parameter to exploit this tradeoff. Our evaluation of LRED shows that LRED provides a powerful mechanism to achieve desired throughput or fairness under high loads and low timeliness environments.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {463--464},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375522},
 doi = {http://doi.acm.org/10.1145/1375457.1375522},
 acmid = {1375522},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deadline, fairness, throughput},
} 

@article{Papp:2008:CMV:1384529.1375523,
 author = {Papp, Gabor and GauthierDickey, Chris},
 title = {Characterizing multiparty voice communication for multiplayer games},
 abstract = {Over the last few years, the number of game players using voice communication to talk to each other while playing games has increased dramatically. In fact, many modern games and game consoles have added voice support instead of expecting third-party companies to provide this technology. Unlike traditional voice-over-IP technology, where most conversations are between two people, voice communication in games often has 5 or more people talking together as they play. We present the first measurement study on the characteristics of multiparty voice communications. Over a 3 month period, we measured over 7,000 sessions on an active multi-party voice communication server to quantify the characteristics of communication generated by game players, including overall server traffic, group sizes, sessions characteristics, and speaking (and silence) durations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {465--466},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375523},
 doi = {http://doi.acm.org/10.1145/1384529.1375523},
 acmid = {1375523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer games, silence periods, talkspurts, voice communication},
} 

@inproceedings{Papp:2008:CMV:1375457.1375523,
 author = {Papp, Gabor and GauthierDickey, Chris},
 title = {Characterizing multiparty voice communication for multiplayer games},
 abstract = {Over the last few years, the number of game players using voice communication to talk to each other while playing games has increased dramatically. In fact, many modern games and game consoles have added voice support instead of expecting third-party companies to provide this technology. Unlike traditional voice-over-IP technology, where most conversations are between two people, voice communication in games often has 5 or more people talking together as they play. We present the first measurement study on the characteristics of multiparty voice communications. Over a 3 month period, we measured over 7,000 sessions on an active multi-party voice communication server to quantify the characteristics of communication generated by game players, including overall server traffic, group sizes, sessions characteristics, and speaking (and silence) durations.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {465--466},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375523},
 doi = {http://doi.acm.org/10.1145/1375457.1375523},
 acmid = {1375523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer games, silence periods, talkspurts, voice communication},
} 

@inproceedings{Meiners:2008:AAR:1375457.1375524,
 author = {Meiners, Chad R. and Liu, Alex X. and Torng, Eric},
 title = {Algorithmic approaches to redesigning tcam-based systems},
 abstract = {},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {467--468},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375524},
 doi = {http://doi.acm.org/10.1145/1375457.1375524},
 acmid = {1375524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCAM, packet classification, pipelin, range expansion},
} 

@article{Meiners:2008:AAR:1384529.1375524,
 author = {Meiners, Chad R. and Liu, Alex X. and Torng, Eric},
 title = {Algorithmic approaches to redesigning tcam-based systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {467--468},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375524},
 doi = {http://doi.acm.org/10.1145/1384529.1375524},
 acmid = {1375524},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCAM, packet classification, pipelin, range expansion},
} 

@inproceedings{Douceur:2008:PAR:1375457.1375526,
 author = {Douceur, John R.},
 title = {Performance analysis in the real world},
 abstract = {What issues are on the minds of industrial performance analysts? Four representatives of world-class product organizations will describe their work at the front lines of measurement, modeling, and performance tuning. Topics will include performance engineering of middleware at IBM, tools for detecting false sharing in large-scale multiprocessors at Hewlett-Packard, kernel thread-scheduling performance in multiprocessors at Microsoft, and low-overhead instrumentation for profiling large-scale services at Google. Plenty of time will be available to ask questions about how to direct our research to have the greatest impact on industrial practice.},
 booktitle = {Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '08},
 year = {2008},
 isbn = {978-1-60558-005-0},
 location = {Annapolis, MD, USA},
 pages = {469--470},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1375457.1375526},
 doi = {http://doi.acm.org/10.1145/1375457.1375526},
 acmid = {1375526},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {engineering, kernel performance, large-scale services, middleware, performance analysis, profiling tools, storage systems},
} 

@article{Douceur:2008:PAR:1384529.1375526,
 author = {Douceur, John R.},
 title = {Performance analysis in the real world},
 abstract = {What issues are on the minds of industrial performance analysts? Four representatives of world-class product organizations will describe their work at the front lines of measurement, modeling, and performance tuning. Topics will include performance engineering of middleware at IBM, tools for detecting false sharing in large-scale multiprocessors at Hewlett-Packard, kernel thread-scheduling performance in multiprocessors at Microsoft, and low-overhead instrumentation for profiling large-scale services at Google. Plenty of time will be available to ask questions about how to direct our research to have the greatest impact on industrial practice.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {36},
 issue = {1},
 month = {June},
 year = {2008},
 issn = {0163-5999},
 pages = {469--470},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1384529.1375526},
 doi = {http://doi.acm.org/10.1145/1384529.1375526},
 acmid = {1375526},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {engineering, kernel performance, large-scale services, middleware, performance analysis, profiling tools, storage systems},
} 

@inproceedings{Mitra:2004:STE:1005686.1005687,
 author = {Mitra, Debasis},
 title = {Stochastic traffic engineering for demand uncertainty and risk-aware network revenue management},
 abstract = {Stochastic traffic engineering for demand uncertainty and risk-aware network revenue management We present a stochastic traffic engineering framework for optimizing bandwidth provisioning and route selection in networks. Traffic demands are uncertain and specified by probability distributions, and the objective is to maximize a risk-adjusted measure of network revenue that is generated by serving demands. Considerable attention is given to the appropriate measure of risk in the network model. We also advance risk-mitigation strategies. The optimization model, which is based on mean-risk analysis, enables a service provider to maximize a combined measure of mean revenue and revenue risk. The framework is intended for off-line traffic engineering, which takes a centralized view of network topology, link capacity and demand. We obtain conditions under which the optimization problem is an instance of convex programming. We study the properties of the solution and show that it asymptotically meets the stochastic efficiency criterion.In our numerical investigations we illustrate the impact of demand uncertainty on various aspects of the optimally traffic engineered solutions. The service provider's tolerance to risk is shown to have a strong influence on the traffic engineering and revenue management decisions. We develop the efficient frontier, which is the set of Pareto optimal pairs of mean revenue and revenue risk, to aid the service provider in selecting its operating point.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1005686.1005687},
 doi = {http://doi.acm.org/10.1145/1005686.1005687},
 acmid = {1005687},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mitra:2004:STE:1012888.1005687,
 author = {Mitra, Debasis},
 title = {Stochastic traffic engineering for demand uncertainty and risk-aware network revenue management},
 abstract = {Stochastic traffic engineering for demand uncertainty and risk-aware network revenue management We present a stochastic traffic engineering framework for optimizing bandwidth provisioning and route selection in networks. Traffic demands are uncertain and specified by probability distributions, and the objective is to maximize a risk-adjusted measure of network revenue that is generated by serving demands. Considerable attention is given to the appropriate measure of risk in the network model. We also advance risk-mitigation strategies. The optimization model, which is based on mean-risk analysis, enables a service provider to maximize a combined measure of mean revenue and revenue risk. The framework is intended for off-line traffic engineering, which takes a centralized view of network topology, link capacity and demand. We obtain conditions under which the optimization problem is an instance of convex programming. We study the properties of the solution and show that it asymptotically meets the stochastic efficiency criterion.In our numerical investigations we illustrate the impact of demand uncertainty on various aspects of the optimally traffic engineered solutions. The service provider's tolerance to risk is shown to have a strong influence on the traffic engineering and revenue management decisions. We develop the efficient frontier, which is the set of Pareto optimal pairs of mean revenue and revenue risk, to aid the service provider in selecting its operating point.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1012888.1005687},
 doi = {http://doi.acm.org/10.1145/1012888.1005687},
 acmid = {1005687},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marin:2004:CPP:1012888.1005691,
 author = {Marin, Gabriel and Mellor-Crummey, John},
 title = {Cross-architecture performance predictions for scientific applications using parameterized models},
 abstract = {This paper describes a toolkit for semi-automatically measuring and modeling static and dynamic characteristics of applications in an architecture-neutral fashion. For predictable applications, models of dynamic characteristics have a convex and differentiable profile. Our toolkit operates on application binaries and succeeds in modeling key application characteristics that determine program performance. We use these characterizations to explore the interactions between an application and a target architecture. We apply our toolkit to SPARC binaries to develop architecture-neutral models of computation and memory access patterns of the ASCI Sweep3D and the NAS SP, BT and LU benchmarks. From our models, we predict the L1, L2 and TLB cache miss counts as well as the overall execution time of these applications on an Origin 2000 system. We evaluate our predictions by comparing them against measurements collected using hardware performance counters.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005691},
 doi = {http://doi.acm.org/10.1145/1012888.1005691},
 acmid = {1005691},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modeling, performance analysis, prediction},
} 

@inproceedings{Marin:2004:CPP:1005686.1005691,
 author = {Marin, Gabriel and Mellor-Crummey, John},
 title = {Cross-architecture performance predictions for scientific applications using parameterized models},
 abstract = {This paper describes a toolkit for semi-automatically measuring and modeling static and dynamic characteristics of applications in an architecture-neutral fashion. For predictable applications, models of dynamic characteristics have a convex and differentiable profile. Our toolkit operates on application binaries and succeeds in modeling key application characteristics that determine program performance. We use these characterizations to explore the interactions between an application and a target architecture. We apply our toolkit to SPARC binaries to develop architecture-neutral models of computation and memory access patterns of the ASCI Sweep3D and the NAS SP, BT and LU benchmarks. From our models, we predict the L1, L2 and TLB cache miss counts as well as the overall execution time of these applications on an Origin 2000 system. We evaluate our predictions by comparing them against measurements collected using hardware performance counters.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005691},
 doi = {http://doi.acm.org/10.1145/1005686.1005691},
 acmid = {1005691},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modeling, performance analysis, prediction},
} 

@inproceedings{Huang:2004:MSV:1005686.1005692,
 author = {Huang, Lan and Peng, Gang and Chiueh, Tzi-cker},
 title = {Multi-dimensional storage virtualization},
 abstract = {Most state-of-the-art commercial storage virtualization systems focus only on one particular storage attribute, capacity. This paper describes the design, implementation and evaluation of a multi-dimensional storage virtualization</i> system called Stonehenge, which is able to virtualize a cluster-based physical storage system along multiple dimensions, including bandwidth, capacity, and latency. As a result, Stonehenge is able to multiplex multiple virtual disks, each with a distinct bandwidth, capacity, and latency attribute, on a single physical storage system as if they are separate physical disks. A key enabling technology for Stonehenge is an efficiency-aware real-time disk scheduling algorithm called dual-queue disk scheduling, which maximizes disk utilization efficiency while providing Quality of Service (QoS) guarantees. To optimize disk utilization efficiency, Stonehenge exploits run-time measurements extensively, for admission control, computing latency-derived bandwidth requirement, and predicting disk service time.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1005686.1005692},
 doi = {http://doi.acm.org/10.1145/1005686.1005692},
 acmid = {1005692},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {quality of service, storage virtualization},
} 

@article{Huang:2004:MSV:1012888.1005692,
 author = {Huang, Lan and Peng, Gang and Chiueh, Tzi-cker},
 title = {Multi-dimensional storage virtualization},
 abstract = {Most state-of-the-art commercial storage virtualization systems focus only on one particular storage attribute, capacity. This paper describes the design, implementation and evaluation of a multi-dimensional storage virtualization</i> system called Stonehenge, which is able to virtualize a cluster-based physical storage system along multiple dimensions, including bandwidth, capacity, and latency. As a result, Stonehenge is able to multiplex multiple virtual disks, each with a distinct bandwidth, capacity, and latency attribute, on a single physical storage system as if they are separate physical disks. A key enabling technology for Stonehenge is an efficiency-aware real-time disk scheduling algorithm called dual-queue disk scheduling, which maximizes disk utilization efficiency while providing Quality of Service (QoS) guarantees. To optimize disk utilization efficiency, Stonehenge exploits run-time measurements extensively, for admission control, computing latency-derived bandwidth requirement, and predicting disk service time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1012888.1005692},
 doi = {http://doi.acm.org/10.1145/1012888.1005692},
 acmid = {1005692},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {quality of service, storage virtualization},
} 

@inproceedings{Blackburn:2004:MRP:1005686.1005693,
 author = {Blackburn, Stephen M. and Cheng, Perry and McKinley, Kathryn S.},
 title = {Myths and realities: the performance impact of garbage collection},
 abstract = {This paper explores and quantifies garbage collection behavior for three whole heap collectors and generational counterparts: copying semi-space, mark-sweep,</i> and reference counting</i>, the canonical algorithms from which essentially all other collection algorithms are derived. Efficient implementations in MMTk, a Java memory management toolkit, in IBM's Jikes RVM share all common mechanisms to provide a clean experimental platform. Instrumentation separates collector and program behavior, and performance counters measure timing and memory behavior on three architectures.Our experimental design reveals key algorithmic features and how they match program characteristics to explain the direct and indirect costs of garbage collection as a function of heap size on the SPEC JVM benchmarks. For example, we find that the contiguous allocation of copying collectors attains significant locality benefits over free-list allocators. The reduced collection costs of the generational algorithms together with the locality benefit of contiguous allocation motivates a copying nursery</i> for newly allocated objects. These benefits dominate the overheads of generational collectors compared with non-generational and no collection, disputing the myth that "no garbage collection is good garbage collection." Performance is less sensitive to the mature space collection algorithm in our benchmarks. However the locality and pointer mutation characteristics for a given program occasionally prefer copying or mark-sweep. This study is unique in its breadth of garbage collection algorithms and its depth of analysis.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005693},
 doi = {http://doi.acm.org/10.1145/1005686.1005693},
 acmid = {1005693},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {generational, java, mark-sweep, reference counting, semi-space},
} 

@article{Blackburn:2004:MRP:1012888.1005693,
 author = {Blackburn, Stephen M. and Cheng, Perry and McKinley, Kathryn S.},
 title = {Myths and realities: the performance impact of garbage collection},
 abstract = {This paper explores and quantifies garbage collection behavior for three whole heap collectors and generational counterparts: copying semi-space, mark-sweep,</i> and reference counting</i>, the canonical algorithms from which essentially all other collection algorithms are derived. Efficient implementations in MMTk, a Java memory management toolkit, in IBM's Jikes RVM share all common mechanisms to provide a clean experimental platform. Instrumentation separates collector and program behavior, and performance counters measure timing and memory behavior on three architectures.Our experimental design reveals key algorithmic features and how they match program characteristics to explain the direct and indirect costs of garbage collection as a function of heap size on the SPEC JVM benchmarks. For example, we find that the contiguous allocation of copying collectors attains significant locality benefits over free-list allocators. The reduced collection costs of the generational algorithms together with the locality benefit of contiguous allocation motivates a copying nursery</i> for newly allocated objects. These benefits dominate the overheads of generational collectors compared with non-generational and no collection, disputing the myth that "no garbage collection is good garbage collection." Performance is less sensitive to the mature space collection algorithm in our benchmarks. However the locality and pointer mutation characteristics for a given program occasionally prefer copying or mark-sweep. This study is unique in its breadth of garbage collection algorithms and its depth of analysis.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005693},
 doi = {http://doi.acm.org/10.1145/1012888.1005693},
 acmid = {1005693},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {generational, java, mark-sweep, reference counting, semi-space},
} 

@article{Jin:2004:IPS:1012888.1005694,
 author = {Jin, Wei and Chase, Jeffrey S. and Kaur, Jasleen},
 title = {Interposed proportional sharing for a storage service utility},
 abstract = {This paper develops and evaluates new share-based scheduling algorithms for differentiated service quality in network services, such as network storage servers. This form of resource control makes it possible to share a server among multiple request flows with probabilistic assurance that each flow receives a specified minimum share of a server's capacity to serve requests. This assurance is important for safe outsourcing of services to shared utilities such as Storage Service Providers.Our approach interposes share-based request dispatching on the network path between the server and its clients. Two new scheduling algorithms are designed to run within an intermediary (e.g., a network switch), where they enforce fair sharing by throttling request flows and reordering requests; these algorithms are adaptations of Start-time Fair Queuing (SFQ) for servers with a configurable degree of internal concurrency. A third algorithm, Request Windows (RW), bounds the outstanding requests for each flow independently; it is amenable to a decentralized implementation, but may restrict concurrency under light load. The analysis and experimental results show that these new algorithms can enforce shares effectively when the shares are not saturated, and that they provide acceptable performance isolation under saturation. Although the evaluation uses a storage service as an example, interposed request scheduling is non-intrusive and views the server as a black box, so it is useful for complex services with no internal support for differentiated service quality.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005694},
 doi = {http://doi.acm.org/10.1145/1012888.1005694},
 acmid = {1005694},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {differentiated service, fair sharing, multiprocessor scheduling, performance isolation, proportional sharing, quality of service, storage services, utility computing, weighted fair queuing},
} 

@inproceedings{Jin:2004:IPS:1005686.1005694,
 author = {Jin, Wei and Chase, Jeffrey S. and Kaur, Jasleen},
 title = {Interposed proportional sharing for a storage service utility},
 abstract = {This paper develops and evaluates new share-based scheduling algorithms for differentiated service quality in network services, such as network storage servers. This form of resource control makes it possible to share a server among multiple request flows with probabilistic assurance that each flow receives a specified minimum share of a server's capacity to serve requests. This assurance is important for safe outsourcing of services to shared utilities such as Storage Service Providers.Our approach interposes share-based request dispatching on the network path between the server and its clients. Two new scheduling algorithms are designed to run within an intermediary (e.g., a network switch), where they enforce fair sharing by throttling request flows and reordering requests; these algorithms are adaptations of Start-time Fair Queuing (SFQ) for servers with a configurable degree of internal concurrency. A third algorithm, Request Windows (RW), bounds the outstanding requests for each flow independently; it is amenable to a decentralized implementation, but may restrict concurrency under light load. The analysis and experimental results show that these new algorithms can enforce shares effectively when the shares are not saturated, and that they provide acceptable performance isolation under saturation. Although the evaluation uses a storage service as an example, interposed request scheduling is non-intrusive and views the server as a black box, so it is useful for complex services with no internal support for differentiated service quality.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005694},
 doi = {http://doi.acm.org/10.1145/1005686.1005694},
 acmid = {1005694},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {differentiated service, fair sharing, multiprocessor scheduling, performance isolation, proportional sharing, quality of service, storage services, utility computing, weighted fair queuing},
} 

@article{Soule:2004:FCH:1012888.1005696,
 author = {Soule, Augustin and Salamatia, Kav\'{e} and Taft, Nina and Emilion, Richard and Papagiannaki, Konstantina},
 title = {Flow classification by histograms: or how to go on safari in the internet},
 abstract = {In order to control and manage highly aggregated Internet traffic flows efficiently, we need to be able to categorize flows into distinct classes and to be knowledgeable about the different behavior of flows belonging to these classes. In this paper we consider the problem of classifying BGP level prefix flows into a small set of homogeneous classes. We argue that using the entire distributional properties of flows can have significant benefits in terms of quality in the derived classification. We propose a method based on modeling flow histograms using Dirichlet Mixture Processes for random distributions. We present an inference procedure based on the Simulated Annealing Expectation Maximization algorithm that estimates all the model parameters as well as flow membership probabilities</i> - the probability that a flow belongs to any given class. One of our key contributions is a new method for Internet flow classification. We show that our method is powerful in that it is capable of examining macroscopic flows while simultaneously making fine distinctions between different traffic classes. We demonstrate that our scheme can address issues with flows being close to class boundaries and the inherent dynamic behaviour of Internet flows.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005696},
 doi = {http://doi.acm.org/10.1145/1012888.1005696},
 acmid = {1005696},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flow classification, internet traffic, parameter estimation},
} 

@inproceedings{Soule:2004:FCH:1005686.1005696,
 author = {Soule, Augustin and Salamatia, Kav\'{e} and Taft, Nina and Emilion, Richard and Papagiannaki, Konstantina},
 title = {Flow classification by histograms: or how to go on safari in the internet},
 abstract = {In order to control and manage highly aggregated Internet traffic flows efficiently, we need to be able to categorize flows into distinct classes and to be knowledgeable about the different behavior of flows belonging to these classes. In this paper we consider the problem of classifying BGP level prefix flows into a small set of homogeneous classes. We argue that using the entire distributional properties of flows can have significant benefits in terms of quality in the derived classification. We propose a method based on modeling flow histograms using Dirichlet Mixture Processes for random distributions. We present an inference procedure based on the Simulated Annealing Expectation Maximization algorithm that estimates all the model parameters as well as flow membership probabilities</i> - the probability that a flow belongs to any given class. One of our key contributions is a new method for Internet flow classification. We show that our method is powerful in that it is capable of examining macroscopic flows while simultaneously making fine distinctions between different traffic classes. We demonstrate that our scheme can address issues with flows being close to class boundaries and the inherent dynamic behaviour of Internet flows.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005696},
 doi = {http://doi.acm.org/10.1145/1005686.1005696},
 acmid = {1005696},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flow classification, internet traffic, parameter estimation},
} 

@inproceedings{Lakhina:2004:SAN:1005686.1005697,
 author = {Lakhina, Anukool and Papagiannaki, Konstantina and Crovella, Mark and Diot, Christophe and Kolaczyk, Eric D. and Taft, Nina},
 title = {Structural analysis of network traffic flows},
 abstract = {Network traffic arises from the superposition of Origin-Destination (OD) flows. Hence, a thorough understanding of OD flows is essential for modeling network traffic, and for addressing a wide variety of problems including traffic engineering, traffic matrix estimation, capacity planning, forecasting and anomaly detection. However, to date, OD flows have not been closely studied, and there is very little known about their properties.We present the first analysis of complete sets of OD flow time-series, taken from two different backbone networks (Abilene and Sprint-Europe). Using Principal Component Analysis (PCA), we find that the set of OD flows has small intrinsic dimension. In fact, even in a network with over a hundred OD flows, these flows can be accurately modeled in time using a small number (10 or less) of independent components or dimensions.We also show how to use PCA to systematically decompose the structure of OD flow timeseries into three main constituents: common periodic trends, short-lived bursts, and noise. We provide insight into how the various constitutents contribute to the overall structure of OD flows and explore the extent to which this decomposition varies over time.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005697},
 doi = {http://doi.acm.org/10.1145/1005686.1005697},
 acmid = {1005697},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network traffic analysis, principal component analysis, traffic engineering},
} 

@article{Lakhina:2004:SAN:1012888.1005697,
 author = {Lakhina, Anukool and Papagiannaki, Konstantina and Crovella, Mark and Diot, Christophe and Kolaczyk, Eric D. and Taft, Nina},
 title = {Structural analysis of network traffic flows},
 abstract = {Network traffic arises from the superposition of Origin-Destination (OD) flows. Hence, a thorough understanding of OD flows is essential for modeling network traffic, and for addressing a wide variety of problems including traffic engineering, traffic matrix estimation, capacity planning, forecasting and anomaly detection. However, to date, OD flows have not been closely studied, and there is very little known about their properties.We present the first analysis of complete sets of OD flow time-series, taken from two different backbone networks (Abilene and Sprint-Europe). Using Principal Component Analysis (PCA), we find that the set of OD flows has small intrinsic dimension. In fact, even in a network with over a hundred OD flows, these flows can be accurately modeled in time using a small number (10 or less) of independent components or dimensions.We also show how to use PCA to systematically decompose the structure of OD flow timeseries into three main constituents: common periodic trends, short-lived bursts, and noise. We provide insight into how the various constitutents contribute to the overall structure of OD flows and explore the extent to which this decomposition varies over time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005697},
 doi = {http://doi.acm.org/10.1145/1012888.1005697},
 acmid = {1005697},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network traffic analysis, principal component analysis, traffic engineering},
} 

@article{Soule:2004:IEL:1012888.1005698,
 author = {Soule, Augustin and Nucci, Antonio and Cruz, Rene and Leonardi, Emilio and Taft, Nina},
 title = {How to identify and estimate the largest traffic matrix elements in a dynamic environment},
 abstract = {In this paper we investigate a new idea for traffic matrix estimation that makes the basic problem less under-constrained, by deliberately changing the routing to obtain additional measurements. Because all these measurements are collected over disparate time intervals, we need to establish models for each Origin-Destination (OD) pair to capture the complex behaviours of internet traffic. We model each OD pair with two components: the diurnal pattern and the fluctuation process. We provide models that incorporate the two components above, to estimate both the first and second order moments of traffic matrices. We do this for both stationary and cyclo-stationary traffic scenarios. We formalize the problem of estimating the second order moment in a way that is completely independent from the first order moment. Moreover, we can estimate the second order moment without needing any routing changes (i.e., without explicit changes to IGP link weights). We prove for the first time, that such a result holds for any realistic topology under the assumption of minimum cost routing</i> and strictly positive link weights</i>. We highlight how the second order moment helps the identification of the top largest OD flows carrying the most significant fraction of network traffic. We then propose a refined methodology consisting of using our variance estimator (without routing changes) to identify the top largest flows, and estimate only these flows. The benefit of this method is that it dramatically reduces the number of routing changes needed. We validate the effectiveness of our methodology and the intuitions behind it by using real aggregated sampled netflow data collected from a commercial Tier-1 backbone.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005698},
 doi = {http://doi.acm.org/10.1145/1012888.1005698},
 acmid = {1005698},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network tomography, traffic matrix estimation},
} 

@inproceedings{Soule:2004:IEL:1005686.1005698,
 author = {Soule, Augustin and Nucci, Antonio and Cruz, Rene and Leonardi, Emilio and Taft, Nina},
 title = {How to identify and estimate the largest traffic matrix elements in a dynamic environment},
 abstract = {In this paper we investigate a new idea for traffic matrix estimation that makes the basic problem less under-constrained, by deliberately changing the routing to obtain additional measurements. Because all these measurements are collected over disparate time intervals, we need to establish models for each Origin-Destination (OD) pair to capture the complex behaviours of internet traffic. We model each OD pair with two components: the diurnal pattern and the fluctuation process. We provide models that incorporate the two components above, to estimate both the first and second order moments of traffic matrices. We do this for both stationary and cyclo-stationary traffic scenarios. We formalize the problem of estimating the second order moment in a way that is completely independent from the first order moment. Moreover, we can estimate the second order moment without needing any routing changes (i.e., without explicit changes to IGP link weights). We prove for the first time, that such a result holds for any realistic topology under the assumption of minimum cost routing</i> and strictly positive link weights</i>. We highlight how the second order moment helps the identification of the top largest OD flows carrying the most significant fraction of network traffic. We then propose a refined methodology consisting of using our variance estimator (without routing changes) to identify the top largest flows, and estimate only these flows. The benefit of this method is that it dramatically reduces the number of routing changes needed. We validate the effectiveness of our methodology and the intuitions behind it by using real aggregated sampled netflow data collected from a commercial Tier-1 backbone.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005698},
 doi = {http://doi.acm.org/10.1145/1005686.1005698},
 acmid = {1005698},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network tomography, traffic matrix estimation},
} 

@inproceedings{Duffield:2004:FSU:1005686.1005699,
 author = {Duffield, Nick and Lund, Carsten and Thorup, Mikkel},
 title = {Flow sampling under hard resource constraints},
 abstract = {Many network management applications use as their data traffic volumes differentiated by attributes such as IP address or port number. IP flow records are commonly collected for this purpose: these enable determination of fine-grained usage of network resources. However, the increasingly large volumes of flow statistics incur concomitant costs in the resources of the measurement infrastructure. This motivates sampling of flow records.This paper addresses sampling strategy for flow records. Recent work has shown that non-uniform sampling is necessary in order to control estimation variance arising from the observed heavy-tailed distribution of flow lengths. However, while this approach controls estimator variance, it does not place hard limits on the number of flows sampled. Such limits are often required during arbitrary downstream sampling, resampling and aggregation operations employed in analysis of the data.This paper proposes a correlated sampling strategy that is able to select an arbitrarily small number of the "best" representatives of a set of flows. We show that usage estimates arising from such selection are unbiased, and show how to estimate their variance, both offline for modeling purposes, and online during the sampling itself. The selection algorithm can be implemented in a queue-like data structure in which memory usage is uniformly bounded during measurement. Finally, we compare the complexity and performance of our scheme with other potential approaches.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005699},
 doi = {http://doi.acm.org/10.1145/1005686.1005699},
 acmid = {1005699},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IP flows, sampling, variance reduction},
} 

@article{Duffield:2004:FSU:1012888.1005699,
 author = {Duffield, Nick and Lund, Carsten and Thorup, Mikkel},
 title = {Flow sampling under hard resource constraints},
 abstract = {Many network management applications use as their data traffic volumes differentiated by attributes such as IP address or port number. IP flow records are commonly collected for this purpose: these enable determination of fine-grained usage of network resources. However, the increasingly large volumes of flow statistics incur concomitant costs in the resources of the measurement infrastructure. This motivates sampling of flow records.This paper addresses sampling strategy for flow records. Recent work has shown that non-uniform sampling is necessary in order to control estimation variance arising from the observed heavy-tailed distribution of flow lengths. However, while this approach controls estimator variance, it does not place hard limits on the number of flows sampled. Such limits are often required during arbitrary downstream sampling, resampling and aggregation operations employed in analysis of the data.This paper proposes a correlated sampling strategy that is able to select an arbitrarily small number of the "best" representatives of a set of flows. We show that usage estimates arising from such selection are unbiased, and show how to estimate their variance, both offline for modeling purposes, and online during the sampling itself. The selection algorithm can be implemented in a queue-like data structure in which memory usage is uniformly bounded during measurement. Finally, we compare the complexity and performance of our scheme with other potential approaches.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005699},
 doi = {http://doi.acm.org/10.1145/1012888.1005699},
 acmid = {1005699},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IP flows, sampling, variance reduction},
} 

@inproceedings{Aalto:2004:TPS:1005686.1005701,
 author = {Aalto, Samuli and Ayesta, Urtzi and Nyberg-Oksanen, Eeva},
 title = {Two-level processor-sharing scheduling disciplines: mean delay analysis},
 abstract = {Inspired by several recent papers that focus on scheduling disciplines for network flows, we present a mean delay analysis of Multilevel Processor Sharing (MLPS) scheduling disciplines in the context of M/G/1 queues. Such disciplines have been proposed to model the effect of the differentiation between short and long TCP flows in the Internet. Under MLPS, jobs are classified into classes depending on their attained service. We consider scheduling disciplines where jobs within the same class are served either with Processor Sharing (PS) or Foreground Background (FB) policy, and the class that contains jobs with the smallest attained service is served first. It is known that the FB policy minimizes (maximizes) the mean delay when the hazard rate of the job size distribution is decreasing (increasing). Our analysis, based on pathwise and meanwise arguments of the unfinished truncated work, shows that Two-Level Processor Sharing (TLPS) disciplines, e.g., FB+PS and PS+PS, are better than PS scheduling when the hazard rate of the job size distribution is decreasing. If the hazard rate is increasing and bounded, we show that PS outperforms PS+PS and FB+PS. We further extend our analysis to study local optimality within a level of an MLPS scheduling discipline.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {97--105},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1005686.1005701},
 doi = {http://doi.acm.org/10.1145/1005686.1005701},
 acmid = {1005701},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FB, LAS, M/G/1, MLPS, PS, mean delay, scheduling, unfinished truncated work},
} 

@article{Aalto:2004:TPS:1012888.1005701,
 author = {Aalto, Samuli and Ayesta, Urtzi and Nyberg-Oksanen, Eeva},
 title = {Two-level processor-sharing scheduling disciplines: mean delay analysis},
 abstract = {Inspired by several recent papers that focus on scheduling disciplines for network flows, we present a mean delay analysis of Multilevel Processor Sharing (MLPS) scheduling disciplines in the context of M/G/1 queues. Such disciplines have been proposed to model the effect of the differentiation between short and long TCP flows in the Internet. Under MLPS, jobs are classified into classes depending on their attained service. We consider scheduling disciplines where jobs within the same class are served either with Processor Sharing (PS) or Foreground Background (FB) policy, and the class that contains jobs with the smallest attained service is served first. It is known that the FB policy minimizes (maximizes) the mean delay when the hazard rate of the job size distribution is decreasing (increasing). Our analysis, based on pathwise and meanwise arguments of the unfinished truncated work, shows that Two-Level Processor Sharing (TLPS) disciplines, e.g., FB+PS and PS+PS, are better than PS scheduling when the hazard rate of the job size distribution is decreasing. If the hazard rate is increasing and bounded, we show that PS outperforms PS+PS and FB+PS. We further extend our analysis to study local optimality within a level of an MLPS scheduling discipline.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {97--105},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1012888.1005701},
 doi = {http://doi.acm.org/10.1145/1012888.1005701},
 acmid = {1005701},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FB, LAS, M/G/1, MLPS, PS, mean delay, scheduling, unfinished truncated work},
} 

@inproceedings{Rai:2004:PAL:1005686.1005702,
 author = {Rai, Idris A. and Urvoy-Keller, Guillaume and Vernon, Mary K. and Biersack, Ernst W.},
 title = {Performance analysis of LAS-based scheduling disciplines in a packet switched network},
 abstract = {The Least Attained Service (LAS) scheduling policy, when used for scheduling packets over the bottleneck link of an Internet path, can greatly reduce the average flow time for short flows while not significantly increasing the average flow time for the long flows that share the same bottleneck. No modification of the packet headers is required to implement the simple LAS policy. However, previous work has also shown that a drawback of the LAS scheduler is that, when link utilization is greater than 70\%, long flows experience large jitter in their packet transfer times as compared to the conventional First-Come-First-Serve (FCFS) link scheduling. This paper proposes and evaluates new differentiated LAS scheduling policies that reduce the jitter for long flows that are identified as "priority" flows.To evaluate the new policies, we develop analytic models to estimate average flow transfer time as a function of flow size, and average packet transmission time as a function of position in the flow, for the single-bottleneck "dumbbell topology" used in many ns simulation studies. Models are developed for FCFS scheduling, LAS scheduling, and each of the new differentiated LAS scheduling policies at the bottleneck link. Over a wide range of configu-rations, the analytic estimates agree very closely with the ns estimates. Thus, the analytic models can be used instead of simulation for comparing the policies with respect to mean flow transfer time (as a function of flow size) and mean packet transfer time. Furthermore, an initial discrepancy between the analytic and simulation estimates revealed errors in the parameter values that are often specified in the widely used ns Web workload generator. We develop an improved Web workload specification, which is used to estimate the packet jitter for long flows (more accurately than with previous simulation workloads).Results for the scheduling policies show that a particular policy, LAS-log, greatly improves the mean flow transfer time for priority long flows while providing performance similar to LAS for the ordinary flows. Simulations show that the LAS-log policy also greatly reduces the jitter in packet delivery times for the priority flows.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {106--117},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005702},
 doi = {http://doi.acm.org/10.1145/1005686.1005702},
 acmid = {1005702},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS and LAS models, LAS-based scheduling and models, models validation, scheduling, service differentiation, simulations},
} 

@article{Rai:2004:PAL:1012888.1005702,
 author = {Rai, Idris A. and Urvoy-Keller, Guillaume and Vernon, Mary K. and Biersack, Ernst W.},
 title = {Performance analysis of LAS-based scheduling disciplines in a packet switched network},
 abstract = {The Least Attained Service (LAS) scheduling policy, when used for scheduling packets over the bottleneck link of an Internet path, can greatly reduce the average flow time for short flows while not significantly increasing the average flow time for the long flows that share the same bottleneck. No modification of the packet headers is required to implement the simple LAS policy. However, previous work has also shown that a drawback of the LAS scheduler is that, when link utilization is greater than 70\%, long flows experience large jitter in their packet transfer times as compared to the conventional First-Come-First-Serve (FCFS) link scheduling. This paper proposes and evaluates new differentiated LAS scheduling policies that reduce the jitter for long flows that are identified as "priority" flows.To evaluate the new policies, we develop analytic models to estimate average flow transfer time as a function of flow size, and average packet transmission time as a function of position in the flow, for the single-bottleneck "dumbbell topology" used in many ns simulation studies. Models are developed for FCFS scheduling, LAS scheduling, and each of the new differentiated LAS scheduling policies at the bottleneck link. Over a wide range of configu-rations, the analytic estimates agree very closely with the ns estimates. Thus, the analytic models can be used instead of simulation for comparing the policies with respect to mean flow transfer time (as a function of flow size) and mean packet transfer time. Furthermore, an initial discrepancy between the analytic and simulation estimates revealed errors in the parameter values that are often specified in the widely used ns Web workload generator. We develop an improved Web workload specification, which is used to estimate the packet jitter for long flows (more accurately than with previous simulation workloads).Results for the scheduling policies show that a particular policy, LAS-log, greatly improves the mean flow transfer time for priority long flows while providing performance similar to LAS for the ordinary flows. Simulations show that the LAS-log policy also greatly reduces the jitter in packet delivery times for the priority flows.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {106--117},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005702},
 doi = {http://doi.acm.org/10.1145/1012888.1005702},
 acmid = {1005702},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS and LAS models, LAS-based scheduling and models, models validation, scheduling, service differentiation, simulations},
} 

@inproceedings{Key:2004:ELT:1005686.1005703,
 author = {Key, Peter and Massouli\'{e}, Laurent and Wang, Bing},
 title = {Emulating low-priority transport at the application layer: a background transfer service},
 abstract = {Low priority data transfer across the wide area is useful in several contexts, for example for the dissemination of large files such as OS updates, content distribution or prefetching. Although the design of such a service is reasonably easy when the underlying network supports service differentiation, it becomes more challenging without such network support. We describe an application level approach to designing a low priority service -- one that is 'lower than best-effort' in the context of the current Internet. We require neither network support nor changes to TCP. Instead, we use a receive window control to limit the transfer rate of the application, and the optimal rate is determined by detecting a change-point. We motivate this joint control-estimation problem by considering a fluid-based optimisation framework, and describe practical solutions, based on stochastic approximation and binary search techniques. Simulation results demonstrate the effectiveness of the approach.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {118--129},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005703},
 doi = {http://doi.acm.org/10.1145/1005686.1005703},
 acmid = {1005703},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application reaction, background transfer, binary search, low priority, stochastic approximation},
} 

@article{Key:2004:ELT:1012888.1005703,
 author = {Key, Peter and Massouli\'{e}, Laurent and Wang, Bing},
 title = {Emulating low-priority transport at the application layer: a background transfer service},
 abstract = {Low priority data transfer across the wide area is useful in several contexts, for example for the dissemination of large files such as OS updates, content distribution or prefetching. Although the design of such a service is reasonably easy when the underlying network supports service differentiation, it becomes more challenging without such network support. We describe an application level approach to designing a low priority service -- one that is 'lower than best-effort' in the context of the current Internet. We require neither network support nor changes to TCP. Instead, we use a receive window control to limit the transfer rate of the application, and the optimal rate is determined by detecting a change-point. We motivate this joint control-estimation problem by considering a fluid-based optimisation framework, and describe practical solutions, based on stochastic approximation and binary search techniques. Simulation results demonstrate the effectiveness of the approach.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {118--129},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005703},
 doi = {http://doi.acm.org/10.1145/1012888.1005703},
 acmid = {1005703},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application reaction, background transfer, binary search, low priority, stochastic approximation},
} 

@article{Raz:2004:RQF:1012888.1005704,
 author = {Raz, David and Levy, Hanoch and Avi-Itzhak, Benjamin},
 title = {A resource-allocation queueing fairness measure},
 abstract = {Fairness is a major issue in the operation of queues, perhaps it is the reason why queues were formed in the first place. Recent studies show that the fairness of a queueing system is important to customers not less than the actual delay they experience. Despite this observation little research has been conducted to study fairness in queues, and no commonly agreed upon measure of queue fairness exists. Two recent research exceptions are Avi-Itzhak and Levy [1], where a fairness measure is proposed, and Wierman and Harchol-Balter [18] (this conference, 2003), where a criterion</i> is proposed for classifying service policies as fair or unfair; the criterion focuses on customer service requirement and deals with fairness with respect to service times.In this work we recognize that the inherent behavior of a queueing system is governed by two major factors: Job seniority</i> (arrival times) and job service requirement</i> (service time). Thus, it is desired that a queueing fairness measure would account for both. To this end we propose a Resource Allocation Queueing Fairness Measure, (RAQFM), that accounts for both relative job seniority and relative service time. The measure allows accounting for individual job discrimination as well as system unfairness. The system measure forms a full scale that can be used to evaluate the level of unfairness under various queueing disciplines. We present several basic properties of the measure. We derive the individual measure as well as the system measure for an M/M/1 queue under five fundamental service policies: Processor Sharing (PS), First Come First Served (FCFS), Non-Preemptive Last Come First Served (NP-LCFS), Preemptive Last Come First Served (P-LCFS), and Random Order of Service (ROS). The results of RAQFM are then compared to those of Wierman and Harchol-Balter [18], and the quite intriguing observed differences are discussed.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {130--141},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005704},
 doi = {http://doi.acm.org/10.1145/1012888.1005704},
 acmid = {1005704},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS, M/M/1, PS, fairness, job scheduling, processor sharing, queue disciplines, resource allocation, unfairness},
} 

@inproceedings{Raz:2004:RQF:1005686.1005704,
 author = {Raz, David and Levy, Hanoch and Avi-Itzhak, Benjamin},
 title = {A resource-allocation queueing fairness measure},
 abstract = {Fairness is a major issue in the operation of queues, perhaps it is the reason why queues were formed in the first place. Recent studies show that the fairness of a queueing system is important to customers not less than the actual delay they experience. Despite this observation little research has been conducted to study fairness in queues, and no commonly agreed upon measure of queue fairness exists. Two recent research exceptions are Avi-Itzhak and Levy [1], where a fairness measure is proposed, and Wierman and Harchol-Balter [18] (this conference, 2003), where a criterion</i> is proposed for classifying service policies as fair or unfair; the criterion focuses on customer service requirement and deals with fairness with respect to service times.In this work we recognize that the inherent behavior of a queueing system is governed by two major factors: Job seniority</i> (arrival times) and job service requirement</i> (service time). Thus, it is desired that a queueing fairness measure would account for both. To this end we propose a Resource Allocation Queueing Fairness Measure, (RAQFM), that accounts for both relative job seniority and relative service time. The measure allows accounting for individual job discrimination as well as system unfairness. The system measure forms a full scale that can be used to evaluate the level of unfairness under various queueing disciplines. We present several basic properties of the measure. We derive the individual measure as well as the system measure for an M/M/1 queue under five fundamental service policies: Processor Sharing (PS), First Come First Served (FCFS), Non-Preemptive Last Come First Served (NP-LCFS), Preemptive Last Come First Served (P-LCFS), and Random Order of Service (ROS). The results of RAQFM are then compared to those of Wierman and Harchol-Balter [18], and the quite intriguing observed differences are discussed.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {130--141},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005704},
 doi = {http://doi.acm.org/10.1145/1005686.1005704},
 acmid = {1005704},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS, M/M/1, PS, fairness, job scheduling, processor sharing, queue disciplines, resource allocation, unfairness},
} 

@inproceedings{Paxson:2004:MA:1005686.1005688,
 author = {Paxson, Vern},
 title = {Measuring adversaries},
 abstract = {Many concepts and techniques developed for general Internet measurement have counterparts in the domain of detecting and analyzing network attacks. The task is greatly complicated, however, by the fact that the object of study is adversarial</i>: attackers do not wish to be "measured" and will take steps to thwart observation. We look at the far-ranging consequences of this different measurement environment: the analysis difficulties-some fundamental-that arise due to subtle ambiguities in the true semantics of observed traffic; new notions of "active measurement"; the highly challenging task of rapidly characterizing Internet-scale pheonmena such as global worm pandemics; the need for detailed application-level analysis and related policy and legal difficulties; attacks that target passive analysis tools; and the inherent "arms race" nature of the undertaking.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {142--142},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1005686.1005688},
 doi = {http://doi.acm.org/10.1145/1005686.1005688},
 acmid = {1005688},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Paxson:2004:MA:1012888.1005688,
 author = {Paxson, Vern},
 title = {Measuring adversaries},
 abstract = {Many concepts and techniques developed for general Internet measurement have counterparts in the domain of detecting and analyzing network attacks. The task is greatly complicated, however, by the fact that the object of study is adversarial</i>: attackers do not wish to be "measured" and will take steps to thwart observation. We look at the far-ranging consequences of this different measurement environment: the analysis difficulties-some fundamental-that arise due to subtle ambiguities in the true semantics of observed traffic; new notions of "active measurement"; the highly challenging task of rapidly characterizing Internet-scale pheonmena such as global worm pandemics; the need for detailed application-level analysis and related policy and legal difficulties; attacks that target passive analysis tools; and the inherent "arms race" nature of the undertaking.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {142--142},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1012888.1005688},
 doi = {http://doi.acm.org/10.1145/1012888.1005688},
 acmid = {1005688},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:2004:FSF:1005686.1005706,
 author = {Kim, Hwangnam and Hou, Jennifer C.},
 title = {A fast simulation framework for IEEE 802.11-operated wireless LANs},
 abstract = {In this paper, we develop a fast simulation framework for IEEE 802.11-operated wireless LANs (WLANs), in which a large number of packets are abstracted as a single fluid chunk, and their behaviors are approximated with analytic fluid models and figured into the simulation. We first derive the analytical model that characterizes data transmission activities in IEEE 802.11-operated WLANs with/without the RTS/CTS mechanism. All the control overhead incurred in the physical and MAC layers, as well as system parameters specified in IEEE 802.11 [12] are faithfully figured in. We validate the model with simulation in cases in which the network is and is not saturated. We then implement, with the use of the time stepping technique [21], the fast simulation framework for WLANs in ns-2</i> [2], and conduct a comprehensive simulation study to evaluate the framework in terms of speed-up and errors incurred under a variety of network configurations.The simulation results indicate that the proposed framework is indeed effective in simulating IEEE 802.11-operated WLANs. It achieves as much as two orders of magnitude improvement in terms of execution time as compared to packet-level simulation. The performance improvement is more pronounced when the number of wireless nodes, the number of applications running on each wireless node, or the number of WLANs increases. The relative error, on the other hand, falls within 2\% in all cases, as long as the value of the time step is appropriately determined.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005706},
 doi = {http://doi.acm.org/10.1145/1005686.1005706},
 acmid = {1005706},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IEEE 802.11, fast simulation, throughput analysis, wireless LANs},
} 

@article{Kim:2004:FSF:1012888.1005706,
 author = {Kim, Hwangnam and Hou, Jennifer C.},
 title = {A fast simulation framework for IEEE 802.11-operated wireless LANs},
 abstract = {In this paper, we develop a fast simulation framework for IEEE 802.11-operated wireless LANs (WLANs), in which a large number of packets are abstracted as a single fluid chunk, and their behaviors are approximated with analytic fluid models and figured into the simulation. We first derive the analytical model that characterizes data transmission activities in IEEE 802.11-operated WLANs with/without the RTS/CTS mechanism. All the control overhead incurred in the physical and MAC layers, as well as system parameters specified in IEEE 802.11 [12] are faithfully figured in. We validate the model with simulation in cases in which the network is and is not saturated. We then implement, with the use of the time stepping technique [21], the fast simulation framework for WLANs in ns-2</i> [2], and conduct a comprehensive simulation study to evaluate the framework in terms of speed-up and errors incurred under a variety of network configurations.The simulation results indicate that the proposed framework is indeed effective in simulating IEEE 802.11-operated WLANs. It achieves as much as two orders of magnitude improvement in terms of execution time as compared to packet-level simulation. The performance improvement is more pronounced when the number of wireless nodes, the number of applications running on each wireless node, or the number of WLANs increases. The relative error, on the other hand, falls within 2\% in all cases, as long as the value of the time step is appropriately determined.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005706},
 doi = {http://doi.acm.org/10.1145/1012888.1005706},
 acmid = {1005706},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IEEE 802.11, fast simulation, throughput analysis, wireless LANs},
} 

@article{Hao:2004:AFM:1012888.1005707,
 author = {Hao, Fang and Kodialam, Murali and Lakshman, T. V.},
 title = {ACCEL-RATE: a faster mechanism for memory efficient per-flow traffic estimation},
 abstract = {Per-flow network traffic measurement is an important component of network traffic management, network performance assessment, and detection of anomalous network events such as incipient DoS attacks. In [1], the authors developed a mechanism called RATE where the focus was on developing a memory efficient scheme for estimating per-flow traffic rates to a specified level of accuracy. The time taken by RATE to estimate the per-flow rates is a function of the specified estimation accuracy and this time is acceptable for several applications. However some applications, such as quickly detecting worm related activity or the tracking of transient traffic, demand faster estimation times. The main contribution of this paper is a new scheme called ACCEL-RATE that, for a specified level of accuracy, can achieve orders of magnitude decrease in per-flow rate estimation times. It achieves this by using a hashing scheme to split the incoming traffic into several sub-streams, estimating the per-flow traffic rates in each of the substreams and then relating it back to the original per-flow traffic rates. We show both theoretically and experimentally that the estimation time of ACCEL-RATE is at least one to two orders of magnitude lower than RATE without any significant increase in the memory size.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005707},
 doi = {http://doi.acm.org/10.1145/1012888.1005707},
 acmid = {1005707},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hao:2004:AFM:1005686.1005707,
 author = {Hao, Fang and Kodialam, Murali and Lakshman, T. V.},
 title = {ACCEL-RATE: a faster mechanism for memory efficient per-flow traffic estimation},
 abstract = {Per-flow network traffic measurement is an important component of network traffic management, network performance assessment, and detection of anomalous network events such as incipient DoS attacks. In [1], the authors developed a mechanism called RATE where the focus was on developing a memory efficient scheme for estimating per-flow traffic rates to a specified level of accuracy. The time taken by RATE to estimate the per-flow rates is a function of the specified estimation accuracy and this time is acceptable for several applications. However some applications, such as quickly detecting worm related activity or the tracking of transient traffic, demand faster estimation times. The main contribution of this paper is a new scheme called ACCEL-RATE that, for a specified level of accuracy, can achieve orders of magnitude decrease in per-flow rate estimation times. It achieves this by using a hashing scheme to split the incoming traffic into several sub-streams, estimating the per-flow traffic rates in each of the substreams and then relating it back to the original per-flow traffic rates. We show both theoretically and experimentally that the estimation time of ACCEL-RATE is at least one to two orders of magnitude lower than RATE without any significant increase in the memory size.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {155--166},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005707},
 doi = {http://doi.acm.org/10.1145/1005686.1005707},
 acmid = {1005707},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Burtscher:2004:VFE:1012888.1005708,
 author = {Burtscher, Martin},
 title = {VPC3: a fast and effective trace-compression algorithm},
 abstract = {Trace files are widely used in research and academia to study the behavior of programs. They are simple to process and guarantee repeatability. Unfortunately, they tend to be very large. This paper describes vpc3</i>, a fundamentally new approach to compressing program traces. Vpc3</i> employs value predictors to bring out and amplify patterns in the traces so that conventional compressors can compress them more effectively. In fact, our approach not only results in much higher compression rates but also provides faster compression and decompression. For example, compared to bzip2</i>, vpc3</i>'s geometric mean compression rate on SPECcpu2000 store address traces is 18.4 times higher, compression is ten times faster, and decompression is three times faster.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1012888.1005708},
 doi = {http://doi.acm.org/10.1145/1012888.1005708},
 acmid = {1005708},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {predictor-based compression, trace compression, trace files},
} 

@inproceedings{Burtscher:2004:VFE:1005686.1005708,
 author = {Burtscher, Martin},
 title = {VPC3: a fast and effective trace-compression algorithm},
 abstract = {Trace files are widely used in research and academia to study the behavior of programs. They are simple to process and guarantee repeatability. Unfortunately, they tend to be very large. This paper describes vpc3</i>, a fundamentally new approach to compressing program traces. Vpc3</i> employs value predictors to bring out and amplify patterns in the traces so that conventional compressors can compress them more effectively. In fact, our approach not only results in much higher compression rates but also provides faster compression and decompression. For example, compared to bzip2</i>, vpc3</i>'s geometric mean compression rate on SPECcpu2000 store address traces is 18.4 times higher, compression is ten times faster, and decompression is three times faster.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1005686.1005708},
 doi = {http://doi.acm.org/10.1145/1005686.1005708},
 acmid = {1005708},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {predictor-based compression, trace compression, trace files},
} 

@inproceedings{Kumar:2004:DSA:1005686.1005709,
 author = {Kumar, Abhishek and Sung, Minho and Xu, Jun (Jim) and Wang, Jia},
 title = {Data streaming algorithms for efficient and accurate estimation of flow size distribution},
 abstract = {Knowing the distribution of the sizes of traffic flows passing through a network link helps a network operator to characterize network resource usage, infer traffic demands, detect traffic anomalies, and accommodate new traffic demands through better traffic engineering. Previous work on estimating the flow size distribution has been focused on making inferences from sampled network traffic. Its accuracy is limited by the (typically) low sampling rate required to make the sampling operation affordable. In this paper we present a novel data streaming algorithm to provide much more accurate estimates of flow distribution, using a "lossy data structure" which consists of an array of counters fitted well into SRAM. For each incoming packet, our algorithm only needs to increment one underlying counter, making the algorithm fast enough even for 40 Gbps (OC-768) links. The data structure is lossy in the sense that sizes of multiple flows may collide into the same counter. Our algorithm uses Bayesian statistical methods such as Expectation Maximization to infer the most likely flow size distribution that results in the observed counter values after collision. Evaluations of this algorithm on large Internet traces obtained from several sources (including a tier-1 ISP) demonstrate that it has very high measurement accuracy (within 2\%). Our algorithm not only dramatically improves the accuracy of flow distribution measurement, but also contributes to the field of data streaming by formalizing an existing methodology and applying it to the context of estimating the flow-distribution.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005709},
 doi = {http://doi.acm.org/10.1145/1005686.1005709},
 acmid = {1005709},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, network measurement, statistical inference, traffic analysis},
} 

@article{Kumar:2004:DSA:1012888.1005709,
 author = {Kumar, Abhishek and Sung, Minho and Xu, Jun (Jim) and Wang, Jia},
 title = {Data streaming algorithms for efficient and accurate estimation of flow size distribution},
 abstract = {Knowing the distribution of the sizes of traffic flows passing through a network link helps a network operator to characterize network resource usage, infer traffic demands, detect traffic anomalies, and accommodate new traffic demands through better traffic engineering. Previous work on estimating the flow size distribution has been focused on making inferences from sampled network traffic. Its accuracy is limited by the (typically) low sampling rate required to make the sampling operation affordable. In this paper we present a novel data streaming algorithm to provide much more accurate estimates of flow distribution, using a "lossy data structure" which consists of an array of counters fitted well into SRAM. For each incoming packet, our algorithm only needs to increment one underlying counter, making the algorithm fast enough even for 40 Gbps (OC-768) links. The data structure is lossy in the sense that sizes of multiple flows may collide into the same counter. Our algorithm uses Bayesian statistical methods such as Expectation Maximization to infer the most likely flow size distribution that results in the observed counter values after collision. Evaluations of this algorithm on large Internet traces obtained from several sources (including a tier-1 ISP) demonstrate that it has very high measurement accuracy (within 2\%). Our algorithm not only dramatically improves the accuracy of flow distribution measurement, but also contributes to the field of data streaming by formalizing an existing methodology and applying it to the context of estimating the flow-distribution.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {177--188},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005709},
 doi = {http://doi.acm.org/10.1145/1012888.1005709},
 acmid = {1005709},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, network measurement, statistical inference, traffic analysis},
} 

@article{Ma:2004:GTA:1012888.1005711,
 author = {Ma, Richard T. B. and Lee, Sam C. M. and Lui, John C. S. and Yau, David K. Y.},
 title = {A game theoretic approach to provide incentive and service differentiation in P2P networks},
 abstract = {Traditional peer-to-peer (P2P) networks do not provide service differentiation and incentive for users. Consequently, users can obtain services without themselves contributing any information or service to a P2P community. This leads to the "free-riding" and "tragedy of the commons" problems, in which the majority of information requests are directed towards a small number of P2P nodes willing to share their resources. The objective of this work is to enable service differentiation in a P2P network based on the amount of services each node has provided to its community, thereby encouraging all network nodes to share resources. We first introduce a resource distribution mechanism between all information sharing nodes. The mechanism is driven by a distributed algorithm which has linear time complexity and guarantees Pareto-optimal resource allocation. Besides giving incentive, the mechanism distributes resources in a way that increases the aggregate utility of the whole network. Second, we model the whole resource request and distribution process as a competition game between the competing nodes. We show that this game has a Nash equilibrium and is collusion-proof. To realize the game, we propose a protocol in which all competing nodes interact with the information providing node to reach Nash equilibrium in a dynamic and efficient manner. Experimental results are reported to illustrate that the protocol achieves its service differentiation objective and can induce productive information sharing by rational network nodes. Finally, we show that our protocol can properly adapt to different node arrival and departure events, and to different forms of network congestion.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1012888.1005711},
 doi = {http://doi.acm.org/10.1145/1012888.1005711},
 acmid = {1005711},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ma:2004:GTA:1005686.1005711,
 author = {Ma, Richard T. B. and Lee, Sam C. M. and Lui, John C. S. and Yau, David K. Y.},
 title = {A game theoretic approach to provide incentive and service differentiation in P2P networks},
 abstract = {Traditional peer-to-peer (P2P) networks do not provide service differentiation and incentive for users. Consequently, users can obtain services without themselves contributing any information or service to a P2P community. This leads to the "free-riding" and "tragedy of the commons" problems, in which the majority of information requests are directed towards a small number of P2P nodes willing to share their resources. The objective of this work is to enable service differentiation in a P2P network based on the amount of services each node has provided to its community, thereby encouraging all network nodes to share resources. We first introduce a resource distribution mechanism between all information sharing nodes. The mechanism is driven by a distributed algorithm which has linear time complexity and guarantees Pareto-optimal resource allocation. Besides giving incentive, the mechanism distributes resources in a way that increases the aggregate utility of the whole network. Second, we model the whole resource request and distribution process as a competition game between the competing nodes. We show that this game has a Nash equilibrium and is collusion-proof. To realize the game, we propose a protocol in which all competing nodes interact with the information providing node to reach Nash equilibrium in a dynamic and efficient manner. Experimental results are reported to illustrate that the protocol achieves its service differentiation objective and can induce productive information sharing by rational network nodes. Finally, we show that our protocol can properly adapt to different node arrival and departure events, and to different forms of network congestion.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1005686.1005711},
 doi = {http://doi.acm.org/10.1145/1005686.1005711},
 acmid = {1005711},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lam:2004:FRS:1005686.1005712,
 author = {Lam, Simon S. and Liu, Huaiyu},
 title = {Failure recovery for structured P2P networks: protocol design and performance evaluation},
 abstract = {Measurement studies indicate a high rate of node dynamics in p2p systems. In this paper, we address the question of how high a rate of node dynamics can be supported by structured</i> p2p networks. We confine our study to the hypercube routing scheme used by several structured p2p systems. To improve system robustness and facilitate failure recovery, we introduce the property of K-consistency</i>, K</i> \&#8805; 1, which generalizes consistency defined previously. (Consistency guarantees connectivity from any node to any other node.) We design and evaluate a failure recovery protocol based upon local information for K-consistent networks. The failure recovery protocol is then integrated with a join protocol that has been proved to construct K-consistent neighbor tables for concurrent joins. The integrated protocols were evaluated by a set of simulation experiments in which nodes joined a 2000-node network and nodes (both old and new) were randomly selected to fail concurrently over 10,000 seconds of simulated time. In each such "churn" experiment, we took a "snapshot" of neighbor tables in the network once every 50 seconds and evaluated connectivity and consistency measures over time as a function of the churn rate, timeout value in failure recovery, and K</i>. Storage and communication overheads were also evaluated. We found our protocols to be effective, efficient, and stable for an average node lifetime as low as 8.3 minutes (the median lifetime measured for Napster and Gnutella was 60 minutes [10]).},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005712},
 doi = {http://doi.acm.org/10.1145/1005686.1005712},
 acmid = {1005712},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure recovery, hypercube routing, k-consistency, peer-to-peer networks, sustainable churn rate},
} 

@article{Lam:2004:FRS:1012888.1005712,
 author = {Lam, Simon S. and Liu, Huaiyu},
 title = {Failure recovery for structured P2P networks: protocol design and performance evaluation},
 abstract = {Measurement studies indicate a high rate of node dynamics in p2p systems. In this paper, we address the question of how high a rate of node dynamics can be supported by structured</i> p2p networks. We confine our study to the hypercube routing scheme used by several structured p2p systems. To improve system robustness and facilitate failure recovery, we introduce the property of K-consistency</i>, K</i> \&#8805; 1, which generalizes consistency defined previously. (Consistency guarantees connectivity from any node to any other node.) We design and evaluate a failure recovery protocol based upon local information for K-consistent networks. The failure recovery protocol is then integrated with a join protocol that has been proved to construct K-consistent neighbor tables for concurrent joins. The integrated protocols were evaluated by a set of simulation experiments in which nodes joined a 2000-node network and nodes (both old and new) were randomly selected to fail concurrently over 10,000 seconds of simulated time. In each such "churn" experiment, we took a "snapshot" of neighbor tables in the network once every 50 seconds and evaluated connectivity and consistency measures over time as a function of the churn rate, timeout value in failure recovery, and K</i>. Storage and communication overheads were also evaluated. We found our protocols to be effective, efficient, and stable for an average node lifetime as low as 8.3 minutes (the median lifetime measured for Napster and Gnutella was 60 minutes [10]).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005712},
 doi = {http://doi.acm.org/10.1145/1012888.1005712},
 acmid = {1005712},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {failure recovery, hypercube routing, k-consistency, peer-to-peer networks, sustainable churn rate},
} 

@article{Wang:2004:ZPN:1012888.1005713,
 author = {Wang, Xiaoming and Zhang, Yueping and Li, Xiafeng and Loguinov, Dmitri},
 title = {On zone-balancing of peer-to-peer networks: analysis of random node join},
 abstract = {Balancing peer-to-peer graphs, including zone-size distributions, has recently become an important topic of peer-to-peer (P2P) research [1], [2], [6], [19], [31], [36]. To bring analytical understanding into the various peer-join mechanisms, we study how zone-balancing decisions made during the initial sampling of the peer space affect the resulting zone sizes and derive several asymptotic results for the maximum and minimum zone sizes that hold with high probability.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005713},
 doi = {http://doi.acm.org/10.1145/1012888.1005713},
 acmid = {1005713},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {balls-into-bins, load-balancing, modeling, peer-to-peer},
} 

@inproceedings{Wang:2004:ZPN:1005686.1005713,
 author = {Wang, Xiaoming and Zhang, Yueping and Li, Xiafeng and Loguinov, Dmitri},
 title = {On zone-balancing of peer-to-peer networks: analysis of random node join},
 abstract = {Balancing peer-to-peer graphs, including zone-size distributions, has recently become an important topic of peer-to-peer (P2P) research [1], [2], [6], [19], [31], [36]. To bring analytical understanding into the various peer-join mechanisms, we study how zone-balancing decisions made during the initial sampling of the peer space affect the resulting zone sizes and derive several asymptotic results for the maximum and minimum zone sizes that hold with high probability.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {211--222},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005713},
 doi = {http://doi.acm.org/10.1145/1005686.1005713},
 acmid = {1005713},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {balls-into-bins, load-balancing, modeling, peer-to-peer},
} 

@article{Kansal:2004:PAT:1012888.1005714,
 author = {Kansal, Aman and Potter, Dunny and Srivastava, Mani B.},
 title = {Performance aware tasking for environmentally powered sensor networks},
 abstract = {The use of environmental energy is now emerging as a feasible energy source for embedded and wireless computing systems such as sensor networks where manual recharging or replacement of batteries is not practical. However, energy supply from environmental sources is highly variable with time. Further, for a distributed system, the energy available at its various locations will be different. These variations strongly influence the way in which environmental energy is used. We present a harvesting theory for determining performance in such systems. First we present a model for characterizing environmental sources. Second, we state and prove two harvesting theorems that help determine the sustainable performance level from a particular source. This theory leads to practical techniques for scheduling processes in energy harvesting systems. Third, we present our implementation of a real embedded system that runs on solar energy and uses our harvesting techniques. The system adjusts its performance level in response to available resources. Fourth, we propose a localized algorithm for increasing the performance of a distributed system by adapting the process scheduling to the spatio-temporal characteristics of the environmental energy in the distributed system. While our theoretical intuition is based on certain abstractions, all the scheduling methods we present are motivated solely from the experimental behavior and resource constraints of practical sensor networking systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005714},
 doi = {http://doi.acm.org/10.1145/1012888.1005714},
 acmid = {1005714},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy harvesting, performance guarantees, process scheduling},
} 

@inproceedings{Kansal:2004:PAT:1005686.1005714,
 author = {Kansal, Aman and Potter, Dunny and Srivastava, Mani B.},
 title = {Performance aware tasking for environmentally powered sensor networks},
 abstract = {The use of environmental energy is now emerging as a feasible energy source for embedded and wireless computing systems such as sensor networks where manual recharging or replacement of batteries is not practical. However, energy supply from environmental sources is highly variable with time. Further, for a distributed system, the energy available at its various locations will be different. These variations strongly influence the way in which environmental energy is used. We present a harvesting theory for determining performance in such systems. First we present a model for characterizing environmental sources. Second, we state and prove two harvesting theorems that help determine the sustainable performance level from a particular source. This theory leads to practical techniques for scheduling processes in energy harvesting systems. Third, we present our implementation of a real embedded system that runs on solar energy and uses our harvesting techniques. The system adjusts its performance level in response to available resources. Fourth, we propose a localized algorithm for increasing the performance of a distributed system by adapting the process scheduling to the spatio-temporal characteristics of the environmental energy in the distributed system. While our theoretical intuition is based on certain abstractions, all the scheduling methods we present are motivated solely from the experimental behavior and resource constraints of practical sensor networking systems.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {223--234},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005714},
 doi = {http://doi.acm.org/10.1145/1005686.1005714},
 acmid = {1005714},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy harvesting, performance guarantees, process scheduling},
} 

@inproceedings{Bonald:2004:PBI:1005686.1005716,
 author = {Bonald, Thomas and Prouti\`{e}re, Alexandre},
 title = {On performance bounds for the integration of elastic and adaptive streaming flows},
 abstract = {We consider a network model where bandwidth is fairly shared by a dynamic number of elastic and adaptive streaming flows. Elastic flows correspond to data transfers while adaptive streaming flows correspond to audio/video applications with variable rate codecs. In particular, the former are characterized by a fixed size (in bits) while the latter are characterized by a fixed duration. This flow-level model turns out to be intractable in general. In this paper, we give performance bounds for both elastic and streaming traffic by means of sample-path arguments. These bounds present the practical interest of being insensitive to traffic characteristics like the distributions of elastic flow size and streaming flow duration.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {235--245},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1005686.1005716},
 doi = {http://doi.acm.org/10.1145/1005686.1005716},
 acmid = {1005716},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive streaming traffic, elastic traffic, flow-level analysis, insensitive bounds, multi-service network},
} 

@article{Bonald:2004:PBI:1012888.1005716,
 author = {Bonald, Thomas and Prouti\`{e}re, Alexandre},
 title = {On performance bounds for the integration of elastic and adaptive streaming flows},
 abstract = {We consider a network model where bandwidth is fairly shared by a dynamic number of elastic and adaptive streaming flows. Elastic flows correspond to data transfers while adaptive streaming flows correspond to audio/video applications with variable rate codecs. In particular, the former are characterized by a fixed size (in bits) while the latter are characterized by a fixed duration. This flow-level model turns out to be intractable in general. In this paper, we give performance bounds for both elastic and streaming traffic by means of sample-path arguments. These bounds present the practical interest of being insensitive to traffic characteristics like the distributions of elastic flow size and streaming flow duration.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {235--245},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1012888.1005716},
 doi = {http://doi.acm.org/10.1145/1012888.1005716},
 acmid = {1005716},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive streaming traffic, elastic traffic, flow-level analysis, insensitive bounds, multi-service network},
} 

@inproceedings{Deb:2004:RVQ:1005686.1005717,
 author = {Deb, Supratim and Srikant, R.},
 title = {Rate-based versus queue-based models of congestion control},
 abstract = {Mathematical models of congestion control capture the congestion indication mechanism at the router in two different ways: rate-based models, where the queue-length at the router does not explicitly appear in the model, and queue-based models, where the queue length at the router is explicitly a part of the model. Even though most congestion indication mechanisms use the queue length to compute the packet marking or dropping probability to indicate congestion, we argue that, depending upon the choice of the parameters of the AQM scheme, one would obtain a rate-based model or a rate-and-queue-based model as the deterministic limit of a stochastic system with a large number of users. We also consider the impact of implementing AQM schemes in the real queue or a virtual queue. If an AQM scheme is implemented in a real queue, we show that, to ensure that the queuing delays are negligible compared to RTTs, one is forced to choose the parameters of a AQM scheme in a manner which yields a rate-based deterministic model. On the other hand, if the AQM scheme is implemented in a virtual queue, small-queue operation is achieved independent of the choice of the parameters, thus showing a robustness property of virtual queue-based schemes.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005717},
 doi = {http://doi.acm.org/10.1145/1005686.1005717},
 acmid = {1005717},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AQM parameters, congestion control, virtual queue},
} 

@article{Deb:2004:RVQ:1012888.1005717,
 author = {Deb, Supratim and Srikant, R.},
 title = {Rate-based versus queue-based models of congestion control},
 abstract = {Mathematical models of congestion control capture the congestion indication mechanism at the router in two different ways: rate-based models, where the queue-length at the router does not explicitly appear in the model, and queue-based models, where the queue length at the router is explicitly a part of the model. Even though most congestion indication mechanisms use the queue length to compute the packet marking or dropping probability to indicate congestion, we argue that, depending upon the choice of the parameters of the AQM scheme, one would obtain a rate-based model or a rate-and-queue-based model as the deterministic limit of a stochastic system with a large number of users. We also consider the impact of implementing AQM schemes in the real queue or a virtual queue. If an AQM scheme is implemented in a real queue, we show that, to ensure that the queuing delays are negligible compared to RTTs, one is forced to choose the parameters of a AQM scheme in a manner which yields a rate-based deterministic model. On the other hand, if the AQM scheme is implemented in a virtual queue, small-queue operation is achieved independent of the choice of the parameters, thus showing a robustness property of virtual queue-based schemes.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {246--257},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005717},
 doi = {http://doi.acm.org/10.1145/1012888.1005717},
 acmid = {1005717},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AQM parameters, congestion control, virtual queue},
} 

@article{Chandrayana:2004:UCC:1012888.1005718,
 author = {Chandrayana, Kartikeya and Kalyanaraman, Shivkumar},
 title = {Uncooperative congestion control},
 abstract = {Traditionally uncooperative rate control schemes have implied open loop protocols such as UDP, CBR, etc. In this paper we show that closed loop uncooperative rate control schemes also exist and that the current AQM proposals cannot efficiently control their mis-behavior. Moreover, these proposals require that AQM be installed at all routers in the Internet which is not only expensive but requires significant network upgrade.In this paper we show that management of uncooperative flows need not be coupled with AQM design but can be viewed as edge based policing question. In this paper we propose an analytical model for managing uncooperative flows in the Internet by re-mapping their utility function to a target range of utility functions. This mapping can be achieved by transparently manipulating congestion penalties conveyed to the uncooperative users.The most interesting aspect of this research is that this task can be performed at the edge of the network with little state information about uncooperative flows. The proposed solution is independent of the buffer management algorithm deployed on the network. As such it works with Drop-Tail queues as well as any AQM scheme. We have analyzed the framework and evaluated it on various single and multi-bottleneck topologies with both Drop-Tail and RED. Our results show that the framework is robust and works well even in presence of background traffic and reverse path congestion.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {258--269},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005718},
 doi = {http://doi.acm.org/10.1145/1012888.1005718},
 acmid = {1005718},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion control, malicious behavior, optimization, re-marking, selfish flows, uncooperative, utility functions},
} 

@inproceedings{Chandrayana:2004:UCC:1005686.1005718,
 author = {Chandrayana, Kartikeya and Kalyanaraman, Shivkumar},
 title = {Uncooperative congestion control},
 abstract = {Traditionally uncooperative rate control schemes have implied open loop protocols such as UDP, CBR, etc. In this paper we show that closed loop uncooperative rate control schemes also exist and that the current AQM proposals cannot efficiently control their mis-behavior. Moreover, these proposals require that AQM be installed at all routers in the Internet which is not only expensive but requires significant network upgrade.In this paper we show that management of uncooperative flows need not be coupled with AQM design but can be viewed as edge based policing question. In this paper we propose an analytical model for managing uncooperative flows in the Internet by re-mapping their utility function to a target range of utility functions. This mapping can be achieved by transparently manipulating congestion penalties conveyed to the uncooperative users.The most interesting aspect of this research is that this task can be performed at the edge of the network with little state information about uncooperative flows. The proposed solution is independent of the buffer management algorithm deployed on the network. As such it works with Drop-Tail queues as well as any AQM scheme. We have analyzed the framework and evaluated it on various single and multi-bottleneck topologies with both Drop-Tail and RED. Our results show that the framework is robust and works well even in presence of background traffic and reverse path congestion.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {258--269},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005718},
 doi = {http://doi.acm.org/10.1145/1005686.1005718},
 acmid = {1005718},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion control, malicious behavior, optimization, re-marking, selfish flows, uncooperative, utility functions},
} 

@article{Applegate:2004:CNF:1012888.1005719,
 author = {Applegate, David and Breslau, Lee and Cohen, Edith},
 title = {Coping with network failures: routing strategies for optimal demand oblivious restoration},
 abstract = {Link and node failures in IP networks pose a challenge for network control algorithms. Routing restoration, which computes new routes that avoid failed links, involves fundamental tradeoffs between efficient use of network resources, complexity of the restoration strategy and disruption to network traffic. In order to achieve a balance between these goals, obtaining routings that provide good performance guarantees under failures is desirable.In this paper, building on previous work that provided performance guarantees under uncertain (and potentially unknown) traffic demands, we develop algorithms for computing optimal restoration paths and a methodology for evaluating the performance guarantees of routing under failures. We then study the performance of route restoration on a diverse collection of ISP networks. Our evaluation uses a competitive analysis type framework, where performance of routing with restoration paths under failures is compared to the best possible performance on the failed network. We conclude that with careful selection of restoration paths one can obtain restoration strategies that retain nearly optimal performance on the failed network while minimizing disruptions to traffic flows that did not traverse the failed parts of the network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {270--281},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005719},
 doi = {http://doi.acm.org/10.1145/1012888.1005719},
 acmid = {1005719},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {demand-oblivious routing, restoration, routing},
} 

@inproceedings{Applegate:2004:CNF:1005686.1005719,
 author = {Applegate, David and Breslau, Lee and Cohen, Edith},
 title = {Coping with network failures: routing strategies for optimal demand oblivious restoration},
 abstract = {Link and node failures in IP networks pose a challenge for network control algorithms. Routing restoration, which computes new routes that avoid failed links, involves fundamental tradeoffs between efficient use of network resources, complexity of the restoration strategy and disruption to network traffic. In order to achieve a balance between these goals, obtaining routings that provide good performance guarantees under failures is desirable.In this paper, building on previous work that provided performance guarantees under uncertain (and potentially unknown) traffic demands, we develop algorithms for computing optimal restoration paths and a methodology for evaluating the performance guarantees of routing under failures. We then study the performance of route restoration on a diverse collection of ISP networks. Our evaluation uses a competitive analysis type framework, where performance of routing with restoration paths under failures is compared to the best possible performance on the failed network. We conclude that with careful selection of restoration paths one can obtain restoration strategies that retain nearly optimal performance on the failed network while minimizing disruptions to traffic flows that did not traverse the failed parts of the network.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {270--281},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005719},
 doi = {http://doi.acm.org/10.1145/1005686.1005719},
 acmid = {1005719},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {demand-oblivious routing, restoration, routing},
} 

@article{Sevcik:2004:SAM:1012888.1005689,
 author = {Sevcik, Kenneth C.},
 title = {Some systems, applications and models I have known},
 abstract = {Being named recipient of the 2004 ACM Sigmetrics Achievement Award has done several things to me. It brought me surprise that I would be singled out from the many people who have made significant and sustained contributions to the field of performance evaluation. It also brought me deep appreciation for all the students and colleagues with whom I have worked and come to know as friends over the years. Finally, it has caused me to ponder and reminisce about many of the research projects and consulting studies in which I have participated.In this talk, I will describe various systems I have used and studied, various applications of interest, and various models that I, and others, have used to try to gain insights into the performance of systems. Some lessons of possible future relevance that emerge from this retrospective look at a wide variety of projects are the following: \&lt;ol\&gt;<li>Exact Answers Are Overrated -- While exact solutions of mathematical models are intellectually satisfying, they are often not needed in practice.</li> <li>Analytic Models Have a Role -- Analytic models can be used to obtain quick and inexpensive answers to performance questions in many situations where neither simulation nor experimentation are feasible.</li> <li>Assumptions Matter -- Subtle changes to the assumptions that underlie an analytic model can substantially alter the conclusions reached based on the model.</li>\&lt;/olAfter considering all the methods of analysis, simulation and experimentation, my recommendation for the very best means to attain substantially improved computer system performance is: Wait thirty years!},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {282--282},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1012888.1005689},
 doi = {http://doi.acm.org/10.1145/1012888.1005689},
 acmid = {1005689},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sevcik:2004:SAM:1005686.1005689,
 author = {Sevcik, Kenneth C.},
 title = {Some systems, applications and models I have known},
 abstract = {Being named recipient of the 2004 ACM Sigmetrics Achievement Award has done several things to me. It brought me surprise that I would be singled out from the many people who have made significant and sustained contributions to the field of performance evaluation. It also brought me deep appreciation for all the students and colleagues with whom I have worked and come to know as friends over the years. Finally, it has caused me to ponder and reminisce about many of the research projects and consulting studies in which I have participated.In this talk, I will describe various systems I have used and studied, various applications of interest, and various models that I, and others, have used to try to gain insights into the performance of systems. Some lessons of possible future relevance that emerge from this retrospective look at a wide variety of projects are the following: \&lt;ol\&gt;<li>Exact Answers Are Overrated -- While exact solutions of mathematical models are intellectually satisfying, they are often not needed in practice.</li> <li>Analytic Models Have a Role -- Analytic models can be used to obtain quick and inexpensive answers to performance questions in many situations where neither simulation nor experimentation are feasible.</li> <li>Assumptions Matter -- Subtle changes to the assumptions that underlie an analytic model can substantially alter the conclusions reached based on the model.</li>\&lt;/olAfter considering all the methods of analysis, simulation and experimentation, my recommendation for the very best means to attain substantially improved computer system performance is: Wait thirty years!},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {282--282},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1005686.1005689},
 doi = {http://doi.acm.org/10.1145/1005686.1005689},
 acmid = {1005689},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tinnakornsrisuphap:2004:CQF:1012888.1005721,
 author = {Tinnakornsrisuphap, Peerapol and La, Richard J.},
 title = {Characterization of queue fluctuations in probabilistic AQM mechanisms},
 abstract = {We develop a framework for studying the interaction of a probabilistic active queue management (AQM) algorithm with a generic end-user congestion-control mechanism. We show that as the number of flows in the network increases, the queue dynamics can be accurately approximated by a simple deterministic process. In addition, we investigate the sources of queue fluctuations in this setup. We characterize two distinct sources of queue fluctuations; one is the deterministic oscillations which can be captured through the aforementioned deterministic process. The other source is the random fluctuations introduced by the probabilistic nature of the marking schemes. We discuss the relationship between these two types of fluctuations and provide insights into how to control them. Concrete examples in this framework are given for several popular algorithms such as Random Early Detection, Random Early Marking and Transmission Control Protocol.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005721},
 doi = {http://doi.acm.org/10.1145/1012888.1005721},
 acmid = {1005721},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active queue management, central limit theorem, queue fluctuations},
} 

@inproceedings{Tinnakornsrisuphap:2004:CQF:1005686.1005721,
 author = {Tinnakornsrisuphap, Peerapol and La, Richard J.},
 title = {Characterization of queue fluctuations in probabilistic AQM mechanisms},
 abstract = {We develop a framework for studying the interaction of a probabilistic active queue management (AQM) algorithm with a generic end-user congestion-control mechanism. We show that as the number of flows in the network increases, the queue dynamics can be accurately approximated by a simple deterministic process. In addition, we investigate the sources of queue fluctuations in this setup. We characterize two distinct sources of queue fluctuations; one is the deterministic oscillations which can be captured through the aforementioned deterministic process. The other source is the random fluctuations introduced by the probabilistic nature of the marking schemes. We discuss the relationship between these two types of fluctuations and provide insights into how to control them. Concrete examples in this framework are given for several popular algorithms such as Random Early Detection, Random Early Marking and Transmission Control Protocol.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005721},
 doi = {http://doi.acm.org/10.1145/1005686.1005721},
 acmid = {1005721},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active queue management, central limit theorem, queue fluctuations},
} 

@article{Vanichpun:2004:OCU:1012888.1005722,
 author = {Vanichpun, Sarut and Makowski, Armand M.},
 title = {The output of a cache under the independent reference model: where did the locality of reference go?},
 abstract = {We consider a cache operating under a demand-driven replacement policy when document requests are modeled according to the Independent Reference Model (IRM). We characterize the popularity pmf of the stream of misses from the cache, the so-called output of the cache, for a large class of demand-driven cache replacement policies. We measure strength of locality of reference in a stream of requests through the skewness of its popularity distribution. Using the notion of majorization to capture this degree of skewness, we show that for the policy A<inf>0</inf></i> and the random policy, the output always has less locality of reference than the input. However, we show by counterexamples that this is not always the case under the LRU and CLIMB policies when the input is selected according to a Zipf-like pmf. In that case, conjectures are offered (and supported by simulations) as to when LRU or CLIMB caching indeed reduces locality of reference.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {295--306},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005722},
 doi = {http://doi.acm.org/10.1145/1012888.1005722},
 acmid = {1005722},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {locality of reference, majorization, output of a cache, popularity},
} 

@inproceedings{Vanichpun:2004:OCU:1005686.1005722,
 author = {Vanichpun, Sarut and Makowski, Armand M.},
 title = {The output of a cache under the independent reference model: where did the locality of reference go?},
 abstract = {We consider a cache operating under a demand-driven replacement policy when document requests are modeled according to the Independent Reference Model (IRM). We characterize the popularity pmf of the stream of misses from the cache, the so-called output of the cache, for a large class of demand-driven cache replacement policies. We measure strength of locality of reference in a stream of requests through the skewness of its popularity distribution. Using the notion of majorization to capture this degree of skewness, we show that for the policy A<inf>0</inf></i> and the random policy, the output always has less locality of reference than the input. However, we show by counterexamples that this is not always the case under the LRU and CLIMB policies when the input is selected according to a Zipf-like pmf. In that case, conjectures are offered (and supported by simulations) as to when LRU or CLIMB caching indeed reduces locality of reference.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {295--306},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005722},
 doi = {http://doi.acm.org/10.1145/1005686.1005722},
 acmid = {1005722},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {locality of reference, majorization, output of a cache, popularity},
} 

@article{Teixeira:2004:DHR:1012888.1005723,
 author = {Teixeira, Renata and Shaikh, Aman and Griffin, Tim and Rexford, Jennifer},
 title = {Dynamics of hot-potato routing in IP networks},
 abstract = {Despite the architectural separation between intradomain and interdomain routing in the Internet, intradomain protocols do influence the path-selection process in the Border Gateway Protocol (BGP). When choosing between multiple equally-good BGP routes, a router selects the one with the closest</i> egress point, based on the intradomain path cost. Under such hot-potato</i> routing, an intradomain event can trigger BGP routing changes. To characterize the influence of hot-potato routing, we conduct controlled experiments with a commercial router. Then, we propose a technique for associating BGP routing changes with events visible in the intradomain protocol, and apply our algorithm to AT\&T's backbone network. We show that (i) hot-potato routing can be a significant source of BGP updates, (ii) BGP updates can lag 60</i> seconds or more behind the intradomain event, (iii) the number of BGP path changes triggered by hot-potato routing has a nearly uniform distribution across destination prefixes, and (iv) the fraction of BGP messages triggered by intradomain changes varies significantly across time and router locations. We show that hot-potato routing changes lead to longer delays in forwarding-plane convergence, shifts in the flow of traffic to neighboring domains, extra externally-visible BGP update messages, and inaccuracies in Internet performance measurements.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {307--319},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1012888.1005723},
 doi = {http://doi.acm.org/10.1145/1012888.1005723},
 acmid = {1005723},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, OSPF, convergence, hot-potato routing},
} 

@inproceedings{Teixeira:2004:DHR:1005686.1005723,
 author = {Teixeira, Renata and Shaikh, Aman and Griffin, Tim and Rexford, Jennifer},
 title = {Dynamics of hot-potato routing in IP networks},
 abstract = {Despite the architectural separation between intradomain and interdomain routing in the Internet, intradomain protocols do influence the path-selection process in the Border Gateway Protocol (BGP). When choosing between multiple equally-good BGP routes, a router selects the one with the closest</i> egress point, based on the intradomain path cost. Under such hot-potato</i> routing, an intradomain event can trigger BGP routing changes. To characterize the influence of hot-potato routing, we conduct controlled experiments with a commercial router. Then, we propose a technique for associating BGP routing changes with events visible in the intradomain protocol, and apply our algorithm to AT\&T's backbone network. We show that (i) hot-potato routing can be a significant source of BGP updates, (ii) BGP updates can lag 60</i> seconds or more behind the intradomain event, (iii) the number of BGP path changes triggered by hot-potato routing has a nearly uniform distribution across destination prefixes, and (iv) the fraction of BGP messages triggered by intradomain changes varies significantly across time and router locations. We show that hot-potato routing changes lead to longer delays in forwarding-plane convergence, shifts in the flow of traffic to neighboring domains, extra externally-visible BGP update messages, and inaccuracies in Internet performance measurements.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {307--319},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1005686.1005723},
 doi = {http://doi.acm.org/10.1145/1005686.1005723},
 acmid = {1005723},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, OSPF, convergence, hot-potato routing},
} 

@inproceedings{Agarwal:2004:IBD:1005686.1005724,
 author = {Agarwal, Sharad and Chuah, Chen-Nee and Bhattacharyya, Supratik and Diot, Christophe},
 title = {The impact of BGP dynamics on intra-domain traffic},
 abstract = {Recent work in network traffic matrix estimation has focused on generating router-to-router or PoP-to-PoP (Point-of-Presence) traffic matrices within an ISP backbone from network link load data. However, these estimation techniques have not considered the impact of inter-domain routing changes in BGP (Border Gateway Protocol). BGP routing changes have the potential to introduce significant errors in estimated traffic matrices by causing traffic shifts between egress routers or PoPs within a single backbone network. We present a methodology to correlate BGP routing table changes with packet traces in order to analyze how BGP dynamics affect traffic fan-out within a large "tier-1" network. Despite an average of 133 BGP routing updates per minute, we find that BGP routing changes do not cause more than 0.03\% of ingress traffic to shift between egress PoPs. This limited impact is mostly due to the relative stability of network prefixes that receive the majority of traffic -- 0.05\% of BGP routing table changes affect intra-domain routes for prefixes that carry 80\% of the traffic. Thus our work validates an important assumption underlying existing techniques for traffic matrix estimation in large IP networks.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {319--330},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005724},
 doi = {http://doi.acm.org/10.1145/1005686.1005724},
 acmid = {1005724},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, traffic analysis, traffic engineering, traffic matrix},
} 

@article{Agarwal:2004:IBD:1012888.1005724,
 author = {Agarwal, Sharad and Chuah, Chen-Nee and Bhattacharyya, Supratik and Diot, Christophe},
 title = {The impact of BGP dynamics on intra-domain traffic},
 abstract = {Recent work in network traffic matrix estimation has focused on generating router-to-router or PoP-to-PoP (Point-of-Presence) traffic matrices within an ISP backbone from network link load data. However, these estimation techniques have not considered the impact of inter-domain routing changes in BGP (Border Gateway Protocol). BGP routing changes have the potential to introduce significant errors in estimated traffic matrices by causing traffic shifts between egress routers or PoPs within a single backbone network. We present a methodology to correlate BGP routing table changes with packet traces in order to analyze how BGP dynamics affect traffic fan-out within a large "tier-1" network. Despite an average of 133 BGP routing updates per minute, we find that BGP routing changes do not cause more than 0.03\% of ingress traffic to shift between egress PoPs. This limited impact is mostly due to the relative stability of network prefixes that receive the majority of traffic -- 0.05\% of BGP routing table changes affect intra-domain routes for prefixes that carry 80\% of the traffic. Thus our work validates an important assumption underlying existing techniques for traffic matrix estimation in large IP networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {319--330},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005724},
 doi = {http://doi.acm.org/10.1145/1012888.1005724},
 acmid = {1005724},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, traffic analysis, traffic engineering, traffic matrix},
} 

@article{Feamster:2004:MBR:1012888.1005726,
 author = {Feamster, Nick and Winick, Jared and Rexford, Jennifer},
 title = {A model of BGP routing for network engineering},
 abstract = {The performance of IP networks depends on a wide variety of dynamic conditions. Traffic shifts, equipment failures, planned maintenance, and topology changes in other parts of the Internet can all degrade performance. To maintain good performance, network operators must continually reconfigure the routing protocols. Operators configure BGP to control how traffic flows to neighboring Autonomous Systems (ASes), as well as how traffic traverses their networks. However, because BGP route selection is distributed, indirectly controlled by configurable policies, and influenced by complex interactions with intradomain routing protocols, operators cannot predict how a particular BGP configuration would behave in practice. To avoid inadvertently degrading network performance, operators need to evaluate the effects of configuration changes before deploying them on a live network</i>. We propose an algorithm that computes the outcome of the BGP route selection process for each router in a single</i> AS, given only a static snapshot of the network state, without simulating the complex details of BGP message passing. We describe a BGP emulator based on this algorithm; the emulator exploits the unique characteristics of routing data to reduce computational overhead. Using data from a large ISP, we show that the emulator correctly computes BGP routing decisions and has a running time that is acceptable for many tasks, such as traffic engineering and capacity planning.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {331--342},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005726},
 doi = {http://doi.acm.org/10.1145/1012888.1005726},
 acmid = {1005726},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, modeling, routing, traffic engineering},
} 

@inproceedings{Feamster:2004:MBR:1005686.1005726,
 author = {Feamster, Nick and Winick, Jared and Rexford, Jennifer},
 title = {A model of BGP routing for network engineering},
 abstract = {The performance of IP networks depends on a wide variety of dynamic conditions. Traffic shifts, equipment failures, planned maintenance, and topology changes in other parts of the Internet can all degrade performance. To maintain good performance, network operators must continually reconfigure the routing protocols. Operators configure BGP to control how traffic flows to neighboring Autonomous Systems (ASes), as well as how traffic traverses their networks. However, because BGP route selection is distributed, indirectly controlled by configurable policies, and influenced by complex interactions with intradomain routing protocols, operators cannot predict how a particular BGP configuration would behave in practice. To avoid inadvertently degrading network performance, operators need to evaluate the effects of configuration changes before deploying them on a live network</i>. We propose an algorithm that computes the outcome of the BGP route selection process for each router in a single</i> AS, given only a static snapshot of the network state, without simulating the complex details of BGP message passing. We describe a BGP emulator based on this algorithm; the emulator exploits the unique characteristics of routing data to reduce computational overhead. Using data from a large ISP, we show that the emulator correctly computes BGP routing decisions and has a running time that is acceptable for many tasks, such as traffic engineering and capacity planning.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {331--342},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005726},
 doi = {http://doi.acm.org/10.1145/1005686.1005726},
 acmid = {1005726},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, modeling, routing, traffic engineering},
} 

@inproceedings{Baccelli:2004:MAS:1005686.1005727,
 author = {Baccelli, Fran\c{c}ois and Chaintreau, Augustin and De Vleeschauwer, Danny and McDonald, David},
 title = {A mean-field analysis of short lived interacting TCP flows},
 abstract = {In this paper, we consider a set of HTTP flows using TCP over a common drop-tail link to download files. After each download, a flow waits for a random think time before requesting the download of another file, whose size is also random. When a flow is active its throughput is increasing with time according to the additive increase rule, but if it suffers losses created when the total transmission rate of the flows exceeds the link rate, its transmission rate is decreased. The throughput obtained by a flow, and the consecutive time to download one file are then given as the consequence of the interaction of all the flows through their total transmission rate and the link's behavior.We study the mean-field model obtained by letting the number of flows go to infinity. This mean-field limit may have two stable regimes : one without congestion in the link, in which the density of transmission rate can be explicitly described, the other one with periodic congestion epochs, where the inter-congestion time can be characterized as the solution of a fixed point equation, that we compute numerically, leading to a density of transmission rate given by as the solution of a Fredholm equation. It is shown that for certain values of the parameters (more precisely when the link capacity per user is not significantly larger than the load per user), each of these two stable regimes can be reached depending on the initial condition. This phenomenon can be seen as an analogue of turbulence in fluid dynamics: for some initial conditions, the transfers progress in a fluid and interaction-less way; for others, the connections interact and slow down because of the resulting fluctuations, which in turn perpetuates interaction forever, in spite of the fact that the load per user is less than the capacity per user. We prove that this phenomenon is present in the Tahoe case and both the numerical method that we develop and simulations suggest that it is present in the Reno case too. It translates into a bi-stability phenomenon for the finite population model within this range of parameters.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {343--354},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005727},
 doi = {http://doi.acm.org/10.1145/1005686.1005727},
 acmid = {1005727},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {HTTP connections, mean-field model},
} 

@article{Baccelli:2004:MAS:1012888.1005727,
 author = {Baccelli, Fran\c{c}ois and Chaintreau, Augustin and De Vleeschauwer, Danny and McDonald, David},
 title = {A mean-field analysis of short lived interacting TCP flows},
 abstract = {In this paper, we consider a set of HTTP flows using TCP over a common drop-tail link to download files. After each download, a flow waits for a random think time before requesting the download of another file, whose size is also random. When a flow is active its throughput is increasing with time according to the additive increase rule, but if it suffers losses created when the total transmission rate of the flows exceeds the link rate, its transmission rate is decreased. The throughput obtained by a flow, and the consecutive time to download one file are then given as the consequence of the interaction of all the flows through their total transmission rate and the link's behavior.We study the mean-field model obtained by letting the number of flows go to infinity. This mean-field limit may have two stable regimes : one without congestion in the link, in which the density of transmission rate can be explicitly described, the other one with periodic congestion epochs, where the inter-congestion time can be characterized as the solution of a fixed point equation, that we compute numerically, leading to a density of transmission rate given by as the solution of a Fredholm equation. It is shown that for certain values of the parameters (more precisely when the link capacity per user is not significantly larger than the load per user), each of these two stable regimes can be reached depending on the initial condition. This phenomenon can be seen as an analogue of turbulence in fluid dynamics: for some initial conditions, the transfers progress in a fluid and interaction-less way; for others, the connections interact and slow down because of the resulting fluctuations, which in turn perpetuates interaction forever, in spite of the fact that the load per user is less than the capacity per user. We prove that this phenomenon is present in the Tahoe case and both the numerical method that we develop and simulations suggest that it is present in the Reno case too. It translates into a bi-stability phenomenon for the finite population model within this range of parameters.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {343--354},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005727},
 doi = {http://doi.acm.org/10.1145/1012888.1005727},
 acmid = {1005727},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {HTTP connections, mean-field model},
} 

@article{Hohn:2004:BRP:1012888.1005728,
 author = {Hohn, N. and Veitch, D. and Papagiannaki, K. and Diot, C.},
 title = {Bridging router performance and queuing theory},
 abstract = {This paper provides an authoritative knowledge of through-router packet delays and therefore a better understanding of data network performance. Thanks to a unique experimental setup, we capture all</i> packets crossing a router for 13 hours and present detailed statistics of their delays. These measurements allow us to build the following physical model for router performance: each packet experiences a minimum router processing time before entering a fluid output queue. Although simple, this model reproduces the router behaviour with excellent accuracy and avoids two common pitfalls. First we show that in-router packet processing time accounts for a significant portion of the overall packet delay and should not be neglected. Second we point out that one should fully understand both link and physical layer characteristics to use the appropriate bandwidth value.Focusing directly on router performance, we provide insights into system busy periods and show precisely how queues build up inside a router. We explain why current practices for inferring delays based on average utilization have fundamental problems, and propose an alternative solution to directly report router delay information based on busy period statistics.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {355--366},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1012888.1005728},
 doi = {http://doi.acm.org/10.1145/1012888.1005728},
 acmid = {1005728},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet delay analysis, router model},
} 

@inproceedings{Hohn:2004:BRP:1005686.1005728,
 author = {Hohn, N. and Veitch, D. and Papagiannaki, K. and Diot, C.},
 title = {Bridging router performance and queuing theory},
 abstract = {This paper provides an authoritative knowledge of through-router packet delays and therefore a better understanding of data network performance. Thanks to a unique experimental setup, we capture all</i> packets crossing a router for 13 hours and present detailed statistics of their delays. These measurements allow us to build the following physical model for router performance: each packet experiences a minimum router processing time before entering a fluid output queue. Although simple, this model reproduces the router behaviour with excellent accuracy and avoids two common pitfalls. First we show that in-router packet processing time accounts for a significant portion of the overall packet delay and should not be neglected. Second we point out that one should fully understand both link and physical layer characteristics to use the appropriate bandwidth value.Focusing directly on router performance, we provide insights into system busy periods and show precisely how queues build up inside a router. We explain why current practices for inferring delays based on average utilization have fundamental problems, and propose an alternative solution to directly report router delay information based on busy period statistics.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {355--366},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1005686.1005728},
 doi = {http://doi.acm.org/10.1145/1005686.1005728},
 acmid = {1005728},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet delay analysis, router model},
} 

@inproceedings{Bonald:2004:ILB:1005686.1005729,
 author = {Bonald, T. and Jonckheere, M. and Prouti\'{e}re, A.},
 title = {Insensitive load balancing},
 abstract = {A large variety of communication systems, including telephone and data networks, can be represented by so-called Whittle networks. The stationary distribution of these networks is insensitive, depending on the service requirements at each node through their mean only. These models are of considerable practical interest as derived engineering rules are robust to the evolution of traffic characteristics. In this paper we relax the usual assumption of static routing and address the issue of dynamic load balancing. Specifically, we identify the class of load balancing policies which preserve insensitivity and characterize optimal strategies in some specific cases. Analytical results are illustrated numerically on a number of toy network examples.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {367--377},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1005686.1005729},
 doi = {http://doi.acm.org/10.1145/1005686.1005729},
 acmid = {1005729},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {insensitivity, load balancing, whittle networks},
} 

@article{Bonald:2004:ILB:1012888.1005729,
 author = {Bonald, T. and Jonckheere, M. and Prouti\'{e}re, A.},
 title = {Insensitive load balancing},
 abstract = {A large variety of communication systems, including telephone and data networks, can be represented by so-called Whittle networks. The stationary distribution of these networks is insensitive, depending on the service requirements at each node through their mean only. These models are of considerable practical interest as derived engineering rules are robust to the evolution of traffic characteristics. In this paper we relax the usual assumption of static routing and address the issue of dynamic load balancing. Specifically, we identify the class of load balancing policies which preserve insensitivity and characterize optimal strategies in some specific cases. Analytical results are illustrated numerically on a number of toy network examples.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {367--377},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1012888.1005729},
 doi = {http://doi.acm.org/10.1145/1012888.1005729},
 acmid = {1005729},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {insensitivity, load balancing, whittle networks},
} 

@article{Bonald:2004:WDP:1012888.1005730,
 author = {Bonald, T. and Borst, S. and Hegde, N. and Prouti\'{e}re, A.},
 title = {Wireless data performance in multi-cell scenarios},
 abstract = {The performance of wireless data systems has been extensively studied in the context of a single base station. In the present paper we investigate the flow-level performance in networks with multiple base stations. We specifically examine the complex, dynamic interaction of the number of active flows in the various cells introduced by the strong impact of interference between neighboring base stations. For the downlink data transmissions that we consider, lower service rates caused by increased interference from neighboring base stations result in longer delays and thus a higher number of active flows. This in turn results in a longer duration of interference on surrounding base stations, causing a strong correlation between the activity states of the base stations. Such a system can be modelled as a network of multi-class processor-sharing queues, where the service rates for the various classes at each queue vary over time as governed by the activity state of the other queues. The complex interaction between the various queues renders an exact analysis intractable in general. A simplified network with only one class per queue reduces to a coupled-processors model, for which there are few results, even in the case of two queues. We thus derive bounds and approximations for key performance metrics like the number of active flows, transfer delays, and flow throughputs in the various cells. Importantly, these bounds and approximations are insensitive, yielding simple expressions, that render the detailed statistical characteristics of the system largely irrelevant.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {378--380},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/1012888.1005730},
 doi = {http://doi.acm.org/10.1145/1012888.1005730},
 acmid = {1005730},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {elastic traffic, fluid regime, insensitivity, multi-class processor-sharing, quasi-stationary regime, stability, time-varying service, wireless data networks},
} 

@inproceedings{Bonald:2004:WDP:1005686.1005730,
 author = {Bonald, T. and Borst, S. and Hegde, N. and Prouti\'{e}re, A.},
 title = {Wireless data performance in multi-cell scenarios},
 abstract = {The performance of wireless data systems has been extensively studied in the context of a single base station. In the present paper we investigate the flow-level performance in networks with multiple base stations. We specifically examine the complex, dynamic interaction of the number of active flows in the various cells introduced by the strong impact of interference between neighboring base stations. For the downlink data transmissions that we consider, lower service rates caused by increased interference from neighboring base stations result in longer delays and thus a higher number of active flows. This in turn results in a longer duration of interference on surrounding base stations, causing a strong correlation between the activity states of the base stations. Such a system can be modelled as a network of multi-class processor-sharing queues, where the service rates for the various classes at each queue vary over time as governed by the activity state of the other queues. The complex interaction between the various queues renders an exact analysis intractable in general. A simplified network with only one class per queue reduces to a coupled-processors model, for which there are few results, even in the case of two queues. We thus derive bounds and approximations for key performance metrics like the number of active flows, transfer delays, and flow throughputs in the various cells. Importantly, these bounds and approximations are insensitive, yielding simple expressions, that render the detailed statistical characteristics of the system largely irrelevant.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {378--380},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/1005686.1005730},
 doi = {http://doi.acm.org/10.1145/1005686.1005730},
 acmid = {1005730},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {elastic traffic, fluid regime, insensitivity, multi-class processor-sharing, quasi-stationary regime, stability, time-varying service, wireless data networks},
} 

@inproceedings{Kapoor:2004:CSA:1005686.1005732,
 author = {Kapoor, Rohit and Chen, Ling-Jyh and Nandan, Alok and Gerla, Mario and Sanadidi, M. Y.},
 title = {CapProbe: a simple and accurate capacity estimation technique for wired and wireless environments},
 abstract = {The problem of estimating the capacity of an Internet path is one of fundamental importance. Due to the multitude of potential applications, a large number of solutions have been proposed and evaluated. The proposed solutions so far have been successful in partially addressing the problem, but have suffered from being slow, obtrusive or inaccurate. In this work, we evaluate CapProbe, a low-cost and accurate end-to-end capacity estimation scheme that relies on packet dispersion techniques as well as end-to-end delays. The key observation that enabled the development of CapProbe is that both compression and expansion of packet pair dispersion are the result of queuing due to cross-traffic. By filtering out queuing effects from packet pair samples, CapProbe is able to estimate capacity accurately in most environments, with minimal processing and probing traffic overhead. In fact, the storage and processing requirements of CapProbe are orders of magnitude smaller than most of the previously proposed schemes. We tested CapProbe through simulation, Internet, Internet2 and wireless experiments. We found that CapProbe error percentage in capacity estimation was within 10\% in almost all cases, and within 5\% in most cases.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {390--391},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005732},
 doi = {http://doi.acm.org/10.1145/1005686.1005732},
 acmid = {1005732},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity estimation, delay, dispersion, packet pair},
} 

@article{Kapoor:2004:CSA:1012888.1005732,
 author = {Kapoor, Rohit and Chen, Ling-Jyh and Nandan, Alok and Gerla, Mario and Sanadidi, M. Y.},
 title = {CapProbe: a simple and accurate capacity estimation technique for wired and wireless environments},
 abstract = {The problem of estimating the capacity of an Internet path is one of fundamental importance. Due to the multitude of potential applications, a large number of solutions have been proposed and evaluated. The proposed solutions so far have been successful in partially addressing the problem, but have suffered from being slow, obtrusive or inaccurate. In this work, we evaluate CapProbe, a low-cost and accurate end-to-end capacity estimation scheme that relies on packet dispersion techniques as well as end-to-end delays. The key observation that enabled the development of CapProbe is that both compression and expansion of packet pair dispersion are the result of queuing due to cross-traffic. By filtering out queuing effects from packet pair samples, CapProbe is able to estimate capacity accurately in most environments, with minimal processing and probing traffic overhead. In fact, the storage and processing requirements of CapProbe are orders of magnitude smaller than most of the previously proposed schemes. We tested CapProbe through simulation, Internet, Internet2 and wireless experiments. We found that CapProbe error percentage in capacity estimation was within 10\% in almost all cases, and within 5\% in most cases.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {390--391},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005732},
 doi = {http://doi.acm.org/10.1145/1012888.1005732},
 acmid = {1005732},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity estimation, delay, dispersion, packet pair},
} 

@article{Sommers:2004:HFT:1012888.1005733,
 author = {Sommers, Joel and Kim, Hyungsuk and Barford, Paul},
 title = {Harpoon: a flow-level traffic generator for router and network tests},
 abstract = {We describe Harpoon, a new application-independent tool for generating representative packet traffic at the IP flow level</i>. Harpoon is a configurable tool for creating TCP and UDP packet flows that have the same byte, packet, temporal, and spatial characteristics as measured at routers in live environments. We validate Harpoon using traces collected from a live router and then demonstrate its capabilities in a series of router performance benchmark tests.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {392--392},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1012888.1005733},
 doi = {http://doi.acm.org/10.1145/1012888.1005733},
 acmid = {1005733},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network flows, traffic generation},
} 

@inproceedings{Sommers:2004:HFT:1005686.1005733,
 author = {Sommers, Joel and Kim, Hyungsuk and Barford, Paul},
 title = {Harpoon: a flow-level traffic generator for router and network tests},
 abstract = {We describe Harpoon, a new application-independent tool for generating representative packet traffic at the IP flow level</i>. Harpoon is a configurable tool for creating TCP and UDP packet flows that have the same byte, packet, temporal, and spatial characteristics as measured at routers in live environments. We validate Harpoon using traces collected from a live router and then demonstrate its capabilities in a series of router performance benchmark tests.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {392--392},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1005686.1005733},
 doi = {http://doi.acm.org/10.1145/1005686.1005733},
 acmid = {1005733},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network flows, traffic generation},
} 

@article{Ribeiro:2004:SAB:1012888.1005734,
 author = {Ribeiro, Vinay J. and Riedi, Rudolf H. and Baraniuk, Richard G.},
 title = {Spatio-temporal available bandwidth estimation with STAB},
 abstract = {We study the problem of locating in space and over time a network path's tight</i> link, that is the link with the least available bandwidth on the path. Tight link localization benefits network-aware applications, provides insight into the causes of network congestion and ways to circumvent it, and aids network operations. We present STAB</i>, a light-weight probing tool to locate tight links. STAB combines the probing concepts of self-induced congestion, tailgating, and packet chirps in a novel fashion. We demonstrate its capabilities through experiments on the Internet and verify our results using router MRTG data.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {394--395},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005734},
 doi = {http://doi.acm.org/10.1145/1012888.1005734},
 acmid = {1005734},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {available bandwidth, bandwidth, bottleneck, chirps, estimation, probing, tailgating, tight link},
} 

@inproceedings{Ribeiro:2004:SAB:1005686.1005734,
 author = {Ribeiro, Vinay J. and Riedi, Rudolf H. and Baraniuk, Richard G.},
 title = {Spatio-temporal available bandwidth estimation with STAB},
 abstract = {We study the problem of locating in space and over time a network path's tight</i> link, that is the link with the least available bandwidth on the path. Tight link localization benefits network-aware applications, provides insight into the causes of network congestion and ways to circumvent it, and aids network operations. We present STAB</i>, a light-weight probing tool to locate tight links. STAB combines the probing concepts of self-induced congestion, tailgating, and packet chirps in a novel fashion. We demonstrate its capabilities through experiments on the Internet and verify our results using router MRTG data.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {394--395},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005734},
 doi = {http://doi.acm.org/10.1145/1005686.1005734},
 acmid = {1005734},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {available bandwidth, bandwidth, bottleneck, chirps, estimation, probing, tailgating, tight link},
} 

@inproceedings{Rajendran:2004:OQS:1005686.1005735,
 author = {Rajendran, Raj Kumar and Rubenstein, Dan},
 title = {Optimizing the quality of scalable video streams on P2P networks},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {396--397},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005735},
 doi = {http://doi.acm.org/10.1145/1005686.1005735},
 acmid = {1005735},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {P2P, quality, scheduling, streaming, video},
} 

@article{Rajendran:2004:OQS:1012888.1005735,
 author = {Rajendran, Raj Kumar and Rubenstein, Dan},
 title = {Optimizing the quality of scalable video streams on P2P networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {396--397},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005735},
 doi = {http://doi.acm.org/10.1145/1012888.1005735},
 acmid = {1005735},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {P2P, quality, scheduling, streaming, video},
} 

@article{Wang:2004:PAT:1012888.1005736,
 author = {Wang, Helen J. and Platt, John and Chen, Yu and Zhang, Ruyun and Wang, Yi-Min},
 title = {PeerPressure for automatic troubleshooting},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {398--399},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005736},
 doi = {http://doi.acm.org/10.1145/1012888.1005736},
 acmid = {1005736},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Bayesian estimates, PeerPressure, automatic troubleshooting, golden state, statistics, system management},
} 

@inproceedings{Wang:2004:PAT:1005686.1005736,
 author = {Wang, Helen J. and Platt, John and Chen, Yu and Zhang, Ruyun and Wang, Yi-Min},
 title = {PeerPressure for automatic troubleshooting},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {398--399},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005736},
 doi = {http://doi.acm.org/10.1145/1005686.1005736},
 acmid = {1005736},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Bayesian estimates, PeerPressure, automatic troubleshooting, golden state, statistics, system management},
} 

@article{Hahner:2004:QAP:1012888.1005737,
 author = {H\"{a}hner, J\"{o}rg and Dudkowski, Dominique and Marr\'{o}n, Pedro Jos\'{e} and Rothermel, Kurt},
 title = {A quantitative analysis of partitioning in mobile ad hoc networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {400--401},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005737},
 doi = {http://doi.acm.org/10.1145/1012888.1005737},
 acmid = {1005737},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile ad hoc networks, network topology, partition metrics},
} 

@inproceedings{Hahner:2004:QAP:1005686.1005737,
 author = {H\"{a}hner, J\"{o}rg and Dudkowski, Dominique and Marr\'{o}n, Pedro Jos\'{e} and Rothermel, Kurt},
 title = {A quantitative analysis of partitioning in mobile ad hoc networks},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {400--401},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005737},
 doi = {http://doi.acm.org/10.1145/1005686.1005737},
 acmid = {1005737},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile ad hoc networks, network topology, partition metrics},
} 

@inproceedings{Zhang:2004:LTL:1005686.1005738,
 author = {Zhang, Dalu and Huang, Weili and Lin, Chen},
 title = {Locating the tightest link of a network path},
 abstract = {The tightest link of a network path is the link where the end-to-end available bandwidth is limited. We propose a new probe technique, called Dual Rate Periodic Streams (DRPS), for finding the location of the tightest link. A DRPS probe is a periodic stream with two rates. Initially, it goes through the path at a comparatively high rate. When arrived at a particular link, the probe shifts its rate to a lower level and keeps the rate. If proper rates are set to the probe, we can control whether the probe is congested or not by adjusting the shift time. When the point of rate shift is in front of the tightest link, the probe can go through the path without congestion, otherwise congestion occurs. Thus, we can find the location of the tightest link by congestion detection at the receiver.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {402--403},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005738},
 doi = {http://doi.acm.org/10.1145/1005686.1005738},
 acmid = {1005738},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {available bandwidth, dual rate periodic streams (DRPS), network measurements, tight link},
} 

@article{Zhang:2004:LTL:1012888.1005738,
 author = {Zhang, Dalu and Huang, Weili and Lin, Chen},
 title = {Locating the tightest link of a network path},
 abstract = {The tightest link of a network path is the link where the end-to-end available bandwidth is limited. We propose a new probe technique, called Dual Rate Periodic Streams (DRPS), for finding the location of the tightest link. A DRPS probe is a periodic stream with two rates. Initially, it goes through the path at a comparatively high rate. When arrived at a particular link, the probe shifts its rate to a lower level and keeps the rate. If proper rates are set to the probe, we can control whether the probe is congested or not by adjusting the shift time. When the point of rate shift is in front of the tightest link, the probe can go through the path without congestion, otherwise congestion occurs. Thus, we can find the location of the tightest link by congestion detection at the receiver.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {402--403},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005738},
 doi = {http://doi.acm.org/10.1145/1012888.1005738},
 acmid = {1005738},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {available bandwidth, dual rate periodic streams (DRPS), network measurements, tight link},
} 

@article{Sullivan:2004:UPR:1012888.1005739,
 author = {Sullivan, David G. and Seltzer, Margo I. and Pfeffer, Avi},
 title = {Using probabilistic reasoning to automate software tuning},
 abstract = {Manually tuning the parameters or "knobs" of a complex software system is an extremely difficult task. Ideally, the process of software tuning should be automated, allowing software systems to reconfigure themselves as needed in response to changing conditions. We present a methodology that uses a probabilistic, graphical model known as an influence diagram as the foundation of an effective, automated approach to software tuning. We have used our methodology to simultaneously tune four knobs from the Berkeley DB embedded database system, and our results show that an influence diagram can effectively generalize from training data for this domain.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {404--405},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005739},
 doi = {http://doi.acm.org/10.1145/1012888.1005739},
 acmid = {1005739},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {influence diagrams, probabilistic reasoning, self-tuning systems},
} 

@inproceedings{Sullivan:2004:UPR:1005686.1005739,
 author = {Sullivan, David G. and Seltzer, Margo I. and Pfeffer, Avi},
 title = {Using probabilistic reasoning to automate software tuning},
 abstract = {Manually tuning the parameters or "knobs" of a complex software system is an extremely difficult task. Ideally, the process of software tuning should be automated, allowing software systems to reconfigure themselves as needed in response to changing conditions. We present a methodology that uses a probabilistic, graphical model known as an influence diagram as the foundation of an effective, automated approach to software tuning. We have used our methodology to simultaneously tune four knobs from the Berkeley DB embedded database system, and our results show that an influence diagram can effectively generalize from training data for this domain.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {404--405},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005739},
 doi = {http://doi.acm.org/10.1145/1005686.1005739},
 acmid = {1005739},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {influence diagrams, probabilistic reasoning, self-tuning systems},
} 

@article{Wang:2004:MSV:1012888.1005740,
 author = {Wang, Bing and Kurose, Jim and Shenoy, Prashant and Towsley, Don},
 title = {Multimedia streaming via TCP: an analytic performance study},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {406--407},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005740},
 doi = {http://doi.acm.org/10.1145/1012888.1005740},
 acmid = {1005740},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multimedia streaming, performance modeling},
} 

@inproceedings{Wang:2004:MSV:1005686.1005740,
 author = {Wang, Bing and Kurose, Jim and Shenoy, Prashant and Towsley, Don},
 title = {Multimedia streaming via TCP: an analytic performance study},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {406--407},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005740},
 doi = {http://doi.acm.org/10.1145/1005686.1005740},
 acmid = {1005740},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multimedia streaming, performance modeling},
} 

@inproceedings{Wynter:2004:PIQ:1005686.1005741,
 author = {Wynter, Laura and Xia, Cathy H. and Zhang, Fan},
 title = {Parameter inference of queueing models for IT systems using end-to-end measurements},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {408--409},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005741},
 doi = {http://doi.acm.org/10.1145/1005686.1005741},
 acmid = {1005741},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {end-to-end measurements, inference, queueing models},
} 

@article{Wynter:2004:PIQ:1012888.1005741,
 author = {Wynter, Laura and Xia, Cathy H. and Zhang, Fan},
 title = {Parameter inference of queueing models for IT systems using end-to-end measurements},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {408--409},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005741},
 doi = {http://doi.acm.org/10.1145/1012888.1005741},
 acmid = {1005741},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {end-to-end measurements, inference, queueing models},
} 

@article{Pfaff:2004:PAB:1012888.1005742,
 author = {Pfaff, Ben},
 title = {Performance analysis of BSTs in system software},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {410--411},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005742},
 doi = {http://doi.acm.org/10.1145/1012888.1005742},
 acmid = {1005742},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AVL tree, BST, binary search tree, red-black tree, splay tree, threaded tree},
} 

@inproceedings{Pfaff:2004:PAB:1005686.1005742,
 author = {Pfaff, Ben},
 title = {Performance analysis of BSTs in system software},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {410--411},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005742},
 doi = {http://doi.acm.org/10.1145/1005686.1005742},
 acmid = {1005742},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AVL tree, BST, binary search tree, red-black tree, splay tree, threaded tree},
} 

@article{Wang:2004:SDP:1012888.1005743,
 author = {Wang, Mengzhi and Au, Kinman and Ailamaki, Anastassia and Brockwell, Anthony and Faloutsos, Christos and Ganger, Gregory R.},
 title = {Storage device performance prediction with CART models},
 abstract = {This work explores the application of a machine learning tool, CART modeling, to storage devices. We have developed approaches to predict a device's performance as a function of input workloads, requiring no knowledge of the device internals. Two uses of CART models are considered: one that predicts per-request response times (and then derives aggregate values) and one that predicts aggregate values directly from workload characteristics. After training on the device in question, both provide reasonably-accurate black box models across a range of test traces from real environments. An expanded version of this paper is available as a technical report [1].},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {412--413},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005743},
 doi = {http://doi.acm.org/10.1145/1012888.1005743},
 acmid = {1005743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance prediction, storage device modeling},
} 

@inproceedings{Wang:2004:SDP:1005686.1005743,
 author = {Wang, Mengzhi and Au, Kinman and Ailamaki, Anastassia and Brockwell, Anthony and Faloutsos, Christos and Ganger, Gregory R.},
 title = {Storage device performance prediction with CART models},
 abstract = {This work explores the application of a machine learning tool, CART modeling, to storage devices. We have developed approaches to predict a device's performance as a function of input workloads, requiring no knowledge of the device internals. Two uses of CART models are considered: one that predicts per-request response times (and then derives aggregate values) and one that predicts aggregate values directly from workload characteristics. After training on the device in question, both provide reasonably-accurate black box models across a range of test traces from real environments. An expanded version of this paper is available as a technical report [1].},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {412--413},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005743},
 doi = {http://doi.acm.org/10.1145/1005686.1005743},
 acmid = {1005743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance prediction, storage device modeling},
} 

@inproceedings{Kamra:2004:CPW:1005686.1005744,
 author = {Kamra, Abhinav and Misra, Vishal and Nahum, Erich},
 title = {Controlling the performance of 3-tiered web sites: modeling, design and implementation},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {414--415},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005744},
 doi = {http://doi.acm.org/10.1145/1005686.1005744},
 acmid = {1005744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TPC-W, admission control, control theory, e-commerce},
} 

@article{Kamra:2004:CPW:1012888.1005744,
 author = {Kamra, Abhinav and Misra, Vishal and Nahum, Erich},
 title = {Controlling the performance of 3-tiered web sites: modeling, design and implementation},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {414--415},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005744},
 doi = {http://doi.acm.org/10.1145/1012888.1005744},
 acmid = {1005744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TPC-W, admission control, control theory, e-commerce},
} 

@article{Roughan:2004:CRT:1012888.1005745,
 author = {Roughan, Matthew and Griffin, Tim and Mao, Morley and Greenberg, Albert and Freeman, Brian},
 title = {Combining routing and traffic data for detection of IP forwarding anomalies},
 abstract = {IP forwarding anomalies, triggered by equipment failures, implementation bugs, or configuration errors, can significantly disrupt and degrade network service. Robust and reliable detection of such anomalies is essential to rapid problem diagnosis, problem mitigation, and repair. We propose a simple, robust method that integrates routing and traffic data streams to reliably detect forwarding anomalies. The overall method is scalable, automated and self-training. We find this technique effectively identifies forwarding anomalies, while avoiding the high false alarms rate that would otherwise result if either stream were used unilaterally.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {416--417},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005745},
 doi = {http://doi.acm.org/10.1145/1012888.1005745},
 acmid = {1005745},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, SNMP, network anomaly detection, routing, traffic},
} 

@inproceedings{Roughan:2004:CRT:1005686.1005745,
 author = {Roughan, Matthew and Griffin, Tim and Mao, Morley and Greenberg, Albert and Freeman, Brian},
 title = {Combining routing and traffic data for detection of IP forwarding anomalies},
 abstract = {IP forwarding anomalies, triggered by equipment failures, implementation bugs, or configuration errors, can significantly disrupt and degrade network service. Robust and reliable detection of such anomalies is essential to rapid problem diagnosis, problem mitigation, and repair. We propose a simple, robust method that integrates routing and traffic data streams to reliably detect forwarding anomalies. The overall method is scalable, automated and self-training. We find this technique effectively identifies forwarding anomalies, while avoiding the high false alarms rate that would otherwise result if either stream were used unilaterally.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {416--417},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005745},
 doi = {http://doi.acm.org/10.1145/1005686.1005745},
 acmid = {1005745},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BGP, SNMP, network anomaly detection, routing, traffic},
} 

@article{Tao:2004:EPB:1012888.1005746,
 author = {Tao, Shu and Xu, Kuai and Xu, Ying and Fei, Teng and Gao, Lixin and Guerin, Roch and Kurose, Jim and Towsley, Don and Zhang, Zhi-Li},
 title = {Exploring the performance benefits of end-to-end path switching},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {418--419},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005746},
 doi = {http://doi.acm.org/10.1145/1012888.1005746},
 acmid = {1005746},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-homing, overlay, path switching},
} 

@inproceedings{Tao:2004:EPB:1005686.1005746,
 author = {Tao, Shu and Xu, Kuai and Xu, Ying and Fei, Teng and Gao, Lixin and Guerin, Roch and Kurose, Jim and Towsley, Don and Zhang, Zhi-Li},
 title = {Exploring the performance benefits of end-to-end path switching},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {418--419},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005746},
 doi = {http://doi.acm.org/10.1145/1005686.1005746},
 acmid = {1005746},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-homing, overlay, path switching},
} 

@article{Kaplan:2004:CFR:1012888.1005747,
 author = {Kaplan, Scott F.},
 title = {Complete or fast reference trace collection for simulating multiprogrammed workloads: choose one},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {420--421},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005747},
 doi = {http://doi.acm.org/10.1145/1012888.1005747},
 acmid = {1005747},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {reference trace collection, trace-driven simulation},
} 

@inproceedings{Kaplan:2004:CFR:1005686.1005747,
 author = {Kaplan, Scott F.},
 title = {Complete or fast reference trace collection for simulating multiprogrammed workloads: choose one},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {420--421},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005747},
 doi = {http://doi.acm.org/10.1145/1005686.1005747},
 acmid = {1005747},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {reference trace collection, trace-driven simulation},
} 

@article{Raghunath:2004:QTR:1012888.1005748,
 author = {Raghunath, Satish and Kalyanaraman, Shivkumar and Ramakrishnan, K. K.},
 title = {Quantifying trade-offs in resource allocation for VPNs},
 abstract = {Virtual Private Networks (VPNs) feature notable characteristics in structure and traffic patterns that allow for efficient resource allocation. A strategy that exploits the underlying characteristics of a VPN can result in significant capacity savings to the service provider.There are a number of admission control and bandwidth provisioning strategies to choose from. We examine trade-offs in design choices in the context of distinctive characteristics of VPNs. We examine the value of signaling-based mechanisms, traffic matrix information and structural characteristics of VPNs in the way they impact resource utilization and service quality. We arrive at important conclusions which could have an impact on the way VPNs are architected. We show that the structure of VPNs profoundly influences achievable resource utilization gains with various admission control and provisioning schemes.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {422--423},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005748},
 doi = {http://doi.acm.org/10.1145/1012888.1005748},
 acmid = {1005748},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hose model, point-to-multipoint, point-to-set, virtual private networks},
} 

@inproceedings{Raghunath:2004:QTR:1005686.1005748,
 author = {Raghunath, Satish and Kalyanaraman, Shivkumar and Ramakrishnan, K. K.},
 title = {Quantifying trade-offs in resource allocation for VPNs},
 abstract = {Virtual Private Networks (VPNs) feature notable characteristics in structure and traffic patterns that allow for efficient resource allocation. A strategy that exploits the underlying characteristics of a VPN can result in significant capacity savings to the service provider.There are a number of admission control and bandwidth provisioning strategies to choose from. We examine trade-offs in design choices in the context of distinctive characteristics of VPNs. We examine the value of signaling-based mechanisms, traffic matrix information and structural characteristics of VPNs in the way they impact resource utilization and service quality. We arrive at important conclusions which could have an impact on the way VPNs are architected. We show that the structure of VPNs profoundly influences achievable resource utilization gains with various admission control and provisioning schemes.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {422--423},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005748},
 doi = {http://doi.acm.org/10.1145/1005686.1005748},
 acmid = {1005748},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hose model, point-to-multipoint, point-to-set, virtual private networks},
} 

@article{Ruan:2004:ONS:1012888.1005749,
 author = {Ruan, Yaoping and Pai, Vivek S.},
 title = {The origins of network server latency \& the myth of connection scheduling},
 abstract = {We investigate the origins of server-induced latency to understand how to improve latency optimization techniques. Using the Flash Web server [4], we analyze latency behavior under various loads. Despite latency profiles that suggest standard queuing delays, we find that most latency actually originates from negative interactions between the application and the locking and blocking mechanisms in the kernel. Modifying the server and kernel to avoid these problems yields both qualitative and quantitative changes in the latency profiles -- latency drops by more than an order of magnitude, and the effective service discipline also improves.We find our modifications also mitigate service burstiness in the application, reducing the event queue lengths dramatically and eliminating any benefit from application-level connection scheduling. We identify one remaining source of unfairness, related to competition in the networking stack. We show that adjusting the TCP congestion window size addresses this problem, reducing latency by an additional factor of three.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {424--425},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005749},
 doi = {http://doi.acm.org/10.1145/1012888.1005749},
 acmid = {1005749},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {connection scheduling, latency, network server},
} 

@inproceedings{Ruan:2004:ONS:1005686.1005749,
 author = {Ruan, Yaoping and Pai, Vivek S.},
 title = {The origins of network server latency \& the myth of connection scheduling},
 abstract = {We investigate the origins of server-induced latency to understand how to improve latency optimization techniques. Using the Flash Web server [4], we analyze latency behavior under various loads. Despite latency profiles that suggest standard queuing delays, we find that most latency actually originates from negative interactions between the application and the locking and blocking mechanisms in the kernel. Modifying the server and kernel to avoid these problems yields both qualitative and quantitative changes in the latency profiles -- latency drops by more than an order of magnitude, and the effective service discipline also improves.We find our modifications also mitigate service burstiness in the application, reducing the event queue lengths dramatically and eliminating any benefit from application-level connection scheduling. We identify one remaining source of unfairness, related to competition in the networking stack. We show that adjusting the TCP congestion window size addresses this problem, reducing latency by an additional factor of three.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {424--425},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005749},
 doi = {http://doi.acm.org/10.1145/1005686.1005749},
 acmid = {1005749},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {connection scheduling, latency, network server},
} 

@article{Anagnostakis:2004:HDE:1012888.1005750,
 author = {Anagnostakis, K. G. and Greenwald, M. B.},
 title = {A hybrid direct-indirect estimator of network internal delays},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {426--427},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005750},
 doi = {http://doi.acm.org/10.1145/1012888.1005750},
 acmid = {1005750},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ICMP timestamp, delay, network tomography},
} 

@inproceedings{Anagnostakis:2004:HDE:1005686.1005750,
 author = {Anagnostakis, K. G. and Greenwald, M. B.},
 title = {A hybrid direct-indirect estimator of network internal delays},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {426--427},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005750},
 doi = {http://doi.acm.org/10.1145/1005686.1005750},
 acmid = {1005750},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ICMP timestamp, delay, network tomography},
} 

@article{Carlsson:2004:MPS:1012888.1005751,
 author = {Carlsson, Niklas and Eager, Derek L. and Vernon, Mary K.},
 title = {Multicast protocols for scalable on-demand download},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {428--429},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005751},
 doi = {http://doi.acm.org/10.1145/1012888.1005751},
 acmid = {1005751},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {batching, cyclic multicast, scalable download protocols},
} 

@inproceedings{Carlsson:2004:MPS:1005686.1005751,
 author = {Carlsson, Niklas and Eager, Derek L. and Vernon, Mary K.},
 title = {Multicast protocols for scalable on-demand download},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {428--429},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005751},
 doi = {http://doi.acm.org/10.1145/1005686.1005751},
 acmid = {1005751},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {batching, cyclic multicast, scalable download protocols},
} 

@article{Pai:2004:IPI:1012888.1005752,
 author = {Pai, Vijay S. and Rixner, Scott and Kim, Hyong-youb},
 title = {Isolating the performance impacts of network interface cards through microbenchmarks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {430--431},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005752},
 doi = {http://doi.acm.org/10.1145/1012888.1005752},
 acmid = {1005752},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network server performance, networking microbenchmarks},
} 

@inproceedings{Pai:2004:IPI:1005686.1005752,
 author = {Pai, Vijay S. and Rixner, Scott and Kim, Hyong-youb},
 title = {Isolating the performance impacts of network interface cards through microbenchmarks},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {430--431},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005752},
 doi = {http://doi.acm.org/10.1145/1005686.1005752},
 acmid = {1005752},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network server performance, networking microbenchmarks},
} 

@inproceedings{Chu:2004:ECU:1005686.1005753,
 author = {Chu, Jacky and Labonte, Kevin and Levine, Brian Neil},
 title = {An evaluation of chord using traces of peer-to-peer file sharing},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '04/Performance '04},
 year = {2004},
 isbn = {1-58113-873-3},
 location = {New York, NY, USA},
 pages = {432--433},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1005686.1005753},
 doi = {http://doi.acm.org/10.1145/1005686.1005753},
 acmid = {1005753},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chu:2004:ECU:1012888.1005753,
 author = {Chu, Jacky and Labonte, Kevin and Levine, Brian Neil},
 title = {An evaluation of chord using traces of peer-to-peer file sharing},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {32},
 issue = {1},
 month = {June},
 year = {2004},
 issn = {0163-5999},
 pages = {432--433},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1012888.1005753},
 doi = {http://doi.acm.org/10.1145/1012888.1005753},
 acmid = {1005753},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reed:2006:PRU:1140277.1140278,
 author = {Reed, Daniel A.},
 title = {Performance and reliability: the ubiquitous challenge},
 abstract = {Legend says that Archimedes remarked, on the discovery of the lever, "Give me a place to stand and I can move the world." Today, computing pervades all aspects of society. "Science" and "computational science" have become largely synonymous, and computing is the intellectual lever that opens the pathway to discovery in diverse domains. As new discoveries increasingly lie at the interstices of traditional disciplines, computing is also the enabler for scholarship in the arts, humanities, creative practice and public policy. Equally importantly, computing supports our critical infrastructure, from monetary and communication systems to the electric power grid.With such pervasive dependence, computing system reliability and performance are ever more critical. Although the mean time before failure (MTBF) of commodity hardware components (i.e., processors, disks, memories, power supplies and networks) is high, their use in large, mission critical systems can still lead to systemic failures. Our thesis is that the "two worlds" of software -- distributed systems and sequential/parallel systems -- must meet, embodying ideas from each, if we are to build resilient systems. This talk surveys some of these challenges and presents possible approaches for resilient design, ranging from intelligent hardware monitoring and adaptation, through low-overhead recovery schemes, statistical sampling and differential scheduling and to alternative models of system software, including evolutionary adaptation.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140278},
 doi = {http://doi.acm.org/10.1145/1140277.1140278},
 acmid = {1140278},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reed:2006:PRU:1140103.1140278,
 author = {Reed, Daniel A.},
 title = {Performance and reliability: the ubiquitous challenge},
 abstract = {Legend says that Archimedes remarked, on the discovery of the lever, "Give me a place to stand and I can move the world." Today, computing pervades all aspects of society. "Science" and "computational science" have become largely synonymous, and computing is the intellectual lever that opens the pathway to discovery in diverse domains. As new discoveries increasingly lie at the interstices of traditional disciplines, computing is also the enabler for scholarship in the arts, humanities, creative practice and public policy. Equally importantly, computing supports our critical infrastructure, from monetary and communication systems to the electric power grid.With such pervasive dependence, computing system reliability and performance are ever more critical. Although the mean time before failure (MTBF) of commodity hardware components (i.e., processors, disks, memories, power supplies and networks) is high, their use in large, mission critical systems can still lead to systemic failures. Our thesis is that the "two worlds" of software -- distributed systems and sequential/parallel systems -- must meet, embodying ideas from each, if we are to build resilient systems. This talk surveys some of these challenges and presents possible approaches for resilient design, ranging from intelligent hardware monitoring and adaptation, through low-overhead recovery schemes, statistical sampling and differential scheduling and to alternative models of system software, including evolutionary adaptation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140278},
 doi = {http://doi.acm.org/10.1145/1140103.1140278},
 acmid = {1140278},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thereska:2006:STA:1140103.1140280,
 author = {Thereska, Eno and Salmon, Brandon and Strunk, John and Wachs, Matthew and Abd-El-Malek, Michael and Lopez, Julio and Ganger, Gregory R.},
 title = {Stardust: tracking activity in a distributed storage system},
 abstract = {Performance monitoring in most distributed systems provides minimal guidance for tuning, problem diagnosis, and decision making. Stardust is a monitoring infrastructure that replaces traditional performance counters with end-to-end traces of requests and allows for efficient querying of performance metrics. Such traces better inform key administrative performance challenges by enabling, for example, extraction of per-workload, per-resource demand information and per-workload latency graphs. This paper reports on our experience building and using end-to-end tracing as an on-line monitoring tool in a distributed storage system. Using diverse system workloads and scenarios, we show that such fine-grained tracing can be made efficient (less than 6\% overhead) and is useful for on- and off-line analysis of system behavior. These experiences make a case for having other systems incorporate such an instrumentation framework.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140280},
 doi = {http://doi.acm.org/10.1145/1140103.1140280},
 acmid = {1140280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Ursa Minor, end-to-end tracing, request causal chain},
} 

@inproceedings{Thereska:2006:STA:1140277.1140280,
 author = {Thereska, Eno and Salmon, Brandon and Strunk, John and Wachs, Matthew and Abd-El-Malek, Michael and Lopez, Julio and Ganger, Gregory R.},
 title = {Stardust: tracking activity in a distributed storage system},
 abstract = {Performance monitoring in most distributed systems provides minimal guidance for tuning, problem diagnosis, and decision making. Stardust is a monitoring infrastructure that replaces traditional performance counters with end-to-end traces of requests and allows for efficient querying of performance metrics. Such traces better inform key administrative performance challenges by enabling, for example, extraction of per-workload, per-resource demand information and per-workload latency graphs. This paper reports on our experience building and using end-to-end tracing as an on-line monitoring tool in a distributed storage system. Using diverse system workloads and scenarios, we show that such fine-grained tracing can be made efficient (less than 6\% overhead) and is useful for on- and off-line analysis of system behavior. These experiences make a case for having other systems incorporate such an instrumentation framework.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140280},
 doi = {http://doi.acm.org/10.1145/1140277.1140280},
 acmid = {1140280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Ursa Minor, end-to-end tracing, request causal chain},
} 

@inproceedings{Pinheiro:2006:ERC:1140277.1140281,
 author = {Pinheiro, Eduardo and Bianchini, Ricardo and Dubnicki, Cezary},
 title = {Exploiting redundancy to conserve energy in storage systems},
 abstract = {This paper makes two main contributions. First, it introduces Diverted Accesses, a technique that leverages the redundancy in storage systems to conserve disk energy. Second, it evaluates the previous (redundancy-oblivious) energy conservation techniques, along with Diverted Accesses, as a function of the amount and type of redundancy in the system. The evaluation is based on novel analytic models of the energy consumed by the techniques. Using these energy models and previous models of reliability, availability, and performance, we can determine the best redundancy configuration for new energy-aware storage systems. To study Diverted Accesses for realistic systems and workloads, we simulate a wide-area storage system under two file-access traces. Our modeling results show that Diverted Accesses is more effective and robust than the redundancy-oblivious techniques. Our simulation results show that our technique can conserve 20-61\% of the disk energy consumed by the wide-area storage system.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140281},
 doi = {http://doi.acm.org/10.1145/1140277.1140281},
 acmid = {1140281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disk energy, energy management, energy modeling},
} 

@article{Pinheiro:2006:ERC:1140103.1140281,
 author = {Pinheiro, Eduardo and Bianchini, Ricardo and Dubnicki, Cezary},
 title = {Exploiting redundancy to conserve energy in storage systems},
 abstract = {This paper makes two main contributions. First, it introduces Diverted Accesses, a technique that leverages the redundancy in storage systems to conserve disk energy. Second, it evaluates the previous (redundancy-oblivious) energy conservation techniques, along with Diverted Accesses, as a function of the amount and type of redundancy in the system. The evaluation is based on novel analytic models of the energy consumed by the techniques. Using these energy models and previous models of reliability, availability, and performance, we can determine the best redundancy configuration for new energy-aware storage systems. To study Diverted Accesses for realistic systems and workloads, we simulate a wide-area storage system under two file-access traces. Our modeling results show that Diverted Accesses is more effective and robust than the redundancy-oblivious techniques. Our simulation results show that our technique can conserve 20-61\% of the disk energy consumed by the wide-area storage system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140281},
 doi = {http://doi.acm.org/10.1145/1140103.1140281},
 acmid = {1140281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disk energy, energy management, energy modeling},
} 

@inproceedings{Modiano:2006:MTW:1140277.1140283,
 author = {Modiano, Eytan and Shah, Devavrat and Zussman, Gil},
 title = {Maximizing throughput in wireless networks via gossiping},
 abstract = {A major challenge in the design of wireless networks is the need for distributed scheduling algorithms that will efficiently share the common spectrum. Recently, a few distributed algorithms for networks in which a node can converse with at most a single neighbor at a time have been presented. These algorithms guarantee 50\% of the maximum possible throughput. We present the first distributed scheduling framework that guarantees maximum throughput</i>. It is based on a combination of a distributed matching algorithm and an algorithm that compares and merges successive matching solutions. The comparison can be done by a deterministic algorithm or by randomized gossip algorithms. In the latter case, the comparison may be inaccurate. Yet, we show that if the matching and gossip algorithms satisfy simple conditions related to their performance and to the inaccuracy of the comparison (respectively), the framework attains the desired throughput.It is shown that the complexities of our algorithms, that achieve nearly 100\% throughput, are comparable to those of the algorithms that achieve 50\% throughput. Finally, we discuss extensions to general interference models. Even for such models, the framework provides a simple distributed throughput optimal algorithm.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {27--38},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140283},
 doi = {http://doi.acm.org/10.1145/1140277.1140283},
 acmid = {1140283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithms, gossip algorithms, matching, scheduling, stability, wireless networks},
} 

@article{Modiano:2006:MTW:1140103.1140283,
 author = {Modiano, Eytan and Shah, Devavrat and Zussman, Gil},
 title = {Maximizing throughput in wireless networks via gossiping},
 abstract = {A major challenge in the design of wireless networks is the need for distributed scheduling algorithms that will efficiently share the common spectrum. Recently, a few distributed algorithms for networks in which a node can converse with at most a single neighbor at a time have been presented. These algorithms guarantee 50\% of the maximum possible throughput. We present the first distributed scheduling framework that guarantees maximum throughput</i>. It is based on a combination of a distributed matching algorithm and an algorithm that compares and merges successive matching solutions. The comparison can be done by a deterministic algorithm or by randomized gossip algorithms. In the latter case, the comparison may be inaccurate. Yet, we show that if the matching and gossip algorithms satisfy simple conditions related to their performance and to the inaccuracy of the comparison (respectively), the framework attains the desired throughput.It is shown that the complexities of our algorithms, that achieve nearly 100\% throughput, are comparable to those of the algorithms that achieve 50\% throughput. Finally, we discuss extensions to general interference models. Even for such models, the framework provides a simple distributed throughput optimal algorithm.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {27--38},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140283},
 doi = {http://doi.acm.org/10.1145/1140103.1140283},
 acmid = {1140283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithms, gossip algorithms, matching, scheduling, stability, wireless networks},
} 

@article{Gao:2006:DET:1140103.1140284,
 author = {Gao, Yan and Chiu, Dah-Ming and Lui, John C.S.},
 title = {Determining the end-to-end throughput capacity in multi-hop networks: methodology and applications},
 abstract = {In this paper, we present a methodology to analytically compute the throughput capacity</i>, or the maximum end-to-end throughput of a given source and destination pair in a multi-hop wireless network. The end-to-end throughput capacity is computed by considering the interference due to neighboring nodes, as well as various modes of hidden node interference. Knowing the throughput capacity is important because it facilitates the design of routing policy, admission control for realtime traffic, as well as load control for wireless networks. We model location-dependent neighboring interference and we use a contention graph to represent these interference relationships. Based on the contention graph, we formulate the individual link capacity as a set of fixed point equations. The end-to-end throughput capacity can then be determined once these link capacities are obtained. To illustrate the utility of our proposed methodology, we present two important applications: (a) route optimization</i> to determine the path with the maximum end-to-end throughput capacity and, (b) optimal offered load control</i> for a given path so that the maximum end-to-end capacity can be achieved. Extensive simulations are carried out to verify and validate the proposed analytical methodology.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140284},
 doi = {http://doi.acm.org/10.1145/1140103.1140284},
 acmid = {1140284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical model for 802.11 protocols, multi-hop ad hoc wireless networks, throughput capacity},
} 

@inproceedings{Gao:2006:DET:1140277.1140284,
 author = {Gao, Yan and Chiu, Dah-Ming and Lui, John C.S.},
 title = {Determining the end-to-end throughput capacity in multi-hop networks: methodology and applications},
 abstract = {In this paper, we present a methodology to analytically compute the throughput capacity</i>, or the maximum end-to-end throughput of a given source and destination pair in a multi-hop wireless network. The end-to-end throughput capacity is computed by considering the interference due to neighboring nodes, as well as various modes of hidden node interference. Knowing the throughput capacity is important because it facilitates the design of routing policy, admission control for realtime traffic, as well as load control for wireless networks. We model location-dependent neighboring interference and we use a contention graph to represent these interference relationships. Based on the contention graph, we formulate the individual link capacity as a set of fixed point equations. The end-to-end throughput capacity can then be determined once these link capacities are obtained. To illustrate the utility of our proposed methodology, we present two important applications: (a) route optimization</i> to determine the path with the maximum end-to-end throughput capacity and, (b) optimal offered load control</i> for a given path so that the maximum end-to-end capacity can be achieved. Extensive simulations are carried out to verify and validate the proposed analytical methodology.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140284},
 doi = {http://doi.acm.org/10.1145/1140277.1140284},
 acmid = {1140284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical model for 802.11 protocols, multi-hop ad hoc wireless networks, throughput capacity},
} 

@inproceedings{Koksal:2006:ICV:1140277.1140285,
 author = {Koksal, Can Emre and Jamieson, Kyle and Telatar, Emre and Thiran, Patrick},
 title = {Impacts of channel variability on link-level throughput in wireless networks},
 abstract = {We study analytically and experimentally the throughput of the packetized time-varying discrete erasure channel with feedback, which closely captures the behavior of many practical physical layers. We observe that the channel variability at different time scales affects the link-level throughput positively or negatively depending on its time scale. We show that the increased variability in the channel at a time scale smaller than a single packet increases the link-level throughput, whereas the variability at a time scale longer than a single packet reduces it. We express the throughput as a function of the number of transmissions per packet and evaluate it as in terms of the cumulants of the samples of the stochastic processes, which model the channel. We also illustrate our results experimentally using mote radios.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {51--62},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140285},
 doi = {http://doi.acm.org/10.1145/1140277.1140285},
 acmid = {1140285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel modelling, channel variability, link estimation},
} 

@article{Koksal:2006:ICV:1140103.1140285,
 author = {Koksal, Can Emre and Jamieson, Kyle and Telatar, Emre and Thiran, Patrick},
 title = {Impacts of channel variability on link-level throughput in wireless networks},
 abstract = {We study analytically and experimentally the throughput of the packetized time-varying discrete erasure channel with feedback, which closely captures the behavior of many practical physical layers. We observe that the channel variability at different time scales affects the link-level throughput positively or negatively depending on its time scale. We show that the increased variability in the channel at a time scale smaller than a single packet increases the link-level throughput, whereas the variability at a time scale longer than a single packet reduces it. We express the throughput as a function of the number of transmissions per packet and evaluate it as in terms of the cumulants of the samples of the stochastic processes, which model the channel. We also illustrate our results experimentally using mote radios.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {51--62},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140285},
 doi = {http://doi.acm.org/10.1145/1140103.1140285},
 acmid = {1140285},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel modelling, channel variability, link estimation},
} 

@inproceedings{Mishra:2006:POC:1140277.1140286,
 author = {Mishra, Arunesh and Shrivastava, Vivek and Banerjee, Suman and Arbaugh, William},
 title = {Partially overlapped channels not considered harmful},
 abstract = {Many wireless channels in different technologies are known to have partial overlap. However, due to the interference effects among such partially overlapped channels, their simultaneous use has typically been avoided. In this paper, we present a first attempt to model partial overlap between channels in a systematic manner. Through the model, we illustrate that the use of partially overlapped channels is not always harmful. In fact, a careful use of some partially overlapped channels can often lead to significant improvements in spectrum utilization and application performance. We demonstrate this through analysis as well as through detailed application-level and MAC-level measurements. Additionally, we illustrate the benefits of our developed model by using it to directly enhance the performance of two previously proposed channel assignment algorithms --- one in the context of wireless LANs and the other in the context of multi-hop wireless mesh networks. Through detailed simulations, we show that use of partially overlapped channels in both these cases can improve end-to-end application throughput by factors between 1.6 and 2.7 in different scenarios, depending on wireless node density. We conclude by observing that the notion of partial overlap can be the right model of flexibility to design efficient channel access mechanisms in the emerging software radio platforms.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140286},
 doi = {http://doi.acm.org/10.1145/1140277.1140286},
 acmid = {1140286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IEEE 802.11, channel assignment, partially overlapped channels},
} 

@article{Mishra:2006:POC:1140103.1140286,
 author = {Mishra, Arunesh and Shrivastava, Vivek and Banerjee, Suman and Arbaugh, William},
 title = {Partially overlapped channels not considered harmful},
 abstract = {Many wireless channels in different technologies are known to have partial overlap. However, due to the interference effects among such partially overlapped channels, their simultaneous use has typically been avoided. In this paper, we present a first attempt to model partial overlap between channels in a systematic manner. Through the model, we illustrate that the use of partially overlapped channels is not always harmful. In fact, a careful use of some partially overlapped channels can often lead to significant improvements in spectrum utilization and application performance. We demonstrate this through analysis as well as through detailed application-level and MAC-level measurements. Additionally, we illustrate the benefits of our developed model by using it to directly enhance the performance of two previously proposed channel assignment algorithms --- one in the context of wireless LANs and the other in the context of multi-hop wireless mesh networks. Through detailed simulations, we show that use of partially overlapped channels in both these cases can improve end-to-end application throughput by factors between 1.6 and 2.7 in different scenarios, depending on wireless node density. We conclude by observing that the notion of partial overlap can be the right model of flexibility to design efficient channel access mechanisms in the emerging software radio platforms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140286},
 doi = {http://doi.acm.org/10.1145/1140103.1140286},
 acmid = {1140286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IEEE 802.11, channel assignment, partially overlapped channels},
} 

@inproceedings{Lieshout:2006:GSS:1140277.1140288,
 author = {Lieshout, P. and Mandjes, M. and Borst, S.},
 title = {GPS scheduling: selection of optimal weights and comparison with strict priorities},
 abstract = {We consider a system with two service classes with heterogeneous traffic characteristics and Quality-of-Service requirements. The available bandwidth is shared between the two traffic classes in accordance with the Generalized Processor Sharing (GPS) discipline. GPS-based scheduling algorithms, such as Weighted Fair Queueing, provide a popular mechanism for service differentiation among heterogeneous traffic classes. While the performance of GPS for given weights has been thoroughly examined, the problem of selecting weight values that maximize the traffic-carrying capacity, has only received limited attention so far. In the present paper, we address the latter problem for the case of general Gaussian traffic sources. Gaussian models cover a wide variety of both long-range dependent and short-range dependent processes, and are especially suitable at relatively high levels of aggregation. In particular, we determine the realizable region, i.e., the combinations of traffic sources that can be supported for given Quality-of-Service requirements in terms of loss and delay metrics. The results yield the remarkable observation that simple priority scheduling strategies achieve nearly the full realizable region. <sup>1</sup>.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140288},
 doi = {http://doi.acm.org/10.1145/1140277.1140288},
 acmid = {1140288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Gaussian traffic, admissible region, generalized processor sharing, loss probabilities, priority scheduling, weight setting},
} 

@article{Lieshout:2006:GSS:1140103.1140288,
 author = {Lieshout, P. and Mandjes, M. and Borst, S.},
 title = {GPS scheduling: selection of optimal weights and comparison with strict priorities},
 abstract = {We consider a system with two service classes with heterogeneous traffic characteristics and Quality-of-Service requirements. The available bandwidth is shared between the two traffic classes in accordance with the Generalized Processor Sharing (GPS) discipline. GPS-based scheduling algorithms, such as Weighted Fair Queueing, provide a popular mechanism for service differentiation among heterogeneous traffic classes. While the performance of GPS for given weights has been thoroughly examined, the problem of selecting weight values that maximize the traffic-carrying capacity, has only received limited attention so far. In the present paper, we address the latter problem for the case of general Gaussian traffic sources. Gaussian models cover a wide variety of both long-range dependent and short-range dependent processes, and are especially suitable at relatively high levels of aggregation. In particular, we determine the realizable region, i.e., the combinations of traffic sources that can be supported for given Quality-of-Service requirements in terms of loss and delay metrics. The results yield the remarkable observation that simple priority scheduling strategies achieve nearly the full realizable region. <sup>1</sup>.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140288},
 doi = {http://doi.acm.org/10.1145/1140103.1140288},
 acmid = {1140288},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Gaussian traffic, admissible region, generalized processor sharing, loss probabilities, priority scheduling, weight setting},
} 

@article{Gromoll:2006:IRP:1140103.1140289,
 author = {Gromoll, H. Christian and Robert, Philippe and Zwart, Bert and Bakker, Richard},
 title = {The impact of reneging in processor sharing queues},
 abstract = {We investigate an overloaded processor sharing queue with renewal arrivals and generally distributed service times. Impatient customers may abandon the queue, or renege, before completing service. The random time representing a customer's patience has a general distribution and may be dependent on his initial service time requirement.We propose a scaling procedure that gives rise to a fluid model, with nontrivial yet tractable steady state behavior. This fluid model captures many essential features of the underlying stochastic model, and we use it to analyze the impact of impatience in processor sharing queues. We show that this impact can be substantial compared with FCFS, and we propose a simple admission control policy to overcome these negative impacts.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {87--96},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1140103.1140289},
 doi = {http://doi.acm.org/10.1145/1140103.1140289},
 acmid = {1140289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {admission control, delay-differential equation, fluid limits, measure valued process, processor sharing, queues in overload, queues with impatience, user behavior},
} 

@inproceedings{Gromoll:2006:IRP:1140277.1140289,
 author = {Gromoll, H. Christian and Robert, Philippe and Zwart, Bert and Bakker, Richard},
 title = {The impact of reneging in processor sharing queues},
 abstract = {We investigate an overloaded processor sharing queue with renewal arrivals and generally distributed service times. Impatient customers may abandon the queue, or renege, before completing service. The random time representing a customer's patience has a general distribution and may be dependent on his initial service time requirement.We propose a scaling procedure that gives rise to a fluid model, with nontrivial yet tractable steady state behavior. This fluid model captures many essential features of the underlying stochastic model, and we use it to analyze the impact of impatience in processor sharing queues. We show that this impact can be substantial compared with FCFS, and we propose a simple admission control policy to overcome these negative impacts.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {87--96},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1140277.1140289},
 doi = {http://doi.acm.org/10.1145/1140277.1140289},
 acmid = {1140289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {admission control, delay-differential equation, fluid limits, measure valued process, processor sharing, queues in overload, queues with impatience, user behavior},
} 

@article{Yang:2006:TAP:1140103.1140290,
 author = {Yang, Chang-Woo and Wierman, Adam and Shakkottai, Sanjay and Harchol-Balter, Mor},
 title = {Tail asymptotics for policies favoring short jobs in a many-flows regime},
 abstract = {Scheduling policies that prioritize short jobs have received growing attention in recent years. The class of SMART policies includes many such disciplines, e.g. Shortest-Remaining-Processing-Time (SRPT) and Preemptive-Shortest-Job-First (PSJF). In this work, we study the delay distribution of SMART policies and contrast this distribution with that of the Least-Attained-Service (LAS) policy, which indirectly favors short jobs by prioritizing jobs with the least attained service (age).We study the delay distribution (rate function) of LAS and the SMART class in a discrete-time queueing system under the many sources regime. Our analysis in this regime (large capacity and large number of flows) hinges on a novel two dimensional queue representation, which creates tie-break rules. These additional rules do not alter the policies, but greatly simplify their analysis. We demonstrate that the queue evolution of all the above policies can be described under this single two dimensional framework.We prove that all SMART policies have the same delay distribution as SRPT and illustrate the improvements SMART policies make over First-Come-First-Served (FCFS). Furthermore, we show that the delay distribution of SMART policies stochastically improves upon the delay distribution of LAS. However, the delay distribution under LAS is not too bad -- the distribution of delay under LAS for most jobs sizes still provides improvement over FCFS. Our results are complementary to prior work that studies delay-tail behavior in the large buffer regime under a single flow.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140290},
 doi = {http://doi.acm.org/10.1145/1140103.1140290},
 acmid = {1140290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LAS, SMART, SRPT, large--deviations, many--sources, rate function, scheduling},
} 

@inproceedings{Yang:2006:TAP:1140277.1140290,
 author = {Yang, Chang-Woo and Wierman, Adam and Shakkottai, Sanjay and Harchol-Balter, Mor},
 title = {Tail asymptotics for policies favoring short jobs in a many-flows regime},
 abstract = {Scheduling policies that prioritize short jobs have received growing attention in recent years. The class of SMART policies includes many such disciplines, e.g. Shortest-Remaining-Processing-Time (SRPT) and Preemptive-Shortest-Job-First (PSJF). In this work, we study the delay distribution of SMART policies and contrast this distribution with that of the Least-Attained-Service (LAS) policy, which indirectly favors short jobs by prioritizing jobs with the least attained service (age).We study the delay distribution (rate function) of LAS and the SMART class in a discrete-time queueing system under the many sources regime. Our analysis in this regime (large capacity and large number of flows) hinges on a novel two dimensional queue representation, which creates tie-break rules. These additional rules do not alter the policies, but greatly simplify their analysis. We demonstrate that the queue evolution of all the above policies can be described under this single two dimensional framework.We prove that all SMART policies have the same delay distribution as SRPT and illustrate the improvements SMART policies make over First-Come-First-Served (FCFS). Furthermore, we show that the delay distribution of SMART policies stochastically improves upon the delay distribution of LAS. However, the delay distribution under LAS is not too bad -- the distribution of delay under LAS for most jobs sizes still provides improvement over FCFS. Our results are complementary to prior work that studies delay-tail behavior in the large buffer regime under a single flow.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140290},
 doi = {http://doi.acm.org/10.1145/1140277.1140290},
 acmid = {1140290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LAS, SMART, SRPT, large--deviations, many--sources, rate function, scheduling},
} 

@inproceedings{Bonald:2006:LHT:1140277.1140291,
 author = {Bonald, Thomas and Penttinen, Aleksi and Virtamo, Jorma},
 title = {On light and heavy traffic approximations of balanced fairness},
 abstract = {Flow level analysis of communication networks with multiple shared resources is generally difficult. A recently introduced sharing scheme called balanced fairness has brought these systems within the realm of tractability. While straightforward in principle, the numerical evaluation of practically interesting performance metrics like per-flow throughput is feasible for limited state spaces only, besides some specific networks where the results are explicit. In the present paper, we study the behaviour of balanced fairness in light and heavy traffic regimes and show how the corresponding performance results can be used to approximate the flow throughput over the whole load range. The results apply to any network, with a state space of arbitrary dimension. A few examples are explicitly worked out to illustrate the concepts.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140291},
 doi = {http://doi.acm.org/10.1145/1140277.1140291},
 acmid = {1140291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {balanced fairness, elastic traffic, flow level analysis, throughput approximation},
} 

@article{Bonald:2006:LHT:1140103.1140291,
 author = {Bonald, Thomas and Penttinen, Aleksi and Virtamo, Jorma},
 title = {On light and heavy traffic approximations of balanced fairness},
 abstract = {Flow level analysis of communication networks with multiple shared resources is generally difficult. A recently introduced sharing scheme called balanced fairness has brought these systems within the realm of tractability. While straightforward in principle, the numerical evaluation of practically interesting performance metrics like per-flow throughput is feasible for limited state spaces only, besides some specific networks where the results are explicit. In the present paper, we study the behaviour of balanced fairness in light and heavy traffic regimes and show how the corresponding performance results can be used to approximate the flow throughput over the whole load range. The results apply to any network, with a state space of arbitrary dimension. A few examples are explicitly worked out to illustrate the concepts.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140291},
 doi = {http://doi.acm.org/10.1145/1140103.1140291},
 acmid = {1140291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {balanced fairness, elastic traffic, flow level analysis, throughput approximation},
} 

@inproceedings{Song:2006:NFF:1140277.1140293,
 author = {Song, Han Hee and Qiu, Lili and Zhang, Yin},
 title = {NetQuest: a flexible framework for large-scale network measurement},
 abstract = {In this paper, we present NetQuest, a flexible framework for large-scale network measurement. We apply Bayesian experimental design</i> to select active measurements that maximize the amount of information we gain about the network path properties subject to given resource constraints. We then apply network inference</i> techniques to reconstruct the properties of interest based on the partial, indirect observations we get through these measurements.By casting network measurement in a general Bayesian decision theoretic framework, we achieve flexibility. Our framework can support a variety of design requirements, including (i) differentiated design for providing better resolution to certain parts of the network, (ii) augmented design for conducting additional measurements given existing observations, and (iii) joint design for supporting multiple users who are interested in different parts of the network. Our framework is also scalable</i> and can design measurement experiments that span thousands of routers and end hosts.We develop a toolkit that realizes the framework on PlanetLab. We conduct extensive evaluation using both real traces and synthetic data. Our results show that the approach can accurately estimate network-wide and individual path properties by only monitoring within 2-10\% of paths. We also demonstrate its effectiveness in providing differentiated monitoring, supporting continuous monitoring, and satisfying the requirements of multiple users.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140293},
 doi = {http://doi.acm.org/10.1145/1140277.1140293},
 acmid = {1140293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Bayesian experimental design, network inference, network measurement, network tomography},
} 

@article{Song:2006:NFF:1140103.1140293,
 author = {Song, Han Hee and Qiu, Lili and Zhang, Yin},
 title = {NetQuest: a flexible framework for large-scale network measurement},
 abstract = {In this paper, we present NetQuest, a flexible framework for large-scale network measurement. We apply Bayesian experimental design</i> to select active measurements that maximize the amount of information we gain about the network path properties subject to given resource constraints. We then apply network inference</i> techniques to reconstruct the properties of interest based on the partial, indirect observations we get through these measurements.By casting network measurement in a general Bayesian decision theoretic framework, we achieve flexibility. Our framework can support a variety of design requirements, including (i) differentiated design for providing better resolution to certain parts of the network, (ii) augmented design for conducting additional measurements given existing observations, and (iii) joint design for supporting multiple users who are interested in different parts of the network. Our framework is also scalable</i> and can design measurement experiments that span thousands of routers and end hosts.We develop a toolkit that realizes the framework on PlanetLab. We conduct extensive evaluation using both real traces and synthetic data. Our results show that the approach can accurately estimate network-wide and individual path properties by only monitoring within 2-10\% of paths. We also demonstrate its effectiveness in providing differentiated monitoring, supporting continuous monitoring, and satisfying the requirements of multiple users.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140293},
 doi = {http://doi.acm.org/10.1145/1140103.1140293},
 acmid = {1140293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Bayesian experimental design, network inference, network measurement, network tomography},
} 

@article{Zhao:2006:RTM:1140103.1140294,
 author = {Zhao, Qi and Ge, Zihui and Wang, Jia and Xu, Jun},
 title = {Robust traffic matrix estimation with imperfect information: making use of multiple data sources},
 abstract = {Estimation of traffic matrices, which provide critical input for network capacity planning and traffic engineering, has recently been recognized as an important research problem. Most of the previous approaches infer traffic matrix from either SNMP link loads or sampled NetFlow records. In this work, we design novel inference techniques that, by statistically correlating SNMP link loads and sampled NetFlow records, allow for much more accurate estimation of traffic matrices than obtainable from either information source alone, even when sampled NetFlow records are available at only a subset of ingress. Our techniques are practically important and useful since both SNMP and NetFlow are now widely supported by vendors and deployed in most of the operational IP networks. More importantly, this research leads us to a new insight that SNMP link loads and sampled NetFlow records can serve as "error correction codes" to each other. This insight helps us to solve a challenging open problem in traffic matrix estimation, "How to deal with dirty data (SNMP and NetFlow measurement errors due to hardware/software/transmission problems)?" We design techniques that, by comparing notes between the above two information sources, identify and remove dirty data, and therefore allow for accurate estimation of the traffic matrices with the cleaned dat.We conducted experiments on real measurement data obtained from a large tier-1 ISP backbone network. We show that, when full deployment of NetFlow is not available, our algorithm can improve estimation accuracy significantly even with a small fraction of NetFlow data. More importantly, we show that dirty data can contaminate a traffic matrix, and identifying and removing them can reduce errors in traffic matrix estimation by up to an order of magnitude. Routing changes is another a key factor that affects estimation accuracy. We show that using them as the a priori, the traffic matrices can be estimated much more accurately than those omitting the routing change. To the best of our knowledge, this work is the first to offer a comprehensive solution which fully takes advantage of using multiple readily available data sources. Our results provide valuable insights on the effectiveness of combining flow measurement and link load measurement.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140294},
 doi = {http://doi.acm.org/10.1145/1140103.1140294},
 acmid = {1140294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network measurement, statistical inference, traffic matrix},
} 

@inproceedings{Zhao:2006:RTM:1140277.1140294,
 author = {Zhao, Qi and Ge, Zihui and Wang, Jia and Xu, Jun},
 title = {Robust traffic matrix estimation with imperfect information: making use of multiple data sources},
 abstract = {Estimation of traffic matrices, which provide critical input for network capacity planning and traffic engineering, has recently been recognized as an important research problem. Most of the previous approaches infer traffic matrix from either SNMP link loads or sampled NetFlow records. In this work, we design novel inference techniques that, by statistically correlating SNMP link loads and sampled NetFlow records, allow for much more accurate estimation of traffic matrices than obtainable from either information source alone, even when sampled NetFlow records are available at only a subset of ingress. Our techniques are practically important and useful since both SNMP and NetFlow are now widely supported by vendors and deployed in most of the operational IP networks. More importantly, this research leads us to a new insight that SNMP link loads and sampled NetFlow records can serve as "error correction codes" to each other. This insight helps us to solve a challenging open problem in traffic matrix estimation, "How to deal with dirty data (SNMP and NetFlow measurement errors due to hardware/software/transmission problems)?" We design techniques that, by comparing notes between the above two information sources, identify and remove dirty data, and therefore allow for accurate estimation of the traffic matrices with the cleaned dat.We conducted experiments on real measurement data obtained from a large tier-1 ISP backbone network. We show that, when full deployment of NetFlow is not available, our algorithm can improve estimation accuracy significantly even with a small fraction of NetFlow data. More importantly, we show that dirty data can contaminate a traffic matrix, and identifying and removing them can reduce errors in traffic matrix estimation by up to an order of magnitude. Routing changes is another a key factor that affects estimation accuracy. We show that using them as the a priori, the traffic matrices can be estimated much more accurately than those omitting the routing change. To the best of our knowledge, this work is the first to offer a comprehensive solution which fully takes advantage of using multiple readily available data sources. Our results provide valuable insights on the effectiveness of combining flow measurement and link load measurement.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140294},
 doi = {http://doi.acm.org/10.1145/1140277.1140294},
 acmid = {1140294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network measurement, statistical inference, traffic matrix},
} 

@article{Lall:2006:DSA:1140103.1140295,
 author = {Lall, Ashwin and Sekar, Vyas and Ogihara, Mitsunori and Xu, Jun and Zhang, Hui},
 title = {Data streaming algorithms for estimating entropy of network traffic},
 abstract = {Using entropy of traffic distributions has been shown to aid a wide variety of network monitoring applications such as anomaly detection, clustering to reveal interesting patterns, and traffic classification. However, realizing this potential benefit in practice requires accurate algorithms that can operate on high-speed links, with low CPU and memory requirements. In this paper, we investigate the problem of estimating the entropy in a streaming computation model. We give lower bounds for this problem, showing that neither approximation nor randomization alone will let us compute the entropy efficiently. We present two algorithms for randomly approximating the entropy in a time and space efficient manner, applicable for use on very high speed (greater than OC-48) links. The first algorithm for entropy estimation is inspired by the structural similarity with the seminal work of Alon et al. for estimating frequency moments, and we provide strong theoretical guarantees on the error and resource usage. Our second algorithm utilizes the observation that the performance of the streaming algorithm can be enhanced by separating the high-frequency items (or elephants) from the low-frequency items (or mice). We evaluate our algorithms on traffic traces from different deployment scenarios.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140295},
 doi = {http://doi.acm.org/10.1145/1140103.1140295},
 acmid = {1140295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, traffic analysis},
} 

@inproceedings{Lall:2006:DSA:1140277.1140295,
 author = {Lall, Ashwin and Sekar, Vyas and Ogihara, Mitsunori and Xu, Jun and Zhang, Hui},
 title = {Data streaming algorithms for estimating entropy of network traffic},
 abstract = {Using entropy of traffic distributions has been shown to aid a wide variety of network monitoring applications such as anomaly detection, clustering to reveal interesting patterns, and traffic classification. However, realizing this potential benefit in practice requires accurate algorithms that can operate on high-speed links, with low CPU and memory requirements. In this paper, we investigate the problem of estimating the entropy in a streaming computation model. We give lower bounds for this problem, showing that neither approximation nor randomization alone will let us compute the entropy efficiently. We present two algorithms for randomly approximating the entropy in a time and space efficient manner, applicable for use on very high speed (greater than OC-48) links. The first algorithm for entropy estimation is inspired by the structural similarity with the seminal work of Alon et al. for estimating frequency moments, and we provide strong theoretical guarantees on the error and resource usage. Our second algorithm utilizes the observation that the performance of the streaming algorithm can be enhanced by separating the high-frequency items (or elephants) from the low-frequency items (or mice). We evaluate our algorithms on traffic traces from different deployment scenarios.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140295},
 doi = {http://doi.acm.org/10.1145/1140277.1140295},
 acmid = {1140295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, traffic analysis},
} 

@article{Lee:2006:SEE:1140103.1140296,
 author = {Lee, Sanghwan and Zhang, Zhi-Li and Sahu, Sambit and Saha, Debanjan},
 title = {On suitability of Euclidean embedding of internet hosts},
 abstract = {In this paper, we investigate the suitability of embedding Internet hosts into a Euclidean space given their pairwise distances (as measured by round-trip time). Using the classical scaling and matrix perturbation theories, we first establish the (sum of the) magnitude of negative</i> eigenvalues of the (doubly-centered, squared) distance matrix as a measure of suitability of Euclidean embedding. We then show that the distance matrix among Internet hosts contains negative eigenvalues of large magnitude</i>, implying that embedding the Internet hosts in a Euclidean space would incur relatively large errors. Motivated by earlier studies, we demonstrate that the inaccuracy of Euclidean embedding is caused by a large degree of triangle inequality violation</i> (TIV) in the Internet distances, which leads to negative eigenvalues of large magnitude. Moreover, we show that the TIVs are likely to occur locally</i>, hence, the distances among these close-by hosts cannot be estimated accurately using a global</i> Euclidean embedding, in addition, increasing the dimension of embedding does not reduce the embedding errors. Based on these insights, we propose a new hybrid model for embedding the network nodes using only a 2-dimensional Euclidean coordinate system and small error adjustment terms</i>. We show that the accuracy of the proposed embedding technique is as good as, if not better, than that of a 7-dimensional Euclidean embedding.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140296},
 doi = {http://doi.acm.org/10.1145/1140103.1140296},
 acmid = {1140296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Euclidean embedding, suitability, triangle inequality},
} 

@inproceedings{Lee:2006:SEE:1140277.1140296,
 author = {Lee, Sanghwan and Zhang, Zhi-Li and Sahu, Sambit and Saha, Debanjan},
 title = {On suitability of Euclidean embedding of internet hosts},
 abstract = {In this paper, we investigate the suitability of embedding Internet hosts into a Euclidean space given their pairwise distances (as measured by round-trip time). Using the classical scaling and matrix perturbation theories, we first establish the (sum of the) magnitude of negative</i> eigenvalues of the (doubly-centered, squared) distance matrix as a measure of suitability of Euclidean embedding. We then show that the distance matrix among Internet hosts contains negative eigenvalues of large magnitude</i>, implying that embedding the Internet hosts in a Euclidean space would incur relatively large errors. Motivated by earlier studies, we demonstrate that the inaccuracy of Euclidean embedding is caused by a large degree of triangle inequality violation</i> (TIV) in the Internet distances, which leads to negative eigenvalues of large magnitude. Moreover, we show that the TIVs are likely to occur locally</i>, hence, the distances among these close-by hosts cannot be estimated accurately using a global</i> Euclidean embedding, in addition, increasing the dimension of embedding does not reduce the embedding errors. Based on these insights, we propose a new hybrid model for embedding the network nodes using only a 2-dimensional Euclidean coordinate system and small error adjustment terms</i>. We show that the accuracy of the proposed embedding technique is as good as, if not better, than that of a 7-dimensional Euclidean embedding.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140296},
 doi = {http://doi.acm.org/10.1145/1140277.1140296},
 acmid = {1140296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Euclidean embedding, suitability, triangle inequality},
} 

@article{Casale:2006:EAE:1140103.1140298,
 author = {Casale, Giuliano},
 title = {An efficient algorithm for the exact analysis of multiclass queueing networks with large population sizes},
 abstract = {We introduce an efficient algorithm for the exact analysis of closed multiclass product-form queueing network models with large population sizes. We adopt a novel approach, based on linear systems of equations, which significantly reduces the cost of computing normalizing constants. With the proposed algorithm, the analysis of a model with N</i> circulating jobs of multiple classes requires essentially the solution of N</i> linear systems with order independent of population sizes.A distinguishing feature of our approach is that we can immediately apply theorems, solution techniques, and decompositions for linear systems to queueing network analysis. Following this idea, we propose a block triangular form of the linear system that further reduces the requirements, in terms of both time and storage, of an exact analysis. An example illustrates the efficiency of the resulting algorithm in presence of large populations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140298},
 doi = {http://doi.acm.org/10.1145/1140103.1140298},
 acmid = {1140298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computational algorithms, exact analysis, multiclass models, normalizing constant, product-form queueing networks},
} 

@inproceedings{Casale:2006:EAE:1140277.1140298,
 author = {Casale, Giuliano},
 title = {An efficient algorithm for the exact analysis of multiclass queueing networks with large population sizes},
 abstract = {We introduce an efficient algorithm for the exact analysis of closed multiclass product-form queueing network models with large population sizes. We adopt a novel approach, based on linear systems of equations, which significantly reduces the cost of computing normalizing constants. With the proposed algorithm, the analysis of a model with N</i> circulating jobs of multiple classes requires essentially the solution of N</i> linear systems with order independent of population sizes.A distinguishing feature of our approach is that we can immediately apply theorems, solution techniques, and decompositions for linear systems to queueing network analysis. Following this idea, we propose a block triangular form of the linear system that further reduces the requirements, in terms of both time and storage, of an exact analysis. An example illustrates the efficiency of the resulting algorithm in presence of large populations.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140298},
 doi = {http://doi.acm.org/10.1145/1140277.1140298},
 acmid = {1140298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computational algorithms, exact analysis, multiclass models, normalizing constant, product-form queueing networks},
} 

@inproceedings{Van Velthoven:2006:TAT:1140277.1140299,
 author = {Van Velthoven, J. and Van Houdt, B. and Blondia, C.},
 title = {Transient analysis of tree-Like processes and its application to random access systems},
 abstract = {A new methodology to assess transient performance measures of tree-like processes is proposed by introducing the concept of tree-like processes with marked time epochs. As opposed to the standard tree-like process, such a process marks part of the time epochs by following a set of Markovian rules. Our interest lies in obtaining the system state at the n</i>-th marked time epoch as well as the mean time at which this n</i>-th marking occurs. The methodology transforms the transient problem into a stationary one by applying a discrete Erlangization and constructing a reset Markov chain. A fast algorithm, with limited memory usage, that exploits the block structure of the reset Markov chain is developed and is based, among others, on Sylvester matrix equations and fast Fourier transforms. The theory of tree-like processes generalizes the well-known paradigm of Quasi-Birth-Death Markov chains and has various applications. We demonstrate our approach on the celebrated Capetanakis-Tsybakov-Mikhailov (CTM) random access protocol yielding new insights on its initial behavior both in normal and overload conditions.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1140277.1140299},
 doi = {http://doi.acm.org/10.1145/1140277.1140299},
 acmid = {1140299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Matrix analytic methods, contention resolution, random access algorithms, transient analysis, tree-like processes},
} 

@article{Van Velthoven:2006:TAT:1140103.1140299,
 author = {Van Velthoven, J. and Van Houdt, B. and Blondia, C.},
 title = {Transient analysis of tree-Like processes and its application to random access systems},
 abstract = {A new methodology to assess transient performance measures of tree-like processes is proposed by introducing the concept of tree-like processes with marked time epochs. As opposed to the standard tree-like process, such a process marks part of the time epochs by following a set of Markovian rules. Our interest lies in obtaining the system state at the n</i>-th marked time epoch as well as the mean time at which this n</i>-th marking occurs. The methodology transforms the transient problem into a stationary one by applying a discrete Erlangization and constructing a reset Markov chain. A fast algorithm, with limited memory usage, that exploits the block structure of the reset Markov chain is developed and is based, among others, on Sylvester matrix equations and fast Fourier transforms. The theory of tree-like processes generalizes the well-known paradigm of Quasi-Birth-Death Markov chains and has various applications. We demonstrate our approach on the celebrated Capetanakis-Tsybakov-Mikhailov (CTM) random access protocol yielding new insights on its initial behavior both in normal and overload conditions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1140103.1140299},
 doi = {http://doi.acm.org/10.1145/1140103.1140299},
 acmid = {1140299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Matrix analytic methods, contention resolution, random access algorithms, transient analysis, tree-like processes},
} 

@inproceedings{Buchholz:2006:BSR:1140277.1140300,
 author = {Buchholz, Peter},
 title = {Bounding stationary results of Tandem networks with MAP input and PH service time distributions},
 abstract = {In this paper, we propose a new approach to compute bounds on stationary measures of queueing systems with an input process described by a Markovian Arrival Process (MAP) and a sequence of stations with Phase Type (PH) service time distributions. Such queueing systems cannot be solved exactly since they have an infinite state space in several natural dimensions. Based on earlier work on the computation of bounds for specific classes of infinite Markov chains, the paper presents a new approach specifically tailored to the analysis of the mentioned class of queueing networks. By increasing the size of the state space of the aggregated Markov chain to be solved for bound computation, bounds can be made arbitrarily tight, but practical limits come up due to the computational complexity. However, we show by means of several examples that tight bounds can be derived with low effort for a large set of queueing systems in the mentioned class.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140300},
 doi = {http://doi.acm.org/10.1145/1140277.1140300},
 acmid = {1140300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chains, Tandem queues, bounds, stationary analysis},
} 

@article{Buchholz:2006:BSR:1140103.1140300,
 author = {Buchholz, Peter},
 title = {Bounding stationary results of Tandem networks with MAP input and PH service time distributions},
 abstract = {In this paper, we propose a new approach to compute bounds on stationary measures of queueing systems with an input process described by a Markovian Arrival Process (MAP) and a sequence of stations with Phase Type (PH) service time distributions. Such queueing systems cannot be solved exactly since they have an infinite state space in several natural dimensions. Based on earlier work on the computation of bounds for specific classes of infinite Markov chains, the paper presents a new approach specifically tailored to the analysis of the mentioned class of queueing networks. By increasing the size of the state space of the aggregated Markov chain to be solved for bound computation, bounds can be made arbitrarily tight, but practical limits come up due to the computational complexity. However, we show by means of several examples that tight bounds can be derived with low effort for a large set of queueing systems in the mentioned class.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {191--202},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140300},
 doi = {http://doi.acm.org/10.1145/1140103.1140300},
 acmid = {1140300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chains, Tandem queues, bounds, stationary analysis},
} 

@inproceedings{Gupta:2006:FCQ:1140277.1140301,
 author = {Gupta, Varun and Harchol-Balter, Mor and Wolf, Alan Scheller and Yechiali, Uri},
 title = {Fundamental characteristics of queues with fluctuating load},
 abstract = {Systems whose arrival or service rates fluctuate over time are very common, but are still not well understood analytically. Stationary formulas are poor predictors of systems with fluctuating load. When the arrival and service processes fluctuate in a Markovian manner, computational methods, such as Matrix-analytic and spectral analysis, have been instrumental in the numerical evaluation of quantities like mean response time. However, such computational tools provide only limited insight into the functional behavior</i> of the system with respect to its primitive input parameters: the arrival rates, service rates, and rate of fluctuation.For example, the shape of the function that maps rate of fluctuation to mean response time is not well understood, even for an M/M/1 system. Is this function increasing, decreasing, monotonic? How is its shape affected by the primitive input parameters? Is there a simple closed-form approximation for the shape of this curve? Turning to user experience: How is the performance experienced by a user arriving into a "high load" period different from that of a user arriving into a "low load" period, or simply a random user. Are there stochastic relations between these? In this paper, we provide the first answers to these fundamental questions.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {203--215},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1140277.1140301},
 doi = {http://doi.acm.org/10.1145/1140277.1140301},
 acmid = {1140301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MAP, MMPP, Ross's conjecture, fluctuating load, non-stationary arrivals/service, stochastic ordering},
} 

@article{Gupta:2006:FCQ:1140103.1140301,
 author = {Gupta, Varun and Harchol-Balter, Mor and Wolf, Alan Scheller and Yechiali, Uri},
 title = {Fundamental characteristics of queues with fluctuating load},
 abstract = {Systems whose arrival or service rates fluctuate over time are very common, but are still not well understood analytically. Stationary formulas are poor predictors of systems with fluctuating load. When the arrival and service processes fluctuate in a Markovian manner, computational methods, such as Matrix-analytic and spectral analysis, have been instrumental in the numerical evaluation of quantities like mean response time. However, such computational tools provide only limited insight into the functional behavior</i> of the system with respect to its primitive input parameters: the arrival rates, service rates, and rate of fluctuation.For example, the shape of the function that maps rate of fluctuation to mean response time is not well understood, even for an M/M/1 system. Is this function increasing, decreasing, monotonic? How is its shape affected by the primitive input parameters? Is there a simple closed-form approximation for the shape of this curve? Turning to user experience: How is the performance experienced by a user arriving into a "high load" period different from that of a user arriving into a "low load" period, or simply a random user. Are there stochastic relations between these? In this paper, we provide the first answers to these fundamental questions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {203--215},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1140103.1140301},
 doi = {http://doi.acm.org/10.1145/1140103.1140301},
 acmid = {1140301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MAP, MMPP, Ross's conjecture, fluctuating load, non-stationary arrivals/service, stochastic ordering},
} 

@inproceedings{Narayanasamy:2006:ALO:1140277.1140303,
 author = {Narayanasamy, Satish and Pereira, Cristiano and Patil, Harish and Cohn, Robert and Calder, Brad},
 title = {Automatic logging of operating system effects to guide application-level architecture simulation},
 abstract = {Modern architecture research relies heavily on application-level detailed pipeline simulation. A time consuming part of building a simulator is correctly emulating the operating system effects, which is required even if the goal is to simulate just the application code, in order to achieve functional correctness of the application's execution. Existing application-level simulators require manually hand coding the emulation of each and every possible system effect (e.g., system call, interrupt, DMA transfer) that can impact the application's execution. Developing such an emulator for a given operating system is a tedious exercise, and it can also be costly to maintain it to support newer versions of that operating system. Furthermore, porting the emulator to a completely different operating system might involve building it all together from scratch.In this paper, we describe a tool that can automatically log operating system effects to guide architecture simulation of application code. The benefits of our approach are: (a) we do not have to build or maintain any infrastructure for emulating the operating system effects, (b) we can support simulation of more complex applications on our application-level simulator, including those applications that use asynchronous interrupts, DMA transfers, etc., and (c) using the system effects logs collected by our tool, we can deterministically re-execute the application to guide architecture simulation that has reproducible results.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {216--227},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140303},
 doi = {http://doi.acm.org/10.1145/1140277.1140303},
 acmid = {1140303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture simulation, checkpoints, emulating system calls},
} 

@article{Narayanasamy:2006:ALO:1140103.1140303,
 author = {Narayanasamy, Satish and Pereira, Cristiano and Patil, Harish and Cohn, Robert and Calder, Brad},
 title = {Automatic logging of operating system effects to guide application-level architecture simulation},
 abstract = {Modern architecture research relies heavily on application-level detailed pipeline simulation. A time consuming part of building a simulator is correctly emulating the operating system effects, which is required even if the goal is to simulate just the application code, in order to achieve functional correctness of the application's execution. Existing application-level simulators require manually hand coding the emulation of each and every possible system effect (e.g., system call, interrupt, DMA transfer) that can impact the application's execution. Developing such an emulator for a given operating system is a tedious exercise, and it can also be costly to maintain it to support newer versions of that operating system. Furthermore, porting the emulator to a completely different operating system might involve building it all together from scratch.In this paper, we describe a tool that can automatically log operating system effects to guide architecture simulation of application code. The benefits of our approach are: (a) we do not have to build or maintain any infrastructure for emulating the operating system effects, (b) we can support simulation of more complex applications on our application-level simulator, including those applications that use asynchronous interrupts, DMA transfers, etc., and (c) using the system effects logs collected by our tool, we can deterministically re-execute the application to guide architecture simulation that has reproducible results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {216--227},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140303},
 doi = {http://doi.acm.org/10.1145/1140103.1140303},
 acmid = {1140303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture simulation, checkpoints, emulating system calls},
} 

@article{Guo:2006:AMC:1140103.1140304,
 author = {Guo, Fei and Solihin, Yan},
 title = {An analytical model for cache replacement policy performance},
 abstract = {Due to the increasing gap between CPU and memory speed, cache performance plays an increasingly critical role in determining the overall performance of microprocessor systems. One of the important factors that a affect cache performance is the cache replacement policy. Despite the importance, current analytical cache performance models ignore the impact of cache replacement policies on cache performance. To the best of our knowledge, this paper is the first to propose an analytical model which predicts the performance of cache replacement policies. The input to our model is a simple circular sequence profiling of each application, which requires very little storage overhead. The output of the model is the predicted miss rates of an application under different replacement policies. The model is based on probability theory and utilizes Markov processes to compute each cache access' miss probability. The model realistic assumptions and relies solely on the statistical properties of the application, without relying on heuristics or rules of thumbs. The model's run time is less than 0.1 seconds, much lower than that of trace simulations. We validate the model by comparing the predicted miss rates of seventeen Spec2000 and NAS benchmark applications against miss rates obtained by detailed execution-driven simulations, across a range of different cache sizes, associativities, and four replacement policies, and show that the model is very accurate. The model's average prediction error is 1.41\%,and there are only 14 out of 952 validation points in which the prediction errors are larger than 10\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140304},
 doi = {http://doi.acm.org/10.1145/1140103.1140304},
 acmid = {1140304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical model, cache performance, replacement policy},
} 

@inproceedings{Guo:2006:AMC:1140277.1140304,
 author = {Guo, Fei and Solihin, Yan},
 title = {An analytical model for cache replacement policy performance},
 abstract = {Due to the increasing gap between CPU and memory speed, cache performance plays an increasingly critical role in determining the overall performance of microprocessor systems. One of the important factors that a affect cache performance is the cache replacement policy. Despite the importance, current analytical cache performance models ignore the impact of cache replacement policies on cache performance. To the best of our knowledge, this paper is the first to propose an analytical model which predicts the performance of cache replacement policies. The input to our model is a simple circular sequence profiling of each application, which requires very little storage overhead. The output of the model is the predicted miss rates of an application under different replacement policies. The model is based on probability theory and utilizes Markov processes to compute each cache access' miss probability. The model realistic assumptions and relies solely on the statistical properties of the application, without relying on heuristics or rules of thumbs. The model's run time is less than 0.1 seconds, much lower than that of trace simulations. We validate the model by comparing the predicted miss rates of seventeen Spec2000 and NAS benchmark applications against miss rates obtained by detailed execution-driven simulations, across a range of different cache sizes, associativities, and four replacement policies, and show that the model is very accurate. The model's average prediction error is 1.41\%,and there are only 14 out of 952 validation points in which the prediction errors are larger than 10\%.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140304},
 doi = {http://doi.acm.org/10.1145/1140277.1140304},
 acmid = {1140304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical model, cache performance, replacement policy},
} 

@article{Olshefski:2006:UMC:1140103.1140305,
 author = {Olshefski, David and Nieh, Jason},
 title = {Understanding the management of client perceived response time},
 abstract = {Understanding and managing the response time of web services is of key importance as dependence on the World Wide Web continues to grow. We present Remote Latency-based Management</i> (RLM), a novel server-side approach for managing pageview response times as perceived by remote clients, in real-time. RLM passively monitors server-side network traffic, accurately tracks the progress of page downloads and their response times in real-time, and dynamically adapts connection setup behavior and web page content as needed to meet response time goals. To manage client perceived pageview response times, RLM builds a novel event node model to guide the use of several techniques for manipulating the packet traffic in and out of a web server complex, including fast SYN and SYN/ACK retransmission, and embedded object removal and rewrite. RLM operates as a stand-alone appliance that simply sits in front of a web server complex, without any changes to existing web clients, servers, or applications. We have implemented RLM on an inexpensive, commodity, Linux-based PC and present experimental results that demonstrate its effectiveness in managing client perceived pageview response times on transactional e-commerce web workloads.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {240--251},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140305},
 doi = {http://doi.acm.org/10.1145/1140103.1140305},
 acmid = {1140305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS, admission control, client perceived response time, web server performance},
} 

@inproceedings{Olshefski:2006:UMC:1140277.1140305,
 author = {Olshefski, David and Nieh, Jason},
 title = {Understanding the management of client perceived response time},
 abstract = {Understanding and managing the response time of web services is of key importance as dependence on the World Wide Web continues to grow. We present Remote Latency-based Management</i> (RLM), a novel server-side approach for managing pageview response times as perceived by remote clients, in real-time. RLM passively monitors server-side network traffic, accurately tracks the progress of page downloads and their response times in real-time, and dynamically adapts connection setup behavior and web page content as needed to meet response time goals. To manage client perceived pageview response times, RLM builds a novel event node model to guide the use of several techniques for manipulating the packet traffic in and out of a web server complex, including fast SYN and SYN/ACK retransmission, and embedded object removal and rewrite. RLM operates as a stand-alone appliance that simply sits in front of a web server complex, without any changes to existing web clients, servers, or applications. We have implemented RLM on an inexpensive, commodity, Linux-based PC and present experimental results that demonstrate its effectiveness in managing client perceived pageview response times on transactional e-commerce web workloads.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {240--251},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140305},
 doi = {http://doi.acm.org/10.1145/1140277.1140305},
 acmid = {1140305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS, admission control, client perceived response time, web server performance},
} 

@article{Thorup:2006:CIP:1140103.1140307,
 author = {Thorup, Mikkel},
 title = {Confidence intervals for priority sampling},
 abstract = {With a priority sample from a set of weighted items, we can provide an unbiased estimate of the total weight of any subset. The strength of priority sampling is that it gives the best possible estimate variance on any set of input weights.For a concrete subset, however, the variance on the estimate of its weight depends strongly on the total set of weights and the distribution of the subset in this set. The variance is, for example, much smaller if weights are heavy tailed.In this paper we show how to generate a confidence interval directly from a priority sample, thus complementing the weight estimates with concrete lower and upper bounds. In particularly we will tell how heavy subsets can likely be hidden when the priority estimate for a subset is zero.Our confidence intervals for priority sampling are evaluated on real and synthetic data and compared with confidence intervals obtained with uniform sampling, weighted sampling with replacement, and threshold sampling.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140307},
 doi = {http://doi.acm.org/10.1145/1140103.1140307},
 acmid = {1140307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {confidence intervals, sampling},
} 

@inproceedings{Thorup:2006:CIP:1140277.1140307,
 author = {Thorup, Mikkel},
 title = {Confidence intervals for priority sampling},
 abstract = {With a priority sample from a set of weighted items, we can provide an unbiased estimate of the total weight of any subset. The strength of priority sampling is that it gives the best possible estimate variance on any set of input weights.For a concrete subset, however, the variance on the estimate of its weight depends strongly on the total set of weights and the distribution of the subset in this set. The variance is, for example, much smaller if weights are heavy tailed.In this paper we show how to generate a confidence interval directly from a priority sample, thus complementing the weight estimates with concrete lower and upper bounds. In particularly we will tell how heavy subsets can likely be hidden when the priority estimate for a subset is zero.Our confidence intervals for priority sampling are evaluated on real and synthetic data and compared with confidence intervals obtained with uniform sampling, weighted sampling with replacement, and threshold sampling.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140307},
 doi = {http://doi.acm.org/10.1145/1140277.1140307},
 acmid = {1140307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {confidence intervals, sampling},
} 

@article{Osogami:2006:FPB:1140103.1140308,
 author = {Osogami, Takayuki and Itoko, Toshinari},
 title = {Finding probably better system configurations quickly},
 abstract = {The performance of computer and communication systems can in theory be optimized by iteratively finding better system configurations. However, a bottleneck is the time required in simulations/experiments for finding a better system configuration in each iteration. We propose algorithms that quickly find a system configuration that is probably better than the "standard" system configuration, where the performance of a given system configuration is estimated via simulations or experiments. We prove that our algorithms make correct decisions with high probability, and various heuristics to reduce the total simulation time are proposed. Numerical experiments show the effectiveness of the proposed algorithms, and this leads to several guidelines for designing efficient and reliable optimization procedures for the performance of computer and communication systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {264--275},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140308},
 doi = {http://doi.acm.org/10.1145/1140103.1140308},
 acmid = {1140308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {local search, performance optimization, ranking and selection, screening, simulation},
} 

@inproceedings{Osogami:2006:FPB:1140277.1140308,
 author = {Osogami, Takayuki and Itoko, Toshinari},
 title = {Finding probably better system configurations quickly},
 abstract = {The performance of computer and communication systems can in theory be optimized by iteratively finding better system configurations. However, a bottleneck is the time required in simulations/experiments for finding a better system configuration in each iteration. We propose algorithms that quickly find a system configuration that is probably better than the "standard" system configuration, where the performance of a given system configuration is estimated via simulations or experiments. We prove that our algorithms make correct decisions with high probability, and various heuristics to reduce the total simulation time are proposed. Numerical experiments show the effectiveness of the proposed algorithms, and this leads to several guidelines for designing efficient and reliable optimization procedures for the performance of computer and communication systems.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {264--275},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140308},
 doi = {http://doi.acm.org/10.1145/1140277.1140308},
 acmid = {1140308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {local search, performance optimization, ranking and selection, screening, simulation},
} 

@article{Bonald:2006:EMN:1140103.1140309,
 author = {Bonald, Thomas},
 title = {The Erlang model with non-poisson call arrivals},
 abstract = {The Erlang formula is known to be insensitive to the holding time distribution beyond the mean. While calls are generally assumed to arrive as a Poisson process, we prove that it is in fact sufficient that users generate sessions</i> according to a Poisson process, each session being composed of a random, finite number of calls and idle periods. A key role is played by the retrial behavior in case of call blocking. We illustrate the results by a number of examples.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {276--286},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1140103.1140309},
 doi = {http://doi.acm.org/10.1145/1140103.1140309},
 acmid = {1140309},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Erlang formula, insensitivity, loss networks},
} 

@inproceedings{Bonald:2006:EMN:1140277.1140309,
 author = {Bonald, Thomas},
 title = {The Erlang model with non-poisson call arrivals},
 abstract = {The Erlang formula is known to be insensitive to the holding time distribution beyond the mean. While calls are generally assumed to arrive as a Poisson process, we prove that it is in fact sufficient that users generate sessions</i> according to a Poisson process, each session being composed of a random, finite number of calls and idle periods. A key role is played by the retrial behavior in case of call blocking. We illustrate the results by a number of examples.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {276--286},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1140277.1140309},
 doi = {http://doi.acm.org/10.1145/1140277.1140309},
 acmid = {1140309},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Erlang formula, insensitivity, loss networks},
} 

@article{Fidler:2006:WDS:1140103.1140310,
 author = {Fidler, Markus and Schmitt, Jens B.},
 title = {On the way to a distributed systems calculus: an end-to-end network calculus with data scaling},
 abstract = {Network calculus is a min-plus system theory which facilitates the efficient derivation of performance bounds for networks of queues. It has successfully been applied to provide end-to-end quality of service guarantees for integrated and differentiated services networks. Yet, a true end-to-end analysis including the various components of end systems as well as taking into account mid-boxes like firewalls, proxies, or media gateways has not been accomplished so far. The particular challenge posed by such systems are transformation processes, like data processing, compression, encoding, and decoding, which may alter data arrivals drastically. The heterogeneity, which is reflected in the granularity of operation, for example multimedia applications process video frames which, however, are represented by packets in the network, complicates the analysis further. To this end this paper evolves a concise network calculus with scaling functions, which allow modelling a wide variety of transformation processes. Combined with the concept of packetizer this theory enables a true end-to-end analysis of distributed systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {287--298},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140310},
 doi = {http://doi.acm.org/10.1145/1140103.1140310},
 acmid = {1140310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network calculus, packetizers, scaling functions},
} 

@inproceedings{Fidler:2006:WDS:1140277.1140310,
 author = {Fidler, Markus and Schmitt, Jens B.},
 title = {On the way to a distributed systems calculus: an end-to-end network calculus with data scaling},
 abstract = {Network calculus is a min-plus system theory which facilitates the efficient derivation of performance bounds for networks of queues. It has successfully been applied to provide end-to-end quality of service guarantees for integrated and differentiated services networks. Yet, a true end-to-end analysis including the various components of end systems as well as taking into account mid-boxes like firewalls, proxies, or media gateways has not been accomplished so far. The particular challenge posed by such systems are transformation processes, like data processing, compression, encoding, and decoding, which may alter data arrivals drastically. The heterogeneity, which is reflected in the granularity of operation, for example multimedia applications process video frames which, however, are represented by packets in the network, complicates the analysis further. To this end this paper evolves a concise network calculus with scaling functions, which allow modelling a wide variety of transformation processes. Combined with the concept of packetizer this theory enables a true end-to-end analysis of distributed systems.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {287--298},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140310},
 doi = {http://doi.acm.org/10.1145/1140277.1140310},
 acmid = {1140310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network calculus, packetizers, scaling functions},
} 

@inproceedings{Peserico:2006:RNC:1140277.1140312,
 author = {Peserico, Enoch and Rudolph, Larry},
 title = {Robust network connectivity: when it's the big picture that matters},
 abstract = {This work analyzes the connectivity of large diameter networks where every link has an independent probability p of failure. We give a (relatively simple) topological condition that guarantees good connectivity between regions of such a network. Good connectivity means that the regions are connected by nearly as many disjoint, fault-free paths as there are when the entire network is fault-free. The topological condition is satisfied in many cases of practical interest, even when two regions are at a distance much larger than the expected "distance between faults", 1/p. We extend this result to networks with failures on nodes, as well as geometric radio networks with random distribution of nodes in a deployment area of a given topography.A rigorous formalization of the intuitive notion of "hole" in a (not necessarily planar) graph is at the heart of our result and our proof. Holes, in the presence of faults, degrade connectivity in the region "around" them to a distance that grows with the size of the hole and the density of faults. Thus, to guarantee good connectivity between two regions even in the presence of faults, the intervening network should not only sport multiple paths, but also not too many large holes.Our result essentially characterizes networks where connectivity depends on the "big picture" structure of the network, and not on the local "noise" caused by faulty or imprecisely positioned nodes and links.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140312},
 doi = {http://doi.acm.org/10.1145/1140277.1140312},
 acmid = {1140312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ad hoc, connectivity, fault, network, percolation, random, resilient, topology},
} 

@article{Peserico:2006:RNC:1140103.1140312,
 author = {Peserico, Enoch and Rudolph, Larry},
 title = {Robust network connectivity: when it's the big picture that matters},
 abstract = {This work analyzes the connectivity of large diameter networks where every link has an independent probability p of failure. We give a (relatively simple) topological condition that guarantees good connectivity between regions of such a network. Good connectivity means that the regions are connected by nearly as many disjoint, fault-free paths as there are when the entire network is fault-free. The topological condition is satisfied in many cases of practical interest, even when two regions are at a distance much larger than the expected "distance between faults", 1/p. We extend this result to networks with failures on nodes, as well as geometric radio networks with random distribution of nodes in a deployment area of a given topography.A rigorous formalization of the intuitive notion of "hole" in a (not necessarily planar) graph is at the heart of our result and our proof. Holes, in the presence of faults, degrade connectivity in the region "around" them to a distance that grows with the size of the hole and the density of faults. Thus, to guarantee good connectivity between two regions even in the presence of faults, the intervening network should not only sport multiple paths, but also not too many large holes.Our result essentially characterizes networks where connectivity depends on the "big picture" structure of the network, and not on the local "noise" caused by faulty or imprecisely positioned nodes and links.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {299--310},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140312},
 doi = {http://doi.acm.org/10.1145/1140103.1140312},
 acmid = {1140312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ad hoc, connectivity, fault, network, percolation, random, resilient, topology},
} 

@article{Dong:2006:PCT:1140103.1140313,
 author = {Dong, Qunfeng and Banerjee, Suman and Wang, Jia and Agrawal, Dheeraj and Shukla, Ashutosh},
 title = {Packet classifiers in ternary CAMs can be smaller},
 abstract = {Serving as the core component in many packet forwarding, differentiating and filtering schemes, packet classification continues to grow its importance in today's IP networks. Currently, most vendors use Ternary CAMs (TCAMs) for packet classification. TCAMs usually use brute-force parallel hardware to simultaneously check for all rules. One of the fundamental problems of TCAMs is that TCAMs suffer from range specifications because rules with range specifications need to be translated into multiple TCAM entries. Hence, the cost of packet classification will increase substantially as the number of TCAM entries grows. As a result, network operators hesitate to configure packet classifiers using range specifications. In this paper, we optimize packet classifier configurations by identifying semantically equivalent rule sets that lead to reduced number of TCAM entries when represented in hardware. In particular, we develop a number of effective techniques, which include: trimming rules, expanding rules, merging rules, and adding rules. Compared with previously proposed techniques which typically require modifications to the packet processor hardware, our scheme does not require any hardware modification, which is highly preferred by ISPs. Moreover, our scheme is complementary to previous techniques in that those techniques can be applied on the rule sets optimized by our scheme. We evaluate the effectiveness and potential of the proposed techniques using extensive experiments based on both real packet classifiers managed by a large tier-1 ISP and synthetic data generated randomly. We observe significant reduction on the number of TCAM entries that are needed to represent the optimized packet classifier configurations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140313},
 doi = {http://doi.acm.org/10.1145/1140103.1140313},
 acmid = {1140313},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, semantic equivalence, ternary CAM},
} 

@inproceedings{Dong:2006:PCT:1140277.1140313,
 author = {Dong, Qunfeng and Banerjee, Suman and Wang, Jia and Agrawal, Dheeraj and Shukla, Ashutosh},
 title = {Packet classifiers in ternary CAMs can be smaller},
 abstract = {Serving as the core component in many packet forwarding, differentiating and filtering schemes, packet classification continues to grow its importance in today's IP networks. Currently, most vendors use Ternary CAMs (TCAMs) for packet classification. TCAMs usually use brute-force parallel hardware to simultaneously check for all rules. One of the fundamental problems of TCAMs is that TCAMs suffer from range specifications because rules with range specifications need to be translated into multiple TCAM entries. Hence, the cost of packet classification will increase substantially as the number of TCAM entries grows. As a result, network operators hesitate to configure packet classifiers using range specifications. In this paper, we optimize packet classifier configurations by identifying semantically equivalent rule sets that lead to reduced number of TCAM entries when represented in hardware. In particular, we develop a number of effective techniques, which include: trimming rules, expanding rules, merging rules, and adding rules. Compared with previously proposed techniques which typically require modifications to the packet processor hardware, our scheme does not require any hardware modification, which is highly preferred by ISPs. Moreover, our scheme is complementary to previous techniques in that those techniques can be applied on the rule sets optimized by our scheme. We evaluate the effectiveness and potential of the proposed techniques using extensive experiments based on both real packet classifiers managed by a large tier-1 ISP and synthetic data generated randomly. We observe significant reduction on the number of TCAM entries that are needed to represent the optimized packet classifier configurations.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {311--322},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140313},
 doi = {http://doi.acm.org/10.1145/1140277.1140313},
 acmid = {1140313},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {packet classification, semantic equivalence, ternary CAM},
} 

@article{Zhao:2006:DNS:1140103.1140314,
 author = {Zhao, Qi and Xu, Jun and Liu, Zhen},
 title = {Design of a novel statistics counter architecture with optimal space and time efficiency},
 abstract = {The problem of how to efficiently maintain a large number (say millions) of statistics counters that need to be incremented at very high speed has received considerable research attention recently. This problem arises in a variety of router management algorithms and data streaming algorithms, where a large array of counters is used to track various network statistics and to implement various counting sketches respectively. While fitting these counters entirely in SRAM meets the access speed requirement, a large amount of SRAM may be needed with a typical counter size of 32 or 64 bits, and hence the high cost. Solutions proposed in recent works have used hybrid architectures where small counters in SRAM are incremented at high speed, and occasionally written back ("flushed") to larger counters in DRAM. Previous solutions have used complex schedulers with tree-like or heap data structures to pick which counters in SRAM are about to overflow, and flush them to the corresponding DRAM counters.In this work, we present a novel hybrid SRAM/DRAM counter architecture that consumes much less SRAM and has a much simpler design of the scheduler than previous approaches. We show, in fact, that our design is optimal in the sense that for a given speed difference between SRAM and DRAM, our design uses the theoretically minimum number of bits per counter in SRAM. Our design uses a small write-back buffer (in SRAM) that stores indices of the overflowed counters (to be flushed to DRAM) and an extremely simple randomized algorithm to statistically guarantee that SRAM counters do not overflow in bursts large enough to fill up the write-back buffer even in the worst case. The statistical guarantee of the algorithm is proven using a combination of worst case analysis for characterizing the worst case counter increment sequence and a new tail bound theorem for bounding the probability of filling up the write-back buffer. Experiments with real Internet traffic traces show that the buffer size required in practice is significantly smaller than needed in the worst case.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {323--334},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140314},
 doi = {http://doi.acm.org/10.1145/1140103.1140314},
 acmid = {1140314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, router, statistics counter},
} 

@inproceedings{Zhao:2006:DNS:1140277.1140314,
 author = {Zhao, Qi and Xu, Jun and Liu, Zhen},
 title = {Design of a novel statistics counter architecture with optimal space and time efficiency},
 abstract = {The problem of how to efficiently maintain a large number (say millions) of statistics counters that need to be incremented at very high speed has received considerable research attention recently. This problem arises in a variety of router management algorithms and data streaming algorithms, where a large array of counters is used to track various network statistics and to implement various counting sketches respectively. While fitting these counters entirely in SRAM meets the access speed requirement, a large amount of SRAM may be needed with a typical counter size of 32 or 64 bits, and hence the high cost. Solutions proposed in recent works have used hybrid architectures where small counters in SRAM are incremented at high speed, and occasionally written back ("flushed") to larger counters in DRAM. Previous solutions have used complex schedulers with tree-like or heap data structures to pick which counters in SRAM are about to overflow, and flush them to the corresponding DRAM counters.In this work, we present a novel hybrid SRAM/DRAM counter architecture that consumes much less SRAM and has a much simpler design of the scheduler than previous approaches. We show, in fact, that our design is optimal in the sense that for a given speed difference between SRAM and DRAM, our design uses the theoretically minimum number of bits per counter in SRAM. Our design uses a small write-back buffer (in SRAM) that stores indices of the overflowed counters (to be flushed to DRAM) and an extremely simple randomized algorithm to statistically guarantee that SRAM counters do not overflow in bursts large enough to fill up the write-back buffer even in the worst case. The statistical guarantee of the algorithm is proven using a combination of worst case analysis for characterizing the worst case counter increment sequence and a new tail bound theorem for bounding the probability of filling up the write-back buffer. Experiments with real Internet traffic traces show that the buffer size required in practice is significantly smaller than needed in the worst case.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {323--334},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140314},
 doi = {http://doi.acm.org/10.1145/1140277.1140314},
 acmid = {1140314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, router, statistics counter},
} 

@inproceedings{Kumar:2006:FMP:1140277.1140316,
 author = {Kumar, Rakesh and Yao, David D. and Bagchi, Amitabha and Ross, Keith W. and Rubenstein, Dan},
 title = {Fluid modeling of pollution proliferation in P2P networks},
 abstract = {P2P systems are highly vulnerable to pollution attacks in which attackers inject multiple versions of corrupted content into the system, which is then further proliferated by unsuspecting users. However, to our knowledge, there are no closed-form solutions that describe this phenomenon, nor are there models that describe how the injection of multiple versions of corrupted content impacts a clients' ability to receive a valid copy. In this paper we develop a suite of fluid models that model pollution proliferation in P2P systems. These fluid models lead to systems of non-linear differential equations. We obtain closed-form solutions for the differential equations; for the remaining models, we efficiently solve the differential equations numerically. The models capture a variety of user behaviors, including propensity for popular versions, abandonment after repeated failure to obtain a good version, freeloading, and local version blacklisting. Our analysis reveals intelligent strategies for attackers as well as strategies for clients seeking to recover non-polluted content within large-scale P2P networks.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {335--346},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140316},
 doi = {http://doi.acm.org/10.1145/1140277.1140316},
 acmid = {1140316},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain, P2P, fluid model, pollution attack},
} 

@article{Kumar:2006:FMP:1140103.1140316,
 author = {Kumar, Rakesh and Yao, David D. and Bagchi, Amitabha and Ross, Keith W. and Rubenstein, Dan},
 title = {Fluid modeling of pollution proliferation in P2P networks},
 abstract = {P2P systems are highly vulnerable to pollution attacks in which attackers inject multiple versions of corrupted content into the system, which is then further proliferated by unsuspecting users. However, to our knowledge, there are no closed-form solutions that describe this phenomenon, nor are there models that describe how the injection of multiple versions of corrupted content impacts a clients' ability to receive a valid copy. In this paper we develop a suite of fluid models that model pollution proliferation in P2P systems. These fluid models lead to systems of non-linear differential equations. We obtain closed-form solutions for the differential equations; for the remaining models, we efficiently solve the differential equations numerically. The models capture a variety of user behaviors, including propensity for popular versions, abandonment after repeated failure to obtain a good version, freeloading, and local version blacklisting. Our analysis reveals intelligent strategies for attackers as well as strategies for clients seeking to recover non-polluted content within large-scale P2P networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {335--346},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140316},
 doi = {http://doi.acm.org/10.1145/1140103.1140316},
 acmid = {1140316},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain, P2P, fluid model, pollution attack},
} 

@article{Li:2006:FSS:1140103.1140317,
 author = {Li, Kang and Zhong, Zhenyu},
 title = {Fast statistical spam filter by approximate classifications},
 abstract = {Statistical-based Bayesian filters have become a popular and important defense against spam. However, despite their effectiveness, their greater processing overhead can prevent them from scaling well for enterprise-level mail servers. For example, the dictionary lookups that are characteristic of this approach are limited by the memory access rate, therefore relatively insensitive to increases in CPU speed. We address this scaling issue by proposing an acceleration technique that speeds up Bayesian filters based on approximate classification. The approximation uses two methods: hash-based lookup and lossy encoding. Lookup approximation is based on the popular Bloom filter data structure with an extension to support value retrieval. Lossy encoding is used to further compress the data structure. While both methods introduce additional errors to a strict Bayesian approach, we show how the errors can be both minimized and biased toward a false negative classification.We demonstrate a 6x speedup over two well-known spam filters (bogofilter and qsf) while achieving an identical false positive rate and similar false negative rate to the original filters.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140103.1140317},
 doi = {http://doi.acm.org/10.1145/1140103.1140317},
 acmid = {1140317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SPAM, approximation, bayesian filter, bloom filter},
} 

@inproceedings{Li:2006:FSS:1140277.1140317,
 author = {Li, Kang and Zhong, Zhenyu},
 title = {Fast statistical spam filter by approximate classifications},
 abstract = {Statistical-based Bayesian filters have become a popular and important defense against spam. However, despite their effectiveness, their greater processing overhead can prevent them from scaling well for enterprise-level mail servers. For example, the dictionary lookups that are characteristic of this approach are limited by the memory access rate, therefore relatively insensitive to increases in CPU speed. We address this scaling issue by proposing an acceleration technique that speeds up Bayesian filters based on approximate classification. The approximation uses two methods: hash-based lookup and lossy encoding. Lookup approximation is based on the popular Bloom filter data structure with an extension to support value retrieval. Lossy encoding is used to further compress the data structure. While both methods introduce additional errors to a strict Bayesian approach, we show how the errors can be both minimized and biased toward a false negative classification.We demonstrate a 6x speedup over two well-known spam filters (bogofilter and qsf) while achieving an identical false positive rate and similar false negative rate to the original filters.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {347--358},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1140277.1140317},
 doi = {http://doi.acm.org/10.1145/1140277.1140317},
 acmid = {1140317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SPAM, approximation, bayesian filter, bloom filter},
} 

@article{Kola:2006:QAB:1140103.1140319,
 author = {Kola, George and Vernon, Mary K.},
 title = {QuickProbe: available bandwidth estimation in two roundtrips},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {359--360},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140319},
 doi = {http://doi.acm.org/10.1145/1140103.1140319},
 acmid = {1140319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {available bandwidth estimation},
} 

@inproceedings{Kola:2006:QAB:1140277.1140319,
 author = {Kola, George and Vernon, Mary K.},
 title = {QuickProbe: available bandwidth estimation in two roundtrips},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {359--360},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140319},
 doi = {http://doi.acm.org/10.1145/1140277.1140319},
 acmid = {1140319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {available bandwidth estimation},
} 

@inproceedings{Kaushik:2006:FTA:1140277.1140320,
 author = {Kaushik, Neena R. and Figueira, Silvia M. and Chiappari, Stephen A.},
 title = {Flexible time-windows for advance reservation in LambdaGrids},
 abstract = {Advance-reservation requests are an essential feature of LambdaGrids, where resources may need to be co-allocated at pre-determined times. In this paper, we discuss unconstrained advance reservations, which use flexible time-windows to lower blocking probability and, consequently, increase resource utilization. We claim and show using simulations that the minimum window size, which theoretically brings the blocking probability to 0, in a first-come-first-served advance reservation model without time-slots, equals the waiting time in a queue-based on-demand model. We also show, with simulations, the window sizes, which bring the blocking probability to its minimum, for an advance reservation model with time-slots.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {361--362},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140320},
 doi = {http://doi.acm.org/10.1145/1140277.1140320},
 acmid = {1140320},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LambdaGrids, advance reservation, flexible time-windows, scheduling},
} 

@article{Kaushik:2006:FTA:1140103.1140320,
 author = {Kaushik, Neena R. and Figueira, Silvia M. and Chiappari, Stephen A.},
 title = {Flexible time-windows for advance reservation in LambdaGrids},
 abstract = {Advance-reservation requests are an essential feature of LambdaGrids, where resources may need to be co-allocated at pre-determined times. In this paper, we discuss unconstrained advance reservations, which use flexible time-windows to lower blocking probability and, consequently, increase resource utilization. We claim and show using simulations that the minimum window size, which theoretically brings the blocking probability to 0, in a first-come-first-served advance reservation model without time-slots, equals the waiting time in a queue-based on-demand model. We also show, with simulations, the window sizes, which bring the blocking probability to its minimum, for an advance reservation model with time-slots.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {361--362},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140320},
 doi = {http://doi.acm.org/10.1145/1140103.1140320},
 acmid = {1140320},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LambdaGrids, advance reservation, flexible time-windows, scheduling},
} 

@inproceedings{Verbowski:2006:APS:1140277.1140321,
 author = {Verbowski, Chad and Kiciman, Emre and Daniels, Brad and Wang, Yi-Min and Roussev, Roussi and Lu, Shan and Lee, Juhan},
 title = {Analyzing persistent state interactions to improve state management},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {363--364},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140321},
 doi = {http://doi.acm.org/10.1145/1140277.1140321},
 acmid = {1140321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {file, persistent state, registry, state management, system management, trace},
} 

@article{Verbowski:2006:APS:1140103.1140321,
 author = {Verbowski, Chad and Kiciman, Emre and Daniels, Brad and Wang, Yi-Min and Roussev, Roussi and Lu, Shan and Lee, Juhan},
 title = {Analyzing persistent state interactions to improve state management},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {363--364},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140321},
 doi = {http://doi.acm.org/10.1145/1140103.1140321},
 acmid = {1140321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {file, persistent state, registry, state management, system management, trace},
} 

@inproceedings{Verloop:2006:DSB:1140277.1140322,
 author = {Verloop, Maaike and N\'{u}\~{n}ez-Queija, Rudesindo and Borst, Sem},
 title = {Delay-optimal scheduling in bandwidth-sharing networks},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140322},
 doi = {http://doi.acm.org/10.1145/1140277.1140322},
 acmid = {1140322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alpha-fair strategies, bandwidth-sharing networks, delay optimization},
} 

@article{Verloop:2006:DSB:1140103.1140322,
 author = {Verloop, Maaike and N\'{u}\~{n}ez-Queija, Rudesindo and Borst, Sem},
 title = {Delay-optimal scheduling in bandwidth-sharing networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140322},
 doi = {http://doi.acm.org/10.1145/1140103.1140322},
 acmid = {1140322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alpha-fair strategies, bandwidth-sharing networks, delay optimization},
} 

@inproceedings{Menth:2006:TPP:1140277.1140323,
 author = {Menth, Michael and Henjes, Robert and Zepfel, Christian and Gehrsitz, Sebastian},
 title = {Throughput performance of popular JMS servers},
 abstract = {The Java Messaging Service (JMS) facilitates communication among distributed software components according to the publish/subscribe principle. If the subscribers install filter rules on the JMS server, JMS can be used as a message routing platform, but it is not clear whether its message throughput is sufficiently high to support large-scale systems. In this paper, we investigate the capacity of three high performance JMS server implementations: FioranoMQ, SunMQ, and WebsphereMQ. In contrast to other studies, we focus on the message throughput in the presence of filters and show that filtering reduces the performance significantly. We present models for the message processing time of each server and validate them by measurement.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {367--368},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140323},
 doi = {http://doi.acm.org/10.1145/1140277.1140323},
 acmid = {1140323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java messaging service, publish/subscribe, server performance},
} 

@article{Menth:2006:TPP:1140103.1140323,
 author = {Menth, Michael and Henjes, Robert and Zepfel, Christian and Gehrsitz, Sebastian},
 title = {Throughput performance of popular JMS servers},
 abstract = {The Java Messaging Service (JMS) facilitates communication among distributed software components according to the publish/subscribe principle. If the subscribers install filter rules on the JMS server, JMS can be used as a message routing platform, but it is not clear whether its message throughput is sufficiently high to support large-scale systems. In this paper, we investigate the capacity of three high performance JMS server implementations: FioranoMQ, SunMQ, and WebsphereMQ. In contrast to other studies, we focus on the message throughput in the presence of filters and show that filtering reduces the performance significantly. We present models for the message processing time of each server and validate them by measurement.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {367--368},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140323},
 doi = {http://doi.acm.org/10.1145/1140103.1140323},
 acmid = {1140323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java messaging service, publish/subscribe, server performance},
} 

@article{Garg:2006:OHR:1140103.1140324,
 author = {Garg, Rahul and Sabharwal, Yogish},
 title = {Optimizing the HPCC randomaccess benchmark on blue Gene/L Supercomputer},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {369--370},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140324},
 doi = {http://doi.acm.org/10.1145/1140103.1140324},
 acmid = {1140324},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarks, high performance computing, randomaccess},
} 

@inproceedings{Garg:2006:OHR:1140277.1140324,
 author = {Garg, Rahul and Sabharwal, Yogish},
 title = {Optimizing the HPCC randomaccess benchmark on blue Gene/L Supercomputer},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {369--370},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140324},
 doi = {http://doi.acm.org/10.1145/1140277.1140324},
 acmid = {1140324},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarks, high performance computing, randomaccess},
} 

@inproceedings{Piotrowski:2006:PSS:1140277.1140325,
 author = {Piotrowski, Tadeusz and Banerjee, Suman and Bhatnagar, Sudeept and Ganguly, Samrat and Izmailov, Rauf},
 title = {Peer-to-peer streaming of stored media: the indirect approach},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {371--372},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140325},
 doi = {http://doi.acm.org/10.1145/1140277.1140325},
 acmid = {1140325},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {media-streaming, overlays, peer-to-peer},
} 

@article{Piotrowski:2006:PSS:1140103.1140325,
 author = {Piotrowski, Tadeusz and Banerjee, Suman and Bhatnagar, Sudeept and Ganguly, Samrat and Izmailov, Rauf},
 title = {Peer-to-peer streaming of stored media: the indirect approach},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {371--372},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140325},
 doi = {http://doi.acm.org/10.1145/1140103.1140325},
 acmid = {1140325},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {media-streaming, overlays, peer-to-peer},
} 

@inproceedings{Dholakia:2006:ANI:1140277.1140326,
 author = {Dholakia, Ajay and Eleftheriou, Evangelos and Hu, Xiao--Yu and Iliadis, Ilias and Menon, Jai and Rao, KK},
 title = {Analysis of a new intra-disk redundancy scheme for high-reliability RAID storage systems in the presence of unrecoverable errors},
 abstract = {Today's data storage systems are increasingly adopting low-cost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds and to a higher risk of unrecoverable media errors. We propose a new XOR-based intra-disk redundancy scheme, called interleaved parity check (IPC), to enhance the reliability of RAID systems that incurs only negligible I/O performance degradation. The proposed scheme introduces an additional level of redundancy inside each disk, on top of the RAID redundancy across multiple disks. The RAID parity provides protection against disk failures, while the proposed scheme aims to protect against media-related unrecoverable errors.We develop a new model capturing the effect of correlated unrecoverable sector errors and subsequently use it to analyze the proposed scheme as well as the traditional redundancy schemes based on Reed-Solomon (RS) codes and single-parity-check (SPC) codes. We derive closed-form expressions for the mean time to data loss (MTTDL) of RAID 5 and RAID 6 systems in the presence of unrecoverable errors and disk failures. We then combine these results for a comprehensive characterization of the reliability of RAID systems that incorporate the proposed IPC redundancy scheme. Our results show that in the practical case of correlated errors, the proposed scheme provides the same reliability as the optimum albeit more complex RS coding scheme. Finally, the throughput performance of incorporating the intra-disk redundancy on various RAID systems is evaluated by means of event-driven simulations. A detailed description of these contributions is given in [1].},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140326},
 doi = {http://doi.acm.org/10.1145/1140277.1140326},
 acmid = {1140326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, file and I/O systems, reliability analysis, stochastic modeling},
} 

@article{Dholakia:2006:ANI:1140103.1140326,
 author = {Dholakia, Ajay and Eleftheriou, Evangelos and Hu, Xiao--Yu and Iliadis, Ilias and Menon, Jai and Rao, KK},
 title = {Analysis of a new intra-disk redundancy scheme for high-reliability RAID storage systems in the presence of unrecoverable errors},
 abstract = {Today's data storage systems are increasingly adopting low-cost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds and to a higher risk of unrecoverable media errors. We propose a new XOR-based intra-disk redundancy scheme, called interleaved parity check (IPC), to enhance the reliability of RAID systems that incurs only negligible I/O performance degradation. The proposed scheme introduces an additional level of redundancy inside each disk, on top of the RAID redundancy across multiple disks. The RAID parity provides protection against disk failures, while the proposed scheme aims to protect against media-related unrecoverable errors.We develop a new model capturing the effect of correlated unrecoverable sector errors and subsequently use it to analyze the proposed scheme as well as the traditional redundancy schemes based on Reed-Solomon (RS) codes and single-parity-check (SPC) codes. We derive closed-form expressions for the mean time to data loss (MTTDL) of RAID 5 and RAID 6 systems in the presence of unrecoverable errors and disk failures. We then combine these results for a comprehensive characterization of the reliability of RAID systems that incorporate the proposed IPC redundancy scheme. Our results show that in the practical case of correlated errors, the proposed scheme provides the same reliability as the optimum albeit more complex RS coding scheme. Finally, the throughput performance of incorporating the intra-disk redundancy on various RAID systems is evaluated by means of event-driven simulations. A detailed description of these contributions is given in [1].},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140326},
 doi = {http://doi.acm.org/10.1145/1140103.1140326},
 acmid = {1140326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, file and I/O systems, reliability analysis, stochastic modeling},
} 

@inproceedings{Bower:2006:AAV:1140277.1140327,
 author = {Bower, Fred A. and Hower, Derek and Yilmaz, Mahmut and Sorin, Daniel J. and Ozev, Sule},
 title = {Applying architectural vulnerability Analysis to hard faults in the microprocessor},
 abstract = {In this paper, we present a new metric, Hard-Fault Architectural Vulnerability Factor (H-AVF), to allow designers to more effectively compare alternate hard-fault tolerance schemes. In order to provide intuition on the use of H-AVF as a metric, we evaluate fault-tolerant level-1 data cache and register file implementations using error correcting codes and a fault-tolerant adder using triple-modular redundancy (TMR). For each of the designs, we compute its H-AVF. We then use these H-AVF values in conjunction with other properties of the design, such as die area and power consumption, to provide composite metrics. The derived metrics provide simple, quantitative measures of the cost-effectiveness of the evaluated designs.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {375--376},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140327},
 doi = {http://doi.acm.org/10.1145/1140277.1140327},
 acmid = {1140327},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer architecture, hard-fault tolerance, reliability},
} 

@article{Bower:2006:AAV:1140103.1140327,
 author = {Bower, Fred A. and Hower, Derek and Yilmaz, Mahmut and Sorin, Daniel J. and Ozev, Sule},
 title = {Applying architectural vulnerability Analysis to hard faults in the microprocessor},
 abstract = {In this paper, we present a new metric, Hard-Fault Architectural Vulnerability Factor (H-AVF), to allow designers to more effectively compare alternate hard-fault tolerance schemes. In order to provide intuition on the use of H-AVF as a metric, we evaluate fault-tolerant level-1 data cache and register file implementations using error correcting codes and a fault-tolerant adder using triple-modular redundancy (TMR). For each of the designs, we compute its H-AVF. We then use these H-AVF values in conjunction with other properties of the design, such as die area and power consumption, to provide composite metrics. The derived metrics provide simple, quantitative measures of the cost-effectiveness of the evaluated designs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {375--376},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140327},
 doi = {http://doi.acm.org/10.1145/1140103.1140327},
 acmid = {1140327},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer architecture, hard-fault tolerance, reliability},
} 

@article{Broberg:2006:MFM:1140103.1140328,
 author = {Broberg, James A. and Liu, Zhen and Xia, Cathy H. and Zhang, Li},
 title = {A multicommodity flow model for distributed stream processing},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {377--378},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140328},
 doi = {http://doi.acm.org/10.1145/1140103.1140328},
 acmid = {1140328},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithms, multicommodity flow, potential function, stream processing},
} 

@inproceedings{Broberg:2006:MFM:1140277.1140328,
 author = {Broberg, James A. and Liu, Zhen and Xia, Cathy H. and Zhang, Li},
 title = {A multicommodity flow model for distributed stream processing},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {377--378},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140328},
 doi = {http://doi.acm.org/10.1145/1140277.1140328},
 acmid = {1140328},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed algorithms, multicommodity flow, potential function, stream processing},
} 

@article{Steiner:2006:CJC:1140103.1140329,
 author = {Steiner, Ian M. and Shuf, Yefim},
 title = {A characterization of a java-based commercial workload on a high-end enterprise server},
 abstract = {While past studies with simple Java benchmarks like SPECjvm98 and SPECjbb2000 have been integral in advancing the industry, this paper illustrates some of the characteristics of a more complex and realistic 3-Tier J2EE (Java 2 Enterprise Edition) commercial workload, SPECjAppServer2004.</i>In the course of this study, we both validate and disprove certain assumptions commonly made by researchers about Java workloads. For instance, on a tuned system having a heap of the appropriate size, the fraction of CPU time spent on garbage collection (GC) for this complex workload is small (\&lt;2\%) compared to commonly studied benchmarks like SPECjbb2000 and SPECjvm98.</i>In addition to high-level statistics on garbage collection and the execution profile, detailed hardware performance characteristics, such as the branch misprediction rates and lock contention, are evaluated and used to motivate future research directions. We also use statistical correlation to evaluate and compare the relative significance of different hardware events to the overall performance of the system.</i>},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {379--380},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140329},
 doi = {http://doi.acm.org/10.1145/1140103.1140329},
 acmid = {1140329},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JVM, Java, garbage collection, locality, memory allocation, memory management, object co-allocation, object placement, run-time systems},
} 

@inproceedings{Steiner:2006:CJC:1140277.1140329,
 author = {Steiner, Ian M. and Shuf, Yefim},
 title = {A characterization of a java-based commercial workload on a high-end enterprise server},
 abstract = {While past studies with simple Java benchmarks like SPECjvm98 and SPECjbb2000 have been integral in advancing the industry, this paper illustrates some of the characteristics of a more complex and realistic 3-Tier J2EE (Java 2 Enterprise Edition) commercial workload, SPECjAppServer2004.</i>In the course of this study, we both validate and disprove certain assumptions commonly made by researchers about Java workloads. For instance, on a tuned system having a heap of the appropriate size, the fraction of CPU time spent on garbage collection (GC) for this complex workload is small (\&lt;2\%) compared to commonly studied benchmarks like SPECjbb2000 and SPECjvm98.</i>In addition to high-level statistics on garbage collection and the execution profile, detailed hardware performance characteristics, such as the branch misprediction rates and lock contention, are evaluated and used to motivate future research directions. We also use statistical correlation to evaluate and compare the relative significance of different hardware events to the overall performance of the system.</i>},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {379--380},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140329},
 doi = {http://doi.acm.org/10.1145/1140277.1140329},
 acmid = {1140329},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JVM, Java, garbage collection, locality, memory allocation, memory management, object co-allocation, object placement, run-time systems},
} 

@article{Guha:2006:OTR:1140103.1140330,
 author = {Guha, Sudipto and Munagala, Kamesh and Sarkar, Saswati},
 title = {Optimizing transmission rate in wireless channels using adaptive probes},
 abstract = {We consider a wireless system with multiple channels where each channel is either on or off, and probing the state of any channel incurs a cost. We present a polynomial time algorithm that determines which channels to probe and also which channel to transmit so as to maximize the difference between the rate of successful transmissions and the cost incurred in probing.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {381--382},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140330},
 doi = {http://doi.acm.org/10.1145/1140103.1140330},
 acmid = {1140330},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-channel, probes, wireless},
} 

@inproceedings{Guha:2006:OTR:1140277.1140330,
 author = {Guha, Sudipto and Munagala, Kamesh and Sarkar, Saswati},
 title = {Optimizing transmission rate in wireless channels using adaptive probes},
 abstract = {We consider a wireless system with multiple channels where each channel is either on or off, and probing the state of any channel incurs a cost. We present a polynomial time algorithm that determines which channels to probe and also which channel to transmit so as to maximize the difference between the rate of successful transmissions and the cost incurred in probing.},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {381--382},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140330},
 doi = {http://doi.acm.org/10.1145/1140277.1140330},
 acmid = {1140330},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-channel, probes, wireless},
} 

@article{Chen:2006:CCB:1140103.1140331,
 author = {Chen, Yiyu and Das, Amitayu and Sivasubramaniam, Anand and Wang, Qian and Harper, R. and Bland, M.},
 title = {Consolidating clients on back-end servers with co-location and frequency control},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {383--384},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140331},
 doi = {http://doi.acm.org/10.1145/1140103.1140331},
 acmid = {1140331},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {co-location, consolidation, energy management, frequency control, server},
} 

@inproceedings{Chen:2006:CCB:1140277.1140331,
 author = {Chen, Yiyu and Das, Amitayu and Sivasubramaniam, Anand and Wang, Qian and Harper, R. and Bland, M.},
 title = {Consolidating clients on back-end servers with co-location and frequency control},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {383--384},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140331},
 doi = {http://doi.acm.org/10.1145/1140277.1140331},
 acmid = {1140331},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {co-location, consolidation, energy management, frequency control, server},
} 

@article{Prasad:2006:CRI:1140103.1140332,
 author = {Prasad, Ravi S. and Dovrolis, Constantine},
 title = {Congestion responsiveness of internet traffic: (a fresh look at an old problem)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {385--386},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140332},
 doi = {http://doi.acm.org/10.1145/1140103.1140332},
 acmid = {1140332},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Prasad:2006:CRI:1140277.1140332,
 author = {Prasad, Ravi S. and Dovrolis, Constantine},
 title = {Congestion responsiveness of internet traffic: (a fresh look at an old problem)},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {385--386},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140332},
 doi = {http://doi.acm.org/10.1145/1140277.1140332},
 acmid = {1140332},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhao:2006:TDN:1140277.1140333,
 author = {Zhao, Yao and Chen, Yan and Bindel, David},
 title = {Towards deterministic network diagnosis},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {387--388},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140333},
 doi = {http://doi.acm.org/10.1145/1140277.1140333},
 acmid = {1140333},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet diagnosis, loss rate, measurement},
} 

@article{Zhao:2006:TDN:1140103.1140333,
 author = {Zhao, Yao and Chen, Yan and Bindel, David},
 title = {Towards deterministic network diagnosis},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {387--388},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140333},
 doi = {http://doi.acm.org/10.1145/1140103.1140333},
 acmid = {1140333},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet diagnosis, loss rate, measurement},
} 

@article{Chan:2006:MAS:1140103.1140334,
 author = {Chan, Haowen and Dash, Debabrata and Perrig, Adrian and Zhang, Hui},
 title = {Modeling adoptability of secure BGP protocols},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {34},
 issue = {1},
 month = {June},
 year = {2006},
 issn = {0163-5999},
 pages = {389--390},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140103.1140334},
 doi = {http://doi.acm.org/10.1145/1140103.1140334},
 acmid = {1140334},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adoptability, critical threshold, incentives compatibility, routing security},
} 

@inproceedings{Chan:2006:MAS:1140277.1140334,
 author = {Chan, Haowen and Dash, Debabrata and Perrig, Adrian and Zhang, Hui},
 title = {Modeling adoptability of secure BGP protocols},
 abstract = {},
 booktitle = {Proceedings of the joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '06/Performance '06},
 year = {2006},
 isbn = {1-59593-319-0},
 location = {Saint Malo, France},
 pages = {389--390},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1140277.1140334},
 doi = {http://doi.acm.org/10.1145/1140277.1140334},
 acmid = {1140334},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adoptability, critical threshold, incentives compatibility, routing security},
} 

@inproceedings{Hoelzle:2005:GIL:1064212.1064213,
 author = {Hoelzle, Urs},
 title = {Google: or how I learned to love terabytes},
 abstract = {Search is one of the most important applications used on the internet, but it also poses some of the most interesting challenges in computer science. Providing high-quality search requires understanding across a wide range of computer science disciplines, from lower-level systems issues like computer architecture and distributed systems to applied areas like information retrieval, machine learning, data mining, and user interface design.In this talk I'll share some interesting observations and measurements obtained at Google, and will illustrate the behind-the-scenes pieces of infrastructure (both hardware and software) that we've built in order to extract this information from many terabytes of data.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064213},
 doi = {http://doi.acm.org/10.1145/1064212.1064213},
 acmid = {1064213},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hoelzle:2005:GIL:1071690.1064213,
 author = {Hoelzle, Urs},
 title = {Google: or how I learned to love terabytes},
 abstract = {Search is one of the most important applications used on the internet, but it also poses some of the most interesting challenges in computer science. Providing high-quality search requires understanding across a wide range of computer science disciplines, from lower-level systems issues like computer architecture and distributed systems to applied areas like information retrieval, machine learning, data mining, and user interface design.In this talk I'll share some interesting observations and measurements obtained at Google, and will illustrate the behind-the-scenes pieces of infrastructure (both hardware and software) that we've built in order to extract this information from many terabytes of data.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064213},
 doi = {http://doi.acm.org/10.1145/1071690.1064213},
 acmid = {1064213},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Massoulie:2005:CRS:1071690.1064215,
 author = {Massouli\'{e}, Laurent and Vojnovi\'{C}, Milan},
 title = {Coupon replication systems},
 abstract = {Motivated by the study of peer-to-peer file swarming systems \&#224; la BitTorrent, we introduce a probabilistic model of coupon replication systems</i>. These systems consist of users, aiming to complete a collection of distinct coupons. Users are characterised by their current collection of coupons, and leave the system once they complete their coupon collection. The system evolution is then specified by describing how users of distinct types meet, and which coupons get replicated upon such encounters.For open systems, with exogenous user arrivals, we derive necessary and sufficient stability conditions in a layered scenario, where encounters are between users holding the same number of coupons. We also consider a system where encounters are between users chosen uniformly at random from the whole population. We show that performance, captured by sojourn time, is asymptotically optimal in both systems as the number of coupon types becomes large.We also consider closed systems with no exogenous user arrivals. In a special scenario where users have only one missing coupon, we evaluate the size of the population ultimately remaining in the system, as the initial number of users, N</i>, goes to infinity. We show that this decreases geometrically with the number of coupons, K</i>. In particular, when the ratio K</i>/log(N</i>) is above a critical threshold, we prove that this number of left-overs is of order log(log(N</i>)).These results suggest that performance of file swarming systems does not depend critically on either altruistic user behavior, or on load balancing strategies such as rarest first</i>.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064215},
 doi = {http://doi.acm.org/10.1145/1071690.1064215},
 acmid = {1064215},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, file swarming, peer-to-peer},
} 

@inproceedings{Massoulie:2005:CRS:1064212.1064215,
 author = {Massouli\'{e}, Laurent and Vojnovi\'{C}, Milan},
 title = {Coupon replication systems},
 abstract = {Motivated by the study of peer-to-peer file swarming systems \&#224; la BitTorrent, we introduce a probabilistic model of coupon replication systems</i>. These systems consist of users, aiming to complete a collection of distinct coupons. Users are characterised by their current collection of coupons, and leave the system once they complete their coupon collection. The system evolution is then specified by describing how users of distinct types meet, and which coupons get replicated upon such encounters.For open systems, with exogenous user arrivals, we derive necessary and sufficient stability conditions in a layered scenario, where encounters are between users holding the same number of coupons. We also consider a system where encounters are between users chosen uniformly at random from the whole population. We show that performance, captured by sojourn time, is asymptotically optimal in both systems as the number of coupon types becomes large.We also consider closed systems with no exogenous user arrivals. In a special scenario where users have only one missing coupon, we evaluate the size of the population ultimately remaining in the system, as the initial number of users, N</i>, goes to infinity. We show that this decreases geometrically with the number of coupons, K</i>. In particular, when the ratio K</i>/log(N</i>) is above a critical threshold, we prove that this number of left-overs is of order log(log(N</i>)).These results suggest that performance of file swarming systems does not depend critically on either altruistic user behavior, or on load balancing strategies such as rarest first</i>.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064215},
 doi = {http://doi.acm.org/10.1145/1064212.1064215},
 acmid = {1064215},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, file swarming, peer-to-peer},
} 

@article{Tang:2005:LTO:1071690.1064216,
 author = {Tang, Chunqiang and Buco, Melissa J. and Chang, Rong N. and Dwarkadas, Sandhya and Luan, Laura Z. and So, Edward and Ward, Christopher},
 title = {Low traffic overlay networks with large routing tables},
 abstract = {The routing tables of Distributed Hash Tables (DHTs) can vary from size O</i>(1) to O</i>(n</i>). Currently, what is lacking is an analytic framework to suggest the optimal routing table size for a given workload. This paper (1) compares DHTs with O</i>(1) to O</i>(n</i>) routing tables and identifies some good design points; and (2) proposes protocols to realize the potential of those good design points.We use total traffic as the uniform metric to compare heterogeneous DHTs and emphasize the balance between maintenance cost and lookup cost. Assuming a node on average processes 1,000 or more lookups during its entire lifetime, our analysis shows that large routing tables actually lead to both low traffic and low lookup hops. These good design points translate into one-hop routing for systems of medium size and two-hop routing for large systems.Existing one-hop or two-hop protocols are based on a hierarchy. We instead demonstrate that it is possible to achieve completely decentralized one-hop or two-hop routing, i.e., without giving up being peer-to-peer. We propose 1h-Calot for one-hop routing and 2h-Calot for two-hop routing. Assuming a moderate lookup rate, compared with DHTs that use O</i>(log n</i>) routing tables, 1h-Calot and 2h-Calot save traffic by up to 70\% while resolving lookups in one or two hops as opposed to O</i>(log n</i>) hops.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064216},
 doi = {http://doi.acm.org/10.1145/1071690.1064216},
 acmid = {1064216},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed hash table, overlay network, peer-to-peer system},
} 

@inproceedings{Tang:2005:LTO:1064212.1064216,
 author = {Tang, Chunqiang and Buco, Melissa J. and Chang, Rong N. and Dwarkadas, Sandhya and Luan, Laura Z. and So, Edward and Ward, Christopher},
 title = {Low traffic overlay networks with large routing tables},
 abstract = {The routing tables of Distributed Hash Tables (DHTs) can vary from size O</i>(1) to O</i>(n</i>). Currently, what is lacking is an analytic framework to suggest the optimal routing table size for a given workload. This paper (1) compares DHTs with O</i>(1) to O</i>(n</i>) routing tables and identifies some good design points; and (2) proposes protocols to realize the potential of those good design points.We use total traffic as the uniform metric to compare heterogeneous DHTs and emphasize the balance between maintenance cost and lookup cost. Assuming a node on average processes 1,000 or more lookups during its entire lifetime, our analysis shows that large routing tables actually lead to both low traffic and low lookup hops. These good design points translate into one-hop routing for systems of medium size and two-hop routing for large systems.Existing one-hop or two-hop protocols are based on a hierarchy. We instead demonstrate that it is possible to achieve completely decentralized one-hop or two-hop routing, i.e., without giving up being peer-to-peer. We propose 1h-Calot for one-hop routing and 2h-Calot for two-hop routing. Assuming a moderate lookup rate, compared with DHTs that use O</i>(log n</i>) routing tables, 1h-Calot and 2h-Calot save traffic by up to 70\% while resolving lookups in one or two hops as opposed to O</i>(log n</i>) hops.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {14--25},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064216},
 doi = {http://doi.acm.org/10.1145/1064212.1064216},
 acmid = {1064216},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed hash table, overlay network, peer-to-peer system},
} 

@article{Leonard:2005:LNF:1071690.1064217,
 author = {Leonard, Derek and Rai, Vivek and Loguinov, Dmitri},
 title = {On lifetime-based node failure and stochastic resilience of decentralized peer-to-peer networks},
 abstract = {To understand how high rates of churn and random departure decisions of end-users affect connectivity of P2P networks, this paper investigates resilience of random graphs to lifetime-based node failure and derives the expected delay before a user is forcefully isolated from the graph and the probability that this occurs within his/her lifetime. Our results indicate that systems with heavy-tailed lifetime distributions are more resilient than those with light-tailed (e.g., exponential) distributions and that for a given average degree, k</i>-regular graphs exhibit the highest resilience. As a practical illustration of our results, each user in a system with n</i> = 100 billion peers, 30-minute average lifetime, and 1-minute node-replacement delay can stay connected to the graph with probability 1 - 1 n</i> using only 9 neighbors. This is in contrast to 37 neighbors required under previous modeling efforts. We finish the paper by showing that many P2P networks are almost surely</i> (i.e., with probability 1-o</i>(1)) connected if they have no isolated nodes and derive a simple model for the probability that a P2P system partitions under churn.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064217},
 doi = {http://doi.acm.org/10.1145/1071690.1064217},
 acmid = {1064217},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {pareto, peer-to-peer, stochastic lifetime resilience},
} 

@inproceedings{Leonard:2005:LNF:1064212.1064217,
 author = {Leonard, Derek and Rai, Vivek and Loguinov, Dmitri},
 title = {On lifetime-based node failure and stochastic resilience of decentralized peer-to-peer networks},
 abstract = {To understand how high rates of churn and random departure decisions of end-users affect connectivity of P2P networks, this paper investigates resilience of random graphs to lifetime-based node failure and derives the expected delay before a user is forcefully isolated from the graph and the probability that this occurs within his/her lifetime. Our results indicate that systems with heavy-tailed lifetime distributions are more resilient than those with light-tailed (e.g., exponential) distributions and that for a given average degree, k</i>-regular graphs exhibit the highest resilience. As a practical illustration of our results, each user in a system with n</i> = 100 billion peers, 30-minute average lifetime, and 1-minute node-replacement delay can stay connected to the graph with probability 1 - 1 n</i> using only 9 neighbors. This is in contrast to 37 neighbors required under previous modeling efforts. We finish the paper by showing that many P2P networks are almost surely</i> (i.e., with probability 1-o</i>(1)) connected if they have no isolated nodes and derive a simple model for the probability that a P2P system partitions under churn.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {26--37},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064217},
 doi = {http://doi.acm.org/10.1145/1064212.1064217},
 acmid = {1064217},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {pareto, peer-to-peer, stochastic lifetime resilience},
} 

@inproceedings{Dumitriu:2005:DRP:1064212.1064218,
 author = {Dumitriu, D. and Knightly, E. and Kuzmanovic, A. and Stoica, I. and Zwaenepoel, W.},
 title = {Denial-of-service resilience in peer-to-peer file sharing systems},
 abstract = {Peer-to-peer (p2p) file sharing systems are characterized by highly replicated content distributed among nodes with enormous aggregate resources for storage and communication. These properties alone are not sufficient, however, to render p2p networks immune to denial-of-service (DoS) attack. In this paper, we study, by means of analytical modeling and simulation, the resilience of p2p file sharing systems against DoS attacks, in which malicious nodes respond to queries with erroneous responses. We consider the file-targeted attacks in current use in the Internet, and we introduce a new class of p2p-network-targeted attacks.In file-targeted attacks, the attacker puts a large number of corrupted versions of a single</i> file on the network. We demonstrate that the effectiveness of these attacks is highly dependent on the clients' behavior. For the attacks to succeed over the long term, clients must be unwilling to share files, slow in removing corrupted files from their machines, and quick to give up downloading when the system is under attack.In network-targeted attacks, attackers respond to queries for any</i> file with erroneous information. Our results indicate that these attacks are highly scalable: increasing the number of malicious nodes yields a hyperexponential decrease in system goodput, and a moderate number of attackers suffices to cause a near-collapse of the entire system. The key factors inducing this vulnerability are (i) hierarchical topologies with misbehaving "supernodes," (ii) high path-length networks in which attackers have increased opportunity to falsify control information, and (iii) power-law networks in which attackers insert themselves into high-degree points in the graph.Finally, we consider the effects of client counter-strategies such as randomized reply selection, redundant and parallel download, and reputation systems. Some counter-strategies (e.g., randomized reply selection) provide considerable immunity to attack (reducing the scaling from hyperexponential to linear), yet significantly hurt performance in the absence of an attack. Other counter-strategies yield little benefit (or penalty). In particular, reputation systems show little impact unless they operate with near perfection.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {38--49},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064218},
 doi = {http://doi.acm.org/10.1145/1064212.1064218},
 acmid = {1064218},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {denial of service, file pollution, network-targeted attacks, peer-to-peer},
} 

@article{Dumitriu:2005:DRP:1071690.1064218,
 author = {Dumitriu, D. and Knightly, E. and Kuzmanovic, A. and Stoica, I. and Zwaenepoel, W.},
 title = {Denial-of-service resilience in peer-to-peer file sharing systems},
 abstract = {Peer-to-peer (p2p) file sharing systems are characterized by highly replicated content distributed among nodes with enormous aggregate resources for storage and communication. These properties alone are not sufficient, however, to render p2p networks immune to denial-of-service (DoS) attack. In this paper, we study, by means of analytical modeling and simulation, the resilience of p2p file sharing systems against DoS attacks, in which malicious nodes respond to queries with erroneous responses. We consider the file-targeted attacks in current use in the Internet, and we introduce a new class of p2p-network-targeted attacks.In file-targeted attacks, the attacker puts a large number of corrupted versions of a single</i> file on the network. We demonstrate that the effectiveness of these attacks is highly dependent on the clients' behavior. For the attacks to succeed over the long term, clients must be unwilling to share files, slow in removing corrupted files from their machines, and quick to give up downloading when the system is under attack.In network-targeted attacks, attackers respond to queries for any</i> file with erroneous information. Our results indicate that these attacks are highly scalable: increasing the number of malicious nodes yields a hyperexponential decrease in system goodput, and a moderate number of attackers suffices to cause a near-collapse of the entire system. The key factors inducing this vulnerability are (i) hierarchical topologies with misbehaving "supernodes," (ii) high path-length networks in which attackers have increased opportunity to falsify control information, and (iii) power-law networks in which attackers insert themselves into high-degree points in the graph.Finally, we consider the effects of client counter-strategies such as randomized reply selection, redundant and parallel download, and reputation systems. Some counter-strategies (e.g., randomized reply selection) provide considerable immunity to attack (reducing the scaling from hyperexponential to linear), yet significantly hurt performance in the absence of an attack. Other counter-strategies yield little benefit (or penalty). In particular, reputation systems show little impact unless they operate with near perfection.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {38--49},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064218},
 doi = {http://doi.acm.org/10.1145/1071690.1064218},
 acmid = {1064218},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {denial of service, file pollution, network-targeted attacks, peer-to-peer},
} 

@inproceedings{Moore:2005:ITC:1064212.1064220,
 author = {Moore, Andrew W. and Zuev, Denis},
 title = {Internet traffic classification using bayesian analysis techniques},
 abstract = {Accurate traffic classification is of fundamental importance to numerous other network activities, from security monitoring to accounting, and from Quality of Service to providing operators with useful forecasts for long-term provisioning. We apply a Na\&#239;ve Bayes estimator to categorize traffic by application. Uniquely, our work capitalizes on hand-classified network data, using it as input to a supervised Na\&#239;ve Bayes estimator. In this paper we illustrate the high level of accuracy achievable with the \Naive Bayes estimator. We further illustrate the improved accuracy of refined variants of this estimator.Our results indicate that with the simplest of Na\&#239;ve Bayes estimator we are able to achieve about 65\% accuracy on per-flow classification and with two powerful refinements we can improve this value to better than 95\%; this is a vast improvement over traditional techniques that achieve 50--70\%. While our technique uses training data, with categories derived from packet-content, all of our training and testing was done using header-derived discriminators. We emphasize this as a powerful aspect of our approach: using samples of well-known traffic to allow the categorization of traffic using commonly available information alone.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {50--60},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064212.1064220},
 doi = {http://doi.acm.org/10.1145/1064212.1064220},
 acmid = {1064220},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flow classification, internet traffic, traffic identification},
} 

@article{Moore:2005:ITC:1071690.1064220,
 author = {Moore, Andrew W. and Zuev, Denis},
 title = {Internet traffic classification using bayesian analysis techniques},
 abstract = {Accurate traffic classification is of fundamental importance to numerous other network activities, from security monitoring to accounting, and from Quality of Service to providing operators with useful forecasts for long-term provisioning. We apply a Na\&#239;ve Bayes estimator to categorize traffic by application. Uniquely, our work capitalizes on hand-classified network data, using it as input to a supervised Na\&#239;ve Bayes estimator. In this paper we illustrate the high level of accuracy achievable with the \Naive Bayes estimator. We further illustrate the improved accuracy of refined variants of this estimator.Our results indicate that with the simplest of Na\&#239;ve Bayes estimator we are able to achieve about 65\% accuracy on per-flow classification and with two powerful refinements we can improve this value to better than 95\%; this is a vast improvement over traditional techniques that achieve 50--70\%. While our technique uses training data, with categories derived from packet-content, all of our training and testing was done using header-derived discriminators. We emphasize this as a powerful aspect of our approach: using samples of well-known traffic to allow the categorization of traffic using commonly available information alone.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {50--60},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1071690.1064220},
 doi = {http://doi.acm.org/10.1145/1071690.1064220},
 acmid = {1064220},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flow classification, internet traffic, traffic identification},
} 

@inproceedings{Kumar:2005:DSA:1064212.1064221,
 author = {Kumar, Abhishek and Sung, Minho and Xu, Jun (Jim) and Zegura, Ellen W.},
 title = {A data streaming algorithm for estimating subpopulation flow size distribution},
 abstract = {Statistical information about the flow sizes in the traffic passing through a network link helps a network operator to characterize network resource usage, infer traffic demands, detect traffic anomalies, and improve network performance through traffic engineering. Previous work on estimating the flow size distribution for the complete population</i> of flows has produced techniques that either make inferences from sampled network traffic, or use data streaming approaches. In this work, we identify and solve a more challenging problem of estimating the size distribution and other statistical information about arbitrary subpopulations</i> of flows. Inferring subpopulation flow statistics is more challenging than the complete population counterpart, since subpopulations of interest are often specified a posteriori</i> (i.e., after the data collection is done), making it impossible for the data collection module to "plan in advance".Our solution consists of a novel mechanism that combines data streaming with traditional packet sampling to provide highly accurate estimates of subpopulation flow statistics. The algorithm employs two data collection modules operating in parallel --- a NetFlow-like packet sampler and a streaming data structure made up of an array of counters. Combining the data collected by these two modules, our estimation algorithm uses a statistical estimation procedure that correlates and decodes the outputs (observations) from both data collection modules to obtain flow statistics for any arbitrary subpopulation. Evaluations of this algorithm on real-world Internet traffic traces demonstrate its high measurement accuracy.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064221},
 doi = {http://doi.acm.org/10.1145/1064212.1064221},
 acmid = {1064221},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EM algorithm, data streaming, flow statistics, statistical inference, traffic analysis},
} 

@article{Kumar:2005:DSA:1071690.1064221,
 author = {Kumar, Abhishek and Sung, Minho and Xu, Jun (Jim) and Zegura, Ellen W.},
 title = {A data streaming algorithm for estimating subpopulation flow size distribution},
 abstract = {Statistical information about the flow sizes in the traffic passing through a network link helps a network operator to characterize network resource usage, infer traffic demands, detect traffic anomalies, and improve network performance through traffic engineering. Previous work on estimating the flow size distribution for the complete population</i> of flows has produced techniques that either make inferences from sampled network traffic, or use data streaming approaches. In this work, we identify and solve a more challenging problem of estimating the size distribution and other statistical information about arbitrary subpopulations</i> of flows. Inferring subpopulation flow statistics is more challenging than the complete population counterpart, since subpopulations of interest are often specified a posteriori</i> (i.e., after the data collection is done), making it impossible for the data collection module to "plan in advance".Our solution consists of a novel mechanism that combines data streaming with traditional packet sampling to provide highly accurate estimates of subpopulation flow statistics. The algorithm employs two data collection modules operating in parallel --- a NetFlow-like packet sampler and a streaming data structure made up of an array of counters. Combining the data collected by these two modules, our estimation algorithm uses a statistical estimation procedure that correlates and decodes the outputs (observations) from both data collection modules to obtain flow statistics for any arbitrary subpopulation. Evaluations of this algorithm on real-world Internet traffic traces demonstrate its high measurement accuracy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {61--72},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064221},
 doi = {http://doi.acm.org/10.1145/1071690.1064221},
 acmid = {1064221},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EM algorithm, data streaming, flow statistics, statistical inference, traffic analysis},
} 

@article{Cohen:2005:PCL:1071690.1064222,
 author = {Cohen, Edith and Lund, Carsten},
 title = {Packet classification in large ISPs: design and evaluation of decision tree classifiers},
 abstract = {Packet classification, although extensively studied, is an evolving problem. Growing and changing needs necessitate the use of larger filters with more complex rules. The increased complexity and size pose implementation challenges on current hardware solutions and drive the development of software classifiers, in particular, decision-tree based classifiers. Important performance measures for these classifiers are time and memory due to required high throughput and use of limited fast memory.We analyze Tier 1 ISP data that includes filters and corresponding traffic from over a hundred edge routers and thousands of interfaces. We provide a comprehensive view on packet classification in an operational network and glean insights that help us design more effective classification algorithms.We propose and evaluate decision tree classifiers with common branches</i>. These classifiers have linear worst-case memory bounds and require much less memory than standard decision tree classifiers, but nonetheless, we show that on our data have similar average and worst-case time performance. We argue that common-branches exploit structure that is present in real-life data sets.We observe a strong Zipf-like pattern in the usage of rules in a classifier, where a very small number of rules resolves the bulk of traffic and most rules are essentially never used. Inspired by this observation, we propose traffic-aware</i> classifiers that obtain superior average-case and bounded worst-case performance. Good average-case can boost performance of software classifiers that can be used in small to medium sized routers and are also important for traffic analysis and traffic engineering.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064222},
 doi = {http://doi.acm.org/10.1145/1071690.1064222},
 acmid = {1064222},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {access control lists, decision trees, packet filtering, routing},
} 

@inproceedings{Cohen:2005:PCL:1064212.1064222,
 author = {Cohen, Edith and Lund, Carsten},
 title = {Packet classification in large ISPs: design and evaluation of decision tree classifiers},
 abstract = {Packet classification, although extensively studied, is an evolving problem. Growing and changing needs necessitate the use of larger filters with more complex rules. The increased complexity and size pose implementation challenges on current hardware solutions and drive the development of software classifiers, in particular, decision-tree based classifiers. Important performance measures for these classifiers are time and memory due to required high throughput and use of limited fast memory.We analyze Tier 1 ISP data that includes filters and corresponding traffic from over a hundred edge routers and thousands of interfaces. We provide a comprehensive view on packet classification in an operational network and glean insights that help us design more effective classification algorithms.We propose and evaluate decision tree classifiers with common branches</i>. These classifiers have linear worst-case memory bounds and require much less memory than standard decision tree classifiers, but nonetheless, we show that on our data have similar average and worst-case time performance. We argue that common-branches exploit structure that is present in real-life data sets.We observe a strong Zipf-like pattern in the usage of rules in a classifier, where a very small number of rules resolves the bulk of traffic and most rules are essentially never used. Inspired by this observation, we propose traffic-aware</i> classifiers that obtain superior average-case and bounded worst-case performance. Good average-case can boost performance of software classifiers that can be used in small to medium sized routers and are also important for traffic analysis and traffic engineering.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {73--84},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064222},
 doi = {http://doi.acm.org/10.1145/1064212.1064222},
 acmid = {1064222},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {access control lists, decision trees, packet filtering, routing},
} 

@article{Keys:2005:RSA:1071690.1064223,
 author = {Keys, Ken and Moore, David and Estan, Cristian},
 title = {A robust system for accurate real-time summaries of internet traffic},
 abstract = {Good performance under extreme workloads and isolation between the resource consumption of concurrent jobs are perennial design goals of computer systems ranging from multitasking servers to network routers. In this paper we present a specialized system that computes multiple summaries of IP traffic in real time and achieves robustness and isolation between tasks in a novel way: by automatically adapting the parameters of the summarization algorithms. In traditional systems, anomalous network behavior such as denial of service attacks or worms can overwhelm the memory or CPU, making the system produce meaningless results exactly when measurement is needed most. In contrast, our measurement system reacts by gracefully degrading the accuracy of the affected summaries.The types of summaries we compute are widely used by network administrators monitoring the workloads of their networks: the ports sending the most traffic, the IP addresses sending or receiving the most traffic or opening the most connections, etc. We evaluate and compare many existing algorithmic solutions for computing these summaries, as well as two new solutions we propose here: "flow sample and hold" and "Bloom filter tuple set counting". Compared to previous solutions, these new solutions offer better memory versus accuracy tradeoffs and have more predictable resource consumption. Finally, we evaluate the actual implementation of a complete system that combines the best of these algorithms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064223},
 doi = {http://doi.acm.org/10.1145/1071690.1064223},
 acmid = {1064223},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive response, measurement, passive monitoring, sampling, traffic estimation},
} 

@inproceedings{Keys:2005:RSA:1064212.1064223,
 author = {Keys, Ken and Moore, David and Estan, Cristian},
 title = {A robust system for accurate real-time summaries of internet traffic},
 abstract = {Good performance under extreme workloads and isolation between the resource consumption of concurrent jobs are perennial design goals of computer systems ranging from multitasking servers to network routers. In this paper we present a specialized system that computes multiple summaries of IP traffic in real time and achieves robustness and isolation between tasks in a novel way: by automatically adapting the parameters of the summarization algorithms. In traditional systems, anomalous network behavior such as denial of service attacks or worms can overwhelm the memory or CPU, making the system produce meaningless results exactly when measurement is needed most. In contrast, our measurement system reacts by gracefully degrading the accuracy of the affected summaries.The types of summaries we compute are widely used by network administrators monitoring the workloads of their networks: the ports sending the most traffic, the IP addresses sending or receiving the most traffic or opening the most connections, etc. We evaluate and compare many existing algorithmic solutions for computing these summaries, as well as two new solutions we propose here: "flow sample and hold" and "Bloom filter tuple set counting". Compared to previous solutions, these new solutions offer better memory versus accuracy tradeoffs and have more predictable resource consumption. Finally, we evaluate the actual implementation of a complete system that combines the best of these algorithms.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064223},
 doi = {http://doi.acm.org/10.1145/1064212.1064223},
 acmid = {1064223},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive response, measurement, passive monitoring, sampling, traffic estimation},
} 

@inproceedings{Choi:2005:PCW:1064212.1064225,
 author = {Choi, Sunwoong and Park, Kihong and Kim, Chong-kwon},
 title = {On the performance characteristics of WLANs: revisited},
 abstract = {Wide-spread deployment of infrastructure WLANs has made Wi-Fi an integral part of today's Internet access technology. Despite its crucial role in affecting end-to-end performance, past research has focused on MAC protocol enhancement, analysis and simulation-based performance evaluation without sufficient consideration for modeling inaccuracies stemming from inter-layer dependencies, including physical layer diversity, that significantly impact performance. We take a fresh look at IEEE 802.11 WLANs, and using a combination of experiment, simulation, and analysis demonstrate its surprisingly agile performance traits. Our main findings are two-fold. First, contention-based MAC throughput degrades gracefully under congested conditions, enabled by physical layer channel diversity that reduces the effective level of MAC contention. In contrast, fairness and jitter significantly degrade at a critical offered load. This duality obviates the need for link layer flow control for throughput improvement but necessitates traffic control for fairness and QoS. Second, TCP-over-WLAN achieves high throughput commensurate with that of wireline TCP under saturated conditions, challenging the widely held perception that TCP throughput fares poorly over WLANs when subject to heavy contention. We show that TCP-over-WLAN prowess is facilitated by the self-regulating actions of DCF and TCP congestion control that jointly drive the shared physical channel at an effective load of 2--3 wireless stations, even when the number of active stations is very large. Our results highlight subtle inter-layer dependencies including the mitigating influence of TCP-over-WLAN on dynamic rate shifting.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064225},
 doi = {http://doi.acm.org/10.1145/1064212.1064225},
 acmid = {1064225},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DCF performance, TCP-over-WLAN performance, inter-layer dependence, physical layer diversity, rate control},
} 

@article{Choi:2005:PCW:1071690.1064225,
 author = {Choi, Sunwoong and Park, Kihong and Kim, Chong-kwon},
 title = {On the performance characteristics of WLANs: revisited},
 abstract = {Wide-spread deployment of infrastructure WLANs has made Wi-Fi an integral part of today's Internet access technology. Despite its crucial role in affecting end-to-end performance, past research has focused on MAC protocol enhancement, analysis and simulation-based performance evaluation without sufficient consideration for modeling inaccuracies stemming from inter-layer dependencies, including physical layer diversity, that significantly impact performance. We take a fresh look at IEEE 802.11 WLANs, and using a combination of experiment, simulation, and analysis demonstrate its surprisingly agile performance traits. Our main findings are two-fold. First, contention-based MAC throughput degrades gracefully under congested conditions, enabled by physical layer channel diversity that reduces the effective level of MAC contention. In contrast, fairness and jitter significantly degrade at a critical offered load. This duality obviates the need for link layer flow control for throughput improvement but necessitates traffic control for fairness and QoS. Second, TCP-over-WLAN achieves high throughput commensurate with that of wireline TCP under saturated conditions, challenging the widely held perception that TCP throughput fares poorly over WLANs when subject to heavy contention. We show that TCP-over-WLAN prowess is facilitated by the self-regulating actions of DCF and TCP congestion control that jointly drive the shared physical channel at an effective load of 2--3 wireless stations, even when the number of active stations is very large. Our results highlight subtle inter-layer dependencies including the mitigating influence of TCP-over-WLAN on dynamic rate shifting.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064225},
 doi = {http://doi.acm.org/10.1145/1071690.1064225},
 acmid = {1064225},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DCF performance, TCP-over-WLAN performance, inter-layer dependence, physical layer diversity, rate control},
} 

@article{Ramaiyan:2005:FPA:1071690.1064226,
 author = {Ramaiyan, Venkatesh and Kumar, Anurag and Altman, Eitan},
 title = {Fixed point analysis of single cell IEEE 802.11e WLANs: uniqueness, multistability and throughput differentiation},
 abstract = {We consider the vector fixed point equations arising out of the analysis of the saturation throughput of a single cell IEEE 802.11e wireless local area network with nodes that have different back-off parameters, including different Arbitration InterFrame Space (AIFS) values. We consider balanced and unbalanced solutions of the fixed point equations arising in homogeneous and nonhomogeneous networks. We are concerned, in particular, with (i) whether the fixed point is balanced within a class, and (ii) whether the fixed point is unique. Our simulations show that when multiple unbalanced fixed points exist in a homogeneous system then the time behaviour of the system demonstrates severe short term unfairness (or multistability</i>). Implications for the use of the fixed point formulation for performance analysis are also discussed. We provide a condition for the fixed point solution to be balanced within a class, and also a condition for uniqueness. We then provide an extension of our general fixed point analysis to capture AIFS based differentiation; again a condition for uniqueness is established. An asymptotic analysis of the fixed point is provided for the case in which packets are never abandoned, and the number of nodes goes to \&#8734;. Finally the fixed point equations are used to obtain insights into the throughput differentiation provided by different initial back-offs, persistence factors, and AIFS, for finite number of nodes, and for differentiation parameter values similar to those in the standard.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064226},
 doi = {http://doi.acm.org/10.1145/1071690.1064226},
 acmid = {1064226},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDCF analysis, QoS in wireless LANs, performance of wireless LANs, short term unfairness},
} 

@inproceedings{Ramaiyan:2005:FPA:1064212.1064226,
 author = {Ramaiyan, Venkatesh and Kumar, Anurag and Altman, Eitan},
 title = {Fixed point analysis of single cell IEEE 802.11e WLANs: uniqueness, multistability and throughput differentiation},
 abstract = {We consider the vector fixed point equations arising out of the analysis of the saturation throughput of a single cell IEEE 802.11e wireless local area network with nodes that have different back-off parameters, including different Arbitration InterFrame Space (AIFS) values. We consider balanced and unbalanced solutions of the fixed point equations arising in homogeneous and nonhomogeneous networks. We are concerned, in particular, with (i) whether the fixed point is balanced within a class, and (ii) whether the fixed point is unique. Our simulations show that when multiple unbalanced fixed points exist in a homogeneous system then the time behaviour of the system demonstrates severe short term unfairness (or multistability</i>). Implications for the use of the fixed point formulation for performance analysis are also discussed. We provide a condition for the fixed point solution to be balanced within a class, and also a condition for uniqueness. We then provide an extension of our general fixed point analysis to capture AIFS based differentiation; again a condition for uniqueness is established. An asymptotic analysis of the fixed point is provided for the case in which packets are never abandoned, and the number of nodes goes to \&#8734;. Finally the fixed point equations are used to obtain insights into the throughput differentiation provided by different initial back-offs, persistence factors, and AIFS, for finite number of nodes, and for differentiation parameter values similar to those in the standard.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064226},
 doi = {http://doi.acm.org/10.1145/1064212.1064226},
 acmid = {1064226},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDCF analysis, QoS in wireless LANs, performance of wireless LANs, short term unfairness},
} 

@article{Lindemann:2005:MEI:1071690.1064227,
 author = {Lindemann, Christoph and Waldhorst, Oliver P.},
 title = {Modeling epidemic information dissemination on mobile devices with finite buffers},
 abstract = {Epidemic algorithms have recently been proposed as an effective solution for disseminating information in large-scale peer-to-peer (P2P) systems and in mobile ad hoc networks (MANET). In this paper, we present a modeling approach for steady-state analysis of epidemic dissemination of information in MANET. As major contribution, the introduced approach explicitly represents the spread of multiple data items, finite buffer capacity at mobile devices and a least recently used buffer replacement scheme. Using the introduced modeling approach, we analyze seven degrees of separation (7DS) as one well-known approach for implementing P2P data sharing in a MANET using epidemic dissemination of information. A validation of results derived from the analytical model against simulation shows excellent agreement. Quantitative performance curves derived from the analytical model yield several insights for optimizing the system design of 7DS.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064227},
 doi = {http://doi.acm.org/10.1145/1071690.1064227},
 acmid = {1064227},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical performance modeling, mobile ad hoc networks, peer-to-peer data sharing, performance-oriented design and evaluation studies of distributed systems},
} 

@inproceedings{Lindemann:2005:MEI:1064212.1064227,
 author = {Lindemann, Christoph and Waldhorst, Oliver P.},
 title = {Modeling epidemic information dissemination on mobile devices with finite buffers},
 abstract = {Epidemic algorithms have recently been proposed as an effective solution for disseminating information in large-scale peer-to-peer (P2P) systems and in mobile ad hoc networks (MANET). In this paper, we present a modeling approach for steady-state analysis of epidemic dissemination of information in MANET. As major contribution, the introduced approach explicitly represents the spread of multiple data items, finite buffer capacity at mobile devices and a least recently used buffer replacement scheme. Using the introduced modeling approach, we analyze seven degrees of separation (7DS) as one well-known approach for implementing P2P data sharing in a MANET using epidemic dissemination of information. A validation of results derived from the analytical model against simulation shows excellent agreement. Quantitative performance curves derived from the analytical model yield several insights for optimizing the system design of 7DS.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064227},
 doi = {http://doi.acm.org/10.1145/1064212.1064227},
 acmid = {1064227},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical performance modeling, mobile ad hoc networks, peer-to-peer data sharing, performance-oriented design and evaluation studies of distributed systems},
} 

@inproceedings{Kumar:2005:AAC:1064212.1064228,
 author = {Kumar, V. S. Anil and Marathe, Madhav V. and Parthasarathy, Srinivasan and Srinivasan, Aravind},
 title = {Algorithmic aspects of capacity in wireless networks},
 abstract = {This paper considers two inter-related questions: (i) Given a wireless ad-hoc network and a collection of source-destination pairs (s<inf>i</inf>,t<inf>i</inf></i>), what is the maximum throughput capacity of the network, i.e. the rate at which data from the sources to their corresponding destinations can be transferred in the network? (ii) Can network protocols be designed that jointly route the packets and schedule transmissions at rates close to the maximum throughput capacity? Much of the earlier work focused on random instances and proved analytical lower and upper bounds on the maximum throughput capacity. Here, in contrast, we consider arbitrary wireless networks. Further, we study the algorithmic aspects of the above questions: the goal is to design provably good algorithms for arbitrary instances. We develop analytical performance evaluation models and distributed algorithms for routing and scheduling which incorporate fairness, energy and dilation (path-length) requirements and provide a unified framework for utilizing the network close to its maximum throughput capacity.Motivated by certain popular wireless protocols used in practice, we also explore "shortest-path like" path selection strategies which maximize the network throughput. The theoretical results naturally suggest an interesting class of congestion aware link metrics which can be directly plugged into</i> several existing routing protocols such as AODV, DSR, etc. We complement the theoretical analysis with extensive simulations. The results indicate that routes obtained using our congestion aware link metrics consistently yield higher throughput than hop-count based shortest path metrics.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064228},
 doi = {http://doi.acm.org/10.1145/1064212.1064228},
 acmid = {1064228},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity modeling, end-to-end scheduling, linear programming, wireless networks},
} 

@article{Kumar:2005:AAC:1071690.1064228,
 author = {Kumar, V. S. Anil and Marathe, Madhav V. and Parthasarathy, Srinivasan and Srinivasan, Aravind},
 title = {Algorithmic aspects of capacity in wireless networks},
 abstract = {This paper considers two inter-related questions: (i) Given a wireless ad-hoc network and a collection of source-destination pairs (s<inf>i</inf>,t<inf>i</inf></i>), what is the maximum throughput capacity of the network, i.e. the rate at which data from the sources to their corresponding destinations can be transferred in the network? (ii) Can network protocols be designed that jointly route the packets and schedule transmissions at rates close to the maximum throughput capacity? Much of the earlier work focused on random instances and proved analytical lower and upper bounds on the maximum throughput capacity. Here, in contrast, we consider arbitrary wireless networks. Further, we study the algorithmic aspects of the above questions: the goal is to design provably good algorithms for arbitrary instances. We develop analytical performance evaluation models and distributed algorithms for routing and scheduling which incorporate fairness, energy and dilation (path-length) requirements and provide a unified framework for utilizing the network close to its maximum throughput capacity.Motivated by certain popular wireless protocols used in practice, we also explore "shortest-path like" path selection strategies which maximize the network throughput. The theoretical results naturally suggest an interesting class of congestion aware link metrics which can be directly plugged into</i> several existing routing protocols such as AODV, DSR, etc. We complement the theoretical analysis with extensive simulations. The results indicate that routes obtained using our congestion aware link metrics consistently yield higher throughput than hop-count based shortest path metrics.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064228},
 doi = {http://doi.acm.org/10.1145/1071690.1064228},
 acmid = {1064228},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity modeling, end-to-end scheduling, linear programming, wireless networks},
} 

@inproceedings{Chen:2005:EEM:1064212.1064230,
 author = {Chen, Zhifeng and Zhang, Yan and Zhou, Yuanyuan and Scott, Heidi and Schiefer, Berni},
 title = {Empirical evaluation of multi-level buffer cache collaboration for storage systems},
 abstract = {To bridge the increasing processor-disk performance gap, buffer caches are used in both storage clients (e.g. database systems) and storage servers to reduce the number of slow disk accesses. These buffer caches need to be managed effectively to deliver the performance commensurate to the aggregate buffer cache size. To address this problem, two paradigms have been proposed recently to collaboratively</i> manage these buffer caches together: the <b>hierarchy-aware caching</i></b> maintains the same <b>I/O</b> interface and is fully transparent to the storage client software, and the <b>aggressively-collaborative caching</i></b> trades off transparency for performance and requires changes to both the interface and the storage client software. Before storage industry starts to implement collaborative caching in real systems, it is crucial to find out whether sacrificing transparency is really worthwhile, i.e., how much can we gain by using the aggressively-collaborative caching instead of the hierarchy-aware caching? To accurately answer this question, it is required to consider all possible combinations of recently proposed local replacement algorithms and optimization techniques in both collaboration paradigms.Our study provides an empirical evaluation to address the above questions. Particularly, we have compared three aggressively-collaborative approaches with two hierarchy-aware approaches for four different types of database/file <b>I/O</b> workloads using traces collected from real commercial systems such as <b>IBM DB2</b>. More importantly, we separate the effects of collaborative caching from local replacement algorithms and optimizations, and uniformly apply several recently proposed local replacement algorithms and optimizations to all five collaboration approaches.When appropriate local optimizations and replacement algorithms are uniformly applied to both hierarchy-aware and aggressively-collaborative caching, the results indicate that hierarchy-aware caching can deliver similar performance as aggressively-collaborative caching. The results show that the aggressively-collaborative caching only provides less than 2.5\% performance improvement on average in simulation and 1.0\% in real system experiments over the hierarchy-aware caching for most workloads and cache configurations. Our sensitivity study indicates that the performance gain of aggressively-collaborative caching is also very small for various storage networks and different cache configurations. Therefore, considering its simplicity and generality, hierarchy-aware caching is more feasible than aggressively-collaborative caching.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064230},
 doi = {http://doi.acm.org/10.1145/1064212.1064230},
 acmid = {1064230},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative caching, database, file system, storage system},
} 

@article{Chen:2005:EEM:1071690.1064230,
 author = {Chen, Zhifeng and Zhang, Yan and Zhou, Yuanyuan and Scott, Heidi and Schiefer, Berni},
 title = {Empirical evaluation of multi-level buffer cache collaboration for storage systems},
 abstract = {To bridge the increasing processor-disk performance gap, buffer caches are used in both storage clients (e.g. database systems) and storage servers to reduce the number of slow disk accesses. These buffer caches need to be managed effectively to deliver the performance commensurate to the aggregate buffer cache size. To address this problem, two paradigms have been proposed recently to collaboratively</i> manage these buffer caches together: the <b>hierarchy-aware caching</i></b> maintains the same <b>I/O</b> interface and is fully transparent to the storage client software, and the <b>aggressively-collaborative caching</i></b> trades off transparency for performance and requires changes to both the interface and the storage client software. Before storage industry starts to implement collaborative caching in real systems, it is crucial to find out whether sacrificing transparency is really worthwhile, i.e., how much can we gain by using the aggressively-collaborative caching instead of the hierarchy-aware caching? To accurately answer this question, it is required to consider all possible combinations of recently proposed local replacement algorithms and optimization techniques in both collaboration paradigms.Our study provides an empirical evaluation to address the above questions. Particularly, we have compared three aggressively-collaborative approaches with two hierarchy-aware approaches for four different types of database/file <b>I/O</b> workloads using traces collected from real commercial systems such as <b>IBM DB2</b>. More importantly, we separate the effects of collaborative caching from local replacement algorithms and optimizations, and uniformly apply several recently proposed local replacement algorithms and optimizations to all five collaboration approaches.When appropriate local optimizations and replacement algorithms are uniformly applied to both hierarchy-aware and aggressively-collaborative caching, the results indicate that hierarchy-aware caching can deliver similar performance as aggressively-collaborative caching. The results show that the aggressively-collaborative caching only provides less than 2.5\% performance improvement on average in simulation and 1.0\% in real system experiments over the hierarchy-aware caching for most workloads and cache configurations. Our sensitivity study indicates that the performance gain of aggressively-collaborative caching is also very small for various storage networks and different cache configurations. Therefore, considering its simplicity and generality, hierarchy-aware caching is more feasible than aggressively-collaborative caching.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064230},
 doi = {http://doi.acm.org/10.1145/1071690.1064230},
 acmid = {1064230},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative caching, database, file system, storage system},
} 

@inproceedings{Butt:2005:PIK:1064212.1064231,
 author = {Butt, Ali R. and Gniady, Chris and Hu, Y. Charlie},
 title = {The performance impact of kernel prefetching on buffer cache replacement algorithms},
 abstract = {A fundamental challenge in improving the file system performance is to design effective block replacement algorithms to minimize buffer cache misses. Despite the well-known interactions between prefetching and caching, almost all buffer cache replacement algorithms have been proposed and studied comparatively without taking into account file system prefetching which exists in all modern operating systems. This paper shows that such kernel prefetching can have a significant impact on the relative performance in terms of the number of actual disk I/Os of many well-known replacement algorithms; it can not only narrow the performance gap but also change the relative performance benefits of different algorithms. These results demonstrate the importance for buffer caching research to take file system prefetching into consideration.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064231},
 doi = {http://doi.acm.org/10.1145/1064212.1064231},
 acmid = {1064231},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer caching, prefetching, replacement algorithms},
} 

@article{Butt:2005:PIK:1071690.1064231,
 author = {Butt, Ali R. and Gniady, Chris and Hu, Y. Charlie},
 title = {The performance impact of kernel prefetching on buffer cache replacement algorithms},
 abstract = {A fundamental challenge in improving the file system performance is to design effective block replacement algorithms to minimize buffer cache misses. Despite the well-known interactions between prefetching and caching, almost all buffer cache replacement algorithms have been proposed and studied comparatively without taking into account file system prefetching which exists in all modern operating systems. This paper shows that such kernel prefetching can have a significant impact on the relative performance in terms of the number of actual disk I/Os of many well-known replacement algorithms; it can not only narrow the performance gap but also change the relative performance benefits of different algorithms. These results demonstrate the importance for buffer caching research to take file system prefetching into consideration.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064231},
 doi = {http://doi.acm.org/10.1145/1071690.1064231},
 acmid = {1064231},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer caching, prefetching, replacement algorithms},
} 

@inproceedings{Berg:2005:FDP:1064212.1064232,
 author = {Berg, Erik and Hagersten, Erik},
 title = {Fast data-locality profiling of native execution},
 abstract = {Performance tools based on hardware counters can efficiently profile the cache behavior of an application and help software developers improve its cache utilization. Simulator-based tools can potentially provide more insights and flexibility and model many different cache configurations, but have the drawback of large run-time overhead.We present StatCache, a performance tool based on a statistical cache model. It has a small run-time overhead while providing much of the flexibility of simulator-based tools. A monitor process running in the background collects sparse memory access statistics about the analyzed application running natively on a host computer. Generic locality information is derived and presented in a code-centric and/or data-centric view.We evaluate the accuracy and performance of the tool using ten SPEC CPU2000 benchmarks. We also exemplify how the flexibility of the tool can be used to better understand the characteristics of cache-related performance problems.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064232},
 doi = {http://doi.acm.org/10.1145/1064212.1064232},
 acmid = {1064232},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache behavior, profiling tool},
} 

@article{Berg:2005:FDP:1071690.1064232,
 author = {Berg, Erik and Hagersten, Erik},
 title = {Fast data-locality profiling of native execution},
 abstract = {Performance tools based on hardware counters can efficiently profile the cache behavior of an application and help software developers improve its cache utilization. Simulator-based tools can potentially provide more insights and flexibility and model many different cache configurations, but have the drawback of large run-time overhead.We present StatCache, a performance tool based on a statistical cache model. It has a small run-time overhead while providing much of the flexibility of simulator-based tools. A monitor process running in the background collects sparse memory access statistics about the analyzed application running natively on a host computer. Generic locality information is derived and presented in a code-centric and/or data-centric view.We evaluate the accuracy and performance of the tool using ten SPEC CPU2000 benchmarks. We also exemplify how the flexibility of the tool can be used to better understand the characteristics of cache-related performance problems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064232},
 doi = {http://doi.acm.org/10.1145/1071690.1064232},
 acmid = {1064232},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache behavior, profiling tool},
} 

@inproceedings{Yotov:2005:AMM:1064212.1064233,
 author = {Yotov, Kamen and Pingali, Keshav and Stodghill, Paul},
 title = {Automatic measurement of memory hierarchy parameters},
 abstract = {The running time of many applications is dominated by the cost of memory operations. To optimize such applications for a given platform, it is necessary to have a detailed knowledge of the memory hierarchy parameters of that platform. In practice, this information is poorly documented if at all. Moreover, there is growing interest in self-tuning, autonomic software systems that can optimize themselves for different platforms; these systems must determine memory hierarchy parameters automatically without human intervention.One solution is to use micro-benchmarks to determine the parameters of the memory hierarchy. In this paper, we argue that existing micro-benchmarks are inadequate, and present novel micro-benchmarks for determining parameters of all levels of the memory hierarchy, including registers, all data caches and the translation look-aside buffer. We have implemented these micro-benchmarks in a tool called X-Ray that can be ported easily to new platforms. We present experimental results that show that X-Ray successfully determines memory hierarchy parameters on current platforms, and compare its accuracy with that of existing tools.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064233},
 doi = {http://doi.acm.org/10.1145/1064212.1064233},
 acmid = {1064233},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomic systems, caches, hardware parameters, measurement, memory hierarchy, micro-benchmarks, optimization, self-tuning},
} 

@article{Yotov:2005:AMM:1071690.1064233,
 author = {Yotov, Kamen and Pingali, Keshav and Stodghill, Paul},
 title = {Automatic measurement of memory hierarchy parameters},
 abstract = {The running time of many applications is dominated by the cost of memory operations. To optimize such applications for a given platform, it is necessary to have a detailed knowledge of the memory hierarchy parameters of that platform. In practice, this information is poorly documented if at all. Moreover, there is growing interest in self-tuning, autonomic software systems that can optimize themselves for different platforms; these systems must determine memory hierarchy parameters automatically without human intervention.One solution is to use micro-benchmarks to determine the parameters of the memory hierarchy. In this paper, we argue that existing micro-benchmarks are inadequate, and present novel micro-benchmarks for determining parameters of all levels of the memory hierarchy, including registers, all data caches and the translation look-aside buffer. We have implemented these micro-benchmarks in a tool called X-Ray that can be ported easily to new platforms. We present experimental results that show that X-Ray successfully determines memory hierarchy parameters on current platforms, and compare its accuracy with that of existing tools.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064233},
 doi = {http://doi.acm.org/10.1145/1071690.1064233},
 acmid = {1064233},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomic systems, caches, hardware parameters, measurement, memory hierarchy, micro-benchmarks, optimization, self-tuning},
} 

@inproceedings{Jonckheere:2005:OIR:1064212.1064235,
 author = {Jonckheere, M. and Virtamo, J.},
 title = {Optimal insensitive routing and bandwidth sharing in simple data networks},
 abstract = {Many communication systems can be efficiently modelled using queueing networks with a stationary distribution that is insensitive to detailed traffic characteristics and depends on arrival rates and mean service requirements only. This robustness enables simple engineering rules and is thus of considerable practical interest. In this paper we extend previous results by relaxing the usual assumption of static routing and balanced service rates to account for both dynamic capacity allocation and dynamic load balancing. This relaxation is necessary to model systems like grid computing, for instance. Our results identify joint dynamic allocation and routing policies for single input reversible networks that are optimal for a wide range of performance metrics. A simple two-pass algorithm is presented for finding the optimal policy. The derived analytical results are applied in a number of simple numerical examples that illustrate their modelling potential.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064235},
 doi = {http://doi.acm.org/10.1145/1064212.1064235},
 acmid = {1064235},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bandwidth allocation, insensitivity, joint optimization, routing},
} 

@article{Jonckheere:2005:OIR:1071690.1064235,
 author = {Jonckheere, M. and Virtamo, J.},
 title = {Optimal insensitive routing and bandwidth sharing in simple data networks},
 abstract = {Many communication systems can be efficiently modelled using queueing networks with a stationary distribution that is insensitive to detailed traffic characteristics and depends on arrival rates and mean service requirements only. This robustness enables simple engineering rules and is thus of considerable practical interest. In this paper we extend previous results by relaxing the usual assumption of static routing and balanced service rates to account for both dynamic capacity allocation and dynamic load balancing. This relaxation is necessary to model systems like grid computing, for instance. Our results identify joint dynamic allocation and routing policies for single input reversible networks that are optimal for a wide range of performance metrics. A simple two-pass algorithm is presented for finding the optimal policy. The derived analytical results are applied in a number of simple numerical examples that illustrate their modelling potential.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064235},
 doi = {http://doi.acm.org/10.1145/1071690.1064235},
 acmid = {1064235},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bandwidth allocation, insensitivity, joint optimization, routing},
} 

@inproceedings{Wierman:2005:NIB:1064212.1064236,
 author = {Wierman, Adam and Harchol-Balter, Mor and Osogami, Takayuki},
 title = {Nearly insensitive bounds on SMART scheduling},
 abstract = {We define the class of SMART scheduling policies. These are policies that bias towards jobs with small remaining service times, jobs with small original sizes, or both, with the motivation of minimizing mean response time and/or mean slowdown. Examples of SMART policies include PSJF, SRPT, and hybrid policies such as RS (which biases according to the product of the remaining size and the original size of a job).For many policies in the SMART class, the mean response time and mean slowdown are not known or have complex representations involving multiple nested integrals, making evaluation difficult. In this work, we prove three main results. First, for all policies in the SMART class, we prove simple upper and lower bounds on mean response time. Second, we show that all policies in the SMART class, surprisingly, have very similar mean response times. Third, we show that the response times of SMART policies are largely insensitive to the variability of the job size distribution. In particular, we focus on the SRPT and PSJF policies and prove insensitive bounds in these cases.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064236},
 doi = {http://doi.acm.org/10.1145/1064212.1064236},
 acmid = {1064236},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, PS, PSJF, SMART, SRPT, preemptive shortest job first, processor sharing, response time, scheduling, shortest remaining processing time},
} 

@article{Wierman:2005:NIB:1071690.1064236,
 author = {Wierman, Adam and Harchol-Balter, Mor and Osogami, Takayuki},
 title = {Nearly insensitive bounds on SMART scheduling},
 abstract = {We define the class of SMART scheduling policies. These are policies that bias towards jobs with small remaining service times, jobs with small original sizes, or both, with the motivation of minimizing mean response time and/or mean slowdown. Examples of SMART policies include PSJF, SRPT, and hybrid policies such as RS (which biases according to the product of the remaining size and the original size of a job).For many policies in the SMART class, the mean response time and mean slowdown are not known or have complex representations involving multiple nested integrals, making evaluation difficult. In this work, we prove three main results. First, for all policies in the SMART class, we prove simple upper and lower bounds on mean response time. Second, we show that all policies in the SMART class, surprisingly, have very similar mean response times. Third, we show that the response times of SMART policies are largely insensitive to the variability of the job size distribution. In particular, we focus on the SRPT and PSJF policies and prove insensitive bounds in these cases.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064236},
 doi = {http://doi.acm.org/10.1145/1071690.1064236},
 acmid = {1064236},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1, PS, PSJF, SMART, SRPT, preemptive shortest job first, processor sharing, response time, scheduling, shortest remaining processing time},
} 

@inproceedings{Kortebi:2005:ENA:1064212.1064237,
 author = {Kortebi, A. and Muscariello, L. and Oueslati, S. and Roberts, J.},
 title = {Evaluating the number of active flows in a scheduler realizing fair statistical bandwidth sharing},
 abstract = {Despite its well-known advantages, per-flow fair queueing has not been deployed in the Internet mainly because of the common belief that such scheduling is not scalable. The objective of the present paper is to demonstrate using trace simulations and analytical evaluations that this belief is misguided. We show that although the number of flows in progress</i> increases with link speed, the number that needs scheduling at any moment is largely independent of this rate. The number of such active</i> flows is a random process typically measured in hundreds even though there may be tens of thousands of flows in progress. The simulations are performed using traces from commercial and research networks with quite different traffic characteristics. Analysis is based on models for balanced fair statistical bandwidth sharing and applies properties of queue busy periods to explain the observed behaviour.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064237},
 doi = {http://doi.acm.org/10.1145/1064212.1064237},
 acmid = {1064237},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical traffic model, fair queueing, statistical bandwidth sharing, trace simulations},
} 

@article{Kortebi:2005:ENA:1071690.1064237,
 author = {Kortebi, A. and Muscariello, L. and Oueslati, S. and Roberts, J.},
 title = {Evaluating the number of active flows in a scheduler realizing fair statistical bandwidth sharing},
 abstract = {Despite its well-known advantages, per-flow fair queueing has not been deployed in the Internet mainly because of the common belief that such scheduling is not scalable. The objective of the present paper is to demonstrate using trace simulations and analytical evaluations that this belief is misguided. We show that although the number of flows in progress</i> increases with link speed, the number that needs scheduling at any moment is largely independent of this rate. The number of such active</i> flows is a random process typically measured in hundreds even though there may be tens of thousands of flows in progress. The simulations are performed using traces from commercial and research networks with quite different traffic characteristics. Analysis is based on models for balanced fair statistical bandwidth sharing and applies properties of queue busy periods to explain the observed behaviour.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064237},
 doi = {http://doi.acm.org/10.1145/1071690.1064237},
 acmid = {1064237},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical traffic model, fair queueing, statistical bandwidth sharing, trace simulations},
} 

@inproceedings{Wierman:2005:CSP:1064212.1064238,
 author = {Wierman, Adam and Harchol-Balter, Mor},
 title = {Classifying scheduling policies with respect to higher moments of conditional response time},
 abstract = {In addition to providing small mean response times, modern applications seek to provide users predictable service and, in some cases, Quality of Service (QoS) guarantees. In order to understand the predictability of response times under a range of scheduling policies, we study the conditional variance in response times seen by jobs of different sizes. We define a metric and a criterion that distinguish between contrasting functional behaviors of conditional variance, and we then classify large groups of scheduling policies.In addition to studying the conditional variance of response times, we also derive metrics appropriate for comparing higher conditional moments of response time across job sizes. We illustrate that common statistics such as raw and central moments are not appropriate when comparing higher conditional moments of response time. Instead, we find that cumulant moments should be used.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064238},
 doi = {http://doi.acm.org/10.1145/1064212.1064238},
 acmid = {1064238},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FB, LAS, M/G/1, PS, PSJF, SET, SRPT, cumulants, foreground-background, least attained service, predictability, processor sharing, response time, scheduling, shortest job first, shortest remaining processing time, variance},
} 

@article{Wierman:2005:CSP:1071690.1064238,
 author = {Wierman, Adam and Harchol-Balter, Mor},
 title = {Classifying scheduling policies with respect to higher moments of conditional response time},
 abstract = {In addition to providing small mean response times, modern applications seek to provide users predictable service and, in some cases, Quality of Service (QoS) guarantees. In order to understand the predictability of response times under a range of scheduling policies, we study the conditional variance in response times seen by jobs of different sizes. We define a metric and a criterion that distinguish between contrasting functional behaviors of conditional variance, and we then classify large groups of scheduling policies.In addition to studying the conditional variance of response times, we also derive metrics appropriate for comparing higher conditional moments of response time across job sizes. We illustrate that common statistics such as raw and central moments are not appropriate when comparing higher conditional moments of response time. Instead, we find that cumulant moments should be used.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064238},
 doi = {http://doi.acm.org/10.1145/1071690.1064238},
 acmid = {1064238},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FB, LAS, M/G/1, PS, PSJF, SET, SRPT, cumulants, foreground-background, least attained service, predictability, processor sharing, response time, scheduling, shortest job first, shortest remaining processing time, variance},
} 

@inproceedings{Jiang:2005:WIT:1064212.1064240,
 author = {Jiang, Hao and Dovrolis, Constantinos},
 title = {Why is the internet traffic bursty in short time scales?},
 abstract = {Internet traffic exhibits multifaceted burstiness and correlation structure over a wide span of time scales. Previous work analyzed this structure in terms of heavy-tailed session characteristics, as well as TCP timeouts and congestion avoidance, in relatively long time scales. We focus on shorter scales, typically less than 100-1000 milliseconds. Our objective is to identify the actual mechanisms that are responsible for creating bursty traffic in those scales. We show that TCP self-clocking, joint with queueing in the network, can shape the packet interarrivals of a TCP connection in a two-level ON-OFF pattern. This structure creates strong correlations and burstiness in time scales that extend up to the Round-Trip Time (RTT) of the connection. This effect is more important for bulk transfers that have a large bandwidth-delay product relative to their window size. Also, the aggregation of many flows, without rescaling their packet interarrivals, does not converge to a Poisson stream, as one might expect from classical superposition results. Instead, the burstiness in those scales can be significantly reduced by TCP pacing. In particular, we focus on the importance of the minimum pacing timer, and show that a 10-millisecond timer would be too coarse for removing short-scale traffic burstiness, while a 1-millisecond timer would be sufficient to make the traffic almost as smooth as a Poisson stream in sub-RTT scales.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064240},
 doi = {http://doi.acm.org/10.1145/1064212.1064240},
 acmid = {1064240},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ON-OFF model, TCP pacing, TCP self-clocking, burstiness, traffic modeling, wavelet-based multiresolution analysis},
} 

@article{Jiang:2005:WIT:1071690.1064240,
 author = {Jiang, Hao and Dovrolis, Constantinos},
 title = {Why is the internet traffic bursty in short time scales?},
 abstract = {Internet traffic exhibits multifaceted burstiness and correlation structure over a wide span of time scales. Previous work analyzed this structure in terms of heavy-tailed session characteristics, as well as TCP timeouts and congestion avoidance, in relatively long time scales. We focus on shorter scales, typically less than 100-1000 milliseconds. Our objective is to identify the actual mechanisms that are responsible for creating bursty traffic in those scales. We show that TCP self-clocking, joint with queueing in the network, can shape the packet interarrivals of a TCP connection in a two-level ON-OFF pattern. This structure creates strong correlations and burstiness in time scales that extend up to the Round-Trip Time (RTT) of the connection. This effect is more important for bulk transfers that have a large bandwidth-delay product relative to their window size. Also, the aggregation of many flows, without rescaling their packet interarrivals, does not converge to a Poisson stream, as one might expect from classical superposition results. Instead, the burstiness in those scales can be significantly reduced by TCP pacing. In particular, we focus on the importance of the minimum pacing timer, and show that a 10-millisecond timer would be too coarse for removing short-scale traffic burstiness, while a 1-millisecond timer would be sufficient to make the traffic almost as smooth as a Poisson stream in sub-RTT scales.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064240},
 doi = {http://doi.acm.org/10.1145/1071690.1064240},
 acmid = {1064240},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ON-OFF model, TCP pacing, TCP self-clocking, burstiness, traffic modeling, wavelet-based multiresolution analysis},
} 

@article{Roughan:2005:FBA:1071690.1064241,
 author = {Roughan, Matthew},
 title = {Fundamental bounds on the accuracy of network performance measurements},
 abstract = {This paper considers the basic problem of "how accurate can we make Internet performance measurements". The answer is somewhat counter-intuitive in that there are bounds on the accuracy of such measurements, no matter how many probes we can use in a given time interval, and thus arises a type of Heisenberg inequality describing the bounds in our knowledge of the performance of a network. The results stem from the fact that we cannot make independent measurements of a system's performance: all such measures are correlated, and these correlations reduce the efficacy of measurements. The degree of correlation is also strongly dependent on system load. The result has important practical implications that reach beyond the design of Internet measurement experiments, into the design of network protocols.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064241},
 doi = {http://doi.acm.org/10.1145/1071690.1064241},
 acmid = {1064241},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error estimation, internet measurement, load balancing, measurement planning, network performance},
} 

@inproceedings{Roughan:2005:FBA:1064212.1064241,
 author = {Roughan, Matthew},
 title = {Fundamental bounds on the accuracy of network performance measurements},
 abstract = {This paper considers the basic problem of "how accurate can we make Internet performance measurements". The answer is somewhat counter-intuitive in that there are bounds on the accuracy of such measurements, no matter how many probes we can use in a given time interval, and thus arises a type of Heisenberg inequality describing the bounds in our knowledge of the performance of a network. The results stem from the fact that we cannot make independent measurements of a system's performance: all such measures are correlated, and these correlations reduce the efficacy of measurements. The degree of correlation is also strongly dependent on system load. The result has important practical implications that reach beyond the design of Internet measurement experiments, into the design of network protocols.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {253--264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064241},
 doi = {http://doi.acm.org/10.1145/1064212.1064241},
 acmid = {1064241},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {error estimation, internet measurement, load balancing, measurement planning, network performance},
} 

@inproceedings{Jain:2005:EEA:1064212.1064242,
 author = {Jain, Manish and Dovrolis, Constantinos},
 title = {End-to-end estimation of the available bandwidth variation range},
 abstract = {The available bandwidth (avail-bw) of a network path is an important performance metric and its end-to-end estimation has recently received significant attention. Previous work focused on the estimation of the average avail-bw, ignoring the significant variability of this metric in different time scales. In this paper, we show how to estimate a given percentile of the avail-bw distribution at a user-specified time scale. If two estimated percentiles cover the bulk of the distribution (say 10\% to 90\%), the user can obtain a practical estimate for the avail-bw variation range. We present two estimation techniques. The first is iterative and non-parametric, meaning that it is more appropriate for very short time scales (typically less than 100ms), or in bottlenecks with limited flow multiplexing (where the avail-bw distribution may be non-Gaussian). The second technique is parametric, because it assumes that the avail-bw follows the Gaussian distribution, and it can produce an estimate faster because it is not iterative. The two techniques have been implemented in a measurement tool called Pathvar. Pathvar can track the avail-bw variation range within 10-20\%, even under non-stationary conditions. Finally, we identify four factors that play a crucial role in the variation range of the avail-bw: traffic load, number of competing flows, rate of competing flows, and of course the measurement time scale.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064242},
 doi = {http://doi.acm.org/10.1145/1064212.1064242},
 acmid = {1064242},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active measurement, bandwidth estimation, network measurement tools, pathvar, traffic variability},
} 

@article{Jain:2005:EEA:1071690.1064242,
 author = {Jain, Manish and Dovrolis, Constantinos},
 title = {End-to-end estimation of the available bandwidth variation range},
 abstract = {The available bandwidth (avail-bw) of a network path is an important performance metric and its end-to-end estimation has recently received significant attention. Previous work focused on the estimation of the average avail-bw, ignoring the significant variability of this metric in different time scales. In this paper, we show how to estimate a given percentile of the avail-bw distribution at a user-specified time scale. If two estimated percentiles cover the bulk of the distribution (say 10\% to 90\%), the user can obtain a practical estimate for the avail-bw variation range. We present two estimation techniques. The first is iterative and non-parametric, meaning that it is more appropriate for very short time scales (typically less than 100ms), or in bottlenecks with limited flow multiplexing (where the avail-bw distribution may be non-Gaussian). The second technique is parametric, because it assumes that the avail-bw follows the Gaussian distribution, and it can produce an estimate faster because it is not iterative. The two techniques have been implemented in a measurement tool called Pathvar. Pathvar can track the avail-bw variation range within 10-20\%, even under non-stationary conditions. Finally, we identify four factors that play a crucial role in the variation range of the avail-bw: traffic load, number of competing flows, rate of competing flows, and of course the measurement time scale.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {265--276},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064242},
 doi = {http://doi.acm.org/10.1145/1071690.1064242},
 acmid = {1064242},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active measurement, bandwidth estimation, network measurement tools, pathvar, traffic variability},
} 

@article{Chiang:2005:NUM:1071690.1064246,
 author = {Chiang, Mung and Lee, J. W. and Calderbank, R. and Palomar, D. and Fazel, M.},
 title = {Network utility maximization with nonconcave, coupled, and reliability-based uilities},
 abstract = {Network Utility Maximization (NUM) has significantly extended the classical network flow problem and provided an emerging framework to design resource allocation algorithms such as TCP congestion control and to understand layering as optimization decomposition. We present a summary of very recent results in the theory and applications of NUM. We show new distributed algorithms that converge to the globally optimal rate allocation for NUM problems with nonconcave utility functions representing inelastic flows, with coupled utility functions representing interference effects or hybrid social-selfish utilities, and with rate-reliability tradeoff through adaptive channel coding in the physical layer. We conclude by discussing how do different decompositions of a generalized NUM problem correspond to different layering architectures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064246},
 doi = {http://doi.acm.org/10.1145/1071690.1064246},
 acmid = {1064246},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiang:2005:NUM:1064212.1064246,
 author = {Chiang, Mung and Lee, J. W. and Calderbank, R. and Palomar, D. and Fazel, M.},
 title = {Network utility maximization with nonconcave, coupled, and reliability-based uilities},
 abstract = {Network Utility Maximization (NUM) has significantly extended the classical network flow problem and provided an emerging framework to design resource allocation algorithms such as TCP congestion control and to understand layering as optimization decomposition. We present a summary of very recent results in the theory and applications of NUM. We show new distributed algorithms that converge to the globally optimal rate allocation for NUM problems with nonconcave utility functions representing inelastic flows, with coupled utility functions representing interference effects or hybrid social-selfish utilities, and with rate-reliability tradeoff through adaptive channel coding in the physical layer. We conclude by discussing how do different decompositions of a generalized NUM problem correspond to different layering architectures.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064246},
 doi = {http://doi.acm.org/10.1145/1064212.1064246},
 acmid = {1064246},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Low:2005:OMI:1071690.1064245,
 author = {Low, Steven and Doyle, J. and Li, L. and Tang, A. and Wang, J.},
 title = {Optimization model of internet protocols},
 abstract = {Layered architecture is one of the most fundamental and influential structures of network design. Can we integrate the various protocol layers into a single coherent theory by regarding them as carrying out an asynchronous distributed primal-dual computation over the network to implicitly solve a global optimization problem? Different layers iterate on different subsets of the decision variables using local information to achieve individual optimalities, but taken together, these local algorithms attempt to achieve a global objective. Such a theory will expose the interconnection between protocol layers and can be used to study rigorously the performance tradeoff in protocol layering as different ways to distribute a centralized computation. In this talk, we describe some preliminary work towards this goal and discuss some of the difficulties of this approach.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064245},
 doi = {http://doi.acm.org/10.1145/1071690.1064245},
 acmid = {1064245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Low:2005:OMI:1064212.1064245,
 author = {Low, Steven and Doyle, J. and Li, L. and Tang, A. and Wang, J.},
 title = {Optimization model of internet protocols},
 abstract = {Layered architecture is one of the most fundamental and influential structures of network design. Can we integrate the various protocol layers into a single coherent theory by regarding them as carrying out an asynchronous distributed primal-dual computation over the network to implicitly solve a global optimization problem? Different layers iterate on different subsets of the decision variables using local information to achieve individual optimalities, but taken together, these local algorithms attempt to achieve a global objective. Such a theory will expose the interconnection between protocol layers and can be used to study rigorously the performance tradeoff in protocol layering as different ways to distribute a centralized computation. In this talk, we describe some preliminary work towards this goal and discuss some of the difficulties of this approach.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064245},
 doi = {http://doi.acm.org/10.1145/1064212.1064245},
 acmid = {1064245},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chiang:2005:OCC:1071690.1064244,
 author = {Chiang, Mung and Low, Steven},
 title = {Optimization and Control of Communication Networks},
 abstract = {Recently, there has been a surge in research activities that utilize the power of recent developments in nonlinear optimization to tackle a wide scope of work in the analysis and design of communication systems, touching every layer of the layered network architecture, and resulting in both intellectual and practical impacts significantly beyond the earlier frameworks. These research activities are driven by both new demands in the areas of communications and networking, and new tools emerging from optimization theory. Such tools include new developments of powerful theories and highly efficient computational algorithms for nonlinear convex optimization, as well as global solution methods and relaxation techniques for nonconvex optimization.Optimization theory can be used to analyze, interpret, or design a communication system, for both forward-engineering and reverse-engineering. Over the last few years, it has been successfully applied to a wide range of communication systems, from the high speed Internet core to wireless networks, from coding and equalization to broadband access, and from information theory to network topology models. Some of the theoretical advances have also been put into practice and started making visible impacts, including new versions of TCP congestion control, power control and scheduling algorithms in wireless networks, and spectrum management in DSL broadband access networks.Under the theme of optimization and control of communication networks, this Hot Topic Session consists of five invited talks covering a wide range of issues, including protocols, pricing, resource allocation, cross layer design, traffic engineering in the Internet, optical transport networks, and wireless networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064244},
 doi = {http://doi.acm.org/10.1145/1071690.1064244},
 acmid = {1064244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiang:2005:OCC:1064212.1064244,
 author = {Chiang, Mung and Low, Steven},
 title = {Optimization and Control of Communication Networks},
 abstract = {Recently, there has been a surge in research activities that utilize the power of recent developments in nonlinear optimization to tackle a wide scope of work in the analysis and design of communication systems, touching every layer of the layered network architecture, and resulting in both intellectual and practical impacts significantly beyond the earlier frameworks. These research activities are driven by both new demands in the areas of communications and networking, and new tools emerging from optimization theory. Such tools include new developments of powerful theories and highly efficient computational algorithms for nonlinear convex optimization, as well as global solution methods and relaxation techniques for nonconvex optimization.Optimization theory can be used to analyze, interpret, or design a communication system, for both forward-engineering and reverse-engineering. Over the last few years, it has been successfully applied to a wide range of communication systems, from the high speed Internet core to wireless networks, from coding and equalization to broadband access, and from information theory to network topology models. Some of the theoretical advances have also been put into practice and started making visible impacts, including new versions of TCP congestion control, power control and scheduling algorithms in wireless networks, and spectrum management in DSL broadband access networks.Under the theme of optimization and control of communication networks, this Hot Topic Session consists of five invited talks covering a wide range of issues, including protocols, pricing, resource allocation, cross layer design, traffic engineering in the Internet, optical transport networks, and wireless networks.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064244},
 doi = {http://doi.acm.org/10.1145/1064212.1064244},
 acmid = {1064244},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shroff:2005:OBA:1064212.1064248,
 author = {Shroff, Ness and Lin, Xiaojun},
 title = {An optimization based approach for cross-layer design in wireless communication networks},
 abstract = {In this talk we study the issue of cross-layer design for rate control in multihop wireless networks. We have developed an optimal cross-layered rate control scheme that jointly computes both the rate allocation and the stabilizing schedule that controls the resources at the underlying layers. However, the scheduling component in this optimal cross-layered rate control scheme has to solve a complex global optimization problem at each time, and is hence too computationally expensive for online implementation. Thus, we study the impact on the performance of cross-layer rate control if the network can only use an imperfect (and potentially distributed) scheduling component that is easier to implement. We study scenarios with both fixed number of users as well as when the number of users change due to arrivals and departures in the system. In each case, we establish desirable results on the performance bounds of cross-layered rate control with imperfect scheduling. Our cross-layered approach provides provably better performance bounds when compared with a layered approach (that does not design rate control and scheduling together). The insights drawn from our analyses also enable us to design a fully distributed cross-layered rate control and scheduling algorithm under a restrictive interference model.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {278--278},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064248},
 doi = {http://doi.acm.org/10.1145/1064212.1064248},
 acmid = {1064248},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shroff:2005:OBA:1071690.1064248,
 author = {Shroff, Ness and Lin, Xiaojun},
 title = {An optimization based approach for cross-layer design in wireless communication networks},
 abstract = {In this talk we study the issue of cross-layer design for rate control in multihop wireless networks. We have developed an optimal cross-layered rate control scheme that jointly computes both the rate allocation and the stabilizing schedule that controls the resources at the underlying layers. However, the scheduling component in this optimal cross-layered rate control scheme has to solve a complex global optimization problem at each time, and is hence too computationally expensive for online implementation. Thus, we study the impact on the performance of cross-layer rate control if the network can only use an imperfect (and potentially distributed) scheduling component that is easier to implement. We study scenarios with both fixed number of users as well as when the number of users change due to arrivals and departures in the system. In each case, we establish desirable results on the performance bounds of cross-layered rate control with imperfect scheduling. Our cross-layered approach provides provably better performance bounds when compared with a layered approach (that does not design rate control and scheduling together). The insights drawn from our analyses also enable us to design a fully distributed cross-layered rate control and scheduling algorithm under a restrictive interference model.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {278--278},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064248},
 doi = {http://doi.acm.org/10.1145/1071690.1064248},
 acmid = {1064248},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Musacchio:2005:AFR:1064212.1064249,
 author = {Musacchio, John and Walrand, Jean},
 title = {Achieving fair rates with ingress policing},
 abstract = {We study a simple ingress policing scheme for a stochastic queuing network that uses a round-robin service discipline, and derive conditions under which the flow rates approach a max-min fair share allocation. The scheme works as follows: Whenever any of a flow's queues exceeds a policing threshold, the network discards that flow's arriving packets at the network ingress, and does so until all of that flow's queues fall below their thresholds. To prove our results, we use previously known results relating the stability of a queuing system to the stability of its fluid limit and extend these results to relate the flow rates of the stochastic system to those of a corresponding fluid model. In particular, we consider the fluid limit of a sequence of queuing networks with increasing thresholds. Using a Lyapunov function derived from the fluid limits, we find that as the policing thresholds are increased the state of the stochastic system is attracted to a relatively smaller and smaller neighborhood surrounding the equilibrium of the fluid model. We then show how this property implies that the achieved flow rates approach the max-min rates predicted by the fluid model.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {278--278},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064249},
 doi = {http://doi.acm.org/10.1145/1064212.1064249},
 acmid = {1064249},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Musacchio:2005:AFR:1071690.1064249,
 author = {Musacchio, John and Walrand, Jean},
 title = {Achieving fair rates with ingress policing},
 abstract = {We study a simple ingress policing scheme for a stochastic queuing network that uses a round-robin service discipline, and derive conditions under which the flow rates approach a max-min fair share allocation. The scheme works as follows: Whenever any of a flow's queues exceeds a policing threshold, the network discards that flow's arriving packets at the network ingress, and does so until all of that flow's queues fall below their thresholds. To prove our results, we use previously known results relating the stability of a queuing system to the stability of its fluid limit and extend these results to relate the flow rates of the stochastic system to those of a corresponding fluid model. In particular, we consider the fluid limit of a sequence of queuing networks with increasing thresholds. Using a Lyapunov function derived from the fluid limits, we find that as the policing thresholds are increased the state of the stochastic system is attracted to a relatively smaller and smaller neighborhood surrounding the equilibrium of the fluid model. We then show how this property implies that the achieved flow rates approach the max-min rates predicted by the fluid model.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {278--278},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064249},
 doi = {http://doi.acm.org/10.1145/1071690.1064249},
 acmid = {1064249},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mitra:2005:JPD:1064212.1064247,
 author = {Mitra, Debasis},
 title = {Joint pricing-network design and stochastic traffic engineering to manage demand uncertainty},
 abstract = {I will describe two networking models, together with their optimization techniques, that span several time scales. In the longest time scale, where the goal is capacity planning, I will describe the work of Bienstock, Raskina, Saniee and Wang that considers joint pricing and network design of optical transport networks. Technological innovations are yielding sharply decreasing unit costs. There is also empirical evidence that suggests that the elasticity of bandwidth demand to price is high. Integrating these features in a unified profit-maximizing model leads to a large- scale nonlinear optimization problem. In this work, efficient solution techniques are developed to maximize the carrier's net present value with respect to pricing strategies and investment decisions for technology acquisitions. In the work of Mitra and Wang the time scale is shorter, the network infrastructure is fixed, and a model for stochastic traffic engineering is given in which the optimization is with respect to bandwidth provisioning and route selection. Traffic demands are uncertain, and the objective is to maximize a risk-adjusted measure of network revenue that is generated by serving demands. Considerable attention is given to the appropriate measure of risk in the network model. Risk-mitigation strategies are also advanced. The optimization model, which is based on mean-risk analysis, enables a service provider to maximize a combined measure of mean revenue and revenue risk. The conditions under which the optimization problem is an instance of convex programming are obtained. The solution is shown to satisfy the stochastic efficiency criterion asymptotically. The efficient frontier, which is the set of Pareto optimal pairs of mean revenue and revenue risk, is obtained.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {278--278},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064212.1064247},
 doi = {http://doi.acm.org/10.1145/1064212.1064247},
 acmid = {1064247},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mitra:2005:JPD:1071690.1064247,
 author = {Mitra, Debasis},
 title = {Joint pricing-network design and stochastic traffic engineering to manage demand uncertainty},
 abstract = {I will describe two networking models, together with their optimization techniques, that span several time scales. In the longest time scale, where the goal is capacity planning, I will describe the work of Bienstock, Raskina, Saniee and Wang that considers joint pricing and network design of optical transport networks. Technological innovations are yielding sharply decreasing unit costs. There is also empirical evidence that suggests that the elasticity of bandwidth demand to price is high. Integrating these features in a unified profit-maximizing model leads to a large- scale nonlinear optimization problem. In this work, efficient solution techniques are developed to maximize the carrier's net present value with respect to pricing strategies and investment decisions for technology acquisitions. In the work of Mitra and Wang the time scale is shorter, the network infrastructure is fixed, and a model for stochastic traffic engineering is given in which the optimization is with respect to bandwidth provisioning and route selection. Traffic demands are uncertain, and the objective is to maximize a risk-adjusted measure of network revenue that is generated by serving demands. Considerable attention is given to the appropriate measure of risk in the network model. Risk-mitigation strategies are also advanced. The optimization model, which is based on mean-risk analysis, enables a service provider to maximize a combined measure of mean revenue and revenue risk. The conditions under which the optimization problem is an instance of convex programming are obtained. The solution is shown to satisfy the stochastic efficiency criterion asymptotically. The efficient frontier, which is the set of Pareto optimal pairs of mean revenue and revenue risk, is obtained.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {278--278},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1071690.1064247},
 doi = {http://doi.acm.org/10.1145/1071690.1064247},
 acmid = {1064247},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ciucu:2005:NSC:1064212.1064251,
 author = {Ciucu, Florin and Burchard, Almut and Liebeherr, J\"{o}rg},
 title = {A network service curve approach for the stochastic analysis of networks},
 abstract = {The stochastic network calculus is an evolving new methodology for backlog and delay analysis of networks that can account for statistical multiplexing gain. This paper advances the stochastic network calculus by deriving a network service curve, which expresses the service given to a flow by the network as a whole in terms of a probabilistic bound. The presented network service curve permits the calculation of statistical end-to-end delay and backlog bounds for broad classes of arrival and service distributions. The benefits of the derived service curve are illustrated for the exponentially bounded burstiness (EBB) traffic model. It is shown that end-to-end performance measures computed with a network service curve are bounded by O</i>(H</i>log H</i>), where H</i> is the number of nodes traversed by a flow. Using currently available techniques that compute end-to-end bounds by adding single node results, the corresponding performance measures are bounded by O</i>(H</i><sup>3</sup>).},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064251},
 doi = {http://doi.acm.org/10.1145/1064212.1064251},
 acmid = {1064251},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network service curve, quality-of-service, stochastic network calculus},
} 

@article{Ciucu:2005:NSC:1071690.1064251,
 author = {Ciucu, Florin and Burchard, Almut and Liebeherr, J\"{o}rg},
 title = {A network service curve approach for the stochastic analysis of networks},
 abstract = {The stochastic network calculus is an evolving new methodology for backlog and delay analysis of networks that can account for statistical multiplexing gain. This paper advances the stochastic network calculus by deriving a network service curve, which expresses the service given to a flow by the network as a whole in terms of a probabilistic bound. The presented network service curve permits the calculation of statistical end-to-end delay and backlog bounds for broad classes of arrival and service distributions. The benefits of the derived service curve are illustrated for the exponentially bounded burstiness (EBB) traffic model. It is shown that end-to-end performance measures computed with a network service curve are bounded by O</i>(H</i>log H</i>), where H</i> is the number of nodes traversed by a flow. Using currently available techniques that compute end-to-end bounds by adding single node results, the corresponding performance measures are bounded by O</i>(H</i><sup>3</sup>).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064251},
 doi = {http://doi.acm.org/10.1145/1071690.1064251},
 acmid = {1064251},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network service curve, quality-of-service, stochastic network calculus},
} 

@inproceedings{Urgaonkar:2005:AMM:1064212.1064252,
 author = {Urgaonkar, Bhuvan and Pacifici, Giovanni and Shenoy, Prashant and Spreitzer, Mike and Tantawi, Asser},
 title = {An analytical model for multi-tier internet services and its applications},
 abstract = {Since many Internet applications employ a multi-tier architecture, in this paper, we focus on the problem of analytically modeling the behavior of such applications. We present a model based on a network of queues, where the queues represent different tiers of the application. Our model is sufficiently general to capture (i) the behavior of tiers with significantly different performance characteristics and (ii) application idiosyncrasies such as session-based workloads, concurrency limits, and caching at intermediate tiers. We validate our model using real multi-tier applications running on a Linux server cluster. Our experiments indicate that our model faithfully captures the performance of these applications for a number of workloads and configurations. For a variety of scenarios, including those with caching at one of the application tiers, the average response times predicted by our model were within the 95\% confidence intervals of the observed average response times. Our experiments also demonstrate the utility of the model for dynamic capacity provisioning, performance prediction, bottleneck identification, and session policing. In one scenario, where the request arrival rate increased from less than 1500 to nearly 4200 requests/min, a dynamic provisioning technique employing our model was able to maintain response time targets by increasing the capacity of two of the application tiers by factors of 2 and 3.5, respectively.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {291--302},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064252},
 doi = {http://doi.acm.org/10.1145/1064212.1064252},
 acmid = {1064252},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MVA algorithm, internet application, queuing model},
} 

@article{Urgaonkar:2005:AMM:1071690.1064252,
 author = {Urgaonkar, Bhuvan and Pacifici, Giovanni and Shenoy, Prashant and Spreitzer, Mike and Tantawi, Asser},
 title = {An analytical model for multi-tier internet services and its applications},
 abstract = {Since many Internet applications employ a multi-tier architecture, in this paper, we focus on the problem of analytically modeling the behavior of such applications. We present a model based on a network of queues, where the queues represent different tiers of the application. Our model is sufficiently general to capture (i) the behavior of tiers with significantly different performance characteristics and (ii) application idiosyncrasies such as session-based workloads, concurrency limits, and caching at intermediate tiers. We validate our model using real multi-tier applications running on a Linux server cluster. Our experiments indicate that our model faithfully captures the performance of these applications for a number of workloads and configurations. For a variety of scenarios, including those with caching at one of the application tiers, the average response times predicted by our model were within the 95\% confidence intervals of the observed average response times. Our experiments also demonstrate the utility of the model for dynamic capacity provisioning, performance prediction, bottleneck identification, and session policing. In one scenario, where the request arrival rate increased from less than 1500 to nearly 4200 requests/min, a dynamic provisioning technique employing our model was able to maintain response time targets by increasing the capacity of two of the application tiers by factors of 2 and 3.5, respectively.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {291--302},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064252},
 doi = {http://doi.acm.org/10.1145/1071690.1064252},
 acmid = {1064252},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MVA algorithm, internet application, queuing model},
} 

@article{Chen:2005:MSE:1071690.1064253,
 author = {Chen, Yiyu and Das, Amitayu and Qin, Wubi and Sivasubramaniam, Anand and Wang, Qian and Gautam, Natarajan},
 title = {Managing server energy and operational costs in hosting centers},
 abstract = {The growing cost of tuning and managing computer systems is leading to out-sourcing of commercial services to hosting centers. These centers provision thousands of dense servers within a relatively small real-estate in order to host the applications/services of different customers who may have been assured by a service-level agreement (SLA). Power consumption of these servers is becoming a serious concern in the design and operation of the hosting centers. The effects of high power consumption manifest not only in the costs spent in designing effective cooling systems to ward off the generated heat, but in the cost of electricity consumption itself. It is crucial to deploy power management strategies in these hosting centers to lower these costs towards enhancing profitability. At the same time, techniques for power management that include shutting down these servers and/or modulating their operational speed, can impact the ability of the hosting center to meet SLAs. In addition, repeated on-off cycles can increase the wear-and-tear of server components, incurring costs for their procurement and replacement. This paper presents a formalism to this problem, and proposes three new online solution strategies based on steady state queuing analysis, feedback control theory, and a hybrid mechanism borrowing ideas from these two. Using real web server traces, we show that these solutions are more adaptive to workload behavior when performing server provisioning and speed control than earlier heuristics towards minimizing operational costs while meeting the SLAs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {303--314},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064253},
 doi = {http://doi.acm.org/10.1145/1071690.1064253},
 acmid = {1064253},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy management, feedback control, performance modeling, server provisioning},
} 

@inproceedings{Chen:2005:MSE:1064212.1064253,
 author = {Chen, Yiyu and Das, Amitayu and Qin, Wubi and Sivasubramaniam, Anand and Wang, Qian and Gautam, Natarajan},
 title = {Managing server energy and operational costs in hosting centers},
 abstract = {The growing cost of tuning and managing computer systems is leading to out-sourcing of commercial services to hosting centers. These centers provision thousands of dense servers within a relatively small real-estate in order to host the applications/services of different customers who may have been assured by a service-level agreement (SLA). Power consumption of these servers is becoming a serious concern in the design and operation of the hosting centers. The effects of high power consumption manifest not only in the costs spent in designing effective cooling systems to ward off the generated heat, but in the cost of electricity consumption itself. It is crucial to deploy power management strategies in these hosting centers to lower these costs towards enhancing profitability. At the same time, techniques for power management that include shutting down these servers and/or modulating their operational speed, can impact the ability of the hosting center to meet SLAs. In addition, repeated on-off cycles can increase the wear-and-tear of server components, incurring costs for their procurement and replacement. This paper presents a formalism to this problem, and proposes three new online solution strategies based on steady state queuing analysis, feedback control theory, and a hybrid mechanism borrowing ideas from these two. Using real web server traces, we show that these solutions are more adaptive to workload behavior when performing server provisioning and speed control than earlier heuristics towards minimizing operational costs while meeting the SLAs.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {303--314},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064253},
 doi = {http://doi.acm.org/10.1145/1064212.1064253},
 acmid = {1064253},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy management, feedback control, performance modeling, server provisioning},
} 

@article{Ruan:2005:EIS:1071690.1064254,
 author = {Ruan, Yaoping and Pai, Vivek S. and Nahum, Erich and Tracey, John M.},
 title = {Evaluating the impact of simultaneous multithreading on network servers using real hardware},
 abstract = {This paper examines the performance of simultaneous multithreading (SMT) for network servers using actual hardware, multiple network server applications, and several workloads. Using three versions of the Intel Xeon processor with Hyper-Threading, we perform macroscopic analysis as well as microarchitectural measurements to understand the origins of the performance bottlenecks for SMT processors in these environments. The results of our evaluation suggest that the current SMT support in the Xeon is application and workload sensitive, and may not yield significant benefits for network servers.In general, we find that enabling SMT on real hardware usually produces only slight performance gains, and can sometimes lead to performance loss. In the uniprocessor case, previous studies appear to have neglected the OS overhead in switching from a uniprocessor kernel to an SMT-enabled kernel. The performance loss associated with such support is comparable to the gains provided by SMT. In the 2-way multiprocessor case, the higher number of memory references from SMT often causes the memory system to become the bottleneck, offsetting any processor utilization gains. This effect is compounded by the growing gap between processor speeds and memory latency. In trying to understand the large gains shown by simulation studies, we find that while the general trends for microarchitectural behavior agree with real hardware, differences in sizing assumptions and performance models yield much more optimistic benefits for SMT than we observe.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {315--326},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064254},
 doi = {http://doi.acm.org/10.1145/1071690.1064254},
 acmid = {1064254},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network server, simultaneous multithreading(SMT)},
} 

@inproceedings{Ruan:2005:EIS:1064212.1064254,
 author = {Ruan, Yaoping and Pai, Vivek S. and Nahum, Erich and Tracey, John M.},
 title = {Evaluating the impact of simultaneous multithreading on network servers using real hardware},
 abstract = {This paper examines the performance of simultaneous multithreading (SMT) for network servers using actual hardware, multiple network server applications, and several workloads. Using three versions of the Intel Xeon processor with Hyper-Threading, we perform macroscopic analysis as well as microarchitectural measurements to understand the origins of the performance bottlenecks for SMT processors in these environments. The results of our evaluation suggest that the current SMT support in the Xeon is application and workload sensitive, and may not yield significant benefits for network servers.In general, we find that enabling SMT on real hardware usually produces only slight performance gains, and can sometimes lead to performance loss. In the uniprocessor case, previous studies appear to have neglected the OS overhead in switching from a uniprocessor kernel to an SMT-enabled kernel. The performance loss associated with such support is comparable to the gains provided by SMT. In the 2-way multiprocessor case, the higher number of memory references from SMT often causes the memory system to become the bottleneck, offsetting any processor utilization gains. This effect is compounded by the growing gap between processor speeds and memory latency. In trying to understand the large gains shown by simulation studies, we find that while the general trends for microarchitectural behavior agree with real hardware, differences in sizing assumptions and performance models yield much more optimistic benefits for SMT than we observe.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {315--326},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064254},
 doi = {http://doi.acm.org/10.1145/1064212.1064254},
 acmid = {1064254},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {network server, simultaneous multithreading(SMT)},
} 

@inproceedings{Donnet:2005:EAL:1064212.1064256,
 author = {Donnet, Benoit and Raoult, Philippe and Friedman, Timur and Crovella, Mark},
 title = {Efficient algorithms for large-scale topology discovery},
 abstract = {There is a growing interest in discovery of internet topology at the interface level. A new generation of highly distributed measurement systems is currently being deployed. Unfortunately, the research community has not examined the problem of how to perform such measurements efficiently and in a network-friendly manner. In this paper we make two contributions toward that end. First, we show that standard topology discovery methods (e.g., skitter) are quite inefficient, repeatedly probing the same interfaces. This is a concern, because when scaled up, such methods will generate so much traffic that they will begin to resemble DDoS attacks. We measure two kinds of redundancy in probing (intra- and inter-monitor) and show that both kinds are important. We show that straightforward approaches to addressing these two kinds of redundancy must take opposite tacks, and are thus fundamentally in conflict. Our second contribution is to propose and evaluate Doubletree, an algorithm that reduces both types of redundancy simultaneously on routers and end systems. The key ideas are to exploit the tree-like structure of routes to and from a single point in order to guide when to stop probing, and to probe each path by starting near its midpoint. Our results show that Doubletree can reduce both types of measurement load on the network dramatically, while permitting discovery of nearly the same set of nodes and links.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {327--338},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064256},
 doi = {http://doi.acm.org/10.1145/1064212.1064256},
 acmid = {1064256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooperative systems, network topology, traceroutes},
} 

@article{Donnet:2005:EAL:1071690.1064256,
 author = {Donnet, Benoit and Raoult, Philippe and Friedman, Timur and Crovella, Mark},
 title = {Efficient algorithms for large-scale topology discovery},
 abstract = {There is a growing interest in discovery of internet topology at the interface level. A new generation of highly distributed measurement systems is currently being deployed. Unfortunately, the research community has not examined the problem of how to perform such measurements efficiently and in a network-friendly manner. In this paper we make two contributions toward that end. First, we show that standard topology discovery methods (e.g., skitter) are quite inefficient, repeatedly probing the same interfaces. This is a concern, because when scaled up, such methods will generate so much traffic that they will begin to resemble DDoS attacks. We measure two kinds of redundancy in probing (intra- and inter-monitor) and show that both kinds are important. We show that straightforward approaches to addressing these two kinds of redundancy must take opposite tacks, and are thus fundamentally in conflict. Our second contribution is to propose and evaluate Doubletree, an algorithm that reduces both types of redundancy simultaneously on routers and end systems. The key ideas are to exploit the tree-like structure of routes to and from a single point in order to guide when to stop probing, and to probe each path by starting near its midpoint. Our results show that Doubletree can reduce both types of measurement load on the network dramatically, while permitting discovery of nearly the same set of nodes and links.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {327--338},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064256},
 doi = {http://doi.acm.org/10.1145/1071690.1064256},
 acmid = {1064256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cooperative systems, network topology, traceroutes},
} 

@article{Mao:2005:API:1071690.1064257,
 author = {Mao, Z. Morley and Qiu, Lili and Wang, Jia and Zhang, Yin},
 title = {On AS-level path inference},
 abstract = {The ability to discover the AS-level path between two end-points is valuable for network diagnosis, performance optimization, and reliability enhancement. Virtually all existing techniques and tools for path discovery require direct access to the source. However, the uncooperative nature of the Internet makes it difficult to get direct access to any remote end-point. Path inference becomes challenging when we have no access to the source or the destination. Moveover even when we have access to the source and know the forward path, it is nontrivial to infer the reverse path, since the Internet routing is often asymmetric.In this paper, we explore the feasibility of AS-level path inference without direct access to either end-points. We describe RouteScope</i>-a tool for inferring AS-level paths by finding the shortest policy paths in an AS graph obtained from BGP tables collected from multiple vantage points. We identify two main factors that affect the path inference accuracy: the accuracy of AS relationship inference and the ability to determine the first AS hop. To address the issues, we propose two novel techniques: a new AS relation-ship inference algorithm, and a novel scheme to infer the first AS hop by exploiting the TTL information in IP packets. We evaluate the effectiveness of RouteScope</i> using both BGP tables and the AS paths collected from public BGP gateways. Our results show that it achieves 70\% - 88\% accuracy in path inference.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {339--349},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1071690.1064257},
 doi = {http://doi.acm.org/10.1145/1071690.1064257},
 acmid = {1064257},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AS-level path, border gateway protocol, internet routing, network topology},
} 

@inproceedings{Mao:2005:API:1064212.1064257,
 author = {Mao, Z. Morley and Qiu, Lili and Wang, Jia and Zhang, Yin},
 title = {On AS-level path inference},
 abstract = {The ability to discover the AS-level path between two end-points is valuable for network diagnosis, performance optimization, and reliability enhancement. Virtually all existing techniques and tools for path discovery require direct access to the source. However, the uncooperative nature of the Internet makes it difficult to get direct access to any remote end-point. Path inference becomes challenging when we have no access to the source or the destination. Moveover even when we have access to the source and know the forward path, it is nontrivial to infer the reverse path, since the Internet routing is often asymmetric.In this paper, we explore the feasibility of AS-level path inference without direct access to either end-points. We describe RouteScope</i>-a tool for inferring AS-level paths by finding the shortest policy paths in an AS graph obtained from BGP tables collected from multiple vantage points. We identify two main factors that affect the path inference accuracy: the accuracy of AS relationship inference and the ability to determine the first AS hop. To address the issues, we propose two novel techniques: a new AS relation-ship inference algorithm, and a novel scheme to infer the first AS hop by exploiting the TTL information in IP packets. We evaluate the effectiveness of RouteScope</i> using both BGP tables and the AS paths collected from public BGP gateways. Our results show that it achieves 70\% - 88\% accuracy in path inference.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {339--349},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064212.1064257},
 doi = {http://doi.acm.org/10.1145/1064212.1064257},
 acmid = {1064257},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AS-level path, border gateway protocol, internet routing, network topology},
} 

@article{Zhao:2005:DSA:1071690.1064258,
 author = {Zhao, Qi (George) and Kumar, Abhishek and Wang, Jia and Xu, Jun (Jim)},
 title = {Data streaming algorithms for accurate and efficient measurement of traffic and flow matrices},
 abstract = {The traffic volume between origin/destination (OD) pairs in a network, known as traffic matrix, is essential for efficient network provisioning and traffic engineering. Existing approaches of estimating the traffic matrix, based on statistical inference and/or packet sampling, usually cannot achieve very high estimation accuracy. In this work, we take a brand new approach in attacking this problem. We propose a novel data streaming algorithm that can process traffic stream at very high speed (e.g., 40 Gbps) and produce traffic digests that are orders of magnitude smaller than the traffic stream. By correlating the digests collected at any OD pair using Bayesian statistics, the volume of traffic flowing between the OD pair can be accurately determined. We also establish principles and techniques for optimally combining this streaming method with sampling, when sampling is necessary due to stringent resource constraints. In addition, we propose another data streaming algorithm that estimates flow matrix</i>, a finer-grained characterization than traffic matrix. Flow matrix is concerned with not only the total traffic between an OD pair (traffic matrix), but also how it splits into flows of various sizes. Through rigorous theoretical analysis and extensive synthetic experiments on real Internet traffic, we demonstrate that these two algorithms can produce very accurate estimation of traffic matrix and flow matrix respectively.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {350--361},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064258},
 doi = {http://doi.acm.org/10.1145/1071690.1064258},
 acmid = {1064258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, network measurement, sampling, statistical inference, traffic matrix},
} 

@inproceedings{Zhao:2005:DSA:1064212.1064258,
 author = {Zhao, Qi (George) and Kumar, Abhishek and Wang, Jia and Xu, Jun (Jim)},
 title = {Data streaming algorithms for accurate and efficient measurement of traffic and flow matrices},
 abstract = {The traffic volume between origin/destination (OD) pairs in a network, known as traffic matrix, is essential for efficient network provisioning and traffic engineering. Existing approaches of estimating the traffic matrix, based on statistical inference and/or packet sampling, usually cannot achieve very high estimation accuracy. In this work, we take a brand new approach in attacking this problem. We propose a novel data streaming algorithm that can process traffic stream at very high speed (e.g., 40 Gbps) and produce traffic digests that are orders of magnitude smaller than the traffic stream. By correlating the digests collected at any OD pair using Bayesian statistics, the volume of traffic flowing between the OD pair can be accurately determined. We also establish principles and techniques for optimally combining this streaming method with sampling, when sampling is necessary due to stringent resource constraints. In addition, we propose another data streaming algorithm that estimates flow matrix</i>, a finer-grained characterization than traffic matrix. Flow matrix is concerned with not only the total traffic between an OD pair (traffic matrix), but also how it splits into flows of various sizes. Through rigorous theoretical analysis and extensive synthetic experiments on real Internet traffic, we demonstrate that these two algorithms can produce very accurate estimation of traffic matrix and flow matrix respectively.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {350--361},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064258},
 doi = {http://doi.acm.org/10.1145/1064212.1064258},
 acmid = {1064258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data streaming, network measurement, sampling, statistical inference, traffic matrix},
} 

@inproceedings{Soule:2005:TMB:1064212.1064259,
 author = {Soule, Augustin and Lakhina, Anukool and Taft, Nina and Papagiannaki, Konstantina and Salamatian, Kave and Nucci, Antonio and Crovella, Mark and Diot, Christophe},
 title = {Traffic matrices: balancing measurements, inference and modeling},
 abstract = {Traffic matrix estimation is well-studied, but in general has been treated simply as a statistical inference problem. In practice, however, network operators seeking traffic matrix information have a range of options available to them. Operators can measure traffic flows directly; they can perform partial flow measurement, and infer missing data using models; or they can perform no flow measurement and infer traffic matrices directly from link counts. The advent of practical flow measurement makes the study of these tradeoffs more important. In particular, an important question is whether judicious modeling, combined with partial flow measurement, can provide traffic matrix estimates that are signficantly better than previous methods at relatively low cost. In this paper we make a number of contributions toward answering this question. First, we provide a taxonomy of the kinds of models that may make use of partial flow measurement, based on the nature of the measurements used and the spatial, temporal, or spatio-temporal correlation exploited. We then evaluate estimation methods which use each kind of model. In the process we propose and evaluate new methods, and extensions to methods previously proposed. We show that, using such methods, small amounts of traffic flow measurements can have significant impacts on the accuracy of traffic matrix estimation, yielding results much better than previous approaches. We also show that different methods differ in their bias and variance properties, suggesting that different methods may be suited to different applications.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {362--373},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064259},
 doi = {http://doi.acm.org/10.1145/1064212.1064259},
 acmid = {1064259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet traffic matrix estimation, kalman filtering, principal components analysis, statistical inference, traffic characterization},
} 

@article{Soule:2005:TMB:1071690.1064259,
 author = {Soule, Augustin and Lakhina, Anukool and Taft, Nina and Papagiannaki, Konstantina and Salamatian, Kave and Nucci, Antonio and Crovella, Mark and Diot, Christophe},
 title = {Traffic matrices: balancing measurements, inference and modeling},
 abstract = {Traffic matrix estimation is well-studied, but in general has been treated simply as a statistical inference problem. In practice, however, network operators seeking traffic matrix information have a range of options available to them. Operators can measure traffic flows directly; they can perform partial flow measurement, and infer missing data using models; or they can perform no flow measurement and infer traffic matrices directly from link counts. The advent of practical flow measurement makes the study of these tradeoffs more important. In particular, an important question is whether judicious modeling, combined with partial flow measurement, can provide traffic matrix estimates that are signficantly better than previous methods at relatively low cost. In this paper we make a number of contributions toward answering this question. First, we provide a taxonomy of the kinds of models that may make use of partial flow measurement, based on the nature of the measurements used and the spatial, temporal, or spatio-temporal correlation exploited. We then evaluate estimation methods which use each kind of model. In the process we propose and evaluate new methods, and extensions to methods previously proposed. We show that, using such methods, small amounts of traffic flow measurements can have significant impacts on the accuracy of traffic matrix estimation, yielding results much better than previous approaches. We also show that different methods differ in their bias and variance properties, suggesting that different methods may be suited to different applications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {362--373},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064259},
 doi = {http://doi.acm.org/10.1145/1071690.1064259},
 acmid = {1064259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet traffic matrix estimation, kalman filtering, principal components analysis, statistical inference, traffic characterization},
} 

@inproceedings{Ganeriwal:2005:RTS:1064212.1064261,
 author = {Ganeriwal, Saurabh and Ganesanl, Deepak and Hansen, Mark and Srivastava, Mani B. and Estrin, Deborah},
 title = {Rate-adaptive time synchronization for long-lived sensor networks},
 abstract = {Time synchronization is critical to sensor networks at many layers of its design and enables better duty-cycling of the radio, accurate localization, beamforming and other collaborative signal processing. While there has been significant work in sensor network synchronization, measurement based studies have been restricted to very short-term (few minutes) datasets and have focused on obtaining accurate instantaneous synchronization. Long-term synchronization has typically been handled by periodic re-synchronization schemes with beacon intervals of a few minutes based on the assumption that long-term drift is too hard to model and predict. Thus, none of this work exploits the temporally correlated behavior of the clock drift. Yet, there are incredible energy gains to be achieved from better modeling and prediction of long-term drift that can provide bounds on long-term synchronization error across a sensor network. Better synchronization can lead to significantly lower duty-cycles of the radio, simplify signal processing and can enable an order of magnitude greater lifetime than current techniques.We measure, evaluate and analyze in-depth the long-term behavior of synchronization skew and drift on typical Mica sensor nodes and develop an efficient long-term time synchronization protocol. We use four real time data sets gathered over periods of 12-30 hours in different environmental conditions to study the interplay between three key parameters that influence long-term synchronization - synchronization rate, history of past synchronization beacons and the estimation scheme. We use this measurement-based study to design an online adaptive time-synchronization algorithm that can adapt to changing clock drift and environmental conditions while achieving application-specified precision with very high probability. We find that our algorithm achieves between one and two orders of magnitude improvement in energy efficiency over currently available time-synchronization approaches.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {374--375},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064261},
 doi = {http://doi.acm.org/10.1145/1064212.1064261},
 acmid = {1064261},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clock drift, sensor networks, time synchronization},
} 

@article{Ganeriwal:2005:RTS:1071690.1064261,
 author = {Ganeriwal, Saurabh and Ganesanl, Deepak and Hansen, Mark and Srivastava, Mani B. and Estrin, Deborah},
 title = {Rate-adaptive time synchronization for long-lived sensor networks},
 abstract = {Time synchronization is critical to sensor networks at many layers of its design and enables better duty-cycling of the radio, accurate localization, beamforming and other collaborative signal processing. While there has been significant work in sensor network synchronization, measurement based studies have been restricted to very short-term (few minutes) datasets and have focused on obtaining accurate instantaneous synchronization. Long-term synchronization has typically been handled by periodic re-synchronization schemes with beacon intervals of a few minutes based on the assumption that long-term drift is too hard to model and predict. Thus, none of this work exploits the temporally correlated behavior of the clock drift. Yet, there are incredible energy gains to be achieved from better modeling and prediction of long-term drift that can provide bounds on long-term synchronization error across a sensor network. Better synchronization can lead to significantly lower duty-cycles of the radio, simplify signal processing and can enable an order of magnitude greater lifetime than current techniques.We measure, evaluate and analyze in-depth the long-term behavior of synchronization skew and drift on typical Mica sensor nodes and develop an efficient long-term time synchronization protocol. We use four real time data sets gathered over periods of 12-30 hours in different environmental conditions to study the interplay between three key parameters that influence long-term synchronization - synchronization rate, history of past synchronization beacons and the estimation scheme. We use this measurement-based study to design an online adaptive time-synchronization algorithm that can adapt to changing clock drift and environmental conditions while achieving application-specified precision with very high probability. We find that our algorithm achieves between one and two orders of magnitude improvement in energy efficiency over currently available time-synchronization approaches.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {374--375},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064261},
 doi = {http://doi.acm.org/10.1145/1071690.1064261},
 acmid = {1064261},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clock drift, sensor networks, time synchronization},
} 

@inproceedings{Wang:2005:IPS:1064212.1064262,
 author = {Wang, An-I A. and Reiher, Peter and Kuenning, Geoff},
 title = {Introducing permuted states for analyzing conflict rates in optimistic replication},
 abstract = {},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {376--377},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064262},
 doi = {http://doi.acm.org/10.1145/1064212.1064262},
 acmid = {1064262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, conflict rates, optimistic replication, permuted states, simulation},
} 

@article{Wang:2005:IPS:1071690.1064262,
 author = {Wang, An-I A. and Reiher, Peter and Kuenning, Geoff},
 title = {Introducing permuted states for analyzing conflict rates in optimistic replication},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {376--377},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064262},
 doi = {http://doi.acm.org/10.1145/1071690.1064262},
 acmid = {1064262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, conflict rates, optimistic replication, permuted states, simulation},
} 

@article{Mickens:2005:PNA:1071690.1064263,
 author = {Mickens, James W. and Noble, Brian D.},
 title = {Predicting node availability in peer-to-peer networks},
 abstract = {Unlike the well-administered servers in traditional distributed systems, machines in peer-to-peer networks have widely varying levels of availability. Accurate modeling of node uptime is crucial for predicting per-machine resource burdens and selecting appropriate data replication strategies. In this research project, we improve upon the accuracy of previous peer-to-peer availability models, which are often too conservative to dynamically predict system availability at a fine-grained level. We test our predictors on availability traces from the PlanetLab distributed test bed and the Microsoft corporate network. Each trace has a distinct predictability profile, and we explain these differences by examining the fundamental uptime classes contained in each trace. We also show how availability-guided replica placement reduces the amount of object copying in a distributed data store.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {378--379},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064263},
 doi = {http://doi.acm.org/10.1145/1071690.1064263},
 acmid = {1064263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability prediction, data availability, distributed object stores, distributed system simulation, machine availability},
} 

@inproceedings{Mickens:2005:PNA:1064212.1064263,
 author = {Mickens, James W. and Noble, Brian D.},
 title = {Predicting node availability in peer-to-peer networks},
 abstract = {Unlike the well-administered servers in traditional distributed systems, machines in peer-to-peer networks have widely varying levels of availability. Accurate modeling of node uptime is crucial for predicting per-machine resource burdens and selecting appropriate data replication strategies. In this research project, we improve upon the accuracy of previous peer-to-peer availability models, which are often too conservative to dynamically predict system availability at a fine-grained level. We test our predictors on availability traces from the PlanetLab distributed test bed and the Microsoft corporate network. Each trace has a distinct predictability profile, and we explain these differences by examining the fundamental uptime classes contained in each trace. We also show how availability-guided replica placement reduces the amount of object copying in a distributed data store.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {378--379},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064263},
 doi = {http://doi.acm.org/10.1145/1064212.1064263},
 acmid = {1064263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability prediction, data availability, distributed object stores, distributed system simulation, machine availability},
} 

@inproceedings{Qiu:2005:TMW:1064212.1064264,
 author = {Qiu, Lili and Bahl, Paramvir and Rao, Ananth and Zhou, Lidong},
 title = {Troubleshooting multihop wireless networks},
 abstract = {Effective network troubleshooting is critical for maintaining efficient and reliable network operation. Troubleshooting is especially challenging in multihop wireless networks because the behavior of such networks depends on complicated interactions between many unpredictable factors such as RF noise, signal propagation, node interference, and traffic flows. In this paper we propose a new direction for research on fault diagnosis in wireless networks. Specifically, we present a diagnostic system that employs trace-driven simulations to detect faults and perform root cause analysis. We apply this approach to diagnose performance problems caused by packet dropping, link congestion, external noise, and MAC misbehavior. In a 25 node multihop wireless network, we are able to diagnose over 10 simultaneous faults of multiple types with more than 80\% coverage. Our framework is general enough for a wide variety of wireless and wired networks.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {380--381},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064264},
 doi = {http://doi.acm.org/10.1145/1064212.1064264},
 acmid = {1064264},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multihop wireless networks, network diagnosis, network management, simulation},
} 

@article{Qiu:2005:TMW:1071690.1064264,
 author = {Qiu, Lili and Bahl, Paramvir and Rao, Ananth and Zhou, Lidong},
 title = {Troubleshooting multihop wireless networks},
 abstract = {Effective network troubleshooting is critical for maintaining efficient and reliable network operation. Troubleshooting is especially challenging in multihop wireless networks because the behavior of such networks depends on complicated interactions between many unpredictable factors such as RF noise, signal propagation, node interference, and traffic flows. In this paper we propose a new direction for research on fault diagnosis in wireless networks. Specifically, we present a diagnostic system that employs trace-driven simulations to detect faults and perform root cause analysis. We apply this approach to diagnose performance problems caused by packet dropping, link congestion, external noise, and MAC misbehavior. In a 25 node multihop wireless network, we are able to diagnose over 10 simultaneous faults of multiple types with more than 80\% coverage. Our framework is general enough for a wide variety of wireless and wired networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {380--381},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064264},
 doi = {http://doi.acm.org/10.1145/1071690.1064264},
 acmid = {1064264},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multihop wireless networks, network diagnosis, network management, simulation},
} 

@article{Raz:2005:FOM:1071690.1064265,
 author = {Raz, David and Avi-Itzhak, Benjamin and Levy, Hanoch},
 title = {Fair operation of multi-server and multi-queue systems},
 abstract = {This work aims at studying the fairness of multi-queue and multi-server queueing systems. We deal with the issues of queue-multiplicity, queue joining policy and queue jockeying and use a quantitative measure (RAQFM) to evaluate them. Our results yield the relative fairness of the mechanisms as a function of the system configuration and parameters. Practitioners can use these results to quantitatively</i> account for system fairness and to weigh efficiency aspects versus fairness aspects in designing and controlling their queueing systems. In particular, we quantitatively demonstrate that: 1) Joining the shortest queue increases fairness, 2) A single "combined" queue system is more fair than "separate" (multi) queue system and 3) Jockeying from the head of a queue is more fair than jockeying from its tail.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {382--383},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064265},
 doi = {http://doi.acm.org/10.1145/1071690.1064265},
 acmid = {1064265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS, fairness, job scheduling, multi-queue, multi-server, resource allocation, unfairness},
} 

@inproceedings{Raz:2005:FOM:1064212.1064265,
 author = {Raz, David and Avi-Itzhak, Benjamin and Levy, Hanoch},
 title = {Fair operation of multi-server and multi-queue systems},
 abstract = {This work aims at studying the fairness of multi-queue and multi-server queueing systems. We deal with the issues of queue-multiplicity, queue joining policy and queue jockeying and use a quantitative measure (RAQFM) to evaluate them. Our results yield the relative fairness of the mechanisms as a function of the system configuration and parameters. Practitioners can use these results to quantitatively</i> account for system fairness and to weigh efficiency aspects versus fairness aspects in designing and controlling their queueing systems. In particular, we quantitatively demonstrate that: 1) Joining the shortest queue increases fairness, 2) A single "combined" queue system is more fair than "separate" (multi) queue system and 3) Jockeying from the head of a queue is more fair than jockeying from its tail.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {382--383},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064265},
 doi = {http://doi.acm.org/10.1145/1064212.1064265},
 acmid = {1064265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FCFS, fairness, job scheduling, multi-queue, multi-server, resource allocation, unfairness},
} 

@inproceedings{Anderson:2005:DSA:1064212.1064266,
 author = {Anderson, Eric and Beyer, Dirk and Chaudhuri, Kamalika and Kelly, Terence and Salazar, Norman and Santos, Cipriano and Swaminathan, Ram and Tarjan, Robert and Wiener, Janet and Zhou, Yunhong},
 title = {Deadline scheduling for animation rendering},
 abstract = {},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {384--385},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064266},
 doi = {http://doi.acm.org/10.1145/1064212.1064266},
 acmid = {1064266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {animation rendering, deadline scheduling, simulation},
} 

@article{Anderson:2005:DSA:1071690.1064266,
 author = {Anderson, Eric and Beyer, Dirk and Chaudhuri, Kamalika and Kelly, Terence and Salazar, Norman and Santos, Cipriano and Swaminathan, Ram and Tarjan, Robert and Wiener, Janet and Zhou, Yunhong},
 title = {Deadline scheduling for animation rendering},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {384--385},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064266},
 doi = {http://doi.acm.org/10.1145/1071690.1064266},
 acmid = {1064266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {animation rendering, deadline scheduling, simulation},
} 

@article{He:2005:SSP:1071690.1064267,
 author = {He, Simin and Sun, Shutao and Zhao, Wei and Zheng, Yanfeng and Gao, Wen},
 title = {Smooth switching problem in buffered crossbar switches},
 abstract = {Scalability considerations drive the switch fabric design to evolve from output queueing to input queueing and further to combined input and crosspoint queueing (CICQ). However, few CICQ switches are known with guaranteed quality of service, and credit-based flow control induces a scalability bottleneck. In this paper, we propose a novel CICQ switch called the smoothed buffered crossbar or sBUX, based on a new design objective of smoothness and on a new rate-based flow control scheme called the smoothed multiplexer or sMUX. It is proved that with a buffer of just four cells at each crosspoint, sBUX can utilize 100\% of the switch capacity to provide deterministic guarantees of bandwidth and fairness, delay and jitter bounds for each flow. In particular, neither credit-based flow control nor speedup is used, and arbitrary fabric-internal latency is allowed between line cards and the switch core.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {386--387},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064267},
 doi = {http://doi.acm.org/10.1145/1071690.1064267},
 acmid = {1064267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CICQ, buffered crossbar, scheduling, smoothness, switch},
} 

@inproceedings{He:2005:SSP:1064212.1064267,
 author = {He, Simin and Sun, Shutao and Zhao, Wei and Zheng, Yanfeng and Gao, Wen},
 title = {Smooth switching problem in buffered crossbar switches},
 abstract = {Scalability considerations drive the switch fabric design to evolve from output queueing to input queueing and further to combined input and crosspoint queueing (CICQ). However, few CICQ switches are known with guaranteed quality of service, and credit-based flow control induces a scalability bottleneck. In this paper, we propose a novel CICQ switch called the smoothed buffered crossbar or sBUX, based on a new design objective of smoothness and on a new rate-based flow control scheme called the smoothed multiplexer or sMUX. It is proved that with a buffer of just four cells at each crosspoint, sBUX can utilize 100\% of the switch capacity to provide deterministic guarantees of bandwidth and fairness, delay and jitter bounds for each flow. In particular, neither credit-based flow control nor speedup is used, and arbitrary fabric-internal latency is allowed between line cards and the switch core.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {386--387},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064267},
 doi = {http://doi.acm.org/10.1145/1064212.1064267},
 acmid = {1064267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CICQ, buffered crossbar, scheduling, smoothness, switch},
} 

@article{He:2005:PTT:1071690.1064268,
 author = {He, Qi and Dovrolis, Constantinos and Ammar, Mostafa},
 title = {Prediction of TCP throughput: formula-based and history-based methods},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {388--389},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064268},
 doi = {http://doi.acm.org/10.1145/1071690.1064268},
 acmid = {1064268},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{He:2005:PTT:1064212.1064268,
 author = {He, Qi and Dovrolis, Constantinos and Ammar, Mostafa},
 title = {Prediction of TCP throughput: formula-based and history-based methods},
 abstract = {},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {388--389},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064268},
 doi = {http://doi.acm.org/10.1145/1064212.1064268},
 acmid = {1064268},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chua:2005:SFE:1064212.1064269,
 author = {Chua, David and Kolaczyk, Eric D. and Crovella, Mark},
 title = {A statistical framework for efficient monitoring of end-to-end network properties},
 abstract = {Network service providers and customers are often concerned with aggregate performance measures that span multiple network paths. Unfortunately, forming such network-wide measures can be difficult, due to the issues of scale involved. As a result, it is of interest to explore the feasibility of methods that dramatically reduce the number of paths measured in such situations while maintaining acceptable accuracy.In previous work [4] we have proposed a statistical framework for efficiently addressing this problem. The key to our method lies in the observation and exploitation of the fact that network paths show significant redundancy (sharing of common links).We now make three contributions in [3]: (1) we generalize the framework to make it more immediately applicable to network measurements encountered in practice; (2) we demonstrate that the observed path redundancy upon which our method is based is robust to variation in key network conditions and characteristics, including the presence of link failures; and (3) we show how the framework may be applied to address three practical problems of interest to network providers and customers, using data from an operating network.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {390--391},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064269},
 doi = {http://doi.acm.org/10.1145/1064212.1064269},
 acmid = {1064269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, networking, statistical analysis},
} 

@article{Chua:2005:SFE:1071690.1064269,
 author = {Chua, David and Kolaczyk, Eric D. and Crovella, Mark},
 title = {A statistical framework for efficient monitoring of end-to-end network properties},
 abstract = {Network service providers and customers are often concerned with aggregate performance measures that span multiple network paths. Unfortunately, forming such network-wide measures can be difficult, due to the issues of scale involved. As a result, it is of interest to explore the feasibility of methods that dramatically reduce the number of paths measured in such situations while maintaining acceptable accuracy.In previous work [4] we have proposed a statistical framework for efficiently addressing this problem. The key to our method lies in the observation and exploitation of the fact that network paths show significant redundancy (sharing of common links).We now make three contributions in [3]: (1) we generalize the framework to make it more immediately applicable to network measurements encountered in practice; (2) we demonstrate that the observed path redundancy upon which our method is based is robust to variation in key network conditions and characteristics, including the presence of link failures; and (3) we show how the framework may be applied to address three practical problems of interest to network providers and customers, using data from an operating network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {390--391},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064269},
 doi = {http://doi.acm.org/10.1145/1071690.1064269},
 acmid = {1064269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithms, networking, statistical analysis},
} 

@article{Zhu:2005:TSA:1071690.1064270,
 author = {Zhu, Ningning and Chen, Jiawu and Chiueh, Tzi-cker and Ellard, Daniel},
 title = {TBBT: scalable and accurate trace replay for file server evaluation},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {392--393},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064270},
 doi = {http://doi.acm.org/10.1145/1071690.1064270},
 acmid = {1064270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {NFS, aging, benchmarks, file system evaluation, trace play},
} 

@inproceedings{Zhu:2005:TSA:1064212.1064270,
 author = {Zhu, Ningning and Chen, Jiawu and Chiueh, Tzi-cker and Ellard, Daniel},
 title = {TBBT: scalable and accurate trace replay for file server evaluation},
 abstract = {},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {392--393},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064270},
 doi = {http://doi.acm.org/10.1145/1064212.1064270},
 acmid = {1064270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {NFS, aging, benchmarks, file system evaluation, trace play},
} 

@article{Sarat:2005:UAD:1071690.1064271,
 author = {Sarat, Sandeep and Pappas, Vasileios and Terzis, Andreas},
 title = {On the use of anycast in DNS},
 abstract = {We present the initial results from our evaluation study on the performance implications of anycast in DNS, using four anycast servers deployed at top-level DNS zones. Our results show that 15\% to 55\% of the queries sent to an anycast group, are answered by the topologically closest server and at least 10\% of the queries experience an additional delay in the order of 100ms. While increased availability is one of the supposed advantages of anycast, we found that outages can last up to multiple minutes, mainly due to slow BGP convergence. On the other hand, the number of outages observed was fairly small, suggesting that anycast provides a generally stable service.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {394--395},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064271},
 doi = {http://doi.acm.org/10.1145/1071690.1064271},
 acmid = {1064271},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sarat:2005:UAD:1064212.1064271,
 author = {Sarat, Sandeep and Pappas, Vasileios and Terzis, Andreas},
 title = {On the use of anycast in DNS},
 abstract = {We present the initial results from our evaluation study on the performance implications of anycast in DNS, using four anycast servers deployed at top-level DNS zones. Our results show that 15\% to 55\% of the queries sent to an anycast group, are answered by the topologically closest server and at least 10\% of the queries experience an additional delay in the order of 100ms. While increased availability is one of the supposed advantages of anycast, we found that outages can last up to multiple minutes, mainly due to slow BGP convergence. On the other hand, the number of outages observed was fairly small, suggesting that anycast provides a generally stable service.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {394--395},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064271},
 doi = {http://doi.acm.org/10.1145/1064212.1064271},
 acmid = {1064271},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mudigonda:2005:MMA:1064212.1064272,
 author = {Mudigonda, Jayaram and Vin, Harrick M. and Yavatkar, Raj},
 title = {Managing memory access latency in packet processing},
 abstract = {In this study, we refute the popular belief [1,2] that packet processing does not benefit from data-caching. We show that a small data-cache of 8KB can bring down the packet processing time by much as 50-90\%, while reducing the off-chip memory bandwidth usage by about 60-95\%. We also show that, unlike general-purpose computing, packet processing, due to its memory-intensive nature, cannot rely exclusively on data-caching to eliminate the memory bottleneck completely.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {396--397},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064272},
 doi = {http://doi.acm.org/10.1145/1064212.1064272},
 acmid = {1064272},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-caches, multithreading, network processors},
} 

@article{Mudigonda:2005:MMA:1071690.1064272,
 author = {Mudigonda, Jayaram and Vin, Harrick M. and Yavatkar, Raj},
 title = {Managing memory access latency in packet processing},
 abstract = {In this study, we refute the popular belief [1,2] that packet processing does not benefit from data-caching. We show that a small data-cache of 8KB can bring down the packet processing time by much as 50-90\%, while reducing the off-chip memory bandwidth usage by about 60-95\%. We also show that, unlike general-purpose computing, packet processing, due to its memory-intensive nature, cannot rely exclusively on data-caching to eliminate the memory bottleneck completely.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {396--397},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064272},
 doi = {http://doi.acm.org/10.1145/1071690.1064272},
 acmid = {1064272},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-caches, multithreading, network processors},
} 

@inproceedings{Bharambe:2005:OBP:1064212.1064273,
 author = {Bharambe, Ashwin R. and Herley, Cormac and Padmanabhan, Venkata N.},
 title = {Some observations on bitTorrent performance},
 abstract = {In this paper, we present a simulation-based study of BitTorrent. Our results confirm that BitTorrent performs near-optimally in terms of uplink bandwidth utilization and download time, except under certain extreme conditions. On fairness, however, our work shows that low bandwidth peers systematically download more than they upload to the network when high bandwidth peers are present. We find that the rate-based</i> tit-for-tat policy is not effective in preventing unfairness. We show how simple changes to the tracker and a stricter, block-based tit-for-tat policy</i>, greatly improves fairness, while maintaining high utilization.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {398--399},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064273},
 doi = {http://doi.acm.org/10.1145/1064212.1064273},
 acmid = {1064273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BitTorrent, bandwidth utilization, fairness},
} 

@article{Bharambe:2005:OBP:1071690.1064273,
 author = {Bharambe, Ashwin R. and Herley, Cormac and Padmanabhan, Venkata N.},
 title = {Some observations on bitTorrent performance},
 abstract = {In this paper, we present a simulation-based study of BitTorrent. Our results confirm that BitTorrent performs near-optimally in terms of uplink bandwidth utilization and download time, except under certain extreme conditions. On fairness, however, our work shows that low bandwidth peers systematically download more than they upload to the network when high bandwidth peers are present. We find that the rate-based</i> tit-for-tat policy is not effective in preventing unfairness. We show how simple changes to the tracker and a stricter, block-based tit-for-tat policy</i>, greatly improves fairness, while maintaining high utilization.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {398--399},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064273},
 doi = {http://doi.acm.org/10.1145/1071690.1064273},
 acmid = {1064273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BitTorrent, bandwidth utilization, fairness},
} 

@article{Machiraju:2005:TPC:1071690.1064274,
 author = {Machiraju, Sridhar and Veitch, Darryl and Baccelli, Fran\c{c}ois and Nucci, Antonio and Bolot, Jean C.},
 title = {Theory and practice of cross-traffic estimation},
 abstract = {Active probing heuristics are usually based on queuing systems. However, a rigorous probabilistic treatment of probing methods has been lacking. For instance, it is not known even in principle, what can and cannot be measured in general, nor the true limitations of existing methods. We provide a probabilistic treatment for the measurement of cross traffic in the 1-hop case. We derive inversion formulae for the cross traffic process, and explain their fundamental limits, using an intuitive geometric framework.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {400--401},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064274},
 doi = {http://doi.acm.org/10.1145/1071690.1064274},
 acmid = {1064274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active probing, cross-traffic estimation},
} 

@inproceedings{Machiraju:2005:TPC:1064212.1064274,
 author = {Machiraju, Sridhar and Veitch, Darryl and Baccelli, Fran\c{c}ois and Nucci, Antonio and Bolot, Jean C.},
 title = {Theory and practice of cross-traffic estimation},
 abstract = {Active probing heuristics are usually based on queuing systems. However, a rigorous probabilistic treatment of probing methods has been lacking. For instance, it is not known even in principle, what can and cannot be measured in general, nor the true limitations of existing methods. We provide a probabilistic treatment for the measurement of cross traffic in the 1-hop case. We derive inversion formulae for the cross traffic process, and explain their fundamental limits, using an intuitive geometric framework.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {400--401},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064274},
 doi = {http://doi.acm.org/10.1145/1064212.1064274},
 acmid = {1064274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active probing, cross-traffic estimation},
} 

@article{Stutzbach:2005:CTG:1071690.1064275,
 author = {Stutzbach, Daniel and Rejaie, Reza},
 title = {Characterizing the two-tier gnutella topology},
 abstract = {Characterizing the properties of peer-to-peer (P2P) overlay topologies in file-sharing applications is essential for understanding their impact on the network, identifying their performance bottlenecks in practice, and evaluating their performance via simulation. Such characterization requires accurate snapshots of the overlay topology which is difficult to capture due to the large size and dynamic nature. Previous studies characterizing overlay topologies not only are outdated but also rely on partial or potentially distorted snapshots. In this extended abstract, we briefly present the first characterization of two-tier Gnutella topologies based on recent and accurate snapshots.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {402--403},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064275},
 doi = {http://doi.acm.org/10.1145/1071690.1064275},
 acmid = {1064275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gnutella, peer-to-peer, topology},
} 

@inproceedings{Stutzbach:2005:CTG:1064212.1064275,
 author = {Stutzbach, Daniel and Rejaie, Reza},
 title = {Characterizing the two-tier gnutella topology},
 abstract = {Characterizing the properties of peer-to-peer (P2P) overlay topologies in file-sharing applications is essential for understanding their impact on the network, identifying their performance bottlenecks in practice, and evaluating their performance via simulation. Such characterization requires accurate snapshots of the overlay topology which is difficult to capture due to the large size and dynamic nature. Previous studies characterizing overlay topologies not only are outdated but also rely on partial or potentially distorted snapshots. In this extended abstract, we briefly present the first characterization of two-tier Gnutella topologies based on recent and accurate snapshots.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {402--403},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064275},
 doi = {http://doi.acm.org/10.1145/1064212.1064275},
 acmid = {1064275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gnutella, peer-to-peer, topology},
} 

@article{Tewari:2005:ASR:1071690.1064276,
 author = {Tewari, Saurabh and Kleinrock, Leonard},
 title = {Analysis of search and replication in unstructured peer-to-peer networks},
 abstract = {This paper investigates the effect of the number of file replicas on search performance in unstructured peer-to-peer networks. We observe that for a search network with a random graph topology where file replicas are uniformly distributed, the hop distance to a replica of a file is logarithmic in the number of replicas. Using this observation we show that flooding-based search is optimized when the number of replicas is proportional to the file request rates. This replica distribution is also optimal for download time and since flooding has logarithmically better search time than random walk under its optimal replica distribution, we investigate the query-processing load using this distribution.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {404--405},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064276},
 doi = {http://doi.acm.org/10.1145/1071690.1064276},
 acmid = {1064276},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flooding, optimal file replication, peer-to-peer, random graphs, replication, search performance, unstructured networks},
} 

@inproceedings{Tewari:2005:ASR:1064212.1064276,
 author = {Tewari, Saurabh and Kleinrock, Leonard},
 title = {Analysis of search and replication in unstructured peer-to-peer networks},
 abstract = {This paper investigates the effect of the number of file replicas on search performance in unstructured peer-to-peer networks. We observe that for a search network with a random graph topology where file replicas are uniformly distributed, the hop distance to a replica of a file is logarithmic in the number of replicas. Using this observation we show that flooding-based search is optimized when the number of replicas is proportional to the file request rates. This replica distribution is also optimal for download time and since flooding has logarithmically better search time than random walk under its optimal replica distribution, we investigate the query-processing load using this distribution.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {404--405},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064276},
 doi = {http://doi.acm.org/10.1145/1064212.1064276},
 acmid = {1064276},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flooding, optimal file replication, peer-to-peer, random graphs, replication, search performance, unstructured networks},
} 

@article{Zhang:2005:IIS:1071690.1064277,
 author = {Zhang, Jianyong and Sivasubramaniam, Anand and Riska, Alma and Wang, Qian and Riedel, Erik},
 title = {An interposed 2-Level I/O scheduling framework for performance virtualization},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {406--407},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064277},
 doi = {http://doi.acm.org/10.1145/1071690.1064277},
 acmid = {1064277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {I/O scheduling, fairness, performance isolation, quality of service, storage systems, virtualization},
} 

@inproceedings{Zhang:2005:IIS:1064212.1064277,
 author = {Zhang, Jianyong and Sivasubramaniam, Anand and Riska, Alma and Wang, Qian and Riedel, Erik},
 title = {An interposed 2-Level I/O scheduling framework for performance virtualization},
 abstract = {},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {406--407},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064277},
 doi = {http://doi.acm.org/10.1145/1064212.1064277},
 acmid = {1064277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {I/O scheduling, fairness, performance isolation, quality of service, storage systems, virtualization},
} 

@inproceedings{Wenisch:2005:TAM:1064212.1064278,
 author = {Wenisch, Thomas F. and Wunderlich, Roland E. and Falsafi, Babak and Hoe, James C.},
 title = {TurboSMARTS: accurate microarchitecture simulation sampling in minutes},
 abstract = {Recent research proposes accelerating processor microarchitecture simulation through statistical sampling. Prior simulation sampling approaches construct accurate model state for each measurement by continuously warming large microarchitectural structures (e.g., caches and the branch predictor) while emulating the billions of instructions between measurements. This approach, called functional warming, occupies hours of runtime while the detailed simulation that is measured requires mere minutes.To eliminate the functional warming bottleneck, we propose TurboSMARTS, a simulation framework that stores functionally-warmed state in a library of small, reusable checkpoints. TurboSMARTS enables the creation of the thousands of checkpoints necessary for accurate sampling by storing only the subset of warmed state accessed during simulation of each brief execution window. TurboSMARTS matches the accuracy of prior simulation sampling techniques (i.e., \&#177;3\% error with 99.7\% confidence), while estimating the performance of an 8-way out-of-order super-scalar processor running SPEC CPU2000 in 91 seconds per benchmark, on average, using a 12 GB checkpoint library.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {408--409},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064278},
 doi = {http://doi.acm.org/10.1145/1064212.1064278},
 acmid = {1064278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointed microarchitecture simulation, simulation sampling},
} 

@article{Wenisch:2005:TAM:1071690.1064278,
 author = {Wenisch, Thomas F. and Wunderlich, Roland E. and Falsafi, Babak and Hoe, James C.},
 title = {TurboSMARTS: accurate microarchitecture simulation sampling in minutes},
 abstract = {Recent research proposes accelerating processor microarchitecture simulation through statistical sampling. Prior simulation sampling approaches construct accurate model state for each measurement by continuously warming large microarchitectural structures (e.g., caches and the branch predictor) while emulating the billions of instructions between measurements. This approach, called functional warming, occupies hours of runtime while the detailed simulation that is measured requires mere minutes.To eliminate the functional warming bottleneck, we propose TurboSMARTS, a simulation framework that stores functionally-warmed state in a library of small, reusable checkpoints. TurboSMARTS enables the creation of the thousands of checkpoints necessary for accurate sampling by storing only the subset of warmed state accessed during simulation of each brief execution window. TurboSMARTS matches the accuracy of prior simulation sampling techniques (i.e., \&#177;3\% error with 99.7\% confidence), while estimating the performance of an 8-way out-of-order super-scalar processor running SPEC CPU2000 in 91 seconds per benchmark, on average, using a 12 GB checkpoint library.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {408--409},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064278},
 doi = {http://doi.acm.org/10.1145/1071690.1064278},
 acmid = {1064278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointed microarchitecture simulation, simulation sampling},
} 

@inproceedings{Hu:2005:RCM:1064212.1064279,
 author = {Hu, Chunyu and Hou, Jennifer C.},
 title = {A reactive channel model for expediting wireless network simulation},
 abstract = {A major problem with leveraging event-driven, packet-level simulation environments, such as ns2</i>[6], J-Sim</i>[1], OpNet</i>[2]), and QualNet</i>[3]), in conducting wireless network simulation is the vast number of events generated, a majority of which are related to signal transmission in the PHY/MAC layers.In this extended abstract, we investigate the operations of signal transmission in the various stages: signal propagation</i>, signal interference</i>, and interaction with the PHY/MAC layers</i>, and identify where events can be reduced without impairing the accuracy. We propose to leverage the MAC/PHY state information, and devise (from the perspective of network simulation) a reactive channel model (RCM) in which nodes explicitly register</i> their interests in receiving certain events according to the MAC/PHY states they are in and the corresponding operations that should be performed. The simulation study indicates that RCM renders an order of magnitude of speed-up without compromising the accuracy of simulation results. An advantage of RCM with respect to the implementation is that there is no need to re-design the channel model for each specific MAC layer, and the modification made in the MAC/PHY layers is quite modest (e.g., a few API changes). This, coupled with the performance gain, suggests that RCM is an attractive, light-weight mechanism for expediting wireless network simulation.},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {410--411},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064279},
 doi = {http://doi.acm.org/10.1145/1064212.1064279},
 acmid = {1064279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel model, network simulation, reactive, scalability},
} 

@article{Hu:2005:RCM:1071690.1064279,
 author = {Hu, Chunyu and Hou, Jennifer C.},
 title = {A reactive channel model for expediting wireless network simulation},
 abstract = {A major problem with leveraging event-driven, packet-level simulation environments, such as ns2</i>[6], J-Sim</i>[1], OpNet</i>[2]), and QualNet</i>[3]), in conducting wireless network simulation is the vast number of events generated, a majority of which are related to signal transmission in the PHY/MAC layers.In this extended abstract, we investigate the operations of signal transmission in the various stages: signal propagation</i>, signal interference</i>, and interaction with the PHY/MAC layers</i>, and identify where events can be reduced without impairing the accuracy. We propose to leverage the MAC/PHY state information, and devise (from the perspective of network simulation) a reactive channel model (RCM) in which nodes explicitly register</i> their interests in receiving certain events according to the MAC/PHY states they are in and the corresponding operations that should be performed. The simulation study indicates that RCM renders an order of magnitude of speed-up without compromising the accuracy of simulation results. An advantage of RCM with respect to the implementation is that there is no need to re-design the channel model for each specific MAC layer, and the modification made in the MAC/PHY layers is quite modest (e.g., a few API changes). This, coupled with the performance gain, suggests that RCM is an attractive, light-weight mechanism for expediting wireless network simulation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {410--411},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064279},
 doi = {http://doi.acm.org/10.1145/1071690.1064279},
 acmid = {1064279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {channel model, network simulation, reactive, scalability},
} 

@article{Groenevelt:2005:MDM:1071690.1064280,
 author = {Groenevelt, Robin and Nain, Philippe and Koole, Ger},
 title = {Message delay in MANET},
 abstract = {A generic stochastic model with only two input parameters is introduced to evaluate the message delay in mobile ad hoc networks (MANETs) where nodes may relay messages. The Laplace-Stieltjes transform (LST) of the message delay is obtained for two protocols: the two-hop and the unrestricted multicopy protocol. From these results we deduce the expected message delays. It is shown that, despite its simplicity, the model accurately predicts the message delay under both relay strategies for a number of mobility models (the random waypoint, random direction and the random walker mobility models).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {33},
 issue = {1},
 month = {June},
 year = {2005},
 issn = {0163-5999},
 pages = {412--413},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1071690.1064280},
 doi = {http://doi.acm.org/10.1145/1071690.1064280},
 acmid = {1064280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay, estimation, mobile ad hoc, modeling, networks, performance prediction, statistics},
} 

@inproceedings{Groenevelt:2005:MDM:1064212.1064280,
 author = {Groenevelt, Robin and Nain, Philippe and Koole, Ger},
 title = {Message delay in MANET},
 abstract = {A generic stochastic model with only two input parameters is introduced to evaluate the message delay in mobile ad hoc networks (MANETs) where nodes may relay messages. The Laplace-Stieltjes transform (LST) of the message delay is obtained for two protocols: the two-hop and the unrestricted multicopy protocol. From these results we deduce the expected message delays. It is shown that, despite its simplicity, the model accurately predicts the message delay under both relay strategies for a number of mobility models (the random waypoint, random direction and the random walker mobility models).},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {412--413},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1064212.1064280},
 doi = {http://doi.acm.org/10.1145/1064212.1064280},
 acmid = {1064280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delay, estimation, mobile ad hoc, modeling, networks, performance prediction, statistics},
} 

@inproceedings{Girbal:2003:DSR:781027.781029,
 author = {Girbal, Sylvain and Mouchard, Gilles and Cohen, Albert and Temam, Olivier},
 title = {DiST: a simple, reliable and scalable method to significantly reduce processor architecture simulation time},
 abstract = {While architecture simulation is often treated as a methodology issue, it is at the core of most processor architecture research works, and simulation speed is often the bottleneck of the typical trial-and-error research process. To speedup simulation during this research process and get trends faster, researchers usually reduce the trace size. More sophisticated techniques like trace sampling or distributed simulation are scarcely used because they are considered unreliable and complex due to their impact on accuracy and the associated warm-up issues.In this article, we present DiST, a practical distributed simulation scheme where, unlike in other simulation techniques that trade accuracy for speed, the user is relieved from most accuracy issues thanks to an automatic and dynamic mechanism for adjusting the warm-up interval size. Moreover, the mechanism is designed so as to always privilege accuracy over speedup. The speedup scales with the amount of available computing resources, bringing an average 7.35 speedup on 10 machines with an average IPC error of 1.81\% and a maximum IPC error of 5.06\%.Besides proposing a solution to the warm-up issues in distributed simulation, we experimentally show that our technique is significantly more accurate than trace size reduction or trace sampling for identical speedups. We also show that not only the error always remains small for IPC and other metrics, but that a researcher can reliably base research decisions on DiST simulation results. Finally, we explain how the DiST tool is designed to be easily pluggable into existing architecture simulators with very few modifications.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781029},
 doi = {http://doi.acm.org/10.1145/781027.781029},
 acmid = {781029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed simulation, processor architecture},
} 

@article{Girbal:2003:DSR:885651.781029,
 author = {Girbal, Sylvain and Mouchard, Gilles and Cohen, Albert and Temam, Olivier},
 title = {DiST: a simple, reliable and scalable method to significantly reduce processor architecture simulation time},
 abstract = {While architecture simulation is often treated as a methodology issue, it is at the core of most processor architecture research works, and simulation speed is often the bottleneck of the typical trial-and-error research process. To speedup simulation during this research process and get trends faster, researchers usually reduce the trace size. More sophisticated techniques like trace sampling or distributed simulation are scarcely used because they are considered unreliable and complex due to their impact on accuracy and the associated warm-up issues.In this article, we present DiST, a practical distributed simulation scheme where, unlike in other simulation techniques that trade accuracy for speed, the user is relieved from most accuracy issues thanks to an automatic and dynamic mechanism for adjusting the warm-up interval size. Moreover, the mechanism is designed so as to always privilege accuracy over speedup. The speedup scales with the amount of available computing resources, bringing an average 7.35 speedup on 10 machines with an average IPC error of 1.81\% and a maximum IPC error of 5.06\%.Besides proposing a solution to the warm-up issues in distributed simulation, we experimentally show that our technique is significantly more accurate than trace size reduction or trace sampling for identical speedups. We also show that not only the error always remains small for IPC and other metrics, but that a researcher can reliably base research decisions on DiST simulation results. Finally, we explain how the DiST tool is designed to be easily pluggable into existing architecture simulators with very few modifications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781029},
 doi = {http://doi.acm.org/10.1145/885651.781029},
 acmid = {781029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed simulation, processor architecture},
} 

@article{Aamodt:2003:FMO:885651.781030,
 author = {Aamodt, Tor M. and Marcuello, Pedro and Chow, Paul and Gonz\'{a}lez, Antonio and Hammarlund, Per and Wang, Hong and Shen, John P.},
 title = {A framework for modeling and optimization of prescient instruction prefetch},
 abstract = {This paper describes a framework for modeling macroscopic program behavior and applies it to optimizing prescient instruction prefetch -- novel technique that uses helper threads to improve single-threaded application performance by performing judicious and timely instruction prefetch. A helper thread is initiated when the main thread encounters a spawn point, and prefetches instructions starting at a distant target point. The target identifies a code region tending to incur I-cache misses that the main thread is likely to execute soon, even though intervening control flow may be unpredictable. The optimization of spawn-target pair selections is formulated by modeling program behavior as a Markov chain based on profile statistics. Execution paths are considered stochastic outcomes, and aspects of program behavior are summarized via path expression mappings. Mappings for computing reaching, and posteriori probability; path length mean, and variance; and expected path footprint are presented. These are used with Tarjan's fast path algorithm to efficiently estimate the benefit of spawn-target pair selections. Using this framework we propose a spawn-target pair selection algorithm for prescient instruction prefetch. This algorithm has been implemented, and evaluated for the Itanium Processor Family architecture. A limit study finds 4.8\%to 17\% speedups on an in-order simultaneous multithreading processor with eight contexts, over nextline and streaming I-prefetch for a set of benchmarks with high I-cache miss rates. The framework in this paper is potentially applicable to other thread speculation techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781030},
 doi = {http://doi.acm.org/10.1145/885651.781030},
 acmid = {781030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, helper threads, instruction prefetch, multithreading, optimization, path expressions},
} 

@inproceedings{Aamodt:2003:FMO:781027.781030,
 author = {Aamodt, Tor M. and Marcuello, Pedro and Chow, Paul and Gonz\'{a}lez, Antonio and Hammarlund, Per and Wang, Hong and Shen, John P.},
 title = {A framework for modeling and optimization of prescient instruction prefetch},
 abstract = {This paper describes a framework for modeling macroscopic program behavior and applies it to optimizing prescient instruction prefetch -- novel technique that uses helper threads to improve single-threaded application performance by performing judicious and timely instruction prefetch. A helper thread is initiated when the main thread encounters a spawn point, and prefetches instructions starting at a distant target point. The target identifies a code region tending to incur I-cache misses that the main thread is likely to execute soon, even though intervening control flow may be unpredictable. The optimization of spawn-target pair selections is formulated by modeling program behavior as a Markov chain based on profile statistics. Execution paths are considered stochastic outcomes, and aspects of program behavior are summarized via path expression mappings. Mappings for computing reaching, and posteriori probability; path length mean, and variance; and expected path footprint are presented. These are used with Tarjan's fast path algorithm to efficiently estimate the benefit of spawn-target pair selections. Using this framework we propose a spawn-target pair selection algorithm for prescient instruction prefetch. This algorithm has been implemented, and evaluated for the Itanium Processor Family architecture. A limit study finds 4.8\%to 17\% speedups on an in-order simultaneous multithreading processor with eight contexts, over nextline and streaming I-prefetch for a set of benchmarks with high I-cache miss rates. The framework in this paper is potentially applicable to other thread speculation techniques.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781030},
 doi = {http://doi.acm.org/10.1145/781027.781030},
 acmid = {781030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, helper threads, instruction prefetch, multithreading, optimization, path expressions},
} 

@article{Xia:2003:QSL:885651.781032,
 author = {Xia, Cathy H. and Liu, Zhen},
 title = {Queueing systems with long-range dependent input process and subexponential service times},
 abstract = {We analyze the asymptotic tail distribution of stationary waiting times and stationary virtual waiting times in a single-server queue with long-range dependent arrival process and subexponential service times. We investigate the joint impact of the long range dependency of the arrival process and of the tail distribution of the service times. We consider two traffic models that have been widely used to characterize the long-range dependence structure, namely, the M/G/8</i> input model and the Fractional Gaussian Noise (FGN) model. We focus on the response times of the customers in a First-Come First-Serve (FCFS) queueing system, although the results carry through to the backlog distribution of the system with any arbitrary queueing discipline. When the arrival process is driven by an M/G/8</i> input model we show that if the residual service time tail distribution F<sub>e</sub></i> is lighter than the residual session duration G<sub>e</sub></i>, then the stationary waiting time is dominated by the long-range dependence structure, which is determined by the residual session duration G<sub>e</sub></i>. If the residual service time distribution F<sub>e</sub></i> is heavier than the residual session duration G<sub>e</sub></i>, then the tail distribution of the stationary waiting time is dominated by that of the residual service time. When the arrival process is modeled by an FGN, we show that the waiting time tail distribution is asymptotically equal to the tail distribution of the residual service time if the latter is asymptotically heavier than Weibull distribution with shape parameter 2-2H, where H is the Hurst parameter of the FGN. If, however, this residual service time is asymptotically lighter than Weibull distribution with shape parameter 2-2H, then the waiting time tail distribution is dominated by the dependence structure of the arrival process so that it is asymptotically equal to Weibull distribution with shape parameter 2-2H.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781032},
 doi = {http://doi.acm.org/10.1145/885651.781032},
 acmid = {781032},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FGN, M/G/8, asymptotic queueing analysis, long-range dependency, subexponential distributions},
} 

@inproceedings{Xia:2003:QSL:781027.781032,
 author = {Xia, Cathy H. and Liu, Zhen},
 title = {Queueing systems with long-range dependent input process and subexponential service times},
 abstract = {We analyze the asymptotic tail distribution of stationary waiting times and stationary virtual waiting times in a single-server queue with long-range dependent arrival process and subexponential service times. We investigate the joint impact of the long range dependency of the arrival process and of the tail distribution of the service times. We consider two traffic models that have been widely used to characterize the long-range dependence structure, namely, the M/G/8</i> input model and the Fractional Gaussian Noise (FGN) model. We focus on the response times of the customers in a First-Come First-Serve (FCFS) queueing system, although the results carry through to the backlog distribution of the system with any arbitrary queueing discipline. When the arrival process is driven by an M/G/8</i> input model we show that if the residual service time tail distribution F<sub>e</sub></i> is lighter than the residual session duration G<sub>e</sub></i>, then the stationary waiting time is dominated by the long-range dependence structure, which is determined by the residual session duration G<sub>e</sub></i>. If the residual service time distribution F<sub>e</sub></i> is heavier than the residual session duration G<sub>e</sub></i>, then the tail distribution of the stationary waiting time is dominated by that of the residual service time. When the arrival process is modeled by an FGN, we show that the waiting time tail distribution is asymptotically equal to the tail distribution of the residual service time if the latter is asymptotically heavier than Weibull distribution with shape parameter 2-2H, where H is the Hurst parameter of the FGN. If, however, this residual service time is asymptotically lighter than Weibull distribution with shape parameter 2-2H, then the waiting time tail distribution is dominated by the dependence structure of the arrival process so that it is asymptotically equal to Weibull distribution with shape parameter 2-2H.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781032},
 doi = {http://doi.acm.org/10.1145/781027.781032},
 acmid = {781032},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FGN, M/G/8, asymptotic queueing analysis, long-range dependency, subexponential distributions},
} 

@inproceedings{Galmes:2003:ACM:781027.781033,
 author = {Galm\'{e}s, Sebasti\`{a} and Puigjaner, Ramon},
 title = {An algorithm for computing the mean response time of a single server queue with generalized on/off traffic arrivals},
 abstract = {In this paper, an exact solution for the response time distribution of a single server, infinite capacity, discrete-time queue is presented. This queue is fed by a flexible discrete-time arrival process, which follows an on/off evolution. A workload variable is associated with each arrival instant, which may correspond to the service demand generated by a single arrival, or represent the number of simultaneous arrivals (bulk arrivals). Accordingly, the analysis focuses on two types of queues: (On/Off)/G/1 and (Batch-On/Off)/D/1. For both cases, a decomposition approach is carried out, which divides the problem into two contributions: the response time experienced by single bursts in isolation, and the increase on the response time caused by the unfinished work that propagates from burst to burst. Particularly, the solution for the unfinished work is derived from a Wiener-Hopf factorization of random walks, which was already used in the analysis of discrete GI/G/1 queues. Compared to other related works, the procedure proposed in this paper is exact, valid for any traffic intensity and has no constraints on the distributions of the input random variables characterizing the process: duration of on and off periods, and workload. From the general solution, an efficient and robust iterative algorithm for computing the expected response time of both queues is developed, which can provide results at any desired precision. This algorithm is numerically evaluated for different types of input distributions and proved against simulation.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/781027.781033},
 doi = {http://doi.acm.org/10.1145/781027.781033},
 acmid = {781033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain, arrival process, queuing model, response time, steady-state},
} 

@article{Galmes:2003:ACM:885651.781033,
 author = {Galm\'{e}s, Sebasti\`{a} and Puigjaner, Ramon},
 title = {An algorithm for computing the mean response time of a single server queue with generalized on/off traffic arrivals},
 abstract = {In this paper, an exact solution for the response time distribution of a single server, infinite capacity, discrete-time queue is presented. This queue is fed by a flexible discrete-time arrival process, which follows an on/off evolution. A workload variable is associated with each arrival instant, which may correspond to the service demand generated by a single arrival, or represent the number of simultaneous arrivals (bulk arrivals). Accordingly, the analysis focuses on two types of queues: (On/Off)/G/1 and (Batch-On/Off)/D/1. For both cases, a decomposition approach is carried out, which divides the problem into two contributions: the response time experienced by single bursts in isolation, and the increase on the response time caused by the unfinished work that propagates from burst to burst. Particularly, the solution for the unfinished work is derived from a Wiener-Hopf factorization of random walks, which was already used in the analysis of discrete GI/G/1 queues. Compared to other related works, the procedure proposed in this paper is exact, valid for any traffic intensity and has no constraints on the distributions of the input random variables characterizing the process: duration of on and off periods, and workload. From the general solution, an efficient and robust iterative algorithm for computing the expected response time of both queues is developed, which can provide results at any desired precision. This algorithm is numerically evaluated for different types of input distributions and proved against simulation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/885651.781033},
 doi = {http://doi.acm.org/10.1145/885651.781033},
 acmid = {781033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chain, arrival process, queuing model, response time, steady-state},
} 

@article{Garetto:2003:MSM:885651.781034,
 author = {Garetto, Michele and Towsley, Don},
 title = {Modeling, simulation and measurements of queuing delay under long-tail internet traffic},
 abstract = {In this paper we describe an analytical approach for estimating the queuing delay distribution on an Internet link carrying realistic TCP traffic, such as that produced by a large number of finite-size connections transferring files whose sizes are taken from a long-tail distribution. The analytical predictions are validated against detailed simulation experiments and real network measurements. Despite its simplicity, our model proves to be accurate and robust under a variety of operating conditions, and offers novel insights into the impact on the network of long-tail flow length distributions. Our contribution is a performance evaluation methodology that could be usefully employed in network dimensioning and engineering.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {47--57},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781034},
 doi = {http://doi.acm.org/10.1145/885651.781034},
 acmid = {781034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markovian models, TCP, queueing analysis},
} 

@inproceedings{Garetto:2003:MSM:781027.781034,
 author = {Garetto, Michele and Towsley, Don},
 title = {Modeling, simulation and measurements of queuing delay under long-tail internet traffic},
 abstract = {In this paper we describe an analytical approach for estimating the queuing delay distribution on an Internet link carrying realistic TCP traffic, such as that produced by a large number of finite-size connections transferring files whose sizes are taken from a long-tail distribution. The analytical predictions are validated against detailed simulation experiments and real network measurements. Despite its simplicity, our model proves to be accurate and robust under a variety of operating conditions, and offers novel insights into the impact on the network of long-tail flow length distributions. Our contribution is a performance evaluation methodology that could be usefully employed in network dimensioning and engineering.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {47--57},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781034},
 doi = {http://doi.acm.org/10.1145/781027.781034},
 acmid = {781034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markovian models, TCP, queueing analysis},
} 

@inproceedings{Bohacek:2003:HSM:781027.781036,
 author = {Bohacek, Stephan and Hespanha, Jo\~{a}o P. and Lee, Junsoo and Obraczka, Katia},
 title = {A hybrid systems modeling framework for fast and accurate simulation of data communication networks},
 abstract = {In this paper we present a general hybrid systems modeling framework to describe the flow of traffic in communication networks. To characterize network behavior, these models use averaging to continuously approximate discrete variables such as congestion window and queue size. Because averaging occurs over short time intervals, one still models discrete events such as the occurrence of a drop and the consequent reaction (e.g., congestion control). The proposed hybrid systems modeling framework fills the gap between packet-level and fluid-based models: by averaging discrete variables over a very short time scale (on the order of a round-trip time), our models are able to capture the dynamics of transient phenomena fairly accurately. This provides significant flexibility in modeling various congestion control mechanisms, different queuing policies, multicast transmission, etc. We validate our hybrid modeling methodology by comparing simulations of the hybrid models against packet-level simulations. We find that the probability density functions produced by \&lt;tt\&gt;ns-2\&lt;/tt\&gt; and our hybrid model match very closely with an L<sup>1</sup></i>-distance of less than 1\%. We also present complexity analysis of \&lt;tt\&gt;ns-2\&lt;/tt\&gt; and the hybrid model. These tests indicate that hybrid models are considerably faster.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {58--69},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781036},
 doi = {http://doi.acm.org/10.1145/781027.781036},
 acmid = {781036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, UDP, congestion control, data communication networks, hybrid systems, simulation},
} 

@article{Bohacek:2003:HSM:885651.781036,
 author = {Bohacek, Stephan and Hespanha, Jo\~{a}o P. and Lee, Junsoo and Obraczka, Katia},
 title = {A hybrid systems modeling framework for fast and accurate simulation of data communication networks},
 abstract = {In this paper we present a general hybrid systems modeling framework to describe the flow of traffic in communication networks. To characterize network behavior, these models use averaging to continuously approximate discrete variables such as congestion window and queue size. Because averaging occurs over short time intervals, one still models discrete events such as the occurrence of a drop and the consequent reaction (e.g., congestion control). The proposed hybrid systems modeling framework fills the gap between packet-level and fluid-based models: by averaging discrete variables over a very short time scale (on the order of a round-trip time), our models are able to capture the dynamics of transient phenomena fairly accurately. This provides significant flexibility in modeling various congestion control mechanisms, different queuing policies, multicast transmission, etc. We validate our hybrid modeling methodology by comparing simulations of the hybrid models against packet-level simulations. We find that the probability density functions produced by \&lt;tt\&gt;ns-2\&lt;/tt\&gt; and our hybrid model match very closely with an L<sup>1</sup></i>-distance of less than 1\%. We also present complexity analysis of \&lt;tt\&gt;ns-2\&lt;/tt\&gt; and the hybrid model. These tests indicate that hybrid models are considerably faster.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {58--69},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781036},
 doi = {http://doi.acm.org/10.1145/885651.781036},
 acmid = {781036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, UDP, congestion control, data communication networks, hybrid systems, simulation},
} 

@article{Samios:2003:MTT:885651.781037,
 author = {Samios, Charalampos (Babis) and Vernon, Mary K.},
 title = {Modeling the throughput of TCP Vegas},
 abstract = {Previous analytic models of TCP Vegas throughput have been developed for loss-free (all-Vegas) networks. This work develops a simple and accurate analytic model for the throughput of a TCP Vegas bulk transfer in the presence of packet loss, as a function of average round trip time, minimum round trip time, and loss rate for the transfer. Similar models have previously been developed for TCP Reno. However, several aspects of TCP Vegas need to be treated differently than their counterparts in Reno. The proposed model captures the key innovative mechanisms that Vegas employs during slow start, congestion avoidance, and congestion recovery. The results include (1) a simple, validated model of TCP Vegas throughput that can be used for equation-based rate control of other flows such as UDP streams, (2) a simple formula to determine, from the measured packet loss rate, whether the network buffers are overcommitted and thus the TCP Vegas flow cannot reach the specified target lower threshold on throughput, (3) new insights into the design and performance of TCP Vegas, and (4) comparisons between TCP Vegas and TCP Reno including new insights regarding incremental deployment of TCP Vegas.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781037},
 doi = {http://doi.acm.org/10.1145/885651.781037},
 acmid = {781037},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, TCP Vegas, performance model, throughput},
} 

@inproceedings{Samios:2003:MTT:781027.781037,
 author = {Samios, Charalampos (Babis) and Vernon, Mary K.},
 title = {Modeling the throughput of TCP Vegas},
 abstract = {Previous analytic models of TCP Vegas throughput have been developed for loss-free (all-Vegas) networks. This work develops a simple and accurate analytic model for the throughput of a TCP Vegas bulk transfer in the presence of packet loss, as a function of average round trip time, minimum round trip time, and loss rate for the transfer. Similar models have previously been developed for TCP Reno. However, several aspects of TCP Vegas need to be treated differently than their counterparts in Reno. The proposed model captures the key innovative mechanisms that Vegas employs during slow start, congestion avoidance, and congestion recovery. The results include (1) a simple, validated model of TCP Vegas throughput that can be used for equation-based rate control of other flows such as UDP streams, (2) a simple formula to determine, from the measured packet loss rate, whether the network buffers are overcommitted and thus the TCP Vegas flow cannot reach the specified target lower threshold on throughput, (3) new insights into the design and performance of TCP Vegas, and (4) comparisons between TCP Vegas and TCP Reno including new insights regarding incremental deployment of TCP Vegas.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {71--81},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781037},
 doi = {http://doi.acm.org/10.1145/781027.781037},
 acmid = {781037},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, TCP Vegas, performance model, throughput},
} 

@inproceedings{Wang:2003:MAU:781027.781038,
 author = {Wang, Jiantao and Tang, Ao and Low, Steven H.},
 title = {Maximum and asymptotic UDP throughput under CHOKe},
 abstract = {A recently proposed active queue management, CHOKe, aims to protect TCP from UDP flows. Simulations have shown that as UDP rate increases, its bandwidth share initially rises but eventually drops. We derive an approximate model of CHOKe and show that, provided the number of TCP flows is large, the UDP bandwidth share peaks at (e+1)<sup>-1</sup> = 0.269</i> when the UDP input rate is slightly larger than the link capacity, and drops to zero as UDP input rate tends to infinity, regardless of the TCP algorithm.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {82--90},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/781027.781038},
 doi = {http://doi.acm.org/10.1145/781027.781038},
 acmid = {781038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AQM, CHOKe, TCP, UDP, bandwidth share},
} 

@article{Wang:2003:MAU:885651.781038,
 author = {Wang, Jiantao and Tang, Ao and Low, Steven H.},
 title = {Maximum and asymptotic UDP throughput under CHOKe},
 abstract = {A recently proposed active queue management, CHOKe, aims to protect TCP from UDP flows. Simulations have shown that as UDP rate increases, its bandwidth share initially rises but eventually drops. We derive an approximate model of CHOKe and show that, provided the number of TCP flows is large, the UDP bandwidth share peaks at (e+1)<sup>-1</sup> = 0.269</i> when the UDP input rate is slightly larger than the link capacity, and drops to zero as UDP input rate tends to infinity, regardless of the TCP algorithm.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {82--90},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/885651.781038},
 doi = {http://doi.acm.org/10.1145/885651.781038},
 acmid = {781038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AQM, CHOKe, TCP, UDP, bandwidth share},
} 

@article{Liu:2003:FMS:885651.781039,
 author = {Liu, Yong and Lo Presti, Francesco and Misra, Vishal and Towsley, Don and Gu, Yu},
 title = {Fluid models and solutions for large-scale IP networks},
 abstract = {In this paper we present a scalable model of a network of Active Queue Management (AQM) routers serving a large population of TCP flows. We present efficient solution techniques that allow one to obtain the transient behavior of the average queue lengths, packet loss probabilities, and average end-to-end latencies. We model different versions of TCP as well as different versions of RED, the most popular AQM scheme currently in use. Comparisons between our models and \&lt;tt\&gt;ns\&lt;/tt\&gt; simulation show our models to be quite accurate while at the same time requiring substantially less time to solve, especially when workloads and bandwidths are high.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {91--101},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781039},
 doi = {http://doi.acm.org/10.1145/885651.781039},
 acmid = {781039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fluid model, large-scale IP networks, simulation},
} 

@inproceedings{Liu:2003:FMS:781027.781039,
 author = {Liu, Yong and Lo Presti, Francesco and Misra, Vishal and Towsley, Don and Gu, Yu},
 title = {Fluid models and solutions for large-scale IP networks},
 abstract = {In this paper we present a scalable model of a network of Active Queue Management (AQM) routers serving a large population of TCP flows. We present efficient solution techniques that allow one to obtain the transient behavior of the average queue lengths, packet loss probabilities, and average end-to-end latencies. We model different versions of TCP as well as different versions of RED, the most popular AQM scheme currently in use. Comparisons between our models and \&lt;tt\&gt;ns\&lt;/tt\&gt; simulation show our models to be quite accurate while at the same time requiring substantially less time to solve, especially when workloads and bandwidths are high.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {91--101},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781039},
 doi = {http://doi.acm.org/10.1145/781027.781039},
 acmid = {781039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fluid model, large-scale IP networks, simulation},
} 

@inproceedings{Banerjee:2003:RMU:781027.781041,
 author = {Banerjee, Suman and Lee, Seungjoon and Bhattacharjee, Bobby and Srinivasan, Aravind},
 title = {Resilient multicast using overlays},
 abstract = {We introduce PRM (Probabilistic Resilient Multicast): a multicast data recovery scheme that improves data delivery ratios while maintaining low end-to-end latencies. PRM has both a proactive and a reactive component; in this paper we describe how PRM can be used to improve the performance of application-layer multicast protocols, especially when there are high packet losses and host failures. Further, using analytic techniques, we show that PRM can guarantee arbitrarily high data delivery ratios and low latency bounds. As a detailed case study, we show how PRM can be applied to the NICE application-layer multicast protocol. We present detailed simulations of the PRM-enhanced NICE protocol for 10,000 node Internet-like topologies. Simulations show that PRM achieves a high delivery ratio (\&gt;</i> 97\%) with a low latency bound (600 ms) for environments with high end-to-end network losses (1-5\%) and high topology change rates (5 changes per second) while incurring very low overheads (\&lt;</i> 5\%).},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {102--113},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781041},
 doi = {http://doi.acm.org/10.1145/781027.781041},
 acmid = {781041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {overlay multicast, randomized forwarding, resilience},
} 

@article{Banerjee:2003:RMU:885651.781041,
 author = {Banerjee, Suman and Lee, Seungjoon and Bhattacharjee, Bobby and Srinivasan, Aravind},
 title = {Resilient multicast using overlays},
 abstract = {We introduce PRM (Probabilistic Resilient Multicast): a multicast data recovery scheme that improves data delivery ratios while maintaining low end-to-end latencies. PRM has both a proactive and a reactive component; in this paper we describe how PRM can be used to improve the performance of application-layer multicast protocols, especially when there are high packet losses and host failures. Further, using analytic techniques, we show that PRM can guarantee arbitrarily high data delivery ratios and low latency bounds. As a detailed case study, we show how PRM can be applied to the NICE application-layer multicast protocol. We present detailed simulations of the PRM-enhanced NICE protocol for 10,000 node Internet-like topologies. Simulations show that PRM achieves a high delivery ratio (\&gt;</i> 97\%) with a low latency bound (600 ms) for environments with high end-to-end network losses (1-5\%) and high topology change rates (5 changes per second) while incurring very low overheads (\&lt;</i> 5\%).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {102--113},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781041},
 doi = {http://doi.acm.org/10.1145/885651.781041},
 acmid = {781041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {overlay multicast, randomized forwarding, resilience},
} 

@article{Zhang:2003:IIL:885651.781042,
 author = {Zhang, Hui and Goel, Ashish and Govindan, Ramesh},
 title = {Incrementally improving lookup latency in distributed hash table systems},
 abstract = {Distributed hash table (DHT) systems are an important class of peer-to-peer routing infrastructures. They enable scalable wide-area storage and retrieval of information, and will support the rapid development of a wide variety of Internet-scale applications ranging from naming systems and file systems to application-layer multicast. DHT systems essentially build an overlay network, but a path on the overlay between any two nodes can be significantly different from the unicast path between those two nodes on the underlying network. As such, the lookup latency in these systems can be quite high and can adversely impact the performance of applications built on top of such systems.In this paper, we discuss a random sampling technique that incrementally improves lookup latency in DHT systems. Our sampling can be implemented using information gleaned from lookups traversing the overlay network. For this reason, we call our approach lookup-parasitic random sampling</i> (LPRS). LPRS is fast, incurs little network overhead, and requires relatively few modifications to existing DHT systems.For idealized versions of DHT systems like Chord, Tapestry and Pastry, we analytically prove that LPRS can result in lookup latencies proportional to the average unicast latency of the network, provided the underlying physical topology has a power-law latency expansion. We then validate this analysis by implementing LPRS in the Chord simulator. Our simulations reveal that LPRS-Chord exhibits a qualitatively better latency scaling behavior relative to unmodified Chord.Finally, we provide evidence which suggests that the Internet router-level topology resembles power-law latency expansion. This finding implies that LPRS has significant practical applicability as a general latency reduction technique for many DHT systems. This finding is also of independent interest since it might inform the design of latency-sensitive topology models for the Internet.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {114--125},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781042},
 doi = {http://doi.acm.org/10.1145/885651.781042},
 acmid = {781042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DHT, latency stretch, peer-to-peer, random sampling},
} 

@inproceedings{Zhang:2003:IIL:781027.781042,
 author = {Zhang, Hui and Goel, Ashish and Govindan, Ramesh},
 title = {Incrementally improving lookup latency in distributed hash table systems},
 abstract = {Distributed hash table (DHT) systems are an important class of peer-to-peer routing infrastructures. They enable scalable wide-area storage and retrieval of information, and will support the rapid development of a wide variety of Internet-scale applications ranging from naming systems and file systems to application-layer multicast. DHT systems essentially build an overlay network, but a path on the overlay between any two nodes can be significantly different from the unicast path between those two nodes on the underlying network. As such, the lookup latency in these systems can be quite high and can adversely impact the performance of applications built on top of such systems.In this paper, we discuss a random sampling technique that incrementally improves lookup latency in DHT systems. Our sampling can be implemented using information gleaned from lookups traversing the overlay network. For this reason, we call our approach lookup-parasitic random sampling</i> (LPRS). LPRS is fast, incurs little network overhead, and requires relatively few modifications to existing DHT systems.For idealized versions of DHT systems like Chord, Tapestry and Pastry, we analytically prove that LPRS can result in lookup latencies proportional to the average unicast latency of the network, provided the underlying physical topology has a power-law latency expansion. We then validate this analysis by implementing LPRS in the Chord simulator. Our simulations reveal that LPRS-Chord exhibits a qualitatively better latency scaling behavior relative to unmodified Chord.Finally, we provide evidence which suggests that the Internet router-level topology resembles power-law latency expansion. This finding implies that LPRS has significant practical applicability as a general latency reduction technique for many DHT systems. This finding is also of independent interest since it might inform the design of latency-sensitive topology models for the Internet.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {114--125},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781042},
 doi = {http://doi.acm.org/10.1145/781027.781042},
 acmid = {781042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DHT, latency stretch, peer-to-peer, random sampling},
} 

@article{Feamster:2003:MEI:885651.781043,
 author = {Feamster, Nick and Andersen, David G. and Balakrishnan, Hari and Kaashoek, M. Frans},
 title = {Measuring the effects of internet path faults on reactive routing},
 abstract = {Empirical evidence suggests that reactive routing systems improve resilience to Internet path failures. They detect and route around faulty paths based on measurements of path performance. This paper seeks to understand why</i> and under what circumstances</i> these techniques are effective.To do so, this paper correlates end-to-end active probing experiments, loss-triggered traceroutes of Internet paths, and BGP routing messages. These correlations shed light on three questions about Internet path failures: (1) Where do failures appear? (2) How long do they last? (3) How do they correlate with BGP routing instability?Data collected over 13 months from an Internet testbed of 31 topologically diverse hosts suggests that most path failures last less than fifteen minutes. Failures that appear in the network core correlate better with BGP instability than failures that appear close to end hosts. On average, most failures precede BGP messages by about four minutes, but there is often increased BGP traffic both before and after failures. Our findings suggest that reactive routing is most effective between hosts that have multiple connections to the Internet. The data set also suggests that passive observations of BGP routing messages could be used to predict about 20\% of impending failures, allowing re-routing systems to react more quickly to failures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {126--137},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781043},
 doi = {http://doi.acm.org/10.1145/885651.781043},
 acmid = {781043},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Feamster:2003:MEI:781027.781043,
 author = {Feamster, Nick and Andersen, David G. and Balakrishnan, Hari and Kaashoek, M. Frans},
 title = {Measuring the effects of internet path faults on reactive routing},
 abstract = {Empirical evidence suggests that reactive routing systems improve resilience to Internet path failures. They detect and route around faulty paths based on measurements of path performance. This paper seeks to understand why</i> and under what circumstances</i> these techniques are effective.To do so, this paper correlates end-to-end active probing experiments, loss-triggered traceroutes of Internet paths, and BGP routing messages. These correlations shed light on three questions about Internet path failures: (1) Where do failures appear? (2) How long do they last? (3) How do they correlate with BGP routing instability?Data collected over 13 months from an Internet testbed of 31 topologically diverse hosts suggests that most path failures last less than fifteen minutes. Failures that appear in the network core correlate better with BGP instability than failures that appear close to end hosts. On average, most failures precede BGP messages by about four minutes, but there is often increased BGP traffic both before and after failures. Our findings suggest that reactive routing is most effective between hosts that have multiple connections to the Internet. The data set also suggests that passive observations of BGP routing messages could be used to predict about 20\% of impending failures, allowing re-routing systems to react more quickly to failures.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {126--137},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781043},
 doi = {http://doi.acm.org/10.1145/781027.781043},
 acmid = {781043},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yegneswaran:2003:IIG:781027.781045,
 author = {Yegneswaran, Vinod and Barford, Paul and Ullrich, Johannes},
 title = {Internet intrusions: global characteristics and prevalence},
 abstract = {Network intrusions have been a fact of life in the Internet for many years. However, as is the case with many other types of Internet-wide phenomena, gaining insight into the global</i> characteristics of intrusions is challenging. In this paper we address this problem by systematically analyzing a set of firewall logs collected over four months from over 1600 different networks world wide. The first part of our study is a general analysis focused on the issues of distribution, categorization and prevalence of intrusions. Our data shows both a large quantity and wide variety of intrusion attempts on a daily basis. We also find that worms like CodeRed, Nimda and SQL Snake persist long after their original release. By projecting intrusion activity as seen in our data sets to the entire Internet we determine that there are typically on the order of 25B intrusion attempts per day and that there is an increasing trend over our measurement period. We further find that sources of intrusions are uniformly spread across the Autonomous System space. However, deeper investigation reveals that a very small collection of sources are responsible for a significant fraction of intrusion attempts in any given month and their on/off patterns exhibit cliques of correlated behavior. We show that the distribution of source IP addresses of the non-worm intrusions as a function of the number of attempts follows Zipf's law. We also find that at daily timescales, intrusion targets often depict significant spatial trends that blur patterns observed from individual "IP telescopes"; this underscores the necessity for a more global approach to intrusion detection. Finally, we investigate the benefits of shared information, and the potential for using this as a foundation for an automated, global intrusion detection framework that would identify and isolate intrusions with greater precision and robustness than systems with limited perspective.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/781027.781045},
 doi = {http://doi.acm.org/10.1145/781027.781045},
 acmid = {781045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet performance and monitoring, network security, wide area measurement},
} 

@article{Yegneswaran:2003:IIG:885651.781045,
 author = {Yegneswaran, Vinod and Barford, Paul and Ullrich, Johannes},
 title = {Internet intrusions: global characteristics and prevalence},
 abstract = {Network intrusions have been a fact of life in the Internet for many years. However, as is the case with many other types of Internet-wide phenomena, gaining insight into the global</i> characteristics of intrusions is challenging. In this paper we address this problem by systematically analyzing a set of firewall logs collected over four months from over 1600 different networks world wide. The first part of our study is a general analysis focused on the issues of distribution, categorization and prevalence of intrusions. Our data shows both a large quantity and wide variety of intrusion attempts on a daily basis. We also find that worms like CodeRed, Nimda and SQL Snake persist long after their original release. By projecting intrusion activity as seen in our data sets to the entire Internet we determine that there are typically on the order of 25B intrusion attempts per day and that there is an increasing trend over our measurement period. We further find that sources of intrusions are uniformly spread across the Autonomous System space. However, deeper investigation reveals that a very small collection of sources are responsible for a significant fraction of intrusion attempts in any given month and their on/off patterns exhibit cliques of correlated behavior. We show that the distribution of source IP addresses of the non-worm intrusions as a function of the number of attempts follows Zipf's law. We also find that at daily timescales, intrusion targets often depict significant spatial trends that blur patterns observed from individual "IP telescopes"; this underscores the necessity for a more global approach to intrusion detection. Finally, we investigate the benefits of shared information, and the potential for using this as a foundation for an automated, global intrusion detection framework that would identify and isolate intrusions with greater precision and robustness than systems with limited perspective.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/885651.781045},
 doi = {http://doi.acm.org/10.1145/885651.781045},
 acmid = {781045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {internet performance and monitoring, network security, wide area measurement},
} 

@article{Gkantsidis:2003:CCP:885651.781046,
 author = {Gkantsidis, Christos and Mihail, Milena and Saberi, Amin},
 title = {Conductance and congestion in power law graphs},
 abstract = {It has been observed that the degrees of the topologies of several communication networks follow heavy tailed statistics. What is the impact of such heavy tailed statistics on the performance of basic communication tasks that a network is presumed to support? How does performance scale with the size of the network? We study routing in families of sparse random graphs whose degrees follow heavy tailed distributions. Instantiations of such random graphs have been proposed as models for the topology of the Internet at the level of Autonomous Systems as well as at the level of routers. Let n</i> be the number of nodes. Suppose that for each pair of nodes with degrees d<sub>u</sub></i> and d<sub>v</sub></i> we have O(d<sub>u</sub> d<sub>v</sub>)</i> units of demand. Thus the total demand is O(n<sup>2</sup>)</i>. We argue analytically and experimentally that in the considered random graph model such demand patterns can be routed so that the flow through each link is at most O(n log<sup>2</sup> n)</i>. This is to be compared with a bound O(n<sup>2</sup>)</i> that holds for arbitrary graphs. Similar results were previously known for sparse random regular graphs, a.k.a. "expander graphs." The significance is that Internet-like topologies, which grow in a dynamic, decentralized fashion and appear highly inhomogeneous, can support routing with performance characteristics comparable to those of their regular counterparts, at least under the assumption of uniform demand and capacities. Our proof uses approximation algorithms for multicommodity flow and establishes strong bounds of a generalization of "expansion," namely "conductance." Besides routing, our bounds on conductance have further implications, most notably on the gap between first and second eigenvalues of the stochastic normalization of the adjacency matrix of the graph.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {148--159},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781046},
 doi = {http://doi.acm.org/10.1145/885651.781046},
 acmid = {781046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conductance, congestion, expansion, internet topology, powerlaw graphs, routing},
} 

@inproceedings{Gkantsidis:2003:CCP:781027.781046,
 author = {Gkantsidis, Christos and Mihail, Milena and Saberi, Amin},
 title = {Conductance and congestion in power law graphs},
 abstract = {It has been observed that the degrees of the topologies of several communication networks follow heavy tailed statistics. What is the impact of such heavy tailed statistics on the performance of basic communication tasks that a network is presumed to support? How does performance scale with the size of the network? We study routing in families of sparse random graphs whose degrees follow heavy tailed distributions. Instantiations of such random graphs have been proposed as models for the topology of the Internet at the level of Autonomous Systems as well as at the level of routers. Let n</i> be the number of nodes. Suppose that for each pair of nodes with degrees d<sub>u</sub></i> and d<sub>v</sub></i> we have O(d<sub>u</sub> d<sub>v</sub>)</i> units of demand. Thus the total demand is O(n<sup>2</sup>)</i>. We argue analytically and experimentally that in the considered random graph model such demand patterns can be routed so that the flow through each link is at most O(n log<sup>2</sup> n)</i>. This is to be compared with a bound O(n<sup>2</sup>)</i> that holds for arbitrary graphs. Similar results were previously known for sparse random regular graphs, a.k.a. "expander graphs." The significance is that Internet-like topologies, which grow in a dynamic, decentralized fashion and appear highly inhomogeneous, can support routing with performance characteristics comparable to those of their regular counterparts, at least under the assumption of uniform demand and capacities. Our proof uses approximation algorithms for multicommodity flow and establishes strong bounds of a generalization of "expansion," namely "conductance." Besides routing, our bounds on conductance have further implications, most notably on the gap between first and second eigenvalues of the stochastic normalization of the adjacency matrix of the graph.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {148--159},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781046},
 doi = {http://doi.acm.org/10.1145/781027.781046},
 acmid = {781046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conductance, congestion, expansion, internet topology, powerlaw graphs, routing},
} 

@inproceedings{Li:2003:RME:781027.781048,
 author = {Li, Tao and John, Lizy Kurian},
 title = {Run-time modeling and estimation of operating system power consumption},
 abstract = {The increasing constraints on power consumption in many computing systems point to the need for power modeling and estimation for all components of a system. The Operating System (OS) constitutes a major software component and dissipates a significant portion of total power in many modern application executions. Therefore, modeling OS power is imperative for accurate software power evaluation, as well as power management (e.g. dynamic thermal control and equal energy scheduling) in the light of OS-intensive workloads. This paper characterizes the power behavior of a commercial OS across a wide spectrum of applications to understand OS energy profiles and then proposes various models to cost-effectively estimate its run-time energy dissipation. The proposed models rely on a few simple parameters and have various degrees of complexity and accuracy. Experiments show that compared with cycle-accurate full-system simulation, the model can predict cumulative OS energy to within 1\% accuracy for a set of benchmark programs evaluated on a high-end superscalar microprocessor. When applied to track run-time OS energy profiles, the proposed routine level OS power model offers superior accuracy than a simpler, flat OS power model, yielding per-routine estimation error of less than 6\%. The most striking observation is the strong correlation between power consumption and the instructions per cycle (IPC) during OS routine executions. Since tools and methodology to measure IPC exist on modern microprocessors, the proposed models can estimate OS power for run-time dynamic thermal and energy management.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {160--171},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781048},
 doi = {http://doi.acm.org/10.1145/781027.781048},
 acmid = {781048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low power, operating system, power estimation},
} 

@article{Li:2003:RME:885651.781048,
 author = {Li, Tao and John, Lizy Kurian},
 title = {Run-time modeling and estimation of operating system power consumption},
 abstract = {The increasing constraints on power consumption in many computing systems point to the need for power modeling and estimation for all components of a system. The Operating System (OS) constitutes a major software component and dissipates a significant portion of total power in many modern application executions. Therefore, modeling OS power is imperative for accurate software power evaluation, as well as power management (e.g. dynamic thermal control and equal energy scheduling) in the light of OS-intensive workloads. This paper characterizes the power behavior of a commercial OS across a wide spectrum of applications to understand OS energy profiles and then proposes various models to cost-effectively estimate its run-time energy dissipation. The proposed models rely on a few simple parameters and have various degrees of complexity and accuracy. Experiments show that compared with cycle-accurate full-system simulation, the model can predict cumulative OS energy to within 1\% accuracy for a set of benchmark programs evaluated on a high-end superscalar microprocessor. When applied to track run-time OS energy profiles, the proposed routine level OS power model offers superior accuracy than a simpler, flat OS power model, yielding per-routine estimation error of less than 6\%. The most striking observation is the strong correlation between power consumption and the instructions per cycle (IPC) during OS routine executions. Since tools and methodology to measure IPC exist on modern microprocessors, the proposed models can estimate OS power for run-time dynamic thermal and energy management.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {160--171},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781048},
 doi = {http://doi.acm.org/10.1145/885651.781048},
 acmid = {781048},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low power, operating system, power estimation},
} 

@inproceedings{Etsion:2003:ECR:781027.781049,
 author = {Etsion, Yoav and Tsafrir, Dan and Feitelson, Dror G.},
 title = {Effects of clock resolution on the scheduling of interactive and soft real-time processes},
 abstract = {It is commonly agreed that scheduling mechanisms in general purpose operating systems do not provide adequate support for modern interactive applications, notably multimedia applications. The common solution to this problem is to devise specialized scheduling mechanisms that take the specific needs of such applications into account. A much simpler alternative is to better tune existing systems. In particular, we show that conventional scheduling algorithms typically only have little and possibly misleading information regarding the CPU usage of processes, because increasing CPU rates have caused the common 100 Hz clock interrupt rate to be coarser than most application time quanta. We therefore conduct an experimental analysis of what happens if this rate is significantly increased. Results indicate that much higher clock interrupt rates are possible with acceptable overheads, and lead to much better information. In addition we show that increasing the clock rate can provide a measure of support for soft real time requirements, even when using a general-purpose operating system. For example, we achieve a sub-millisecond latency under heavily loaded conditions.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {172--183},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781049},
 doi = {http://doi.acm.org/10.1145/781027.781049},
 acmid = {781049},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Linux, clock interrupt rate, interactive process, overhead, scheduling, soft real-time, tuning},
} 

@article{Etsion:2003:ECR:885651.781049,
 author = {Etsion, Yoav and Tsafrir, Dan and Feitelson, Dror G.},
 title = {Effects of clock resolution on the scheduling of interactive and soft real-time processes},
 abstract = {It is commonly agreed that scheduling mechanisms in general purpose operating systems do not provide adequate support for modern interactive applications, notably multimedia applications. The common solution to this problem is to devise specialized scheduling mechanisms that take the specific needs of such applications into account. A much simpler alternative is to better tune existing systems. In particular, we show that conventional scheduling algorithms typically only have little and possibly misleading information regarding the CPU usage of processes, because increasing CPU rates have caused the common 100 Hz clock interrupt rate to be coarser than most application time quanta. We therefore conduct an experimental analysis of what happens if this rate is significantly increased. Results indicate that much higher clock interrupt rates are possible with acceptable overheads, and lead to much better information. In addition we show that increasing the clock rate can provide a measure of support for soft real time requirements, even when using a general-purpose operating system. For example, we achieve a sub-millisecond latency under heavily loaded conditions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {172--183},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781049},
 doi = {http://doi.acm.org/10.1145/885651.781049},
 acmid = {781049},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Linux, clock interrupt rate, interactive process, overhead, scheduling, soft real-time, tuning},
} 

@inproceedings{Osogami:2003:ACS:781027.781050,
 author = {Osogami, Takayuki and Harchol-Balter, Mor and Scheller-Wolf, Alan},
 title = {Analysis of cycle stealing with switching cost},
 abstract = {We consider the scenario of two processors, each serving its own workload, where one of the processors (known as the "donor") can help the other processor (known as the "beneficiary") with its jobs, during times when the donor processor is idle. That is the beneficiary processor "steals idle cycles" from the donor processor. We assume that both donor jobs and beneficiary jobs may have generally-distributed service requirements. We assume that there is a switching cost required for the donor processor to start working on the beneficiary jobs, as well as a switching cost required for the donor processor to return to working on its own jobs. We also allow for threshold constraints, whereby the donor processor only initiates helping the beneficiary if both the donor is idle and the number of jobs at the beneficiary exceeds a certain threshold.We analyze the mean response time for the donor and beneficiary processors. Our analysis is approximate, but can be made as accurate as desired, and is validated via simulation. Results of the analysis illuminate several interesting principles with respect to the general benefits of cycle stealing and the design of cycle stealing policies.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {184--195},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781050},
 doi = {http://doi.acm.org/10.1145/781027.781050},
 acmid = {781050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cycle stealing, distributed system, load sharing, matrix analytic, server farm, starvation, supercomputing, task assignment, unfairness},
} 

@article{Osogami:2003:ACS:885651.781050,
 author = {Osogami, Takayuki and Harchol-Balter, Mor and Scheller-Wolf, Alan},
 title = {Analysis of cycle stealing with switching cost},
 abstract = {We consider the scenario of two processors, each serving its own workload, where one of the processors (known as the "donor") can help the other processor (known as the "beneficiary") with its jobs, during times when the donor processor is idle. That is the beneficiary processor "steals idle cycles" from the donor processor. We assume that both donor jobs and beneficiary jobs may have generally-distributed service requirements. We assume that there is a switching cost required for the donor processor to start working on the beneficiary jobs, as well as a switching cost required for the donor processor to return to working on its own jobs. We also allow for threshold constraints, whereby the donor processor only initiates helping the beneficiary if both the donor is idle and the number of jobs at the beneficiary exceeds a certain threshold.We analyze the mean response time for the donor and beneficiary processors. Our analysis is approximate, but can be made as accurate as desired, and is validated via simulation. Results of the analysis illuminate several interesting principles with respect to the general benefits of cycle stealing and the design of cycle stealing policies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {184--195},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781050},
 doi = {http://doi.acm.org/10.1145/885651.781050},
 acmid = {781050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cycle stealing, distributed system, load sharing, matrix analytic, server farm, starvation, supercomputing, task assignment, unfairness},
} 

@inproceedings{Ye:2003:RRS:781027.781052,
 author = {Ye, Tao and Kalyanaraman, Shivkumar},
 title = {A recursive random search algorithm for large-scale network parameter configuration},
 abstract = {Parameter configuration is a common procedure used in large-scale network protocols to support multiple operational goals. It can be formulated as a black-box optimization problem and solved with an efficient search algorithm. This paper proposes a new heuristic search algorithm, Recursive Random Search(RRS), for large-scale network parameter optimization. The RRS algorithm is based on the initial high-efficiency feature of random sampling and it attempts to maintain this high efficiency by constantly "restarting" random sampling with adjusted sample spaces. Besides the high efficiency, the RRS algorithm is robust to the effect of random noise and trivial parameters in the objective function because of its root in random sampling. These features are very important for the efficient optimization of network protocol configuration. The performance of RRS is demonstrated with the tests on a suite of benchmark functions. The algorithm has been applied to the configuration of several network protocols, such as RED, OSPF and BGP. One example application in OSPF routing algorithm is presented.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {196--205},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/781027.781052},
 doi = {http://doi.acm.org/10.1145/781027.781052},
 acmid = {781052},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black-box optimization, global optimization, network management, random sampling},
} 

@article{Ye:2003:RRS:885651.781052,
 author = {Ye, Tao and Kalyanaraman, Shivkumar},
 title = {A recursive random search algorithm for large-scale network parameter configuration},
 abstract = {Parameter configuration is a common procedure used in large-scale network protocols to support multiple operational goals. It can be formulated as a black-box optimization problem and solved with an efficient search algorithm. This paper proposes a new heuristic search algorithm, Recursive Random Search(RRS), for large-scale network parameter optimization. The RRS algorithm is based on the initial high-efficiency feature of random sampling and it attempts to maintain this high efficiency by constantly "restarting" random sampling with adjusted sample spaces. Besides the high efficiency, the RRS algorithm is robust to the effect of random noise and trivial parameters in the objective function because of its root in random sampling. These features are very important for the efficient optimization of network protocol configuration. The performance of RRS is demonstrated with the tests on a suite of benchmark functions. The algorithm has been applied to the configuration of several network protocols, such as RED, OSPF and BGP. One example application in OSPF routing algorithm is presented.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {196--205},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/885651.781052},
 doi = {http://doi.acm.org/10.1145/885651.781052},
 acmid = {781052},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {black-box optimization, global optimization, network management, random sampling},
} 

@inproceedings{Zhang:2003:FAC:781027.781053,
 author = {Zhang, Yin and Roughan, Matthew and Duffield, Nick and Greenberg, Albert},
 title = {Fast accurate computation of large-scale IP traffic matrices from link loads},
 abstract = {A matrix giving the traffic volumes between origin and destination in a network has tremendously potential utility for network capacity planning and management. Unfortunately, traffic matrices are generally unavailable in large operational IP networks. On the other hand, link load measurements are readily available in IP networks. In this paper, we propose a new method for practical and rapid inference of traffic matrices in IP networks from link load measurements, augmented by readily available network and routing configuration information. We apply and validate the method by computing backbone-router to backbone-router traffic matrices on a large operational tier-1 IP network -- a problem an order of magnitude larger than any other comparable method has tackled. The results show that the method is remarkably fast and accurate, delivering the traffic matrix in under five seconds.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {206--217},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781053},
 doi = {http://doi.acm.org/10.1145/781027.781053},
 acmid = {781053},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SNMP, traffic engineering, traffic matrix estimation},
} 

@article{Zhang:2003:FAC:885651.781053,
 author = {Zhang, Yin and Roughan, Matthew and Duffield, Nick and Greenberg, Albert},
 title = {Fast accurate computation of large-scale IP traffic matrices from link loads},
 abstract = {A matrix giving the traffic volumes between origin and destination in a network has tremendously potential utility for network capacity planning and management. Unfortunately, traffic matrices are generally unavailable in large operational IP networks. On the other hand, link load measurements are readily available in IP networks. In this paper, we propose a new method for practical and rapid inference of traffic matrices in IP networks from link load measurements, augmented by readily available network and routing configuration information. We apply and validate the method by computing backbone-router to backbone-router traffic matrices on a large operational tier-1 IP network -- a problem an order of magnitude larger than any other comparable method has tackled. The results show that the method is remarkably fast and accurate, delivering the traffic matrix in under five seconds.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {206--217},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781053},
 doi = {http://doi.acm.org/10.1145/885651.781053},
 acmid = {781053},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SNMP, traffic engineering, traffic matrix estimation},
} 

@article{Rai:2003:ALS:885651.781055,
 author = {Rai, Idris A. and Urvoy-Keller, Guillaume and Biersack, Ernst W.},
 title = {Analysis of LAS scheduling for job size distributions with high variance},
 abstract = {Recent studies of Internet traffic have shown that flow size distributions often exhibit a high variability property in the sense that most of the flows are short and more than half of the total load is constituted by a small percentage of the largest flows. In the light of this observation, it is interesting to revisit scheduling policies that are known to favor small jobs in order to quantify the benefit for small and the penalty for large jobs. Among all scheduling policies that do not require knowledge of job size, the least attained service</i> (LAS) scheduling policy is known to favor small jobs the most. We investigate the M/G/1/LAS queue for both, load ? \&lt; 1</i> and ? = 1</i>. Our analysis shows that for job size distributions with a high variability property, LAS favors short jobs with a negligible penalty to the few largest jobs, and that LAS achieves a mean response time over all jobs that is close to the mean response time achieved by SRPT.Finally, we implement LAS in the ns-2 network simulator to study its performance benefits for TCP flows. When LAS is used to schedule packets over the bottleneck link, more than 99\% of the shortest flows experience smaller mean response times under LAS than under FIFO and only the largest jobs observe a negligible increase in response time. The benefit of using LAS as compared to FIFO is most pronounced at high load.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {218--228},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781055},
 doi = {http://doi.acm.org/10.1145/885651.781055},
 acmid = {781055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high variability property, least attained service, size-based scheduling, web objects response time},
} 

@inproceedings{Rai:2003:ALS:781027.781055,
 author = {Rai, Idris A. and Urvoy-Keller, Guillaume and Biersack, Ernst W.},
 title = {Analysis of LAS scheduling for job size distributions with high variance},
 abstract = {Recent studies of Internet traffic have shown that flow size distributions often exhibit a high variability property in the sense that most of the flows are short and more than half of the total load is constituted by a small percentage of the largest flows. In the light of this observation, it is interesting to revisit scheduling policies that are known to favor small jobs in order to quantify the benefit for small and the penalty for large jobs. Among all scheduling policies that do not require knowledge of job size, the least attained service</i> (LAS) scheduling policy is known to favor small jobs the most. We investigate the M/G/1/LAS queue for both, load ? \&lt; 1</i> and ? = 1</i>. Our analysis shows that for job size distributions with a high variability property, LAS favors short jobs with a negligible penalty to the few largest jobs, and that LAS achieves a mean response time over all jobs that is close to the mean response time achieved by SRPT.Finally, we implement LAS in the ns-2 network simulator to study its performance benefits for TCP flows. When LAS is used to schedule packets over the bottleneck link, more than 99\% of the shortest flows experience smaller mean response times under LAS than under FIFO and only the largest jobs observe a negligible increase in response time. The benefit of using LAS as compared to FIFO is most pronounced at high load.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {218--228},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781055},
 doi = {http://doi.acm.org/10.1145/781027.781055},
 acmid = {781055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high variability property, least attained service, size-based scheduling, web objects response time},
} 

@article{Friedman:2003:FEW:885651.781056,
 author = {Friedman, Eric J. and Henderson, Shane G.},
 title = {Fairness and efficiency in web server protocols},
 abstract = {We consider the problem of designing a preemptive protocol that is both fair and efficient when one is only concerned with the sojourn time of the job and not intermediate results. Our Fair Sojourn Protocol (FSP) is both efficient, in a strong sense (similar to the shortest remaining processing time protocol: SRPT), and fair, in the sense of guaranteeing that it weakly outperforms processor sharing (PS) for every job on any sample path.Our primary motivation is web serving in which the standard protocol is PS, while recent work proposes using SRPT or variants. Our work suggests both a framework in which to evaluate proposed protocols and an attractive new protocol, FSP.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {229--237},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/885651.781056},
 doi = {http://doi.acm.org/10.1145/885651.781056},
 acmid = {781056},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {processor sharing, starvation avoidance},
} 

@inproceedings{Friedman:2003:FEW:781027.781056,
 author = {Friedman, Eric J. and Henderson, Shane G.},
 title = {Fairness and efficiency in web server protocols},
 abstract = {We consider the problem of designing a preemptive protocol that is both fair and efficient when one is only concerned with the sojourn time of the job and not intermediate results. Our Fair Sojourn Protocol (FSP) is both efficient, in a strong sense (similar to the shortest remaining processing time protocol: SRPT), and fair, in the sense of guaranteeing that it weakly outperforms processor sharing (PS) for every job on any sample path.Our primary motivation is web serving in which the standard protocol is PS, while recent work proposes using SRPT or variants. Our work suggests both a framework in which to evaluate proposed protocols and an attractive new protocol, FSP.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {229--237},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/781027.781056},
 doi = {http://doi.acm.org/10.1145/781027.781056},
 acmid = {781056},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {processor sharing, starvation avoidance},
} 

@inproceedings{Wierman:2003:CSP:781027.781057,
 author = {Wierman, Adam and Harchol-Balter, Mor},
 title = {Classifying scheduling policies with respect to unfairness in an M/GI/1},
 abstract = {It is common to evaluate scheduling policies based on their mean response times. Another important, but sometimes opposing, performance metric is a scheduling policy's fairness. For example, a policy that biases towards small job sizes so as to minimize mean response time may end up being unfair to large job sizes. In this paper we define three types of unfairness and demonstrate large classes of scheduling policies that fall into each type. We end with a discussion on which jobs are the ones being treated unfairly.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {238--249},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781057},
 doi = {http://doi.acm.org/10.1145/781027.781057},
 acmid = {781057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FB, LAS, M/G/1, PS, SET, SRPT, feedback, least attained service, processor sharing, scheduling, shortest elapsed time, shortest remaining processing time, slowdown, unfairness},
} 

@article{Wierman:2003:CSP:885651.781057,
 author = {Wierman, Adam and Harchol-Balter, Mor},
 title = {Classifying scheduling policies with respect to unfairness in an M/GI/1},
 abstract = {It is common to evaluate scheduling policies based on their mean response times. Another important, but sometimes opposing, performance metric is a scheduling policy's fairness. For example, a policy that biases towards small job sizes so as to minimize mean response time may end up being unfair to large job sizes. In this paper we define three types of unfairness and demonstrate large classes of scheduling policies that fall into each type. We end with a discussion on which jobs are the ones being treated unfairly.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {238--249},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781057},
 doi = {http://doi.acm.org/10.1145/885651.781057},
 acmid = {781057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FB, LAS, M/G/1, PS, SET, SRPT, feedback, least attained service, processor sharing, scheduling, shortest elapsed time, shortest remaining processing time, slowdown, unfairness},
} 

@inproceedings{Alouf:2003:EMM:781027.781059,
 author = {Alouf, Sara and Altman, Eitan and Barakat, Chadi and Nain, Philippe},
 title = {Estimating membership in a multicast session},
 abstract = {We propose two novel on-line estimation algorithms to determine the size of a dynamic multicast group. We first use a Wiener filter to derive an optimal estimator for the membership size of the session in case the join process is Poisson and the lifetime of participants is distributed exponentially. We next develop the best first-order linear filter from which we derive an estimator that holds for any lifetime distribution. We apply this approach to the case where the lifetime distribution is hyperexponential. Both estimators hold under any traffic regime. Applying both estimators on real traces corresponding to video sessions, we find that both schemes behave well, one of which performs slightly better than the other in some cases. We further provide guidelines on how to tune the parameters involved in both schemes in order to achieve high quality estimation while simultaneously avoiding feedback implosion.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {250--260},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781059},
 doi = {http://doi.acm.org/10.1145/781027.781059},
 acmid = {781059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/8 queue, Wiener filter, first-order linear filter, membership size, multicast applications, on-line estimation},
} 

@article{Alouf:2003:EMM:885651.781059,
 author = {Alouf, Sara and Altman, Eitan and Barakat, Chadi and Nain, Philippe},
 title = {Estimating membership in a multicast session},
 abstract = {We propose two novel on-line estimation algorithms to determine the size of a dynamic multicast group. We first use a Wiener filter to derive an optimal estimator for the membership size of the session in case the join process is Poisson and the lifetime of participants is distributed exponentially. We next develop the best first-order linear filter from which we derive an estimator that holds for any lifetime distribution. We apply this approach to the case where the lifetime distribution is hyperexponential. Both estimators hold under any traffic regime. Applying both estimators on real traces corresponding to video sessions, we find that both schemes behave well, one of which performs slightly better than the other in some cases. We further provide guidelines on how to tune the parameters involved in both schemes in order to achieve high quality estimation while simultaneously avoiding feedback implosion.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {250--260},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781059},
 doi = {http://doi.acm.org/10.1145/885651.781059},
 acmid = {781059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/8 queue, Wiener filter, first-order linear filter, membership size, multicast applications, on-line estimation},
} 

@inproceedings{Ramabhadran:2003:EIS:781027.781060,
 author = {Ramabhadran, Sriram and Varghese, George},
 title = {Efficient implementation of a statistics counter architecture},
 abstract = {Internet routers and switches need to maintain millions of (e.g., per prefix) counters at up to OC-768 speeds that are essential for traffic engineering. Unfortunately, the speed requirements require the use of large amounts of expensive SRAM memory. Shah et al [1]introduced a cheaper statistics counter architecture that uses a much smaller amount of SRAM by using the SRAM as a cache together with a (cheap) backing DRAM that stores the complete counters. Counters in SRAM are periodically updated to the DRAM before they overflow under the control of a counter management algorithm. Shah et al [1] also devised a counter management algorithm called LCF</i> that they prove uses an optimal amount of SRAM. Unfortunately, it is difficult to implement LCF</i> at high speeds because it requires sorting to evict the largest counter in the SRAM. This paper removes this bottleneck in [1] by proposing a counter management algorithm called LR(T)</i> (Largest Recent with thresh-old T</i>) that avoids sorting by only keeping a bitmap that tracks counters that are larger than threshold T</i>. This allows LR(T)</i> to be practically realizable using only at most 2 bits extra per counter and a simple pipelined data structure. Despite this, we show through a formal analysis, that for a particular value of the threshold T</i>, the LR(T)</i> requires an optimal amount of SRAM, matching LCF</i>. Further,we also describe an implementation, based on a novel data structure called aggregated bitmap, that allows the LR(T)</i> algorithm to be realized at line rates.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {261--271},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781060},
 doi = {http://doi.acm.org/10.1145/781027.781060},
 acmid = {781060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {router, statistics counter},
} 

@article{Ramabhadran:2003:EIS:885651.781060,
 author = {Ramabhadran, Sriram and Varghese, George},
 title = {Efficient implementation of a statistics counter architecture},
 abstract = {Internet routers and switches need to maintain millions of (e.g., per prefix) counters at up to OC-768 speeds that are essential for traffic engineering. Unfortunately, the speed requirements require the use of large amounts of expensive SRAM memory. Shah et al [1]introduced a cheaper statistics counter architecture that uses a much smaller amount of SRAM by using the SRAM as a cache together with a (cheap) backing DRAM that stores the complete counters. Counters in SRAM are periodically updated to the DRAM before they overflow under the control of a counter management algorithm. Shah et al [1] also devised a counter management algorithm called LCF</i> that they prove uses an optimal amount of SRAM. Unfortunately, it is difficult to implement LCF</i> at high speeds because it requires sorting to evict the largest counter in the SRAM. This paper removes this bottleneck in [1] by proposing a counter management algorithm called LR(T)</i> (Largest Recent with thresh-old T</i>) that avoids sorting by only keeping a bitmap that tracks counters that are larger than threshold T</i>. This allows LR(T)</i> to be practically realizable using only at most 2 bits extra per counter and a simple pipelined data structure. Despite this, we show through a formal analysis, that for a particular value of the threshold T</i>, the LR(T)</i> requires an optimal amount of SRAM, matching LCF</i>. Further,we also describe an implementation, based on a novel data structure called aggregated bitmap, that allows the LR(T)</i> algorithm to be realized at line rates.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {261--271},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781060},
 doi = {http://doi.acm.org/10.1145/885651.781060},
 acmid = {781060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {router, statistics counter},
} 

@article{Vera:2003:DCL:885651.781062,
 author = {Vera, Xavier and Lisper, Bj\"{o}rn and Xue, Jingling},
 title = {Data cache locking for higher program predictability},
 abstract = {Caches have become increasingly important with the widening gap between main memory and processor speeds. However, they are a source of unpredictability due to their characteristics, resulting in programs behaving in a different way than expected.Cache locking mechanisms adapt caches to the needs of real-time systems. Locking the cache is a solution that trades performance for predictability: at a cost of generally lower performance, the time of accessing the memory becomes predictable.This paper combines compile-time cache analysis</i> with data cache locking</i> to estimate the worst-case memory performance (WCMP) in a safe, tight and fast way. In order to get predictable cache behavior, we first lock the cache for those parts of the code where the static analysis fails. To minimize the performance degradation, our method loads the cache, if necessary, with data likely to be accessed.Experimental results show that this scheme is fully predictable, without compromising the performance of the transformed program. When compared to an algorithm that assumes compulsory misses when the state of the cache is unknown, our approach eliminates all overestimation for the set of benchmarks, giving an exact WCMP of the transformed program without any significant decrease in performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/885651.781062},
 doi = {http://doi.acm.org/10.1145/885651.781062},
 acmid = {781062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data cache analysis, worst-case execution time},
} 

@inproceedings{Vera:2003:DCL:781027.781062,
 author = {Vera, Xavier and Lisper, Bj\"{o}rn and Xue, Jingling},
 title = {Data cache locking for higher program predictability},
 abstract = {Caches have become increasingly important with the widening gap between main memory and processor speeds. However, they are a source of unpredictability due to their characteristics, resulting in programs behaving in a different way than expected.Cache locking mechanisms adapt caches to the needs of real-time systems. Locking the cache is a solution that trades performance for predictability: at a cost of generally lower performance, the time of accessing the memory becomes predictable.This paper combines compile-time cache analysis</i> with data cache locking</i> to estimate the worst-case memory performance (WCMP) in a safe, tight and fast way. In order to get predictable cache behavior, we first lock the cache for those parts of the code where the static analysis fails. To minimize the performance degradation, our method loads the cache, if necessary, with data likely to be accessed.Experimental results show that this scheme is fully predictable, without compromising the performance of the transformed program. When compared to an algorithm that assumes compulsory misses when the state of the cache is unknown, our approach eliminates all overestimation for the set of benchmarks, giving an exact WCMP of the transformed program without any significant decrease in performance.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {272--282},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/781027.781062},
 doi = {http://doi.acm.org/10.1145/781027.781062},
 acmid = {781062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data cache analysis, worst-case execution time},
} 

@article{Hankins:2003:ENS:885651.781063,
 author = {Hankins, Richard A. and Patel, Jignesh M.},
 title = {Effect of node size on the performance of cache-conscious B<sup>+</sup>-trees},
 abstract = {In main-memory databases, the number of processor cache misses has a critical impact on the performance of the system. Cache-conscious indices are designed to improve performance by reducing the number of processor cache misses that are incurred during a search operation. Conventional wisdom suggests that the index's node size should be equal to the cache line size in order to minimize the number of cache misses and improve performance. As we show in this paper, this design choice ignores additional effects, such as the number of instructions executed and the number of TLB misses, which play a significant role in determining the overall performance. To capture the impact of node size on the performance of a cache-conscious B+ tree (CSB+-tree), we first develop an analytical model based on the fundamental components of the search process. This model is then validated with an actual implementation, demonstrating that the model is accurate. Both the analytical model and experiments confirm that using node sizes much larger than the cache line size can result in better search performance for the CSB+-tree.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/885651.781063},
 doi = {http://doi.acm.org/10.1145/885651.781063},
 acmid = {781063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {B<sup>+</sup>-tree, cache-conscious, index},
} 

@inproceedings{Hankins:2003:ENS:781027.781063,
 author = {Hankins, Richard A. and Patel, Jignesh M.},
 title = {Effect of node size on the performance of cache-conscious B<sup>+</sup>-trees},
 abstract = {In main-memory databases, the number of processor cache misses has a critical impact on the performance of the system. Cache-conscious indices are designed to improve performance by reducing the number of processor cache misses that are incurred during a search operation. Conventional wisdom suggests that the index's node size should be equal to the cache line size in order to minimize the number of cache misses and improve performance. As we show in this paper, this design choice ignores additional effects, such as the number of instructions executed and the number of TLB misses, which play a significant role in determining the overall performance. To capture the impact of node size on the performance of a cache-conscious B+ tree (CSB+-tree), we first develop an analytical model based on the fundamental components of the search process. This model is then validated with an actual implementation, demonstrating that the model is accurate. Both the analytical model and experiments confirm that using node sizes much larger than the cache line size can result in better search performance for the CSB+-tree.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {283--294},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/781027.781063},
 doi = {http://doi.acm.org/10.1145/781027.781063},
 acmid = {781063},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {B<sup>+</sup>-tree, cache-conscious, index},
} 

@inproceedings{Wang:2003:ATC:781027.781065,
 author = {Wang, Bokyung and Singh, Suresh},
 title = {Analysis of TCP's computational energy cost for mobile computing},
 abstract = {In this paper we present results from a measurement study of TCP (Transmission Control Protocol) running over a wireless link. Our primary goal was on obtaining a breakdown of the computational energy cost of TCP at the sender and receiver (excluding radio energy costs) as a first step in developing techniques to reduce this cost in actual systems. We analyzed the energy consumption of TCP in FreeBSD 5 running on a wireless laptop. Our initial results showed that 60 - 70\% of the energy cost (for transmission or reception) is accounted for by the Kernel -- NIC (Network Interface Card) copy operation. Of the remainder, 15\% is accounted for in the copy operation from user space to kernel space with the remaining 15\% being accounted for by TCP processing costs. We then further analyzed the TCP processing cost and determined the cost of computing checksums accounts for 20 -- 30\% of TCP processing cost.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {296--297},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781065},
 doi = {http://doi.acm.org/10.1145/781027.781065},
 acmid = {781065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, energy, mobile, wireless},
} 

@article{Wang:2003:ATC:885651.781065,
 author = {Wang, Bokyung and Singh, Suresh},
 title = {Analysis of TCP's computational energy cost for mobile computing},
 abstract = {In this paper we present results from a measurement study of TCP (Transmission Control Protocol) running over a wireless link. Our primary goal was on obtaining a breakdown of the computational energy cost of TCP at the sender and receiver (excluding radio energy costs) as a first step in developing techniques to reduce this cost in actual systems. We analyzed the energy consumption of TCP in FreeBSD 5 running on a wireless laptop. Our initial results showed that 60 - 70\% of the energy cost (for transmission or reception) is accounted for by the Kernel -- NIC (Network Interface Card) copy operation. Of the remainder, 15\% is accounted for in the copy operation from user space to kernel space with the remaining 15\% being accounted for by TCP processing costs. We then further analyzed the TCP processing cost and determined the cost of computing checksums accounts for 20 -- 30\% of TCP processing cost.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {296--297},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781065},
 doi = {http://doi.acm.org/10.1145/885651.781065},
 acmid = {781065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, energy, mobile, wireless},
} 

@inproceedings{Dutta:2003:TGP:781027.781066,
 author = {Dutta, Rudra and Huang, Shu and Rouskas, George N.},
 title = {Traffic grooming in path, star, and tree networks: complexity, bounds, and algorithms},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {298--299},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781066},
 doi = {http://doi.acm.org/10.1145/781027.781066},
 acmid = {781066},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dutta:2003:TGP:885651.781066,
 author = {Dutta, Rudra and Huang, Shu and Rouskas, George N.},
 title = {Traffic grooming in path, star, and tree networks: complexity, bounds, and algorithms},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {298--299},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781066},
 doi = {http://doi.acm.org/10.1145/885651.781066},
 acmid = {781066},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chandra:2003:DRA:781027.781067,
 author = {Chandra, Abhishek and Gong, Weibo and Shenoy, Prashant},
 title = {Dynamic resource allocation for shared data centers using online measurements},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {300--301},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781067},
 doi = {http://doi.acm.org/10.1145/781027.781067},
 acmid = {781067},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptation, data centers, prediction, resource allocation},
} 

@article{Chandra:2003:DRA:885651.781067,
 author = {Chandra, Abhishek and Gong, Weibo and Shenoy, Prashant},
 title = {Dynamic resource allocation for shared data centers using online measurements},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {300--301},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781067},
 doi = {http://doi.acm.org/10.1145/885651.781067},
 acmid = {781067},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptation, data centers, prediction, resource allocation},
} 

@article{Zhang:2003:SSA:885651.781068,
 author = {Zhang, Honggang and Towsley, Don and Hollot, C. V. and Misra, Vishal},
 title = {A self-tuning structure for adaptation in TCP/AQM networks},
 abstract = {Congestion control in TCP/AQM networks is expected to perform well for a wide-range of conditions, but recent advances in modeling and analysis indicate that present AQM schemes need an extra dose of adaptability to cope. This paper answers the call and proposes a self-tuning structure wherein AQM parameters are automatically tuned in response to on-line estimation of link capacity and traffic load. This approach is applicable to any AQM scheme that is parameterizable in terms of link capacity and TCP load. In this paper, we will describe this self-tuning structure, illustrate its application to PI and RED AQMs, provide stability analysis, and conduct \&lt;tt\&gt;ns\&lt;/tt\&gt; simulations to compare with both fixed AQM schemes and the recently proposed adaptive RED.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {302--303},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781068},
 doi = {http://doi.acm.org/10.1145/885651.781068},
 acmid = {781068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, active queue management, adaptive control, self-tuning},
} 

@inproceedings{Zhang:2003:SSA:781027.781068,
 author = {Zhang, Honggang and Towsley, Don and Hollot, C. V. and Misra, Vishal},
 title = {A self-tuning structure for adaptation in TCP/AQM networks},
 abstract = {Congestion control in TCP/AQM networks is expected to perform well for a wide-range of conditions, but recent advances in modeling and analysis indicate that present AQM schemes need an extra dose of adaptability to cope. This paper answers the call and proposes a self-tuning structure wherein AQM parameters are automatically tuned in response to on-line estimation of link capacity and traffic load. This approach is applicable to any AQM scheme that is parameterizable in terms of link capacity and TCP load. In this paper, we will describe this self-tuning structure, illustrate its application to PI and RED AQMs, provide stability analysis, and conduct \&lt;tt\&gt;ns\&lt;/tt\&gt; simulations to compare with both fixed AQM schemes and the recently proposed adaptive RED.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {302--303},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781068},
 doi = {http://doi.acm.org/10.1145/781027.781068},
 acmid = {781068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, active queue management, adaptive control, self-tuning},
} 

@article{Teixeira:2003:CMP:885651.781069,
 author = {Teixeira, Renata and Marzullo, Keith and Savage, Stefan and Voelker, Geoffrey M.},
 title = {Characterizing and measuring path diversity of internet topologies},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {304--305},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781069},
 doi = {http://doi.acm.org/10.1145/885651.781069},
 acmid = {781069},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Teixeira:2003:CMP:781027.781069,
 author = {Teixeira, Renata and Marzullo, Keith and Savage, Stefan and Voelker, Geoffrey M.},
 title = {Characterizing and measuring path diversity of internet topologies},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {304--305},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781069},
 doi = {http://doi.acm.org/10.1145/781027.781069},
 acmid = {781069},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goyal:2003:CPQ:885651.781070,
 author = {Goyal, Pawan and Modha, Dharmendra S. and Tewari, Renu},
 title = {CacheCOW: providing QoS for storage system caches},
 abstract = {Managed hosting and enterprise wide resource consolidation trends are increasingly leading to sharing of storage resources across multiple classes, corresponding to different applications/customers, each with a possibly different Quality of Service (QoS) requirement. To enable a storage system to meet diverse QoS requirements, we present two algorithms for dynamically allocating cache space among multiple classes of workloads. Our algorithms dynamically adapt the cache space allocated to each class in response to the observed response time, the temporal locality of reference, and the arrival pattern for each class. Using trace driven simulations collected from large storage system installations, we experimentally demonstrate that the algorithms not only meet the QoS requirements, but also increase the throughput by achieving a higher hit rate whenever feasible.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {306--307},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781070},
 doi = {http://doi.acm.org/10.1145/885651.781070},
 acmid = {781070},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caching, quality-of-service, storage systems},
} 

@inproceedings{Goyal:2003:CPQ:781027.781070,
 author = {Goyal, Pawan and Modha, Dharmendra S. and Tewari, Renu},
 title = {CacheCOW: providing QoS for storage system caches},
 abstract = {Managed hosting and enterprise wide resource consolidation trends are increasingly leading to sharing of storage resources across multiple classes, corresponding to different applications/customers, each with a possibly different Quality of Service (QoS) requirement. To enable a storage system to meet diverse QoS requirements, we present two algorithms for dynamically allocating cache space among multiple classes of workloads. Our algorithms dynamically adapt the cache space allocated to each class in response to the observed response time, the temporal locality of reference, and the arrival pattern for each class. Using trace driven simulations collected from large storage system installations, we experimentally demonstrate that the algorithms not only meet the QoS requirements, but also increase the throughput by achieving a higher hit rate whenever feasible.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {306--307},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781070},
 doi = {http://doi.acm.org/10.1145/781027.781070},
 acmid = {781070},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caching, quality-of-service, storage systems},
} 

@inproceedings{Lumetta:2003:CED:781027.781071,
 author = {Lumetta, Steven S. and Patel, Sanjay J.},
 title = {Characterization of essential dynamic instructions},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {308--309},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781071},
 doi = {http://doi.acm.org/10.1145/781027.781071},
 acmid = {781071},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic instruction stream, reverse analysis},
} 

@article{Lumetta:2003:CED:885651.781071,
 author = {Lumetta, Steven S. and Patel, Sanjay J.},
 title = {Characterization of essential dynamic instructions},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {308--309},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781071},
 doi = {http://doi.acm.org/10.1145/885651.781071},
 acmid = {781071},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic instruction stream, reverse analysis},
} 

@article{Chakraborty:2003:EMP:885651.781072,
 author = {Chakraborty, Srijan and Yau, David K. Y. and Lui, John C. S.},
 title = {On the effectiveness of movement prediction to reduce energy consumption in wireless communication},
 abstract = {Node movement can be exploited to reduce the energy consumption of wireless network communication. The strategy consists in delaying communication until a mobile node moves close to its target</i> peer node, within an application-imposed deadline. We evaluate the performance of various heuristics that, based on the movement history of the mobile node, estimate an optimal time (in the sense of least energy use) of communication subject to the delay constraint. We evaluate the impact of node movement model, length of movement history maintained, allowable delay, single hop versus multiple hop communication, and size of data transfer on the energy consumption. We also present measurement results on an iPAQ pocket PC that quantify energy consumption in executing the prediction algorithms. Our results show that, with relatively simple and hence efficient prediction heuristics, energy savings in communication can significantly outweigh the energy expenses in executing the prediction algorithms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {310--311},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781072},
 doi = {http://doi.acm.org/10.1145/885651.781072},
 acmid = {781072},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ad hoc network, energy efficiency, movement prediction, wireless communication},
} 

@inproceedings{Chakraborty:2003:EMP:781027.781072,
 author = {Chakraborty, Srijan and Yau, David K. Y. and Lui, John C. S.},
 title = {On the effectiveness of movement prediction to reduce energy consumption in wireless communication},
 abstract = {Node movement can be exploited to reduce the energy consumption of wireless network communication. The strategy consists in delaying communication until a mobile node moves close to its target</i> peer node, within an application-imposed deadline. We evaluate the performance of various heuristics that, based on the movement history of the mobile node, estimate an optimal time (in the sense of least energy use) of communication subject to the delay constraint. We evaluate the impact of node movement model, length of movement history maintained, allowable delay, single hop versus multiple hop communication, and size of data transfer on the energy consumption. We also present measurement results on an iPAQ pocket PC that quantify energy consumption in executing the prediction algorithms. Our results show that, with relatively simple and hence efficient prediction heuristics, energy savings in communication can significantly outweigh the energy expenses in executing the prediction algorithms.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {310--311},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781072},
 doi = {http://doi.acm.org/10.1145/781027.781072},
 acmid = {781072},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ad hoc network, energy efficiency, movement prediction, wireless communication},
} 

@inproceedings{Gorinsky:2003:RMC:781027.781073,
 author = {Gorinsky, Sergey and Jain, Sugat and Vin, Harrick and Zhang, Yongguang},
 title = {Robustness of multicast congestion control to inflated subscription},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {312--313},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781073},
 doi = {http://doi.acm.org/10.1145/781027.781073},
 acmid = {781073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion control, fair bandwidth allocation, misbehaving receivers, multicast, robustness},
} 

@article{Gorinsky:2003:RMC:885651.781073,
 author = {Gorinsky, Sergey and Jain, Sugat and Vin, Harrick and Zhang, Yongguang},
 title = {Robustness of multicast congestion control to inflated subscription},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {312--313},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781073},
 doi = {http://doi.acm.org/10.1145/885651.781073},
 acmid = {781073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {congestion control, fair bandwidth allocation, misbehaving receivers, multicast, robustness},
} 

@article{Konrad:2003:CAN:885651.781074,
 author = {Konrad, Almudena and Joseph, Anthony D.},
 title = {Choosing an accurate network path model},
 abstract = {We present a novel domain analysis methodology that enables network researchers to quickly select the most accurate modeling and analysis method or methods for a given wired or wireless network path and network characteristic of interest e.g.</i>, delay, loss, or error process). Our approach includes two classical models, and two data preconditioning models developed in the Tapas</i> project.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {314--315},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781074},
 doi = {http://doi.acm.org/10.1145/885651.781074},
 acmid = {781074},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov, modeling, network characteristics, traces, wireless},
} 

@inproceedings{Konrad:2003:CAN:781027.781074,
 author = {Konrad, Almudena and Joseph, Anthony D.},
 title = {Choosing an accurate network path model},
 abstract = {We present a novel domain analysis methodology that enables network researchers to quickly select the most accurate modeling and analysis method or methods for a given wired or wireless network path and network characteristic of interest e.g.</i>, delay, loss, or error process). Our approach includes two classical models, and two data preconditioning models developed in the Tapas</i> project.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {314--315},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781074},
 doi = {http://doi.acm.org/10.1145/781027.781074},
 acmid = {781074},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov, modeling, network characteristics, traces, wireless},
} 

@article{Akella:2003:EEW:885651.781075,
 author = {Akella, Aditya and Seshan, Srinivasan and Shaikh, Anees},
 title = {An empirical evaluation of wide-area internet bottlenecks},
 abstract = {Performance limitations in the current Internet are thought to lie at the edges of the network -- i.e</i> last mile connectivity to users, or access links of stub ASes. As these links are upgraded, however, it is important to consider where new bottlenecks and hot-spots are likely to arise. Through an extensive measurement study, we discover, classify and characterize non-access bottleneck links in terms of their location, latency and available capacity. We find that nearly half of the paths explored have a non-access bottleneck with available capacity less than 50 Mbps. The bottlenecks identified are roughly equally split between intra-ISP links and links between ISPs. These results have implications on issues such as the choice of access providers and route optimization.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {316--317},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781075},
 doi = {http://doi.acm.org/10.1145/885651.781075},
 acmid = {781075},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bottleneck links, network measurement},
} 

@inproceedings{Akella:2003:EEW:781027.781075,
 author = {Akella, Aditya and Seshan, Srinivasan and Shaikh, Anees},
 title = {An empirical evaluation of wide-area internet bottlenecks},
 abstract = {Performance limitations in the current Internet are thought to lie at the edges of the network -- i.e</i> last mile connectivity to users, or access links of stub ASes. As these links are upgraded, however, it is important to consider where new bottlenecks and hot-spots are likely to arise. Through an extensive measurement study, we discover, classify and characterize non-access bottleneck links in terms of their location, latency and available capacity. We find that nearly half of the paths explored have a non-access bottleneck with available capacity less than 50 Mbps. The bottlenecks identified are roughly equally split between intra-ISP links and links between ISPs. These results have implications on issues such as the choice of access providers and route optimization.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {316--317},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781075},
 doi = {http://doi.acm.org/10.1145/781027.781075},
 acmid = {781075},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bottleneck links, network measurement},
} 

@article{Perelman:2003:USA:885651.781076,
 author = {Perelman, Erez and Hamerly, Greg and Van Biesbrouck, Michael and Sherwood, Timothy and Calder, Brad},
 title = {Using SimPoint for accurate and efficient simulation},
 abstract = {Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of a single industry standard benchmark at this level of detail takes on the order of months to complete. This problem is exacerbated by the fact that to properly perform an architectural evaluation requires multiple benchmarks to be evaluated across many separate runs. To address this issue we recently created a tool called SimPoint that automatically finds a small set of Simulation Points to represent the complete execution of a program for efficient and accurate simulation. In this paper we describe how to use the SimPoint tool, and introduce an improved SimPoint algorithm designed to significantly reduce the simulation time required when the simulation environment relies upon fast-forwarding.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {318--319},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781076},
 doi = {http://doi.acm.org/10.1145/885651.781076},
 acmid = {781076},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SimPoint, clustering, fast-forwarding, sampling, simulation},
} 

@inproceedings{Perelman:2003:USA:781027.781076,
 author = {Perelman, Erez and Hamerly, Greg and Van Biesbrouck, Michael and Sherwood, Timothy and Calder, Brad},
 title = {Using SimPoint for accurate and efficient simulation},
 abstract = {Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of a single industry standard benchmark at this level of detail takes on the order of months to complete. This problem is exacerbated by the fact that to properly perform an architectural evaluation requires multiple benchmarks to be evaluated across many separate runs. To address this issue we recently created a tool called SimPoint that automatically finds a small set of Simulation Points to represent the complete execution of a program for efficient and accurate simulation. In this paper we describe how to use the SimPoint tool, and introduce an improved SimPoint algorithm designed to significantly reduce the simulation time required when the simulation environment relies upon fast-forwarding.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {318--319},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781076},
 doi = {http://doi.acm.org/10.1145/781027.781076},
 acmid = {781076},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SimPoint, clustering, fast-forwarding, sampling, simulation},
} 

@article{Broido:2003:SDU:885651.781077,
 author = {Broido, Andre and Nemeth, Evi and Claffy, K. C.},
 title = {Spectroscopy of DNS update traffic},
 abstract = {We study attempts to dynamically update DNS records for private (RFC1918) addresses, by analyzing the frequency spectrum of updates observed at an authoritative nameserver for these addresses. Using a discrete autocorrelation algorithm we found that updates series have periods of 60 or 75 minutes, which we identified as default settings of out-of-the-box Microsoft Windows 2000 and XP DNS software.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {320--321},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781077},
 doi = {http://doi.acm.org/10.1145/885651.781077},
 acmid = {781077},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNS, RFC1918 addresses, root servers, spectroscopy},
} 

@inproceedings{Broido:2003:SDU:781027.781077,
 author = {Broido, Andre and Nemeth, Evi and Claffy, K. C.},
 title = {Spectroscopy of DNS update traffic},
 abstract = {We study attempts to dynamically update DNS records for private (RFC1918) addresses, by analyzing the frequency spectrum of updates observed at an authoritative nameserver for these addresses. Using a discrete autocorrelation algorithm we found that updates series have periods of 60 or 75 minutes, which we identified as default settings of out-of-the-box Microsoft Windows 2000 and XP DNS software.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {320--321},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781077},
 doi = {http://doi.acm.org/10.1145/781027.781077},
 acmid = {781077},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNS, RFC1918 addresses, root servers, spectroscopy},
} 

@inproceedings{Marsan:2003:GAM:781027.781078,
 author = {Marsan, M. Ajmone and Franceschinis, M. and Giaccone, P. and Leonardi, E. and Neri, F. and Tarello, A.},
 title = {Gated asymptotic modEls (GAMEs): a new tool for the stability analysis of queueing systems},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {322--323},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781078},
 doi = {http://doi.acm.org/10.1145/781027.781078},
 acmid = {781078},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fluid models, queueing theory, stability},
} 

@article{Marsan:2003:GAM:885651.781078,
 author = {Marsan, M. Ajmone and Franceschinis, M. and Giaccone, P. and Leonardi, E. and Neri, F. and Tarello, A.},
 title = {Gated asymptotic modEls (GAMEs): a new tool for the stability analysis of queueing systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {322--323},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781078},
 doi = {http://doi.acm.org/10.1145/885651.781078},
 acmid = {781078},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fluid models, queueing theory, stability},
} 

@article{Avrachenkov:2003:PQF:885651.781079,
 author = {Avrachenkov, K. E. and Vilchevsky, N. O. and Shevlyakov, G. L.},
 title = {Priority queueing with finite buffer size and randomized push-out mechanism},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {324--325},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781079},
 doi = {http://doi.acm.org/10.1145/885651.781079},
 acmid = {781079},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DiffServ, finite buffer, packet loss probability, priority queueing, randomized push-out},
} 

@inproceedings{Avrachenkov:2003:PQF:781027.781079,
 author = {Avrachenkov, K. E. and Vilchevsky, N. O. and Shevlyakov, G. L.},
 title = {Priority queueing with finite buffer size and randomized push-out mechanism},
 abstract = {},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {324--325},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781079},
 doi = {http://doi.acm.org/10.1145/781027.781079},
 acmid = {781079},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DiffServ, finite buffer, packet loss probability, priority queueing, randomized push-out},
} 

@article{Roughan:2003:PET:885651.781080,
 author = {Roughan, Matthew and Thorup, Mikkel and Zhang, Yin},
 title = {Performance of estimated traffic matrices in traffic engineering},
 abstract = {We consider the performance of estimated traffic matrices in traffic engineering. More precisely, we first optimize the routing in an IP backbone to minimize congestion with the estimated traffic matrix. We then test the performance of the resulting routing on the real traffic matrix.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {31},
 issue = {1},
 month = {June},
 year = {2003},
 issn = {0163-5999},
 pages = {326--327},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/885651.781080},
 doi = {http://doi.acm.org/10.1145/885651.781080},
 acmid = {781080},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPLS, OSPF, SNMP, traffic engineering, traffic estimation},
} 

@inproceedings{Roughan:2003:PET:781027.781080,
 author = {Roughan, Matthew and Thorup, Mikkel and Zhang, Yin},
 title = {Performance of estimated traffic matrices in traffic engineering},
 abstract = {We consider the performance of estimated traffic matrices in traffic engineering. More precisely, we first optimize the routing in an IP backbone to minimize congestion with the estimated traffic matrix. We then test the performance of the resulting routing on the real traffic matrix.},
 booktitle = {Proceedings of the 2003 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '03},
 year = {2003},
 isbn = {1-58113-664-1},
 location = {San Diego, CA, USA},
 pages = {326--327},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/781027.781080},
 doi = {http://doi.acm.org/10.1145/781027.781080},
 acmid = {781080},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPLS, OSPF, SNMP, traffic engineering, traffic estimation},
} 

@inproceedings{Narlikar:2001:PMF:378420.378423,
 author = {Narlikar, Girija and Zane, Francis},
 title = {Performance modeling for fast IP lookups},
 abstract = {In this paper, we examine algorithms and data structures for the longest prefix match operation required for routing IP packets. Previous work, aimed at hardware implementations, has focused on quantifying worst case lookup time and memory usage. With the advent of fast programmable platforms, whether network processor or PC-based, metrics which look instead at average case behavior and memory cache performance become more important. To address this, we consider a family of data structures capturing the important techniques used in known fast IP lookup schemes. For these data structures, we construct a model which, given an input trace, estimates cache miss rates and predicts average case lookup performance. This model is validated using traces with varying characteristics. Using the model, we then choose the best data structure from this family for particular hardware platforms and input traces; we find that the optimal data structure differs in different settings. The model can also be used to select the appropriate hardware configurations for future lookup engines. The lookup performance of the selected data structures is competitive with the fastest available software implementations.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378423},
 doi = {http://doi.acm.org/10.1145/378420.378423},
 acmid = {378423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Narlikar:2001:PMF:384268.378423,
 author = {Narlikar, Girija and Zane, Francis},
 title = {Performance modeling for fast IP lookups},
 abstract = {In this paper, we examine algorithms and data structures for the longest prefix match operation required for routing IP packets. Previous work, aimed at hardware implementations, has focused on quantifying worst case lookup time and memory usage. With the advent of fast programmable platforms, whether network processor or PC-based, metrics which look instead at average case behavior and memory cache performance become more important. To address this, we consider a family of data structures capturing the important techniques used in known fast IP lookup schemes. For these data structures, we construct a model which, given an input trace, estimates cache miss rates and predicts average case lookup performance. This model is validated using traces with varying characteristics. Using the model, we then choose the best data structure from this family for particular hardware platforms and input traces; we find that the optimal data structure differs in different settings. The model can also be used to select the appropriate hardware configurations for future lookup engines. The lookup performance of the selected data structures is competitive with the fastest available software implementations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378423},
 doi = {http://doi.acm.org/10.1145/384268.378423},
 acmid = {378423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Qie:2001:SCS:384268.378425,
 author = {Qie, Xiaohu and Bavier, Andy and Peterson, Larry and Karlin, Scott},
 title = {Scheduling computations on a software-based router},
 abstract = {Recent efforts to add new services to the Internet have increased the interest in software-based routers that are easy to extend and evolve. This paper describes our experiences implementing a software-based router, with a particular focus on the main difficulty we encountered: how to schedule the router's CPU cycles. The scheduling decision is complicated by the desire to differentiate the level of service for different packet flows, which leads to two fundamental conflicts: (1) assigning processor shares in a way that keeps the processes along the forwarding path in balance while meeting QoS promises, and (2) adjusting the level of batching in a way that minimizes overhead while meeting QoS promises.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378425},
 doi = {http://doi.acm.org/10.1145/384268.378425},
 acmid = {378425},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Qie:2001:SCS:378420.378425,
 author = {Qie, Xiaohu and Bavier, Andy and Peterson, Larry and Karlin, Scott},
 title = {Scheduling computations on a software-based router},
 abstract = {Recent efforts to add new services to the Internet have increased the interest in software-based routers that are easy to extend and evolve. This paper describes our experiences implementing a software-based router, with a particular focus on the main difficulty we encountered: how to schedule the router's CPU cycles. The scheduling decision is complicated by the desire to differentiate the level of service for different packet flows, which leads to two fundamental conflicts: (1) assigning processor shares in a way that keeps the processes along the forwarding path in balance while meeting QoS promises, and (2) adjusting the level of batching in a way that minimizes overhead while meeting QoS promises.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378425},
 doi = {http://doi.acm.org/10.1145/378420.378425},
 acmid = {378425},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Su:2001:DMR:378420.378426,
 author = {Su, Xun and de Veciana, Gustavo},
 title = {Dynamic multi-path routing: asymptotic approximation and simulations},
 abstract = {In this paper we study the dynamic multi-path routing problem. We focus on an operating regime where traffic flows arrive at and depart from the network in a bursty fashion, and where the delays involved in link state advertisement may lead to "synchronization" effects that adversely impact the performance of dynamic single-path routing schemes.We start by analyzing a simple network of parallel links, where the goal is to minimize the average increase in network congestion on the time scale of link state advertisements. We consider an asymptotic regime leading to an optimization problem permitting closed-form analysis of the number of links over which dynamic multi-path routing should be conducted. Based on our analytical result we examine three types of dynamic routing schemes, and identify a robust policy, i.e.,</i> routing the traffic to a set of links with loads within a factor of the least loaded, that exhibits robust performance. We then propose a similar policy for mesh networks and show by simulation some of its desirable properties. The main results suggest that our proposal would provide significant performance improvement for high speed networks carrying bursty traffic flows.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378426},
 doi = {http://doi.acm.org/10.1145/378420.378426},
 acmid = {378426},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Su:2001:DMR:384268.378426,
 author = {Su, Xun and de Veciana, Gustavo},
 title = {Dynamic multi-path routing: asymptotic approximation and simulations},
 abstract = {In this paper we study the dynamic multi-path routing problem. We focus on an operating regime where traffic flows arrive at and depart from the network in a bursty fashion, and where the delays involved in link state advertisement may lead to "synchronization" effects that adversely impact the performance of dynamic single-path routing schemes.We start by analyzing a simple network of parallel links, where the goal is to minimize the average increase in network congestion on the time scale of link state advertisements. We consider an asymptotic regime leading to an optimization problem permitting closed-form analysis of the number of links over which dynamic multi-path routing should be conducted. Based on our analytical result we examine three types of dynamic routing schemes, and identify a robust policy, i.e.,</i> routing the traffic to a set of links with loads within a factor of the least loaded, that exhibits robust performance. We then propose a similar policy for mesh networks and show by simulation some of its desirable properties. The main results suggest that our proposal would provide significant performance improvement for high speed networks carrying bursty traffic flows.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378426},
 doi = {http://doi.acm.org/10.1145/384268.378426},
 acmid = {378426},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jones:2001:PRS:378420.378427,
 author = {Jones, Michael B. and Saroiu, Stefan},
 title = {Predictability requirements of a soft modem},
 abstract = {Soft Modems</i> use the main processor to execute modem functions traditionally performed by hardware on the modem card. To function correctly, soft modems require that ongoing signal processing computations be performed on the host CPU in a timely manner. Thus, signal processing is a commonly occurring background real-time application---one running on systems that were not designed to support predictable real-time execution. This paper presents a detailed study of the performance characteristics and resource requirements of a popular soft modem. Understanding these requirements should inform the efforts of those designing and building operating systems needing to support soft modems. Furthermore, we believe that the conclusions of this study also apply to other existing and upcoming soft devices, such as soft Digital Subscriber Line (DSL) cards. We conclude that (1) signal processing in an interrupt handler is not only unnecessary but also detrimental to the predictability of other computations in the system and (2) a real-time scheduler can provide predictability for the soft modem while minimizing its impact on other computations in the system.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {37--49},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/378420.378427},
 doi = {http://doi.acm.org/10.1145/378420.378427},
 acmid = {378427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU scheduling, Rialto, Rialto/NT, Windows 2000, Windows NT, open real-time system, real-time, signal processing, soft devices, soft modem},
} 

@article{Jones:2001:PRS:384268.378427,
 author = {Jones, Michael B. and Saroiu, Stefan},
 title = {Predictability requirements of a soft modem},
 abstract = {Soft Modems</i> use the main processor to execute modem functions traditionally performed by hardware on the modem card. To function correctly, soft modems require that ongoing signal processing computations be performed on the host CPU in a timely manner. Thus, signal processing is a commonly occurring background real-time application---one running on systems that were not designed to support predictable real-time execution. This paper presents a detailed study of the performance characteristics and resource requirements of a popular soft modem. Understanding these requirements should inform the efforts of those designing and building operating systems needing to support soft modems. Furthermore, we believe that the conclusions of this study also apply to other existing and upcoming soft devices, such as soft Digital Subscriber Line (DSL) cards. We conclude that (1) signal processing in an interrupt handler is not only unnecessary but also detrimental to the predictability of other computations in the system and (2) a real-time scheduler can provide predictability for the soft modem while minimizing its impact on other computations in the system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {37--49},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/384268.378427},
 doi = {http://doi.acm.org/10.1145/384268.378427},
 acmid = {378427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU scheduling, Rialto, Rialto/NT, Windows 2000, Windows NT, open real-time system, real-time, signal processing, soft devices, soft modem},
} 

@article{Lorch:2001:IDV:384268.378429,
 author = {Lorch, Jacob R. and Smith, Alan Jay},
 title = {Improving dynamic voltage scaling algorithms with <italic>PACE</italic>},
 abstract = {This paper addresses algorithms for dynamically varying (scaling) CPU speed and voltage in order to save energy. Such scaling is useful and effective when it is immaterial when a task completes, as long as it meets some deadline. We show how to modify any scaling algorithm to keep performance the same but minimize expected energy consumption. We refer to our approach as PACE (Processor Acceleration to Conserve Energy) since the resulting schedule increases speed as the task progresses. Since PACE depends on the probability distribution of the task's work requirement, we present methods for estimating this distribution and evaluate these methods on a variety of real workloads. We also show how to approximate the optimal schedule with one that changes speed a limited number of times. Using PACE causes very little additional overhead, and yields substantial reductions in CPU energy consumption. Simulations using real workloads show it reduces the CPU energy consumption of previously published algorithms by up to 49.5\%, with an average of 20.6\%, without any effect on performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {50--61},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378429},
 doi = {http://doi.acm.org/10.1145/384268.378429},
 acmid = {378429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lorch:2001:IDV:378420.378429,
 author = {Lorch, Jacob R. and Smith, Alan Jay},
 title = {Improving dynamic voltage scaling algorithms with <italic>PACE</italic>},
 abstract = {This paper addresses algorithms for dynamically varying (scaling) CPU speed and voltage in order to save energy. Such scaling is useful and effective when it is immaterial when a task completes, as long as it meets some deadline. We show how to modify any scaling algorithm to keep performance the same but minimize expected energy consumption. We refer to our approach as PACE (Processor Acceleration to Conserve Energy) since the resulting schedule increases speed as the task progresses. Since PACE depends on the probability distribution of the task's work requirement, we present methods for estimating this distribution and evaluate these methods on a variety of real workloads. We also show how to approximate the optimal schedule with one that changes speed a limited number of times. Using PACE causes very little additional overhead, and yields substantial reductions in CPU energy consumption. Simulations using real workloads show it reduces the CPU energy consumption of previously published algorithms by up to 49.5\%, with an average of 20.6\%, without any effect on performance.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {50--61},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378429},
 doi = {http://doi.acm.org/10.1145/378420.378429},
 acmid = {378429},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vaidyanathan:2001:AIS:384268.378434,
 author = {Vaidyanathan, Kalyanaraman and Harper, Richard E. and Hunter, Steven W. and Trivedi, Kishor S.},
 title = {Analysis and implementation of software rejuvenation in cluster systems},
 abstract = {Several recent studies have reported the phenomenon of "software aging", one in which the state of a software system degrades with time. This may eventually lead to performance degradation of the software or crash/hang failure or both. "Software rejuvenation" is a pro-active technique aimed to prevent unexpected or unplanned outages due to aging. The basic idea is to stop the running software, clean its internal state and restart it. In this paper, we discuss software rejuvenation as applied to cluster systems. This is both an innovative and an efficient way to improve cluster system availability and productivity. Using Stochastic Reward Nets (SRNs), we model and analyze cluster systems which employ software rejuvenation. For our proposed time-based rejuvenation policy, we determine the optimal rejuvenation interval based on system availability and cost. We also introduce a new rejuvenation policy based on prediction and show that it can dramatically increase system availability and reduce downtime cost. These models are very general and can capture a multitude of cluster system characteristics, failure behavior and performability measures, which we are just beginning to explore. We then briefly describe an implementation of a software rejuvenation system that performs periodic and predictive rejuvenation, and show some empirical data from systems that exhibit aging},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {62--71},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378434},
 doi = {http://doi.acm.org/10.1145/384268.378434},
 acmid = {378434},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vaidyanathan:2001:AIS:378420.378434,
 author = {Vaidyanathan, Kalyanaraman and Harper, Richard E. and Hunter, Steven W. and Trivedi, Kishor S.},
 title = {Analysis and implementation of software rejuvenation in cluster systems},
 abstract = {Several recent studies have reported the phenomenon of "software aging", one in which the state of a software system degrades with time. This may eventually lead to performance degradation of the software or crash/hang failure or both. "Software rejuvenation" is a pro-active technique aimed to prevent unexpected or unplanned outages due to aging. The basic idea is to stop the running software, clean its internal state and restart it. In this paper, we discuss software rejuvenation as applied to cluster systems. This is both an innovative and an efficient way to improve cluster system availability and productivity. Using Stochastic Reward Nets (SRNs), we model and analyze cluster systems which employ software rejuvenation. For our proposed time-based rejuvenation policy, we determine the optimal rejuvenation interval based on system availability and cost. We also introduce a new rejuvenation policy based on prediction and show that it can dramatically increase system availability and reduce downtime cost. These models are very general and can capture a multitude of cluster system characteristics, failure behavior and performability measures, which we are just beginning to explore. We then briefly describe an implementation of a software rejuvenation system that performs periodic and predictive rejuvenation, and show some empirical data from systems that exhibit aging},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {62--71},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378434},
 doi = {http://doi.acm.org/10.1145/378420.378434},
 acmid = {378434},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Loh:2001:TAE:378420.378437,
 author = {Loh, Gabriel},
 title = {A time-stamping algorithm for efficient performance estimation of superscalar processors},
 abstract = {The increasing complexity of modern superscalar microprocessors makes the evaluation of new designs and techniques much more difficult. Fast and accurate methods for simulating program execution on realistic and hypothetical processor models are of great interest to many computer architects and compiler writers. There are many existing techniques, from profile based runtime estimation to complete cycle-level simulations. Many researchers choose to sacrifice the speed of profiling for the accuracy obtainable by cycle-level simulators. This paper presents a technique that provides accurate performance predictions, while avoiding the complexity associated with a complete processor emulator. The approach augments a fast in-order simulator with a time-stamping algorithm that provides a very good estimate of program running time. This algorithm achieves an average accuracy that is within 7.5\% of a cycle-level out-of-order simulator in approximately 41\% of the running time on the eight SPECInt95 integer benchmarks.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {72--81},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378437},
 doi = {http://doi.acm.org/10.1145/378420.378437},
 acmid = {378437},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Loh:2001:TAE:384268.378437,
 author = {Loh, Gabriel},
 title = {A time-stamping algorithm for efficient performance estimation of superscalar processors},
 abstract = {The increasing complexity of modern superscalar microprocessors makes the evaluation of new designs and techniques much more difficult. Fast and accurate methods for simulating program execution on realistic and hypothetical processor models are of great interest to many computer architects and compiler writers. There are many existing techniques, from profile based runtime estimation to complete cycle-level simulations. Many researchers choose to sacrifice the speed of profiling for the accuracy obtainable by cycle-level simulators. This paper presents a technique that provides accurate performance predictions, while avoiding the complexity associated with a complete processor emulator. The approach augments a fast in-order simulator with a time-stamping algorithm that provides a very good estimate of program running time. This algorithm achieves an average accuracy that is within 7.5\% of a cycle-level out-of-order simulator in approximately 41\% of the running time on the eight SPECInt95 integer benchmarks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {72--81},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378437},
 doi = {http://doi.acm.org/10.1145/384268.378437},
 acmid = {378437},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bonald:2001:IFI:378420.378438,
 author = {Bonald, Thomas and Massouli\'{e}, Laurent},
 title = {Impact of fairness on Internet performance},
 abstract = {We discuss the relevance of fairness as a design objective for congestion control mechanisms in the Internet. Specifically, we consider a backbone network shared by a dynamic number of short-lived flows, and study the impact of bandwidth sharing on network performance. In particular, we prove that for a broad class of fair bandwidth allocations, the total number of flows in progress remains finite if the load of every link is less than one. We also show that provided the bandwidth allocation is "sufficiently" fair, performance is optimal in the sense that the throughput of the flows is mainly determined by their access rate. Neither property is guaranteed with unfair bandwidth allocations, when priority is given to one class of flow with respect to another. This suggests current proposals for a differentiated services Internet may lead to suboptimal utilization of network resources.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {82--91},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378438},
 doi = {http://doi.acm.org/10.1145/378420.378438},
 acmid = {378438},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bonald:2001:IFI:384268.378438,
 author = {Bonald, Thomas and Massouli\'{e}, Laurent},
 title = {Impact of fairness on Internet performance},
 abstract = {We discuss the relevance of fairness as a design objective for congestion control mechanisms in the Internet. Specifically, we consider a backbone network shared by a dynamic number of short-lived flows, and study the impact of bandwidth sharing on network performance. In particular, we prove that for a broad class of fair bandwidth allocations, the total number of flows in progress remains finite if the load of every link is less than one. We also show that provided the bandwidth allocation is "sufficiently" fair, performance is optimal in the sense that the throughput of the flows is mainly determined by their access rate. Neither property is guaranteed with unfair bandwidth allocations, when priority is given to one class of flow with respect to another. This suggests current proposals for a differentiated services Internet may lead to suboptimal utilization of network resources.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {82--91},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378438},
 doi = {http://doi.acm.org/10.1145/384268.378438},
 acmid = {378438},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Salamatian:2001:HMM:384268.378439,
 author = {Salamatian, Kav\'{e} and Vaton, Sandrine},
 title = {Hidden Markov modeling for network communication channels},
 abstract = {In this paper we perform the statistical analysis of an Internet communication channel. Our study is based on a Hidden Markov Model (HMM). The channel switches between different states; to each state corresponds the probability that a packet sent by the transmitter will be lost. The transition between the different states of the channel is governed by a Markov chain; this Markov chain is not observed directly, but the received packet flow provides some probabilistic information about the current state of the channel, as well as some information about the parameters of the model. In this paper we detail some useful algorithms for the estimation of the channel parameters, and for making inference about the state of the channel. We discuss the relevance of the Markov model of the channel; we also discuss how many states are required to pertinently model a real communication channel.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {92--101},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378439},
 doi = {http://doi.acm.org/10.1145/384268.378439},
 acmid = {378439},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Internet modelling, active measurement expectation-maximization, hidden Markov model, network state estimation},
} 

@inproceedings{Salamatian:2001:HMM:378420.378439,
 author = {Salamatian, Kav\'{e} and Vaton, Sandrine},
 title = {Hidden Markov modeling for network communication channels},
 abstract = {In this paper we perform the statistical analysis of an Internet communication channel. Our study is based on a Hidden Markov Model (HMM). The channel switches between different states; to each state corresponds the probability that a packet sent by the transmitter will be lost. The transition between the different states of the channel is governed by a Markov chain; this Markov chain is not observed directly, but the received packet flow provides some probabilistic information about the current state of the channel, as well as some information about the parameters of the model. In this paper we detail some useful algorithms for the estimation of the channel parameters, and for making inference about the state of the channel. We discuss the relevance of the Markov model of the channel; we also discuss how many states are required to pertinently model a real communication channel.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {92--101},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378439},
 doi = {http://doi.acm.org/10.1145/378420.378439},
 acmid = {378439},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Internet modelling, active measurement expectation-maximization, hidden Markov model, network state estimation},
} 

@article{Cao:2001:NIT:384268.378440,
 author = {Cao, Jin and Cleveland, William S. and Lin, Dong and Sun, Don X.},
 title = {On the nonstationarity of Internet traffic},
 abstract = {Traffic variables on an uncongested Internet wire exhibit a pervasive nonstationarity. As the rate of new TCP connections increases, arrival processes (packet and connection) tend locally toward Poisson, and time series variables (packet sizes, transferred file sizes, and connection round-trip times) tend locally toward independent. The cause of the nonstationarity is superposition: the intermingling of sequences of connections between different source-destination pairs, and the intermingling of sequences of packets from different connections. We show this empirically by extensive study of packet traces for nine links coming from four packet header databases. We show it theoretically by invoking the mathematical theory of point processes and time series. If the connection rate on a link gets sufficiently high, the variables can be quite close to Poisson and independent; if major congestion occurs on the wire before the rate gets sufficiently high, then the progression toward Poisson and independent can be arrested for some variables.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384268.378440},
 doi = {http://doi.acm.org/10.1145/384268.378440},
 acmid = {378440},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cao:2001:NIT:378420.378440,
 author = {Cao, Jin and Cleveland, William S. and Lin, Dong and Sun, Don X.},
 title = {On the nonstationarity of Internet traffic},
 abstract = {Traffic variables on an uncongested Internet wire exhibit a pervasive nonstationarity. As the rate of new TCP connections increases, arrival processes (packet and connection) tend locally toward Poisson, and time series variables (packet sizes, transferred file sizes, and connection round-trip times) tend locally toward independent. The cause of the nonstationarity is superposition: the intermingling of sequences of connections between different source-destination pairs, and the intermingling of sequences of packets from different connections. We show this empirically by extensive study of packet traces for nine links coming from four packet header databases. We show it theoretically by invoking the mathematical theory of point processes and time series. If the connection rate on a link gets sufficiently high, the variables can be quite close to Poisson and independent; if major congestion occurs on the wire before the rate gets sufficiently high, then the progression toward Poisson and independent can be arrested for some variables.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378420.378440},
 doi = {http://doi.acm.org/10.1145/378420.378440},
 acmid = {378440},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hsieh:2001:PCC:384268.378441,
 author = {Hsieh, Hung-Yun and Sivakumar, Raghupathy},
 title = {Performance comparison of cellular and multi-hop wireless networks: a quantitative study},
 abstract = {In this paper we study the performance trade-offs between conventional cellular and multi-hop ad-hoc wireless networks. We compare through simulations the performance of the two network models in terms of raw network capacity, end-to-end throughput, end-to-end delay, power consumption, per-node fairness (for throughput, delay, and power), and impact of mobility on the network performance. The simulation results show that while adhoc networks perform better in terms of throughput, delay, and power, they suffer from unfairness and poor network performance in the event of mobility.We discuss the trade-offs involved in the performance of the two network models, identify the specific reasons behind them, and argue that the trade-offs preclude the adoption of either network model as a clear solution for future wireless communication systems. Finally, we present a simple hybrid wireless network model that has the combined advantages of cellular and ad-hoc wireless networks but does not suffer from the disadvantages of either.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {113--122},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378441},
 doi = {http://doi.acm.org/10.1145/384268.378441},
 acmid = {378441},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hsieh:2001:PCC:378420.378441,
 author = {Hsieh, Hung-Yun and Sivakumar, Raghupathy},
 title = {Performance comparison of cellular and multi-hop wireless networks: a quantitative study},
 abstract = {In this paper we study the performance trade-offs between conventional cellular and multi-hop ad-hoc wireless networks. We compare through simulations the performance of the two network models in terms of raw network capacity, end-to-end throughput, end-to-end delay, power consumption, per-node fairness (for throughput, delay, and power), and impact of mobility on the network performance. The simulation results show that while adhoc networks perform better in terms of throughput, delay, and power, they suffer from unfairness and poor network performance in the event of mobility.We discuss the trade-offs involved in the performance of the two network models, identify the specific reasons behind them, and argue that the trade-offs preclude the adoption of either network model as a clear solution for future wireless communication systems. Finally, we present a simple hybrid wireless network model that has the combined advantages of cellular and ad-hoc wireless networks but does not suffer from the disadvantages of either.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {113--122},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378441},
 doi = {http://doi.acm.org/10.1145/378420.378441},
 acmid = {378441},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hegde:2001:BLM:384268.378442,
 author = {Hegde, Nidhi and Sohraby, Khosrow},
 title = {Blocking in large mobile cellular networks with bursty traffic},
 abstract = {We consider large cellular networks. The traffic entering the network is assumed to be correlated in both space</i> and time.</i> The space dependency captures the possible correlation between the arrivals to different nodes in the network, while the time dependency captures the time correlation between arrivals to each node. We model such traffic with a Markov-Modulated Poisson Process(MMPP).It is shown that even in the single node environment, the problem is not mathematically tractable. A model with an infinite number of circuits is used to approximate the finite model. A novel recursive methodology is introduced in finding the joint moments of the number of busy circuits in different cells in the network leading to accurate determination of blocking probability. A simple mixed-Poisson distribution is introduced as an accurate approximation of the distribution of the number ofbusy circuits.We show that for certain cases, in the system with an infinite number of circuits in each cell, there is no effect of mobility on the performance of the system. Our numerical results indicate that the traffic burstiness has a major impact on the system performance. The mixed-Poisson approximation is found to be a very good fit to the exact finite model. The performance of this approximation using few moments is affected by traffic burstiness and average load. We find that in a reasonable range of traffic burstiness, the mixed-Poisson distribution provides a close approximation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378442},
 doi = {http://doi.acm.org/10.1145/384268.378442},
 acmid = {378442},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hegde:2001:BLM:378420.378442,
 author = {Hegde, Nidhi and Sohraby, Khosrow},
 title = {Blocking in large mobile cellular networks with bursty traffic},
 abstract = {We consider large cellular networks. The traffic entering the network is assumed to be correlated in both space</i> and time.</i> The space dependency captures the possible correlation between the arrivals to different nodes in the network, while the time dependency captures the time correlation between arrivals to each node. We model such traffic with a Markov-Modulated Poisson Process(MMPP).It is shown that even in the single node environment, the problem is not mathematically tractable. A model with an infinite number of circuits is used to approximate the finite model. A novel recursive methodology is introduced in finding the joint moments of the number of busy circuits in different cells in the network leading to accurate determination of blocking probability. A simple mixed-Poisson distribution is introduced as an accurate approximation of the distribution of the number ofbusy circuits.We show that for certain cases, in the system with an infinite number of circuits in each cell, there is no effect of mobility on the performance of the system. Our numerical results indicate that the traffic burstiness has a major impact on the system performance. The mixed-Poisson approximation is found to be a very good fit to the exact finite model. The performance of this approximation using few moments is affected by traffic burstiness and average load. We find that in a reasonable range of traffic burstiness, the mixed-Poisson distribution provides a close approximation.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {123--132},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378442},
 doi = {http://doi.acm.org/10.1145/378420.378442},
 acmid = {378442},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kumar:2001:CEF:378420.378443,
 author = {Kumar, Apurva and Gupta, Rajeev},
 title = {Capacity evaluation of frequency hopping based ad-hoc systems},
 abstract = {The IEEE 802.15 Wireless Personal Area Networks (WPAN) study group has been working on evolving a standard for short-range wireless connectivity between low complexity and low power devices operating within the personal operating space (POS). The scenarios envisioned for WPANs are likely to involve a large number of POSs operating in an indoor environment. Among short-range wireless technologies, Bluetooth<sup>TM 1</sup> based ad-hoc connectivity comes closest to satisfying the WPAN requirements. Bluetooth provides a gross rate of 1 Mbps per network and allows several such networks to overlap using frequency hopping. The 'aggregate throughput' thus achieved is much higher than 1 Mbps. In the absence of external interfering sources, aggregate throughput is limited by self interference which depends upon, (i) physical layer parameters like hopping rate, hopping sequences, transmitted power, receiver sensitivity, modulation, forward error correction (ii) channel characteristics like coherence bandwidth and coherence time (iii) spatial characteristics. In this work we consider the problem of finding the capacity of Bluetooth based ad-hoc systems by accurately modeling the Bluetooth physical layer and the indoor wireless channel. We predict the throughput in Bluetooth based ad-hoc systems as a function of a generalized set of parameters using realistic scenarios and assumptions.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {133--142},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378443},
 doi = {http://doi.acm.org/10.1145/378420.378443},
 acmid = {378443},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GFSK, ad-hoc networks, bit error rate, bluetooth technology, capacity, forward error correction, frequency hopping, throughput},
} 

@article{Kumar:2001:CEF:384268.378443,
 author = {Kumar, Apurva and Gupta, Rajeev},
 title = {Capacity evaluation of frequency hopping based ad-hoc systems},
 abstract = {The IEEE 802.15 Wireless Personal Area Networks (WPAN) study group has been working on evolving a standard for short-range wireless connectivity between low complexity and low power devices operating within the personal operating space (POS). The scenarios envisioned for WPANs are likely to involve a large number of POSs operating in an indoor environment. Among short-range wireless technologies, Bluetooth<sup>TM 1</sup> based ad-hoc connectivity comes closest to satisfying the WPAN requirements. Bluetooth provides a gross rate of 1 Mbps per network and allows several such networks to overlap using frequency hopping. The 'aggregate throughput' thus achieved is much higher than 1 Mbps. In the absence of external interfering sources, aggregate throughput is limited by self interference which depends upon, (i) physical layer parameters like hopping rate, hopping sequences, transmitted power, receiver sensitivity, modulation, forward error correction (ii) channel characteristics like coherence bandwidth and coherence time (iii) spatial characteristics. In this work we consider the problem of finding the capacity of Bluetooth based ad-hoc systems by accurately modeling the Bluetooth physical layer and the indoor wireless channel. We predict the throughput in Bluetooth based ad-hoc systems as a function of a generalized set of parameters using realistic scenarios and assumptions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {133--142},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378443},
 doi = {http://doi.acm.org/10.1145/384268.378443},
 acmid = {378443},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GFSK, ad-hoc networks, bit error rate, bluetooth technology, capacity, forward error correction, frequency hopping, throughput},
} 

@inproceedings{Qiu:2001:NPF:378420.378777,
 author = {Qiu, Dongyu and Shroff, Ness B.},
 title = {A new predictive flow control scheme for efficient network utilization and QoS},
 abstract = {In this paper we develop a new predictive flow control scheme and analyze its performance. This scheme controls the non-real-time traffic based on predicting the real-time traffic. The goal of the work is to operate the network in a low congestion, high throughput regime. We provide a rigorous analysis of the performance of our flow control method and show that the algorithm has attractive and useful properties. From our analysis we obtain an explicit condition that gives us design guidelines on how to choose a predictor. We learn that it is especially important to take the queueing effect into account in developing the predictor. We also provide numerical results comparing different predictors that use varying degrees of information from the network.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {143--153},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378420.378777},
 doi = {http://doi.acm.org/10.1145/378420.378777},
 acmid = {378777},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Qiu:2001:NPF:384268.378777,
 author = {Qiu, Dongyu and Shroff, Ness B.},
 title = {A new predictive flow control scheme for efficient network utilization and QoS},
 abstract = {In this paper we develop a new predictive flow control scheme and analyze its performance. This scheme controls the non-real-time traffic based on predicting the real-time traffic. The goal of the work is to operate the network in a low congestion, high throughput regime. We provide a rigorous analysis of the performance of our flow control method and show that the algorithm has attractive and useful properties. From our analysis we obtain an explicit condition that gives us design guidelines on how to choose a predictor. We learn that it is especially important to take the queueing effect into account in developing the predictor. We also provide numerical results comparing different predictors that use varying degrees of information from the network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {143--153},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384268.378777},
 doi = {http://doi.acm.org/10.1145/384268.378777},
 acmid = {378777},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Paschalidis:2001:MEB:378420.378778,
 author = {Paschalidis, Ioannis Ch. and Vassilaras, Spyridon},
 title = {Model-based estimation of buffer overflow probabilities from measurements},
 abstract = {We consider the problem of estimating buffer overflow probabilities when the statistics of the input traffic are not known and have to be estimated from measurements. We start by investigating the use of Markov-modulated processes in modeling the input traffic and propose a method for selecting an optimal model based on Akaike's Information Criterion. We then consider a queue fed by such a Markov-modulated input process and use large deviations asymptotics to obtain the buffer overflow probability. The expression for this probability is affected by estimation errors in the parameters of the input model. We analyze the effect of these errors and propose a new, more robust, estimator which is less likely to underestimate the overflow probability than the estimator obtained by certainty equivalence. As such, it is appropriate in situations where the overflow probability is associated with Quality of Service (QoS)</i> and we need to provide firm QoS guarantees. Nevertheless, as the number of observations increases, the proposed estimator converges with probability 1 to the appropriate target, and thus, does not lead to resource underutilization in this limit.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378778},
 doi = {http://doi.acm.org/10.1145/378420.378778},
 acmid = {378778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Akaike's information criterion, Markov-modulated processes, effective bandwidth, estimation, large deviations},
} 

@article{Paschalidis:2001:MEB:384268.378778,
 author = {Paschalidis, Ioannis Ch. and Vassilaras, Spyridon},
 title = {Model-based estimation of buffer overflow probabilities from measurements},
 abstract = {We consider the problem of estimating buffer overflow probabilities when the statistics of the input traffic are not known and have to be estimated from measurements. We start by investigating the use of Markov-modulated processes in modeling the input traffic and propose a method for selecting an optimal model based on Akaike's Information Criterion. We then consider a queue fed by such a Markov-modulated input process and use large deviations asymptotics to obtain the buffer overflow probability. The expression for this probability is affected by estimation errors in the parameters of the input model. We analyze the effect of these errors and propose a new, more robust, estimator which is less likely to underestimate the overflow probability than the estimator obtained by certainty equivalence. As such, it is appropriate in situations where the overflow probability is associated with Quality of Service (QoS)</i> and we need to provide firm QoS guarantees. Nevertheless, as the number of observations increases, the proposed estimator converges with probability 1 to the appropriate target, and thus, does not lead to resource underutilization in this limit.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378778},
 doi = {http://doi.acm.org/10.1145/384268.378778},
 acmid = {378778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Akaike's information criterion, Markov-modulated processes, effective bandwidth, estimation, large deviations},
} 

@article{Dutta:2001:OTG:384268.378779,
 author = {Dutta, Rudra and Rouskas, George N.},
 title = {On optimal traffic grooming in WDM rings},
 abstract = {We consider the problem of designing a virtual topology to minimize electronic routing, that is, grooming traffic, in wavelength routed optical rings. We present a new framework consisting of a sequence of bounds, both upper and lower, in which each successive bound is at least as strong as the previous one. The successive bounds take larger amounts of computation to evaluate, and the number of bounds to be evaluated for a given problem instance is only limited by the computational power available. The bounds are based on decomposing the ring into sets of nodes arranged in a path, and adopting the locally optimal topology within each set. Our approach can be applied to many virtual topology problems on rings. The upper bounds we obtain also provide a useful series of heuristic solutions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384268.378779},
 doi = {http://doi.acm.org/10.1145/384268.378779},
 acmid = {378779},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dutta:2001:OTG:378420.378779,
 author = {Dutta, Rudra and Rouskas, George N.},
 title = {On optimal traffic grooming in WDM rings},
 abstract = {We consider the problem of designing a virtual topology to minimize electronic routing, that is, grooming traffic, in wavelength routed optical rings. We present a new framework consisting of a sequence of bounds, both upper and lower, in which each successive bound is at least as strong as the previous one. The successive bounds take larger amounts of computation to evaluate, and the number of bounds to be evaluated for a given problem instance is only limited by the computational power available. The bounds are based on decomposing the ring into sets of nodes arranged in a path, and adopting the locally optimal topology within each set. Our approach can be applied to many virtual topology problems on rings. The upper bounds we obtain also provide a useful series of heuristic solutions.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378420.378779},
 doi = {http://doi.acm.org/10.1145/378420.378779},
 acmid = {378779},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Le Boudec:2001:PVL:378420.378780,
 author = {Le Boudec, Jean-Yves},
 title = {Some properties of variable length packet shapers},
 abstract = {The min-plus theory of greedy shapers has been developed after Cruz's results on the calculus of network delays. An example of greedy shaper is the buffered leaky bucket controller. The theory of greedy shapers establishes a number of properties; for example, re-shaping keeps original arrival constraints. The existing theory applies in all rigor either to fluid systems, or to packets of constant size such as ATM. For variable length packets, the distortion introduced by packetization affects the theory, which is no longer valid. Chang has introduced the concept of packetizer, which models the effect of variable length packets, and has also developed a max-plus theory of shapers. In this paper, we start with the min-plus theory, and obtain results on greedy shapers for variable length packets which are not readily explained with the max-plus theory of Chang. We show a fundamental result, namely, the min-plus representation of a packetized greedy shaper. This allows us to prove that, under some assumptions, re-shaping a flow of variable length packets does keep original arrival constraints. However, we show on some examples that if the assumptions are not satisfied, then the property may not hold any more. We also demonstrate the equivalence of implementing a buffered leaky bucket controller based on either virtual finish times or on bucket replenishment.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {175--183},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/378420.378780},
 doi = {http://doi.acm.org/10.1145/378420.378780},
 acmid = {378780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {leaky bucket, min-plus algebra, network calculus, shaper},
} 

@article{Le Boudec:2001:PVL:384268.378780,
 author = {Le Boudec, Jean-Yves},
 title = {Some properties of variable length packet shapers},
 abstract = {The min-plus theory of greedy shapers has been developed after Cruz's results on the calculus of network delays. An example of greedy shaper is the buffered leaky bucket controller. The theory of greedy shapers establishes a number of properties; for example, re-shaping keeps original arrival constraints. The existing theory applies in all rigor either to fluid systems, or to packets of constant size such as ATM. For variable length packets, the distortion introduced by packetization affects the theory, which is no longer valid. Chang has introduced the concept of packetizer, which models the effect of variable length packets, and has also developed a max-plus theory of shapers. In this paper, we start with the min-plus theory, and obtain results on greedy shapers for variable length packets which are not readily explained with the max-plus theory of Chang. We show a fundamental result, namely, the min-plus representation of a packetized greedy shaper. This allows us to prove that, under some assumptions, re-shaping a flow of variable length packets does keep original arrival constraints. However, we show on some examples that if the assumptions are not satisfied, then the property may not hold any more. We also demonstrate the equivalence of implementing a buffered leaky bucket controller based on either virtual finish times or on bucket replenishment.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {175--183},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/384268.378780},
 doi = {http://doi.acm.org/10.1145/384268.378780},
 acmid = {378780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {leaky bucket, min-plus algebra, network calculus, shaper},
} 

@article{Chang:2001:PMI:384268.378782,
 author = {Chang, Cheng-Shang and Chiu, Yuh-ming and Song, Wheyming Tina},
 title = {On the performance of multiplexing independent regulated inputs},
 abstract = {In this paper, we consider the performance analysis problem for a work conserving link with a large number of independent regulated inputs. For such a problem, we derive simple stochastic bounds under a general traffic constraint for the inputs. The bound for queue length is shown to be a stochastic extension of the deterministic worst case bound and it is asymptotically tighter than the bound in Kesidis and Konstantopoulos [23]. We also test the bound by considering periodic inputs with independent starting phases. Based on Sanov's theorem and importance sampling, we propose a fast simulation algorithm that achieves significant variance reduction. The simulations results are compared with our stochastic bound and the bound in [23].},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {184--193},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378782},
 doi = {http://doi.acm.org/10.1145/384268.378782},
 acmid = {378782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fast simulation, multiplexing, performance bounds},
} 

@inproceedings{Chang:2001:PMI:378420.378782,
 author = {Chang, Cheng-Shang and Chiu, Yuh-ming and Song, Wheyming Tina},
 title = {On the performance of multiplexing independent regulated inputs},
 abstract = {In this paper, we consider the performance analysis problem for a work conserving link with a large number of independent regulated inputs. For such a problem, we derive simple stochastic bounds under a general traffic constraint for the inputs. The bound for queue length is shown to be a stochastic extension of the deterministic worst case bound and it is asymptotically tighter than the bound in Kesidis and Konstantopoulos [23]. We also test the bound by considering periodic inputs with independent starting phases. Based on Sanov's theorem and importance sampling, we propose a fast simulation algorithm that achieves significant variance reduction. The simulations results are compared with our stochastic bound and the bound in [23].},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {184--193},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378782},
 doi = {http://doi.acm.org/10.1145/378420.378782},
 acmid = {378782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fast simulation, multiplexing, performance bounds},
} 

@inproceedings{Shuf:2001:CMB:378420.378783,
 author = {Shuf, Yefim and Serrano, Mauricio J. and Gupta, Manish and Singh, Jaswinder Pal},
 title = {Characterizing the memory behavior of Java workloads: a structured view and opportunities for optimizations},
 abstract = {This paper studies the memory behavior of important Java workloads used in benchmarking Java Virtual Machines (JVMs), based on instrumentation of both application and library code in a state-of-the-art JVM, and provides structured information about these workloads to help guide systems' design. We begin by characterizing the inherent memory behavior of the benchmarks, such as information on the breakup of heap accesses among different categories and on the hotness of references to fields and methods. We then provide detailed information about misses in the data TLB and caches, including the distribution of misses over different kinds of accesses and over different methods. In the process, we make interesting discoveries about TLB behavior and limitations of data prefetching schemes discussed in the literature in dealing with pointer-intensive Java codes. Throughout this paper, we develop a set of recommendations to computer architects and compiler writers on how to optimize computer systems and system software to run Java programs more efficiently. This paper also makes the first attempt to compare the characteristics of SPECjvm98 to those of a server-oriented benchmark, pBOB, and explain why the current set of SPECjvm98 benchmarks may not be adequate for a comprehensive and objective evaluation of JVMs and just-in-time (JIT) compilers.We discover that the fraction of accesses to array elements is quite significant, demonstrate that the number of "hot spots" in the benchmarks is small, and show that field reordering cannot yield significant performance gains. We also show that even a fairly large L2 data cache is not effective for many Java benchmarks. We observe that instructions used to prefetch data into the L2 data cache are often squashed because of high TLB miss rates and because the TLB does not usually have the translation information needed to prefetch the data into the L2 data cache. We also find that co-allocation of frequently used method tables can reduce the number of TLB misses and lower the cost of accessing type information block entries in virtual method calls and runtime type checking.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {194--205},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378783},
 doi = {http://doi.acm.org/10.1145/378420.378783},
 acmid = {378783},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shuf:2001:CMB:384268.378783,
 author = {Shuf, Yefim and Serrano, Mauricio J. and Gupta, Manish and Singh, Jaswinder Pal},
 title = {Characterizing the memory behavior of Java workloads: a structured view and opportunities for optimizations},
 abstract = {This paper studies the memory behavior of important Java workloads used in benchmarking Java Virtual Machines (JVMs), based on instrumentation of both application and library code in a state-of-the-art JVM, and provides structured information about these workloads to help guide systems' design. We begin by characterizing the inherent memory behavior of the benchmarks, such as information on the breakup of heap accesses among different categories and on the hotness of references to fields and methods. We then provide detailed information about misses in the data TLB and caches, including the distribution of misses over different kinds of accesses and over different methods. In the process, we make interesting discoveries about TLB behavior and limitations of data prefetching schemes discussed in the literature in dealing with pointer-intensive Java codes. Throughout this paper, we develop a set of recommendations to computer architects and compiler writers on how to optimize computer systems and system software to run Java programs more efficiently. This paper also makes the first attempt to compare the characteristics of SPECjvm98 to those of a server-oriented benchmark, pBOB, and explain why the current set of SPECjvm98 benchmarks may not be adequate for a comprehensive and objective evaluation of JVMs and just-in-time (JIT) compilers.We discover that the fraction of accesses to array elements is quite significant, demonstrate that the number of "hot spots" in the benchmarks is small, and show that field reordering cannot yield significant performance gains. We also show that even a fairly large L2 data cache is not effective for many Java benchmarks. We observe that instructions used to prefetch data into the L2 data cache are often squashed because of high TLB miss rates and because the TLB does not usually have the translation information needed to prefetch the data into the L2 data cache. We also find that co-allocation of frequently used method tables can reduce the number of TLB misses and lower the cost of accessing type information block entries in virtual method calls and runtime type checking.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {194--205},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378783},
 doi = {http://doi.acm.org/10.1145/384268.378783},
 acmid = {378783},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sohoni:2001:SMS:378420.378784,
 author = {Sohoni, Sohum and Min, Rui and Xu, Zhiyong and Hu, Yiming},
 title = {A study of memory system performance of multimedia applications},
 abstract = {Multimedia applications are fast becoming one of the dominating workloads for modern computer systems. Since these applications normally have large data sets and little data-reuse, many researchers believe that they have poor memory behavior compared to traditional programs, and that current cache architectures cannot handle them well. It is therefore important to quantitatively characterize the memory behavior of these applications in order to provide insights for future design and research of memory systems. However, very few results on this topic have been published. This paper presents a comprehensive research on the memory requirements of a group of programs that are representative of multimedia applications. These programs include a subset of the popular MediaBench suite and several large multimedia programs running on the Linux, Windows NT and Tru UNIX operating systems. We performed extensive measurement and trace-driven simulation experiments. We then compared the memory utilization of these programs to that of SPECint95 applications. We found that multimedia applications actually have better memory behavior than SPECint95 programs. The high cache hit rates of multimedia applications can be contributed to the following three factors. Most multimedia applications apply block partitioning algorithms to the input data, and work on small blocks of data that easily fit into the cache. Secondly, within these blocks, there is significant data reuse as well as spatial locality. The third reason is that a large number of references generated by multimedia applications are to their internal data structures, which are relatively small and can also easily fit into reasonably-sized caches.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {206--215},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378784},
 doi = {http://doi.acm.org/10.1145/378420.378784},
 acmid = {378784},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sohoni:2001:SMS:384268.378784,
 author = {Sohoni, Sohum and Min, Rui and Xu, Zhiyong and Hu, Yiming},
 title = {A study of memory system performance of multimedia applications},
 abstract = {Multimedia applications are fast becoming one of the dominating workloads for modern computer systems. Since these applications normally have large data sets and little data-reuse, many researchers believe that they have poor memory behavior compared to traditional programs, and that current cache architectures cannot handle them well. It is therefore important to quantitatively characterize the memory behavior of these applications in order to provide insights for future design and research of memory systems. However, very few results on this topic have been published. This paper presents a comprehensive research on the memory requirements of a group of programs that are representative of multimedia applications. These programs include a subset of the popular MediaBench suite and several large multimedia programs running on the Linux, Windows NT and Tru UNIX operating systems. We performed extensive measurement and trace-driven simulation experiments. We then compared the memory utilization of these programs to that of SPECint95 applications. We found that multimedia applications actually have better memory behavior than SPECint95 programs. The high cache hit rates of multimedia applications can be contributed to the following three factors. Most multimedia applications apply block partitioning algorithms to the input data, and work on small blocks of data that easily fit into the cache. Secondly, within these blocks, there is significant data reuse as well as spatial locality. The third reason is that a large number of references generated by multimedia applications are to their internal data structures, which are relatively small and can also easily fit into reasonably-sized caches.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {206--215},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378784},
 doi = {http://doi.acm.org/10.1145/384268.378784},
 acmid = {378784},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bu:2001:FPA:378420.378786,
 author = {Bu, Tian and Towsley, Don},
 title = {Fixed point approximations for TCP behavior in an AQM network},
 abstract = {In this paper, we explore the use of fixed point methods to evaluate the performance of a large population of TCP flows traversing a network of routers implementing active queue management (AQM) such as RED (random early detection). Both AQM routers that drop and that mark packets are considered along with infinite and finite duration TCP flows. In the case of finite duration flows, we restrict ourselves to networks containing one congested router. In all cases, we formulate a fixed point problem with the router average queue lengths as unknowns. Once these are obtained, other metrics such as router loss probability, TCP flow throughput, TCP flow end-to-end loss rates, average round trip time, and average session duration are easily obtained. Comparison with simulation for a variety of scenarios shows that the model is accurate in its predictions (mean errors less than 5\%). Last, we establish monotonicity properties exhibited by the solution for a single congested router that explains several interesting observations, such as TCP SACK suffers higher loss than TCP Reno.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {216--225},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378786},
 doi = {http://doi.acm.org/10.1145/378420.378786},
 acmid = {378786},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bu:2001:FPA:384268.378786,
 author = {Bu, Tian and Towsley, Don},
 title = {Fixed point approximations for TCP behavior in an AQM network},
 abstract = {In this paper, we explore the use of fixed point methods to evaluate the performance of a large population of TCP flows traversing a network of routers implementing active queue management (AQM) such as RED (random early detection). Both AQM routers that drop and that mark packets are considered along with infinite and finite duration TCP flows. In the case of finite duration flows, we restrict ourselves to networks containing one congested router. In all cases, we formulate a fixed point problem with the router average queue lengths as unknowns. Once these are obtained, other metrics such as router loss probability, TCP flow throughput, TCP flow end-to-end loss rates, average round trip time, and average session duration are easily obtained. Comparison with simulation for a variety of scenarios shows that the model is accurate in its predictions (mean errors less than 5\%). Last, we establish monotonicity properties exhibited by the solution for a single congested router that explains several interesting observations, such as TCP SACK suffers higher loss than TCP Reno.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {216--225},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378786},
 doi = {http://doi.acm.org/10.1145/384268.378786},
 acmid = {378786},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Low:2001:UTV:384268.378787,
 author = {Low, Steven H. and Peterson, Larry and Wang, Limin},
 title = {Understanding TCP vegas: a duality model},
 abstract = {This paper presents a model of the TCP Vegas congestion control mechanism as a distributed optimization algorithm. Doing so has three important benefits. First, it helps us gain a fundamental understanding of why TCP Vegas works, and an appreciation of its limitations. Second, it allows us to prove that Vegas stabilizes at a weighted proportionally fair allocation of network capacity when there is sufficient buffering in the network. Third, it suggests how we might use explicit feedback to allow each Vegas source to determine the optimal sending rate when there is insufficient buffering in the network. We present simulation results that validate our conclusions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {226--235},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378787},
 doi = {http://doi.acm.org/10.1145/384268.378787},
 acmid = {378787},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Low:2001:UTV:378420.378787,
 author = {Low, Steven H. and Peterson, Larry and Wang, Limin},
 title = {Understanding TCP vegas: a duality model},
 abstract = {This paper presents a model of the TCP Vegas congestion control mechanism as a distributed optimization algorithm. Doing so has three important benefits. First, it helps us gain a fundamental understanding of why TCP Vegas works, and an appreciation of its limitations. Second, it allows us to prove that Vegas stabilizes at a weighted proportionally fair allocation of network capacity when there is sufficient buffering in the network. Third, it suggests how we might use explicit feedback to allow each Vegas source to determine the optimal sending rate when there is insufficient buffering in the network. We present simulation results that validate our conclusions.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {226--235},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378787},
 doi = {http://doi.acm.org/10.1145/378420.378787},
 acmid = {378787},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Talim:2001:CRW:384268.378788,
 author = {Talim, J. and Liu, Z. and Nain, Ph. and Coffman,Jr., E. G.},
 title = {Controlling the robots of Web search engines},
 abstract = {Robots are deployed by a Web search engine for collecting information from different Web servers in order to maintain the currency of its data base of Web pages. In this paper, we investigate the number of robots to be used by a search engine so as to maximize the currency of the data base without putting an unnecessary load on the network. We adopt a finite-buffer queueing model to represent the system. The arrivals to the queueing system are Web pages brought by the robots; service corresponds to the indexing of these pages. Good performance requires that the number of robots, and thus the arrival rate of the queueing system, be chosen so that the indexing queue is rarely starved or saturated. Thus, we formulate a multi-criteria stochastic optimization problem with the loss rate and empty-buffer probability being the criteria. We take the common approach of reducing the problem to one with a single objective that is a linear function of the given criteria. Both static and dynamic policies can be considered. In the static setting the number of robots is held fixed; in the dynamic setting robots may be re-activated/de-activated as a function of the state. Under the assumption that arrivals form a Poisson process and that service times are independent and exponentially distributed random variables, we determine an optimal decision rule for the dynamic setting, i.e., a rule that varies the number of robots in such a way as to minimize a given linear function of the loss rate and empty-buffer probability. Our results are compared with known results for the static case. A numerical study indicates that substantial gains can be achieved by dynamically controlling the activity of the robots.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {236--244},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/384268.378788},
 doi = {http://doi.acm.org/10.1145/384268.378788},
 acmid = {378788},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov decision process, Web search engines, queues, web robots},
} 

@inproceedings{Talim:2001:CRW:378420.378788,
 author = {Talim, J. and Liu, Z. and Nain, Ph. and Coffman,Jr., E. G.},
 title = {Controlling the robots of Web search engines},
 abstract = {Robots are deployed by a Web search engine for collecting information from different Web servers in order to maintain the currency of its data base of Web pages. In this paper, we investigate the number of robots to be used by a search engine so as to maximize the currency of the data base without putting an unnecessary load on the network. We adopt a finite-buffer queueing model to represent the system. The arrivals to the queueing system are Web pages brought by the robots; service corresponds to the indexing of these pages. Good performance requires that the number of robots, and thus the arrival rate of the queueing system, be chosen so that the indexing queue is rarely starved or saturated. Thus, we formulate a multi-criteria stochastic optimization problem with the loss rate and empty-buffer probability being the criteria. We take the common approach of reducing the problem to one with a single objective that is a linear function of the given criteria. Both static and dynamic policies can be considered. In the static setting the number of robots is held fixed; in the dynamic setting robots may be re-activated/de-activated as a function of the state. Under the assumption that arrivals form a Poisson process and that service times are independent and exponentially distributed random variables, we determine an optimal decision rule for the dynamic setting, i.e., a rule that varies the number of robots in such a way as to minimize a given linear function of the loss rate and empty-buffer probability. Our results are compared with known results for the static case. A numerical study indicates that substantial gains can be achieved by dynamically controlling the activity of the robots.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {236--244},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/378420.378788},
 doi = {http://doi.acm.org/10.1145/378420.378788},
 acmid = {378788},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov decision process, Web search engines, queues, web robots},
} 

@inproceedings{Smith:2001:TPH:378420.378789,
 author = {Smith, F. Donelson and Campos, F\'{e}lix Hern\'{a}ndez and Jeffay, Kevin and Ott, David},
 title = {What TCP/IP protocol headers can tell us about the web},
 abstract = {We report the results of a large-scale empirical study of web traffic. Our study is based on over 500 GB of TCP/IP protocol-header traces collected in 1999 and 2000 (approximately one year apart) from the high-speed link connecting The University of North Carolina at Chapel Hill to its Internet service provider. We also use a set of smaller traces from the NLANR repository taken at approximately the same times for comparison. The principal results from this study are: (1) empirical data suitable for constructing traffic generating models of contemporary web traffic, (2) new characterizations of TCP connection usage showing the effects of HTTP protocol improvement, notably persistent connections (e.g.</i>, about 50\% of web objects are now transferred on persistent connections), and (3) new characterizations of web usage and content structure that reflect the influences of "banner ads," server load balancing, and content distribution. A novel aspect of this study is a demonstration that a relatively light-weight methodology based on passive tracing of only TCP/IP headers and off-line analysis tools can provide timely, high quality data about web traffic. We hope this will encourage more researchers to undertake on-going data collection and provide the research community with data about the rapidly evolving characteristics of web traffic.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {245--256},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378789},
 doi = {http://doi.acm.org/10.1145/378420.378789},
 acmid = {378789},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:2001:TPH:384268.378789,
 author = {Smith, F. Donelson and Campos, F\'{e}lix Hern\'{a}ndez and Jeffay, Kevin and Ott, David},
 title = {What TCP/IP protocol headers can tell us about the web},
 abstract = {We report the results of a large-scale empirical study of web traffic. Our study is based on over 500 GB of TCP/IP protocol-header traces collected in 1999 and 2000 (approximately one year apart) from the high-speed link connecting The University of North Carolina at Chapel Hill to its Internet service provider. We also use a set of smaller traces from the NLANR repository taken at approximately the same times for comparison. The principal results from this study are: (1) empirical data suitable for constructing traffic generating models of contemporary web traffic, (2) new characterizations of TCP connection usage showing the effects of HTTP protocol improvement, notably persistent connections (e.g.</i>, about 50\% of web objects are now transferred on persistent connections), and (3) new characterizations of web usage and content structure that reflect the influences of "banner ads," server load balancing, and content distribution. A novel aspect of this study is a demonstration that a relatively light-weight methodology based on passive tracing of only TCP/IP headers and off-line analysis tools can provide timely, high quality data about web traffic. We hope this will encourage more researchers to undertake on-going data collection and provide the research community with data about the rapidly evolving characteristics of web traffic.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {245--256},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378789},
 doi = {http://doi.acm.org/10.1145/384268.378789},
 acmid = {378789},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nahum:2001:EWC:378420.378790,
 author = {Nahum, Erich M. and Rosu, Marcel-Catalin and Seshan, Srinivasan and Almeida, Jussara},
 title = {The effects of wide-area conditions on WWW server performance},
 abstract = {WWW workload generators are used to evaluate web server performance, and thus have a large impact on what performance optimizations are applied to servers. However, current benchmarks ignore a crucial component: how these servers perform in the environment in which they are intended to be used, namely the wide-area Internet.This paper shows how WAN conditions can affect WWW server performance. We examine these effects using an experimental test-bed which emulates WAN characteristics in a live setting, by introducing factors such as delay and packet loss in a controlled and reproducible fashion. We study how these factors interact with the host TCP implementation and what influence they have on web server performance. We demonstrate that when more realistic wide-area conditions are introduced, servers exhibit very different performance properties and scaling behaviors, which are not exposed by existing benchmarks running on LANs. We show that observed throughputs can give misleading information about server performance, and thus find that maximum throughput, or capacity, is a more useful metric. We find that packet losses can reduce server capacity by as much as 50 percent and increase response time as seen by the client. We show that using TCP SACK can reduce client response time, without reducing server capacity.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {257--267},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378420.378790},
 doi = {http://doi.acm.org/10.1145/378420.378790},
 acmid = {378790},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nahum:2001:EWC:384268.378790,
 author = {Nahum, Erich M. and Rosu, Marcel-Catalin and Seshan, Srinivasan and Almeida, Jussara},
 title = {The effects of wide-area conditions on WWW server performance},
 abstract = {WWW workload generators are used to evaluate web server performance, and thus have a large impact on what performance optimizations are applied to servers. However, current benchmarks ignore a crucial component: how these servers perform in the environment in which they are intended to be used, namely the wide-area Internet.This paper shows how WAN conditions can affect WWW server performance. We examine these effects using an experimental test-bed which emulates WAN characteristics in a live setting, by introducing factors such as delay and packet loss in a controlled and reproducible fashion. We study how these factors interact with the host TCP implementation and what influence they have on web server performance. We demonstrate that when more realistic wide-area conditions are introduced, servers exhibit very different performance properties and scaling behaviors, which are not exposed by existing benchmarks running on LANs. We show that observed throughputs can give misleading information about server performance, and thus find that maximum throughput, or capacity, is a more useful metric. We find that packet losses can reduce server capacity by as much as 50 percent and increase response time as seen by the client. We show that using TCP SACK can reduce client response time, without reducing server capacity.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {257--267},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384268.378790},
 doi = {http://doi.acm.org/10.1145/384268.378790},
 acmid = {378790},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nain:2001:MQS:378420.378791,
 author = {Nain, Philippe and N\'{u}\~{n}ez-Queija, Redusindo},
 title = {A M/M/ queue in a semi-Markovian environment},
 abstract = {We consider an M/M/1 queue in a semi-Markovian environment. The environment is modeled by a two-state semi-Markov process with arbitrary sojourn time distributions F</i><inf>0</inf>(x</i>) and F</i><inf>1</inf>(x</i>). When in state i</i> = 0, 1, customers are generated according to a Poisson process with intensity \&amp;lambda;<inf>i</i></inf> and customers are served according to an exponential distribution with rate \&amp;mu;<inf>i</i></inf>. Using the theory of Riemann-Hilbert boundary value problems we compute the z</i>-transform of the queue-length distribution when either F</i><inf>0</inf>(x</i>) or F</i><inf>1</inf>(x</i>) has a rational Laplace-Stieltjes transform and the other may be a general --- possibly heavy-tailed --- distribution. The arrival process can be used to model bursty traffic and/or traffic exhibiting long-range dependence, a situation which is commonly encountered in networking. The closed-form results lend themselves for numerical evaluation of performance measures, in particular the mean queue-length.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {268--278},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378420.378791},
 doi = {http://doi.acm.org/10.1145/378420.378791},
 acmid = {378791},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Riemann-Hilbert boundary value problem, aueueing, bursty traffic, communication networks, heavy-tailed distribution, long-range dependence, stochastic modeling},
} 

@article{Nain:2001:MQS:384268.378791,
 author = {Nain, Philippe and N\'{u}\~{n}ez-Queija, Redusindo},
 title = {A M/M/ queue in a semi-Markovian environment},
 abstract = {We consider an M/M/1 queue in a semi-Markovian environment. The environment is modeled by a two-state semi-Markov process with arbitrary sojourn time distributions F</i><inf>0</inf>(x</i>) and F</i><inf>1</inf>(x</i>). When in state i</i> = 0, 1, customers are generated according to a Poisson process with intensity \&amp;lambda;<inf>i</i></inf> and customers are served according to an exponential distribution with rate \&amp;mu;<inf>i</i></inf>. Using the theory of Riemann-Hilbert boundary value problems we compute the z</i>-transform of the queue-length distribution when either F</i><inf>0</inf>(x</i>) or F</i><inf>1</inf>(x</i>) has a rational Laplace-Stieltjes transform and the other may be a general --- possibly heavy-tailed --- distribution. The arrival process can be used to model bursty traffic and/or traffic exhibiting long-range dependence, a situation which is commonly encountered in networking. The closed-form results lend themselves for numerical evaluation of performance measures, in particular the mean queue-length.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {268--278},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384268.378791},
 doi = {http://doi.acm.org/10.1145/384268.378791},
 acmid = {378791},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Riemann-Hilbert boundary value problem, aueueing, bursty traffic, communication networks, heavy-tailed distribution, long-range dependence, stochastic modeling},
} 

@article{Bansal:2001:ASS:384268.378792,
 author = {Bansal, Nikhil and Harchol-Balter, Mor},
 title = {Analysis of SRPT scheduling: investigating unfairness},
 abstract = {The Shortest-Remaining-Processing-Time (SRPT) scheduling policy has long been known to be optimal for minimizing mean response time (sojourn time). Despite this fact, SRPT scheduling is rarely used in practice. It is believed that the performance improvements of SRPT over other scheduling policies stem from the fact that SRPT unfairly penalizes the large jobs in order to help the small jobs. This belief has led people to instead adopt "fair" scheduling policies such as Processor-Sharing (PS), which produces the same expected slowdown for jobs of all sizes.This paper investigates formally the problem of unfairness in SRPT scheduling as compared with PS scheduling. The analysis assumes an M/G/1 model, and emphasizes job size distributions with a heavy-tailed property, as are characteristic of empirical workloads. The analysis shows that the degree of unfairness under SRPT is surprisingly small.The M/G/1/SRPT and M/G/1/PS queues are also analyzed under overload and closed-form expressions for mean response time as a function of job size are proved in this setting.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/384268.378792},
 doi = {http://doi.acm.org/10.1145/384268.378792},
 acmid = {378792},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bansal:2001:ASS:378420.378792,
 author = {Bansal, Nikhil and Harchol-Balter, Mor},
 title = {Analysis of SRPT scheduling: investigating unfairness},
 abstract = {The Shortest-Remaining-Processing-Time (SRPT) scheduling policy has long been known to be optimal for minimizing mean response time (sojourn time). Despite this fact, SRPT scheduling is rarely used in practice. It is believed that the performance improvements of SRPT over other scheduling policies stem from the fact that SRPT unfairly penalizes the large jobs in order to help the small jobs. This belief has led people to instead adopt "fair" scheduling policies such as Processor-Sharing (PS), which produces the same expected slowdown for jobs of all sizes.This paper investigates formally the problem of unfairness in SRPT scheduling as compared with PS scheduling. The analysis assumes an M/G/1 model, and emphasizes job size distributions with a heavy-tailed property, as are characteristic of empirical workloads. The analysis shows that the degree of unfairness under SRPT is surprisingly small.The M/G/1/SRPT and M/G/1/PS queues are also analyzed under overload and closed-form expressions for mean response time as a function of job size are proved in this setting.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/378420.378792},
 doi = {http://doi.acm.org/10.1145/378420.378792},
 acmid = {378792},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Luthi:2001:IPC:378420.378794,
 author = {L\"{u}thi, Johannes and Llad\'{o}, Catalina M.},
 title = {Interval parameters for capturing uncertainties in an EJB performance model},
 abstract = {Exact as well as approximate analytical solutions for quantitative performance models of computer systems are usually obtained by performing a series of arithmetical operations on the input parameters of the model. However, especially during early phases of system design and implementation, not all the parameter values are usually known exactly. In related research contributions, intervals have been proposed as a means to capture parameter uncertainties. Furthermore, methods to adapt existing solution algorithms to parameter intervals have been discussed. In this paper we present the adaptation of an existing performance model to parameter intervals. The approximate solution of a queueing network modelling an Enterprise JavaBeans server implementation is adapted to interval arithmetic in order to represent the uncertainty in some of the parameters of the model. A new interval splitting method is applied to obtain reasonable tight performance measure intervals. Monotonicity properties of intermediate computation results are exploited to achieve a more efficient interval solution. In addition, parts of the original solution algorithm are modified to increase the efficiency of the corresponding interval arithmetical solution.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/378420.378794},
 doi = {http://doi.acm.org/10.1145/378420.378794},
 acmid = {378794},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed systems, enterprise JavaBeans, interval parameters, parameter uncertainties, performance models, queueing},
} 

@article{Luthi:2001:IPC:384268.378794,
 author = {L\"{u}thi, Johannes and Llad\'{o}, Catalina M.},
 title = {Interval parameters for capturing uncertainties in an EJB performance model},
 abstract = {Exact as well as approximate analytical solutions for quantitative performance models of computer systems are usually obtained by performing a series of arithmetical operations on the input parameters of the model. However, especially during early phases of system design and implementation, not all the parameter values are usually known exactly. In related research contributions, intervals have been proposed as a means to capture parameter uncertainties. Furthermore, methods to adapt existing solution algorithms to parameter intervals have been discussed. In this paper we present the adaptation of an existing performance model to parameter intervals. The approximate solution of a queueing network modelling an Enterprise JavaBeans server implementation is adapted to interval arithmetic in order to represent the uncertainty in some of the parameters of the model. A new interval splitting method is applied to obtain reasonable tight performance measure intervals. Monotonicity properties of intermediate computation results are exploited to achieve a more efficient interval solution. In addition, parts of the original solution algorithm are modified to increase the efficiency of the corresponding interval arithmetical solution.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/384268.378794},
 doi = {http://doi.acm.org/10.1145/384268.378794},
 acmid = {378794},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed systems, enterprise JavaBeans, interval parameters, parameter uncertainties, performance models, queueing},
} 

@article{El-Sayed:2001:ASS:384268.378799,
 author = {El-Sayed, Hesham and Cameron, Don and Woodside, Murray},
 title = {Automation support for software performance engineering},
 abstract = {To evaluate the performance of a software design one must create a model of the software, together with the execution platform and configuration. Assuming that the "platform": (processors, networks, and operating systems) are specified by the designer, a good "configuration" (the allocation of tasks to processors, priorities, and other aspects of the installation) must be determined. Finding one may be a barrier to rapid evaluation; it is a more serious barrier if there are many platforms to be considered. This paper describes an automated heuristic procedure for configuring a software system described by a layered architectural software model, onto a set of processors, and choosing priorities. The procedure attempts to meet a soft-real-time performance specification, in which any number of scenarios have deadlines which must be realized some percentage of the time. It has been successful in configuring large systems with both soft and hard deadlines.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {301--311},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/384268.378799},
 doi = {http://doi.acm.org/10.1145/384268.378799},
 acmid = {378799},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{El-Sayed:2001:ASS:378420.378799,
 author = {El-Sayed, Hesham and Cameron, Don and Woodside, Murray},
 title = {Automation support for software performance engineering},
 abstract = {To evaluate the performance of a software design one must create a model of the software, together with the execution platform and configuration. Assuming that the "platform": (processors, networks, and operating systems) are specified by the designer, a good "configuration" (the allocation of tasks to processors, priorities, and other aspects of the installation) must be determined. Finding one may be a barrier to rapid evaluation; it is a more serious barrier if there are many platforms to be considered. This paper describes an automated heuristic procedure for configuring a software system described by a layered architectural software model, onto a set of processors, and choosing priorities. The procedure attempts to meet a soft-real-time performance specification, in which any number of scenarios have deadlines which must be realized some percentage of the time. It has been successful in configuring large systems with both soft and hard deadlines.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {301--311},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/378420.378799},
 doi = {http://doi.acm.org/10.1145/378420.378799},
 acmid = {378799},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bradshaw:2001:PBP:378420.378801,
 author = {Bradshaw, Michael K. and Wang, Bing and Sen, Subhabrata and Gao, Lixin and Kurose, Jim and Shenoy, Prashant and Towsley, Don},
 title = {Periodic broadcast and patching services: implementation, measurement, and analysis in an Internet streaming video testbed},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {312--313},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378801},
 doi = {http://doi.acm.org/10.1145/378420.378801},
 acmid = {378801},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bradshaw:2001:PBP:384268.378801,
 author = {Bradshaw, Michael K. and Wang, Bing and Sen, Subhabrata and Gao, Lixin and Kurose, Jim and Shenoy, Prashant and Towsley, Don},
 title = {Periodic broadcast and patching services: implementation, measurement, and analysis in an Internet streaming video testbed},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {312--313},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378801},
 doi = {http://doi.acm.org/10.1145/384268.378801},
 acmid = {378801},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yang:2001:TSR:378420.378803,
 author = {Yang, Yang Richard and Li, Xiaozhou and Lam, Simon S. and Zhang, Xincheng},
 title = {Towards scalable and reliable group key management},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {314--315},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378803},
 doi = {http://doi.acm.org/10.1145/378420.378803},
 acmid = {378803},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yang:2001:TSR:384268.378803,
 author = {Yang, Yang Richard and Li, Xiaozhou and Lam, Simon S. and Zhang, Xincheng},
 title = {Towards scalable and reliable group key management},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {314--315},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378803},
 doi = {http://doi.acm.org/10.1145/384268.378803},
 acmid = {378803},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bremler-Barr:2001:RPC:384268.378805,
 author = {Bremler-Barr, Anat and Afek, Yehuda and Kaplan, Haim and Cohen, Edith and Merritt, Michael},
 title = {Restoration path concatenation: fast recovery of MPLS paths},
 abstract = {A new general theory about restoration</i> of network paths is first introduced. The theory pertains to restoration of shortest paths in a network following failure, e.g., we prove that a shortest path in a network after removing k</i> edges is the concatenation of at most k</i> + 1 shortest paths in the original network.The theory is then combined with efficient path concatenation techniques in MPLS (multi-protocol label switching), to achieve powerful schemes for restoration in MPLS based networks. We thus transform MPLS into a flexible and robust method for forwarding packets in a network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {316--317},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378805},
 doi = {http://doi.acm.org/10.1145/384268.378805},
 acmid = {378805},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bremler-Barr:2001:RPC:378420.378805,
 author = {Bremler-Barr, Anat and Afek, Yehuda and Kaplan, Haim and Cohen, Edith and Merritt, Michael},
 title = {Restoration path concatenation: fast recovery of MPLS paths},
 abstract = {A new general theory about restoration</i> of network paths is first introduced. The theory pertains to restoration of shortest paths in a network following failure, e.g., we prove that a shortest path in a network after removing k</i> edges is the concatenation of at most k</i> + 1 shortest paths in the original network.The theory is then combined with efficient path concatenation techniques in MPLS (multi-protocol label switching), to achieve powerful schemes for restoration in MPLS based networks. We thus transform MPLS into a flexible and robust method for forwarding packets in a network.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {316--317},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378805},
 doi = {http://doi.acm.org/10.1145/378420.378805},
 acmid = {378805},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Savvides:2001:MNW:378420.378808,
 author = {Savvides, Andreas and Park, Sung and Srivastava, Mani B.},
 title = {On modeling networks of wireless microsensors},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {318--319},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378808},
 doi = {http://doi.acm.org/10.1145/378420.378808},
 acmid = {378808},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Savvides:2001:MNW:384268.378808,
 author = {Savvides, Andreas and Park, Sung and Srivastava, Mani B.},
 title = {On modeling networks of wireless microsensors},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {318--319},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378808},
 doi = {http://doi.acm.org/10.1145/384268.378808},
 acmid = {378808},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tsigas:2001:EPN:384268.378810,
 author = {Tsigas, Philippas and Zhang, Yi},
 title = {Evaluating the performance of non-blocking synchronization on shared-memory multiprocessors},
 abstract = {Parallel programs running on shared memory multiprocessors coordinate via shared data objects/structures. To ensure the consistency of the shared data structures, programs typically rely on some forms of software synchronisations. Unfortunately typical software synchronisation mechanisms usually result in poor performance because they produce large amounts of memory and interconnection network contention and, more significantly, because they produce convoy effects that degrade significantly in multiprogramming environments: if one process holding a lock is preempted, other processes on different processors waiting for the lock will not be able to proceed. Researchers have introduced non-blocking synchronisation to address the above problems. Non-blocking implementations allow multiple tasks to access a shared object at the same time, but without enforcing mutual exclusion to accomplish this. However, its performance implications are not well understood on modern systems or on real applications. In this paper we study the impact of the non-blocking synchronisation on parallel applications running on top of a modern, 64 processor, cache-coherent, shared memory multiprocessor system: the SGI Origin 2000. Cache-coherent non-uniform memory access (ccNUMA) shared memory multiprocessor systems have attracted considerable research and commercial interest in the last years. In addition to the performance results on a modern system, we also investigate the key synchronisation schemes that are used in multiprocessor applications and their efficient transformation to non-blocking ones. Evaluating the impact of the synchronisation performance on applications is important for several reasons. First, micro-benchmarks can not capture every aspect of primitive performance. It is hard to predict the primitive impact on the application performance. For example, a look or barrier that generates a lot of additional network traffic might have little impact on applications. Second, even in applications that spend significant time in synchronisation operations, the synchronisation time might be dominated by wait time due to load imbalance and lock serialisation in the application, which better implementations of synchronisation may not be helpful in reducing. Third, micro-benchmarks rarely capture (generate) scenarios that occur in real applications.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {320--321},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378810},
 doi = {http://doi.acm.org/10.1145/384268.378810},
 acmid = {378810},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tsigas:2001:EPN:378420.378810,
 author = {Tsigas, Philippas and Zhang, Yi},
 title = {Evaluating the performance of non-blocking synchronization on shared-memory multiprocessors},
 abstract = {Parallel programs running on shared memory multiprocessors coordinate via shared data objects/structures. To ensure the consistency of the shared data structures, programs typically rely on some forms of software synchronisations. Unfortunately typical software synchronisation mechanisms usually result in poor performance because they produce large amounts of memory and interconnection network contention and, more significantly, because they produce convoy effects that degrade significantly in multiprogramming environments: if one process holding a lock is preempted, other processes on different processors waiting for the lock will not be able to proceed. Researchers have introduced non-blocking synchronisation to address the above problems. Non-blocking implementations allow multiple tasks to access a shared object at the same time, but without enforcing mutual exclusion to accomplish this. However, its performance implications are not well understood on modern systems or on real applications. In this paper we study the impact of the non-blocking synchronisation on parallel applications running on top of a modern, 64 processor, cache-coherent, shared memory multiprocessor system: the SGI Origin 2000. Cache-coherent non-uniform memory access (ccNUMA) shared memory multiprocessor systems have attracted considerable research and commercial interest in the last years. In addition to the performance results on a modern system, we also investigate the key synchronisation schemes that are used in multiprocessor applications and their efficient transformation to non-blocking ones. Evaluating the impact of the synchronisation performance on applications is important for several reasons. First, micro-benchmarks can not capture every aspect of primitive performance. It is hard to predict the primitive impact on the application performance. For example, a look or barrier that generates a lot of additional network traffic might have little impact on applications. Second, even in applications that spend significant time in synchronisation operations, the synchronisation time might be dominated by wait time due to load imbalance and lock serialisation in the application, which better implementations of synchronisation may not be helpful in reducing. Third, micro-benchmarks rarely capture (generate) scenarios that occur in real applications.
},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {320--321},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378810},
 doi = {http://doi.acm.org/10.1145/378420.378810},
 acmid = {378810},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ng:2001:OHP:384268.378813,
 author = {Ng, Wee Teck and Hillyer, Bruce K.},
 title = {Obtaining high performance for storage outsourcing},
 abstract = {The viability of storage outsourcing is critically dependent on the access performance of remote storage. We study this issue by measuring the behavior of a broad variety of I/O-intensive benchmarks as they access remote storage over an IP network. We measure the effect of network latencies that correspond to distances ranging from a local neighborhood to halfway across a continent. We then measure the effect of latency-hiding mechanisms. Our results indicate that, in many cases, the adverse effects of network delay can be rendered inconsequential by clever file system and operating system techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {322--323},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378813},
 doi = {http://doi.acm.org/10.1145/384268.378813},
 acmid = {378813},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ng:2001:OHP:378420.378813,
 author = {Ng, Wee Teck and Hillyer, Bruce K.},
 title = {Obtaining high performance for storage outsourcing},
 abstract = {The viability of storage outsourcing is critically dependent on the access performance of remote storage. We study this issue by measuring the behavior of a broad variety of I/O-intensive benchmarks as they access remote storage over an IP network. We measure the effect of network latencies that correspond to distances ranging from a local neighborhood to halfway across a continent. We then measure the effect of latency-hiding mechanisms. Our results indicate that, in many cases, the adverse effects of network delay can be rendered inconsequential by clever file system and operating system techniques.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {322--323},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378813},
 doi = {http://doi.acm.org/10.1145/378420.378813},
 acmid = {378813},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Padamanabban:2001:DGL:384268.378814,
 author = {Padamanabban, Venkata N. and Subramanian, Lealkshminarayanan},
 title = {Determining the geographic location of Internet hosts},
 abstract = {We study the problem of determining the geographic location of an Internet host knowing only its IP address. We have developed three distinct techniques, GeoTrack, GeoPing,</i> and GeoCluster,</i> to address this problem. These techniques exploit information derived from the DNS, network delay measurements, and inter-domain routing. We have evaluated our techniques using extensive and varied datasets.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {324--325},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378814},
 doi = {http://doi.acm.org/10.1145/384268.378814},
 acmid = {378814},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Padamanabban:2001:DGL:378420.378814,
 author = {Padamanabban, Venkata N. and Subramanian, Lealkshminarayanan},
 title = {Determining the geographic location of Internet hosts},
 abstract = {We study the problem of determining the geographic location of an Internet host knowing only its IP address. We have developed three distinct techniques, GeoTrack, GeoPing,</i> and GeoCluster,</i> to address this problem. These techniques exploit information derived from the DNS, network delay measurements, and inter-domain routing. We have evaluated our techniques using extensive and varied datasets.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {324--325},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378814},
 doi = {http://doi.acm.org/10.1145/378420.378814},
 acmid = {378814},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mandjes:2001:LCA:384268.378816,
 author = {Mandjes, Michel and Saniee, Iraj and Stolyar, Alexander},
 title = {Load chracterization and anomaly detection for voice over IP traffic},
 abstract = {We consider the problem of traffic anomaly detection in IP networks. Traffic anomalies arise when there is overload due to failures in a network. We present general formulae for the variance of the cumulative traffic over a fixed time interval and show how the derived analytical expression simplifies for the case of voice over IP traffic, the focus of this paper. To detect load anomalies, we show it is sufficient to consider cumulative traffic over relatively long intervals such as 5 minutes. This approach substantially extends the current practice in IP network management where only the first order statistics and fixed thresholds are used to identify abnormal behavior.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {326--327},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378816},
 doi = {http://doi.acm.org/10.1145/384268.378816},
 acmid = {378816},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SNMP-based load characterization, VoIP traffic anomaly detection, variance estimation},
} 

@inproceedings{Mandjes:2001:LCA:378420.378816,
 author = {Mandjes, Michel and Saniee, Iraj and Stolyar, Alexander},
 title = {Load chracterization and anomaly detection for voice over IP traffic},
 abstract = {We consider the problem of traffic anomaly detection in IP networks. Traffic anomalies arise when there is overload due to failures in a network. We present general formulae for the variance of the cumulative traffic over a fixed time interval and show how the derived analytical expression simplifies for the case of voice over IP traffic, the focus of this paper. To detect load anomalies, we show it is sufficient to consider cumulative traffic over relatively long intervals such as 5 minutes. This approach substantially extends the current practice in IP network management where only the first order statistics and fixed thresholds are used to identify abnormal behavior.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {326--327},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378816},
 doi = {http://doi.acm.org/10.1145/378420.378816},
 acmid = {378816},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SNMP-based load characterization, VoIP traffic anomaly detection, variance estimation},
} 

@article{Downey:2001:SCF:384268.378824,
 author = {Downey, Allen B.},
 title = {The structural cause of file size distributions},
 abstract = {We propose a user model that explains the shape of the distribution of file sizes in local file systems and in the World Wide Web. We examine evidence from 562 file systems, 38 web clients and 6 web servers, and find that the model is a good description of these systems. These results cast doubt on the widespread view that the distribution of file sizes is long-tailed and that long-tailed distributions are the cause of self-similarity in the Internet.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {328--329},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378824},
 doi = {http://doi.acm.org/10.1145/384268.378824},
 acmid = {378824},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {file sizes, long-tailed distributions, self-similarity},
} 

@inproceedings{Downey:2001:SCF:378420.378824,
 author = {Downey, Allen B.},
 title = {The structural cause of file size distributions},
 abstract = {We propose a user model that explains the shape of the distribution of file sizes in local file systems and in the World Wide Web. We examine evidence from 562 file systems, 38 web clients and 6 web servers, and find that the model is a good description of these systems. These results cast doubt on the widespread view that the distribution of file sizes is long-tailed and that long-tailed distributions are the cause of self-similarity in the Internet.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {328--329},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378824},
 doi = {http://doi.acm.org/10.1145/378420.378824},
 acmid = {378824},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {file sizes, long-tailed distributions, self-similarity},
} 

@article{Bhargava:2001:UAM:384268.378826,
 author = {Bhargava, Rishi and Goel, Ashish and Meyerson, Adam},
 title = {Using approximate majorization to characterize protocol fairness},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {330--331},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378826},
 doi = {http://doi.acm.org/10.1145/384268.378826},
 acmid = {378826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bhargava:2001:UAM:378420.378826,
 author = {Bhargava, Rishi and Goel, Ashish and Meyerson, Adam},
 title = {Using approximate majorization to characterize protocol fairness},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {330--331},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378826},
 doi = {http://doi.acm.org/10.1145/378420.378826},
 acmid = {378826},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mellor-Crummey:2001:PUI:384268.378828,
 author = {Mellor-Crummey, John and Fowler, Robert and Whalley, David},
 title = {On providing useful information for analyzing and tuning applications},
 abstract = {Application performance tuning is a complex process that requires correlating many types of information with source code to locate and analyze performance problems bottle-necks. Existing performance tools don't adequately support this process in one or more dimensions. We describe two performance tools, MHsim</i> and HPCView</i>, that we built to support our own work on data layout and optimizing compilers. Both tools report their results in scope-hierarchy views of the corresponding source code and produce their output as HTML databases that can be analyzed portably and collaboratively using a commodity browser.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {332--333},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378828},
 doi = {http://doi.acm.org/10.1145/384268.378828},
 acmid = {378828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mellor-Crummey:2001:PUI:378420.378828,
 author = {Mellor-Crummey, John and Fowler, Robert and Whalley, David},
 title = {On providing useful information for analyzing and tuning applications},
 abstract = {Application performance tuning is a complex process that requires correlating many types of information with source code to locate and analyze performance problems bottle-necks. Existing performance tools don't adequately support this process in one or more dimensions. We describe two performance tools, MHsim</i> and HPCView</i>, that we built to support our own work on data layout and optimizing compilers. Both tools report their results in scope-hierarchy views of the corresponding source code and produce their output as HTML databases that can be analyzed portably and collaboratively using a commodity browser.
},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {332--333},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378828},
 doi = {http://doi.acm.org/10.1145/378420.378828},
 acmid = {378828},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shahabi:2001:ATE:384268.378830,
 author = {Shahabi, Cyrus and Kolahdouzan, Mohammad R. and Barish, Greg and Zimmermann, Roger and Yao, Didi and Fu, Kun and Zhang, Lingling},
 title = {Alternative techniques for the efficient acquisition of haptic data},
 abstract = {Immersive environments are those that surround users in an artificial world. These environments consist of a composition of various types of immersidata: unique data types that are combined to render a virtual experience. Acquisition, for storage and future querying, of information describing sessions in these environments is challenging because of the real-time demands and sizeable amounts of data to be managed. In this paper, we summarize a comparison of techniques for achieving the efficient acquisition of one type of immersidata, the haptic data type, which describes the movement, rotation, and force associated with user-directed objects in an immersive environment. In addition to describing a general process for real-time sampling and recording of this type of data, we propose three distinct sampling strategies: fixed, grouped, and adaptive. We conducted several experiments with a real haptic device and found that there are tradeoffs between the accuracy, efficiency, and complexity of implementation for each of the proposed techniques. While it is possible to use any of these approaches for real-time haptic data acquisition, we found that an adaptive sampling strategy provided the most efficiency without significant loss in accuracy. As immersive environments become more complex and contain more haptic sensors, techniques such as adaptive sampling can be useful for improving scalability of real-time data acquisition.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {334--335},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378830},
 doi = {http://doi.acm.org/10.1145/384268.378830},
 acmid = {378830},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {haptic data acquisition, immersidata, immersive technologies, sampling},
} 

@inproceedings{Shahabi:2001:ATE:378420.378830,
 author = {Shahabi, Cyrus and Kolahdouzan, Mohammad R. and Barish, Greg and Zimmermann, Roger and Yao, Didi and Fu, Kun and Zhang, Lingling},
 title = {Alternative techniques for the efficient acquisition of haptic data},
 abstract = {Immersive environments are those that surround users in an artificial world. These environments consist of a composition of various types of immersidata: unique data types that are combined to render a virtual experience. Acquisition, for storage and future querying, of information describing sessions in these environments is challenging because of the real-time demands and sizeable amounts of data to be managed. In this paper, we summarize a comparison of techniques for achieving the efficient acquisition of one type of immersidata, the haptic data type, which describes the movement, rotation, and force associated with user-directed objects in an immersive environment. In addition to describing a general process for real-time sampling and recording of this type of data, we propose three distinct sampling strategies: fixed, grouped, and adaptive. We conducted several experiments with a real haptic device and found that there are tradeoffs between the accuracy, efficiency, and complexity of implementation for each of the proposed techniques. While it is possible to use any of these approaches for real-time haptic data acquisition, we found that an adaptive sampling strategy provided the most efficiency without significant loss in accuracy. As immersive environments become more complex and contain more haptic sensors, techniques such as adaptive sampling can be useful for improving scalability of real-time data acquisition.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {334--335},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378830},
 doi = {http://doi.acm.org/10.1145/378420.378830},
 acmid = {378830},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {haptic data acquisition, immersidata, immersive technologies, sampling},
} 

@article{Dinda:2001:OPR:384268.378836,
 author = {Dinda, Peter A.},
 title = {Online prediction of the running time of tasks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {336--337},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378836},
 doi = {http://doi.acm.org/10.1145/384268.378836},
 acmid = {378836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dinda:2001:OPR:378420.378836,
 author = {Dinda, Peter A.},
 title = {Online prediction of the running time of tasks},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {336--337},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378836},
 doi = {http://doi.acm.org/10.1145/378420.378836},
 acmid = {378836},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Almeida:2001:ARB:378420.378838,
 author = {Almeida, Virgil\'{\i}o and Menasc\'{e}, Daniel and Riedi, Rudolf and Peligrinelli, Fl\'{a}via and Fonseca, Rodrigo and Meira,Jr., Wagner},
 title = {Analyzing robot behavior in e-business sites},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {338--339},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378838},
 doi = {http://doi.acm.org/10.1145/378420.378838},
 acmid = {378838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Almeida:2001:ARB:384268.378838,
 author = {Almeida, Virgil\'{\i}o and Menasc\'{e}, Daniel and Riedi, Rudolf and Peligrinelli, Fl\'{a}via and Fonseca, Rodrigo and Meira,Jr., Wagner},
 title = {Analyzing robot behavior in e-business sites},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {338--339},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378838},
 doi = {http://doi.acm.org/10.1145/384268.378838},
 acmid = {378838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Almeida:2001:CUA:384268.378843,
 author = {Almeida, Jussara M. and Krueger, Jeffrey and Vernon, Mary K.},
 title = {Characterization of user access to streaming media files},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {340--341},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378843},
 doi = {http://doi.acm.org/10.1145/384268.378843},
 acmid = {378843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Almeida:2001:CUA:378420.378843,
 author = {Almeida, Jussara M. and Krueger, Jeffrey and Vernon, Mary K.},
 title = {Characterization of user access to streaming media files},
 abstract = {},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {340--341},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378843},
 doi = {http://doi.acm.org/10.1145/378420.378843},
 acmid = {378843},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bonald:2001:PME:384268.378845,
 author = {Bonald, Thomas and Roberts, James},
 title = {Performance modeling of elastic traffic in overload},
 abstract = {While providers generally aim to avoid congestion by adequate provisioning, overload can clearly occur on certain network links. In this paper we propose some simple preliminary models for an overloaded link accounting for user impatience and reattempt behavior.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {342--343},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378845},
 doi = {http://doi.acm.org/10.1145/384268.378845},
 acmid = {378845},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bonald:2001:PME:378420.378845,
 author = {Bonald, Thomas and Roberts, James},
 title = {Performance modeling of elastic traffic in overload},
 abstract = {While providers generally aim to avoid congestion by adequate provisioning, overload can clearly occur on certain network links. In this paper we propose some simple preliminary models for an overloaded link accounting for user impatience and reattempt behavior.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {342--343},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378845},
 doi = {http://doi.acm.org/10.1145/378420.378845},
 acmid = {378845},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Qiu:2001:FFI:378420.378849,
 author = {Qiu, Lili and Varghese, George and Suri, Subhash},
 title = {Fast firewall implementations for software-based and hardware-based routers},
 abstract = {Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and diffserv. Classification can be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields is known to be difficult, and has poor worst-case complexity.In this paper, we re-examine two basic mechanisms that have been dismissed in the literature as being too inefficient: backtracking search and set pruning tries. We find using real databases that the time for backtracking search is much better than the worst-case bound; instead of \&amp;Omega;((logN</i>)<sup>k</i>-1</sup>), the search time is only roughly twice the optimal search time<sup>1</sup>. Similarly, we find that set pruning tries (using a DAG optimization) have much better storage costs than the worstcase bound. We also propose several new techniques to further improve the two basic mechanisms. Our major ideas are (i) backtracking search on a small memory budget, (ii) a novel compression algorithm, (iii) pipelining the search, (iv) the ability to trade-off smoothly between backtracking and set pruning, and (v) algorithms to effectively make use of hardware if hardware is available. We quantify the performance gain of each technique using real databases. We show that on real firewall databases our schemes, with the accompanying optimizations, are close to optimal in time and storage.},
 booktitle = {Proceedings of the 2001 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '01},
 year = {2001},
 isbn = {1-58113-334-0},
 location = {Cambridge, Massachusetts, United States},
 pages = {344--345},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/378420.378849},
 doi = {http://doi.acm.org/10.1145/378420.378849},
 acmid = {378849},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Qiu:2001:FFI:384268.378849,
 author = {Qiu, Lili and Varghese, George and Suri, Subhash},
 title = {Fast firewall implementations for software-based and hardware-based routers},
 abstract = {Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and diffserv. Classification can be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields is known to be difficult, and has poor worst-case complexity.In this paper, we re-examine two basic mechanisms that have been dismissed in the literature as being too inefficient: backtracking search and set pruning tries. We find using real databases that the time for backtracking search is much better than the worst-case bound; instead of \&amp;Omega;((logN</i>)<sup>k</i>-1</sup>), the search time is only roughly twice the optimal search time<sup>1</sup>. Similarly, we find that set pruning tries (using a DAG optimization) have much better storage costs than the worstcase bound. We also propose several new techniques to further improve the two basic mechanisms. Our major ideas are (i) backtracking search on a small memory budget, (ii) a novel compression algorithm, (iii) pipelining the search, (iv) the ability to trade-off smoothly between backtracking and set pruning, and (v) algorithms to effectively make use of hardware if hardware is available. We quantify the performance gain of each technique using real databases. We show that on real firewall databases our schemes, with the accompanying optimizations, are close to optimal in time and storage.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {29},
 issue = {1},
 month = {June},
 year = {2001},
 issn = {0163-5999},
 pages = {344--345},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/384268.378849},
 doi = {http://doi.acm.org/10.1145/384268.378849},
 acmid = {378849},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pasztor:2002:PBP:511334.511336,
 author = {P\'{a}sztor, Attila and Veitch, Darryl},
 title = {PC based precision timing without GPS},
 abstract = {A highly accurate monitoring solution for active network measurement is provided without the need for GPS, based on an alternative software clock for PC's running Unix. With respect to clock rate,</i> it's performance exceeds common GPS and NTP synchronized software clock accuracy. It is based on the TSC register counting CPU cycles and offers a resolution of around 1ns, a rate stability of 0.1PPM equal to that of the underlying hardware, and a processing overhead well under 1\&micro;s per timestamp. It is scalable and can be run in parallel with the usual clock. It is argued that accurate rate, and not synchronised offset, is the key requirement of a clock for network measurement. The clock requires an accurate estimation of the CPU cycle period. Two calibration methods which do not require a reference clock at the calibration point are given. To the TSC clock we add timestamping optimisations to create two high accuracy monitors, one based on Linux and the other on Real-Time Linux. The TSC-RT-Linux monitor has offset fluctuations of the order of 1\&micro;s. The clock is ideally suited for high precision active measurement.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/511334.511336},
 doi = {http://doi.acm.org/10.1145/511334.511336},
 acmid = {511336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPS, NTP, network measurement, pc clocks, software clock, synchronization, timing},
} 

@article{Pasztor:2002:PBP:511399.511336,
 author = {P\'{a}sztor, Attila and Veitch, Darryl},
 title = {PC based precision timing without GPS},
 abstract = {A highly accurate monitoring solution for active network measurement is provided without the need for GPS, based on an alternative software clock for PC's running Unix. With respect to clock rate,</i> it's performance exceeds common GPS and NTP synchronized software clock accuracy. It is based on the TSC register counting CPU cycles and offers a resolution of around 1ns, a rate stability of 0.1PPM equal to that of the underlying hardware, and a processing overhead well under 1\&micro;s per timestamp. It is scalable and can be run in parallel with the usual clock. It is argued that accurate rate, and not synchronised offset, is the key requirement of a clock for network measurement. The clock requires an accurate estimation of the CPU cycle period. Two calibration methods which do not require a reference clock at the calibration point are given. To the TSC clock we add timestamping optimisations to create two high accuracy monitors, one based on Linux and the other on Real-Time Linux. The TSC-RT-Linux monitor has offset fluctuations of the order of 1\&micro;s. The clock is ideally suited for high precision active measurement.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/511399.511336},
 doi = {http://doi.acm.org/10.1145/511399.511336},
 acmid = {511336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPS, NTP, network measurement, pc clocks, software clock, synchronization, timing},
} 

@article{Coates:2002:MLN:511399.511337,
 author = {Coates, Mark and Castro, Rui and Nowak, Robert and Gadhiok, Manik and King, Ryan and Tsang, Yolanda},
 title = {Maximum likelihood network topology identification from edge-based unicast measurements},
 abstract = {Network tomography is a process for inferring "internal" link-level delay and loss performance information based on end-to-end (edge) network measurements. These methods require knowledge of the network topology; therefore a first crucial step in the tomography process is topology identification. This paper considers the problem of discovering network topology solely from host-based, unicast measurements, without internal network cooperation. First, we introduce a novel delay-based measurement scheme that does not require clock synchronization, making it more practical than other previous proposals. In contrast to methods that rely on network cooperation , our methodology has the potential to identify layer two elements (provided they are logical topology branching points and induce some measurable delay). Second, we propose a maximum penalized likelihood criterion for topology identification. This is a global optimality criterion, in contrast to other recent proposals for topology identification that employ suboptimal, pair-merging strategies. We develop a novel Markov Chain Monte Carlo (MCMC) procedure for rapid determination of the most likely topologies. The performance of our new probing scheme and identification algorithm is explored through simulation and Internet experiments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/511399.511337},
 doi = {http://doi.acm.org/10.1145/511399.511337},
 acmid = {511337},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coates:2002:MLN:511334.511337,
 author = {Coates, Mark and Castro, Rui and Nowak, Robert and Gadhiok, Manik and King, Ryan and Tsang, Yolanda},
 title = {Maximum likelihood network topology identification from edge-based unicast measurements},
 abstract = {Network tomography is a process for inferring "internal" link-level delay and loss performance information based on end-to-end (edge) network measurements. These methods require knowledge of the network topology; therefore a first crucial step in the tomography process is topology identification. This paper considers the problem of discovering network topology solely from host-based, unicast measurements, without internal network cooperation. First, we introduce a novel delay-based measurement scheme that does not require clock synchronization, making it more practical than other previous proposals. In contrast to methods that rely on network cooperation , our methodology has the potential to identify layer two elements (provided they are logical topology branching points and induce some measurable delay). Second, we propose a maximum penalized likelihood criterion for topology identification. This is a global optimality criterion, in contrast to other recent proposals for topology identification that employ suboptimal, pair-merging strategies. We develop a novel Markov Chain Monte Carlo (MCMC) procedure for rapid determination of the most likely topologies. The performance of our new probing scheme and identification algorithm is explored through simulation and Internet experiments.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/511334.511337},
 doi = {http://doi.acm.org/10.1145/511334.511337},
 acmid = {511337},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bu:2002:NTG:511334.511338,
 author = {Bu, Tian and Duffield, Nick and Presti, Francesco Lo and Towsley, Don},
 title = {Network tomography on general topologies},
 abstract = {In this paper we consider the problem of inferring link-level loss rates from end-to-end multicast measurements taken from a collection of trees. We give conditions under which loss rates are identifiable on a specified set of links. Two algorithms are presented to perform the link-level inferences for those links on which losses can be identified. One, the minimum variance weighted average (MVWA) algorithm</i> treats the trees separately and then averages the results. The second, based on expectation-maximization (EM)</i> merges all of the measurements into one computation. Simulations show that EM is slightly more accurate than MVWA, most likely due to its more efficient use of the measurements. We also describe extensions to the inference of link-level delay, inference from end-to-end unicast measurements, and inference when some measurements are missing.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/511334.511338},
 doi = {http://doi.acm.org/10.1145/511334.511338},
 acmid = {511338},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bu:2002:NTG:511399.511338,
 author = {Bu, Tian and Duffield, Nick and Presti, Francesco Lo and Towsley, Don},
 title = {Network tomography on general topologies},
 abstract = {In this paper we consider the problem of inferring link-level loss rates from end-to-end multicast measurements taken from a collection of trees. We give conditions under which loss rates are identifiable on a specified set of links. Two algorithms are presented to perform the link-level inferences for those links on which losses can be identified. One, the minimum variance weighted average (MVWA) algorithm</i> treats the trees separately and then averages the results. The second, based on expectation-maximization (EM)</i> merges all of the measurements into one computation. Simulations show that EM is slightly more accurate than MVWA, most likely due to its more efficient use of the measurements. We also describe extensions to the inference of link-level delay, inference from end-to-end unicast measurements, and inference when some measurements are missing.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/511399.511338},
 doi = {http://doi.acm.org/10.1145/511399.511338},
 acmid = {511338},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jiang:2002:LEL:511399.511340,
 author = {Jiang, Song and Zhang, Xiaodong},
 title = {LIRS: an efficient low inter-reference recency set replacement policy to improve buffer cache performance},
 abstract = {Although LRU replacement policy has been commonly used in the buffer cache management, it is well known for its inability to cope with access patterns with weak locality. Previous work, such as LRU-K and 2Q, attempts to enhance LRU capacity by making use of additional history information of previous block references other than only the recency information used in LRU. These algorithms greatly increase complexity and/or can not consistently provide performance improvement. Many recently proposed policies, such as UBM and SEQ, improve replacement performance by exploiting access regularities in references. They only address LRU problems on certain specific and well-defined cases such as access patterns like sequences and loops. Motivated by the limits of previous studies, we propose an efficient buffer cache replacement policy, called Low Inter-reference Recency Set</i> (LIRS). LIRS effectively addresses the limits of LRU by using recency to evaluate Inter-Reference Recency (IRR) for making a replacement decision. This is in contrast to what LRU does: directly using recency to predict next reference timing. At the same time, LIRS almost retains the same simple assumption of LRU to predict future access behavior of blocks. Our objectives are to effectively address the limits of LRU for a general purpose, to retain the low overhead merit of LRU, and to outperform those replacement policies relying on the access regularity detections. Conducting simulations with a variety of traces and a wide range of cache sizes, we show that LIRS significantly outperforms LRU, and outperforms other existing replacement algorithms in most cases. Furthermore, we show that the additional cost for implementing LIRS is trivial in comparison with LRU.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {31--42},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511340},
 doi = {http://doi.acm.org/10.1145/511399.511340},
 acmid = {511340},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jiang:2002:LEL:511334.511340,
 author = {Jiang, Song and Zhang, Xiaodong},
 title = {LIRS: an efficient low inter-reference recency set replacement policy to improve buffer cache performance},
 abstract = {Although LRU replacement policy has been commonly used in the buffer cache management, it is well known for its inability to cope with access patterns with weak locality. Previous work, such as LRU-K and 2Q, attempts to enhance LRU capacity by making use of additional history information of previous block references other than only the recency information used in LRU. These algorithms greatly increase complexity and/or can not consistently provide performance improvement. Many recently proposed policies, such as UBM and SEQ, improve replacement performance by exploiting access regularities in references. They only address LRU problems on certain specific and well-defined cases such as access patterns like sequences and loops. Motivated by the limits of previous studies, we propose an efficient buffer cache replacement policy, called Low Inter-reference Recency Set</i> (LIRS). LIRS effectively addresses the limits of LRU by using recency to evaluate Inter-Reference Recency (IRR) for making a replacement decision. This is in contrast to what LRU does: directly using recency to predict next reference timing. At the same time, LIRS almost retains the same simple assumption of LRU to predict future access behavior of blocks. Our objectives are to effectively address the limits of LRU for a general purpose, to retain the low overhead merit of LRU, and to outperform those replacement policies relying on the access regularity detections. Conducting simulations with a variety of traces and a wide range of cache sizes, we show that LIRS significantly outperforms LRU, and outperforms other existing replacement algorithms in most cases. Furthermore, we show that the additional cost for implementing LIRS is trivial in comparison with LRU.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {31--42},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511340},
 doi = {http://doi.acm.org/10.1145/511334.511340},
 acmid = {511340},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Squillante:2002:MAD:511399.511341,
 author = {Squillante, Mark S. and Zhang, Yanyong and Sivasubramaniam, Anand and Gautam, Natarajan and Franke, Hubertus and Moreira, Jose},
 title = {Modeling and analysis of dynamic coscheduling in parallel and distributed environments},
 abstract = {Scheduling in large-scale parallel systems has been and continues to be an important and challenging research problem. Several key factors, including the increasing use of off-the-shelf clusters of workstations to build such parallel systems, have resulted in the emergence of a new class of scheduling strategies, broadly referred to as dynamic coscheduling. Unfortunately, the size of both the design and performance spaces of these emerging scheduling strategies is quite large, due in part to the numerous dynamic interactions among the different components of the parallel computing environment as well as the wide range of applications and systems that can comprise the parallel environment. This in turn makes it difficult to fully explore the benefits and limitations of the various proposed dynamic coscheduling approaches for large-scale systems solely with the use of simulation and/or experimentation.To gain a better understanding of the fundamental properties of different dynamic coscheduling methods, we formulate a general mathematical model of this class of scheduling strategies within a unified framework that allows us to investigate a wide range of parallel environments. We derive a matrix-analytic analysis based on a stochastic decomposition and a fixed-point iteration. A large number of numerical experiments are performed in part to examine the accuracy of our approach. These numerical results are in excellent agreement with detailed simulation results. Our mathematical model and analysis is then used to explore several fundamental design and performance tradeoffs associated with the class of dynamic coscheduling policies across a broad spectrum of parallel computing environments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {43--54},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511341},
 doi = {http://doi.acm.org/10.1145/511399.511341},
 acmid = {511341},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Squillante:2002:MAD:511334.511341,
 author = {Squillante, Mark S. and Zhang, Yanyong and Sivasubramaniam, Anand and Gautam, Natarajan and Franke, Hubertus and Moreira, Jose},
 title = {Modeling and analysis of dynamic coscheduling in parallel and distributed environments},
 abstract = {Scheduling in large-scale parallel systems has been and continues to be an important and challenging research problem. Several key factors, including the increasing use of off-the-shelf clusters of workstations to build such parallel systems, have resulted in the emergence of a new class of scheduling strategies, broadly referred to as dynamic coscheduling. Unfortunately, the size of both the design and performance spaces of these emerging scheduling strategies is quite large, due in part to the numerous dynamic interactions among the different components of the parallel computing environment as well as the wide range of applications and systems that can comprise the parallel environment. This in turn makes it difficult to fully explore the benefits and limitations of the various proposed dynamic coscheduling approaches for large-scale systems solely with the use of simulation and/or experimentation.To gain a better understanding of the fundamental properties of different dynamic coscheduling methods, we formulate a general mathematical model of this class of scheduling strategies within a unified framework that allows us to investigate a wide range of parallel environments. We derive a matrix-analytic analysis based on a stochastic decomposition and a fixed-point iteration. A large number of numerical experiments are performed in part to examine the accuracy of our approach. These numerical results are in excellent agreement with detailed simulation results. Our mathematical model and analysis is then used to explore several fundamental design and performance tradeoffs associated with the class of dynamic coscheduling policies across a broad spectrum of parallel computing environments.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {43--54},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511341},
 doi = {http://doi.acm.org/10.1145/511334.511341},
 acmid = {511341},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bachmat:2002:AMS:511334.511342,
 author = {Bachmat, Eitan and Schindler, Jiri},
 title = {Analysis of methods for scheduling low priority disk drive tasks},
 abstract = {This paper analyzes various algorithms for scheduling low priority disk drive tasks. The derived closed form solution is applicable to class of greedy algorithms that include a variety of background disk scanning applications. By paying close attention to many characteristics of modern disk drives, the analytical solutions achieve very high accuracy---the difference between the predicted response times and the measurements on two different disks is only 3\% for all but one examined workload. This paper also proves a theorem which shows that background tasks implemented by greedy algorithms can be accomplished with very little seek penalty. Using greedy algorithm gives a 10\% shorter response time for the foreground application requests and up to a 20\% decrease in total background task run time compared to results from previously published techniques.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {55--65},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511342},
 doi = {http://doi.acm.org/10.1145/511334.511342},
 acmid = {511342},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bachmat:2002:AMS:511399.511342,
 author = {Bachmat, Eitan and Schindler, Jiri},
 title = {Analysis of methods for scheduling low priority disk drive tasks},
 abstract = {This paper analyzes various algorithms for scheduling low priority disk drive tasks. The derived closed form solution is applicable to class of greedy algorithms that include a variety of background disk scanning applications. By paying close attention to many characteristics of modern disk drives, the analytical solutions achieve very high accuracy---the difference between the predicted response times and the measurements on two different disks is only 3\% for all but one examined workload. This paper also proves a theorem which shows that background tasks implemented by greedy algorithms can be accomplished with very little seek penalty. Using greedy algorithm gives a 10\% shorter response time for the foreground application requests and up to a 20\% decrease in total background task run time compared to results from previously published techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {55--65},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511342},
 doi = {http://doi.acm.org/10.1145/511399.511342},
 acmid = {511342},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Snavely:2002:SJP:511399.511343,
 author = {Snavely, Allan and Tullsen, Dean M. and Voelker, Geoff},
 title = {Symbiotic jobscheduling with priorities for a simultaneous multithreading processor},
 abstract = {Simultaneous Multithreading machines benefit from jobscheduling software that monitors how well coscheduled jobs share CPU resources, and coschedules jobs that interact well to make more efficient use of those resources. As a result, informed coscheduling can yield significant performance gains over naive schedulers. However, prior work on coscheduling focused on equal-priority job mixes, which is an unrealistic assumption for modern operating systems.This paper demonstrates that a scheduler for an SMT machine can both satisfy process priorities and symbiotically schedule low and high priority threads to increase system throughput. Naive priority schedulers dedicate the machine to high priority jobs to meet priority goals, and as a result decrease opportunities for increased performance from multithreading and coscheduling. More informed schedulers, however, can dynamically monitor the progress and resource utilization of jobs on the machine, and dynamically adjust the degree of multithreading to improve performance while still meeting priority goals.Using detailed simulation of an SMT architecture, we introduce and evaluate a series of five software and hardware-assisted priority schedulers. Overall, our results indicate that coscheduling priority jobs can significantly increase system throughput by as much as 40\%, and that (1) the benefit depends upon the relative priority of the coscheduled jobs, and (2) more sophisticated schedulers are more effective when the differences in priorities are greatest. We show that our priority schedulers can decrease average turnaround times for a random jobmix by as much as 33\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {66--76},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511343},
 doi = {http://doi.acm.org/10.1145/511399.511343},
 acmid = {511343},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {job scheduling, priorities, simultaneous multithreading},
} 

@inproceedings{Snavely:2002:SJP:511334.511343,
 author = {Snavely, Allan and Tullsen, Dean M. and Voelker, Geoff},
 title = {Symbiotic jobscheduling with priorities for a simultaneous multithreading processor},
 abstract = {Simultaneous Multithreading machines benefit from jobscheduling software that monitors how well coscheduled jobs share CPU resources, and coschedules jobs that interact well to make more efficient use of those resources. As a result, informed coscheduling can yield significant performance gains over naive schedulers. However, prior work on coscheduling focused on equal-priority job mixes, which is an unrealistic assumption for modern operating systems.This paper demonstrates that a scheduler for an SMT machine can both satisfy process priorities and symbiotically schedule low and high priority threads to increase system throughput. Naive priority schedulers dedicate the machine to high priority jobs to meet priority goals, and as a result decrease opportunities for increased performance from multithreading and coscheduling. More informed schedulers, however, can dynamically monitor the progress and resource utilization of jobs on the machine, and dynamically adjust the degree of multithreading to improve performance while still meeting priority goals.Using detailed simulation of an SMT architecture, we introduce and evaluate a series of five software and hardware-assisted priority schedulers. Overall, our results indicate that coscheduling priority jobs can significantly increase system throughput by as much as 40\%, and that (1) the benefit depends upon the relative priority of the coscheduled jobs, and (2) more sophisticated schedulers are more effective when the differences in priorities are greatest. We show that our priority schedulers can decrease average turnaround times for a random jobmix by as much as 33\%.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {66--76},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511343},
 doi = {http://doi.acm.org/10.1145/511334.511343},
 acmid = {511343},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {job scheduling, priorities, simultaneous multithreading},
} 

@inproceedings{Harrison:2002:PTD:511334.511345,
 author = {Harrison, Peter G. and Knottenbelt, William J.},
 title = {Passage time distributions in large Markov chains},
 abstract = {Probability distributions of response times are important in the design and analysis of transaction processing systems and computer-communication systems. We present a general technique for deriving such distributions from high-level modelling formalisms whose state spaces can be mapped onto finite Markov chains. We use a load-balanced, distributed implementation to find the Laplace transform of the first passage time density and its derivatives at arbitrary values of the transform parameter s</i>. Setting s</i> = 0 yields moments while the full passage time distribution is obtained using a novel distributed Laplace transform inverter based on the Laguerre method. We validate our method against a variety of simple densities, cycle time densities in certain overtake-free (tree-like) queueing networks and a simulated Petri net model. Our implementation is thereby rigorously validated and has already been applied to substantial Markov chains with over 1 million states. Corresponding theoretical results for semi-Markov chains are also presented.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {77--85},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/511334.511345},
 doi = {http://doi.acm.org/10.1145/511334.511345},
 acmid = {511345},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harrison:2002:PTD:511399.511345,
 author = {Harrison, Peter G. and Knottenbelt, William J.},
 title = {Passage time distributions in large Markov chains},
 abstract = {Probability distributions of response times are important in the design and analysis of transaction processing systems and computer-communication systems. We present a general technique for deriving such distributions from high-level modelling formalisms whose state spaces can be mapped onto finite Markov chains. We use a load-balanced, distributed implementation to find the Laplace transform of the first passage time density and its derivatives at arbitrary values of the transform parameter s</i>. Setting s</i> = 0 yields moments while the full passage time distribution is obtained using a novel distributed Laplace transform inverter based on the Laguerre method. We validate our method against a variety of simple densities, cycle time densities in certain overtake-free (tree-like) queueing networks and a simulated Petri net model. Our implementation is thereby rigorously validated and has already been applied to substantial Markov chains with over 1 million states. Corresponding theoretical results for semi-Markov chains are also presented.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {77--85},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/511399.511345},
 doi = {http://doi.acm.org/10.1145/511399.511345},
 acmid = {511345},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Riska:2002:EAS:511399.511346,
 author = {Riska, Alma and Smirni, Evgenia},
 title = {Exact aggregate solutions for M/G/1-type Markov processes},
 abstract = {We introduce a new methodology for the exact analysis of M/G/1-type Markov processes. The methodology uses basic, well-known results for Markov chains by exploiting the structure of the repetitive portion of the chain and recasting the overall problem into the computation of the solution of a finite linear system. The methodology allows for the calculation of the aggregate probability of a finite set of classes of states from the state space, appropriately defined. Further, it allows for the computation of a set of measures of interest such as the system queue length or any of its higher moments. The proposed methodology is exact. Detailed experiments illustrate that the methodology is also numerically stable, and in many cases can yield significantly less expensive solutions when compared with other methods, as shown by detailed time and space complexity analysis.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {86--96},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511346},
 doi = {http://doi.acm.org/10.1145/511399.511346},
 acmid = {511346},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1-type processes, Markov chains, aggregation, matrix analytic method},
} 

@inproceedings{Riska:2002:EAS:511334.511346,
 author = {Riska, Alma and Smirni, Evgenia},
 title = {Exact aggregate solutions for M/G/1-type Markov processes},
 abstract = {We introduce a new methodology for the exact analysis of M/G/1-type Markov processes. The methodology uses basic, well-known results for Markov chains by exploiting the structure of the repetitive portion of the chain and recasting the overall problem into the computation of the solution of a finite linear system. The methodology allows for the calculation of the aggregate probability of a finite set of classes of states from the state space, appropriately defined. Further, it allows for the computation of a set of measures of interest such as the system queue length or any of its higher moments. The proposed methodology is exact. Detailed experiments illustrate that the methodology is also numerically stable, and in many cases can yield significantly less expensive solutions when compared with other methods, as shown by detailed time and space complexity analysis.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {86--96},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511346},
 doi = {http://doi.acm.org/10.1145/511334.511346},
 acmid = {511346},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {M/G/1-type processes, Markov chains, aggregation, matrix analytic method},
} 

@article{Jin:2002:SMD:511399.511347,
 author = {Jin, Shudong and Bestavros, Azer},
 title = {Scalability of multicast delivery for non-sequential streaming access},
 abstract = {To serve asynchronous requests using multicast, two categories of techniques---stream merging and periodic broadcasting---have been proposed. For sequential streaming access, where requests are uninterrupted from the beginning to the end of an object, these techniques are highly scalable: the required server bandwidth for stream merging grows logarithmically</i> as request arrival rate, and the required server bandwidth for periodic broadcasting varies logarithmically</i> as the inverse of start-up delay. A sequential access model, however, is inappropriate to model partial requests and client interactivity observed in various streaming access workloads. This paper analytically and experimentally studies the scalability of multicast delivery under a non-sequential access model where requests start at random points in the object. We show that the required server bandwidth for any protocol providing immediate service grows at least as the square root</i> of request arrival rate, and the required server bandwidth for any protocol providing delayed service grows linearly</i> with the inverse of start-up delay. We also investigate the impact of limited client receiving bandwidth on scalability. We optimize practical protocols which provide immediate service to non-sequential requests. The protocols utilize limited client receiving bandwidth, and they are near-optimal in that the required server bandwidth is very close to its lower bound.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {97--107},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511347},
 doi = {http://doi.acm.org/10.1145/511399.511347},
 acmid = {511347},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jin:2002:SMD:511334.511347,
 author = {Jin, Shudong and Bestavros, Azer},
 title = {Scalability of multicast delivery for non-sequential streaming access},
 abstract = {To serve asynchronous requests using multicast, two categories of techniques---stream merging and periodic broadcasting---have been proposed. For sequential streaming access, where requests are uninterrupted from the beginning to the end of an object, these techniques are highly scalable: the required server bandwidth for stream merging grows logarithmically</i> as request arrival rate, and the required server bandwidth for periodic broadcasting varies logarithmically</i> as the inverse of start-up delay. A sequential access model, however, is inappropriate to model partial requests and client interactivity observed in various streaming access workloads. This paper analytically and experimentally studies the scalability of multicast delivery under a non-sequential access model where requests start at random points in the object. We show that the required server bandwidth for any protocol providing immediate service grows at least as the square root</i> of request arrival rate, and the required server bandwidth for any protocol providing delayed service grows linearly</i> with the inverse of start-up delay. We also investigate the impact of limited client receiving bandwidth on scalability. We optimize practical protocols which provide immediate service to non-sequential requests. The protocols utilize limited client receiving bandwidth, and they are near-optimal in that the required server bandwidth is very close to its lower bound.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {97--107},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511347},
 doi = {http://doi.acm.org/10.1145/511334.511347},
 acmid = {511347},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mauer:2002:FTS:511399.511349,
 author = {Mauer, Carl J. and Hill, Mark D. and Wood, David A.},
 title = {Full-system timing-first simulation},
 abstract = {Computer system designers often evaluate future design alternatives with detailed simulators that strive for functional fidelity</i> (to execute relevant workloads) and performance fidelity</i> (to rank design alternatives). Trends toward multi-threaded architectures, more complex micro-architectures, and richer workloads, make authoring detailed simulators increasingly difficult. To manage simulator complexity, this paper advocates decoupled simulator organizations that separate functional and performance concerns. Furthermore, we define an approach, called timing-first simulation,</i> that uses an augmented timing simulator to execute instructions important to performance in conjunction with a functional simulator to insure correctness. This design simplifies software development, leverages existing simulators, and can model micro-architecture timing in detail.We describe the timing-first organization and our experiences implementing TFsim, a full-system multiprocessor performance simulator. TFsim models a pipelined, out-of-order micro-architecture in detail, was developed in less than one person-year, and performs competitively with previously-published simulators. TFsim's timing simulator implements dynamically common instructions (99.99\% of them), while avoiding the vast and exacting implementation efforts necessary to run unmodified commercial operating systems and workloads. Virtutech Simics, a full-system functional simulator, checks and corrects the timing simulator's execution, contributing 18-36\% to the overall run-time. TFsim's mostly correct functional implementation introduces a worst-case performance error of 4.8\% for our commercial workloads. Some additional simulator performance is gained by verifying functional correctness less often, at the cost of some additional performance error.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {108--116},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/511399.511349},
 doi = {http://doi.acm.org/10.1145/511399.511349},
 acmid = {511349},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mauer:2002:FTS:511334.511349,
 author = {Mauer, Carl J. and Hill, Mark D. and Wood, David A.},
 title = {Full-system timing-first simulation},
 abstract = {Computer system designers often evaluate future design alternatives with detailed simulators that strive for functional fidelity</i> (to execute relevant workloads) and performance fidelity</i> (to rank design alternatives). Trends toward multi-threaded architectures, more complex micro-architectures, and richer workloads, make authoring detailed simulators increasingly difficult. To manage simulator complexity, this paper advocates decoupled simulator organizations that separate functional and performance concerns. Furthermore, we define an approach, called timing-first simulation,</i> that uses an augmented timing simulator to execute instructions important to performance in conjunction with a functional simulator to insure correctness. This design simplifies software development, leverages existing simulators, and can model micro-architecture timing in detail.We describe the timing-first organization and our experiences implementing TFsim, a full-system multiprocessor performance simulator. TFsim models a pipelined, out-of-order micro-architecture in detail, was developed in less than one person-year, and performs competitively with previously-published simulators. TFsim's timing simulator implements dynamically common instructions (99.99\% of them), while avoiding the vast and exacting implementation efforts necessary to run unmodified commercial operating systems and workloads. Virtutech Simics, a full-system functional simulator, checks and corrects the timing simulator's execution, contributing 18-36\% to the overall run-time. TFsim's mostly correct functional implementation introduces a worst-case performance error of 4.8\% for our commercial workloads. Some additional simulator performance is gained by verifying functional correctness less often, at the cost of some additional performance error.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {108--116},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/511334.511349},
 doi = {http://doi.acm.org/10.1145/511334.511349},
 acmid = {511349},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jin:2002:PPR:511399.511350,
 author = {Jin, Ruoming and Agrawal, Gagan},
 title = {Performance prediction for random write reductions: a case study in modeling shared memory programs},
 abstract = {In this paper, we revisit the problem of performance prediction on shared memory parallel machines, motivated by the need for selecting parallelization strategy for random write reductions.</i> Such reductions frequently arise in data mining algorithms.In our previous work, we have developed a number of techniques for parallelizing this class of reductions. Our previous work has shown that each of the three techniques, full replication, optimized full locking,</i> and cache-sensitive,</i> can outperform others depending upon problem, dataset, and machine parameters. Therefore, an important question is, "Can we predict the performance of these techniques for a given problem, dataset, and machine?".</i>This paper addresses this question by developing an analytical performance model that captures a two-level cache, coherence cache misses, TLB misses, locking overheads, and contention for memory. Analytical model is combined with results from micro-benchmarking to predict performance on real machines. We have validated our model on two different SMP machines. Our results show that our model effectively captures the impact of memory hierarchy (two-level cache and TLB) as well as the factors that limit parallelism (contention for locks, memory contention, and coherence cache misses). The difference between predicted and measured performance is within 20\% in almost all cases. Moreover, the model is quite accurate in predicting the relative performance of the three parallelization techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511350},
 doi = {http://doi.acm.org/10.1145/511399.511350},
 acmid = {511350},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jin:2002:PPR:511334.511350,
 author = {Jin, Ruoming and Agrawal, Gagan},
 title = {Performance prediction for random write reductions: a case study in modeling shared memory programs},
 abstract = {In this paper, we revisit the problem of performance prediction on shared memory parallel machines, motivated by the need for selecting parallelization strategy for random write reductions.</i> Such reductions frequently arise in data mining algorithms.In our previous work, we have developed a number of techniques for parallelizing this class of reductions. Our previous work has shown that each of the three techniques, full replication, optimized full locking,</i> and cache-sensitive,</i> can outperform others depending upon problem, dataset, and machine parameters. Therefore, an important question is, "Can we predict the performance of these techniques for a given problem, dataset, and machine?".</i>This paper addresses this question by developing an analytical performance model that captures a two-level cache, coherence cache misses, TLB misses, locking overheads, and contention for memory. Analytical model is combined with results from micro-benchmarking to predict performance on real machines. We have validated our model on two different SMP machines. Our results show that our model effectively captures the impact of memory hierarchy (two-level cache and TLB) as well as the factors that limit parallelism (contention for locks, memory contention, and coherence cache misses). The difference between predicted and measured performance is within 20\% in almost all cases. Moreover, the model is quite accurate in predicting the relative performance of the three parallelization techniques.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {117--128},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511350},
 doi = {http://doi.acm.org/10.1145/511334.511350},
 acmid = {511350},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kandiraju:2002:CDT:511334.511351,
 author = {Kandiraju, Gokul B. and Sivasubramaniam, Anand},
 title = {Characterizing the <i>d</i>-TLB behavior of SPEC CPU2000 benchmarks},
 abstract = {Despite the numerous optimization and evaluation studies that have been conducted with TLBs over the years, there is still a deficiency in an indepth understanding of TLB characteristics from an application angle. This paper presents a detailed characterization study of the TLB behavior of the SPEC CPU2000 benchmark suite. The contributions of this work are in identifying important application characteristics for TLB studies, quantifying the SPEC2000 application behavior for these characteristics, as well as making pronouncements and suggestions for future research based on these results.Around one-fourth of the SPEC2000 applications (ammp, apsi, galgel, lucas, mcf, twolf and vpr) have significant TLB missrates. Both capacity and associativity are influencing factors on miss-rates, though they do not necessarily go hand-in-hand. Multi-level TLBs are definitely useful for these applications in cutting down access times without significant miss rate degradation. Superpaging to combine TLB entries may not be rewarding for many of these applications. Software management of TLBs in terms of determining what entries to prefetch, what entries to replace, and what entries to pin has a lot of potential to cut down miss rates considerably. Specifically, the potential benefits of prefetching TLB entries is examined, and Distance Prefetching is shown to give good prediction accuracy for these applications.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {129--139},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511351},
 doi = {http://doi.acm.org/10.1145/511334.511351},
 acmid = {511351},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kandiraju:2002:CDT:511399.511351,
 author = {Kandiraju, Gokul B. and Sivasubramaniam, Anand},
 title = {Characterizing the <i>d</i>-TLB behavior of SPEC CPU2000 benchmarks},
 abstract = {Despite the numerous optimization and evaluation studies that have been conducted with TLBs over the years, there is still a deficiency in an indepth understanding of TLB characteristics from an application angle. This paper presents a detailed characterization study of the TLB behavior of the SPEC CPU2000 benchmark suite. The contributions of this work are in identifying important application characteristics for TLB studies, quantifying the SPEC2000 application behavior for these characteristics, as well as making pronouncements and suggestions for future research based on these results.Around one-fourth of the SPEC2000 applications (ammp, apsi, galgel, lucas, mcf, twolf and vpr) have significant TLB missrates. Both capacity and associativity are influencing factors on miss-rates, though they do not necessarily go hand-in-hand. Multi-level TLBs are definitely useful for these applications in cutting down access times without significant miss rate degradation. Superpaging to combine TLB entries may not be rewarding for many of these applications. Software management of TLBs in terms of determining what entries to prefetch, what entries to replace, and what entries to pin has a lot of potential to cut down miss rates considerably. Specifically, the potential benefits of prefetching TLB entries is examined, and Distance Prefetching is shown to give good prediction accuracy for these applications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {129--139},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511351},
 doi = {http://doi.acm.org/10.1145/511399.511351},
 acmid = {511351},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hertz:2002:EGC:511399.511352,
 author = {Hertz, Matthew and Blackburn, Stephen M and Moss, J Eliot B and McKinley, Kathryn S. and Stefanovi\'{c}, Darko},
 title = {Error-free garbage collection traces: how to cheat and not get caught},
 abstract = {Programmers are writing a large and rapidly growing number of programs in object-oriented languages such as Java that require garbage collection (GC). To explore the design and evaluation of GC algorithms quickly, researchers are using simulation based on traces of object allocation and lifetime behavior. The brute force</i> method generates perfect traces using a whole-heap GC at every potential GC point in the program. Because this process is prohibitively expensive, researchers often use granulated</i> traces by collecting only periodically, e.g., every 32K bytes of allocation.We extend the state of the art for simulating GC algorithms in two ways. First, we present a systematic methodology and results on the effects of trace granularity for a variety of copying GC algorithms. We show that trace granularity often distorts GC performance results compared with perfect traces, and that some GC algorithms are more sensitive to this effect than others. Second, we introduce and measure the performance of a new precise algorithm for generating GC traces which is over 800 times faster than the brute force method. Our algorithm, called Merlin, frequently timestamps objects and later uses the timestamps of dead objects to reconstruct precisely when they died. It performs only periodic garbage collections and achieves high accuracy at low cost, eliminating any reason to use granulated traces.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {140--151},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511352},
 doi = {http://doi.acm.org/10.1145/511399.511352},
 acmid = {511352},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hertz:2002:EGC:511334.511352,
 author = {Hertz, Matthew and Blackburn, Stephen M and Moss, J Eliot B and McKinley, Kathryn S. and Stefanovi\'{c}, Darko},
 title = {Error-free garbage collection traces: how to cheat and not get caught},
 abstract = {Programmers are writing a large and rapidly growing number of programs in object-oriented languages such as Java that require garbage collection (GC). To explore the design and evaluation of GC algorithms quickly, researchers are using simulation based on traces of object allocation and lifetime behavior. The brute force</i> method generates perfect traces using a whole-heap GC at every potential GC point in the program. Because this process is prohibitively expensive, researchers often use granulated</i> traces by collecting only periodically, e.g., every 32K bytes of allocation.We extend the state of the art for simulating GC algorithms in two ways. First, we present a systematic methodology and results on the effects of trace granularity for a variety of copying GC algorithms. We show that trace granularity often distorts GC performance results compared with perfect traces, and that some GC algorithms are more sensitive to this effect than others. Second, we introduce and measure the performance of a new precise algorithm for generating GC traces which is over 800 times faster than the brute force method. Our algorithm, called Merlin, frequently timestamps objects and later uses the timestamps of dead objects to reconstruct precisely when they died. It performs only periodic garbage collections and achieves high accuracy at low cost, eliminating any reason to use granulated traces.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {140--151},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511352},
 doi = {http://doi.acm.org/10.1145/511334.511352},
 acmid = {511352},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cameron:2002:HMS:511334.511354,
 author = {Cameron, Craig W. and Low, Steven H. and Wei, David X.},
 title = {High-density model for server allocation and placement},
 abstract = {It is well known that optimal server placement is NP-hard. We present an approximate model for the case when both clients and servers are dense, and propose a simple server allocation and placement algorithm based on high-rate vector quantization theory. The key idea is to regard the location of a request as a random variable with probability density that is proportional to the demand at that location, and the problem of server placement as source coding, i.e., to optimally map a source value (request location) to a code-word (server location) to minimize distortion (network cost). This view has led to a joint server allocation and placement algorithm that has a time-complexity that is linear in the number of clients. Simulations are presented to illustrate its performance.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {152--159},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/511334.511354},
 doi = {http://doi.acm.org/10.1145/511334.511354},
 acmid = {511354},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, high density, server placement and allocation},
} 

@article{Cameron:2002:HMS:511399.511354,
 author = {Cameron, Craig W. and Low, Steven H. and Wei, David X.},
 title = {High-density model for server allocation and placement},
 abstract = {It is well known that optimal server placement is NP-hard. We present an approximate model for the case when both clients and servers are dense, and propose a simple server allocation and placement algorithm based on high-rate vector quantization theory. The key idea is to regard the location of a request as a random variable with probability density that is proportional to the demand at that location, and the problem of server placement as source coding, i.e., to optimally map a source value (request location) to a code-word (server location) to minimize distortion (network cost). This view has led to a joint server allocation and placement algorithm that has a time-complexity that is linear in the number of clients. Simulations are presented to illustrate its performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {152--159},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/511399.511354},
 doi = {http://doi.acm.org/10.1145/511399.511354},
 acmid = {511354},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content distribution, high density, server placement and allocation},
} 

@article{Olshefski:2002:ICR:511399.511355,
 author = {Olshefski, David P. and Nieh, Jason and Agrawal, Dakshi},
 title = {Inferring client response time at the web server},
 abstract = {As businesses continue to grow their World Wide Web presence, it is becoming increasingly vital for them to have quantitative measures of the client perceived response times of their web services. We present Certes (CliEnt Response Time Estimated by the Server), an online server-based mechanism for web servers to measure client perceived response time, as if measured at the client. Certes is based on a model of TCP that quantifies the effect that connection drops have on perceived client response time, by using three simple server-side measurements: connection drop rate, connection accept rate and connection completion rate. The mechanism does not require modifications to http servers or web pages, does not rely on probing or third party sampling, and does not require client-side modifications or scripting. Certes can be used to measure response times for any web content, not just HTML. We have implemented Certes and compared its response time measurements with those obtained with detailed client instrumentation. Our results demonstrate that Certes provides accurate server-based measurements of client response times in HTTP 1.0/1.1 [14] environments, even with rapidly changing workloads. Certes runs online in constant time with very low overhead. It can be used at web sites and server farms to verify compliance with service level objectives.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {160--171},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511355},
 doi = {http://doi.acm.org/10.1145/511399.511355},
 acmid = {511355},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {client perceived response time, web server},
} 

@inproceedings{Olshefski:2002:ICR:511334.511355,
 author = {Olshefski, David P. and Nieh, Jason and Agrawal, Dakshi},
 title = {Inferring client response time at the web server},
 abstract = {As businesses continue to grow their World Wide Web presence, it is becoming increasingly vital for them to have quantitative measures of the client perceived response times of their web services. We present Certes (CliEnt Response Time Estimated by the Server), an online server-based mechanism for web servers to measure client perceived response time, as if measured at the client. Certes is based on a model of TCP that quantifies the effect that connection drops have on perceived client response time, by using three simple server-side measurements: connection drop rate, connection accept rate and connection completion rate. The mechanism does not require modifications to http servers or web pages, does not rely on probing or third party sampling, and does not require client-side modifications or scripting. Certes can be used to measure response times for any web content, not just HTML. We have implemented Certes and compared its response time measurements with those obtained with detailed client instrumentation. Our results demonstrate that Certes provides accurate server-based measurements of client response times in HTTP 1.0/1.1 [14] environments, even with rapidly changing workloads. Certes runs online in constant time with very low overhead. It can be used at web sites and server farms to verify compliance with service level objectives.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {160--171},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511355},
 doi = {http://doi.acm.org/10.1145/511334.511355},
 acmid = {511355},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {client perceived response time, web server},
} 

@article{Lee:2002:ACD:511399.511356,
 author = {Lee, Sam C. M. and Lui, John C. S. and Yau, David K. Y.},
 title = {Admission control and dynamic adaptation for a proportional-delay diffserv-enabled web server},
 abstract = {We consider a web server that can provide differentiated services to clients with different QoS requirements. The web server can provide N</i> \&gt; 1 classes of service. Rather than using a strict priority policy , which may lead to request starvation, the web server provides a proportional-delay differentiated service (PDDS) to heterogeneous clients. An operator for the web server can specify "fixed" performance spacings between classes, namely, r<inf>i,i+1</inf></i> \&gt; 1, for i</i> = 1,\&hellip;,N</i> - 1. Requests in class i</i> + 1 are guaranteed to have an average waiting time which is 1/r<inf>i,i+1</inf></i> of the average waiting time of class i</i> requests. With PDDS, we can provide consistent performance spacings over a wide range of system loadings. In addition, each client can specify a maximum average waiting time requirement to be guaranteed by the web server. We propose two efficient admission control algorithms so that a web server can provide the QoS guarantees and, at the same time, classify each client to its "lowest" admissible class, resulting in lowest usage cost for the client. We also consider how to perform end-point dynamic adaptation such that clients can submit requests at lower class and further reduce their usage cost, without violating their QoS requirements. We propose two dynamic adaptation algorithms: one is server-based and the other is client-based. The client-based adaptation is based on a non-cooperative game technique. We report diverse experimental results to illustrate the effectiveness of these algorithms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {172--182},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511356},
 doi = {http://doi.acm.org/10.1145/511399.511356},
 acmid = {511356},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:2002:ACD:511334.511356,
 author = {Lee, Sam C. M. and Lui, John C. S. and Yau, David K. Y.},
 title = {Admission control and dynamic adaptation for a proportional-delay diffserv-enabled web server},
 abstract = {We consider a web server that can provide differentiated services to clients with different QoS requirements. The web server can provide N</i> \&gt; 1 classes of service. Rather than using a strict priority policy , which may lead to request starvation, the web server provides a proportional-delay differentiated service (PDDS) to heterogeneous clients. An operator for the web server can specify "fixed" performance spacings between classes, namely, r<inf>i,i+1</inf></i> \&gt; 1, for i</i> = 1,\&hellip;,N</i> - 1. Requests in class i</i> + 1 are guaranteed to have an average waiting time which is 1/r<inf>i,i+1</inf></i> of the average waiting time of class i</i> requests. With PDDS, we can provide consistent performance spacings over a wide range of system loadings. In addition, each client can specify a maximum average waiting time requirement to be guaranteed by the web server. We propose two efficient admission control algorithms so that a web server can provide the QoS guarantees and, at the same time, classify each client to its "lowest" admissible class, resulting in lowest usage cost for the client. We also consider how to perform end-point dynamic adaptation such that clients can submit requests at lower class and further reduce their usage cost, without violating their QoS requirements. We propose two dynamic adaptation algorithms: one is server-based and the other is client-based. The client-based adaptation is based on a non-cooperative game technique. We report diverse experimental results to illustrate the effectiveness of these algorithms.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {172--182},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511356},
 doi = {http://doi.acm.org/10.1145/511334.511356},
 acmid = {511356},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tan:2002:QSE:511334.511358,
 author = {Tan, Haonan and Eager, Derek L. and Vernon, Mary K. and Guo, Hongfei},
 title = {Quality of service evaluations of multicast streaming protocols},
 abstract = {Recently proposed scalable on-demand streaming protocols have previously been evaluated using a system cost measure termed the "required server bandwidth". For the scalable protocols that provide immediate service to each client when the server is not overloaded, this paper develops simple analytic models to evaluate two client-oriented quality of service metrics, namely (1) the mean client waiting time in systems where clients are willing to wait if a (well-provisioned) server is temporarily overloaded, and (2) the fraction of clients who balk (i.e., leave without receiving their requested media content) in systems where the clients will tolerate no or only very low service delays during a temporary overload. The models include novel approximate MVA techniques that appear to extend the range of applicability of customized AMVA to include questions focussed on state probabilities rather than on mean values, and to systems in which the operating points of interest do not include substantial client queues. For example, the new AMVA models accurately estimate the server bandwidth needed to achieve a balking rate as low as one in ten thousand. The analytic models can easily be applied to determine the server bandwidth needed for a given number of media files, anticipated total client request rate and file access frequencies, and target balking rate or mean wait. Results show that (a) scalable media servers that are configured with the "required server bandwidth" defined in previous work have low mean wait but may have unacceptably high client balking rates (i.e., greater than one in twenty), (b) for high to moderate client load, only a 10 - 50\% increase in the previously defined required server bandwidth is needed to achieve a very low balking rate (e.g., one in ten thousand), and (c) media server performance (either mean wait or balking rate) degrades rapidly if the actual client load is more than 10\% greater than the anticipated load.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511358},
 doi = {http://doi.acm.org/10.1145/511334.511358},
 acmid = {511358},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tan:2002:QSE:511399.511358,
 author = {Tan, Haonan and Eager, Derek L. and Vernon, Mary K. and Guo, Hongfei},
 title = {Quality of service evaluations of multicast streaming protocols},
 abstract = {Recently proposed scalable on-demand streaming protocols have previously been evaluated using a system cost measure termed the "required server bandwidth". For the scalable protocols that provide immediate service to each client when the server is not overloaded, this paper develops simple analytic models to evaluate two client-oriented quality of service metrics, namely (1) the mean client waiting time in systems where clients are willing to wait if a (well-provisioned) server is temporarily overloaded, and (2) the fraction of clients who balk (i.e., leave without receiving their requested media content) in systems where the clients will tolerate no or only very low service delays during a temporary overload. The models include novel approximate MVA techniques that appear to extend the range of applicability of customized AMVA to include questions focussed on state probabilities rather than on mean values, and to systems in which the operating points of interest do not include substantial client queues. For example, the new AMVA models accurately estimate the server bandwidth needed to achieve a balking rate as low as one in ten thousand. The analytic models can easily be applied to determine the server bandwidth needed for a given number of media files, anticipated total client request rate and file access frequencies, and target balking rate or mean wait. Results show that (a) scalable media servers that are configured with the "required server bandwidth" defined in previous work have low mean wait but may have unacceptably high client balking rates (i.e., greater than one in twenty), (b) for high to moderate client load, only a 10 - 50\% increase in the previously defined required server bandwidth is needed to achieve a very low balking rate (e.g., one in ten thousand), and (c) media server performance (either mean wait or balking rate) degrades rapidly if the actual client load is more than 10\% greater than the anticipated load.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511358},
 doi = {http://doi.acm.org/10.1145/511399.511358},
 acmid = {511358},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Balachandran:2002:CUB:511334.511359,
 author = {Balachandran, Anand and Voelker, Geoffrey M. and Bahl, Paramvir and Rangan, P. Venkat},
 title = {Characterizing user behavior and network performance in a public wireless LAN},
 abstract = {This paper presents and analyzes user behavior and network performance in a public-area wireless network using a workload captured at a well-attended ACM conference. The goals of our study are: (1) to extend our understanding of wireless user behavior and wireless network performance; (2) to characterize wireless users in terms of a parameterized model for use with analytic and simulation studies involving wireless LAN traffic; and (3) to apply our workload analysis results to issues in wireless network deployment, such as capacity planning, and potential network optimizations, such as algorithms for load balancing across multiple access points (APs) in a wireless network.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {195--205},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511359},
 doi = {http://doi.acm.org/10.1145/511334.511359},
 acmid = {511359},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Balachandran:2002:CUB:511399.511359,
 author = {Balachandran, Anand and Voelker, Geoffrey M. and Bahl, Paramvir and Rangan, P. Venkat},
 title = {Characterizing user behavior and network performance in a public wireless LAN},
 abstract = {This paper presents and analyzes user behavior and network performance in a public-area wireless network using a workload captured at a well-attended ACM conference. The goals of our study are: (1) to extend our understanding of wireless user behavior and wireless network performance; (2) to characterize wireless users in terms of a parameterized model for use with analytic and simulation studies involving wireless LAN traffic; and (3) to apply our workload analysis results to issues in wireless network deployment, such as capacity planning, and potential network optimizations, such as algorithms for load balancing across multiple access points (APs) in a wireless network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {195--205},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511359},
 doi = {http://doi.acm.org/10.1145/511399.511359},
 acmid = {511359},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Singh:2002:ECT:511399.511360,
 author = {Singh, Harkirat and Singh, Suresh},
 title = {Energy consumption of TCP Reno, Newreno, and SACK in multi-hop wireless networks},
 abstract = {In this paper we compare the energy consumption behavior of three versions of TCP --- Reno, Newreno, and SACK. The experiments were performed on a wireless testbed where we measured the energy consumed at the sender node. Our results indicate that, in most cases, using total energy consumed as the metric, SACK outperforms Newreno and Reno while Newreno performs better than Reno. The experiments emulated a large set of network conditions including variable round trip times, random loss, bursty loss, and packet reordering. We also estimated the idealized energy for each of the three implementations (i.e., we subtract out the energy consumed when the sender is idle) and here, surprisingly, we find that in many instances SACK performs poorly compared to the other two implementations. We conclude that if the mobile device has a very low idle power consumption then SACK is not the best implementation to use for bursty or random loss. On the other hand, if the idle power consumption is significant, then SACK is the best choice since it has the lowest overall energy consumption.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {206--216},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511360},
 doi = {http://doi.acm.org/10.1145/511399.511360},
 acmid = {511360},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, energy, mobile, wireless},
} 

@inproceedings{Singh:2002:ECT:511334.511360,
 author = {Singh, Harkirat and Singh, Suresh},
 title = {Energy consumption of TCP Reno, Newreno, and SACK in multi-hop wireless networks},
 abstract = {In this paper we compare the energy consumption behavior of three versions of TCP --- Reno, Newreno, and SACK. The experiments were performed on a wireless testbed where we measured the energy consumed at the sender node. Our results indicate that, in most cases, using total energy consumed as the metric, SACK outperforms Newreno and Reno while Newreno performs better than Reno. The experiments emulated a large set of network conditions including variable round trip times, random loss, bursty loss, and packet reordering. We also estimated the idealized energy for each of the three implementations (i.e., we subtract out the energy consumed when the sender is idle) and here, surprisingly, we find that in many instances SACK performs poorly compared to the other two implementations. We conclude that if the mobile device has a very low idle power consumption then SACK is not the best implementation to use for bursty or random loss. On the other hand, if the idle power consumption is significant, then SACK is the best choice since it has the lowest overall energy consumption.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {206--216},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511360},
 doi = {http://doi.acm.org/10.1145/511334.511360},
 acmid = {511360},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {TCP, energy, mobile, wireless},
} 

@inproceedings{Heath:2002:ICA:511334.511362,
 author = {Heath, Taliver and Martin, Richard P. and Nguyen, Thu D.},
 title = {Improving cluster availability using workstation validation},
 abstract = {We demonstrate a framework for improving the availability of cluster based Internet services. Our approach models Internet services as a collection of interconnected components, each possessing well defined interfaces and failure semantics. Such a decomposition allows designers to engineer high availability based on an understanding of the interconnections and isolated fault behavior of each component, as opposed to ad-hoc methods. In this work, we focus on using the entire commodity workstation as a component because it possesses natural, fault-isolated interfaces. We define a failure event as a reboot because not only is a workstation unavailable during a reboot, but also because reboots are symptomatic of a larger class of failures, such as configuration and operator errors. Our observations of 3 distinct clusters show that the time between reboots is best modeled by a Weibull distribution with shape parameters of less than 1, implying that a workstation becomes more reliable the longer it has been operating. Leveraging this observed property, we design an allocation strategy which withholds recently rebooted workstations from active service, validating their stability before allowing them to return to service. We show via simulation that this policy leads to a 70-30 rule-of-thumb: For a constant utilization, approximately 70\% of the workstation failures can be masked from end clients with 30\% extra capacity added to the cluster, provided reboots are not strongly correlated. We also found our technique is most sensitive to the burstiness of reboots as opposed to absolute lengths of workstation uptimes.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {217--227},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511362},
 doi = {http://doi.acm.org/10.1145/511334.511362},
 acmid = {511362},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heath:2002:ICA:511399.511362,
 author = {Heath, Taliver and Martin, Richard P. and Nguyen, Thu D.},
 title = {Improving cluster availability using workstation validation},
 abstract = {We demonstrate a framework for improving the availability of cluster based Internet services. Our approach models Internet services as a collection of interconnected components, each possessing well defined interfaces and failure semantics. Such a decomposition allows designers to engineer high availability based on an understanding of the interconnections and isolated fault behavior of each component, as opposed to ad-hoc methods. In this work, we focus on using the entire commodity workstation as a component because it possesses natural, fault-isolated interfaces. We define a failure event as a reboot because not only is a workstation unavailable during a reboot, but also because reboots are symptomatic of a larger class of failures, such as configuration and operator errors. Our observations of 3 distinct clusters show that the time between reboots is best modeled by a Weibull distribution with shape parameters of less than 1, implying that a workstation becomes more reliable the longer it has been operating. Leveraging this observed property, we design an allocation strategy which withholds recently rebooted workstations from active service, validating their stability before allowing them to return to service. We show via simulation that this policy leads to a 70-30 rule-of-thumb: For a constant utilization, approximately 70\% of the workstation failures can be masked from end clients with 30\% extra capacity added to the cluster, provided reboots are not strongly correlated. We also found our technique is most sensitive to the burstiness of reboots as opposed to absolute lengths of workstation uptimes.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {217--227},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511362},
 doi = {http://doi.acm.org/10.1145/511399.511362},
 acmid = {511362},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lai:2002:LWT:511334.511363,
 author = {Lai, Albert and Nieh, Jason},
 title = {Limits of wide-area thin-client computing},
 abstract = {While many application service providers have proposed using thin-client computing to deliver computational services over the Internet, little work has been done to evaluate the effectiveness of thin-client computing in a wide-area network. To assess the potential of thin-client computing in the context of future commodity high-bandwidth Internet access, we have used a novel, non-invasive slow-motion benchmarking technique to evaluate the performance of several popular thin-client computing platforms in delivering computational services cross-country over Internet2. Our results show that using thin-client computing in a wide-area network environment can deliver acceptable performance over Internet2, even when client and server are located thousands of miles apart on opposite ends of the country. However, performance varies widely among thin-client platforms and not all platforms are suitable for this environment. While many thin-client systems are touted as being bandwidth efficient, we show that network latency is often the key factor in limiting wide-area thin-client performance. Furthermore, we show that the same techniques used to improve bandwidth efficiency often result in worse overall performance in wide-area networks. We characterize and analyze the different design choices in the various thin-client platforms and explain which of these choices should be selected for supporting wide-area computing services.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511334.511363},
 doi = {http://doi.acm.org/10.1145/511334.511363},
 acmid = {511363},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lai:2002:LWT:511399.511363,
 author = {Lai, Albert and Nieh, Jason},
 title = {Limits of wide-area thin-client computing},
 abstract = {While many application service providers have proposed using thin-client computing to deliver computational services over the Internet, little work has been done to evaluate the effectiveness of thin-client computing in a wide-area network. To assess the potential of thin-client computing in the context of future commodity high-bandwidth Internet access, we have used a novel, non-invasive slow-motion benchmarking technique to evaluate the performance of several popular thin-client computing platforms in delivering computational services cross-country over Internet2. Our results show that using thin-client computing in a wide-area network environment can deliver acceptable performance over Internet2, even when client and server are located thousands of miles apart on opposite ends of the country. However, performance varies widely among thin-client platforms and not all platforms are suitable for this environment. While many thin-client systems are touted as being bandwidth efficient, we show that network latency is often the key factor in limiting wide-area thin-client performance. Furthermore, we show that the same techniques used to improve bandwidth efficiency often result in worse overall performance in wide-area networks. We characterize and analyze the different design choices in the various thin-client platforms and explain which of these choices should be selected for supporting wide-area computing services.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {228--239},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511399.511363},
 doi = {http://doi.acm.org/10.1145/511399.511363},
 acmid = {511363},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vetter:2002:DSP:511334.511364,
 author = {Vetter, Jeffrey},
 title = {Dynamic statistical profiling of communication activity in distributed applications},
 abstract = {Performance analysis of communication activity for a terascale application with traditional message tracing can be overwhelming in terms of overhead, perturbation, and storage. We propose a novel alternative that enables dynamic statistical profiling of an application's communication activity using message sampling. We have implemented an operational prototype, named P<sc>HOTON</sc>, and our evidence shows that this new approach can provide an accurate, low-overhead, tractable alternative for performance analysis of communication activity. P<sc>HOTON</sc> consists of two components: a Message Passing Interface (MPI) profiling layer that implements sampling and analysis, and a modified MPI runtime that appends a small but necessary amount of information to individual messages. More importantly, this alternative enables an assortment of runtime analysis techniques so that, in contrast to post-mortem, trace-based techniques, the raw performance data can be jettisoned immediately after analysis. Our investigation shows that message sampling can reduce overhead to imperceptible levels for many applications. Experiments on several applications demonstrate the viability of this approach. For example, with one application, our technique reduced the analysis overhead from 154\% for traditional tracing to 6\% for statistical profiling. We also evaluate different sampling techniques in this framework. The coverage of the sample space provided by purely random sampling is superior to counter- and timer-based sampling. Also, P<sc>HOTON</sc>'s design reveals that frugal modifications to the MPI runtime system could facilitate such techniques on production computing systems, and it suggests that this sampling technique could execute continuously for long-running applications.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {240--250},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511334.511364},
 doi = {http://doi.acm.org/10.1145/511334.511364},
 acmid = {511364},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vetter:2002:DSP:511399.511364,
 author = {Vetter, Jeffrey},
 title = {Dynamic statistical profiling of communication activity in distributed applications},
 abstract = {Performance analysis of communication activity for a terascale application with traditional message tracing can be overwhelming in terms of overhead, perturbation, and storage. We propose a novel alternative that enables dynamic statistical profiling of an application's communication activity using message sampling. We have implemented an operational prototype, named P<sc>HOTON</sc>, and our evidence shows that this new approach can provide an accurate, low-overhead, tractable alternative for performance analysis of communication activity. P<sc>HOTON</sc> consists of two components: a Message Passing Interface (MPI) profiling layer that implements sampling and analysis, and a modified MPI runtime that appends a small but necessary amount of information to individual messages. More importantly, this alternative enables an assortment of runtime analysis techniques so that, in contrast to post-mortem, trace-based techniques, the raw performance data can be jettisoned immediately after analysis. Our investigation shows that message sampling can reduce overhead to imperceptible levels for many applications. Experiments on several applications demonstrate the viability of this approach. For example, with one application, our technique reduced the analysis overhead from 154\% for traditional tracing to 6\% for statistical profiling. We also evaluate different sampling techniques in this framework. The coverage of the sample space provided by purely random sampling is superior to counter- and timer-based sampling. Also, P<sc>HOTON</sc>'s design reveals that frugal modifications to the MPI runtime system could facilitate such techniques on production computing systems, and it suggests that this sampling technique could execute continuously for long-running applications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {240--250},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/511399.511364},
 doi = {http://doi.acm.org/10.1145/511399.511364},
 acmid = {511364},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cook:2002:TRP:511399.511366,
 author = {Cook, Jeanine and Oliver, Richard L. and Johnson, Eric E.},
 title = {Toward reducing processor simulation time via dynamic reduction of microarchitecture complexity},
 abstract = {As processor microarchitectures continue to increase in complexity, so does the time required to explore the design space. Performing cycle-accurate, detailed timing simulation of a realistic workload on a proposed processor microarchitecture often incurs a prohibitively large time cost. We propose a method to reduce the time cost of simulation by dynamically varying the complexity of the processor model throughout the simulation. In this paper, we give first evidence of the feasibility of this approach. We demonstrate that there are significant amounts of time during a simulation where a reduced processor model accurately tracks important behavior of a full model, and that by simulating the reduced model during these times the total simulation time can be reduced. Finally, we discuss metrics for detecting areas where the two processor models track each other, which is crucial for dynamically deciding when to use a reduced rather than a full model.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {252--253},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511366},
 doi = {http://doi.acm.org/10.1145/511399.511366},
 acmid = {511366},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cook:2002:TRP:511334.511366,
 author = {Cook, Jeanine and Oliver, Richard L. and Johnson, Eric E.},
 title = {Toward reducing processor simulation time via dynamic reduction of microarchitecture complexity},
 abstract = {As processor microarchitectures continue to increase in complexity, so does the time required to explore the design space. Performing cycle-accurate, detailed timing simulation of a realistic workload on a proposed processor microarchitecture often incurs a prohibitively large time cost. We propose a method to reduce the time cost of simulation by dynamically varying the complexity of the processor model throughout the simulation. In this paper, we give first evidence of the feasibility of this approach. We demonstrate that there are significant amounts of time during a simulation where a reduced processor model accurately tracks important behavior of a full model, and that by simulating the reduced model during these times the total simulation time can be reduced. Finally, we discuss metrics for detecting areas where the two processor models track each other, which is crucial for dynamically deciding when to use a reduced rather than a full model.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {252--253},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511366},
 doi = {http://doi.acm.org/10.1145/511334.511366},
 acmid = {511366},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shih:2002:ETC:511334.511367,
 author = {Shih, Jimmy S. and Katz, Randy H.},
 title = {Evaluating tradeoffs of congestion pricing for voice calls},
 abstract = {We conducted user experiments and simulations to understand the tradeoffs of congestion pricing between system performance and user satisfaction for a large community of users. We found that congestion pricing can be effective for voice calls because it only needs to be applied occasionally and that users are responsive to occasional price increases.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {254--255},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511367},
 doi = {http://doi.acm.org/10.1145/511334.511367},
 acmid = {511367},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shih:2002:ETC:511399.511367,
 author = {Shih, Jimmy S. and Katz, Randy H.},
 title = {Evaluating tradeoffs of congestion pricing for voice calls},
 abstract = {We conducted user experiments and simulations to understand the tradeoffs of congestion pricing between system performance and user satisfaction for a large community of users. We found that congestion pricing can be effective for voice calls because it only needs to be applied occasionally and that users are responsive to occasional price increases.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {254--255},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511367},
 doi = {http://doi.acm.org/10.1145/511399.511367},
 acmid = {511367},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sivan-Zimet:2002:WBO:511334.511368,
 author = {Sivan-Zimet, Miriam and Madhyastha, Tara M.},
 title = {Workload based optimization of probe-based storage},
 abstract = {The performance gap between microprocessors and secondary storage is still a limitation in today's systems. Academia and industry are developing new technologies to overcome this gap, such as improved read-write head technology and higher storage densities. One promising new technology is probe-based storage[1]. Characteristics of probe-based storage include small size, high density, high parallelism, low power consumption, and rectilinear motion. We have created a probe-based storage simulation model, configurable to different design points, and identify its sensitivity to various parameters.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {256--257},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511368},
 doi = {http://doi.acm.org/10.1145/511334.511368},
 acmid = {511368},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sivan-Zimet:2002:WBO:511399.511368,
 author = {Sivan-Zimet, Miriam and Madhyastha, Tara M.},
 title = {Workload based optimization of probe-based storage},
 abstract = {The performance gap between microprocessors and secondary storage is still a limitation in today's systems. Academia and industry are developing new technologies to overcome this gap, such as improved read-write head technology and higher storage densities. One promising new technology is probe-based storage[1]. Characteristics of probe-based storage include small size, high density, high parallelism, low power consumption, and rectilinear motion. We have created a probe-based storage simulation model, configurable to different design points, and identify its sensitivity to various parameters.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {256--257},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511368},
 doi = {http://doi.acm.org/10.1145/511399.511368},
 acmid = {511368},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lv:2002:SRU:511334.511369,
 author = {Lv, Qin and Cao, Pei and Cohen, Edith and Li, Kai and Shenker, Scott},
 title = {Search and replication in unstructured peer-to-peer networks},
 abstract = {Decentralized and unstructured peer-to-peer networks such as Gnutella are attractive for certain applications because they require no centralized directories and no precise control over network topology or data placement. However, the flooding-based query algorithm used in Gnutella does not scale; each individual query generates a large amount of traffic and large systems quickly become overwhelmed by the query-induced load. This paper explores various alternatives to Gnutella's query algorithm and data replication strategy. We propose a query algorithm based on multiple random walks that resolves queries almost as quickly as Gnutella's flooding method while reducing the network traffic by two orders of magnitude in many cases. We also present a distributed replication strategy that yields close-to-optimal performance.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {258--259},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511369},
 doi = {http://doi.acm.org/10.1145/511334.511369},
 acmid = {511369},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lv:2002:SRU:511399.511369,
 author = {Lv, Qin and Cao, Pei and Cohen, Edith and Li, Kai and Shenker, Scott},
 title = {Search and replication in unstructured peer-to-peer networks},
 abstract = {Decentralized and unstructured peer-to-peer networks such as Gnutella are attractive for certain applications because they require no centralized directories and no precise control over network topology or data placement. However, the flooding-based query algorithm used in Gnutella does not scale; each individual query generates a large amount of traffic and large systems quickly become overwhelmed by the query-induced load. This paper explores various alternatives to Gnutella's query algorithm and data replication strategy. We propose a query algorithm based on multiple random walks that resolves queries almost as quickly as Gnutella's flooding method while reducing the network traffic by two orders of magnitude in many cases. We also present a distributed replication strategy that yields close-to-optimal performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {258--259},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511369},
 doi = {http://doi.acm.org/10.1145/511399.511369},
 acmid = {511369},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chandramouli:2002:ALT:511334.511370,
 author = {Chandramouli, Y. and Neidhardt, Arnold},
 title = {Application level traffic measurements for capacity engineering},
 abstract = {In general, the traffic characteristics of the individual applications that constitute the aggregate traffic on a network can be important for capacity engineering. In this paper, we demonstrate based on mathematical analysis the value of application specific measurements even when there is no service differentiation. In other words, under certain assumptions, we obtain the result that errors in engineering can occur, and in particular, under-engineering can occur when traffic characteristics of individual applications are ignored. The assumptions are that the individual applications can be modeled adequately as Fractional Brownian Motions and that measurements are available only at relatively coarse time scales. The results in this paper emphasize the value of collecting fine-grained traffic measurements.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {260--261},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511370},
 doi = {http://doi.acm.org/10.1145/511334.511370},
 acmid = {511370},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chandramouli:2002:ALT:511399.511370,
 author = {Chandramouli, Y. and Neidhardt, Arnold},
 title = {Application level traffic measurements for capacity engineering},
 abstract = {In general, the traffic characteristics of the individual applications that constitute the aggregate traffic on a network can be important for capacity engineering. In this paper, we demonstrate based on mathematical analysis the value of application specific measurements even when there is no service differentiation. In other words, under certain assumptions, we obtain the result that errors in engineering can occur, and in particular, under-engineering can occur when traffic characteristics of individual applications are ignored. The assumptions are that the individual applications can be modeled adequately as Fractional Brownian Motions and that measurements are available only at relatively coarse time scales. The results in this paper emphasize the value of collecting fine-grained traffic measurements.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {260--261},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511370},
 doi = {http://doi.acm.org/10.1145/511399.511370},
 acmid = {511370},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Williamson:2002:CT:511399.511371,
 author = {Williamson, Carey and Wu, Qian},
 title = {Context-aware TCP/IP},
 abstract = {This paper discusses the design and evaluation of CATNIP, a Context-Aware Transport/Network Internet Protocol for the Web. This integrated protocol uses application-layer knowledge (i.e., Web document size) to provide explicit context information to the TCP and IP protocols. While this approach violates the traditional layered Internet protocol architecture, it enables informed decision-making, both at network endpoints and at network routers, regarding flow control, congestion control, and packet discard decisions.The ns-2 network simulator is used to evaluate the performance of the context-aware TCP/IP approach, using a simple network topology and a synthetic Web workload. Simulation results indicate a 10-20\% reduction in TCP packet loss using simple endpoint control mechanisms. More importantly, using CATNIP context information at IP routers can produce 20-80\% reductions in the mean Web page retrieval times, and 60-90\% reductions in the standard deviation of retrieval times.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {262--263},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511371},
 doi = {http://doi.acm.org/10.1145/511399.511371},
 acmid = {511371},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Williamson:2002:CT:511334.511371,
 author = {Williamson, Carey and Wu, Qian},
 title = {Context-aware TCP/IP},
 abstract = {This paper discusses the design and evaluation of CATNIP, a Context-Aware Transport/Network Internet Protocol for the Web. This integrated protocol uses application-layer knowledge (i.e., Web document size) to provide explicit context information to the TCP and IP protocols. While this approach violates the traditional layered Internet protocol architecture, it enables informed decision-making, both at network endpoints and at network routers, regarding flow control, congestion control, and packet discard decisions.The ns-2 network simulator is used to evaluate the performance of the context-aware TCP/IP approach, using a simple network topology and a synthetic Web workload. Simulation results indicate a 10-20\% reduction in TCP packet loss using simple endpoint control mechanisms. More importantly, using CATNIP context information at IP routers can produce 20-80\% reductions in the mean Web page retrieval times, and 60-90\% reductions in the standard deviation of retrieval times.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {262--263},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511371},
 doi = {http://doi.acm.org/10.1145/511334.511371},
 acmid = {511371},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barakat:2002:IBT:511399.511372,
 author = {Barakat, Chadi and Thiran, Patrick and Iannaccone, Gianluca and Diot, Christophe},
 title = {On Internet backbone traffic modeling},
 abstract = {The motivation of this work is to design a traffic model that can be used in routers or by network administrators to assist in network design and management. Currently, network operators have very basic information about the traffic. They mostly use SNMP, which provides average throughput information over 5 minutes intervals. An analytical model can provide more accurate information on the traffic such as its variation and its auto-correlation at short timescales.In contrast to other works (see [2] and the references therein), we choose to model the traffic on a link that is not</i> congested (congestion possibly appears elsewhere in the Internet). This assumption is valid (and in fact is the rule) for backbone links that are generally over-provisioned (i.e., the network is designed so that a backbone link does not reach 50\% utilization in the absence of link failure [4]). This choice is driven by our main objective, which is to provide a link dimensioning tool usable in backbone network management.We opt for a model of the traffic at the flow level. Modeling the traffic at the packet level is very difficult, since traffic on a link is the result of a high level of multiplexing of numerous flows whose behavior is strongly influenced by the transport protocol and by the application. A flow in our model is a very generic notion. It can be a TCP connection or a UDP stream (described by source and destination IP addresses, source and destination port numbers and the protocol number), or it can be a destination address prefix (e.g., destination IP address in the form a.b.0.0/16). The definition of a flow is deliberately kept general, which allows our model to be applied to different applications and to different transport mechanisms. The model can however be specified to some particular traffic types such as FTP and HTTP. By specifying the model to a certain traffic type, one must expect to obtain better results.Data flows arrive to a backbone link at random times, transport a random volume of data, and stay active for random periods. Given information on flows, our model aims to compute the total (aggregate) rate of data observed on the backbone link. We are interested in capturing the dynamics of the total data rate at short timescales (i.e., of the order of hundreds of milliseconds). This dynamics can be completely characterized using simple mathematical tools, namely the shot-noise process [3]. Our main contribution is the computation of simple expressions for important measures of backbone traffic such as its average, its variance, and its auto-correlation function. These expressions are functions of a few number of parameters that can be easily computed by a router (e.g., using a tool such as NetFlow, which provides flow information in Cisco routers).Our model can be helpful for managing and dimensioning IP backbone networks. Knowing the average and the variance of the traffic allows an ISP to provision the links of its backbone so as to avoid congestion. Congestion can be avoided at short timescales of the order of hundreds of milliseconds. The auto-correlation function of the traffic can be used to propose predictors for its future values. The prediction of the traffic has diverse applications in managing the resources of the backbone. One interesting application is the use of a short-term prediction to optimize packet routing and load balancing. Our model can also be used to assess the impact on backbone traffic of changes made in the rest of the Internet such as the addition of a new customer, a new application, or a new transport mechanism. The ISP can plan the provisioning of its backbone so as to absorb the resulting change of traffic before this change takes place.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511372},
 doi = {http://doi.acm.org/10.1145/511399.511372},
 acmid = {511372},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barakat:2002:IBT:511334.511372,
 author = {Barakat, Chadi and Thiran, Patrick and Iannaccone, Gianluca and Diot, Christophe},
 title = {On Internet backbone traffic modeling},
 abstract = {The motivation of this work is to design a traffic model that can be used in routers or by network administrators to assist in network design and management. Currently, network operators have very basic information about the traffic. They mostly use SNMP, which provides average throughput information over 5 minutes intervals. An analytical model can provide more accurate information on the traffic such as its variation and its auto-correlation at short timescales.In contrast to other works (see [2] and the references therein), we choose to model the traffic on a link that is not</i> congested (congestion possibly appears elsewhere in the Internet). This assumption is valid (and in fact is the rule) for backbone links that are generally over-provisioned (i.e., the network is designed so that a backbone link does not reach 50\% utilization in the absence of link failure [4]). This choice is driven by our main objective, which is to provide a link dimensioning tool usable in backbone network management.We opt for a model of the traffic at the flow level. Modeling the traffic at the packet level is very difficult, since traffic on a link is the result of a high level of multiplexing of numerous flows whose behavior is strongly influenced by the transport protocol and by the application. A flow in our model is a very generic notion. It can be a TCP connection or a UDP stream (described by source and destination IP addresses, source and destination port numbers and the protocol number), or it can be a destination address prefix (e.g., destination IP address in the form a.b.0.0/16). The definition of a flow is deliberately kept general, which allows our model to be applied to different applications and to different transport mechanisms. The model can however be specified to some particular traffic types such as FTP and HTTP. By specifying the model to a certain traffic type, one must expect to obtain better results.Data flows arrive to a backbone link at random times, transport a random volume of data, and stay active for random periods. Given information on flows, our model aims to compute the total (aggregate) rate of data observed on the backbone link. We are interested in capturing the dynamics of the total data rate at short timescales (i.e., of the order of hundreds of milliseconds). This dynamics can be completely characterized using simple mathematical tools, namely the shot-noise process [3]. Our main contribution is the computation of simple expressions for important measures of backbone traffic such as its average, its variance, and its auto-correlation function. These expressions are functions of a few number of parameters that can be easily computed by a router (e.g., using a tool such as NetFlow, which provides flow information in Cisco routers).Our model can be helpful for managing and dimensioning IP backbone networks. Knowing the average and the variance of the traffic allows an ISP to provision the links of its backbone so as to avoid congestion. Congestion can be avoided at short timescales of the order of hundreds of milliseconds. The auto-correlation function of the traffic can be used to propose predictors for its future values. The prediction of the traffic has diverse applications in managing the resources of the backbone. One interesting application is the use of a short-term prediction to optimize packet routing and load balancing. Our model can also be used to assess the impact on backbone traffic of changes made in the rest of the Internet such as the addition of a new customer, a new application, or a new transport mechanism. The ISP can plan the provisioning of its backbone so as to absorb the resulting change of traffic before this change takes place.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511372},
 doi = {http://doi.acm.org/10.1145/511334.511372},
 acmid = {511372},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thomasian:2002:NDS:511399.511373,
 author = {Thomasian, Alexander and Liu, Chang},
 title = {Some new disk scheduling policies and their performance},
 abstract = {Advances in magnetic recording technology have resulted in a rapid increase in disk capacities, but improvements in the mechanical characteristics of disks have been quite modest. For example, the access time to random disk blocks has decreased by a mere factor of two, while disk capacities have increased by several orders of magnitude. OLTP applications subject disks to a very demanding workload consisting of accesses to randomly distributed disk blocks and gain limited benefit from caching and prefetching (at the onboard disk cache). We propose some new disk scheduling methods to address the limited disk access bandwidth problem.Some well-known disk scheduling methods are: (i) FCFS. (ii) Shortest Seek Time First (SSTF). (iii) SCAN and Cyclical SCAN (CSCAN). The latter moves the disk arm to its beginning point after each SCAN so that requests at all disk cylinders are treated symmetrically. (iv) CSCAN with a lookahead of next i</i> requests (CSCAN-LAi) takes into account latency to reorder their processing to minimize the sum of their service times. (v) Shortest Access Time First (SATF), which provides the best performance [2]. (vi) SATF with lookahead for i</i> requests (SATF-LAi).In the case of SATF-LAi with i</i> = 2 after the completion of request X</i> the scheduler chooses requests A</i> and B</i> such that the sum of their service times processed consecutively, i.e., t<inf>X,A</inf></i> + at<inf>A,B</inf>,</i> is minimized. In SATF with flexible lookahead</i> only request A</i> is definitely processed and request B</i> is processed provided that it is selected in the next round. We refer to a</i> as the discount factor</i> (0 \&le; a</i> \&le; 1), because less weight is attached to the service time of request B,</i> since it may not be processed after request A.</i> The case a</i> = 0 corresponds to pure SATF. When a</i> = 1 we consider a variant called SATF with fixed lookahead</i> where B</i> is processed unconditionally after A</i> before any other other (perhaps more favorable recent) requests. Thus requests are processed two at a time, unless only one request is available. More generally requests in the temporal neighborhood of request A</i> are given higher priority.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511373},
 doi = {http://doi.acm.org/10.1145/511399.511373},
 acmid = {511373},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thomasian:2002:NDS:511334.511373,
 author = {Thomasian, Alexander and Liu, Chang},
 title = {Some new disk scheduling policies and their performance},
 abstract = {Advances in magnetic recording technology have resulted in a rapid increase in disk capacities, but improvements in the mechanical characteristics of disks have been quite modest. For example, the access time to random disk blocks has decreased by a mere factor of two, while disk capacities have increased by several orders of magnitude. OLTP applications subject disks to a very demanding workload consisting of accesses to randomly distributed disk blocks and gain limited benefit from caching and prefetching (at the onboard disk cache). We propose some new disk scheduling methods to address the limited disk access bandwidth problem.Some well-known disk scheduling methods are: (i) FCFS. (ii) Shortest Seek Time First (SSTF). (iii) SCAN and Cyclical SCAN (CSCAN). The latter moves the disk arm to its beginning point after each SCAN so that requests at all disk cylinders are treated symmetrically. (iv) CSCAN with a lookahead of next i</i> requests (CSCAN-LAi) takes into account latency to reorder their processing to minimize the sum of their service times. (v) Shortest Access Time First (SATF), which provides the best performance [2]. (vi) SATF with lookahead for i</i> requests (SATF-LAi).In the case of SATF-LAi with i</i> = 2 after the completion of request X</i> the scheduler chooses requests A</i> and B</i> such that the sum of their service times processed consecutively, i.e., t<inf>X,A</inf></i> + at<inf>A,B</inf>,</i> is minimized. In SATF with flexible lookahead</i> only request A</i> is definitely processed and request B</i> is processed provided that it is selected in the next round. We refer to a</i> as the discount factor</i> (0 \&le; a</i> \&le; 1), because less weight is attached to the service time of request B,</i> since it may not be processed after request A.</i> The case a</i> = 0 corresponds to pure SATF. When a</i> = 1 we consider a variant called SATF with fixed lookahead</i> where B</i> is processed unconditionally after A</i> before any other other (perhaps more favorable recent) requests. Thus requests are processed two at a time, unless only one request is available. More generally requests in the temporal neighborhood of request A</i> are given higher priority.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511373},
 doi = {http://doi.acm.org/10.1145/511334.511373},
 acmid = {511373},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:2002:SCC:511399.511374,
 author = {Lee, Kang-Won and Amiri, Khalil and Sahu, Sambit and Venkatramani, Chitra},
 title = {On the sensitivity of cooperative caching performance to workload and network characteristics},
 abstract = {A rich body of literature exists on several aspects of cooperative caching [1, 2, 3, 4, 5], including object placement and replacement algorithms [1], mechanisms for reducing the overhead of cooperation [2, 3], and the performance impact of cooperation [3, 4, 5]. However, while several studies have focused on quantifying the performance benefit of cooperative caching, their conclusions on the effectiveness of such cooperation vary significantly. The source of this apparent disagreement lies mainly in their different assumptions about workload and network characteristics, and about the degree of cooperation among caches.To more comprehensively evaluate the practical benefit of cooperative caching, we explore the sensitivity of the benefit of cooperation to workload characteristics such as object popularity distribution, temporal locality, one time referencing behavior,</i> and to network characteristics such as latencies between clients, proxies, and servers.</i> Furthermore, we identify a critical workload characteristic, which we call average access density,</i> and show that it has a crucial impact on the effectiveness of cooperative caching.In this extended abstract, we report on a few important results selected from our extensive study reported in [6]. In particular, assuming an LFU-based cache management policy, we arrive at the following conclusions. First, cooperative caching is only effective when the average access density</i> (defined as the ratio of the number of requests to the number of distinct objects in a time window) is relatively high. Second, the effectiveness of cooperative caching decreases as the skew in object popularity increases. Higher skew means that only a small number of objects are most frequently accessed reducing the benefit of larger caches, and therefore of cooperation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511374},
 doi = {http://doi.acm.org/10.1145/511399.511374},
 acmid = {511374},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:2002:SCC:511334.511374,
 author = {Lee, Kang-Won and Amiri, Khalil and Sahu, Sambit and Venkatramani, Chitra},
 title = {On the sensitivity of cooperative caching performance to workload and network characteristics},
 abstract = {A rich body of literature exists on several aspects of cooperative caching [1, 2, 3, 4, 5], including object placement and replacement algorithms [1], mechanisms for reducing the overhead of cooperation [2, 3], and the performance impact of cooperation [3, 4, 5]. However, while several studies have focused on quantifying the performance benefit of cooperative caching, their conclusions on the effectiveness of such cooperation vary significantly. The source of this apparent disagreement lies mainly in their different assumptions about workload and network characteristics, and about the degree of cooperation among caches.To more comprehensively evaluate the practical benefit of cooperative caching, we explore the sensitivity of the benefit of cooperation to workload characteristics such as object popularity distribution, temporal locality, one time referencing behavior,</i> and to network characteristics such as latencies between clients, proxies, and servers.</i> Furthermore, we identify a critical workload characteristic, which we call average access density,</i> and show that it has a crucial impact on the effectiveness of cooperative caching.In this extended abstract, we report on a few important results selected from our extensive study reported in [6]. In particular, assuming an LFU-based cache management policy, we arrive at the following conclusions. First, cooperative caching is only effective when the average access density</i> (defined as the ratio of the number of requests to the number of distinct objects in a time window) is relatively high. Second, the effectiveness of cooperative caching decreases as the skew in object popularity increases. Higher skew means that only a small number of objects are most frequently accessed reducing the benefit of larger caches, and therefore of cooperation.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511374},
 doi = {http://doi.acm.org/10.1145/511334.511374},
 acmid = {511374},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anantharaman:2002:MAT:511399.511375,
 author = {Anantharaman, Vaidyanathan and Sivakumar, Raghupathy},
 title = {A microscopic analysis of TCP performance over wireless ad-hoc networks},
 abstract = {Ad-hoc networks are multi-hop wireless networks that can operate without the services of an established backbone infrastructure. While such networks have obvious applications in the military and disaster relief environments, more recent works that have motivated their use even in regular wireless packet data networks have increased their significance. The focus of this paper is to study the performance of the TCP transport layer protocol over ad-hoc networks.Recent works in transport protocols for ad-hoc networks have investigated the impact of ad-hoc network characteristics on TCP's performance, and proposed schemes that help TCP overcome the negative impact of such characteristics as random wireless loss and mobility. The primary mechanism proposed involves sending an explicit link failure notification (ELFN) to the source from the point of link failure. The source, upon receiving the ELFN freezes</i> TCP's timers and state, re-computes a new route to the destination, and either releases the timers and state or re-starts them from their respective initial values. While the goal of ELFN based approaches is to prevent the route disruption time from adversely impacting TCP's performance, in this paper we contend that there are several other factors that influence TCP's performance degradation. We briefly outline the different factors below:\&bull; TCP Losses:</i> Every route failure induces upto a TCP-window worth of packet losses. While the losses have an absolute impact on the performance degradation, the TCP source also reacts to the losses by reducing the size of its window. Note that ELFN will prevent this negative impact on TCP's performance by appropriately freezing TCP's state.\&bull; MAC Failure Detection Time:</i> Since the MAC layer (802.11) has to go through multiple retransmissions before concluding link failure, there is a distinct component associated with the time taken to actually detect link failure since the occurrence of the failure. Importantly, the detection time increases with increasing load in the network. While an external mechanism to detect link failures (e.g. through periodic beacones at the routing layer) would solve this problem, it comes at the cost of beacon overheads and associated trade-offs.\&bull; MAC Packet Arrival:</i> When a failure is detected as described above, the link failure indication is sent only to the source of the packet that triggered the detection. If another source is using the same link in the path to its destination, the node upstream of the link failure will wait until it receives a packet from that source before informing it of the link failure. This also contributes to the magnitude of the delay after which a source realizes that a path is broken.\&bull; Route Computation Time:</i> Once a source is informed of a path failure, the time taken to recompute the route also increases with increasing load. With ELFN, for a load of 25 connections, the per-flow average of the aggregate time spent in route computation during a 100 second simulation was as high as 15 seconds. In addition to the absolute impact of the idle periods, TCP is also likely to experience timeouts, especially in the heavily loaded scenarios where the route computation time can be high.In the next section, we present a framework of mechanisms called Atra</i> targeted toward addressing each of the above components. We show through representative simulation results that the proposed mechanisms outperform both the default protocol stack and an ELFN-enabled protocol stack substantially. We assume the default protocol stack to comprise of the IEEE 802.11 MAC protocol, the Dynamic Source Routing (DSR) routing protocol, and TCP-NewReno as the transport layer protocol. For a more detailed analysis of TCP performance in mobile ad-hoc networks, and description of the Atra framework, please see [1].},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511375},
 doi = {http://doi.acm.org/10.1145/511399.511375},
 acmid = {511375},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anantharaman:2002:MAT:511334.511375,
 author = {Anantharaman, Vaidyanathan and Sivakumar, Raghupathy},
 title = {A microscopic analysis of TCP performance over wireless ad-hoc networks},
 abstract = {Ad-hoc networks are multi-hop wireless networks that can operate without the services of an established backbone infrastructure. While such networks have obvious applications in the military and disaster relief environments, more recent works that have motivated their use even in regular wireless packet data networks have increased their significance. The focus of this paper is to study the performance of the TCP transport layer protocol over ad-hoc networks.Recent works in transport protocols for ad-hoc networks have investigated the impact of ad-hoc network characteristics on TCP's performance, and proposed schemes that help TCP overcome the negative impact of such characteristics as random wireless loss and mobility. The primary mechanism proposed involves sending an explicit link failure notification (ELFN) to the source from the point of link failure. The source, upon receiving the ELFN freezes</i> TCP's timers and state, re-computes a new route to the destination, and either releases the timers and state or re-starts them from their respective initial values. While the goal of ELFN based approaches is to prevent the route disruption time from adversely impacting TCP's performance, in this paper we contend that there are several other factors that influence TCP's performance degradation. We briefly outline the different factors below:\&bull; TCP Losses:</i> Every route failure induces upto a TCP-window worth of packet losses. While the losses have an absolute impact on the performance degradation, the TCP source also reacts to the losses by reducing the size of its window. Note that ELFN will prevent this negative impact on TCP's performance by appropriately freezing TCP's state.\&bull; MAC Failure Detection Time:</i> Since the MAC layer (802.11) has to go through multiple retransmissions before concluding link failure, there is a distinct component associated with the time taken to actually detect link failure since the occurrence of the failure. Importantly, the detection time increases with increasing load in the network. While an external mechanism to detect link failures (e.g. through periodic beacones at the routing layer) would solve this problem, it comes at the cost of beacon overheads and associated trade-offs.\&bull; MAC Packet Arrival:</i> When a failure is detected as described above, the link failure indication is sent only to the source of the packet that triggered the detection. If another source is using the same link in the path to its destination, the node upstream of the link failure will wait until it receives a packet from that source before informing it of the link failure. This also contributes to the magnitude of the delay after which a source realizes that a path is broken.\&bull; Route Computation Time:</i> Once a source is informed of a path failure, the time taken to recompute the route also increases with increasing load. With ELFN, for a load of 25 connections, the per-flow average of the aggregate time spent in route computation during a 100 second simulation was as high as 15 seconds. In addition to the absolute impact of the idle periods, TCP is also likely to experience timeouts, especially in the heavily loaded scenarios where the route computation time can be high.In the next section, we present a framework of mechanisms called Atra</i> targeted toward addressing each of the above components. We show through representative simulation results that the proposed mechanisms outperform both the default protocol stack and an ELFN-enabled protocol stack substantially. We assume the default protocol stack to comprise of the IEEE 802.11 MAC protocol, the Dynamic Source Routing (DSR) routing protocol, and TCP-NewReno as the transport layer protocol. For a more detailed analysis of TCP performance in mobile ad-hoc networks, and description of the Atra framework, please see [1].},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511375},
 doi = {http://doi.acm.org/10.1145/511334.511375},
 acmid = {511375},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Choi:2002:ARS:511399.511376,
 author = {Choi, Baek-Young and Park, Jaesung and Zhang, Zhi-Li},
 title = {Adaptive random sampling for load change detection},
 abstract = {Timely detection of changes in traffic load is critical for initiating appropriate traffic engineering mechanisms. Accurate measurement of traffic is essential since the efficacy of change detection depends on the accuracy of traffic estimation. However, precise</i> traffic measurement involves inspecting every</i> packet traversing a link, resulting in significant overhead, particularly on high speed links. Sampling</i> techniques for traffic load estimation</i> are proposed as a way to limit the measurement overhead. In this paper, we address the problem of bounding</i> sampling error within a pre-specified tolerance level and propose an adaptive random sampling</i> technique that determines the minimum</i> sampling probability adaptively according to traffic dynamics. Using real network traffic traces, we show that the proposed adaptive random sampling technique indeed produces the desired accuracy, while also yielding significant reduction in the amount of traffic samples. We also investigate the impact of sampling errors on the performance of load change detection.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511376},
 doi = {http://doi.acm.org/10.1145/511399.511376},
 acmid = {511376},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {change detection, sampling},
} 

@inproceedings{Choi:2002:ARS:511334.511376,
 author = {Choi, Baek-Young and Park, Jaesung and Zhang, Zhi-Li},
 title = {Adaptive random sampling for load change detection},
 abstract = {Timely detection of changes in traffic load is critical for initiating appropriate traffic engineering mechanisms. Accurate measurement of traffic is essential since the efficacy of change detection depends on the accuracy of traffic estimation. However, precise</i> traffic measurement involves inspecting every</i> packet traversing a link, resulting in significant overhead, particularly on high speed links. Sampling</i> techniques for traffic load estimation</i> are proposed as a way to limit the measurement overhead. In this paper, we address the problem of bounding</i> sampling error within a pre-specified tolerance level and propose an adaptive random sampling</i> technique that determines the minimum</i> sampling probability adaptively according to traffic dynamics. Using real network traffic traces, we show that the proposed adaptive random sampling technique indeed produces the desired accuracy, while also yielding significant reduction in the amount of traffic samples. We also investigate the impact of sampling errors on the performance of load change detection.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511376},
 doi = {http://doi.acm.org/10.1145/511334.511376},
 acmid = {511376},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {change detection, sampling},
} 

@inproceedings{Zhao:2002:MEN:511334.511377,
 author = {Zhao, Zhili and Ametha, Jayesh and Darbha, Swaroop and Reddy, A. L. Narasimha},
 title = {A method for estimating non-responsive traffic at a router},
 abstract = {In this paper, we propose a scheme for estimating the proportion of the incoming traffic that is not responding to congestion at a router. The idea of the proposed scheme is that if the observed queue length and packet drop probability do not match with the predicted results from the TCP model, then the error must come from the non-responsive traffic; it can then be used for estimating non-responsive traffic. The proposed scheme utilizes queue length history, packet drop history, expected TCP and queue dynamics to estimate the proportion. We show that the proposed scheme is effective over a wide range of traffic scenarios through simulations.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {274--275},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511377},
 doi = {http://doi.acm.org/10.1145/511334.511377},
 acmid = {511377},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control theory, estimation, non-responsive traffic, traffic modeling},
} 

@article{Zhao:2002:MEN:511399.511377,
 author = {Zhao, Zhili and Ametha, Jayesh and Darbha, Swaroop and Reddy, A. L. Narasimha},
 title = {A method for estimating non-responsive traffic at a router},
 abstract = {In this paper, we propose a scheme for estimating the proportion of the incoming traffic that is not responding to congestion at a router. The idea of the proposed scheme is that if the observed queue length and packet drop probability do not match with the predicted results from the TCP model, then the error must come from the non-responsive traffic; it can then be used for estimating non-responsive traffic. The proposed scheme utilizes queue length history, packet drop history, expected TCP and queue dynamics to estimate the proportion. We show that the proposed scheme is effective over a wide range of traffic scenarios through simulations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {274--275},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511377},
 doi = {http://doi.acm.org/10.1145/511399.511377},
 acmid = {511377},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control theory, estimation, non-responsive traffic, traffic modeling},
} 

@inproceedings{Guo:2002:SFU:511334.511378,
 author = {Guo, Liang and Matta, Ibrahim},
 title = {Scheduling flows with unknown sizes: approximate analysis},
 abstract = {Previous job scheduling studies indicate that providing rapid response to interactive jobs which place frequent but small demands, can reduce the overall system average response time [1], especially when the job size distribution is skewed (see [2] and references therein). Since the distribution of Internet flows is skewed, it is natural to design a network system that favors short file transfers through service differentiation. However, to maintain system scalability, detailed per-flow state such as flow length is generally not available inside the network. As a result, we usually resort to a threshold-based heuristic to identify and give preference to short flows. Specifically, packets from a new flow are always given the highest priority. However, the priority is reduced once the flow has transferred a certain amount of packets.In this paper, we use the MultiLevel (ML) feedback queue [3] to characterize this discriminatory system. However, the solution given in [3] is in the form of an integral equation, and to date the equation has been solved only for job size distribution that has the form of mixed exponential functions. We adopt an alternative approach, namely using a conservation law by Kleinrock [1], to solve for the average response time in such system. To that end, we approximate the average response time of jobs by a linear function in the job size and solve for the stretch (service slowdown) factors. We show by simulation that such approximation works well for job (flow) size distributions that possess the heavy-tailed property [2], although it does not work so well for exponential distributions.Due to the limited space available, in Section 2 we briefly describe the queueing model and summarize our approximation approach to solving for the average response time of the M/G/1/ML queueing system. We conclude our paper in Section 3.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {276--277},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511378},
 doi = {http://doi.acm.org/10.1145/511334.511378},
 acmid = {511378},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Guo:2002:SFU:511399.511378,
 author = {Guo, Liang and Matta, Ibrahim},
 title = {Scheduling flows with unknown sizes: approximate analysis},
 abstract = {Previous job scheduling studies indicate that providing rapid response to interactive jobs which place frequent but small demands, can reduce the overall system average response time [1], especially when the job size distribution is skewed (see [2] and references therein). Since the distribution of Internet flows is skewed, it is natural to design a network system that favors short file transfers through service differentiation. However, to maintain system scalability, detailed per-flow state such as flow length is generally not available inside the network. As a result, we usually resort to a threshold-based heuristic to identify and give preference to short flows. Specifically, packets from a new flow are always given the highest priority. However, the priority is reduced once the flow has transferred a certain amount of packets.In this paper, we use the MultiLevel (ML) feedback queue [3] to characterize this discriminatory system. However, the solution given in [3] is in the form of an integral equation, and to date the equation has been solved only for job size distribution that has the form of mixed exponential functions. We adopt an alternative approach, namely using a conservation law by Kleinrock [1], to solve for the average response time in such system. To that end, we approximate the average response time of jobs by a linear function in the job size and solve for the stretch (service slowdown) factors. We show by simulation that such approximation works well for job (flow) size distributions that possess the heavy-tailed property [2], although it does not work so well for exponential distributions.Due to the limited space available, in Section 2 we briefly describe the queueing model and summarize our approximation approach to solving for the average response time of the M/G/1/ML queueing system. We conclude our paper in Section 3.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {276--277},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511378},
 doi = {http://doi.acm.org/10.1145/511399.511378},
 acmid = {511378},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Alouf:2002:FVC:511399.511379,
 author = {Alouf, Sara and Huet, Fabrice and Nain, Philippe},
 title = {Forwarders vs. centralized server: an evaluation of two approaches for locating mobile agents},
 abstract = {The Internet has allowed the creation of huge amounts of data located on many sites. Performing complex operations on some data requires that the data be transferred first to the machine on which the operations are to be executed, which may require a non-negligible amount of bandwidth and may seriously limit performance if it is the bottleneck. However, instead of moving the data to the code, it is possible to move the code to the data, and perform all the operations locally. This simple idea has led to a new paradigm called code-mobility:</i> a mobile object --- sometimes called an agent --- is given a list of destinations and a series of operations to perform on each one of them. The agent will visit all of the destinations, perform the requested operations and possibly pass the result on to another object. Any mobility mechanism must first provide a way to migrate code from one host to another. It must also ensure that any communication following a migration will not be impaired by it, namely that two objects should still be able to communicate even if one of them has migrated. Such a mechanism is referred to as a location</i> mechanism since it often relies on the knowledge of the location of the objects to ensure communications. Two location mechanisms are widely used: the first one uses a centralized server whereas the second one relies on special objects called forwarders.</i>This paper evaluates and compares the performance of an existing implementation of these approaches in terms of cost of communication in presence of migration. Based on a Markov chain analysis, we will construct and solve two mathematical models, one for each mechanism and will use them to evaluate the cost of location. For the purpose of validation, we have developed for each mechanism a benchmark that uses ProActive</i> [2], a Java library that provides all the necessary primitives for code mobility. Experiments conducted on a LAN and on a MAN have validated both models and have shown that the location server always performs better than the forwarders. Using our analytical models we will nevertheless identify situations where the opposite conclusion holds. However, under most operational conditions location servers will perform better than forwarders.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {278--279},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511379},
 doi = {http://doi.acm.org/10.1145/511399.511379},
 acmid = {511379},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Alouf:2002:FVC:511334.511379,
 author = {Alouf, Sara and Huet, Fabrice and Nain, Philippe},
 title = {Forwarders vs. centralized server: an evaluation of two approaches for locating mobile agents},
 abstract = {The Internet has allowed the creation of huge amounts of data located on many sites. Performing complex operations on some data requires that the data be transferred first to the machine on which the operations are to be executed, which may require a non-negligible amount of bandwidth and may seriously limit performance if it is the bottleneck. However, instead of moving the data to the code, it is possible to move the code to the data, and perform all the operations locally. This simple idea has led to a new paradigm called code-mobility:</i> a mobile object --- sometimes called an agent --- is given a list of destinations and a series of operations to perform on each one of them. The agent will visit all of the destinations, perform the requested operations and possibly pass the result on to another object. Any mobility mechanism must first provide a way to migrate code from one host to another. It must also ensure that any communication following a migration will not be impaired by it, namely that two objects should still be able to communicate even if one of them has migrated. Such a mechanism is referred to as a location</i> mechanism since it often relies on the knowledge of the location of the objects to ensure communications. Two location mechanisms are widely used: the first one uses a centralized server whereas the second one relies on special objects called forwarders.</i>This paper evaluates and compares the performance of an existing implementation of these approaches in terms of cost of communication in presence of migration. Based on a Markov chain analysis, we will construct and solve two mathematical models, one for each mechanism and will use them to evaluate the cost of location. For the purpose of validation, we have developed for each mechanism a benchmark that uses ProActive</i> [2], a Java library that provides all the necessary primitives for code mobility. Experiments conducted on a LAN and on a MAN have validated both models and have shown that the location server always performs better than the forwarders. Using our analytical models we will nevertheless identify situations where the opposite conclusion holds. However, under most operational conditions location servers will perform better than forwarders.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {278--279},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511379},
 doi = {http://doi.acm.org/10.1145/511334.511379},
 acmid = {511379},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chang:2002:TCR:511334.511380,
 author = {Chang, Hyunseok and Govindan, Ramesh and Jamin, Sugih and Shenker, Scott J. and Willinger, Walter},
 title = {Towards capturing representative AS-level Internet topologies},
 abstract = {For the past two years,there has been a significant increase in research activities related to studying and modeling the Internet's topology, especially at the level of autonomous systems</i> (ASs). A closer look at the measurements that form the basis for all these studies reveals that the data sets used consist of the BGP routing tables collected by the Oregon route server (henceforth, the Oregon route-views</i>) [1]. So far, there has been anecdotal evidence and an intuitive understanding among researchers in the field that BGP-derived AS connectivity is not complete. However, as far as we know, there has been no systematic study on quantifying</i> the completeness of currently known AS-level Internet topologies. Our main objective in this paper is to quantify the completeness of Internet AS maps constructed from the Oregon route-views and to attempt to capture more representative</i> AS-level Internet topology. One of the main contributions of this paper is in developing a methodology that enables quantitative investigations into issues related to the (in)completeness of BGP-derived AS maps.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {280--281},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511380},
 doi = {http://doi.acm.org/10.1145/511334.511380},
 acmid = {511380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chang:2002:TCR:511399.511380,
 author = {Chang, Hyunseok and Govindan, Ramesh and Jamin, Sugih and Shenker, Scott J. and Willinger, Walter},
 title = {Towards capturing representative AS-level Internet topologies},
 abstract = {For the past two years,there has been a significant increase in research activities related to studying and modeling the Internet's topology, especially at the level of autonomous systems</i> (ASs). A closer look at the measurements that form the basis for all these studies reveals that the data sets used consist of the BGP routing tables collected by the Oregon route server (henceforth, the Oregon route-views</i>) [1]. So far, there has been anecdotal evidence and an intuitive understanding among researchers in the field that BGP-derived AS connectivity is not complete. However, as far as we know, there has been no systematic study on quantifying</i> the completeness of currently known AS-level Internet topologies. Our main objective in this paper is to quantify the completeness of Internet AS maps constructed from the Oregon route-views and to attempt to capture more representative</i> AS-level Internet topology. One of the main contributions of this paper is in developing a methodology that enables quantitative investigations into issues related to the (in)completeness of BGP-derived AS maps.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {280--281},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511380},
 doi = {http://doi.acm.org/10.1145/511399.511380},
 acmid = {511380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brownlee:2002:ISS:511334.511381,
 author = {Brownlee, Nevil and claffy, kc},
 title = {Internet stream size distributions},
 abstract = {We present and discuss stream size and lifetime distributions for web and non-web TCP traffic on a campus OC12 link at UC San Diego. The distributions are stable over long periods, and show that on this link only 3\% of the streams last longer than one minute, and that only about 0.5\% of them are bigger than 100 kBytes. Although there are large streams (elephants) on this link, the bulk of its traffic is composed of many small streams (mice).},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {282--283},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511381},
 doi = {http://doi.acm.org/10.1145/511334.511381},
 acmid = {511381},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brownlee:2002:ISS:511399.511381,
 author = {Brownlee, Nevil and claffy, kc},
 title = {Internet stream size distributions},
 abstract = {We present and discuss stream size and lifetime distributions for web and non-web TCP traffic on a campus OC12 link at UC San Diego. The distributions are stable over long periods, and show that on this link only 3\% of the streams last longer than one minute, and that only about 0.5\% of them are bigger than 100 kBytes. Although there are large streams (elephants) on this link, the bulk of its traffic is composed of many small streams (mice).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {282--283},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511381},
 doi = {http://doi.acm.org/10.1145/511399.511381},
 acmid = {511381},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhu:2002:LDB:511399.511382,
 author = {Zhu, Yingwu and Hu, Yiming},
 title = {Can large disk built-in caches really improve system performance?},
 abstract = {Via detailed file system and disk system simulation, we examine the impact of disk built-in caches on the system performance. Our results indicate that the current trend of using large built-in caches is unnecessary and a waste of money and power for most users. Disk manufacturers could use much smaller built-in caches to reduce the cost as well as power-consumption, without affecting performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {284--285},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511382},
 doi = {http://doi.acm.org/10.1145/511399.511382},
 acmid = {511382},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhu:2002:LDB:511334.511382,
 author = {Zhu, Yingwu and Hu, Yiming},
 title = {Can large disk built-in caches really improve system performance?},
 abstract = {Via detailed file system and disk system simulation, we examine the impact of disk built-in caches on the system performance. Our results indicate that the current trend of using large built-in caches is unnecessary and a waste of money and power for most users. Disk manufacturers could use much smaller built-in caches to reduce the cost as well as power-consumption, without affecting performance.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {284--285},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511382},
 doi = {http://doi.acm.org/10.1145/511334.511382},
 acmid = {511382},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Simmonds:2002:WSB:511334.511383,
 author = {Simmonds, Rob and Williamson, Carey and Bradford, Russell and Arlitt, Martin and Unger, Brian},
 title = {Web server benchmarking using parallel WAN emulation},
 abstract = {This paper discusses the use of a parallel discrete-event network emulator called the Internet Protocol Traffic and Network Emulator (IP-TNE) for Web server benchmarking. The experiments in this paper demonstrate the feasibility of high-performance WAN emulation using parallel discrete-event simulation techniques on shared-memory multiprocessors. Our experiments with the Apache Web server achieve 3400 HTTP transactions per second for simple Web workloads, and 1000 HTTP transactions per second for realistic Web workloads, for static document retrieval across emulated WAN topologies of up to 4096 concurrent Web/TCP clients. The results show that WAN characteristics, including round-trip delays, link speeds, packet losses, packet sizes, and bandwidth asymmetry, all have significant impacts on Web server performance. WAN emulation enables stress testing and benchmarking of Web server performance in ways that may not be possible in simple LAN test scenarios.},
 booktitle = {Proceedings of the 2002 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '02},
 year = {2002},
 isbn = {1-58113-531-9},
 location = {Marina Del Rey, California},
 pages = {286--287},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511334.511383},
 doi = {http://doi.acm.org/10.1145/511334.511383},
 acmid = {511383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Simmonds:2002:WSB:511399.511383,
 author = {Simmonds, Rob and Williamson, Carey and Bradford, Russell and Arlitt, Martin and Unger, Brian},
 title = {Web server benchmarking using parallel WAN emulation},
 abstract = {This paper discusses the use of a parallel discrete-event network emulator called the Internet Protocol Traffic and Network Emulator (IP-TNE) for Web server benchmarking. The experiments in this paper demonstrate the feasibility of high-performance WAN emulation using parallel discrete-event simulation techniques on shared-memory multiprocessors. Our experiments with the Apache Web server achieve 3400 HTTP transactions per second for simple Web workloads, and 1000 HTTP transactions per second for realistic Web workloads, for static document retrieval across emulated WAN topologies of up to 4096 concurrent Web/TCP clients. The results show that WAN characteristics, including round-trip delays, link speeds, packet losses, packet sizes, and bandwidth asymmetry, all have significant impacts on Web server performance. WAN emulation enables stress testing and benchmarking of Web server performance in ways that may not be possible in simple LAN test scenarios.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {30},
 issue = {1},
 month = {June},
 year = {2002},
 issn = {0163-5999},
 pages = {286--287},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/511399.511383},
 doi = {http://doi.acm.org/10.1145/511399.511383},
 acmid = {511383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chu:2000:CES:339331.339337,
 author = {Chu, Yang-hua and Rao, Sanjay G. and Zhang, Hui},
 title = {A case for end system multicast (keynote address)},
 abstract = {The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, ten years after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture for small and sparse groups, where end systems implement all multicast related functionality including membership management and packet replication. We call such a scheme End System Multicast. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delay than IP Multicast. In this paper, we study this question in the context of the Narada protocol. In Narada, end systems self-organize into an overlay structure using a fully distributed protocol. In addition, Narada attempts to optimize the efficiency of the overlay based on end-to-end measurements. We present details of Narada and evaluate it using both simulation and Internet experiments. Preliminary results are encouraging. In most simulations and Internet experiments, the delay and bandwidth penalty are low. We believe the potential benefits of repartitioning multicast functionality between end systems and routers significantly outweigh the performance penalty incurred.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339337},
 doi = {http://doi.acm.org/10.1145/339331.339337},
 acmid = {339337},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chu:2000:CES:345063.339337,
 author = {Chu, Yang-hua and Rao, Sanjay G. and Zhang, Hui},
 title = {A case for end system multicast (keynote address)},
 abstract = {The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, ten years after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture for small and sparse groups, where end systems implement all multicast related functionality including membership management and packet replication. We call such a scheme End System Multicast. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delay than IP Multicast. In this paper, we study this question in the context of the Narada protocol. In Narada, end systems self-organize into an overlay structure using a fully distributed protocol. In addition, Narada attempts to optimize the efficiency of the overlay based on end-to-end measurements. We present details of Narada and evaluate it using both simulation and Internet experiments. Preliminary results are encouraging. In most simulations and Internet experiments, the delay and bandwidth penalty are low. We believe the potential benefits of repartitioning multicast functionality between end systems and routers significantly outweigh the performance penalty incurred.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339337},
 doi = {http://doi.acm.org/10.1145/345063.339337},
 acmid = {339337},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Legout:2000:PFC:339331.339340,
 author = {Legout, A. and Biersack, E. W.},
 title = {PLM: fast convergence for cumulative layered multicast transmisson schemes},
 abstract = {A major challenge in the Internet is to deliver live audio/video content with a good quality and to transfer files to large number of heterogeneous receivers. Multicast and cumulative layered transmission are two mechanisms of interest to accomplish this task efficiently. However, protocols using these mechanisms suffer from slow convergence time, lack of inter-protocol fairness or TCP-fairness, and loss induced by the join experiments.In this paper we define and investigate the properties of a new multicast congestion control protocol (called PLM) for audio/video and file transfer applications based on a cumulative layered multicast transmission. A fundamental contribution of this paper is the introduction and evaluation of a new and efficient technique based on packet pair to infer which layers to join. We evaluated PLM for a large variety of scenarios and show that it converges fast to the optimal link utilization, induces no loss to track the available bandwidth, has inter-protocol fairness and TCP-fairness, and scales with the number of receivers and the number of sessions. Moreover, all these properties hold in self similar and multifractal environment.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339340},
 doi = {http://doi.acm.org/10.1145/339331.339340},
 acmid = {339340},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FS-paradigm, capacity inference, congestion control, cumulative layers, mulitcast, packet pair},
} 

@article{Legout:2000:PFC:345063.339340,
 author = {Legout, A. and Biersack, E. W.},
 title = {PLM: fast convergence for cumulative layered multicast transmisson schemes},
 abstract = {A major challenge in the Internet is to deliver live audio/video content with a good quality and to transfer files to large number of heterogeneous receivers. Multicast and cumulative layered transmission are two mechanisms of interest to accomplish this task efficiently. However, protocols using these mechanisms suffer from slow convergence time, lack of inter-protocol fairness or TCP-fairness, and loss induced by the join experiments.In this paper we define and investigate the properties of a new multicast congestion control protocol (called PLM) for audio/video and file transfer applications based on a cumulative layered multicast transmission. A fundamental contribution of this paper is the introduction and evaluation of a new and efficient technique based on packet pair to infer which layers to join. We evaluated PLM for a large variety of scenarios and show that it converges fast to the optimal link utilization, induces no loss to track the available bandwidth, has inter-protocol fairness and TCP-fairness, and scales with the number of receivers and the number of sessions. Moreover, all these properties hold in self similar and multifractal environment.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339340},
 doi = {http://doi.acm.org/10.1145/345063.339340},
 acmid = {339340},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FS-paradigm, capacity inference, congestion control, cumulative layers, mulitcast, packet pair},
} 

@article{Sahu:2000:ASD:345063.339342,
 author = {Sahu, Sambit and Nain, Philippe and Diot, Christophe and Firoiu, Victor and Towsley, Don},
 title = {On achievable service differentiation with token bucket marking for TCP},
 abstract = {The Differentiated services (diffserv) architecture has been proposed as a scalable solution for providing service differentiation among flows without any per-flow buffer management inside the core of the network. It has been advocated that it is feasible to provide service differentiation among a set of flows by choosing an appropriate ``marking profile" for each flow. In this paper, we examine (i) whether it is possible to provide service differentiation among a set of TCP flows by choosing appropriate marking profiles for each flow, (ii) under what circumstances, the marking profiles are able to influence the service that a TCP flow receives, and, (iii) how to choose a correct profile to achieve a given service level. We derive a simple, and yet accurate, analytical model for determining the achieved rate of a TCP flow when edge-routers use ``token bucket" packet marking and core-routers use active queue management for preferential packet dropping. From our study, we observe three important results: (i) the achieved rate is not proportional to the assured rate, (ii) it is not always possible to achieve the assured rate and, (iii) there exist ranges of values of the achieved rate for which token bucket parameters have no influence. We find that it is not easy to regulate the service level achieved by a TCP flow by solely setting the profile parameters. In addition, we derive conditions that determine when the bucket size influences the achieved rate, and rates that can be achieved and those that cannot. Our study provides insight for choosing appropriate token bucket parameters for the achievable rates.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {23--33},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339342},
 doi = {http://doi.acm.org/10.1145/345063.339342},
 acmid = {339342},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sahu:2000:ASD:339331.339342,
 author = {Sahu, Sambit and Nain, Philippe and Diot, Christophe and Firoiu, Victor and Towsley, Don},
 title = {On achievable service differentiation with token bucket marking for TCP},
 abstract = {The Differentiated services (diffserv) architecture has been proposed as a scalable solution for providing service differentiation among flows without any per-flow buffer management inside the core of the network. It has been advocated that it is feasible to provide service differentiation among a set of flows by choosing an appropriate ``marking profile" for each flow. In this paper, we examine (i) whether it is possible to provide service differentiation among a set of TCP flows by choosing appropriate marking profiles for each flow, (ii) under what circumstances, the marking profiles are able to influence the service that a TCP flow receives, and, (iii) how to choose a correct profile to achieve a given service level. We derive a simple, and yet accurate, analytical model for determining the achieved rate of a TCP flow when edge-routers use ``token bucket" packet marking and core-routers use active queue management for preferential packet dropping. From our study, we observe three important results: (i) the achieved rate is not proportional to the assured rate, (ii) it is not always possible to achieve the assured rate and, (iii) there exist ranges of values of the achieved rate for which token bucket parameters have no influence. We find that it is not easy to regulate the service level achieved by a TCP flow by solely setting the profile parameters. In addition, we derive conditions that determine when the bucket size influences the achieved rate, and rates that can be achieved and those that cannot. Our study provides insight for choosing appropriate token bucket parameters for the achievable rates.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {23--33},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339342},
 doi = {http://doi.acm.org/10.1145/339331.339342},
 acmid = {339342},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bolosky:2000:FSD:345063.339345,
 author = {Bolosky, William J. and Douceur, John R. and Ely, David and Theimer, Marvin},
 title = {Feasibility of a serverless distributed file system deployed on an existing set of desktop PCs},
 abstract = {We consider an architecture for a serverless distributed file system that does not assume mutual trust among the client computers. The system provides security, availability, and reliability by distributing multiple encrypted replicas of each file among the client machines. To assess the feasibility of deploying this system on an existing desktop infrastructure, we measure and analyze a large set of client machines in a commercial environment. In particular, we measure and report results on disk usage and content; file activity; and machine uptimes, lifetimes, and loads. We conclude that the measured desktop infrastructure would passably support our proposed system, providing availability on the order of one unfilled file request per user per thousand days.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {34--43},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339345},
 doi = {http://doi.acm.org/10.1145/345063.339345},
 acmid = {339345},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, availability, feasibility analysis, personal computer usage data, reliability, security, serverless distributed file system architecture, trust, workload characterization},
} 

@inproceedings{Bolosky:2000:FSD:339331.339345,
 author = {Bolosky, William J. and Douceur, John R. and Ely, David and Theimer, Marvin},
 title = {Feasibility of a serverless distributed file system deployed on an existing set of desktop PCs},
 abstract = {We consider an architecture for a serverless distributed file system that does not assume mutual trust among the client computers. The system provides security, availability, and reliability by distributing multiple encrypted replicas of each file among the client machines. To assess the feasibility of deploying this system on an existing desktop infrastructure, we measure and analyze a large set of client machines in a commercial environment. In particular, we measure and report results on disk usage and content; file activity; and machine uptimes, lifetimes, and loads. We conclude that the measured desktop infrastructure would passably support our proposed system, providing availability on the order of one unfilled file request per user per thousand days.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {34--43},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339345},
 doi = {http://doi.acm.org/10.1145/339331.339345},
 acmid = {339345},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, availability, feasibility analysis, personal computer usage data, reliability, security, serverless distributed file system architecture, trust, workload characterization},
} 

@inproceedings{Santos:2000:CRD:339331.339352,
 author = {Santos, Jose Renato and Muntz, Richard R. and Ribeiro-Neto, Berthier},
 title = {Comparing random data allocation and data striping in multimedia servers},
 abstract = {We compare performance of a multimedia storage server based on a random data allocation layout and block replication with traditional data striping techniques. Data striping techniques in multimedia servers are often designed for restricted workloads, e.g. sequential access patterns with CBR (constant bit rate) requirements. On the other hand, a system based on random data allocation can support virtually any type of multimedia application, including VBR (variable bit rate) video or audio, and interactive applications with unpredictable access patterns, such as 3D interactive virtual worlds, interactive scientific visualizations, etc. Surprisingly, our results show that system performance with random data allocation is competitive and sometimes even outperforms traditional data striping techniques, for the workloads for which data striping is designed to work best; i.e. streams with sequential access patterns and CBR requirements. Due to its superiority in supporting general workloads and competitive system performance, we believe that random data allocation will be the scheme of choice for next generation multimedia servers.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {44--55},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339352},
 doi = {http://doi.acm.org/10.1145/339331.339352},
 acmid = {339352},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Santos:2000:CRD:345063.339352,
 author = {Santos, Jose Renato and Muntz, Richard R. and Ribeiro-Neto, Berthier},
 title = {Comparing random data allocation and data striping in multimedia servers},
 abstract = {We compare performance of a multimedia storage server based on a random data allocation layout and block replication with traditional data striping techniques. Data striping techniques in multimedia servers are often designed for restricted workloads, e.g. sequential access patterns with CBR (constant bit rate) requirements. On the other hand, a system based on random data allocation can support virtually any type of multimedia application, including VBR (variable bit rate) video or audio, and interactive applications with unpredictable access patterns, such as 3D interactive virtual worlds, interactive scientific visualizations, etc. Surprisingly, our results show that system performance with random data allocation is competitive and sometimes even outperforms traditional data striping techniques, for the workloads for which data striping is designed to work best; i.e. streams with sequential access patterns and CBR requirements. Due to its superiority in supporting general workloads and competitive system performance, we believe that random data allocation will be the scheme of choice for next generation multimedia servers.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {44--55},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339352},
 doi = {http://doi.acm.org/10.1145/345063.339352},
 acmid = {339352},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Griffin:2000:MPM:339331.339354,
 author = {Griffin, John Linwood and Schlosser, Steven W. and Ganger, Gregory R. and Nagle, David F.},
 title = {Modeling and performance of MEMS-based storage devices},
 abstract = {MEMS-based storage devices are seen by many as promising alternatives to disk drives. Fabricated using conventional CMOS processes, MEMS-based storage consists of thousands of small, mechanical probe tips that access gigabytes of high-density, nonvolatile magnetic storage. This paper takes a first step towards understanding the performance characteristics of these devices by mapping them onto a disk-like metaphor. Using simulation models based on the mechanics equations governing the devices' operation, this work explores how different physical characteristics (e.g., actuator forces and per-tip data rates) impact the design trade-offs and performance of MEMS-based storage. Overall results indicate that average access times for MEMS-based storage are 6.5 times faster than for a modern disk (1.5 ms vs. 9.7 ms). Results from filesystem and database bench-marks show that this improvement reduces application I/O stall times up to 70\%, resulting in overall performance improvements of 3X.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {56--65},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339354},
 doi = {http://doi.acm.org/10.1145/339331.339354},
 acmid = {339354},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Griffin:2000:MPM:345063.339354,
 author = {Griffin, John Linwood and Schlosser, Steven W. and Ganger, Gregory R. and Nagle, David F.},
 title = {Modeling and performance of MEMS-based storage devices},
 abstract = {MEMS-based storage devices are seen by many as promising alternatives to disk drives. Fabricated using conventional CMOS processes, MEMS-based storage consists of thousands of small, mechanical probe tips that access gigabytes of high-density, nonvolatile magnetic storage. This paper takes a first step towards understanding the performance characteristics of these devices by mapping them onto a disk-like metaphor. Using simulation models based on the mechanics equations governing the devices' operation, this work explores how different physical characteristics (e.g., actuator forces and per-tip data rates) impact the design trade-offs and performance of MEMS-based storage. Overall results indicate that average access times for MEMS-based storage are 6.5 times faster than for a modern disk (1.5 ms vs. 9.7 ms). Results from filesystem and database bench-marks show that this improvement reduces application I/O stall times up to 70\%, resulting in overall performance improvements of 3X.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {56--65},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339354},
 doi = {http://doi.acm.org/10.1145/345063.339354},
 acmid = {339354},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Raunak:2000:IPC:339331.339357,
 author = {Raunak, Mohammad S. and Shenoy, Prashant and Goyal, Pawan and Ramamritham, Krithi},
 title = {Implications of proxy caching for provisioning networks and servers},
 abstract = {In this paper, we examine the potential benefits of web proxy caches in improving the effective capacity of servers and networks. Since networks and servers are typically provisioned based on a high percentile of the load, we focus on the effects of proxy caching on the tail of the load distribution. We find that, unlike their substantial impact on the average load, proxies have a diminished impact on the tail of the load distribution. The exact reduction in the tail and the corresponding capacity savings depend on the percentile of the load distribution chosen for provisioning networks and servers\&mdash;the higher the percentile, the smaller the savings. In particular, compared to over a 50\% reduction in the average load, the savings in network and server capacity is only 20-35\% for the 99<supscrpt>th</supscrpt> percentile of the load distribution. We also find that while proxies can be somewhat useful in smoothing out some of the burstiness in web workloads; the resulting workload continues, however, to exhibit substantial burstiness and a heavy-tailed nature. We identify large objects with poor locality to be the limiting factor that diminishes the impact of proxies on the tail of load distribution. We conclude that, while proxies are immensely useful to users due to the reduction in the average response time, they are less effective in improving the capacities of networks and servers.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339357},
 doi = {http://doi.acm.org/10.1145/339331.339357},
 acmid = {339357},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raunak:2000:IPC:345063.339357,
 author = {Raunak, Mohammad S. and Shenoy, Prashant and Goyal, Pawan and Ramamritham, Krithi},
 title = {Implications of proxy caching for provisioning networks and servers},
 abstract = {In this paper, we examine the potential benefits of web proxy caches in improving the effective capacity of servers and networks. Since networks and servers are typically provisioned based on a high percentile of the load, we focus on the effects of proxy caching on the tail of the load distribution. We find that, unlike their substantial impact on the average load, proxies have a diminished impact on the tail of the load distribution. The exact reduction in the tail and the corresponding capacity savings depend on the percentile of the load distribution chosen for provisioning networks and servers\&mdash;the higher the percentile, the smaller the savings. In particular, compared to over a 50\% reduction in the average load, the savings in network and server capacity is only 20-35\% for the 99<supscrpt>th</supscrpt> percentile of the load distribution. We also find that while proxies can be somewhat useful in smoothing out some of the burstiness in web workloads; the resulting workload continues, however, to exhibit substantial burstiness and a heavy-tailed nature. We identify large objects with poor locality to be the limiting factor that diminishes the impact of proxies on the tail of load distribution. We conclude that, while proxies are immensely useful to users due to the reduction in the average response time, they are less effective in improving the capacities of networks and servers.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339357},
 doi = {http://doi.acm.org/10.1145/345063.339357},
 acmid = {339357},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yang:2000:CWC:339331.339360,
 author = {Yang, Jiong and Wang, Wei and Muntz, Richard},
 title = {Collaborative Web caching based on proxy affinities},
 abstract = {With the exponential growth of hosts and traffic workloads on the Internet, collaborative web caching has been recognized as an efficient solution to alleviate web page server bottlenecks and reduce traffic. However, cache discovery, i.e., locating where a page is cached, is a challenging problem, especially in the fast growing World Wide Web environment, where the number of participating proxies can be very large. In this paper, we propose a new scheme which employs proxy affinities to maintain a dynamic distributed collaborative caching infrastructure. Web pages are partitioned into clusters according to proxy reference patterns. All proxies which frequently access some page(s) in the same web page cluster form an ``information group". When web pages belonging to a web page cluster are deleted from or added into a proxy's cache, only proxies in the associated information group are notified. This scheme can be shown to greatly reduce the number of messages and other overhead on individual proxies while maintaining a high cache hit rate. Finally, we employ trace driven simulation to evaluate our web caching scheme using three web access trace logs to verify that our caching structure can provide significant benefits on real workloads.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {78--89},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339360},
 doi = {http://doi.acm.org/10.1145/339331.339360},
 acmid = {339360},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yang:2000:CWC:345063.339360,
 author = {Yang, Jiong and Wang, Wei and Muntz, Richard},
 title = {Collaborative Web caching based on proxy affinities},
 abstract = {With the exponential growth of hosts and traffic workloads on the Internet, collaborative web caching has been recognized as an efficient solution to alleviate web page server bottlenecks and reduce traffic. However, cache discovery, i.e., locating where a page is cached, is a challenging problem, especially in the fast growing World Wide Web environment, where the number of participating proxies can be very large. In this paper, we propose a new scheme which employs proxy affinities to maintain a dynamic distributed collaborative caching infrastructure. Web pages are partitioned into clusters according to proxy reference patterns. All proxies which frequently access some page(s) in the same web page cluster form an ``information group". When web pages belonging to a web page cluster are deleted from or added into a proxy's cache, only proxies in the associated information group are notified. This scheme can be shown to greatly reduce the number of messages and other overhead on individual proxies while maintaining a high cache hit rate. Finally, we employ trace driven simulation to evaluate our web caching scheme using three web access trace logs to verify that our caching structure can provide significant benefits on real workloads.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {78--89},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339360},
 doi = {http://doi.acm.org/10.1145/345063.339360},
 acmid = {339360},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aron:2000:CRM:345063.339383,
 author = {Aron, Mohit and Druschel, Peter and Zwaenepoel, Willy},
 title = {Cluster reserves: a mechanism for resource management in cluster-based network servers},
 abstract = {In network (e.g., Web) servers, it is often desirable to isolate the performance of different classes of requests from each other. That is, one seeks to achieve that a certain minimal proportion of server resources are available for a class of requests, independent of the load imposed by other requests. Recent work demonstrates how to achieve this performance isolation in servers consisting of a single, centralized node; however, achieving performance isolation in a distributed, cluster based server remains a problem.This paper introduces a new abstraction, the cluster reserve, which represents a resource principal in a cluster based network server. We present a design and evaluate a prototype implementation that extends existing techniques for performance isolation on a single node server to cluster based servers.In our design, the dynamic cluster-wide resource management problem is formulated as a constrained optimization problem, with the resource allocations on individual machines as independent variables, and the desired cluster-wide resource allocations as constraints. Periodically collected resource usages serve as further inputs to the problem.Experimental results show that cluster reserves are effective in providing performance isolation in cluster based servers. We demonstrate that, in a number of different scenarios, cluster reserves are effective in ensuring performance isolation while enabling high utilization of the server resources.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {90--101},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339383},
 doi = {http://doi.acm.org/10.1145/345063.339383},
 acmid = {339383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Aron:2000:CRM:339331.339383,
 author = {Aron, Mohit and Druschel, Peter and Zwaenepoel, Willy},
 title = {Cluster reserves: a mechanism for resource management in cluster-based network servers},
 abstract = {In network (e.g., Web) servers, it is often desirable to isolate the performance of different classes of requests from each other. That is, one seeks to achieve that a certain minimal proportion of server resources are available for a class of requests, independent of the load imposed by other requests. Recent work demonstrates how to achieve this performance isolation in servers consisting of a single, centralized node; however, achieving performance isolation in a distributed, cluster based server remains a problem.This paper introduces a new abstraction, the cluster reserve, which represents a resource principal in a cluster based network server. We present a design and evaluate a prototype implementation that extends existing techniques for performance isolation on a single node server to cluster based servers.In our design, the dynamic cluster-wide resource management problem is formulated as a constrained optimization problem, with the resource allocations on individual machines as independent variables, and the desired cluster-wide resource allocations as constraints. Periodically collected resource usages serve as further inputs to the problem.Experimental results show that cluster reserves are effective in providing performance isolation in cluster based servers. We demonstrate that, in a number of different scenarios, cluster reserves are effective in ensuring performance isolation while enabling high utilization of the server resources.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {90--101},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339383},
 doi = {http://doi.acm.org/10.1145/339331.339383},
 acmid = {339383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barakat:2000:APS:339331.339388,
 author = {Barakat, Chadi and Altman, Eitan},
 title = {Analysis of the phenomenon of several slow start phases in TCP (poster session)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {102--103},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339388},
 doi = {http://doi.acm.org/10.1145/339331.339388},
 acmid = {339388},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barakat:2000:APS:345063.339388,
 author = {Barakat, Chadi and Altman, Eitan},
 title = {Analysis of the phenomenon of several slow start phases in TCP (poster session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {102--103},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339388},
 doi = {http://doi.acm.org/10.1145/345063.339388},
 acmid = {339388},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wong:2000:PGQ:339331.339389,
 author = {Wong, Wai-Man R. and Muntz, Richard R.},
 title = {Providing guaranteed quality of service for interactive visualization applications (poster session)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {104--105},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339389},
 doi = {http://doi.acm.org/10.1145/339331.339389},
 acmid = {339389},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wong:2000:PGQ:345063.339389,
 author = {Wong, Wai-Man R. and Muntz, Richard R.},
 title = {Providing guaranteed quality of service for interactive visualization applications (poster session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {104--105},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339389},
 doi = {http://doi.acm.org/10.1145/345063.339389},
 acmid = {339389},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:2000:IMF:339331.339390,
 author = {Wang, Xin and Yu, C. and Schulzrinne, Henning and Stirpe, Paul and Wu, Wei},
 title = {IP multicast fault recovery in PIM over OSPF (poster session)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {106--107},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339390},
 doi = {http://doi.acm.org/10.1145/339331.339390},
 acmid = {339390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:2000:IMF:345063.339390,
 author = {Wang, Xin and Yu, C. and Schulzrinne, Henning and Stirpe, Paul and Wu, Wei},
 title = {IP multicast fault recovery in PIM over OSPF (poster session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {106--107},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339390},
 doi = {http://doi.acm.org/10.1145/345063.339390},
 acmid = {339390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lety:2000:CMG:339331.339392,
 author = {L\'{e}ty, Emmanuel and Turletti, Thierry and Baccelli, Fran\c{c}ois},
 title = {Cell-based multicast grouping in large-scale virtual environments (poster session) (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {108--109},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339392},
 doi = {http://doi.acm.org/10.1145/339331.339392},
 acmid = {339392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lety:2000:CMG:345063.339392,
 author = {L\'{e}ty, Emmanuel and Turletti, Thierry and Baccelli, Fran\c{c}ois},
 title = {Cell-based multicast grouping in large-scale virtual environments (poster session) (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {108--109},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339392},
 doi = {http://doi.acm.org/10.1145/345063.339392},
 acmid = {339392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jin:2000:TLW:345063.339393,
 author = {Jin, Shudong and Bestavros, Azer},
 title = {Temporal locality in Web request streams (poster session)  (extended abstract): sources, characteristics, and caching implications},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {110--111},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339393},
 doi = {http://doi.acm.org/10.1145/345063.339393},
 acmid = {339393},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jin:2000:TLW:339331.339393,
 author = {Jin, Shudong and Bestavros, Azer},
 title = {Temporal locality in Web request streams (poster session)  (extended abstract): sources, characteristics, and caching implications},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {110--111},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339393},
 doi = {http://doi.acm.org/10.1145/339331.339393},
 acmid = {339393},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schindler:2000:ADD:339331.339397,
 author = {Schindler, Jiri and Ganger, Gregory R.},
 title = {Automated disk drive characterization (poster session)},
 abstract = {DIXtrac is a program that automatically characterizes the performance of modern disk drives. This extended abstract overviews the contents of [3], which describes and validates DIXtrac's algorithms for extracting accurate values for over 100 performance-critical parameters in 2-6 minutes without human intervention or special hardware support. The extracted data includes detailed layout and geometry information, mechanical timings, cache management policies, and command processing overheads. DIXtrac is validated by configuring a detailed disk simulator with its extracted parameters; in most cases, the resulting accuracies match those of the most accurate disk simulators reported in the literature. To date, DIXtrac has been successfully used on ten different models from four different manufacturers. A growing database of validated disk characteristics is available in DiskSim [1] format at http://www.ece.cmu.edu/~ganger/disksim/diskspecs.html.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {112--113},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339397},
 doi = {http://doi.acm.org/10.1145/339331.339397},
 acmid = {339397},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schindler:2000:ADD:345063.339397,
 author = {Schindler, Jiri and Ganger, Gregory R.},
 title = {Automated disk drive characterization (poster session)},
 abstract = {DIXtrac is a program that automatically characterizes the performance of modern disk drives. This extended abstract overviews the contents of [3], which describes and validates DIXtrac's algorithms for extracting accurate values for over 100 performance-critical parameters in 2-6 minutes without human intervention or special hardware support. The extracted data includes detailed layout and geometry information, mechanical timings, cache management policies, and command processing overheads. DIXtrac is validated by configuring a detailed disk simulator with its extracted parameters; in most cases, the resulting accuracies match those of the most accurate disk simulators reported in the literature. To date, DIXtrac has been successfully used on ten different models from four different manufacturers. A growing database of validated disk characteristics is available in DiskSim [1] format at http://www.ece.cmu.edu/~ganger/disksim/diskspecs.html.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {112--113},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339397},
 doi = {http://doi.acm.org/10.1145/345063.339397},
 acmid = {339397},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fang:2000:OSP:345063.339398,
 author = {Fang, Zhen and Zhang, Lixin and Carter, John and McKee, Sally and Hsieh, Wilson},
 title = {Online superpage promotion revisited (poster session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {114--115},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339398},
 doi = {http://doi.acm.org/10.1145/345063.339398},
 acmid = {339398},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fang:2000:OSP:339331.339398,
 author = {Fang, Zhen and Zhang, Lixin and Carter, John and McKee, Sally and Hsieh, Wilson},
 title = {Online superpage promotion revisited (poster session)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {114--115},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339398},
 doi = {http://doi.acm.org/10.1145/339331.339398},
 acmid = {339398},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nikolaidis:2000:ILB:345063.339400,
 author = {Nikolaidis, Ioanis and Li, Fulu and Hu, Ailan},
 title = {An inherently loss-less and bandwidth-efficient periodic broadcast scheme for VBR video (poster session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {116--117},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339400},
 doi = {http://doi.acm.org/10.1145/345063.339400},
 acmid = {339400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nikolaidis:2000:ILB:339331.339400,
 author = {Nikolaidis, Ioanis and Li, Fulu and Hu, Ailan},
 title = {An inherently loss-less and bandwidth-efficient periodic broadcast scheme for VBR video (poster session)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {116--117},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339400},
 doi = {http://doi.acm.org/10.1145/339331.339400},
 acmid = {339400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Koksal:2000:ASF:339331.339401,
 author = {Koksal, Can Emre and Kassab, Hisham and Balakrishnan, Hari},
 title = {An analysis of short-term fairness in wireless media access protocols (poster session)},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {118--119},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339401},
 doi = {http://doi.acm.org/10.1145/339331.339401},
 acmid = {339401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Koksal:2000:ASF:345063.339401,
 author = {Koksal, Can Emre and Kassab, Hisham and Balakrishnan, Hari},
 title = {An analysis of short-term fairness in wireless media access protocols (poster session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {118--119},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339401},
 doi = {http://doi.acm.org/10.1145/345063.339401},
 acmid = {339401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Joshi:2000:RDH:345063.339403,
 author = {Joshi, Srinath R. and Rhee, Injong},
 title = {RESCU: dynamic hybrid packet-loss recovery for video transmission over the Internet (poster session)},
 abstract = {The current Internet is not reliable; packet loss rates are frequently high, and varying over time. Transmitting high-quality interactive video over the Internet is challenging because the quality of compressed video is very susceptible to packet losses. Loss of packets belonging to a video frame manifests itself not only in the reduced quality of that frame but also in the propagation of that distortion to successive frames. This error propagation problem is inherent in many motion-based video codecs due to the interdependence of encoded video frames. This paper presents a dynamic loss recovery scheme, called RESCU, to address the error propagation problem. In this new scheme, picture coding patterns are dynamically adapted to current network conditions in order to maximize the effectiveness of hybrid transport level recovery (employing both forward error correction and retransmission) in reducing error propagation. Since RESCU does not introduce any playout delay at the receiver, it is suitable for interactive video communication. An experimental study based on actual Internet transmission traces representing various network conditions shows that dynamic hybrid RESCU exhibits better error resilience and incurs much less bit overhead than existing error recovery techniques such as NEWPRED and Intra-H.261.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {120--121},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339403},
 doi = {http://doi.acm.org/10.1145/345063.339403},
 acmid = {339403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Joshi:2000:RDH:339331.339403,
 author = {Joshi, Srinath R. and Rhee, Injong},
 title = {RESCU: dynamic hybrid packet-loss recovery for video transmission over the Internet (poster session)},
 abstract = {The current Internet is not reliable; packet loss rates are frequently high, and varying over time. Transmitting high-quality interactive video over the Internet is challenging because the quality of compressed video is very susceptible to packet losses. Loss of packets belonging to a video frame manifests itself not only in the reduced quality of that frame but also in the propagation of that distortion to successive frames. This error propagation problem is inherent in many motion-based video codecs due to the interdependence of encoded video frames. This paper presents a dynamic loss recovery scheme, called RESCU, to address the error propagation problem. In this new scheme, picture coding patterns are dynamically adapted to current network conditions in order to maximize the effectiveness of hybrid transport level recovery (employing both forward error correction and retransmission) in reducing error propagation. Since RESCU does not introduce any playout delay at the receiver, it is suitable for interactive video communication. An experimental study based on actual Internet transmission traces representing various network conditions shows that dynamic hybrid RESCU exhibits better error resilience and incurs much less bit overhead than existing error recovery techniques such as NEWPRED and Intra-H.261.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {120--121},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339403},
 doi = {http://doi.acm.org/10.1145/339331.339403},
 acmid = {339403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Padmanabhan:2000:CAD:345063.339405,
 author = {Padmanabhan, Venkata N. and Qiu, Lili},
 title = {The content and access dynamics of a busy Web server (poster session)},
 abstract = {We study the MSNBC Web site, one of the busiest in the Internet today. We analyze the dynamics of content creation and modification as well as client accesses. Our key findings are (a) files tend to change little upon modification, (b) a small set of files get modified repeatedly, (c) file popularity follows a Zipf-like distribution with an \&agr; much larger than reported in previous, proxy-based studies, and (d) there is significant temporal stability in file popularity but not much stability in the domains from which popular content is accessed. We discuss implications of these findings.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {122--123},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/345063.339405},
 doi = {http://doi.acm.org/10.1145/345063.339405},
 acmid = {339405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Padmanabhan:2000:CAD:339331.339405,
 author = {Padmanabhan, Venkata N. and Qiu, Lili},
 title = {The content and access dynamics of a busy Web server (poster session)},
 abstract = {We study the MSNBC Web site, one of the busiest in the Internet today. We analyze the dynamics of content creation and modification as well as client accesses. Our key findings are (a) files tend to change little upon modification, (b) a small set of files get modified repeatedly, (c) file popularity follows a Zipf-like distribution with an \&agr; much larger than reported in previous, proxy-based studies, and (d) there is significant temporal stability in file popularity but not much stability in the domains from which popular content is accessed. We discuss implications of these findings.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {122--123},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/339331.339405},
 doi = {http://doi.acm.org/10.1145/339331.339405},
 acmid = {339405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Altman:2000:TPB:345063.350541,
 author = {Altman, Eitan and Avrachenkov, Konstantin and Barakat, Chadi},
 title = {TCP in presence of bursty losses},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {124--133},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.350541},
 doi = {http://doi.acm.org/10.1145/345063.350541},
 acmid = {350541},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Altman:2000:TPB:339331.350541,
 author = {Altman, Eitan and Avrachenkov, Konstantin and Barakat, Chadi},
 title = {TCP in presence of bursty losses},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {124--133},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.350541},
 doi = {http://doi.acm.org/10.1145/339331.350541},
 acmid = {350541},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martin:2000:IDR:345063.339408,
 author = {Martin, Jim and Nilsson, Arne and Rhee, Injong},
 title = {The incremental deployability of RTT-based congestion avoidance for high speed TCP Internet connections},
 abstract = {Our research focuses on end-to-end congestion avoidance algorithms that use round trip time (RTT) fluctuations as an indicator of the level of network congestion. The algorithms are referred to as delay-based congestion avoidance or DCA. Due to the economics associated with deploying change within an existing network, we are interested in an incrementally deployable enhancement to the TCP/Reno protocol. For instance, TCP/Vegas, a DCA algorithm, has been proposed as an incremental enhancement. Requiring relatively minor modifications to a TCP sender, TCP/Vegas has been shown to increase end-to-end TCP throughput primarily by avoiding packet loss. We study DCA in today's best effort Internet where IP switches are subject to thousands of TCP flows resulting in congestion with time scales that span orders of magnitude. Our results suggest that RTT-based congestion avoidance may not be reliably incrementally deployed in this environment. Through extensive measurement and simulation, we find that when TCP/DCA (i.e., a TCP/Reno sender that is extended with DCA) is deployed over a high speed Internet path, the flow generally experiences degraded throughput compared to an unmodified TCP/Reno flow. We show (1) that the congestion information contained in RTT samples is not sufficient to predict packet loss reliably and (2) that the congestion avoidance in response to delay increase has minimal impact on the congestion level over the path when the total DCA traffic at the bottleneck consumes less than 10\% of the bottleneck bandwidth.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {134--144},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339408},
 doi = {http://doi.acm.org/10.1145/345063.339408},
 acmid = {339408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RTT measurement, TCP, congestion avoidance},
} 

@inproceedings{Martin:2000:IDR:339331.339408,
 author = {Martin, Jim and Nilsson, Arne and Rhee, Injong},
 title = {The incremental deployability of RTT-based congestion avoidance for high speed TCP Internet connections},
 abstract = {Our research focuses on end-to-end congestion avoidance algorithms that use round trip time (RTT) fluctuations as an indicator of the level of network congestion. The algorithms are referred to as delay-based congestion avoidance or DCA. Due to the economics associated with deploying change within an existing network, we are interested in an incrementally deployable enhancement to the TCP/Reno protocol. For instance, TCP/Vegas, a DCA algorithm, has been proposed as an incremental enhancement. Requiring relatively minor modifications to a TCP sender, TCP/Vegas has been shown to increase end-to-end TCP throughput primarily by avoiding packet loss. We study DCA in today's best effort Internet where IP switches are subject to thousands of TCP flows resulting in congestion with time scales that span orders of magnitude. Our results suggest that RTT-based congestion avoidance may not be reliably incrementally deployed in this environment. Through extensive measurement and simulation, we find that when TCP/DCA (i.e., a TCP/Reno sender that is extended with DCA) is deployed over a high speed Internet path, the flow generally experiences degraded throughput compared to an unmodified TCP/Reno flow. We show (1) that the congestion information contained in RTT samples is not sufficient to predict packet loss reliably and (2) that the congestion avoidance in response to delay increase has minimal impact on the congestion level over the path when the total DCA traffic at the bottleneck consumes less than 10\% of the bottleneck bandwidth.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {134--144},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339408},
 doi = {http://doi.acm.org/10.1145/339331.339408},
 acmid = {339408},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RTT measurement, TCP, congestion avoidance},
} 

@article{Rubenstein:2000:DSC:345063.339410,
 author = {Rubenstein, Dan and Kurose, Jim and Towsley, Don},
 title = {Detecting shared congestion of flows via end-to-end measurement},
 abstract = {Current Internet congestion control protocols operate independently on a per-flow basis. Recent work has demonstrated that cooperative congestion control strategies between flows can improve performance for a variety of applications, ranging from aggregated TCP transmissions to multiple-sender multicast applications. However, in order for this cooperation to be effective, one must first identify the flows that are congested at the same set of resources. In this paper, we present techniques based on loss or delay observations at end-hosts to infer whether or not two flows experiencing congestion are congested at the same network resources. We validate these techniques via queueing analysis, simulation, and experimentation within the Internet.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {145--155},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339410},
 doi = {http://doi.acm.org/10.1145/345063.339410},
 acmid = {339410},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rubenstein:2000:DSC:339331.339410,
 author = {Rubenstein, Dan and Kurose, Jim and Towsley, Don},
 title = {Detecting shared congestion of flows via end-to-end measurement},
 abstract = {Current Internet congestion control protocols operate independently on a per-flow basis. Recent work has demonstrated that cooperative congestion control strategies between flows can improve performance for a variety of applications, ranging from aggregated TCP transmissions to multiple-sender multicast applications. However, in order for this cooperation to be effective, one must first identify the flows that are congested at the same set of resources. In this paper, we present techniques based on loss or delay observations at end-hosts to infer whether or not two flows experiencing congestion are congested at the same network resources. We validate these techniques via queueing analysis, simulation, and experimentation within the Internet.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {145--155},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339410},
 doi = {http://doi.acm.org/10.1145/339331.339410},
 acmid = {339410},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:2000:MAL:345063.339412,
 author = {Wang, Xin and Schulzrinne, Henning and Kandlur, Dilip and Verma, Dinesh},
 title = {Measurement and analysis of LDAP performance},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {156--165},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339412},
 doi = {http://doi.acm.org/10.1145/345063.339412},
 acmid = {339412},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:2000:MAL:339331.339412,
 author = {Wang, Xin and Schulzrinne, Henning and Kandlur, Dilip and Verma, Dinesh},
 title = {Measurement and analysis of LDAP performance},
 abstract = {},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {156--165},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339412},
 doi = {http://doi.acm.org/10.1145/339331.339412},
 acmid = {339412},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cleveland:2000:IPG:345063.339413,
 author = {Cleveland, William S. and Lin, Dong and Sun, Don X.},
 title = {IP packet generation: statistical models for TCP start times based on connection-rate superposition},
 abstract = {TCP start times for HTTP are nonstationary. The nonstationarity occurs because the start times on a link, a point process, are a superposition of source traffic point processes, and the statistics of superposition changes as the number of superposed processes changes. The start time rate is a measure of the number of traffic sources. The univariate distribution of the inter-arrival times is approximately Weibull, and as the rate increases, the Weibull shape parameter goes to 1, an exponential distribution. The autocorrelation of the log inter-arrival times is described by a simple, two-parameter process: white noise plus a long-range persistent time series. As the rate increases, the variance of the persistent series tends to zero, so the log times tend to white noise. A parsimonious statistical model for log inter-arrivals accounts for the autocorrelation, the Weibull distribution, and the nonstationarity in the two with the rate. The model, whose purpose is to provide stochastic input to a network simulator, has the desirable property that the superposition point process is generated as a single stream. The parameters of the model are functions of the rate, so to generate start times, only the rate is specified. As the rate increases, the model tends to a Poisson process. These results arise from theoretical and empirical study based on the concept of connection-rate superposition. The theory is the mathematics of superposed point processes, and the empiricism is an analysis of 23 million TCP connections organized into 10704 blocks of approximately 15 minutes each.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {166--177},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339413},
 doi = {http://doi.acm.org/10.1145/345063.339413},
 acmid = {339413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cleveland:2000:IPG:339331.339413,
 author = {Cleveland, William S. and Lin, Dong and Sun, Don X.},
 title = {IP packet generation: statistical models for TCP start times based on connection-rate superposition},
 abstract = {TCP start times for HTTP are nonstationary. The nonstationarity occurs because the start times on a link, a point process, are a superposition of source traffic point processes, and the statistics of superposition changes as the number of superposed processes changes. The start time rate is a measure of the number of traffic sources. The univariate distribution of the inter-arrival times is approximately Weibull, and as the rate increases, the Weibull shape parameter goes to 1, an exponential distribution. The autocorrelation of the log inter-arrival times is described by a simple, two-parameter process: white noise plus a long-range persistent time series. As the rate increases, the variance of the persistent series tends to zero, so the log times tend to white noise. A parsimonious statistical model for log inter-arrivals accounts for the autocorrelation, the Weibull distribution, and the nonstationarity in the two with the rate. The model, whose purpose is to provide stochastic input to a network simulator, has the desirable property that the superposition point process is generated as a single stream. The parameters of the model are functions of the rate, so to generate start times, only the rate is specified. As the rate increases, the model tends to a Poisson process. These results arise from theoretical and empirical study based on the concept of connection-rate superposition. The theory is the mathematics of superposed point processes, and the empiricism is an analysis of 23 million TCP connections organized into 10704 blocks of approximately 15 minutes each.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {166--177},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339413},
 doi = {http://doi.acm.org/10.1145/339331.339413},
 acmid = {339413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hegde:2000:ISH:345063.339414,
 author = {Hegde, Nidhi and Sohraby, Khosrow},
 title = {On the impact of soft hand-off in cellular systems},
 abstract = {We present a model for soft, hand-off in wireless cellular networks. In such networks, due to overlapping cells, hand-offs are not instantaneous and multiple channels may be occupied by a single mobile for a non-zero freeze time period.We provide a mathematical model of wireless cellular networks with soft hand-offs. We examine different performance measures and show that freeze time may have a major impact on the system performance if the mobility rate is not negligible. Both exact and approximate formulations are given. Different fixed-point approximation methods are used to reduce the complexity of the exact solution. Various performance measures such as new and hand-off blocking and probability of a call dropout are carefully examined.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339414},
 doi = {http://doi.acm.org/10.1145/345063.339414},
 acmid = {339414},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hegde:2000:ISH:339331.339414,
 author = {Hegde, Nidhi and Sohraby, Khosrow},
 title = {On the impact of soft hand-off in cellular systems},
 abstract = {We present a model for soft, hand-off in wireless cellular networks. In such networks, due to overlapping cells, hand-offs are not instantaneous and multiple channels may be occupied by a single mobile for a non-zero freeze time period.We provide a mathematical model of wireless cellular networks with soft hand-offs. We examine different performance measures and show that freeze time may have a major impact on the system performance if the mobility rate is not negligible. Both exact and approximate formulations are given. Different fixed-point approximation methods are used to reduce the complexity of the exact solution. Various performance measures such as new and hand-off blocking and probability of a call dropout are carefully examined.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339414},
 doi = {http://doi.acm.org/10.1145/339331.339414},
 acmid = {339414},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shakkottai:2000:DAP:339331.339415,
 author = {Shakkottai, Sanjay and Srikant, R.},
 title = {Delay asymptotics for a priority queueing system},
 abstract = {In this paper, we study discrete-time priority queueing systems fed by a large number of arrival streams. We first provide bounds on the actual delay asymptote in terms of the virtual delay asymptote. Then, under suitable assumptions on the arrival process to the queue, we show that these asymptotes are the same. We then consider a priority queueing system with two queues. Using the earlier result, we derive an upper bound on the tail probability of the delay. Under certain assumptions on the rate function of the arrival process, we show that the upper bound is tight. We then consider a system with Markovian arrivals and numerically evaluate the delay tail probability and validate these results with simulations.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {188--195},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/339331.339415},
 doi = {http://doi.acm.org/10.1145/339331.339415},
 acmid = {339415},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shakkottai:2000:DAP:345063.339415,
 author = {Shakkottai, Sanjay and Srikant, R.},
 title = {Delay asymptotics for a priority queueing system},
 abstract = {In this paper, we study discrete-time priority queueing systems fed by a large number of arrival streams. We first provide bounds on the actual delay asymptote in terms of the virtual delay asymptote. Then, under suitable assumptions on the arrival process to the queue, we show that these asymptotes are the same. We then consider a priority queueing system with two queues. Using the earlier result, we derive an upper bound on the tail probability of the delay. Under certain assumptions on the rate function of the arrival process, we show that the upper bound is tight. We then consider a system with Markovian arrivals and numerically evaluate the delay tail probability and validate these results with simulations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {188--195},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/345063.339415},
 doi = {http://doi.acm.org/10.1145/345063.339415},
 acmid = {339415},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Golubchik:2000:FAI:345063.339416,
 author = {Golubchik, Leana and Lui, John C. S.},
 title = {A fast and accurate iterative solution of a multi-class threshold-based queueing system with hysteresis},
 abstract = {Our main goal in this work is to develop an efficient method for solving such models and computing the corresponding performance measures of interest, which can subsequently be used in evaluating designs of threshold-based systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {196--206},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339416},
 doi = {http://doi.acm.org/10.1145/345063.339416},
 acmid = {339416},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Golubchik:2000:FAI:339331.339416,
 author = {Golubchik, Leana and Lui, John C. S.},
 title = {A fast and accurate iterative solution of a multi-class threshold-based queueing system with hysteresis},
 abstract = {Our main goal in this work is to develop an efficient method for solving such models and computing the corresponding performance measures of interest, which can subsequently be used in evaluating designs of threshold-based systems.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {196--206},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339416},
 doi = {http://doi.acm.org/10.1145/339331.339416},
 acmid = {339416},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Miner:2000:UES:339331.339417,
 author = {Miner, Andrew S. and Ciardo, Gianfranco and Donatelli, Susanna},
 title = {Using the exact state space of a Markov model to compute approximate stationary measures},
 abstract = {We present a new approximation algorithm based on an exact representation of the state space S, using decision diagrams, and of the transition rate matrix R, using Kronecker algebra, for a Markov model with K submodels. Our algorithm builds and solves K Markov chains, each corresponding to a different aggregation of the exact process, guided by the structure of the decision diagram, and iterates on their solution until their entries are stable. We prove that exact results are obtained if the overall model has a product-form solution. Advantages of our method include good accuracy, low memory requirements, fast execution times, and a high degree of automation, since the only additional information required to apply it is a partition of the model into the K submodels. As far as we know, this is the first time an approximation algorithm has been proposed where knowledge of the exact state space is explicitly used.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339417},
 doi = {http://doi.acm.org/10.1145/339331.339417},
 acmid = {339417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Miner:2000:UES:345063.339417,
 author = {Miner, Andrew S. and Ciardo, Gianfranco and Donatelli, Susanna},
 title = {Using the exact state space of a Markov model to compute approximate stationary measures},
 abstract = {We present a new approximation algorithm based on an exact representation of the state space S, using decision diagrams, and of the transition rate matrix R, using Kronecker algebra, for a Markov model with K submodels. Our algorithm builds and solves K Markov chains, each corresponding to a different aggregation of the exact process, guided by the structure of the decision diagram, and iterates on their solution until their entries are stable. We prove that exact results are obtained if the overall model has a product-form solution. Advantages of our method include good accuracy, low memory requirements, fast execution times, and a high degree of automation, since the only additional information required to apply it is a partition of the model into the K submodels. As far as we know, this is the first time an approximation algorithm has been proposed where knowledge of the exact state space is explicitly used.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339417},
 doi = {http://doi.acm.org/10.1145/345063.339417},
 acmid = {339417},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eager:2000:ATH:339331.339418,
 author = {Eager, Derek L. and Sorin, Daniel J. and Vernon, Mary K.},
 title = {AMVA techniques for high service time variability},
 abstract = {Motivated by experience gained during the validation of a recent Approximate Mean Value Analysis (AMVA) model of modern shared memory architectures, this paper re-examines the ``standard" AMVA approximation for non-exponential FCFS queues. We find that this approximation is often inaccurate for FCFS queues with high service time variability. For such queues, we propose and evaluate: (1) AMVA estimates of the mean residual service time at an arrival instant that are much more accurate than the standard AMVA estimate, (2) a new AMVA technique that provides a much more accurate estimate of mean center residence time than the standard AMVA estimate, and (3) a new AMVA technique for computing the mean residence time at a ``downstream" queue which has a more bursty arrival process than is assumed in the standard AMVA equations. Together, these new techniques increase the range of applications to which AMVA may be fruitfully applied, so that for example, the memory system architecture of shared memory systems with complex modern processors can be analyzed with these computationally efficient methods.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339418},
 doi = {http://doi.acm.org/10.1145/339331.339418},
 acmid = {339418},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eager:2000:ATH:345063.339418,
 author = {Eager, Derek L. and Sorin, Daniel J. and Vernon, Mary K.},
 title = {AMVA techniques for high service time variability},
 abstract = {Motivated by experience gained during the validation of a recent Approximate Mean Value Analysis (AMVA) model of modern shared memory architectures, this paper re-examines the ``standard" AMVA approximation for non-exponential FCFS queues. We find that this approximation is often inaccurate for FCFS queues with high service time variability. For such queues, we propose and evaluate: (1) AMVA estimates of the mean residual service time at an arrival instant that are much more accurate than the standard AMVA estimate, (2) a new AMVA technique that provides a much more accurate estimate of mean center residence time than the standard AMVA estimate, and (3) a new AMVA technique for computing the mean residence time at a ``downstream" queue which has a more bursty arrival process than is assumed in the standard AMVA equations. Together, these new techniques increase the range of applications to which AMVA may be fruitfully applied, so that for example, the memory system architecture of shared memory systems with complex modern processors can be analyzed with these computationally efficient methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339418},
 doi = {http://doi.acm.org/10.1145/345063.339418},
 acmid = {339418},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ofelt:2000:EPP:339331.339419,
 author = {Ofelt, David and Hennessy, John L.},
 title = {Efficient performance prediction for modern microprocessors},
 abstract = {Generating an accurate estimate of the performance of a program on a given system is important to a large number of people. Computer architects, compiler writers, and developers all need insight into a machine's performance. There are a number of performance estimation techniques in use, from profile-based approaches to full machine simulation. This paper discusses a profile-based performance estimation technique that uses a lightweight instrumentation phase that runs in order number of dynamic instructions, followed by an analysis phase that runs in roughly order number of static instructions. This technique accurately predicts the performance of the core pipeline of a detailed out-of-order issue processor model while scheduling far fewer instructions than does full simulation. The difference between the predicted execution time and the time obtained from full simulation is only a few percent.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {229--239},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339419},
 doi = {http://doi.acm.org/10.1145/339331.339419},
 acmid = {339419},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ofelt:2000:EPP:345063.339419,
 author = {Ofelt, David and Hennessy, John L.},
 title = {Efficient performance prediction for modern microprocessors},
 abstract = {Generating an accurate estimate of the performance of a program on a given system is important to a large number of people. Computer architects, compiler writers, and developers all need insight into a machine's performance. There are a number of performance estimation techniques in use, from profile-based approaches to full machine simulation. This paper discusses a profile-based performance estimation technique that uses a lightweight instrumentation phase that runs in order number of dynamic instructions, followed by an analysis phase that runs in roughly order number of static instructions. This technique accurately predicts the performance of the core pipeline of a detailed out-of-order issue processor model while scheduling far fewer instructions than does full simulation. The difference between the predicted execution time and the time obtained from full simulation is only a few percent.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {229--239},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339419},
 doi = {http://doi.acm.org/10.1145/345063.339419},
 acmid = {339419},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Endo:2000:IIP:345063.339420,
 author = {Endo, Yasuhiro and Seltzer, Margo},
 title = {Improving interactive performance using TIPME},
 abstract = {On the vast majority of today's computers, the dominant form of computation is GUI-based user interaction. In such an environment, the user's perception is the final arbiter of performance. Human-factors research shows that a user's perception of performance is affected by unexpectedly long delays. However, most performance-tuning techniques currently rely on throughput-sensitive benchmarks. While these techniques improve the average performance of the system, they do little to detect or eliminate response-time variabilities\&mdash;in particular, unexpectedly long delays.We introduce a measurement infrastructure that allows us to improve user-perceived performance by helping us to identify and eliminate the causes of the unexpected long response times that users find unacceptable. We describe TIPME (The Interactive Performance Monitoring Environment), a collection of measurement tools that allowed us to quickly and easily diagnose interactive performance ``bugs" in a mature operating system. We present two case studies that demonstrate the effectiveness of our measurement infrastructure. Each of the performance problems we identify drastically affects variability in response time in a mature system, demonstrating that current tuning techniques do not address this class of performance problems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {240--251},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339420},
 doi = {http://doi.acm.org/10.1145/345063.339420},
 acmid = {339420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interactive performance, monitoring},
} 

@inproceedings{Endo:2000:IIP:339331.339420,
 author = {Endo, Yasuhiro and Seltzer, Margo},
 title = {Improving interactive performance using TIPME},
 abstract = {On the vast majority of today's computers, the dominant form of computation is GUI-based user interaction. In such an environment, the user's perception is the final arbiter of performance. Human-factors research shows that a user's perception of performance is affected by unexpectedly long delays. However, most performance-tuning techniques currently rely on throughput-sensitive benchmarks. While these techniques improve the average performance of the system, they do little to detect or eliminate response-time variabilities\&mdash;in particular, unexpectedly long delays.We introduce a measurement infrastructure that allows us to improve user-perceived performance by helping us to identify and eliminate the causes of the unexpected long response times that users find unacceptable. We describe TIPME (The Interactive Performance Monitoring Environment), a collection of measurement tools that allowed us to quickly and easily diagnose interactive performance ``bugs" in a mature operating system. We present two case studies that demonstrate the effectiveness of our measurement infrastructure. Each of the performance problems we identify drastically affects variability in response time in a mature system, demonstrating that current tuning techniques do not address this class of performance problems.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {240--251},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339420},
 doi = {http://doi.acm.org/10.1145/339331.339420},
 acmid = {339420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interactive performance, monitoring},
} 

@article{Farkas:2000:QEC:345063.339421,
 author = {Farkas, Keith I. and Flinn, Jason and Back, Godmar and Grunwald, Dirk and Anderson, Jennifer M.},
 title = {Quantifying the energy consumption of a pocket computer and a Java virtual machine},
 abstract = {In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/345063.339421},
 doi = {http://doi.acm.org/10.1145/345063.339421},
 acmid = {339421},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Farkas:2000:QEC:339331.339421,
 author = {Farkas, Keith I. and Flinn, Jason and Back, Godmar and Grunwald, Dirk and Anderson, Jennifer M.},
 title = {Quantifying the energy consumption of a pocket computer and a Java virtual machine},
 abstract = {In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25\%.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {252--263},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/339331.339421},
 doi = {http://doi.acm.org/10.1145/339331.339421},
 acmid = {339421},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:2000:MSB:339331.339422,
 author = {Kim, Jin-Soo and Hsu, Yarsun},
 title = {Memory system behavior of Java programs: methodology and analysis},
 abstract = {This paper studies the memory system behavior of Java programs by analyzing memory reference traces of several SPECjvm98 applications running with a Just-In-Time (JIT) compiler. Trace information is collected by an exception-based tracing tool called JTRACE, without any instrumentation to the Java programs or the JIT compiler.First, we find that the overall cache miss ratio is increased due to garbage collection, which suffers from higher cache misses compared to the application. We also note that going beyond 2-way cache associativity improves the cache miss ratio marginally. Second, we observe that Java programs generate a substantial amount of short-lived objects. However, the size of frequently-referenced long-lived objects is more important to the cache performance, because it tends to determine the application's working set size. Finally, we note that the default heap configuration which starts from a small initial heap size is very inefficient since it invokes a garbage collector frequently. Although the direct costs of garbage collection decrease as we increase the available heap size, there exists an optimal heap size which minimizes the total execution time due to the interaction with the virtual memory performance.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {264--274},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339422},
 doi = {http://doi.acm.org/10.1145/339331.339422},
 acmid = {339422},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:2000:MSB:345063.339422,
 author = {Kim, Jin-Soo and Hsu, Yarsun},
 title = {Memory system behavior of Java programs: methodology and analysis},
 abstract = {This paper studies the memory system behavior of Java programs by analyzing memory reference traces of several SPECjvm98 applications running with a Just-In-Time (JIT) compiler. Trace information is collected by an exception-based tracing tool called JTRACE, without any instrumentation to the Java programs or the JIT compiler.First, we find that the overall cache miss ratio is increased due to garbage collection, which suffers from higher cache misses compared to the application. We also note that going beyond 2-way cache associativity improves the cache miss ratio marginally. Second, we observe that Java programs generate a substantial amount of short-lived objects. However, the size of frequently-referenced long-lived objects is more important to the cache performance, because it tends to determine the application's working set size. Finally, we note that the default heap configuration which starts from a small initial heap size is very inefficient since it invokes a garbage collector frequently. Although the direct costs of garbage collection decrease as we increase the available heap size, there exists an optimal heap size which minimizes the total execution time due to the interaction with the virtual memory performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {264--274},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339422},
 doi = {http://doi.acm.org/10.1145/345063.339422},
 acmid = {339422},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Karlsson:2000:AMW:345063.339423,
 author = {Karlsson, Magnus and Stenstr\"{o}m, Per},
 title = {An analytical model of the working-set sizes in decision-support systems},
 abstract = {This paper presents an analytical model to study how working sets scale with database size and other applications parameters in decision-support systems (DSS). The model uses application parameters, that are measured on down-scaled database executions, to predict cache miss ratios for executions of large databases.By applying the model to two database engines and typical DSS queries we find that, even for large databases, the most performance-critical working set is small and is caused by the instructions and private data that are required to access a single tuple. Consequently, its size is not affected by the database size. Surprisingly, database data may also exhibit temporal locality but the size of its working set critically depends on the structure of the query, the method of scanning, and the size and the content of the database.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {275--285},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339423},
 doi = {http://doi.acm.org/10.1145/345063.339423},
 acmid = {339423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Karlsson:2000:AMW:339331.339423,
 author = {Karlsson, Magnus and Stenstr\"{o}m, Per},
 title = {An analytical model of the working-set sizes in decision-support systems},
 abstract = {This paper presents an analytical model to study how working sets scale with database size and other applications parameters in decision-support systems (DSS). The model uses application parameters, that are measured on down-scaled database executions, to predict cache miss ratios for executions of large databases.By applying the model to two database engines and typical DSS queries we find that, even for large databases, the most performance-critical working set is small and is caused by the instructions and private data that are required to access a single tuple. Consequently, its size is not affected by the database size. Surprisingly, database data may also exhibit temporal locality but the size of its working set critically depends on the structure of the query, the method of scanning, and the size and the content of the database.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {275--285},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339423},
 doi = {http://doi.acm.org/10.1145/339331.339423},
 acmid = {339423},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Choi:2000:TAC:345063.339424,
 author = {Choi, Jongmoo and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun},
 title = {Towards application/file-level characterization of block references: a case for fine-grained buffer management},
 abstract = {Two contributions are made in this paper. First, we show that system level characterization of file block references is inadequate for maximizing buffer cache performance. We show that a finer-grained characterization approach is needed. Though application level characterization methods have been proposed, this is the first attempt, to the best of our knowledge, to consider file level characterizations. We propose an Application/File-level Characterization (AFC) scheme where we detect on-line the reference characteristics at the application level and then at the file level, if necessary. The results of this characterization are used to employ appropriate replacement policies in the buffer cache to maximize performance. The second contribution is in proposing an efficient and fair buffer allocation scheme. Application or file level resource management is infeasible unless there exists an allocation scheme that is efficient and fair. We propose the \&Dgr;HIT allocation scheme that takes away a block from the application/file where the removal results in the smallest reduction in the number of expected buffer cache hits. Both the AFC and \&Dgr;HIT schemes are on-line schemes that detect and allocate as applications execute. Experiments using trace-driven simulations show that substantial performance improvements can be made. For single application executions the hit ratio increased an average of 13 percentage points compared to the LRU policy, with a maximum increase of 59 percentage points, while for multiple application executions, the increase is an average of 12 percentage points, with a maximum of 32 percentage points for the workloads considered.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {286--295},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339424},
 doi = {http://doi.acm.org/10.1145/345063.339424},
 acmid = {339424},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Choi:2000:TAC:339331.339424,
 author = {Choi, Jongmoo and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun},
 title = {Towards application/file-level characterization of block references: a case for fine-grained buffer management},
 abstract = {Two contributions are made in this paper. First, we show that system level characterization of file block references is inadequate for maximizing buffer cache performance. We show that a finer-grained characterization approach is needed. Though application level characterization methods have been proposed, this is the first attempt, to the best of our knowledge, to consider file level characterizations. We propose an Application/File-level Characterization (AFC) scheme where we detect on-line the reference characteristics at the application level and then at the file level, if necessary. The results of this characterization are used to employ appropriate replacement policies in the buffer cache to maximize performance. The second contribution is in proposing an efficient and fair buffer allocation scheme. Application or file level resource management is infeasible unless there exists an allocation scheme that is efficient and fair. We propose the \&Dgr;HIT allocation scheme that takes away a block from the application/file where the removal results in the smallest reduction in the number of expected buffer cache hits. Both the AFC and \&Dgr;HIT schemes are on-line schemes that detect and allocate as applications execute. Experiments using trace-driven simulations show that substantial performance improvements can be made. For single application executions the hit ratio increased an average of 13 percentage points compared to the LRU policy, with a maximum increase of 59 percentage points, while for multiple application executions, the increase is an average of 12 percentage points, with a maximum of 32 percentage points for the workloads considered.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {286--295},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339424},
 doi = {http://doi.acm.org/10.1145/339331.339424},
 acmid = {339424},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kodialam:2000:OMR:339331.339425,
 author = {Kodialam, Murali S. and Lakshman, T. V. and Sengupta, Sudipta},
 title = {Online multicast routing with bandwidth guarantees: a new approach using multicast network flow},
 abstract = {This paper presents a new algorithm for on-line routing of bandwidth-guaranteed multicasts where routing requests arrive one-by-one without there being any a priori knowledge of future requests. A multicast routing request consists of a source s, a set of receivers R, and a bandwidth requirement b. This multicast routing problem arises in many contexts. Two applications of interest are routing of point-to-multipoint label-switched paths in Multi-Protocol Label Switched (MPLS) networks, and the provision of bandwidth guaranteed Virtual Private Network (VPN) services under the ``hose" service model [17]. Offline multicast routing algorithms cannot be used since they require a priori knowledge of all multicast requests that are to be routed. Instead, on-line algorithms that handle requests arriving one-by-one and that satisfy as many potential future demands as possible are needed. The newly developed algorithm is an on-line algorithm and is based on the idea that a newly routed multicast must follow a route that does not ``interfere too much" with network paths that may be critical to satisfy future demands. We develop a multicast tree selection heuristic that is based on the idea of deferred loading of certain ``critical" links. These critical links are identified by the algorithm as links that, if heavily loaded, would make it impossible to satisfy future demands between certain ingress-egress pairs. The presented algorithm uses link-state information and some auxilliary capacity information for multicast tree selection and is amenable to distributed implementation. Unlike previous algorithms, the proposed algorithm exploits any available knowledge of the network ingress-egress points of potential future demands even though the demands themselves are unknown and performs very well.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {296--306},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339425},
 doi = {http://doi.acm.org/10.1145/339331.339425},
 acmid = {339425},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Steiner tree, multicast routing, network flow, traffic engineering},
} 

@article{Kodialam:2000:OMR:345063.339425,
 author = {Kodialam, Murali S. and Lakshman, T. V. and Sengupta, Sudipta},
 title = {Online multicast routing with bandwidth guarantees: a new approach using multicast network flow},
 abstract = {This paper presents a new algorithm for on-line routing of bandwidth-guaranteed multicasts where routing requests arrive one-by-one without there being any a priori knowledge of future requests. A multicast routing request consists of a source s, a set of receivers R, and a bandwidth requirement b. This multicast routing problem arises in many contexts. Two applications of interest are routing of point-to-multipoint label-switched paths in Multi-Protocol Label Switched (MPLS) networks, and the provision of bandwidth guaranteed Virtual Private Network (VPN) services under the ``hose" service model [17]. Offline multicast routing algorithms cannot be used since they require a priori knowledge of all multicast requests that are to be routed. Instead, on-line algorithms that handle requests arriving one-by-one and that satisfy as many potential future demands as possible are needed. The newly developed algorithm is an on-line algorithm and is based on the idea that a newly routed multicast must follow a route that does not ``interfere too much" with network paths that may be critical to satisfy future demands. We develop a multicast tree selection heuristic that is based on the idea of deferred loading of certain ``critical" links. These critical links are identified by the algorithm as links that, if heavily loaded, would make it impossible to satisfy future demands between certain ingress-egress pairs. The presented algorithm uses link-state information and some auxilliary capacity information for multicast tree selection and is amenable to distributed implementation. Unlike previous algorithms, the proposed algorithm exploits any available knowledge of the network ingress-egress points of potential future demands even though the demands themselves are unknown and performs very well.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {296--306},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339425},
 doi = {http://doi.acm.org/10.1145/345063.339425},
 acmid = {339425},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Steiner tree, multicast routing, network flow, traffic engineering},
} 

@article{Gao:2000:SIR:345063.339426,
 author = {Gao, Lixin and Rexford, Jennifer},
 title = {Stable Internet routing without global coordination},
 abstract = {The Border Gateway Protocol (BGP) allows an autonomous system (AS) to apply diverse local policies for selecting routes and propagating reachability information to other domains. However, BGP permits ASes to have conflicting policies that can lead to routing instability. This paper proposes a set of guidelines for an AS to follow in setting its routing policies, without requiring coordination with other ASes. Our approach exploits the Internet's hierarchical structure and the commercial relationships between ASes to impose a partial order on the set of routes to each destination. The guidelines conform to conventional traffic-engineering practices of ISPs, and provide each AS with significant flexibility in selecting its local policies. Furthermore, the guidelines ensure route convergence even under changes in the topology and routing policies. Drawing on a formal model of BGP, we prove that following our proposed policy guidelines guarantees route convergence. We also describe how our methodology can be applied to new types of relationships between ASes, how to verify the hierarchical AS relationships, and how to realize our policy guidelines. Our approach has significant practical value since it preserves the ability of each AS to apply complex local policies without divulging its BGP configurations to others.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {307--317},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/345063.339426},
 doi = {http://doi.acm.org/10.1145/345063.339426},
 acmid = {339426},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gao:2000:SIR:339331.339426,
 author = {Gao, Lixin and Rexford, Jennifer},
 title = {Stable Internet routing without global coordination},
 abstract = {The Border Gateway Protocol (BGP) allows an autonomous system (AS) to apply diverse local policies for selecting routes and propagating reachability information to other domains. However, BGP permits ASes to have conflicting policies that can lead to routing instability. This paper proposes a set of guidelines for an AS to follow in setting its routing policies, without requiring coordination with other ASes. Our approach exploits the Internet's hierarchical structure and the commercial relationships between ASes to impose a partial order on the set of routes to each destination. The guidelines conform to conventional traffic-engineering practices of ISPs, and provide each AS with significant flexibility in selecting its local policies. Furthermore, the guidelines ensure route convergence even under changes in the topology and routing policies. Drawing on a formal model of BGP, we prove that following our proposed policy guidelines guarantees route convergence. We also describe how our methodology can be applied to new types of relationships between ASes, how to verify the hierarchical AS relationships, and how to realize our policy guidelines. Our approach has significant practical value since it preserves the ability of each AS to apply complex local policies without divulging its BGP configurations to others.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {307--317},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/339331.339426},
 doi = {http://doi.acm.org/10.1145/339331.339426},
 acmid = {339426},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Korkmaz:2000:EAF:339331.339427,
 author = {Korkmaz, Turgay and Krunz, Marwan and Tragoudas, Spyros},
 title = {An efficient algorithm for finding a path subject to two additive constraints},
 abstract = {One of the key issues in providing end-to-end quality-of-service guarantees in packet networks is how to determine a feasible route that satisfies a set of constraints while simultaneously maintaining high utilization of network resources. In general, finding a path subject to multiple additive constraints (e.g., delay, delay-jitter) is an NP-complete problem that cannot be exactly solved in polynomial time. Accordingly, heuristics and approximation algorithms are often used to address to this problem. Previously proposed algorithms suffer from either excessive computational cost or low performance. In this paper, we provide an efficient approximation algorithm for finding a path subject to two additive constraints. The worst-case computational complexity of this algorithm is within a logarithmic number of calls to Dijkstra's shortest path algorithm. Its average complexity is much lower than that, as demonstrated by simulation results. The performance of the proposed algorithm is justified via theoretical performance bounds. To achieve further performance improvement, several extensions to the basic algorithm are also provided at low extra computational cost. Extensive simulations are used to demonstrate the high performance of the proposed algorithm and to contrast it with other path selection algorithms.},
 booktitle = {Proceedings of the 2000 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '00},
 year = {2000},
 isbn = {1-58113-194-1},
 location = {Santa Clara, California, United States},
 pages = {318--327},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/339331.339427},
 doi = {http://doi.acm.org/10.1145/339331.339427},
 acmid = {339427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS routing, multiple constrained path selection, scalable routing},
} 

@article{Korkmaz:2000:EAF:345063.339427,
 author = {Korkmaz, Turgay and Krunz, Marwan and Tragoudas, Spyros},
 title = {An efficient algorithm for finding a path subject to two additive constraints},
 abstract = {One of the key issues in providing end-to-end quality-of-service guarantees in packet networks is how to determine a feasible route that satisfies a set of constraints while simultaneously maintaining high utilization of network resources. In general, finding a path subject to multiple additive constraints (e.g., delay, delay-jitter) is an NP-complete problem that cannot be exactly solved in polynomial time. Accordingly, heuristics and approximation algorithms are often used to address to this problem. Previously proposed algorithms suffer from either excessive computational cost or low performance. In this paper, we provide an efficient approximation algorithm for finding a path subject to two additive constraints. The worst-case computational complexity of this algorithm is within a logarithmic number of calls to Dijkstra's shortest path algorithm. Its average complexity is much lower than that, as demonstrated by simulation results. The performance of the proposed algorithm is justified via theoretical performance bounds. To achieve further performance improvement, several extensions to the basic algorithm are also provided at low extra computational cost. Extensive simulations are used to demonstrate the high performance of the proposed algorithm and to contrast it with other path selection algorithms.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {28},
 issue = {1},
 month = {June},
 year = {2000},
 issn = {0163-5999},
 pages = {318--327},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/345063.339427},
 doi = {http://doi.acm.org/10.1145/345063.339427},
 acmid = {339427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS routing, multiple constrained path selection, scalable routing},
} 

@inproceedings{Ribeiro:1999:SNL:301453.301475,
 author = {Ribeiro, Vinay J. and Riedi, Rudolf H. and Crouse, Matthew S. and Baraniuk, Richard G.},
 title = {Simulation of nonGaussian long-range-dependent traffic using wavelets},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301475},
 doi = {http://doi.acm.org/10.1145/301453.301475},
 acmid = {301475},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ribeiro:1999:SNL:301464.301475,
 author = {Ribeiro, Vinay J. and Riedi, Rudolf H. and Crouse, Matthew S. and Baraniuk, Richard G.},
 title = {Simulation of nonGaussian long-range-dependent traffic using wavelets},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301475},
 doi = {http://doi.acm.org/10.1145/301464.301475},
 acmid = {301475},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhao:1999:BCM:301453.301476,
 author = {Zhao, Wei and Tripathi, Satish K.},
 title = {Bandwidth-efficient continuous media streaming through optimal multiplexing},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301476},
 doi = {http://doi.acm.org/10.1145/301453.301476},
 acmid = {301476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {admission control, bandwidth allocation, feasible region, multimedia streaming, multiplexing, quality-of-service, temporal smoothing, transmission scheduling},
} 

@article{Zhao:1999:BCM:301464.301476,
 author = {Zhao, Wei and Tripathi, Satish K.},
 title = {Bandwidth-efficient continuous media streaming through optimal multiplexing},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301476},
 doi = {http://doi.acm.org/10.1145/301464.301476},
 acmid = {301476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {admission control, bandwidth allocation, feasible region, multimedia streaming, multiplexing, quality-of-service, temporal smoothing, transmission scheduling},
} 

@article{Kumar:1999:ESS:301464.301477,
 author = {Kumar, Sanjeev and Jiang, Dongming and Chandra, Rohit and Singh, Jaswinder Pal},
 title = {Evaluating synchronization on shared address space multiprocessors: methodology and performance},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301477},
 doi = {http://doi.acm.org/10.1145/301464.301477},
 acmid = {301477},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kumar:1999:ESS:301453.301477,
 author = {Kumar, Sanjeev and Jiang, Dongming and Chandra, Rohit and Singh, Jaswinder Pal},
 title = {Evaluating synchronization on shared address space multiprocessors: methodology and performance},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301477},
 doi = {http://doi.acm.org/10.1145/301453.301477},
 acmid = {301477},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Acharya:1999:AUI:301453.301478,
 author = {Acharya, Anurag and Setia, Sanjeev},
 title = {Availability and utility of idle memory in workstation clusters},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301478},
 doi = {http://doi.acm.org/10.1145/301453.301478},
 acmid = {301478},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Acharya:1999:AUI:301464.301478,
 author = {Acharya, Anurag and Setia, Sanjeev},
 title = {Availability and utility of idle memory in workstation clusters},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301478},
 doi = {http://doi.acm.org/10.1145/301464.301478},
 acmid = {301478},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kaplan:1999:TRV:301464.301479,
 author = {Kaplan, Scott F. and Smaragdakis, Yannis and Wilson, Paul R.},
 title = {Trace reduction for virtual memory simulations},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301479},
 doi = {http://doi.acm.org/10.1145/301464.301479},
 acmid = {301479},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kaplan:1999:TRV:301453.301479,
 author = {Kaplan, Scott F. and Smaragdakis, Yannis and Wilson, Paul R.},
 title = {Trace reduction for virtual memory simulations},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301479},
 doi = {http://doi.acm.org/10.1145/301453.301479},
 acmid = {301479},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Douceur:1999:LSF:301464.301480,
 author = {Douceur, John R. and Bolosky, William J.},
 title = {A large-scale study of file-system contents},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301480},
 doi = {http://doi.acm.org/10.1145/301464.301480},
 acmid = {301480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, directory hierarchy, file-system contents, static data snapshot, workload characterization},
} 

@inproceedings{Douceur:1999:LSF:301453.301480,
 author = {Douceur, John R. and Bolosky, William J.},
 title = {A large-scale study of file-system contents},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301480},
 doi = {http://doi.acm.org/10.1145/301453.301480},
 acmid = {301480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analytical modeling, directory hierarchy, file-system contents, static data snapshot, workload characterization},
} 

@inproceedings{Martin:1999:NSH:301453.301481,
 author = {Martin, Richard P. and Culler, David E.},
 title = {NFS sensitivity to high performance networks},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {71--82},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301481},
 doi = {http://doi.acm.org/10.1145/301453.301481},
 acmid = {301481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martin:1999:NSH:301464.301481,
 author = {Martin, Richard P. and Culler, David E.},
 title = {NFS sensitivity to high performance networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {71--82},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301481},
 doi = {http://doi.acm.org/10.1145/301464.301481},
 acmid = {301481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barve:1999:MOI:301464.301482,
 author = {Barve, Rakesh and Shriver, Elizabeth and Gibbons, Phillip B. and Hillyer, Bruce K. and Matias, Yossi and Vitter, Jeffrey Scott},
 title = {Modeling and optimizing I/O throughput of multiple disks on a bus},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {83--92},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301482},
 doi = {http://doi.acm.org/10.1145/301464.301482},
 acmid = {301482},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barve:1999:MOI:301453.301482,
 author = {Barve, Rakesh and Shriver, Elizabeth and Gibbons, Phillip B. and Hillyer, Bruce K. and Matias, Yossi and Vitter, Jeffrey Scott},
 title = {Modeling and optimizing I/O throughput of multiple disks on a bus},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {83--92},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301482},
 doi = {http://doi.acm.org/10.1145/301453.301482},
 acmid = {301482},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sethuraman:1999:OSS:301453.301483,
 author = {Sethuraman, Jay and Squillante, Mark S.},
 title = {Optimal stochastic scheduling in multiclass parallel queues},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {93--102},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301483},
 doi = {http://doi.acm.org/10.1145/301453.301483},
 acmid = {301483},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sethuraman:1999:OSS:301464.301483,
 author = {Sethuraman, Jay and Squillante, Mark S.},
 title = {Optimal stochastic scheduling in multiclass parallel queues},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {93--102},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301483},
 doi = {http://doi.acm.org/10.1145/301464.301483},
 acmid = {301483},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Varki:1999:MVT:301453.301484,
 author = {Varki, Elizabeth},
 title = {Mean value technique for closed fork-join networks},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {103--112},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301484},
 doi = {http://doi.acm.org/10.1145/301453.301484},
 acmid = {301484},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Varki:1999:MVT:301464.301484,
 author = {Varki, Elizabeth},
 title = {Mean value technique for closed fork-join networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {103--112},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301484},
 doi = {http://doi.acm.org/10.1145/301464.301484},
 acmid = {301484},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Franaszek:1999:MFS:301453.301485,
 author = {Franaszek, Peter A. and Heidelberger, Philip and Wazlowski, Michael},
 title = {On management of free space in compressed memory systems},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {113--121},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/301453.301485},
 doi = {http://doi.acm.org/10.1145/301453.301485},
 acmid = {301485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Franaszek:1999:MFS:301464.301485,
 author = {Franaszek, Peter A. and Heidelberger, Philip and Wazlowski, Michael},
 title = {On management of free space in compressed memory systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {113--121},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/301464.301485},
 doi = {http://doi.acm.org/10.1145/301464.301485},
 acmid = {301485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smaragdakis:1999:ESE:301453.301486,
 author = {Smaragdakis, Yannis and Kaplan, Scott and Wilson, Paul},
 title = {EELRU: simple and effective adaptive page replacement},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {122--133},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301486},
 doi = {http://doi.acm.org/10.1145/301453.301486},
 acmid = {301486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smaragdakis:1999:ESE:301464.301486,
 author = {Smaragdakis, Yannis and Kaplan, Scott and Wilson, Paul},
 title = {EELRU: simple and effective adaptive page replacement},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {122--133},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301486},
 doi = {http://doi.acm.org/10.1145/301464.301486},
 acmid = {301486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1999:ESP:301453.301487,
 author = {Lee, Donghee and Choi, Jongmoo and Kim, Jong-Hun and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun and Kim, Chong Sang},
 title = {On the existence of a spectrum of policies that subsumes the least recently used (LRU) and least frequently used (LFU) policies},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301487},
 doi = {http://doi.acm.org/10.1145/301453.301487},
 acmid = {301487},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1999:ESP:301464.301487,
 author = {Lee, Donghee and Choi, Jongmoo and Kim, Jong-Hun and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun and Kim, Chong Sang},
 title = {On the existence of a spectrum of policies that subsumes the least recently used (LRU) and least frequently used (LFU) policies},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {134--143},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301487},
 doi = {http://doi.acm.org/10.1145/301464.301487},
 acmid = {301487},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ludwig:1999:MTT:301464.301488,
 author = {Ludwig, Reiner and Rathonyi, Bela and Konrad, Almudena and Oden, Kimberly and Joseph, Anthony},
 title = {Multi-layer tracing of TCP over a reliable wireless link},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {144--154},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301464.301488},
 doi = {http://doi.acm.org/10.1145/301464.301488},
 acmid = {301488},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GSM, TCP, measurement tools, wireless},
} 

@inproceedings{Ludwig:1999:MTT:301453.301488,
 author = {Ludwig, Reiner and Rathonyi, Bela and Konrad, Almudena and Oden, Kimberly and Joseph, Anthony},
 title = {Multi-layer tracing of TCP over a reliable wireless link},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {144--154},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301453.301488},
 doi = {http://doi.acm.org/10.1145/301453.301488},
 acmid = {301488},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GSM, TCP, measurement tools, wireless},
} 

@article{Anjum:1999:BDT:301464.301550,
 author = {Anjum, Farooq and Tassiulas, Leandros},
 title = {On the behavior of different TCP algorithms over a wireless channel with correlated packet losses},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {155--165},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301464.301550},
 doi = {http://doi.acm.org/10.1145/301464.301550},
 acmid = {301550},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anjum:1999:BDT:301453.301550,
 author = {Anjum, Farooq and Tassiulas, Leandros},
 title = {On the behavior of different TCP algorithms over a wireless channel with correlated packet losses},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {155--165},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/301453.301550},
 doi = {http://doi.acm.org/10.1145/301453.301550},
 acmid = {301550},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sripanidkulchai:1999:TVN:301453.301553,
 author = {Sripanidkulchai, Kunwadee and Myers, Andy and Zhang, Hui},
 title = {A third-party value-added network service approach to reliable multicast},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {166--177},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301453.301553},
 doi = {http://doi.acm.org/10.1145/301453.301553},
 acmid = {301553},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sripanidkulchai:1999:TVN:301464.301553,
 author = {Sripanidkulchai, Kunwadee and Myers, Andy and Zhang, Hui},
 title = {A third-party value-added network service approach to reliable multicast},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {166--177},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/301464.301553},
 doi = {http://doi.acm.org/10.1145/301464.301553},
 acmid = {301553},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fan:1999:WPL:301453.301557,
 author = {Fan, Li and Cao, Pei and Lin, Wei and Jacobson, Quinn},
 title = {Web prefetching between low-bandwidth clients and proxies: potential and performance},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301557},
 doi = {http://doi.acm.org/10.1145/301453.301557},
 acmid = {301557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fan:1999:WPL:301464.301557,
 author = {Fan, Li and Cao, Pei and Lin, Wei and Jacobson, Quinn},
 title = {Web prefetching between low-bandwidth clients and proxies: potential and performance},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301557},
 doi = {http://doi.acm.org/10.1145/301464.301557},
 acmid = {301557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barford:1999:PEH:301464.301560,
 author = {Barford, Paul and Crovella, Mark},
 title = {A performance evaluation of hyper text transfer protocols},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {188--197},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301464.301560},
 doi = {http://doi.acm.org/10.1145/301464.301560},
 acmid = {301560},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barford:1999:PEH:301453.301560,
 author = {Barford, Paul and Crovella, Mark},
 title = {A performance evaluation of hyper text transfer protocols},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {188--197},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/301453.301560},
 doi = {http://doi.acm.org/10.1145/301453.301560},
 acmid = {301560},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhu:1999:HRM:301464.301567,
 author = {Zhu, Huican and Smith, Ben and Yang, Tao},
 title = {Hierarchical resource management for Web server clusters with dynamic content},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {198--199},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301567},
 doi = {http://doi.acm.org/10.1145/301464.301567},
 acmid = {301567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhu:1999:HRM:301453.301567,
 author = {Zhu, Huican and Smith, Ben and Yang, Tao},
 title = {Hierarchical resource management for Web server clusters with dynamic content},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {198--199},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301567},
 doi = {http://doi.acm.org/10.1145/301453.301567},
 acmid = {301567},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Liao:1999:AGC:301464.302127,
 author = {Liao, Cheng and Martonosi, Margaret and Clark, Douglas W.},
 title = {An adaptive globally-synchronizing clock algorithm and its implementation on a Myrinet-based PC cluster},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {200--201},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.302127},
 doi = {http://doi.acm.org/10.1145/301464.302127},
 acmid = {302127},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Liao:1999:AGC:301453.302127,
 author = {Liao, Cheng and Martonosi, Margaret and Clark, Douglas W.},
 title = {An adaptive globally-synchronizing clock algorithm and its implementation on a Myrinet-based PC cluster},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {200--201},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.302127},
 doi = {http://doi.acm.org/10.1145/301453.302127},
 acmid = {302127},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chou:1999:PSD:301464.301568,
 author = {Chou, ChengFu and Golubchik, Leana and Lui, John C. S.},
 title = {A performance study of dynamic replication techniques in continuous media servers},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {202--203},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301568},
 doi = {http://doi.acm.org/10.1145/301464.301568},
 acmid = {301568},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chou:1999:PSD:301453.301568,
 author = {Chou, ChengFu and Golubchik, Leana and Lui, John C. S.},
 title = {A performance study of dynamic replication techniques in continuous media servers},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {202--203},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301568},
 doi = {http://doi.acm.org/10.1145/301453.301568},
 acmid = {301568},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dovrolis:1999:RDS:301464.301571,
 author = {Dovrolis, Constantinos and Stiliadis, Dimitrios},
 title = {Relative differentiated services in the Internet: issues and mechanisms},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {204--205},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301571},
 doi = {http://doi.acm.org/10.1145/301464.301571},
 acmid = {301571},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dovrolis:1999:RDS:301453.301571,
 author = {Dovrolis, Constantinos and Stiliadis, Dimitrios},
 title = {Relative differentiated services in the Internet: issues and mechanisms},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {204--205},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301571},
 doi = {http://doi.acm.org/10.1145/301453.301571},
 acmid = {301571},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bartels:1999:PLF:301464.301572,
 author = {Bartels, Gretta and Karlin, Anna and Anderson, Darrell and Chase, Jeffrey and Levy, Henry and Voelker, Geoffrey},
 title = {Potentials and limitations of fault-based Markov prefetching for virtual memory pages},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {206--207},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301572},
 doi = {http://doi.acm.org/10.1145/301464.301572},
 acmid = {301572},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bartels:1999:PLF:301453.301572,
 author = {Bartels, Gretta and Karlin, Anna and Anderson, Darrell and Chase, Jeffrey and Levy, Henry and Voelker, Geoffrey},
 title = {Potentials and limitations of fault-based Markov prefetching for virtual memory pages},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {206--207},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301572},
 doi = {http://doi.acm.org/10.1145/301453.301572},
 acmid = {301572},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Crowley:1999:UTS:301453.301573,
 author = {Crowley, Patrick and Baer, Jean-Loup},
 title = {On the use of trace sampling for architectural studies of desktop applications},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {208--209},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301573},
 doi = {http://doi.acm.org/10.1145/301453.301573},
 acmid = {301573},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Crowley:1999:UTS:301464.301573,
 author = {Crowley, Patrick and Baer, Jean-Loup},
 title = {On the use of trace sampling for architectural studies of desktop applications},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {208--209},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301573},
 doi = {http://doi.acm.org/10.1145/301464.301573},
 acmid = {301573},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bhola:1999:WMH:301464.301574,
 author = {Bhola, Sumeer and Ahamad, Mustaque},
 title = {Workload modeling for highly interactive applications},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {210--211},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301574},
 doi = {http://doi.acm.org/10.1145/301464.301574},
 acmid = {301574},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bhola:1999:WMH:301453.301574,
 author = {Bhola, Sumeer and Ahamad, Mustaque},
 title = {Workload modeling for highly interactive applications},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {210--211},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301574},
 doi = {http://doi.acm.org/10.1145/301453.301574},
 acmid = {301574},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Venkitaraman:1999:DEC:301453.301575,
 author = {Venkitaraman, Narayanan and Kim, Tae-eun and Lee, Kang-Won},
 title = {Design and evaluation of congestion control algorithms in the future Internet},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {212--213},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301575},
 doi = {http://doi.acm.org/10.1145/301453.301575},
 acmid = {301575},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Venkitaraman:1999:DEC:301464.301575,
 author = {Venkitaraman, Narayanan and Kim, Tae-eun and Lee, Kang-Won},
 title = {Design and evaluation of congestion control algorithms in the future Internet},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {212--213},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301575},
 doi = {http://doi.acm.org/10.1145/301464.301575},
 acmid = {301575},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Elnozahy:1999:ATC:301464.301577,
 author = {Elnozahy, E. N.},
 title = {Address trace compression through loop detection and reduction},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {214--215},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301577},
 doi = {http://doi.acm.org/10.1145/301464.301577},
 acmid = {301577},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address traces, compression, control flow analysis, traces},
} 

@inproceedings{Elnozahy:1999:ATC:301453.301577,
 author = {Elnozahy, E. N.},
 title = {Address trace compression through loop detection and reduction},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {214--215},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301577},
 doi = {http://doi.acm.org/10.1145/301453.301577},
 acmid = {301577},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address traces, compression, control flow analysis, traces},
} 

@article{Nahum:1999:PIW:301464.301579,
 author = {Nahum, Erich and Barzilai, Tsipora and Kandlur, Dilip},
 title = {Performance issues in WWW servers},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {216--217},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301579},
 doi = {http://doi.acm.org/10.1145/301464.301579},
 acmid = {301579},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nahum:1999:PIW:301453.301579,
 author = {Nahum, Erich and Barzilai, Tsipora and Kandlur, Dilip},
 title = {Performance issues in WWW servers},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {216--217},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301579},
 doi = {http://doi.acm.org/10.1145/301453.301579},
 acmid = {301579},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ng:1999:SBT:301464.301580,
 author = {Ng, T. S. Eugene and Stephens, Donpaul C. and Stoica, Ion and Zhang, Hui},
 title = {Supporting best-effort traffic with fair service curve},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {218--219},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301580},
 doi = {http://doi.acm.org/10.1145/301464.301580},
 acmid = {301580},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ng:1999:SBT:301453.301580,
 author = {Ng, T. S. Eugene and Stephens, Donpaul C. and Stoica, Ion and Zhang, Hui},
 title = {Supporting best-effort traffic with fair service curve},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {218--219},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301580},
 doi = {http://doi.acm.org/10.1145/301453.301580},
 acmid = {301580},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Padhye:1999:TRA:301464.301581,
 author = {Padhye, Jitendra and Kurose, Jim and Towsley, Don and Koodli, Rajeev},
 title = {A TCP-friendly rate adjustment protocol for continuous media flows over best effort networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {220--221},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301581},
 doi = {http://doi.acm.org/10.1145/301464.301581},
 acmid = {301581},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Padhye:1999:TRA:301453.301581,
 author = {Padhye, Jitendra and Kurose, Jim and Towsley, Don and Koodli, Rajeev},
 title = {A TCP-friendly rate adjustment protocol for continuous media flows over best effort networks},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {220--221},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301581},
 doi = {http://doi.acm.org/10.1145/301453.301581},
 acmid = {301581},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Downey:1999:UPE:301464.301582,
 author = {Downey, Allen B.},
 title = {Using pathchar to estimate Internet link characteristics},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {222--223},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301582},
 doi = {http://doi.acm.org/10.1145/301464.301582},
 acmid = {301582},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Downey:1999:UPE:301453.301582,
 author = {Downey, Allen B.},
 title = {Using pathchar to estimate Internet link characteristics},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {222--223},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301582},
 doi = {http://doi.acm.org/10.1145/301453.301582},
 acmid = {301582},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hershko:1999:SSM:301453.301583,
 author = {Hershko, Yuval and Segal, Daniel and Shachnai, Hadas},
 title = {Self-tuning synchronization mechanisms in network operating systems},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {224--225},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301583},
 doi = {http://doi.acm.org/10.1145/301453.301583},
 acmid = {301583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hershko:1999:SSM:301464.301583,
 author = {Hershko, Yuval and Segal, Daniel and Shachnai, Hadas},
 title = {Self-tuning synchronization mechanisms in network operating systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {224--225},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301583},
 doi = {http://doi.acm.org/10.1145/301464.301583},
 acmid = {301583},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bose:1999:PEV:301464.301584,
 author = {Bose, Pradip},
 title = {Performance evaluation and validation of microprocessors},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {226--227},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301584},
 doi = {http://doi.acm.org/10.1145/301464.301584},
 acmid = {301584},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance evaluation, processor design, validation},
} 

@inproceedings{Bose:1999:PEV:301453.301584,
 author = {Bose, Pradip},
 title = {Performance evaluation and validation of microprocessors},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {226--227},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301584},
 doi = {http://doi.acm.org/10.1145/301453.301584},
 acmid = {301584},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance evaluation, processor design, validation},
} 

@article{Majumdar:1999:CMC:301464.301585,
 author = {Majumdar, Shikharesh and Streibel, Dale and Beninger, Bruce and Carroll, Brian and Verma, Neveenta and Liu, Minru},
 title = {Controlling memory contention on a scalable multiprocessor-based telephone switch},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {228--229},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301585},
 doi = {http://doi.acm.org/10.1145/301464.301585},
 acmid = {301585},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Majumdar:1999:CMC:301453.301585,
 author = {Majumdar, Shikharesh and Streibel, Dale and Beninger, Bruce and Carroll, Brian and Verma, Neveenta and Liu, Minru},
 title = {Controlling memory contention on a scalable multiprocessor-based telephone switch},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {228--229},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301585},
 doi = {http://doi.acm.org/10.1145/301453.301585},
 acmid = {301585},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cervetto:1999:MPA:301464.301586,
 author = {Cervetto, Eugenio},
 title = {Model-based performance analysis of an EDP/ERP-oriented wide area network},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {230--231},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301586},
 doi = {http://doi.acm.org/10.1145/301464.301586},
 acmid = {301586},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDP, ERP, performance modeling, performance prediction, wide-area network},
} 

@inproceedings{Cervetto:1999:MPA:301453.301586,
 author = {Cervetto, Eugenio},
 title = {Model-based performance analysis of an EDP/ERP-oriented wide area network},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {230--231},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301586},
 doi = {http://doi.acm.org/10.1145/301453.301586},
 acmid = {301586},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EDP, ERP, performance modeling, performance prediction, wide-area network},
} 

@inproceedings{Ramanathan:1999:VSA:301453.301587,
 author = {Ramanathan, Srinivas and Perry, Edward H.},
 title = {The value of a systematic approach to measurement and analysis: an ISP case study},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {232--233},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301587},
 doi = {http://doi.acm.org/10.1145/301453.301587},
 acmid = {301587},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramanathan:1999:VSA:301464.301587,
 author = {Ramanathan, Srinivas and Perry, Edward H.},
 title = {The value of a systematic approach to measurement and analysis: an ISP case study},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {232--233},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301587},
 doi = {http://doi.acm.org/10.1145/301464.301587},
 acmid = {301587},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Siebert:1999:IPD:301464.301588,
 author = {Siebert, Janet},
 title = {Improving performance of data analysis in data warehouses: a methodology and case study},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {27},
 issue = {1},
 month = {May},
 year = {1999},
 issn = {0163-5999},
 pages = {234--235},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301464.301588},
 doi = {http://doi.acm.org/10.1145/301464.301588},
 acmid = {301588},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLDB, data analysis, data warehouse, performance, synthetic join},
} 

@inproceedings{Siebert:1999:IPD:301453.301588,
 author = {Siebert, Janet},
 title = {Improving performance of data analysis in data warehouses: a methodology and case study},
 abstract = {},
 booktitle = {Proceedings of the 1999 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '99},
 year = {1999},
 isbn = {1-58113-083-X},
 location = {Atlanta, Georgia, United States},
 pages = {234--235},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/301453.301588},
 doi = {http://doi.acm.org/10.1145/301453.301588},
 acmid = {301588},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLDB, data analysis, data warehouse, performance, synthetic join},
} 

@inproceedings{Srinivasan:1998:FIL:277851.277863,
 author = {Srinivasan, V. and Varghese, George},
 title = {Faster IP lookups using controlled prefix expansion},
 abstract = {Internet (IP) address lookup is a major bottleneck in high performance routers. IP address lookup is challenging because it requires a longest matching prefix</i> lookup. It is compounded by increasing routing table sizes, increased traffic, higher speed links, and the migration to 128 bit IPv6 addresses. We describe how IP lookups can be made faster using a new technique called controlled prefix expansion</i>. Controlled prefix expansion, together with optimization techniques based on dynamic programming, can be used to improve the speed of the best known IP lookup algorithms by at least a factor of two. When applied to trie search, our techniques provide a range of algorithms whose performance can be tuned. For example, with 1 MB of L2 cache, trie search of the MaeEast database with 38,000 prefixes can be done in a worst case search time of 181 nsec, a worst case insert/delete time of 2.5 msec, and an average insert/delete time of 4 usec. Our actual experiments used 512 KB L2 cache to obtain a worst-case search time of 226 nsec, a worst-case worst case insert/delete time of 2.5 msec and an average insert/delete time of 4 usec. We also describe how our techniques can be used to improve the speed of binary search on prefix lengths to provide a scalable solution for IPv6. Our approach to algorithm design is based on measurements using the VTune tool on a Pentium to obtain dynamic clock cycle counts.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277863},
 doi = {http://doi.acm.org/10.1145/277851.277863},
 acmid = {277863},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Srinivasan:1998:FIL:277858.277863,
 author = {Srinivasan, V. and Varghese, George},
 title = {Faster IP lookups using controlled prefix expansion},
 abstract = {Internet (IP) address lookup is a major bottleneck in high performance routers. IP address lookup is challenging because it requires a longest matching prefix</i> lookup. It is compounded by increasing routing table sizes, increased traffic, higher speed links, and the migration to 128 bit IPv6 addresses. We describe how IP lookups can be made faster using a new technique called controlled prefix expansion</i>. Controlled prefix expansion, together with optimization techniques based on dynamic programming, can be used to improve the speed of the best known IP lookup algorithms by at least a factor of two. When applied to trie search, our techniques provide a range of algorithms whose performance can be tuned. For example, with 1 MB of L2 cache, trie search of the MaeEast database with 38,000 prefixes can be done in a worst case search time of 181 nsec, a worst case insert/delete time of 2.5 msec, and an average insert/delete time of 4 usec. Our actual experiments used 512 KB L2 cache to obtain a worst-case search time of 226 nsec, a worst-case worst case insert/delete time of 2.5 msec and an average insert/delete time of 4 usec. We also describe how our techniques can be used to improve the speed of binary search on prefix lengths to provide a scalable solution for IPv6. Our approach to algorithm design is based on measurements using the VTune tool on a Pentium to obtain dynamic clock cycle counts.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277863},
 doi = {http://doi.acm.org/10.1145/277858.277863},
 acmid = {277863},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Paxson:1998:CMP:277851.277865,
 author = {Paxson, Vern},
 title = {On calibrating measurements of packet transit times},
 abstract = {We discuss the problem of detecting errors in measurements of the total delay experienced by packets transmitted through a wide-area network. We assume that we have measurements of the transmission times of a group of packets sent from an originating host, A,</i> and a corresponding set of measurements of their arrival times at their destination host, B,</i> recorded by two separate clocks. We also assume that we have a similar series of measurements of packets sent from B</i> to A</i> (as might occur when recording a TCP connection), but we do not assume that the clock at A</i> is synchronized with the clock at B,</i> nor that they run at the same frequency. We develop robust algorithms for detecting abrupt adjustments to either clock, and for estimating the relative skew between the clocks. By analyzing a large set of measurements of Internet TCP connections, we find that both clock adjustments and relative skew are sufficiently common that failing to detect them can lead to potentially large errors when analyzing packet transit times. We further find that synchronizing clocks using a network time protocol such as NTP does not free them from such errors.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {11--21},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277865},
 doi = {http://doi.acm.org/10.1145/277851.277865},
 acmid = {277865},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Paxson:1998:CMP:277858.277865,
 author = {Paxson, Vern},
 title = {On calibrating measurements of packet transit times},
 abstract = {We discuss the problem of detecting errors in measurements of the total delay experienced by packets transmitted through a wide-area network. We assume that we have measurements of the transmission times of a group of packets sent from an originating host, A,</i> and a corresponding set of measurements of their arrival times at their destination host, B,</i> recorded by two separate clocks. We also assume that we have a similar series of measurements of packets sent from B</i> to A</i> (as might occur when recording a TCP connection), but we do not assume that the clock at A</i> is synchronized with the clock at B,</i> nor that they run at the same frequency. We develop robust algorithms for detecting abrupt adjustments to either clock, and for estimating the relative skew between the clocks. By analyzing a large set of measurements of Internet TCP connections, we find that both clock adjustments and relative skew are sufficiently common that failing to detect them can lead to potentially large errors when analyzing packet transit times. We further find that synchronizing clocks using a network time protocol such as NTP does not free them from such errors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {11--21},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277865},
 doi = {http://doi.acm.org/10.1145/277858.277865},
 acmid = {277865},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:1998:MCP:277858.277867,
 author = {Wang, Randolph Y. and Krishnamurthy, Arvind and Martin, Richard P. and Anderson, Thomas E. and Culler, David E.},
 title = {Modeling communication pipeline latency},
 abstract = {In this paper, we study how to minimize the latency of a message through a network that consists of a number of store-and-forward stages. This research is especially relevant for today's low overhead communication systems that employ dedicated processing elements for protocol processing. We develop an abstract pipeline model that reveals a crucial performance tradeoff involving the effects of the overhead of the bottleneck stage and the bandwidth of the remaining stages. We exploit this tradeoff to develop a suite of fragmentation algorithms designed to minimize message latency. We also provide an experimental methodology that enables the construction of customized pipeline algorithms that can adapt to the specific system characteristics and application workloads. By applying this methodology to the Myrinet-GAM system, we have improved its latency by up to 51\%. Our theoretical framework is also applicable to pipelined systems beyond the context of high speed networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277867},
 doi = {http://doi.acm.org/10.1145/277858.277867},
 acmid = {277867},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:1998:MCP:277851.277867,
 author = {Wang, Randolph Y. and Krishnamurthy, Arvind and Martin, Richard P. and Anderson, Thomas E. and Culler, David E.},
 title = {Modeling communication pipeline latency},
 abstract = {In this paper, we study how to minimize the latency of a message through a network that consists of a number of store-and-forward stages. This research is especially relevant for today's low overhead communication systems that employ dedicated processing elements for protocol processing. We develop an abstract pipeline model that reveals a crucial performance tradeoff involving the effects of the overhead of the bottleneck stage and the bandwidth of the remaining stages. We exploit this tradeoff to develop a suite of fragmentation algorithms designed to minimize message latency. We also provide an experimental methodology that enables the construction of customized pipeline algorithms that can adapt to the specific system characteristics and application workloads. By applying this methodology to the Myrinet-GAM system, we have improved its latency by up to 51\%. Our theoretical framework is also applicable to pipelined systems beyond the context of high speed networks.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277867},
 doi = {http://doi.acm.org/10.1145/277851.277867},
 acmid = {277867},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Voelker:1998:ICP:277851.277869,
 author = {Voelker, Geoffrey M. and Anderson, Eric J. and Kimbrel, Tracy and Feeley, Michael J. and Chase, Jeffrey S. and Karlin, Anna R. and Levy, Henry M.},
 title = {Implementing cooperative prefetching and caching in a globally-managed memory system},
 abstract = {This paper presents cooperative prefetching and caching</i> --- the use of network-wide global resources (memories, CPUs, and disks) to support prefetching and caching in the presence of hints of future demands. Cooperative prefetching and caching effectively unites disk-latency reduction techniques from three lines of research: prefetching algorithms, cluster-wide memory management, and parallel I/O. When used together, these techniques greatly increase the power of prefetching relative to a conventional (non-global-memory) system. We have designed and implemented PGMS, a cooperative prefetching and caching system, under the Digital Unix operating system running on a 1.28 Gb/sec Myrinet-connected cluster of DEC Alpha workstations. Our measurements and analysis show that by using available global resources, cooperative prefetching can obtain significant speedups for I/O-bound programs. For example, for a graphics rendering application, our system achieves a speedup of 4.9 over a non-prefetching version of the same program, and a 3.1-fold improvement over that program using local-disk prefetching alone.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277869},
 doi = {http://doi.acm.org/10.1145/277851.277869},
 acmid = {277869},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Voelker:1998:ICP:277858.277869,
 author = {Voelker, Geoffrey M. and Anderson, Eric J. and Kimbrel, Tracy and Feeley, Michael J. and Chase, Jeffrey S. and Karlin, Anna R. and Levy, Henry M.},
 title = {Implementing cooperative prefetching and caching in a globally-managed memory system},
 abstract = {This paper presents cooperative prefetching and caching</i> --- the use of network-wide global resources (memories, CPUs, and disks) to support prefetching and caching in the presence of hints of future demands. Cooperative prefetching and caching effectively unites disk-latency reduction techniques from three lines of research: prefetching algorithms, cluster-wide memory management, and parallel I/O. When used together, these techniques greatly increase the power of prefetching relative to a conventional (non-global-memory) system. We have designed and implemented PGMS, a cooperative prefetching and caching system, under the Digital Unix operating system running on a 1.28 Gb/sec Myrinet-connected cluster of DEC Alpha workstations. Our measurements and analysis show that by using available global resources, cooperative prefetching can obtain significant speedups for I/O-bound programs. For example, for a graphics rendering application, our system achieves a speedup of 4.9 over a non-prefetching version of the same program, and a 3.1-fold improvement over that program using local-disk prefetching alone.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277869},
 doi = {http://doi.acm.org/10.1145/277858.277869},
 acmid = {277869},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shenoy:1998:CDS:277851.277871,
 author = {Shenoy, Prashant J. and Vin, Harrick M.},
 title = {Cello: a disk scheduling framework for next generation operating systems},
 abstract = {In this paper, we present the Cello disk scheduling framework for meeting the diverse service requirements of applications. Cello employs a two-level disk scheduling architecture, consisting of a class-independent scheduler and a set of class-specific schedulers. The two levels of the framework allocate disk bandwidth at two time-scales: the class-independent scheduler governs the coarse-grain allocation of bandwidth to application classes, while the class-specific schedulers control the fine-grain interleaving of requests. The two levels of the architecture separate application-independent mechanisms from application-specific scheduling policies, and thereby facilitate the co-existence of multiple class-specific schedulers. We demonstrate that Cello is suitable for next generation operating systems since: (i) it aligns the service provided with the application requirements, (ii) it protects application classes from one another, (iii) it is work-conserving and can adapt to changes in work-load, (iv) it minimizes the seek time and rotational latency overhead incurred during access, and (v) it is computationally efficient.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {44--55},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277851.277871},
 doi = {http://doi.acm.org/10.1145/277851.277871},
 acmid = {277871},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shenoy:1998:CDS:277858.277871,
 author = {Shenoy, Prashant J. and Vin, Harrick M.},
 title = {Cello: a disk scheduling framework for next generation operating systems},
 abstract = {In this paper, we present the Cello disk scheduling framework for meeting the diverse service requirements of applications. Cello employs a two-level disk scheduling architecture, consisting of a class-independent scheduler and a set of class-specific schedulers. The two levels of the framework allocate disk bandwidth at two time-scales: the class-independent scheduler governs the coarse-grain allocation of bandwidth to application classes, while the class-specific schedulers control the fine-grain interleaving of requests. The two levels of the architecture separate application-independent mechanisms from application-specific scheduling policies, and thereby facilitate the co-existence of multiple class-specific schedulers. We demonstrate that Cello is suitable for next generation operating systems since: (i) it aligns the service provided with the application requirements, (ii) it protects application classes from one another, (iii) it is work-conserving and can adapt to changes in work-load, (iv) it minimizes the seek time and rotational latency overhead incurred during access, and (v) it is computationally efficient.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {44--55},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277858.277871},
 doi = {http://doi.acm.org/10.1145/277858.277871},
 acmid = {277871},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rosti:1998:IIP:277851.277873,
 author = {Rosti, Emilia and Serazzi, Giuseppe and Smirni, Evgenia and Squillante, Mark S.},
 title = {The impact of I/O on program behavior and parallel scheduling},
 abstract = {In this paper we systematically examine various performance issues involved in the coordinated allocation of processor and disk resources in large-scale parallel computer systems. Models are formulated to investigate the I/O and computation behavior of parallel programs and workloads, and to analyze parallel scheduling policies under such workloads. These models are parameterized by measurements of parallel programs, and they are solved via analytic methods and simulation. Our results provide important insights into the performance of parallel applications and resource management strategies when I/O demands are not negligible.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {56--65},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277873},
 doi = {http://doi.acm.org/10.1145/277851.277873},
 acmid = {277873},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rosti:1998:IIP:277858.277873,
 author = {Rosti, Emilia and Serazzi, Giuseppe and Smirni, Evgenia and Squillante, Mark S.},
 title = {The impact of I/O on program behavior and parallel scheduling},
 abstract = {In this paper we systematically examine various performance issues involved in the coordinated allocation of processor and disk resources in large-scale parallel computer systems. Models are formulated to investigate the I/O and computation behavior of parallel programs and workloads, and to analyze parallel scheduling policies under such workloads. These models are parameterized by measurements of parallel programs, and they are solved via analytic methods and simulation. Our results provide important insights into the performance of parallel applications and resource management strategies when I/O demands are not negligible.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {56--65},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277873},
 doi = {http://doi.acm.org/10.1145/277858.277873},
 acmid = {277873},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bajaj:1998:SPU:277851.277875,
 author = {Bajaj, Sandeep and Breslau, Lee and Shenker, Scott},
 title = {Is service priority useful in networks?},
 abstract = {A key question in the definition of new services for the Internet is whether to provide a single class of relaxed real-time service or multiple levels differentiated by their delay characteristics. In that context we pose the question: is service priority useful in networks? We argue that, contrary to some of our earlier work, to properly address this question one cannot just consider raw network-centric performance numbers, such as the delay distribution. Rather, one must incorporate two new elements into the analysis: the utility functions of the applications (how application performance depends on network service), and the adaptive nature of applications (how applications react to changing network service). This last point is especially crucial; modern Internet applications are designed to tolerate a wide range of network service quality, and they do so by adapting to the current network conditions. Most previous investigations of network performance have neglected to include this adaptive behavior.In this paper we present an analysis of service priority in the context of audio applications embodying these two elements: utility functions and adaptation. Our investigation is far from conclusive. The definitive answer to the question depends on many factors that are outside the scope of this paper and are, at present, unknowable, such as the burstiness of future Internet traffic and the relative offered loads of best-effort and real-time applications. Despite these shortcomings, our analysis illustrates this new approach to evaluating network design decisions, and sheds some light on the properties of adaptive applications.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277851.277875},
 doi = {http://doi.acm.org/10.1145/277851.277875},
 acmid = {277875},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bajaj:1998:SPU:277858.277875,
 author = {Bajaj, Sandeep and Breslau, Lee and Shenker, Scott},
 title = {Is service priority useful in networks?},
 abstract = {A key question in the definition of new services for the Internet is whether to provide a single class of relaxed real-time service or multiple levels differentiated by their delay characteristics. In that context we pose the question: is service priority useful in networks? We argue that, contrary to some of our earlier work, to properly address this question one cannot just consider raw network-centric performance numbers, such as the delay distribution. Rather, one must incorporate two new elements into the analysis: the utility functions of the applications (how application performance depends on network service), and the adaptive nature of applications (how applications react to changing network service). This last point is especially crucial; modern Internet applications are designed to tolerate a wide range of network service quality, and they do so by adapting to the current network conditions. Most previous investigations of network performance have neglected to include this adaptive behavior.In this paper we present an analysis of service priority in the context of audio applications embodying these two elements: utility functions and adaptation. Our investigation is far from conclusive. The definitive answer to the question depends on many factors that are outside the scope of this paper and are, at present, unknowable, such as the burstiness of future Internet traffic and the relative offered loads of best-effort and real-time applications. Despite these shortcomings, our analysis illustrates this new approach to evaluating network design decisions, and sheds some light on the properties of adaptive applications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {66--77},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277858.277875},
 doi = {http://doi.acm.org/10.1145/277858.277875},
 acmid = {277875},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kalampoukas:1998:ITT:277851.277877,
 author = {Kalampoukas, Lampros and Varma, Anujan and Ramakrishnan, K. K.},
 title = {Improving TCP throughput over two-way asymmetric links: analysis and solutions},
 abstract = {The sharing of a common buffer by TCP data segments and acknowledgments in a network or internet has been known to produce the effect of ack compression</i>, often causing dramatic reductions in throughput. We study several schemes for improving the performance of two-way TCP traffic over asymmetric links where the bandwidths in the two directions may differ substantially, possibly by many orders of magnitude. These approaches reduce the effect of ack compression by carefully controlling the flow of data packets and acknowledgments. We first examine a scheme where acknowledgments are transmitted at a higher priority than data. By analysis and simulation, we show that prioritizing acks can lead to starvation of the low-bandwidth connection. Next, we introduce and analyze a connection-level backpressure mechanism designed to limit the maximum amount of data buffered in the outgoing IP queue of the source of the low-bandwidth connection. We show that this approach, while minimizing the queueing delay for acks, results in unfair bandwidth allocation on the slow link. Finally, our preferred solution separates the acks from data packets in the outgoing queue, and makes use of a connection-level bandwidth allocation mechanism to control their bandwidth shares. We show that this scheme overcomes the limitations of the previous approaches, provides isolation, and enables precise control of the connection throughputs. We present analytical models of the dynamic behavior of each of these approaches, derive closed-form expressions for the expected connection efficiencies in each case, and validate them with simulation results.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {78--89},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277851.277877},
 doi = {http://doi.acm.org/10.1145/277851.277877},
 acmid = {277877},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kalampoukas:1998:ITT:277858.277877,
 author = {Kalampoukas, Lampros and Varma, Anujan and Ramakrishnan, K. K.},
 title = {Improving TCP throughput over two-way asymmetric links: analysis and solutions},
 abstract = {The sharing of a common buffer by TCP data segments and acknowledgments in a network or internet has been known to produce the effect of ack compression</i>, often causing dramatic reductions in throughput. We study several schemes for improving the performance of two-way TCP traffic over asymmetric links where the bandwidths in the two directions may differ substantially, possibly by many orders of magnitude. These approaches reduce the effect of ack compression by carefully controlling the flow of data packets and acknowledgments. We first examine a scheme where acknowledgments are transmitted at a higher priority than data. By analysis and simulation, we show that prioritizing acks can lead to starvation of the low-bandwidth connection. Next, we introduce and analyze a connection-level backpressure mechanism designed to limit the maximum amount of data buffered in the outgoing IP queue of the source of the low-bandwidth connection. We show that this approach, while minimizing the queueing delay for acks, results in unfair bandwidth allocation on the slow link. Finally, our preferred solution separates the acks from data packets in the outgoing queue, and makes use of a connection-level bandwidth allocation mechanism to control their bandwidth shares. We show that this scheme overcomes the limitations of the previous approaches, provides isolation, and enables precise control of the connection throughputs. We present analytical models of the dynamic behavior of each of these approaches, derive closed-form expressions for the expected connection efficiencies in each case, and validate them with simulation results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {78--89},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/277858.277877},
 doi = {http://doi.acm.org/10.1145/277858.277877},
 acmid = {277877},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raman:1998:ABG:277858.277880,
 author = {Raman, Suchitra and McCanne, Steven and Shenker, Scott},
 title = {Asymptotic behavior of global recovery in SRM},
 abstract = {The development and deployment of a large-scale, wide-area multicast infrastructure in the Internet has enabled a new family of multi-party, collaborative applications. Several of these applications, such as multimedia slide shows, shared whiteboards, and large-scale multi-player games, require reliable</i> multicast transport, yet the underlying multicast infrastructure provides only a best-effort delivery service. A difficult challenge in the design of efficient protocols that provide reliable service on top of the best-effort multicast service is to maintain acceptable performance as the protocol scales</i> to very large session sizes distributed across the wide area. The Scalable, Reliable Multicast (SRM) protocol [6] is a receiver-driven scheme based on negative acknowledgments (NACKs) reliable multicast protocol that uses randomized timers to limit the amount of protocol overhead in the face of large multicast groups, but the behavior of SRM at extremely large scales is not well-understood.In this paper, we use analysis and simulation to investigate the scaling behavior of global loss recovery in SRM. We study the protocol's control-traffic overhead as a function of group size for various topologies and protocol parameters, on a set of simple, representative topologies --- the cone (a variant of a clique), the linear chain, and the binary tree. We find that this overhead, as a function of group size, depends strongly on the topology: for the cone, it is always linear; for the chain, it is between constant and logarithmic; and for the tree, it is between constant and linear.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277880},
 doi = {http://doi.acm.org/10.1145/277858.277880},
 acmid = {277880},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Raman:1998:ABG:277851.277880,
 author = {Raman, Suchitra and McCanne, Steven and Shenker, Scott},
 title = {Asymptotic behavior of global recovery in SRM},
 abstract = {The development and deployment of a large-scale, wide-area multicast infrastructure in the Internet has enabled a new family of multi-party, collaborative applications. Several of these applications, such as multimedia slide shows, shared whiteboards, and large-scale multi-player games, require reliable</i> multicast transport, yet the underlying multicast infrastructure provides only a best-effort delivery service. A difficult challenge in the design of efficient protocols that provide reliable service on top of the best-effort multicast service is to maintain acceptable performance as the protocol scales</i> to very large session sizes distributed across the wide area. The Scalable, Reliable Multicast (SRM) protocol [6] is a receiver-driven scheme based on negative acknowledgments (NACKs) reliable multicast protocol that uses randomized timers to limit the amount of protocol overhead in the face of large multicast groups, but the behavior of SRM at extremely large scales is not well-understood.In this paper, we use analysis and simulation to investigate the scaling behavior of global loss recovery in SRM. We study the protocol's control-traffic overhead as a function of group size for various topologies and protocol parameters, on a set of simple, representative topologies --- the cone (a variant of a clique), the linear chain, and the binary tree. We find that this overhead, as a function of group size, depends strongly on the topology: for the cone, it is always linear; for the chain, it is between constant and logarithmic; and for the tree, it is between constant and linear.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {90--99},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277880},
 doi = {http://doi.acm.org/10.1145/277851.277880},
 acmid = {277880},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boxma:1998:BPF:277858.277881,
 author = {Boxma, O. J. and Dumas, V.},
 title = {The busy period in the fluid queue},
 abstract = {Consider a fluid queue fed by N</i> on/off sources. It is assumed that the silence periods of the sources are exponentially distributed, whereas the activity periods are generally distributed. The inflow rate of each source, when active, is at least as large as the outflow rate of the buffer.We make two contributions to the performance analysis of this model. Firstly, we determine the Laplace-Stieltjes transforms of the distributions of the busy periods that start with an active period of source i, i</i> = 1,\&amp;hellip;,N</i>, as the unique solution in [0, 1]<sup>N</sup> of a set of N</i> equations. Thus we also find the Laplace-Stieltjes transform of the distribution of an arbitrary busy period.Secondly, we relate the tail behaviour of the busy period distributions to the tail behaviour of the activity period distributions. We show that the tails of all busy period distributions are regularly varying of index - \&amp;nu; iff the heaviest of the tails of the activity period distributions are regularly varying of index - \&amp;nu; We provide explicit equivalents of the former in terms of the latter, which show that the contribution of the sources with lighter associated tails is equivalent to a simple reduction of the outflow rate. These results have implications for the performance analysis of networks of fluid queues.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {100--110},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277881},
 doi = {http://doi.acm.org/10.1145/277858.277881},
 acmid = {277881},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boxma:1998:BPF:277851.277881,
 author = {Boxma, O. J. and Dumas, V.},
 title = {The busy period in the fluid queue},
 abstract = {Consider a fluid queue fed by N</i> on/off sources. It is assumed that the silence periods of the sources are exponentially distributed, whereas the activity periods are generally distributed. The inflow rate of each source, when active, is at least as large as the outflow rate of the buffer.We make two contributions to the performance analysis of this model. Firstly, we determine the Laplace-Stieltjes transforms of the distributions of the busy periods that start with an active period of source i, i</i> = 1,\&amp;hellip;,N</i>, as the unique solution in [0, 1]<sup>N</sup> of a set of N</i> equations. Thus we also find the Laplace-Stieltjes transform of the distribution of an arbitrary busy period.Secondly, we relate the tail behaviour of the busy period distributions to the tail behaviour of the activity period distributions. We show that the tails of all busy period distributions are regularly varying of index - \&amp;nu; iff the heaviest of the tails of the activity period distributions are regularly varying of index - \&amp;nu; We provide explicit equivalents of the former in terms of the latter, which show that the contribution of the sources with lighter associated tails is equivalent to a simple reduction of the outflow rate. These results have implications for the performance analysis of networks of fluid queues.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {100--110},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277881},
 doi = {http://doi.acm.org/10.1145/277851.277881},
 acmid = {277881},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Li:1998:TLP:277858.277884,
 author = {Li, Guang-Liang and Cui, Jun-Hong and Li, Bo and Li, Fang-Ming},
 title = {Transient loss performance of a class of finite buffer queueing systems},
 abstract = {Performance-oriented studies typically rely on the assumption that the stochastic process modeling the phenomenon of interest is already in steady state. This assumption is, however, not valid if the life cycle of the phenomenon under study is not large enough, since usually a stochastic process cannot reach steady state unless time evolves towards infinity. Therefore, it is important to address performance issues in transient state.Previous work in transient analysis of queueing systems usually focuses on Markov models. This paper, in contrast, presents an analysis of transient loss performance for a class of finite buffer queueing systems that are not necessarily Markovian. We obtain closed-form transient loss performance measures. Based on the loss measures, we compare transient loss performance against steady-state loss performance and examine how different assumptions on the arrival process will affect transient loss behavior of the queueing system. We also discuss how to guarantee transient loss performance. The analysis is illustrated with numerical results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {111--120},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277884},
 doi = {http://doi.acm.org/10.1145/277858.277884},
 acmid = {277884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {queueing systems, stochastic modeling, transient loss performance},
} 

@inproceedings{Li:1998:TLP:277851.277884,
 author = {Li, Guang-Liang and Cui, Jun-Hong and Li, Bo and Li, Fang-Ming},
 title = {Transient loss performance of a class of finite buffer queueing systems},
 abstract = {Performance-oriented studies typically rely on the assumption that the stochastic process modeling the phenomenon of interest is already in steady state. This assumption is, however, not valid if the life cycle of the phenomenon under study is not large enough, since usually a stochastic process cannot reach steady state unless time evolves towards infinity. Therefore, it is important to address performance issues in transient state.Previous work in transient analysis of queueing systems usually focuses on Markov models. This paper, in contrast, presents an analysis of transient loss performance for a class of finite buffer queueing systems that are not necessarily Markovian. We obtain closed-form transient loss performance measures. Based on the loss measures, we compare transient loss performance against steady-state loss performance and examine how different assumptions on the arrival process will affect transient loss behavior of the queueing system. We also discuss how to guarantee transient loss performance. The analysis is illustrated with numerical results.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {111--120},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277884},
 doi = {http://doi.acm.org/10.1145/277851.277884},
 acmid = {277884},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {queueing systems, stochastic modeling, transient loss performance},
} 

@article{McKinnon:1998:QAB:277858.277888,
 author = {McKinnon, Martin W. and Rouskas, George N. and Perros, Harry G.},
 title = {Queueing-based analysis of broadcast optical networks},
 abstract = {We consider broadcast WDM networks operating with schedules that mask the transceiver tuning latency. We develop and analyze a queueing model of the network in order to obtain the queue-length distribution and the packet loss probability at the transmitting and receiving side of the nodes. The analysis is carried out assuming finite buffer sizes, non-uniform destination probabilities and two-state MMBP traffic sources; the latter naturally capture the notion of burstiness and correlation, two important characteristics of traffic in high-speed networks. We present results which establish that the performance of the network is a complex function of a number of system parameters, including the load balancing and scheduling algorithms, the number of available channels, and the buffer capacity. We also show that the behavior of the network in terms of packet loss probability as these parameters are varied cannot be predicted without an accurate analysis. Our work makes it possible to study the interactions among the system parameters, and to predict, explain and fine tune the performance of the network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277888},
 doi = {http://doi.acm.org/10.1145/277858.277888},
 acmid = {277888},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov modulated Bernoulli process, discrete-time queueing networks, optical networks, wavelength division multiplexing},
} 

@inproceedings{McKinnon:1998:QAB:277851.277888,
 author = {McKinnon, Martin W. and Rouskas, George N. and Perros, Harry G.},
 title = {Queueing-based analysis of broadcast optical networks},
 abstract = {We consider broadcast WDM networks operating with schedules that mask the transceiver tuning latency. We develop and analyze a queueing model of the network in order to obtain the queue-length distribution and the packet loss probability at the transmitting and receiving side of the nodes. The analysis is carried out assuming finite buffer sizes, non-uniform destination probabilities and two-state MMBP traffic sources; the latter naturally capture the notion of burstiness and correlation, two important characteristics of traffic in high-speed networks. We present results which establish that the performance of the network is a complex function of a number of system parameters, including the load balancing and scheduling algorithms, the number of available channels, and the buffer capacity. We also show that the behavior of the network in terms of packet loss probability as these parameters are varied cannot be predicted without an accurate analysis. Our work makes it possible to study the interactions among the system parameters, and to predict, explain and fine tune the performance of the network.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277888},
 doi = {http://doi.acm.org/10.1145/277851.277888},
 acmid = {277888},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov modulated Bernoulli process, discrete-time queueing networks, optical networks, wavelength division multiplexing},
} 

@inproceedings{Bavier:1998:PME:277851.277892,
 author = {Bavier, Andy C. and Montz, A. Brady and Peterson, Larry L.},
 title = {Predicting MPEG execution times},
 abstract = {This paper reports on a set of experiments that measure the amount of CPU processing needed to decode MPEG-compressed video in software. These experiments were designed to discover indicators that could be used to predict how many cycles are required to decode a given frame. Such predictors can be used to do more accurate CPU scheduling. We found that by considering both frame type and size, it is possible to construct a linear model of MPEG decoding with R</i><sup>2</sup> values of 0.97 and higher. Moreover, this model can be used to predict decoding times at both the frame and packet level that are almost always accurate to within 25\% of the actual decode times. This is a surprising result given the large variability in MPEG decoding times, and suggests that it is feasible to design systems that make quality of service guarantees for MPEG-encoded video.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {131--140},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277892},
 doi = {http://doi.acm.org/10.1145/277851.277892},
 acmid = {277892},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bavier:1998:PME:277858.277892,
 author = {Bavier, Andy C. and Montz, A. Brady and Peterson, Larry L.},
 title = {Predicting MPEG execution times},
 abstract = {This paper reports on a set of experiments that measure the amount of CPU processing needed to decode MPEG-compressed video in software. These experiments were designed to discover indicators that could be used to predict how many cycles are required to decode a given frame. Such predictors can be used to do more accurate CPU scheduling. We found that by considering both frame type and size, it is possible to construct a linear model of MPEG decoding with R</i><sup>2</sup> values of 0.97 and higher. Moreover, this model can be used to predict decoding times at both the frame and packet level that are almost always accurate to within 25\% of the actual decode times. This is a surprising result given the large variability in MPEG decoding times, and suggests that it is feasible to design systems that make quality of service guarantees for MPEG-encoded video.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {131--140},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277892},
 doi = {http://doi.acm.org/10.1145/277858.277892},
 acmid = {277892},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gribble:1998:SFS:277851.277894,
 author = {Gribble, Steven D. and Manku, Gurmeet Singh and Roselli, Drew and Brewer, Eric A. and Gibson, Timothy J. and Miller, Ethan L.},
 title = {Self-similarity in file systems},
 abstract = {We demonstrate that high-level file system events exhibit self-similar behaviour, but only for short-term time scales of approximately under a day. We do so through the analysis of four sets of traces that span time scales of milliseconds through months, and that differ in the trace collection method, the filesystems being traced, and the chronological times of the tracing. Two sets of detailed, short-term file system trace data are analyzed; both are shown to have self-similar like behaviour, with consistent Hurst parameters (a measure of self-similarity) for all file system traffic as well as individual classes of file system events. Long-term file system trace data is then analyzed, and we discover that the traces' high variability and self-similar behaviour does not persist across time scales of days, weeks, and months. Using the short-term trace data, we show that sources of file system traffic exhibit ON/OFF source behaviour, which is characterized by highly variably lengthed bursts of activity, followed by similarly variably lengthed periods of inactivity. This ON/OFF behaviour is used to motivate a simple technique for synthesizing a stream of events that exhibit the same self-similar short-term behaviour as was observed in the file system traces.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277894},
 doi = {http://doi.acm.org/10.1145/277851.277894},
 acmid = {277894},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gribble:1998:SFS:277858.277894,
 author = {Gribble, Steven D. and Manku, Gurmeet Singh and Roselli, Drew and Brewer, Eric A. and Gibson, Timothy J. and Miller, Ethan L.},
 title = {Self-similarity in file systems},
 abstract = {We demonstrate that high-level file system events exhibit self-similar behaviour, but only for short-term time scales of approximately under a day. We do so through the analysis of four sets of traces that span time scales of milliseconds through months, and that differ in the trace collection method, the filesystems being traced, and the chronological times of the tracing. Two sets of detailed, short-term file system trace data are analyzed; both are shown to have self-similar like behaviour, with consistent Hurst parameters (a measure of self-similarity) for all file system traffic as well as individual classes of file system events. Long-term file system trace data is then analyzed, and we discover that the traces' high variability and self-similar behaviour does not persist across time scales of days, weeks, and months. Using the short-term trace data, we show that sources of file system traffic exhibit ON/OFF source behaviour, which is characterized by highly variably lengthed bursts of activity, followed by similarly variably lengthed periods of inactivity. This ON/OFF behaviour is used to motivate a simple technique for synthesizing a stream of events that exhibit the same self-similar short-term behaviour as was observed in the file system traces.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277894},
 doi = {http://doi.acm.org/10.1145/277858.277894},
 acmid = {277894},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barford:1998:GRW:277851.277897,
 author = {Barford, Paul and Crovella, Mark},
 title = {Generating representative Web workloads for network and server performance evaluation},
 abstract = {One role for workload generation is as a means for understanding how servers and networks respond to variation in load. This enables management and capacity planning based on current and projected usage. This paper applies a number of observations of Web server usage to create a realistic Web workload generation tool which mimics a set of real users accessing a server. The tool, called <sc>Surge</sc> (Scalable URL Reference Generator) generates references matching empirical measurements of 1) server file size distribution; 2) request size distribution; 3) relative file popularity; 4) embedded file references; 5) temporal locality of reference; and 6) idle periods of individual users. This paper reviews the essential elements required in the generation of a representative Web workload. It also addresses the technical challenges to satisfying this large set of simultaneous constraints on the properties of the reference stream, the solutions we adopted, and their associated accuracy. Finally, we present evidence that <sc>Surge</sc> exercises servers in a manner significantly different from other Web server benchmarks.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {151--160},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277897},
 doi = {http://doi.acm.org/10.1145/277851.277897},
 acmid = {277897},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barford:1998:GRW:277858.277897,
 author = {Barford, Paul and Crovella, Mark},
 title = {Generating representative Web workloads for network and server performance evaluation},
 abstract = {One role for workload generation is as a means for understanding how servers and networks respond to variation in load. This enables management and capacity planning based on current and projected usage. This paper applies a number of observations of Web server usage to create a realistic Web workload generation tool which mimics a set of real users accessing a server. The tool, called <sc>Surge</sc> (Scalable URL Reference Generator) generates references matching empirical measurements of 1) server file size distribution; 2) request size distribution; 3) relative file popularity; 4) embedded file references; 5) temporal locality of reference; and 6) idle periods of individual users. This paper reviews the essential elements required in the generation of a representative Web workload. It also addresses the technical challenges to satisfying this large set of simultaneous constraints on the properties of the reference stream, the solutions we adopted, and their associated accuracy. Finally, we present evidence that <sc>Surge</sc> exercises servers in a manner significantly different from other Web server benchmarks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {151--160},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277897},
 doi = {http://doi.acm.org/10.1145/277858.277897},
 acmid = {277897},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ji:1998:PMM:277858.277900,
 author = {Ji, Minwen and Felten, Edward W. and Li, Kai},
 title = {Performance measurements for multithreaded programs},
 abstract = {Multithreaded programming is an effective way to exploit concurrency, but it is difficult to debug and tune a highly threaded program. This paper describes a performance tool called Tmon for monitoring, analyzing and tuning the performance of multithreaded programs. The performance tool has two novel features: it uses "thread waiting time" as a measure and constructs thread waiting graphs to show thread dependencies and thus performance bottlenecks, and it identifies "semi-busy-waiting" points where CPU cycles are wasted in condition checking and context switching. We have implemented the Tmon tool and, as a case study, we have used it to measure and tune a heavily threaded file system. We used four workloads to tune different aspects of the file system. We were able to improve the file system bandwidth and throughput significantly. In one case, we were able to improve the bandwidth by two orders of magnitude.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277900},
 doi = {http://doi.acm.org/10.1145/277858.277900},
 acmid = {277900},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ji:1998:PMM:277851.277900,
 author = {Ji, Minwen and Felten, Edward W. and Li, Kai},
 title = {Performance measurements for multithreaded programs},
 abstract = {Multithreaded programming is an effective way to exploit concurrency, but it is difficult to debug and tune a highly threaded program. This paper describes a performance tool called Tmon for monitoring, analyzing and tuning the performance of multithreaded programs. The performance tool has two novel features: it uses "thread waiting time" as a measure and constructs thread waiting graphs to show thread dependencies and thus performance bottlenecks, and it identifies "semi-busy-waiting" points where CPU cycles are wasted in condition checking and context switching. We have implemented the Tmon tool and, as a case study, we have used it to measure and tune a heavily threaded file system. We used four workloads to tune different aspects of the file system. We were able to improve the file system bandwidth and throughput significantly. In one case, we were able to improve the bandwidth by two orders of magnitude.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277900},
 doi = {http://doi.acm.org/10.1145/277851.277900},
 acmid = {277900},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jiang:1998:MES:277858.277902,
 author = {Jiang, Dongming and Singh, Jaswinder Pal},
 title = {A methodology and an evaluation of the SGI Origin2000},
 abstract = {As hardware-coherent, distributed shared memory (DSM) multiprocessing becomes popular commercially, it is important to evaluate modern realizations to understand how they perform and scale for a range of interesting applications and to identify the nature of the key bottlenecks. This paper evaluates the SGI Origin2000---the machine that perhaps has the most aggressive communication architecture of the recent cache-coherent offerings---and, in doing so, articulates a sound methodology for evaluating real systems. We examine data access and synchronization microbenchmarks; speedups for different application classes, problem sizes and scaling models; detailed interactions and time breakdowns using performance tools; and the impact of special hardware support. We find that overall the Origin appears to deliver on the promise of cache-coherent shared address space multiprocessing, at least at the 32-processor scale we examine. The machine is quite easy to program for performance and has fewer organizational problems than previous systems we have examined. However, some important trouble spots are also identified, especially related to contention that is apparently caused by engineering decisions to share resources among processors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {171--181},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277902},
 doi = {http://doi.acm.org/10.1145/277858.277902},
 acmid = {277902},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jiang:1998:MES:277851.277902,
 author = {Jiang, Dongming and Singh, Jaswinder Pal},
 title = {A methodology and an evaluation of the SGI Origin2000},
 abstract = {As hardware-coherent, distributed shared memory (DSM) multiprocessing becomes popular commercially, it is important to evaluate modern realizations to understand how they perform and scale for a range of interesting applications and to identify the nature of the key bottlenecks. This paper evaluates the SGI Origin2000---the machine that perhaps has the most aggressive communication architecture of the recent cache-coherent offerings---and, in doing so, articulates a sound methodology for evaluating real systems. We examine data access and synchronization microbenchmarks; speedups for different application classes, problem sizes and scaling models; detailed interactions and time breakdowns using performance tools; and the impact of special hardware support. We find that overall the Origin appears to deliver on the promise of cache-coherent shared address space multiprocessing, at least at the 32-processor scale we examine. The machine is quite easy to program for performance and has fewer organizational problems than previous systems we have examined. However, some important trouble spots are also identified, especially related to contention that is apparently caused by engineering decisions to share resources among processors.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {171--181},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277902},
 doi = {http://doi.acm.org/10.1145/277851.277902},
 acmid = {277902},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shriver:1998:ABM:277851.277906,
 author = {Shriver, Elizabeth and Merchant, Arif and Wilkes, John},
 title = {An analytic behavior model for disk drives with readahead caches and request reordering},
 abstract = {Modern disk drives read-ahead data and reorder incoming requests in a workload-dependent fashion. This improves their performance, but makes simple analytical models of them inadequate for performance prediction, capacity planning, workload balancing, and so on. To address this problem we have developed a new analytic model for disk drives that do readahead and request reordering. We did so by developing performance models of the disk drive components (queues, caches, and the disk mechanism) and a workload transformation technique for composing them. Our model includes the effects of workload-specific parameters such as request size and spatial locality. The result is capable of predicting the behavior of a variety of real-world devices to within 17\% across a variety of workloads and disk drives.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {182--191},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277906},
 doi = {http://doi.acm.org/10.1145/277851.277906},
 acmid = {277906},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shriver:1998:ABM:277858.277906,
 author = {Shriver, Elizabeth and Merchant, Arif and Wilkes, John},
 title = {An analytic behavior model for disk drives with readahead caches and request reordering},
 abstract = {Modern disk drives read-ahead data and reorder incoming requests in a workload-dependent fashion. This improves their performance, but makes simple analytical models of them inadequate for performance prediction, capacity planning, workload balancing, and so on. To address this problem we have developed a new analytic model for disk drives that do readahead and request reordering. We did so by developing performance models of the disk drive components (queues, caches, and the disk mechanism) and a workload transformation technique for composing them. Our model includes the effects of workload-specific parameters such as request size and spatial locality. The result is capable of predicting the behavior of a variety of real-world devices to within 17\% across a variety of workloads and disk drives.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {182--191},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277906},
 doi = {http://doi.acm.org/10.1145/277858.277906},
 acmid = {277906},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fraguela:1998:MSA:277851.277910,
 author = {Fraguela, Basilio B. and Doallo, Ram\'{o}n and Zapata, Emilio L.},
 title = {Modeling set associative caches behavior for irregular computations},
 abstract = {While much work has been devoted to the study of cache behavior during the execution of codes with regular access patterns, little attention has been paid to irregular codes. An important portion of these codes are scientific applications that handle compressed sparse matrices. In this work a probabilistic model for the prediction of the number of misses on a K</i>-way associative cache memory considering sparse matrices with a uniform or banded distribution is presented. Two different irregular kernels are considered: the sparse matrix-vector product and the transposition of a sparse matrix. The model was validated with simulations on synthetic uniform matrices and banded matrices from the Harwell-Boeing collection.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {192--201},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277910},
 doi = {http://doi.acm.org/10.1145/277851.277910},
 acmid = {277910},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache performance, irregular computation, probabilistic model, sparse matrix},
} 

@article{Fraguela:1998:MSA:277858.277910,
 author = {Fraguela, Basilio B. and Doallo, Ram\'{o}n and Zapata, Emilio L.},
 title = {Modeling set associative caches behavior for irregular computations},
 abstract = {While much work has been devoted to the study of cache behavior during the execution of codes with regular access patterns, little attention has been paid to irregular codes. An important portion of these codes are scientific applications that handle compressed sparse matrices. In this work a probabilistic model for the prediction of the number of misses on a K</i>-way associative cache memory considering sparse matrices with a uniform or banded distribution is presented. Two different irregular kernels are considered: the sparse matrix-vector product and the transposition of a sparse matrix. The model was validated with simulations on synthetic uniform matrices and banded matrices from the Harwell-Boeing collection.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {192--201},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277910},
 doi = {http://doi.acm.org/10.1145/277858.277910},
 acmid = {277910},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache performance, irregular computation, probabilistic model, sparse matrix},
} 

@article{Jiang:1998:IFN:277858.277913,
 author = {Jiang, Tianji and Ammar, Mostafa H. and Zegura, Ellen W.},
 title = {Inter-receiver fairness: a novel performance measure for multicast ABR sessions},
 abstract = {In a multicast ABR service, a connection is typically restricted to the rate allowed on the bottleneck link in the distribution tree from the source to the set of receivers. Because of this, receivers in the connection can experience inter-receiver unfairness</i>, when the preferred operating rates of the receivers are different. In this paper we explore the issue of improving the inter-receiver fairness in a multicast ABR connection by allowing the connection to operate at a rate higher than what is allowed by the multicast tree's bottleneck link. Since this can result in cell loss to some receivers, we operate with the knowledge of each receiver's application-specific loss tolerance. The multicast connection rate is not allowed to increase beyond the point where the cell loss on a path to a receiver exceeds this receiver's loss tolerance. Based on these ideas we develop an inter-receiver fairness measure and a technique for determining the rate that maximizes this measure. We show possible switch algorithms that can be used to convey the parameters needed to compute the function to the connection's source. In addition we develop a global network measure that helps us assess the effect of increasing inter-receiver fairness on the total network delivered throughput. We also briefly explore improving inter-receiver fairness through the use of multiple virtual circuits to carry traffic for a single multicast session. A set of examples demonstrate the use of the inter-receiver fairness concept in various network scenarios.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {202--211},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277913},
 doi = {http://doi.acm.org/10.1145/277858.277913},
 acmid = {277913},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jiang:1998:IFN:277851.277913,
 author = {Jiang, Tianji and Ammar, Mostafa H. and Zegura, Ellen W.},
 title = {Inter-receiver fairness: a novel performance measure for multicast ABR sessions},
 abstract = {In a multicast ABR service, a connection is typically restricted to the rate allowed on the bottleneck link in the distribution tree from the source to the set of receivers. Because of this, receivers in the connection can experience inter-receiver unfairness</i>, when the preferred operating rates of the receivers are different. In this paper we explore the issue of improving the inter-receiver fairness in a multicast ABR connection by allowing the connection to operate at a rate higher than what is allowed by the multicast tree's bottleneck link. Since this can result in cell loss to some receivers, we operate with the knowledge of each receiver's application-specific loss tolerance. The multicast connection rate is not allowed to increase beyond the point where the cell loss on a path to a receiver exceeds this receiver's loss tolerance. Based on these ideas we develop an inter-receiver fairness measure and a technique for determining the rate that maximizes this measure. We show possible switch algorithms that can be used to convey the parameters needed to compute the function to the connection's source. In addition we develop a global network measure that helps us assess the effect of increasing inter-receiver fairness on the total network delivered throughput. We also briefly explore improving inter-receiver fairness through the use of multiple virtual circuits to carry traffic for a single multicast session. A set of examples demonstrate the use of the inter-receiver fairness concept in various network scenarios.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {202--211},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277913},
 doi = {http://doi.acm.org/10.1145/277851.277913},
 acmid = {277913},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Courcoubetis:1998:AEL:277851.277915,
 author = {Courcoubetis, Costas and Siris, Vasilios A. and Stamoulis, George D.},
 title = {Application and evaluation of large deviation techniques for traffic engineering in broadband networks},
 abstract = {Accurate yet simple methods for traffic engineering are important for efficient dimensioning of broadband networks. The goal of this paper is to apply and evaluate large deviation techniques for traffic engineering. In particular, we employ the recently developed theory of effective bandwidths</i>, where the effective bandwidth depends not only on the statistical characteristics of the traffic stream, but also on a link's operating point through two parameters, the space</i> and time</i> parameters, which are computed using the many sources asymptotic</i>. We show that this effective bandwidth definition can accurately quantify resource usage. Furthermore, we estimate and interpret values of the space and time parameters for various mixes of real traffic demonstrating how these values can be used to clarify the effects on the link performance of the time scales of burstiness of the traffic input, of the link parameters (capacity and buffer), and of traffic control mechanisms, such as traffic shaping. Our approach relies on off-line analysis of traffic traces, the granularity of which is determined by the time parameter of the link, and our experiments involve a large set of MPEG-1 compressed video and Internet Wide Area Network (WAN) traces, as well as modeled voice traffic.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277915},
 doi = {http://doi.acm.org/10.1145/277851.277915},
 acmid = {277915},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ATM, broadband networks, effective bandwidths, large deviations, traffic engineering},
} 

@article{Courcoubetis:1998:AEL:277858.277915,
 author = {Courcoubetis, Costas and Siris, Vasilios A. and Stamoulis, George D.},
 title = {Application and evaluation of large deviation techniques for traffic engineering in broadband networks},
 abstract = {Accurate yet simple methods for traffic engineering are important for efficient dimensioning of broadband networks. The goal of this paper is to apply and evaluate large deviation techniques for traffic engineering. In particular, we employ the recently developed theory of effective bandwidths</i>, where the effective bandwidth depends not only on the statistical characteristics of the traffic stream, but also on a link's operating point through two parameters, the space</i> and time</i> parameters, which are computed using the many sources asymptotic</i>. We show that this effective bandwidth definition can accurately quantify resource usage. Furthermore, we estimate and interpret values of the space and time parameters for various mixes of real traffic demonstrating how these values can be used to clarify the effects on the link performance of the time scales of burstiness of the traffic input, of the link parameters (capacity and buffer), and of traffic control mechanisms, such as traffic shaping. Our approach relies on off-line analysis of traffic traces, the granularity of which is determined by the time parameter of the link, and our experiments involve a large set of MPEG-1 compressed video and Internet Wide Area Network (WAN) traces, as well as modeled voice traffic.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277915},
 doi = {http://doi.acm.org/10.1145/277858.277915},
 acmid = {277915},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ATM, broadband networks, effective bandwidths, large deviations, traffic engineering},
} 

@inproceedings{Neidhardt:1998:CRT:277851.277923,
 author = {Neidhardt, Arnold L. and Wang, Jonathan L.},
 title = {The concept of relevant time scales and its application to queuing analysis of self-similar traffic (or is Hurst naughty or nice?)},
 abstract = {Recent traffic analyses from various packet networks have shown the existence of long-range dependence in bursty traffic. In evaluating its impact on queuing performance, earlier investigations have noted how the presence of long-range dependence, or a high value of the Hurst parameter H</i>, is often associated with surprisingly large queue sizes. As a result, a common impression has been created of expecting queuing performance to be worse as H</i> increases, but this impression can be misleading. In fact, there are examples in which larger values of H</i> are associated with smaller queues. So the question is how can one tell whether queuing performance would improve or degrade as H</i> rises? In this paper, we show that the relative queuing performance can be assessed by identifying a couple of time scales. First, in comparing a high-H</i> process with a low-H</i> process, there is a unique time scale t<inf>m</inf></i> at which the variances of the two processes match (assuming exact, second-order self similarity for both processes). Second, there are time scales t<inf>qi</inf></i> that are most relevant for queuing the arrivals of process i</i>. If both of the queuing scales t<inf>qi</inf></i> exceed the variance-matching scale t<inf>m</inf></i>, then the high-H</i> queue is worse; if the queuing scales are smaller, then the low-H</i> queue is worse. However, no firm prediction can be made in the remaining case of t<inf>m</inf></i> falling between the two queuing scales. Numerical examples are given to demonstrate our results.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277923},
 doi = {http://doi.acm.org/10.1145/277851.277923},
 acmid = {277923},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Neidhardt:1998:CRT:277858.277923,
 author = {Neidhardt, Arnold L. and Wang, Jonathan L.},
 title = {The concept of relevant time scales and its application to queuing analysis of self-similar traffic (or is Hurst naughty or nice?)},
 abstract = {Recent traffic analyses from various packet networks have shown the existence of long-range dependence in bursty traffic. In evaluating its impact on queuing performance, earlier investigations have noted how the presence of long-range dependence, or a high value of the Hurst parameter H</i>, is often associated with surprisingly large queue sizes. As a result, a common impression has been created of expecting queuing performance to be worse as H</i> increases, but this impression can be misleading. In fact, there are examples in which larger values of H</i> are associated with smaller queues. So the question is how can one tell whether queuing performance would improve or degrade as H</i> rises? In this paper, we show that the relative queuing performance can be assessed by identifying a couple of time scales. First, in comparing a high-H</i> process with a low-H</i> process, there is a unique time scale t<inf>m</inf></i> at which the variances of the two processes match (assuming exact, second-order self similarity for both processes). Second, there are time scales t<inf>qi</inf></i> that are most relevant for queuing the arrivals of process i</i>. If both of the queuing scales t<inf>qi</inf></i> exceed the variance-matching scale t<inf>m</inf></i>, then the high-H</i> queue is worse; if the queuing scales are smaller, then the low-H</i> queue is worse. However, no firm prediction can be made in the remaining case of t<inf>m</inf></i> falling between the two queuing scales. Numerical examples are given to demonstrate our results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277923},
 doi = {http://doi.acm.org/10.1145/277858.277923},
 acmid = {277923},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arpaci-Dusseau:1998:SII:277851.277927,
 author = {Arpaci-Dusseau, Andrea C. and Culler, David E. and Mainwaring, Alan M.},
 title = {Scheduling with implicit information in distributed systems},
 abstract = {Implicit coscheduling</i> is a distributed algorithm for time-sharing communicating processes in a cluster of workstations. By observing and reacting to implicit information, local schedulers in the system make independent decisions that dynamically coordinate the scheduling of communicating processes. The principal mechanism involved is two-phase spin-blocking</i>: a process waiting for a message response spins for some amount of time, and then relinquishes the processor if the response does not arrive.In this paper, we describe our experience implementing implicit coscheduling on a cluster of 16 UltraSPARC I workstations; this has led to contributions in three main areas. First, we more rigorously analyze the two-phase spin-block algorithm and show that spin time should be increased when a process is receiving messages. Second, we present performance measurements for a wide range of synthetic benchmarks and for seven Split-C parallel applications. Finally, we show how implicit coscheduling behaves under different job layouts and scaling, and discuss preliminary results for achieving fairness.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277851.277927},
 doi = {http://doi.acm.org/10.1145/277851.277927},
 acmid = {277927},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arpaci-Dusseau:1998:SII:277858.277927,
 author = {Arpaci-Dusseau, Andrea C. and Culler, David E. and Mainwaring, Alan M.},
 title = {Scheduling with implicit information in distributed systems},
 abstract = {Implicit coscheduling</i> is a distributed algorithm for time-sharing communicating processes in a cluster of workstations. By observing and reacting to implicit information, local schedulers in the system make independent decisions that dynamically coordinate the scheduling of communicating processes. The principal mechanism involved is two-phase spin-blocking</i>: a process waiting for a message response spins for some amount of time, and then relinquishes the processor if the response does not arrive.In this paper, we describe our experience implementing implicit coscheduling on a cluster of 16 UltraSPARC I workstations; this has led to contributions in three main areas. First, we more rigorously analyze the two-phase spin-block algorithm and show that spin time should be increased when a process is receiving messages. Second, we present performance measurements for a wide range of synthetic benchmarks and for seven Split-C parallel applications. Finally, we show how implicit coscheduling behaves under different job layouts and scaling, and discuss preliminary results for achieving fairness.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {233--243},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/277858.277927},
 doi = {http://doi.acm.org/10.1145/277858.277927},
 acmid = {277927},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nguyen:1998:SPS:277858.277930,
 author = {Nguyen, Thu D. and Zahorjan, John},
 title = {Scheduling policies to support distributed 3D multimedia applications},
 abstract = {We consider the problem of scheduling the rendering component of 3D multimedia applications on a cluster of workstations connected via a local area network. Our goal is to meet a periodic real-time constraint.In abstract terms, the problem we address is how best to schedule tasks with unpredictable service times on distinct processing nodes so as to meet a real-time deadline, given that all communication among nodes entails some (possibly large) overhead. We consider two distinct classes of schemes, static,</i> in which task reallocations are scheduled to occur at specific times, and dynamic,</i> in which reallocations are triggered by some processor going idle. For both classes we further examine both global</i> reassignments, in which all nodes are rescheduled at a rescheduling moment, and local</i> reassignments, in which only a subset of the nodes engage in rescheduling at any one time.We show that global dynamic policies work best over a range of parameterizations appropriate to such systems. We introduce a new policy, Dynamic with Shadowing, that places a small number of tasks in the schedules of multiple workstations to reduce the amount of communication required to complete the schedule. This policy is shown to dominate the other alternatives considered over most of the parameter space.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {244--253},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277930},
 doi = {http://doi.acm.org/10.1145/277858.277930},
 acmid = {277930},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nguyen:1998:SPS:277851.277930,
 author = {Nguyen, Thu D. and Zahorjan, John},
 title = {Scheduling policies to support distributed 3D multimedia applications},
 abstract = {We consider the problem of scheduling the rendering component of 3D multimedia applications on a cluster of workstations connected via a local area network. Our goal is to meet a periodic real-time constraint.In abstract terms, the problem we address is how best to schedule tasks with unpredictable service times on distinct processing nodes so as to meet a real-time deadline, given that all communication among nodes entails some (possibly large) overhead. We consider two distinct classes of schemes, static,</i> in which task reallocations are scheduled to occur at specific times, and dynamic,</i> in which reallocations are triggered by some processor going idle. For both classes we further examine both global</i> reassignments, in which all nodes are rescheduled at a rescheduling moment, and local</i> reassignments, in which only a subset of the nodes engage in rescheduling at any one time.We show that global dynamic policies work best over a range of parameterizations appropriate to such systems. We introduce a new policy, Dynamic with Shadowing, that places a small number of tasks in the schedules of multiple workstations to reduce the amount of communication required to complete the schedule. This policy is shown to dominate the other alternatives considered over most of the parameter space.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {244--253},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277930},
 doi = {http://doi.acm.org/10.1145/277851.277930},
 acmid = {277930},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Moritz:1998:LMN:277851.277933,
 author = {Moritz, Csaba Andras and Frank, Matthew I.},
 title = {LoGPC: modeling network contention in message-passing programs},
 abstract = {In many real applications, for example those with frequent and irregular communication patterns or those using large messages, network contention and contention for message processing resources can be a significant part of the total execution time. This paper presents a new cost model, called LoGPC, that extends the LogP [9] and LogGP [4] models to account for the impact of network contention and network interface DMA behavior on the performance of message-passing programs.We validate LoGPC by analyzing three applications implemented with Active Messages [11, 18] on the MIT Alewife multiprocessor. Our analysis shows that network contention accounts for up to 50\% of the total execution time. In addition, we show that the impact of communication locality on the communication costs is at most a factor of two on Alewife. Finally, we use the model to identify tradeoffs between synchronous and asynchronous message passing styles.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {254--263},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277851.277933},
 doi = {http://doi.acm.org/10.1145/277851.277933},
 acmid = {277933},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Moritz:1998:LMN:277858.277933,
 author = {Moritz, Csaba Andras and Frank, Matthew I.},
 title = {LoGPC: modeling network contention in message-passing programs},
 abstract = {In many real applications, for example those with frequent and irregular communication patterns or those using large messages, network contention and contention for message processing resources can be a significant part of the total execution time. This paper presents a new cost model, called LoGPC, that extends the LogP [9] and LogGP [4] models to account for the impact of network contention and network interface DMA behavior on the performance of message-passing programs.We validate LoGPC by analyzing three applications implemented with Active Messages [11, 18] on the MIT Alewife multiprocessor. Our analysis shows that network contention accounts for up to 50\% of the total execution time. In addition, we show that the impact of communication locality on the communication costs is at most a factor of two on Alewife. Finally, we use the model to identify tradeoffs between synchronous and asynchronous message passing styles.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {254--263},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/277858.277933},
 doi = {http://doi.acm.org/10.1145/277858.277933},
 acmid = {277933},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Barve:1998:MOI:277858.277936,
 author = {Barve, Rakesh and Shriver, Elizabeth and Gibbons, Phillip B. and Hillyer, Bruce K. and Matias, Yossi and Vitter, Jeffrey Scott},
 title = {Modeling and optimizing I/O throughput of multiple disks on a bus (summary)},
 abstract = {For a wide variety of computational tasks, disk I/O continues to be a serious obstacle to high performance. The focus of the present paper is on systems that use multiple disks per SCSI bus. We measured the performance of concurrent random I/Os, and observed bus-related phenomena that impair performance. We describe these phenomena, and present a new I/O performance model that accurately predicts the average bandwidth achieved by a heavy workload of random reads from disks on a SCSI bus. This model, although relatively simple, predicts performance on several platforms to within 12\% for I/O sizes in the range 16-128 KB. We describe a technique to improve the I/O bandwidth by 10-20\% for random-access workloads that have large I/Os and high concurrency. This technique increases the percentage of disk head positioning time that is overlapped with data transfers, and increases the percentage of transfers that occur at bus bandwidth, rather than at disk-head bandwidth.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277936},
 doi = {http://doi.acm.org/10.1145/277858.277936},
 acmid = {277936},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Barve:1998:MOI:277851.277936,
 author = {Barve, Rakesh and Shriver, Elizabeth and Gibbons, Phillip B. and Hillyer, Bruce K. and Matias, Yossi and Vitter, Jeffrey Scott},
 title = {Modeling and optimizing I/O throughput of multiple disks on a bus (summary)},
 abstract = {For a wide variety of computational tasks, disk I/O continues to be a serious obstacle to high performance. The focus of the present paper is on systems that use multiple disks per SCSI bus. We measured the performance of concurrent random I/Os, and observed bus-related phenomena that impair performance. We describe these phenomena, and present a new I/O performance model that accurately predicts the average bandwidth achieved by a heavy workload of random reads from disks on a SCSI bus. This model, although relatively simple, predicts performance on several platforms to within 12\% for I/O sizes in the range 16-128 KB. We describe a technique to improve the I/O bandwidth by 10-20\% for random-access workloads that have large I/Os and high concurrency. This technique increases the percentage of disk head positioning time that is overlapped with data transfers, and increases the percentage of transfers that occur at bus bandwidth, rather than at disk-head bandwidth.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277936},
 doi = {http://doi.acm.org/10.1145/277851.277936},
 acmid = {277936},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blumofe:1998:PWS:277851.277939,
 author = {Blumofe, Robert D. and Papadopoulos, Dionisios},
 title = {The performance of work stealing in multiprogrammed environments (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277939},
 doi = {http://doi.acm.org/10.1145/277851.277939},
 acmid = {277939},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Blumofe:1998:PWS:277858.277939,
 author = {Blumofe, Robert D. and Papadopoulos, Dionisios},
 title = {The performance of work stealing in multiprogrammed environments (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277939},
 doi = {http://doi.acm.org/10.1145/277858.277939},
 acmid = {277939},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Crovella:1998:TAD:277851.277942,
 author = {Crovella, Mark E. and Harchol-Balter, Mor and Murta, Cristina D.},
 title = {Task assignment in a distributed system (extended abstract): improving performance by unbalancing load},
 abstract = {We consider the problem of task assignment in a distributed system (such as a distributed Web server) in which task sizes are drawn from a heavy-tailed distribution. Many task assignment algorithms are based on the heuristic that balancing the load at the server hosts will result in optimal performance. We show this conventional wisdom is less true when the task size distribution is heavy-tailed (as is the case for Web file sizes). We introduce a new task assignment policy, called Size Interval Task Assignment with Variable Load (SITA-V). SITA-V purposely operates the server hosts at different loads, and directs smaller tasks to the lighter-loaded hosts. The result is that SITA-V provably decreases the mean task slowdown by significant factors (up to 1000 or more) where the more heavy-tailed the workload, the greater the improvement factor. We evaluate the tradeoff between improvement in slowdown and increase in waiting time in a system using SITA-V, and show conditions under which SITA-V represents a particularly appealing policy.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277942},
 doi = {http://doi.acm.org/10.1145/277851.277942},
 acmid = {277942},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Crovella:1998:TAD:277858.277942,
 author = {Crovella, Mark E. and Harchol-Balter, Mor and Murta, Cristina D.},
 title = {Task assignment in a distributed system (extended abstract): improving performance by unbalancing load},
 abstract = {We consider the problem of task assignment in a distributed system (such as a distributed Web server) in which task sizes are drawn from a heavy-tailed distribution. Many task assignment algorithms are based on the heuristic that balancing the load at the server hosts will result in optimal performance. We show this conventional wisdom is less true when the task size distribution is heavy-tailed (as is the case for Web file sizes). We introduce a new task assignment policy, called Size Interval Task Assignment with Variable Load (SITA-V). SITA-V purposely operates the server hosts at different loads, and directs smaller tasks to the lighter-loaded hosts. The result is that SITA-V provably decreases the mean task slowdown by significant factors (up to 1000 or more) where the more heavy-tailed the workload, the greater the improvement factor. We evaluate the tradeoff between improvement in slowdown and increase in waiting time in a system using SITA-V, and show conditions under which SITA-V represents a particularly appealing policy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277942},
 doi = {http://doi.acm.org/10.1145/277858.277942},
 acmid = {277942},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Manley:1998:SSB:277858.277945,
 author = {Manley, Stephen and Seltzer, Margo and Courage, Michael},
 title = {A self-scaling and self-configuring benchmark for Web servers  (extended abstract)},
 abstract = {World Wide Web clients and servers have become some of the most important applications in our computing base, and we need realistic and meaningful ways of measuring their performance. Current server benchmarks do not capture the wide variation that we see in servers and are not accurate in their characterization of web traffic. In this paper, we present a self-configuring, scalable benchmark that generates a server benchmark load based on actual server loads. In contrast to other web benchmarks, our benchmark focuses on request latency instead of focusing exclusively on throughput sensitive metrics. We present our new benchmark hBench:Web, and demonstrate how it accurately models the load of an actual server. The benchmark can also be used to assess how continued growth or changes in the workload will affect future performance. Using existing log histories, we now that these predictions are sufficiently realistic to provide insight into tomorrow's Web performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277945},
 doi = {http://doi.acm.org/10.1145/277858.277945},
 acmid = {277945},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CGI, World Wide Web, benchmark, scaling, self-configuring},
} 

@inproceedings{Manley:1998:SSB:277851.277945,
 author = {Manley, Stephen and Seltzer, Margo and Courage, Michael},
 title = {A self-scaling and self-configuring benchmark for Web servers  (extended abstract)},
 abstract = {World Wide Web clients and servers have become some of the most important applications in our computing base, and we need realistic and meaningful ways of measuring their performance. Current server benchmarks do not capture the wide variation that we see in servers and are not accurate in their characterization of web traffic. In this paper, we present a self-configuring, scalable benchmark that generates a server benchmark load based on actual server loads. In contrast to other web benchmarks, our benchmark focuses on request latency instead of focusing exclusively on throughput sensitive metrics. We present our new benchmark hBench:Web, and demonstrate how it accurately models the load of an actual server. The benchmark can also be used to assess how continued growth or changes in the workload will affect future performance. Using existing log histories, we now that these predictions are sufficiently realistic to provide insight into tomorrow's Web performance.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277945},
 doi = {http://doi.acm.org/10.1145/277851.277945},
 acmid = {277945},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CGI, World Wide Web, benchmark, scaling, self-configuring},
} 

@inproceedings{Rousskov:1998:PCP:277851.277946,
 author = {Rousskov, Alex and Soloviev, Valery},
 title = {On performance of caching proxies (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277946},
 doi = {http://doi.acm.org/10.1145/277851.277946},
 acmid = {277946},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rousskov:1998:PCP:277858.277946,
 author = {Rousskov, Alex and Soloviev, Valery},
 title = {On performance of caching proxies (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277946},
 doi = {http://doi.acm.org/10.1145/277858.277946},
 acmid = {277946},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Waldby:1998:TAR:277851.277947,
 author = {Waldby, J. and Madhow, U. and Lakshman, T. V.},
 title = {Total acknowledgements (extended abstract): a robust feedback mechanism for end-to-end congestion control},
 abstract = {End-to-end data transport protocols have two main functions: error recovery and congestion control. The information required by the sender to perform these functions is provided by acknowledgements (ACKs) from the receiver. The Internet transport protocol, TCP/IP, uses cumulative acknowledgements (CACKs), which provide a robust but minimal mechanism for error recovery which is inadequate for heterogeneous networks with random loss. Furthermore, TCP's congestion control mechanism is based on counting ACKs, and is therefore vulnerable to loss of ACKs on the reverse path, particularly when the latter may be slower than the forward path, as in asymmetric networks. The contributions of this paper are as follows:(a) We show that a simple enhancement of CACK provides sufficient information for end-to-end congestion control</i>. We term this ACK format total ACKs (TACKs).(b) We devise a novel ACK format that uses TACKs for congestion control, and negative ACKs (NACKs) for efficient error recovery. Typically, the main concern with NACKs is that of robustness to ACK loss, and we address this using an implementation that provides enough redundancy to provide such robustness.(c) We use the TACK+NACK acknowledgement format as the basis for a new transport protocol that provides efficient error recovery and dynamic congestion control. The protocol provides large performance gains over TCP in an environment with random loss, and is robust against loss of ACKs in the reverse path. In particular, the protocol gives high throughput upto a designed level of random loss, independent of the bandwidth-delay product. This is in contrast to TCP, whose throughput deteriorates drastically if the random loss probability is higher than the inverse square of the bandwidth-delay product.},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {274--275},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277947},
 doi = {http://doi.acm.org/10.1145/277851.277947},
 acmid = {277947},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Waldby:1998:TAR:277858.277947,
 author = {Waldby, J. and Madhow, U. and Lakshman, T. V.},
 title = {Total acknowledgements (extended abstract): a robust feedback mechanism for end-to-end congestion control},
 abstract = {End-to-end data transport protocols have two main functions: error recovery and congestion control. The information required by the sender to perform these functions is provided by acknowledgements (ACKs) from the receiver. The Internet transport protocol, TCP/IP, uses cumulative acknowledgements (CACKs), which provide a robust but minimal mechanism for error recovery which is inadequate for heterogeneous networks with random loss. Furthermore, TCP's congestion control mechanism is based on counting ACKs, and is therefore vulnerable to loss of ACKs on the reverse path, particularly when the latter may be slower than the forward path, as in asymmetric networks. The contributions of this paper are as follows:(a) We show that a simple enhancement of CACK provides sufficient information for end-to-end congestion control</i>. We term this ACK format total ACKs (TACKs).(b) We devise a novel ACK format that uses TACKs for congestion control, and negative ACKs (NACKs) for efficient error recovery. Typically, the main concern with NACKs is that of robustness to ACK loss, and we address this using an implementation that provides enough redundancy to provide such robustness.(c) We use the TACK+NACK acknowledgement format as the basis for a new transport protocol that provides efficient error recovery and dynamic congestion control. The protocol provides large performance gains over TCP in an environment with random loss, and is robust against loss of ACKs in the reverse path. In particular, the protocol gives high throughput upto a designed level of random loss, independent of the bandwidth-delay product. This is in contrast to TCP, whose throughput deteriorates drastically if the random loss probability is higher than the inverse square of the bandwidth-delay product.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {274--275},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277947},
 doi = {http://doi.acm.org/10.1145/277858.277947},
 acmid = {277947},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Willis:1998:PCR:277851.277948,
 author = {Willis, Thomas E. and Adams,III, George B.},
 title = {Portable, continuous recording of complete computer behavior with low overhead (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {276--277},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277948},
 doi = {http://doi.acm.org/10.1145/277851.277948},
 acmid = {277948},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Willis:1998:PCR:277858.277948,
 author = {Willis, Thomas E. and Adams,III, George B.},
 title = {Portable, continuous recording of complete computer behavior with low overhead (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {276--277},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277948},
 doi = {http://doi.acm.org/10.1145/277858.277948},
 acmid = {277948},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Acharya:1998:UIM:277851.277949,
 author = {Acharya, Anurag and Setia, Sanjeev},
 title = {Using idle memory for data-intensive computations (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {278--279},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277949},
 doi = {http://doi.acm.org/10.1145/277851.277949},
 acmid = {277949},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Acharya:1998:UIM:277858.277949,
 author = {Acharya, Anurag and Setia, Sanjeev},
 title = {Using idle memory for data-intensive computations (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {278--279},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277949},
 doi = {http://doi.acm.org/10.1145/277858.277949},
 acmid = {277949},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Aboutabl:1998:TDD:277851.277950,
 author = {Aboutabl, Mohamed and Agrawala, Ashok and Decotignie, Jean-Dominique},
 title = {Temporally determinate disk access (extended abstract): an experimental approach},
 abstract = {},
 booktitle = {Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '98/PERFORMANCE '98},
 year = {1998},
 isbn = {0-89791-982-3},
 location = {Madison, Wisconsin, United States},
 pages = {280--281},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277851.277950},
 doi = {http://doi.acm.org/10.1145/277851.277950},
 acmid = {277950},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aboutabl:1998:TDD:277858.277950,
 author = {Aboutabl, Mohamed and Agrawala, Ashok and Decotignie, Jean-Dominique},
 title = {Temporally determinate disk access (extended abstract): an experimental approach},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {26},
 issue = {1},
 month = {June},
 year = {1998},
 issn = {0163-5999},
 pages = {280--281},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/277858.277950},
 doi = {http://doi.acm.org/10.1145/277858.277950},
 acmid = {277950},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Braun:1997:APL:258623.258628,
 author = {Braun, Hans-Werner},
 title = {Architecture and performance of large internets, based on terrestrial and satellite infrastructure},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/258623.258628},
 doi = {http://doi.acm.org/10.1145/258623.258628},
 acmid = {258628},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Braun:1997:APL:258612.258628,
 author = {Braun, Hans-Werner},
 title = {Architecture and performance of large internets, based on terrestrial and satellite infrastructure},
 abstract = {},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/258612.258628},
 doi = {http://doi.acm.org/10.1145/258612.258628},
 acmid = {258628},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Balakrishnan:1997:ASW:258612.258631,
 author = {Balakrishnan, Hari and Stemm, Mark and Seshan, Srinivasan and Katz, Randy H.},
 title = {Analyzing stability in wide-area network performance},
 abstract = {The Internet is a very large scale, complex, dynamical system that is hard to model and analyze. In this paper, we develop and analyze statistical models for the observed end-to-end network performance based on extensive packet-level traces (consisting of approximately 1.5 billion packets) collected from the primary Web site for the Atlanta Summer Olympic Games in 1996. We find that observed mean throughputs for these transfers measured over 60 million complete connections vary widely as a function of end-host location and time of day, confirming that the Internet is characterized by a large degree of heterogeneity. Despite this heterogeneity, we find (using best-fit linear regression techniques) that we can express the throughput for Web transfers to most hosts as a random variable with a log-normal distribution. Then, using observed throughput as the control parameter, we attempt to quantify the spatial</i> (statistical similarity across neighboring hosts) and temporal</i> (persistence over time) stability of network performance. We find that Internet hosts that are close to each other often have almost identically distributed probability distributions of throughput. We also find that throughputs to individual hosts often do not change appreciably for several minutes. Overall, these results indicate that there is promise in protocol mechanisms that cache and share network characteristics both within a single host and amongst nearby hosts.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {2--12},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258631},
 doi = {http://doi.acm.org/10.1145/258612.258631},
 acmid = {258631},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Balakrishnan:1997:ASW:258623.258631,
 author = {Balakrishnan, Hari and Stemm, Mark and Seshan, Srinivasan and Katz, Randy H.},
 title = {Analyzing stability in wide-area network performance},
 abstract = {The Internet is a very large scale, complex, dynamical system that is hard to model and analyze. In this paper, we develop and analyze statistical models for the observed end-to-end network performance based on extensive packet-level traces (consisting of approximately 1.5 billion packets) collected from the primary Web site for the Atlanta Summer Olympic Games in 1996. We find that observed mean throughputs for these transfers measured over 60 million complete connections vary widely as a function of end-host location and time of day, confirming that the Internet is characterized by a large degree of heterogeneity. Despite this heterogeneity, we find (using best-fit linear regression techniques) that we can express the throughput for Web transfers to most hosts as a random variable with a log-normal distribution. Then, using observed throughput as the control parameter, we attempt to quantify the spatial</i> (statistical similarity across neighboring hosts) and temporal</i> (persistence over time) stability of network performance. We find that Internet hosts that are close to each other often have almost identically distributed probability distributions of throughput. We also find that throughputs to individual hosts often do not change appreciably for several minutes. Overall, these results indicate that there is promise in protocol mechanisms that cache and share network characteristics both within a single host and amongst nearby hosts.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {2--12},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258631},
 doi = {http://doi.acm.org/10.1145/258623.258631},
 acmid = {258631},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Maltzahn:1997:PIE:258623.258668,
 author = {Maltzahn, Carlos and Richardson, Kathy J. and Grunwald, Dirk},
 title = {Performance issues of enterprise level web proxies},
 abstract = {Enterprise level web proxies relay world-wide web traffic between private networks and the Internet. They improve security, save network bandwidth, and reduce network latency. While the performance of web proxies has been analyzed based on synthetic workloads, little is known about their performance on real workloads. In this paper we present a study of two web proxies (CERN and Squid) executing real workloads on Digital's Palo Alto Gateway. We demonstrate that the simple CERN proxy architecture outperforms all but the latest version of Squid and continues to outperform cacheless configurations. For the measured load levels the Squid proxy used at least as many CPU, memory, and disk resources as CERN, in some configurations significantly more resources. At higher load levels the resource utilization requirements will cross and Squid will be the one using fewer resources. Lastly we found that cache hit rates of around 30\% had very little effect on the requests service time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258668},
 doi = {http://doi.acm.org/10.1145/258623.258668},
 acmid = {258668},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Maltzahn:1997:PIE:258612.258668,
 author = {Maltzahn, Carlos and Richardson, Kathy J. and Grunwald, Dirk},
 title = {Performance issues of enterprise level web proxies},
 abstract = {Enterprise level web proxies relay world-wide web traffic between private networks and the Internet. They improve security, save network bandwidth, and reduce network latency. While the performance of web proxies has been analyzed based on synthetic workloads, little is known about their performance on real workloads. In this paper we present a study of two web proxies (CERN and Squid) executing real workloads on Digital's Palo Alto Gateway. We demonstrate that the simple CERN proxy architecture outperforms all but the latest version of Squid and continues to outperform cacheless configurations. For the measured load levels the Squid proxy used at least as many CPU, memory, and disk resources as CERN, in some configurations significantly more resources. At higher load levels the resource utilization requirements will cross and Squid will be the one using fewer resources. Lastly we found that cache hit rates of around 30\% had very little effect on the requests service time.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258668},
 doi = {http://doi.acm.org/10.1145/258612.258668},
 acmid = {258668},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heyman:1997:NMA:258623.258670,
 author = {Heyman, D. P. and Lakshman, T. V. and Neidhardt, Arnold L.},
 title = {A new method for analysing feedback-based protocols with applications to engineering Web traffic over the Internet},
 abstract = {Most of the studies of feedback-based flow and congestion control consider only persistent sources which always have data to send. However, with the rapid growth of Internet applications built on TCP/IP such as the World Wide Web and the standardization of traffic management schemes such as Available Bit Rate (ABR) in Asynchronous Transfer Mode (ATM) networks, it is essential to evaluate the performance of feedback-based protocols using traffic models which are specific to dominant applications. This paper presents a method for analysing feedback-based protocols with a Web-user-like input traffic where the source alternates between "transfer" periods followed by "think" periods. Our key results, which are presented for the TCP protocol, are:(1) The goodputs and the fraction of time that the system has some given number of transferring sources are insensitive</i> to the distributions of transfer (file or page) sizes and think times except through the ratio of their means. Thus, apart from network round-trip times, only the ratio of average transfer sizes and think times of users need be known to size the network for achieving a specific quality of service.(2) The Engset model can be adapted to accurately compute goodputs for TCP and TCP over ATM, with different buffer management schemes. Though only these adaptations are given in the paper, the method based on the Engset model can be applied to analyze other feedback systems, such as ATM ABR, by finding a protocol specific adaptation. Hence, the method we develop is useful not only for analysing TCP using a source model significantly different from the commonly used persistent sources, but also can be useful for analysing other feedback schemes.(3) Comparisons of simulated TCP traffic to measured Ethernet traffic shows qualitatively similar autocorrelation when think times follow a Pareto distribution with infinite variance. Also, the simulated and measured traffic have long range dependence. In this sense our traffic model, which purports to be Web-user-like, also agrees with measured traffic.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {24--38},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/258623.258670},
 doi = {http://doi.acm.org/10.1145/258623.258670},
 acmid = {258670},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heyman:1997:NMA:258612.258670,
 author = {Heyman, D. P. and Lakshman, T. V. and Neidhardt, Arnold L.},
 title = {A new method for analysing feedback-based protocols with applications to engineering Web traffic over the Internet},
 abstract = {Most of the studies of feedback-based flow and congestion control consider only persistent sources which always have data to send. However, with the rapid growth of Internet applications built on TCP/IP such as the World Wide Web and the standardization of traffic management schemes such as Available Bit Rate (ABR) in Asynchronous Transfer Mode (ATM) networks, it is essential to evaluate the performance of feedback-based protocols using traffic models which are specific to dominant applications. This paper presents a method for analysing feedback-based protocols with a Web-user-like input traffic where the source alternates between "transfer" periods followed by "think" periods. Our key results, which are presented for the TCP protocol, are:(1) The goodputs and the fraction of time that the system has some given number of transferring sources are insensitive</i> to the distributions of transfer (file or page) sizes and think times except through the ratio of their means. Thus, apart from network round-trip times, only the ratio of average transfer sizes and think times of users need be known to size the network for achieving a specific quality of service.(2) The Engset model can be adapted to accurately compute goodputs for TCP and TCP over ATM, with different buffer management schemes. Though only these adaptations are given in the paper, the method based on the Engset model can be applied to analyze other feedback systems, such as ATM ABR, by finding a protocol specific adaptation. Hence, the method we develop is useful not only for analysing TCP using a source model significantly different from the commonly used persistent sources, but also can be useful for analysing other feedback schemes.(3) Comparisons of simulated TCP traffic to measured Ethernet traffic shows qualitatively similar autocorrelation when think times follow a Pareto distribution with infinite variance. Also, the simulated and measured traffic have long range dependence. In this sense our traffic model, which purports to be Web-user-like, also agrees with measured traffic.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {24--38},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/258612.258670},
 doi = {http://doi.acm.org/10.1145/258612.258670},
 acmid = {258670},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ma:1997:QME:258623.258672,
 author = {Ma, Qingming and Ramakrishnan, K. K.},
 title = {Queue management for explicit rate based congestion control},
 abstract = {Rate based congestion control has been considered desirable, both to deal with the high bandwidth-delay products of today's high speed networks, and to match the needs of emerging multimedia applications. Explicit rate control achieves low loss because sources transmit smoothly at a rate adjusted through feedback to be within the capacity of the resources in the network. However, large feedback delays, presence of higher priority traffic, and varying transient situations make it difficult to ensure feasibility</i> (i.e., keep the aggregate arrival rate below the bottleneck resource's capacity) while also maintaining high resource utilization. These conditions along with the "fast start" desired by data applications often result in substantial queue buildups.We describe a scheme that manages the queue buildup at a switch even under the most aggressive patterns of sources, in the context of the Explicit Rate option for the Available Bit Rate (ABR) congestion control scheme. A switch observes the buildup of its queue, and uses it to reduce the portion of the link capacity allocated to sources bottlenecked at that link. We use the concept of a "virtual" queue, which tracks the amount of queue that has been "reduced", but has not yet taken effect at the switch. We take advantage of the natural timing of "resource management" (RM) cells transmitted by sources. The scheme is elegant in that it is simple, and we show that it reduces the queue buildup, in some cases, by more than two orders of magnitude and the queue size remains around a desired target. It maintains max-min fairness even when the queue is being drained. The scheme is scalable, and is as responsive as can be expected: within the constraints of the feedback delay. Finally, no changes are needed to the ATM Forum defined source/destination policies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {39--51},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258623.258672},
 doi = {http://doi.acm.org/10.1145/258623.258672},
 acmid = {258672},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ma:1997:QME:258612.258672,
 author = {Ma, Qingming and Ramakrishnan, K. K.},
 title = {Queue management for explicit rate based congestion control},
 abstract = {Rate based congestion control has been considered desirable, both to deal with the high bandwidth-delay products of today's high speed networks, and to match the needs of emerging multimedia applications. Explicit rate control achieves low loss because sources transmit smoothly at a rate adjusted through feedback to be within the capacity of the resources in the network. However, large feedback delays, presence of higher priority traffic, and varying transient situations make it difficult to ensure feasibility</i> (i.e., keep the aggregate arrival rate below the bottleneck resource's capacity) while also maintaining high resource utilization. These conditions along with the "fast start" desired by data applications often result in substantial queue buildups.We describe a scheme that manages the queue buildup at a switch even under the most aggressive patterns of sources, in the context of the Explicit Rate option for the Available Bit Rate (ABR) congestion control scheme. A switch observes the buildup of its queue, and uses it to reduce the portion of the link capacity allocated to sources bottlenecked at that link. We use the concept of a "virtual" queue, which tracks the amount of queue that has been "reduced", but has not yet taken effect at the switch. We take advantage of the natural timing of "resource management" (RM) cells transmitted by sources. The scheme is elegant in that it is simple, and we show that it reduces the queue buildup, in some cases, by more than two orders of magnitude and the queue size remains around a desired target. It maintains max-min fairness even when the queue is being drained. The scheme is scalable, and is as responsive as can be expected: within the constraints of the feedback delay. Finally, no changes are needed to the ATM Forum defined source/destination policies.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {39--51},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258612.258672},
 doi = {http://doi.acm.org/10.1145/258612.258672},
 acmid = {258672},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ott:1997:TOA:258612.258674,
 author = {Ott, Teunis J. and Aggarwal, Neil},
 title = {TCP over ATM: ABR or UBR?},
 abstract = {This paper reports on a simulation study of the relative performances of the ATM ABR and UBR service categories in transporting TCP/IP flows through an ATM Network. The objective is two-fold: (i) to understand the interaction between the window - based end-to-end flowcontrol TCP and the rate based flowcontrol ABR which is restricted to the ATM part of the network, and (ii) to decide whether the greater complexity of ABR (than UBR) pays off in better performance of ABR (than UBR).The most important conclusion is that there does not seem to be strong evidence that for TCP/IP workloads the greater complexity of ABR pays off in better performance.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {52--63},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258674},
 doi = {http://doi.acm.org/10.1145/258612.258674},
 acmid = {258674},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ott:1997:TOA:258623.258674,
 author = {Ott, Teunis J. and Aggarwal, Neil},
 title = {TCP over ATM: ABR or UBR?},
 abstract = {This paper reports on a simulation study of the relative performances of the ATM ABR and UBR service categories in transporting TCP/IP flows through an ATM Network. The objective is two-fold: (i) to understand the interaction between the window - based end-to-end flowcontrol TCP and the rate based flowcontrol ABR which is restricted to the ATM part of the network, and (ii) to decide whether the greater complexity of ABR (than UBR) pays off in better performance of ABR (than UBR).The most important conclusion is that there does not seem to be strong evidence that for TCP/IP workloads the greater complexity of ABR pays off in better performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {52--63},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258674},
 doi = {http://doi.acm.org/10.1145/258623.258674},
 acmid = {258674},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kasera:1997:SRM:258623.258676,
 author = {Kasera, Sneha K. and Kurose, Jim and Towsley, Don},
 title = {Scalable reliable multicast using multiple multicast groups},
 abstract = {We examine an approach for providing reliable, scalable multicast communication, using multiple multicast groups for reducing receiver processing costs in a multicast session. In this approach a single multicast group is used for the original transmission of packets. Retransmissions of packets are done to separate multicast groups, which receivers dynamically join or leave. We first show that by using an infinite number of multicast groups, processing overhead at the receivers are substantially reduced. Next, we show that, for a specific negative acknowledgment (NAK)-based protocol, most of this reduction can be obtained by using only a small number of multicast groups for a wide range of system parameters. Finally, we present a local filtering scheme for minimizing join/leave signaling when multiple multicast groups are used.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {64--74},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258676},
 doi = {http://doi.acm.org/10.1145/258623.258676},
 acmid = {258676},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kasera:1997:SRM:258612.258676,
 author = {Kasera, Sneha K. and Kurose, Jim and Towsley, Don},
 title = {Scalable reliable multicast using multiple multicast groups},
 abstract = {We examine an approach for providing reliable, scalable multicast communication, using multiple multicast groups for reducing receiver processing costs in a multicast session. In this approach a single multicast group is used for the original transmission of packets. Retransmissions of packets are done to separate multicast groups, which receivers dynamically join or leave. We first show that by using an infinite number of multicast groups, processing overhead at the receivers are substantially reduced. Next, we show that, for a specific negative acknowledgment (NAK)-based protocol, most of this reduction can be obtained by using only a small number of multicast groups for a wide range of system parameters. Finally, we present a local filtering scheme for minimizing join/leave signaling when multiple multicast groups are used.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {64--74},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258676},
 doi = {http://doi.acm.org/10.1145/258612.258676},
 acmid = {258676},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rajamony:1997:PDS:258612.258678,
 author = {Rajamony, Ramakrishnan and Cox, Alan L.},
 title = {Performance debugging shared memory parallel programs using run-time dependence analysis},
 abstract = {We describe a new approach to performance debugging that focuses on automatically identifying computation transformations to reduce synchronization and communication. By grouping writes together into equivalence classes</i>, we are able to tractably collect information from long-running programs. Our performance debugger analyzes this information and suggests computation transformations in terms of the source code. We present the transformations suggested by the debugger on a suite of four applications. For Barnes-Hut and Shallow, implementing the debugger suggestions improved the performance by a factor of 1.32 and 34 times respectively on an 8-processor IBM SP2. For Ocean, our debugger identified excess synchronization that did not have a significant impact on performance. ILINK, a genetic linkage analysis program widely used by geneticists, is already well optimized. We use it only to demonstrate the feasibility of our approach to long-running applications.We also give details on how our approach can be implemented. We use novel techniques to convert control dependences to data dependences, and to compute the source operands of stores. We report on the impact of our instrumentation on the same application suite we use for performance debugging. The instrumentation slows down the execution by a factor of between 4 and 169 times. The log files produced during execution were all less than 2.5 Mbytes in size.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {75--87},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258612.258678},
 doi = {http://doi.acm.org/10.1145/258612.258678},
 acmid = {258678},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rajamony:1997:PDS:258623.258678,
 author = {Rajamony, Ramakrishnan and Cox, Alan L.},
 title = {Performance debugging shared memory parallel programs using run-time dependence analysis},
 abstract = {We describe a new approach to performance debugging that focuses on automatically identifying computation transformations to reduce synchronization and communication. By grouping writes together into equivalence classes</i>, we are able to tractably collect information from long-running programs. Our performance debugger analyzes this information and suggests computation transformations in terms of the source code. We present the transformations suggested by the debugger on a suite of four applications. For Barnes-Hut and Shallow, implementing the debugger suggestions improved the performance by a factor of 1.32 and 34 times respectively on an 8-processor IBM SP2. For Ocean, our debugger identified excess synchronization that did not have a significant impact on performance. ILINK, a genetic linkage analysis program widely used by geneticists, is already well optimized. We use it only to demonstrate the feasibility of our approach to long-running applications.We also give details on how our approach can be implemented. We use novel techniques to convert control dependences to data dependences, and to compute the source operands of stores. We report on the impact of our instrumentation on the same application suite we use for performance debugging. The instrumentation slows down the execution by a factor of between 4 and 169 times. The log files produced during execution were all less than 2.5 Mbytes in size.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {75--87},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258623.258678},
 doi = {http://doi.acm.org/10.1145/258623.258678},
 acmid = {258678},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Herbordt:1997:PSC:258612.258679,
 author = {Herbordt, Martin C. and Kidwai, Owais and Weems, Charles C.},
 title = {Preprototyping SIMD coprocessors using virtual machine emulation and trace compilation},
 abstract = {The use of massively parallel SIMD array architectures is proliferating in the area of domain specific coprocessors. Even so, they have undergone few systematic empirical studies. The underlying problems include the size of the architecture space, the lack of portability of the test programs, and the inherent complexity of simulating up to hundreds of thousands of processing elements. We address the computational cost problem with a novel approach to trace-based simulation. Code is run on an abstract virtual machine to generate a coarse-grained trace, which is then refined through a series of transformations (a process we call trace compilation</i>) wherein greater resolution is obtained with respect to the details of the target machine. We have found this technique to be one to two orders of magnitude faster than instruction-level simulation while still retaining much of the accuracy of the model. Furthermore, abstract machine traces must be regenerated for only a small fraction of the possible parameter combinations. Using virtual machine emulation and trace compilation also addresses program portability by allowing the user to code in a single data parallel language with a single compiler, regardless of the target architecture. This technique has already been used to generate significant results with respect to SIMD array architectures, a sample of which are presented here.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {88--99},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258679},
 doi = {http://doi.acm.org/10.1145/258612.258679},
 acmid = {258679},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Herbordt:1997:PSC:258623.258679,
 author = {Herbordt, Martin C. and Kidwai, Owais and Weems, Charles C.},
 title = {Preprototyping SIMD coprocessors using virtual machine emulation and trace compilation},
 abstract = {The use of massively parallel SIMD array architectures is proliferating in the area of domain specific coprocessors. Even so, they have undergone few systematic empirical studies. The underlying problems include the size of the architecture space, the lack of portability of the test programs, and the inherent complexity of simulating up to hundreds of thousands of processing elements. We address the computational cost problem with a novel approach to trace-based simulation. Code is run on an abstract virtual machine to generate a coarse-grained trace, which is then refined through a series of transformations (a process we call trace compilation</i>) wherein greater resolution is obtained with respect to the details of the target machine. We have found this technique to be one to two orders of magnitude faster than instruction-level simulation while still retaining much of the accuracy of the model. Furthermore, abstract machine traces must be regenerated for only a small fraction of the possible parameter combinations. Using virtual machine emulation and trace compilation also addresses program portability by allowing the user to code in a single data parallel language with a single compiler, regardless of the target architecture. This technique has already been used to generate significant results with respect to SIMD array architectures, a sample of which are presented here.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {88--99},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258679},
 doi = {http://doi.acm.org/10.1145/258623.258679},
 acmid = {258679},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tomkins:1997:IMP:258623.258680,
 author = {Tomkins, Andrew and Patterson, R. Hugo and Gibson, Garth},
 title = {Informed multi-process prefetching and caching},
 abstract = {Informed prefetching and caching based on application disclosure of future I/O accesses (hints) can dramatically reduce the execution time of I/O-intensive applications. A recent study showed that, in the context of a single hinting application, prefetching and caching algorithms should adapt to the dynamic load on the disks to obtain the best performance. In this paper, we show how to incorporate adaptivity to disk load into the TIP2 system, which uses cost-benefit analysis</i> to allocate global resources among multiple processes. We compare the resulting system, which we call TIPTOE (TIP with Temporal Overload Estimators) to Cao et al's LRU-SP allocation scheme, also modified to include adaptive prefetching. Using disk-accurate trace-driven simulation we show that, averaged over eleven experiments involving pairs of hinting applications, and with data striped over one to ten disks, TIPTOE delivers 7\% lower execution time than LRU-SP. Where the computation and I/O demands of each experiment are closely matched, in a two-disk array, TIPTOE delivers 18\% lower execution time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {100--114},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/258623.258680},
 doi = {http://doi.acm.org/10.1145/258623.258680},
 acmid = {258680},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tomkins:1997:IMP:258612.258680,
 author = {Tomkins, Andrew and Patterson, R. Hugo and Gibson, Garth},
 title = {Informed multi-process prefetching and caching},
 abstract = {Informed prefetching and caching based on application disclosure of future I/O accesses (hints) can dramatically reduce the execution time of I/O-intensive applications. A recent study showed that, in the context of a single hinting application, prefetching and caching algorithms should adapt to the dynamic load on the disks to obtain the best performance. In this paper, we show how to incorporate adaptivity to disk load into the TIP2 system, which uses cost-benefit analysis</i> to allocate global resources among multiple processes. We compare the resulting system, which we call TIPTOE (TIP with Temporal Overload Estimators) to Cao et al's LRU-SP allocation scheme, also modified to include adaptive prefetching. Using disk-accurate trace-driven simulation we show that, averaged over eleven experiments involving pairs of hinting applications, and with data striped over one to ten disks, TIPTOE delivers 7\% lower execution time than LRU-SP. Where the computation and I/O demands of each experiment are closely matched, in a two-disk array, TIPTOE delivers 18\% lower execution time.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {100--114},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/258612.258680},
 doi = {http://doi.acm.org/10.1145/258612.258680},
 acmid = {258680},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Glass:1997:APR:258612.258681,
 author = {Glass, Gideon and Cao, Pei},
 title = {Adaptive page replacement based on memory reference behavior},
 abstract = {As disk performance continues to lag behind that of memory systems and processors, virtual memory management becomes increasingly important for overall system performance. In this paper we study the page reference behavior of a collection of memory-intensive applications, and propose a new virtual memory page replacement algorithm, SEQ. SEQ detects long sequences of page faults and applies most-recently-used replacement to those sequences. Simulations show that for a large class of applications, SEQ performs close to the optimal replacement algorithm, and significantly better than Least-Recently-Used (LRU). In addition, SEQ performs similarly to LRU for applications that do not exhibit sequential faulting.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {115--126},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258681},
 doi = {http://doi.acm.org/10.1145/258612.258681},
 acmid = {258681},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Glass:1997:APR:258623.258681,
 author = {Glass, Gideon and Cao, Pei},
 title = {Adaptive page replacement based on memory reference behavior},
 abstract = {As disk performance continues to lag behind that of memory systems and processors, virtual memory management becomes increasingly important for overall system performance. In this paper we study the page reference behavior of a collection of memory-intensive applications, and propose a new virtual memory page replacement algorithm, SEQ. SEQ detects long sequences of page faults and applies most-recently-used replacement to those sequences. Simulations show that for a large class of applications, SEQ performs close to the optimal replacement algorithm, and significantly better than Least-Recently-Used (LRU). In addition, SEQ performs similarly to LRU for applications that do not exhibit sequential faulting.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {115--126},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258681},
 doi = {http://doi.acm.org/10.1145/258623.258681},
 acmid = {258681},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Voelker:1997:MSL:258623.258682,
 author = {Voelker, Geoffrey M. and Jamrozik, Herv\'{e} A. and Vernon, Mary K. and Levy, Henry M. and Lazowska, Edward D.},
 title = {Managing server load in global memory systems},
 abstract = {New high-speed switched networks have reduced the latency of network page transfers significantly below that of local disk. This trend has led to the development of systems that use network-wide memory, or global</i> memory, as a cache for virtual memory pages or file blocks. A crucial issue in the implementation of these global memory systems is the selection of the target nodes to receive replaced pages. Current systems use various forms of an approximate global LRU algorithm for making these selections. However, using age information alone can lead to suboptimal performance in two ways. First, workload characteristics can lead to uneven distributions of old pages across servers, causing increased contention delays. Second, the global memory traffic imposed on a node can degrade the performance of local jobs on that node.This paper studies the potential benefit and the potential harm of using load information, in addition to age information, in global memory replacement policies. Using an analytic queueing network model, we show the extent to which server load can degrade remote memory latency and how load balancing solves this problem. Load balancing requests can cause the system to deviate from the global LRU replacement policy, however. Using trace-driven simulation, we study the impact on application performance of deviating from the LRU replacement policy. We find that deviating from strict LRU, even significantly for some applications, does not affect application performance. Based upon these results, we conclude that global memory systems can gain substantial benefit from load balancing requests with little harm from suboptimal replacement decisions. Finally, we illustrate the use of the intuition gained from the model and simulation experiments by proposing a new family of algorithms that incorporate load considerations as well as age information in global memory replacement decisions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {127--138},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258682},
 doi = {http://doi.acm.org/10.1145/258623.258682},
 acmid = {258682},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Voelker:1997:MSL:258612.258682,
 author = {Voelker, Geoffrey M. and Jamrozik, Herv\'{e} A. and Vernon, Mary K. and Levy, Henry M. and Lazowska, Edward D.},
 title = {Managing server load in global memory systems},
 abstract = {New high-speed switched networks have reduced the latency of network page transfers significantly below that of local disk. This trend has led to the development of systems that use network-wide memory, or global</i> memory, as a cache for virtual memory pages or file blocks. A crucial issue in the implementation of these global memory systems is the selection of the target nodes to receive replaced pages. Current systems use various forms of an approximate global LRU algorithm for making these selections. However, using age information alone can lead to suboptimal performance in two ways. First, workload characteristics can lead to uneven distributions of old pages across servers, causing increased contention delays. Second, the global memory traffic imposed on a node can degrade the performance of local jobs on that node.This paper studies the potential benefit and the potential harm of using load information, in addition to age information, in global memory replacement policies. Using an analytic queueing network model, we show the extent to which server load can degrade remote memory latency and how load balancing solves this problem. Load balancing requests can cause the system to deviate from the global LRU replacement policy, however. Using trace-driven simulation, we study the impact on application performance of deviating from the LRU replacement policy. We find that deviating from strict LRU, even significantly for some applications, does not affect application performance. Based upon these results, we conclude that global memory systems can gain substantial benefit from load balancing requests with little harm from suboptimal replacement decisions. Finally, we illustrate the use of the intuition gained from the model and simulation experiments by proposing a new family of algorithms that incorporate load considerations as well as age information in global memory replacement decisions.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {127--138},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258682},
 doi = {http://doi.acm.org/10.1145/258612.258682},
 acmid = {258682},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Woodward:1997:SBM:258612.258683,
 author = {Woodward, Michael E.},
 title = {Size-limited batch movement in product-form closed discrete-time queueing networks},
 abstract = {Existing models for product-form closed discrete-time queueing networks with batch movement of customers implicitly assume that batch sizes are unrestricted. In many practical modelling situations however, it is necessary to impose restrictions on the batch sizes, and this paper examines the repercushions of such restrictions on the product-form properties of the networks. It is shown that when batch sizes are restricted independently then, in general, the resulting networks cannot have a product-form equilibrium distribution. Sufficient conditions to retain a product-form are derived in the cases when batch sizes are either correlated or depend on the state of the network. Examples of applying the results to obtain product-form networks with both correlated and state dependent batch movement are given.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {139--146},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/258612.258683},
 doi = {http://doi.acm.org/10.1145/258612.258683},
 acmid = {258683},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Woodward:1997:SBM:258623.258683,
 author = {Woodward, Michael E.},
 title = {Size-limited batch movement in product-form closed discrete-time queueing networks},
 abstract = {Existing models for product-form closed discrete-time queueing networks with batch movement of customers implicitly assume that batch sizes are unrestricted. In many practical modelling situations however, it is necessary to impose restrictions on the batch sizes, and this paper examines the repercushions of such restrictions on the product-form properties of the networks. It is shown that when batch sizes are restricted independently then, in general, the resulting networks cannot have a product-form equilibrium distribution. Sufficient conditions to retain a product-form are derived in the cases when batch sizes are either correlated or depend on the state of the network. Examples of applying the results to obtain product-form networks with both correlated and state dependent batch movement are given.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {139--146},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/258623.258683},
 doi = {http://doi.acm.org/10.1145/258623.258683},
 acmid = {258683},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Golubchik:1997:BPM:258612.258684,
 author = {Golubchik, Leana and Lui, John C. S.},
 title = {Bounding of performance measures for a threshold-based queueing system with hysteresis},
 abstract = {In this paper, we consider a K</i>-server threshold-based queueing system with hysteresis in which the number of servers, employed for servicing customers, is governed by a forward threshold</i> vector F</i>= (F</i><inf>1</inf>, F</i><inf>2</inf>, \&amp;hellip;, F</i><inf>K</i>-1</inf>) (where F</i><inf>1</inf> \&amp;lt; F</i><inf>2</inf> \&amp;lt; \&amp;hellip; \&amp;lt; F</i><inf>K</i>-1</inf>) and a reverse threshold</i> vector R</i>= (R</i><inf>1</inf>, R</i><inf>2</inf>, \&amp;hellip;, R</i><inf>K</i>-1</inf>) (where R</i><inf>1</inf> \&amp;lt; R</i><inf>2</inf> \&amp;lt; \&amp;hellip; \&amp;lt; R</i><inf>K</i>-1</inf>). There are many applications where a threshold-based queueing system can be of great use. The main motivation for using a threshold-based approach in such applications is that they incur significant server setup, usage, and removal costs. And, as in most practical situations, an important concern is not only the system performance but rather its cost/performance ratio. The motivation for use of hysteresis is to control the cost during momentary fluctuations in workload. An important and distinguishing characteristic of our work is that in our model we consider the time to add a server to be non-negligible.</i> This is a more accurate model, for many applications, than previously considered in other works. Our main goal in this work is to develop an efficient method for computing the steady state probabilities of a multi-server threshold queueing system with hysteresis, which will, in turn, allow computation of various performance measures.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {147--157},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258684},
 doi = {http://doi.acm.org/10.1145/258612.258684},
 acmid = {258684},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Golubchik:1997:BPM:258623.258684,
 author = {Golubchik, Leana and Lui, John C. S.},
 title = {Bounding of performance measures for a threshold-based queueing system with hysteresis},
 abstract = {In this paper, we consider a K</i>-server threshold-based queueing system with hysteresis in which the number of servers, employed for servicing customers, is governed by a forward threshold</i> vector F</i>= (F</i><inf>1</inf>, F</i><inf>2</inf>, \&amp;hellip;, F</i><inf>K</i>-1</inf>) (where F</i><inf>1</inf> \&amp;lt; F</i><inf>2</inf> \&amp;lt; \&amp;hellip; \&amp;lt; F</i><inf>K</i>-1</inf>) and a reverse threshold</i> vector R</i>= (R</i><inf>1</inf>, R</i><inf>2</inf>, \&amp;hellip;, R</i><inf>K</i>-1</inf>) (where R</i><inf>1</inf> \&amp;lt; R</i><inf>2</inf> \&amp;lt; \&amp;hellip; \&amp;lt; R</i><inf>K</i>-1</inf>). There are many applications where a threshold-based queueing system can be of great use. The main motivation for using a threshold-based approach in such applications is that they incur significant server setup, usage, and removal costs. And, as in most practical situations, an important concern is not only the system performance but rather its cost/performance ratio. The motivation for use of hysteresis is to control the cost during momentary fluctuations in workload. An important and distinguishing characteristic of our work is that in our model we consider the time to add a server to be non-negligible.</i> This is a more accurate model, for many applications, than previously considered in other works. Our main goal in this work is to develop an efficient method for computing the steady state probabilities of a multi-server threshold queueing system with hysteresis, which will, in turn, allow computation of various performance measures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {147--157},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258684},
 doi = {http://doi.acm.org/10.1145/258623.258684},
 acmid = {258684},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lehoczky:1997:URQ:258612.258685,
 author = {Lehoczky, John P.},
 title = {Using real-time queueing theory to control lateness in real-time systems},
 abstract = {This paper presents real-time queueing theory,</i> a new theory which embeds the ability of real-time scheduling theory to determine whether task timing requirements are met into the context of queueing models. Specifically, this paper extends the analysis developed in Lehoczky [9] to the GI/M/1 case. The paper also applies these models to study queue control strategies which can control customer lateness. Arriving customers have deadlines drawn from a general deadline distribution. The state variable for the queueing system must include the number in the queue (with supplementary variables as needed to create a Markov model) and the lead-time</i> (deadline minus current time) of each customer; thus the state space is infinite dimensional. One can represent the state of the system as a measure on the real line and can represent that measure by its Fourier transform. Thus, a real-time queueing system can be characterized as a Markov process evolving on the space of Fourier transforms, and this paper presents a characterization of the instantaneous simultaneous lead-time profile of all the customers in the queue. This profile is complicated; however, in the heavy traffic case, a simple description of the lead-time profile emerges, namely that the lead-time profile behaves like a Brownian motion evolving on a particular manifold of Fourier transforms; the manifold depending upon the queue discipline and the customer deadline distributions. This approximation is very accurate when compared with simulations. Real-time queueing theory focuses on how well a particular queue discipline meets customer timing requirements, and focuses on the dynamic rather than the equilibrium behavior of the system. As such, it offers the potential to study control strategies to ensure that customers meet their deadlines. This paper illustrates the analysis and performance evaluation for certain queue control strategies. Generalizations to more complicated models and to queueing networks are discussed.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258685},
 doi = {http://doi.acm.org/10.1145/258612.258685},
 acmid = {258685},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lehoczky:1997:URQ:258623.258685,
 author = {Lehoczky, John P.},
 title = {Using real-time queueing theory to control lateness in real-time systems},
 abstract = {This paper presents real-time queueing theory,</i> a new theory which embeds the ability of real-time scheduling theory to determine whether task timing requirements are met into the context of queueing models. Specifically, this paper extends the analysis developed in Lehoczky [9] to the GI/M/1 case. The paper also applies these models to study queue control strategies which can control customer lateness. Arriving customers have deadlines drawn from a general deadline distribution. The state variable for the queueing system must include the number in the queue (with supplementary variables as needed to create a Markov model) and the lead-time</i> (deadline minus current time) of each customer; thus the state space is infinite dimensional. One can represent the state of the system as a measure on the real line and can represent that measure by its Fourier transform. Thus, a real-time queueing system can be characterized as a Markov process evolving on the space of Fourier transforms, and this paper presents a characterization of the instantaneous simultaneous lead-time profile of all the customers in the queue. This profile is complicated; however, in the heavy traffic case, a simple description of the lead-time profile emerges, namely that the lead-time profile behaves like a Brownian motion evolving on a particular manifold of Fourier transforms; the manifold depending upon the queue discipline and the customer deadline distributions. This approximation is very accurate when compared with simulations. Real-time queueing theory focuses on how well a particular queue discipline meets customer timing requirements, and focuses on the dynamic rather than the equilibrium behavior of the system. As such, it offers the potential to study control strategies to ensure that customers meet their deadlines. This paper illustrates the analysis and performance evaluation for certain queue control strategies. Generalizations to more complicated models and to queueing networks are discussed.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258685},
 doi = {http://doi.acm.org/10.1145/258623.258685},
 acmid = {258685},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nahum:1997:CBN:258612.258686,
 author = {Nahum, Erich and Yates, David and Kurose, Jim and Towsley, Don},
 title = {Cache behavior of network protocols},
 abstract = {In this paper we present a performance study of memory reference behavior in network protocol processing, using an Internet-based protocol stack implemented in the x</i>-kernel running in user space on a MIPS R4400-based Silicon Graphics machine. We use the protocols to drive a validated execution-driven architectural simulator of our machine. We characterize the behavior of network protocol processing, deriving statistics such as cache miss rates and percentage of time spent waiting for memory. We also determine how sensitive protocol processing is to the architectural environment, varying factors such as cache size and associativity, and predict performance on future machines.We show that network protocol cache behavior varies widely, with miss rates ranging from 0 to 28 percent, depending on the scenario. We find instruction cache behavior has the greatest effect on protocol latency under most cases, and that cold cache behavior is very different from warm cache behavior. We demonstrate the upper bounds on performance that can be expected by improving memory behavior, and the impact of features such as associativity and larger cache sizes. In particular, we find that TCP is more sensitive to cache behavior than UDP, gaining larger benefits from improved associativity and bigger caches. We predict that network protocols will scale well with CPU speeds in the future.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258686},
 doi = {http://doi.acm.org/10.1145/258612.258686},
 acmid = {258686},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nahum:1997:CBN:258623.258686,
 author = {Nahum, Erich and Yates, David and Kurose, Jim and Towsley, Don},
 title = {Cache behavior of network protocols},
 abstract = {In this paper we present a performance study of memory reference behavior in network protocol processing, using an Internet-based protocol stack implemented in the x</i>-kernel running in user space on a MIPS R4400-based Silicon Graphics machine. We use the protocols to drive a validated execution-driven architectural simulator of our machine. We characterize the behavior of network protocol processing, deriving statistics such as cache miss rates and percentage of time spent waiting for memory. We also determine how sensitive protocol processing is to the architectural environment, varying factors such as cache size and associativity, and predict performance on future machines.We show that network protocol cache behavior varies widely, with miss rates ranging from 0 to 28 percent, depending on the scenario. We find instruction cache behavior has the greatest effect on protocol latency under most cases, and that cold cache behavior is very different from warm cache behavior. We demonstrate the upper bounds on performance that can be expected by improving memory behavior, and the impact of features such as associativity and larger cache sizes. In particular, we find that TCP is more sensitive to cache behavior than UDP, gaining larger benefits from improved associativity and bigger caches. We predict that network protocols will scale well with CPU speeds in the future.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258686},
 doi = {http://doi.acm.org/10.1145/258623.258686},
 acmid = {258686},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Knightly:1997:SMR:258623.258687,
 author = {Knightly, Edward W.},
 title = {Second moment resource allocation in multi-service networks},
 abstract = {A crucial problem for the efficient design and management of integrated services networks is how to best allocate network resources for heterogeneous and bursty traffic streams in multiplexers that support prioritized service disciplines. In this paper, we introduce a new approach for determining per-connection performance parameters such as delay-bound violation probability and loss probability in multi-service networks. The approach utilizes a traffic characterization consisting of the variances of a stream's rate distribution over multiple interval lengths, which captures its burstiness properties and autocorrelation structure. From this traffic characterization, we provide a simple and efficient resource allocation algorithm by deriving stochastic delay-bounds for static priority schedulers and employing a Gaussian approximation over intervals. To evaluate the scheme, we perform trace-driven simulation experiments with long traces of MPEG-compressed video and show that our approach is accurate enough to capture most of the inherent statistical multiplexing gain, achieving average network utilizations of up to 90\% for these traces and substantially outperforming previous "effective bandwidth" techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {181--191},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258687},
 doi = {http://doi.acm.org/10.1145/258623.258687},
 acmid = {258687},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Knightly:1997:SMR:258612.258687,
 author = {Knightly, Edward W.},
 title = {Second moment resource allocation in multi-service networks},
 abstract = {A crucial problem for the efficient design and management of integrated services networks is how to best allocate network resources for heterogeneous and bursty traffic streams in multiplexers that support prioritized service disciplines. In this paper, we introduce a new approach for determining per-connection performance parameters such as delay-bound violation probability and loss probability in multi-service networks. The approach utilizes a traffic characterization consisting of the variances of a stream's rate distribution over multiple interval lengths, which captures its burstiness properties and autocorrelation structure. From this traffic characterization, we provide a simple and efficient resource allocation algorithm by deriving stochastic delay-bounds for static priority schedulers and employing a Gaussian approximation over intervals. To evaluate the scheme, we perform trace-driven simulation experiments with long traces of MPEG-compressed video and show that our approach is accurate enough to capture most of the inherent statistical multiplexing gain, achieving average network utilizations of up to 90\% for these traces and substantially outperforming previous "effective bandwidth" techniques.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {181--191},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258687},
 doi = {http://doi.acm.org/10.1145/258612.258687},
 acmid = {258687},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krunz:1997:CVM:258612.258688,
 author = {Krunz, Marwan and Tripathi, Satish K.},
 title = {On the characterization of VBR MPEG streams},
 abstract = {We present a comprehensive model for variable-bit-rate MPEG video streams. This model captures the bit-rate variations at multiple time scales. Long-term variations are captured by incorporating scene changes, which are most noticeable in the fluctuations of I</i> frames. The size of an I</i> frame is modeled by the sum of two random components: a scene-related component and an AR(2) component that accounts for the fluctuations within a scene. Two random processes of i.i.d.</i> rvs are used to model the sizes of P</i> and B</i> frames, respectively. The complete model is then obtained by intermixing the three sub-models according to a given GOP pattern. It is shown that the composite model exhibits long-range dependence (LRD) in the sense that its autocorrelation function is non-summable. The LRD behavior is caused by the repetitive GOP pattern which induces periodic cross-correlations between different types of frames. Using standard statistical methods, we successfully fit our model to several empirical video traces. We then study the queueing performance for video traffic at a statistical multiplexer. The results show that the model is sufficiently accurate in predicting the queueing performance for real video streams.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {192--202},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258688},
 doi = {http://doi.acm.org/10.1145/258612.258688},
 acmid = {258688},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krunz:1997:CVM:258623.258688,
 author = {Krunz, Marwan and Tripathi, Satish K.},
 title = {On the characterization of VBR MPEG streams},
 abstract = {We present a comprehensive model for variable-bit-rate MPEG video streams. This model captures the bit-rate variations at multiple time scales. Long-term variations are captured by incorporating scene changes, which are most noticeable in the fluctuations of I</i> frames. The size of an I</i> frame is modeled by the sum of two random components: a scene-related component and an AR(2) component that accounts for the fluctuations within a scene. Two random processes of i.i.d.</i> rvs are used to model the sizes of P</i> and B</i> frames, respectively. The complete model is then obtained by intermixing the three sub-models according to a given GOP pattern. It is shown that the composite model exhibits long-range dependence (LRD) in the sense that its autocorrelation function is non-summable. The LRD behavior is caused by the repetitive GOP pattern which induces periodic cross-correlations between different types of frames. Using standard statistical methods, we successfully fit our model to several empirical video traces. We then study the queueing performance for video traffic at a statistical multiplexer. The results show that the model is sufficiently accurate in predicting the queueing performance for real video streams.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {192--202},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258688},
 doi = {http://doi.acm.org/10.1145/258623.258688},
 acmid = {258688},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:1997:FSA:258623.258689,
 author = {Smith, Keith A. and Seltzer, Margo I.},
 title = {File system aging\&mdash;increasing the relevance of file system benchmarks},
 abstract = {Benchmarks are important because they provide a means for users and researchers to characterize how their workloads will perform on different systems and different system architectures. The field of file system design is no different from other areas of research in this regard, and a variety of file system benchmarks are in use, representing a wide range of the different user workloads that may be run on a file system. A realistic benchmark, however, is only one of the tools that is required in order to understand how a file system design will perform in the real world. The benchmark must also be executed on a realistic file system. While the simplest approach may be to measure the performance of an empty file system, this represents a state that is seldom encountered by real users. In order to study file systems in more representative conditions, we present a methodology for aging a test file system by replaying a workload similar to that experienced by a real file system over a period of many months, or even years. Our aging tools allow the same aging workload to be applied to multiple versions of the same file system, allowing scientific evaluation of the relative merits of competing file system designs.In addition to describing our aging tools, we demonstrate their use by applying them to evaluate two enhancements to the file layout policies of the UNIX fast file system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {203--213},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258689},
 doi = {http://doi.acm.org/10.1145/258623.258689},
 acmid = {258689},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smith:1997:FSA:258612.258689,
 author = {Smith, Keith A. and Seltzer, Margo I.},
 title = {File system aging\&mdash;increasing the relevance of file system benchmarks},
 abstract = {Benchmarks are important because they provide a means for users and researchers to characterize how their workloads will perform on different systems and different system architectures. The field of file system design is no different from other areas of research in this regard, and a variety of file system benchmarks are in use, representing a wide range of the different user workloads that may be run on a file system. A realistic benchmark, however, is only one of the tools that is required in order to understand how a file system design will perform in the real world. The benchmark must also be executed on a realistic file system. While the simplest approach may be to measure the performance of an empty file system, this represents a state that is seldom encountered by real users. In order to study file systems in more representative conditions, we present a methodology for aging a test file system by replaying a workload similar to that experienced by a real file system over a period of many months, or even years. Our aging tools allow the same aging workload to be applied to multiple versions of the same file system, allowing scientific evaluation of the relative merits of competing file system designs.In addition to describing our aging tools, we demonstrate their use by applying them to evaluate two enhancements to the file layout policies of the UNIX fast file system.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {203--213},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258689},
 doi = {http://doi.acm.org/10.1145/258612.258689},
 acmid = {258689},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brown:1997:OSB:258623.258690,
 author = {Brown, Aaron B. and Seltzer, Margo I.},
 title = {Operating system benchmarking in the wake of <italic>lmbench</italic>: a case study of the performance of NetBSD on the Intel x86 architecture},
 abstract = {The lmbench</i> suite of operating system microbenchmarks provides a set of portable programs for use in cross-platform comparisons. We have augmented the lmbench</i> suite to increase its flexibility and precision, and to improve its methodological and statistical operation. This enables the detailed study of interactions between the operating system and the hardware architecture. We describe modifications to lmbench,</i> and then use our new benchmark suite, hbench:OS,</i> to examine how the performance of operating system primitives under NetBSD has scaled with the processor evolution of the Intel x86 architecture. Our analysis shows that off-chip memory system design continues to influence operating system performance in a significant way and that key design decisions (such as suboptimal choices of DRAM and cache technology, and memory-bus and cache coherency protocols) can essentially nullify the performance benefits of the aggressive execution core and sophisticated on-chip memory system of a modern processor such as the Intel Pentium Pro.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {214--224},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258690},
 doi = {http://doi.acm.org/10.1145/258623.258690},
 acmid = {258690},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brown:1997:OSB:258612.258690,
 author = {Brown, Aaron B. and Seltzer, Margo I.},
 title = {Operating system benchmarking in the wake of <italic>lmbench</italic>: a case study of the performance of NetBSD on the Intel x86 architecture},
 abstract = {The lmbench</i> suite of operating system microbenchmarks provides a set of portable programs for use in cross-platform comparisons. We have augmented the lmbench</i> suite to increase its flexibility and precision, and to improve its methodological and statistical operation. This enables the detailed study of interactions between the operating system and the hardware architecture. We describe modifications to lmbench,</i> and then use our new benchmark suite, hbench:OS,</i> to examine how the performance of operating system primitives under NetBSD has scaled with the processor evolution of the Intel x86 architecture. Our analysis shows that off-chip memory system design continues to influence operating system performance in a significant way and that key design decisions (such as suboptimal choices of DRAM and cache technology, and memory-bus and cache coherency protocols) can essentially nullify the performance benefits of the aggressive execution core and sophisticated on-chip memory system of a modern processor such as the Intel Pentium Pro.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {214--224},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258690},
 doi = {http://doi.acm.org/10.1145/258612.258690},
 acmid = {258690},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Acharya:1997:UEI:258623.258691,
 author = {Acharya, Anurag and Edjlali, Guy and Saltz, Joel},
 title = {The utility of exploiting idle workstations for parallel computation},
 abstract = {In this paper, we examine the utility of exploiting idle workstations for parallel computation. We attempt to answer the following questions. First, given a workstation pool, for what fraction of time can we expect to find a cluster of k</i> workstations available? This provides an estimate of the opportunity for parallel computation. Second, how stable is a cluster of free machines and how does the stability vary with the size of the cluster? This indicates how frequently a parallel computation might have to stop for adapting to changes in processor availability. Third, what is the distribution of workstation idle-times? This information is useful for selecting workstations to place computation on. Fourth, how much benefit can a user expect? To state this in concrete terms, if I have a pool of size S,</i> how big a parallel machine should I expect to get for free by harvesting idle machines. Finally, how much benefit can be achieved on a real machine and how hard does a parallel programmer have to work to make this happen? To answer the workstation-availability questions, we have analyzed 14-day traces from three workstation pools. To determine the equivalent parallel machine, we have simulated the execution of a group of well-known parallel programs on these workstation pools. To gain an understanding of the practical problems, we have developed the system support required for adaptive parallel programs and have used it to build an adaptive parallel computational fluid dynamics application.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {225--234},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/258623.258691},
 doi = {http://doi.acm.org/10.1145/258623.258691},
 acmid = {258691},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Acharya:1997:UEI:258612.258691,
 author = {Acharya, Anurag and Edjlali, Guy and Saltz, Joel},
 title = {The utility of exploiting idle workstations for parallel computation},
 abstract = {In this paper, we examine the utility of exploiting idle workstations for parallel computation. We attempt to answer the following questions. First, given a workstation pool, for what fraction of time can we expect to find a cluster of k</i> workstations available? This provides an estimate of the opportunity for parallel computation. Second, how stable is a cluster of free machines and how does the stability vary with the size of the cluster? This indicates how frequently a parallel computation might have to stop for adapting to changes in processor availability. Third, what is the distribution of workstation idle-times? This information is useful for selecting workstations to place computation on. Fourth, how much benefit can a user expect? To state this in concrete terms, if I have a pool of size S,</i> how big a parallel machine should I expect to get for free by harvesting idle machines. Finally, how much benefit can be achieved on a real machine and how hard does a parallel programmer have to work to make this happen? To answer the workstation-availability questions, we have analyzed 14-day traces from three workstation pools. To determine the equivalent parallel machine, we have simulated the execution of a group of well-known parallel programs on these workstation pools. To gain an understanding of the practical problems, we have developed the system support required for adaptive parallel programs and have used it to build an adaptive parallel computational fluid dynamics application.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {225--234},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/258612.258691},
 doi = {http://doi.acm.org/10.1145/258612.258691},
 acmid = {258691},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Qin:1997:PEC:258612.258692,
 author = {Qin, Xiaohan and Baer, Jean-Loup},
 title = {A performance evaluation of cluster architectures},
 abstract = {This paper investigates the performance of shared-memory cluster-based architectures where each cluster is a shared-bus multiprocessor augmented with a protocol processor maintaining cache coherence across clusters. For a given number of processors, sixteen in this study, we evaluate the performance of various cluster configurations. We also consider the impact of adding a remote shared cache in each cluster. We use Mean Value Analysis to estimate the cache miss latencies of various types and the overall execution time. The service demands of shared resources are characterized in detail by examining the sub-requests issued in resolving cache misses. In addition to the architectural system parameters and the service demands on resources, the analytical model needs parameters pertinent to applications. The latter, in particular cache miss profiles, are obtained by trace-driven simulation of three benchmarks.Our results show that without remote caches the performance of cluster-based architectures is mixed. In some configurations, the negative effects of the longer latency of inter-cluster misses and of the contention on the protocol processor are too large to counter-balance the lower contention on the data buses. For two out of the three applications best results are obtained when the system has clusters of size 2 or 4. The cluster-based architectures with remote caches consistently outperform the single bus system for all 3 applications. We also exercise the model with parameters reflecting the current trend in technology making the processor relatively faster than the bus and memory. Under these new conditions, our results show a clear performance advantage for the cluster-based architectures, with or without remote caches, over single bus systems.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {237--247},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258612.258692},
 doi = {http://doi.acm.org/10.1145/258612.258692},
 acmid = {258692},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Qin:1997:PEC:258623.258692,
 author = {Qin, Xiaohan and Baer, Jean-Loup},
 title = {A performance evaluation of cluster architectures},
 abstract = {This paper investigates the performance of shared-memory cluster-based architectures where each cluster is a shared-bus multiprocessor augmented with a protocol processor maintaining cache coherence across clusters. For a given number of processors, sixteen in this study, we evaluate the performance of various cluster configurations. We also consider the impact of adding a remote shared cache in each cluster. We use Mean Value Analysis to estimate the cache miss latencies of various types and the overall execution time. The service demands of shared resources are characterized in detail by examining the sub-requests issued in resolving cache misses. In addition to the architectural system parameters and the service demands on resources, the analytical model needs parameters pertinent to applications. The latter, in particular cache miss profiles, are obtained by trace-driven simulation of three benchmarks.Our results show that without remote caches the performance of cluster-based architectures is mixed. In some configurations, the negative effects of the longer latency of inter-cluster misses and of the contention on the protocol processor are too large to counter-balance the lower contention on the data buses. For two out of the three applications best results are obtained when the system has clusters of size 2 or 4. The cluster-based architectures with remote caches consistently outperform the single bus system for all 3 applications. We also exercise the model with parameters reflecting the current trend in technology making the processor relatively faster than the bus and memory. Under these new conditions, our results show a clear performance advantage for the cluster-based architectures, with or without remote caches, over single bus systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {237--247},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/258623.258692},
 doi = {http://doi.acm.org/10.1145/258623.258692},
 acmid = {258692},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chiueh:1997:DED:258623.258693,
 author = {Chiueh, Tzi-cker and Varadarajan, Srinidhi},
 title = {Design and evaluation of a DRAM-based shared memory ATM switch},
 abstract = {Beluga</i> is a single-chip switch architecture specifically targeted at local area ATM networks, and it features three architectural innovations. First, an interconnection hierarchy composed of multiple switching fabrics is built into the chip to provide both low-latency cell transfer when the traffic is light and low cell drop rate under heavy load. Secondly, to improve silicon efficiency, Beluga is based on shared memory architecture, and the buffers are implemented using DRAM rather than SRAM technology. Heavy interleaving and selective invalidation are used to address long latency and periodic refreshing problems, respectively. Thirdly, Beluga supports multicast with minimal physical bit replication. It also separates support for unicast and multicast cells to optimize for the common case, where multicast cells occur infrequently. This paper describes the design details of Beluga</i> and the results of a comprehensive simulation study to quantify the performance impact of each of its architectural features. The most important result from this research is that DRAM-based buffer implementation significantly reduces the cell-drop rate during heavy while exhibiting almost identical cell latency to SRAM-based implementation during light load. Therefore, we believe DRAM makes an attractive alternative for switch buffer implementation, especially for single-chip architecture such as Beluga.</i>},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258693},
 doi = {http://doi.acm.org/10.1145/258623.258693},
 acmid = {258693},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiueh:1997:DED:258612.258693,
 author = {Chiueh, Tzi-cker and Varadarajan, Srinidhi},
 title = {Design and evaluation of a DRAM-based shared memory ATM switch},
 abstract = {Beluga</i> is a single-chip switch architecture specifically targeted at local area ATM networks, and it features three architectural innovations. First, an interconnection hierarchy composed of multiple switching fabrics is built into the chip to provide both low-latency cell transfer when the traffic is light and low cell drop rate under heavy load. Secondly, to improve silicon efficiency, Beluga is based on shared memory architecture, and the buffers are implemented using DRAM rather than SRAM technology. Heavy interleaving and selective invalidation are used to address long latency and periodic refreshing problems, respectively. Thirdly, Beluga supports multicast with minimal physical bit replication. It also separates support for unicast and multicast cells to optimize for the common case, where multicast cells occur infrequently. This paper describes the design details of Beluga</i> and the results of a comprehensive simulation study to quantify the performance impact of each of its architectural features. The most important result from this research is that DRAM-based buffer implementation significantly reduces the cell-drop rate during heavy while exhibiting almost identical cell latency to SRAM-based implementation during light load. Therefore, we believe DRAM makes an attractive alternative for switch buffer implementation, especially for single-chip architecture such as Beluga.</i>},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258693},
 doi = {http://doi.acm.org/10.1145/258612.258693},
 acmid = {258693},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Song:1997:ERC:258623.258695,
 author = {Song, Junehwa and Dan, Asit and Sitaram, Dinkar},
 title = {Efficient retrieval of composite multimedia objects in the <italic>JINSIL</italic> distributed system},
 abstract = {In a distributed environment, presentation of structured, composite multimedia information poses new challenges in dealing with variable bandwidth (BW) requirement and synchronization of media data objects. The detailed knowledge of BW requirement obtained by analyzing the document structure can be used to create a prefetch schedule that results in efficient utilization of system resources. A distributed environment consists of various system components that are either dedicated to a client or shared across multiple clients. Shared system components could benefit from Fine Granularity Advanced Reservation (FGAR)</i> of resources based on true BW requirement. Prefetching by utilizing advance knowledge of BW requirement can further improve resource utilization. In this paper, we describe the JINSIL retrieval system that takes into account the available bandwidth and buffer resources and the nature of sharing in each component on the delivery path. It reshapes BW requirement, creates prefetch schedule for efficient resource utilization in each component, and reserves necessary BW and buffer. We also consider good choices for placement of prefetch buffers across various system components.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {260--271},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258623.258695},
 doi = {http://doi.acm.org/10.1145/258623.258695},
 acmid = {258695},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Song:1997:ERC:258612.258695,
 author = {Song, Junehwa and Dan, Asit and Sitaram, Dinkar},
 title = {Efficient retrieval of composite multimedia objects in the <italic>JINSIL</italic> distributed system},
 abstract = {In a distributed environment, presentation of structured, composite multimedia information poses new challenges in dealing with variable bandwidth (BW) requirement and synchronization of media data objects. The detailed knowledge of BW requirement obtained by analyzing the document structure can be used to create a prefetch schedule that results in efficient utilization of system resources. A distributed environment consists of various system components that are either dedicated to a client or shared across multiple clients. Shared system components could benefit from Fine Granularity Advanced Reservation (FGAR)</i> of resources based on true BW requirement. Prefetching by utilizing advance knowledge of BW requirement can further improve resource utilization. In this paper, we describe the JINSIL retrieval system that takes into account the available bandwidth and buffer resources and the nature of sharing in each component on the delivery path. It reshapes BW requirement, creates prefetch schedule for efficient resource utilization in each component, and reserves necessary BW and buffer. We also consider good choices for placement of prefetch buffers across various system components.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {260--271},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/258612.258695},
 doi = {http://doi.acm.org/10.1145/258612.258695},
 acmid = {258695},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gibson:1997:FSS:258623.258696,
 author = {Gibson, Garth A. and Nagle, David F. and Amiri, Khalil and Chang, Fay W. and Feinberg, Eugene M. and Gobioff, Howard and Lee, Chen and Ozceri, Berend and Riedel, Erik and Rochberg, David and Zelenka, Jim},
 title = {File server scaling with network-attached secure disks},
 abstract = {By providing direct data transfer between storage and client, network-attached storage devices have the potential to improve scalability for existing distributed file systems (by removing the server as a bottleneck) and bandwidth for new parallel and distributed file systems (through network striping and more efficient data paths). Together, these advantages influence a large enough fraction of the storage market to make commodity network-attached storage feasible. Realizing the technology's full potential requires careful consideration across a wide range of file system, networking and security issues. This paper contrasts two network-attached storage architectures---(1) Networked SCSI disks (NetSCSI) are network-attached storage devices with minimal changes from the familiar SCSI interface, while (2) Network-Attached Secure Disks (NASD) are drives that support independent client access to drive object services. To estimate the potential performance benefits of these architectures, we develop an analytic model and perform trace-driven replay experiments based on AFS and NFS traces. Our results suggest that NetSCSI can reduce file server load during a burst of NFS or AFS activity by about 30\%. With the NASD architecture, server load (during burst activity) can be reduced by a factor of up to five for AFS and up to ten for NFS.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {272--284},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258623.258696},
 doi = {http://doi.acm.org/10.1145/258623.258696},
 acmid = {258696},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gibson:1997:FSS:258612.258696,
 author = {Gibson, Garth A. and Nagle, David F. and Amiri, Khalil and Chang, Fay W. and Feinberg, Eugene M. and Gobioff, Howard and Lee, Chen and Ozceri, Berend and Riedel, Erik and Rochberg, David and Zelenka, Jim},
 title = {File server scaling with network-attached secure disks},
 abstract = {By providing direct data transfer between storage and client, network-attached storage devices have the potential to improve scalability for existing distributed file systems (by removing the server as a bottleneck) and bandwidth for new parallel and distributed file systems (through network striping and more efficient data paths). Together, these advantages influence a large enough fraction of the storage market to make commodity network-attached storage feasible. Realizing the technology's full potential requires careful consideration across a wide range of file system, networking and security issues. This paper contrasts two network-attached storage architectures---(1) Networked SCSI disks (NetSCSI) are network-attached storage devices with minimal changes from the familiar SCSI interface, while (2) Network-Attached Secure Disks (NASD) are drives that support independent client access to drive object services. To estimate the potential performance benefits of these architectures, we develop an analytic model and perform trace-driven replay experiments based on AFS and NFS traces. Our results suggest that NetSCSI can reduce file server load during a burst of NFS or AFS activity by about 30\%. With the NASD architecture, server load (during burst activity) can be reduced by a factor of up to five for AFS and up to ten for NFS.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {272--284},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258612.258696},
 doi = {http://doi.acm.org/10.1145/258612.258696},
 acmid = {258696},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tsiolis:1997:GCC:258623.258697,
 author = {Tsiolis, Athanassios K. and Vernon, Mary K.},
 title = {Group-guaranteed channel capacity in multimedia storage servers},
 abstract = {One of the open questions in the design of multimedia storage servers is in what order to serve incoming requests. Given the capability provided by the disk layout and scheduling algorithms to serve multiple streams simultaneously, improved request scheduling algorithms can reduce customer waiting times. This results in better service and/or lower customer loss. In this paper we define a new class of request scheduling algorithms, called Group-Guaranteed Server Capacity (GGSC), that preassign server channel capacity to groups of objects. We also define a particular formal method for computing the assigned capacities to achieve a given performance objective. We observe that the FCFS policy can provide the precise time of service to incoming customer requests. Under this assumption, we compare the performance of one of the new GGSC algorithms, GGSC<inf>W</inf>-FCFS, against FCFS and against two other recently proposed scheduling algorithms: Maximum Factored Queue length (MFQ), and the FCFS-n algorithm that preassigns capacity only to each of the n</i> most popular objects. The algorithms are compared for both competitive market</i> and captured audience</i> environments.Key findings of the algorithm comparisons are that: (1) FCFS-n has no advantage over FCFS if FCFS gives time of service guarantees to arriving customers, (2) FCFS and GGSC<inf>W</inf>-FCFS are superior to MFQ for both competitive and captive audience environments, (3) for competitive servers that are configured for customer loss less than 10\%, FCFS is superior to all other algorithms examined in this paper, and (4) for captive audience environments that have objects with variable playback length, GGSC<inf>W</inf>-FCFS is the most promising of the policies considered in this paper. The conclusions for FCFS-n and MFQ differ from previous work because we focus on competitive environments with customer loss under 10\%, we assume FCFS can provide time of service guarantees to all arriving customers, and we consider the distribution of customer waiting time as well as the average waiting time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {25},
 issue = {1},
 month = {June},
 year = {1997},
 issn = {0163-5999},
 pages = {285--297},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258623.258697},
 doi = {http://doi.acm.org/10.1145/258623.258697},
 acmid = {258697},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tsiolis:1997:GCC:258612.258697,
 author = {Tsiolis, Athanassios K. and Vernon, Mary K.},
 title = {Group-guaranteed channel capacity in multimedia storage servers},
 abstract = {One of the open questions in the design of multimedia storage servers is in what order to serve incoming requests. Given the capability provided by the disk layout and scheduling algorithms to serve multiple streams simultaneously, improved request scheduling algorithms can reduce customer waiting times. This results in better service and/or lower customer loss. In this paper we define a new class of request scheduling algorithms, called Group-Guaranteed Server Capacity (GGSC), that preassign server channel capacity to groups of objects. We also define a particular formal method for computing the assigned capacities to achieve a given performance objective. We observe that the FCFS policy can provide the precise time of service to incoming customer requests. Under this assumption, we compare the performance of one of the new GGSC algorithms, GGSC<inf>W</inf>-FCFS, against FCFS and against two other recently proposed scheduling algorithms: Maximum Factored Queue length (MFQ), and the FCFS-n algorithm that preassigns capacity only to each of the n</i> most popular objects. The algorithms are compared for both competitive market</i> and captured audience</i> environments.Key findings of the algorithm comparisons are that: (1) FCFS-n has no advantage over FCFS if FCFS gives time of service guarantees to arriving customers, (2) FCFS and GGSC<inf>W</inf>-FCFS are superior to MFQ for both competitive and captive audience environments, (3) for competitive servers that are configured for customer loss less than 10\%, FCFS is superior to all other algorithms examined in this paper, and (4) for captive audience environments that have objects with variable playback length, GGSC<inf>W</inf>-FCFS is the most promising of the policies considered in this paper. The conclusions for FCFS-n and MFQ differ from previous work because we focus on competitive environments with customer loss under 10\%, we assume FCFS can provide time of service guarantees to all arriving customers, and we consider the distribution of customer waiting time as well as the average waiting time.},
 booktitle = {Proceedings of the 1997 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '97},
 year = {1997},
 isbn = {0-89791-909-2},
 location = {Seattle, Washington, United States},
 pages = {285--297},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/258612.258697},
 doi = {http://doi.acm.org/10.1145/258612.258697},
 acmid = {258697},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chapin:1995:MSP:223586.223588,
 author = {Chapin, John and Herrod, A. and Rosenblum, Mendel and Gupta, Anoop},
 title = {Memory system performance of UNIX on CC-NUMA multiprocessors},
 abstract = {This study characterizes the performance of a variant of UNIX SVR4 on a large shared-memory multiprocessor and analyzes the effects of possible OS and architectural changes. We use a nonintrusive cache miss monitor to trace the execution of an OS-intensive multiprogrammed workload on the Stanford DASH, a 32-CPU CC-NUMA multiprocessor (CC-NUMA multiprocessors have cache-coherent shared memory that is physically distributed across the machine). We find that our version of UNIX accounts for 24\% of the workload's total execution time. A surprisingly large fraction of OS time (79\%) is spent on memory system stalls, divided equally between instruction and data cache miss time. In analyzing techniques to reduce instruction cache miss stall time, we find that replication of only 7\% of the OS code would allow 80\% of instruction cache misses to be serviced locally on a CC-NUMA machine. For data cache misses, we find that a small number of routines account for 96\% of OS data cache stall time. We find that most of these misses are coherence (communication) misses, and larger caches will not necessarily help. After presenting detailed performance data, we analyze the benefits of several OS changes and predict the effects of altering the cache configuration, degree of clustering, and cache coherence mechanism of the machine. (This paper is available via http://wwwflash.stanford.edu.)},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/223586.223588},
 doi = {http://doi.acm.org/10.1145/223586.223588},
 acmid = {223588},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chapin:1995:MSP:223587.223588,
 author = {Chapin, John and Herrod, A. and Rosenblum, Mendel and Gupta, Anoop},
 title = {Memory system performance of UNIX on CC-NUMA multiprocessors},
 abstract = {This study characterizes the performance of a variant of UNIX SVR4 on a large shared-memory multiprocessor and analyzes the effects of possible OS and architectural changes. We use a nonintrusive cache miss monitor to trace the execution of an OS-intensive multiprogrammed workload on the Stanford DASH, a 32-CPU CC-NUMA multiprocessor (CC-NUMA multiprocessors have cache-coherent shared memory that is physically distributed across the machine). We find that our version of UNIX accounts for 24\% of the workload's total execution time. A surprisingly large fraction of OS time (79\%) is spent on memory system stalls, divided equally between instruction and data cache miss time. In analyzing techniques to reduce instruction cache miss stall time, we find that replication of only 7\% of the OS code would allow 80\% of instruction cache misses to be serviced locally on a CC-NUMA machine. For data cache misses, we find that a small number of routines account for 96\% of OS data cache stall time. We find that most of these misses are coherence (communication) misses, and larger caches will not necessarily help. After presenting detailed performance data, we analyze the benefits of several OS changes and predict the effects of altering the cache configuration, degree of clustering, and cache coherence mechanism of the machine. (This paper is available via http://wwwflash.stanford.edu.)},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {1--13},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/223587.223588},
 doi = {http://doi.acm.org/10.1145/223587.223588},
 acmid = {223588},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bedichek:1995:TFA:223587.223589,
 author = {Bedichek, Robert C.},
 title = {Talisman: fast and accurate multicomputer simulation},
 abstract = {Talisman is a simulator that models the execution semantics and timing of a multicomputer. Talisman is unique in combining high semantic accuracy, high timing accuracy, portability, and</i> good performance. This good performance allows users to run significant programs on large simulated multicomputers. The combination of high accuracy and good performance yields an ideal tool for evaluating architectural trade-offs. Talisman models the semantics of virtual memory, a circuit-switched internode interconnect, I/O devices, and instruction execution in both user and supervisor modes. It also models the timing of processor pipelines, caches, local memory buses, and a circuit-switched interconnect. Talisman executes the same program binary images as a hardware prototype at a cost of about 100 host instructions per simulated instruction. On a suite of accuracy benchmarks run on the hardware and the simulator, Talisman and the prototype differ in reported running times by only a few percent.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223587.223589},
 doi = {http://doi.acm.org/10.1145/223587.223589},
 acmid = {223589},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bedichek:1995:TFA:223586.223589,
 author = {Bedichek, Robert C.},
 title = {Talisman: fast and accurate multicomputer simulation},
 abstract = {Talisman is a simulator that models the execution semantics and timing of a multicomputer. Talisman is unique in combining high semantic accuracy, high timing accuracy, portability, and</i> good performance. This good performance allows users to run significant programs on large simulated multicomputers. The combination of high accuracy and good performance yields an ideal tool for evaluating architectural trade-offs. Talisman models the semantics of virtual memory, a circuit-switched internode interconnect, I/O devices, and instruction execution in both user and supervisor modes. It also models the timing of processor pipelines, caches, local memory buses, and a circuit-switched interconnect. Talisman executes the same program binary images as a hardware prototype at a cost of about 100 host instructions per simulated instruction. On a suite of accuracy benchmarks run on the hardware and the simulator, Talisman and the prototype differ in reported running times by only a few percent.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223586.223589},
 doi = {http://doi.acm.org/10.1145/223586.223589},
 acmid = {223589},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Golubchik:1995:RID:223587.223590,
 author = {Golubchik, Leana and Lui, John C. S. and Muntz, Richard},
 title = {Reducing I/O demand in video-on-demand storage servers},
 abstract = {Recent technological advances have made multimedia on-demand services, such as home entertainment and home-shopping, important to the consumer market. One of the most challenging aspects of this type of service is providing access either instantaneously or within a small and reasonable latency upon request. In this paper, we discuss a novel approach, termed adaptive piggybacking, which can be used to provide on-demand or nearly-on-demand service and at the same time reduce the I/O demand on the multimedia storage server.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223590},
 doi = {http://doi.acm.org/10.1145/223587.223590},
 acmid = {223590},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Golubchik:1995:RID:223586.223590,
 author = {Golubchik, Leana and Lui, John C. S. and Muntz, Richard},
 title = {Reducing I/O demand in video-on-demand storage servers},
 abstract = {Recent technological advances have made multimedia on-demand services, such as home entertainment and home-shopping, important to the consumer market. One of the most challenging aspects of this type of service is providing access either instantaneously or within a small and reasonable latency upon request. In this paper, we discuss a novel approach, termed adaptive piggybacking, which can be used to provide on-demand or nearly-on-demand service and at the same time reduce the I/O demand on the multimedia storage server.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223590},
 doi = {http://doi.acm.org/10.1145/223586.223590},
 acmid = {223590},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghandeharizadeh:1995:CSD:223586.223591,
 author = {Ghandeharizadeh, Shahram and Kim, Seon Ho and Shahabi, Cyrus},
 title = {On configuring a single disk continuous media server},
 abstract = {The past decade has witnessed a proliferation of repositories that store and retrieve continuous media data types, e.g., audio and video objects. These repositories are expected to play a major role in several emerging applications, e.g., library information systems, educational applications, entertainment industry, etc. To support the display of a video object, the system partitions each object into fixed size blocks. All blocks of an object reside permanently on the disk drive. When displaying an object, the system stages the blocks of the object into memory one at a time for immediate display. In the presence of multiple displays referencing different objects, the bandwidth of the disk drive is multiplexed among requests, introducing disk seeks. Disk seeks reduce the useful utilization of the disk bandwidth and result in a lower number of simultaneous displays (throughput).This paper characterizes the impact of disk seeks on the throughput of the system. It describes REBECA as a mechanism that maximizes the throughput of the system by minimizing the time attributed to each incurred seek. A limitation of REBECA is that it increases the latency observed by each request. We quantify this throughput vs latency tradeoff of REBECA and, develop an efficient technique that computes its configuration parameters to realize the performance requirements (desired latency and throughput) of an application.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223591},
 doi = {http://doi.acm.org/10.1145/223586.223591},
 acmid = {223591},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ghandeharizadeh:1995:CSD:223587.223591,
 author = {Ghandeharizadeh, Shahram and Kim, Seon Ho and Shahabi, Cyrus},
 title = {On configuring a single disk continuous media server},
 abstract = {The past decade has witnessed a proliferation of repositories that store and retrieve continuous media data types, e.g., audio and video objects. These repositories are expected to play a major role in several emerging applications, e.g., library information systems, educational applications, entertainment industry, etc. To support the display of a video object, the system partitions each object into fixed size blocks. All blocks of an object reside permanently on the disk drive. When displaying an object, the system stages the blocks of the object into memory one at a time for immediate display. In the presence of multiple displays referencing different objects, the bandwidth of the disk drive is multiplexed among requests, introducing disk seeks. Disk seeks reduce the useful utilization of the disk bandwidth and result in a lower number of simultaneous displays (throughput).This paper characterizes the impact of disk seeks on the throughput of the system. It describes REBECA as a mechanism that maximizes the throughput of the system by minimizing the time attributed to each incurred seek. A limitation of REBECA is that it increases the latency observed by each request. We quantify this throughput vs latency tradeoff of REBECA and, develop an efficient technique that computes its configuration parameters to realize the performance requirements (desired latency and throughput) of an application.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223591},
 doi = {http://doi.acm.org/10.1145/223587.223591},
 acmid = {223591},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krunz:1995:TMV:223587.223592,
 author = {Krunz, Marwan and Hughes, Herman},
 title = {A traffic for MPEG-coded VBR streams},
 abstract = {Compression of digital video is the only viable means to transport real-time full-motion video over BISDN/ATM networks. Traffic streams generated by video compressors exhibit complicated patterns which vary from one compression scheme to another. In this paper we investigate the traffic characteristics of video streams which are compressed based on the MPEG standard. Our study is based on 23 minutes of video obtained from an entertainment movie. A particular significance of our data is that it contains all types of coded frames, namely: Intra-coded (I), Prediction (P), and Bidirectional (B) MPEG frames. We describe the statistical behavior of the VBR stream using histograms and autocorrelation functions. A procedure is developed to determine the instants of a scene change based on the changes in the size of successive I</i> frames. It is found that the length of a scene can be modeled by a geometric distribution.A model for an MPEG traffic source is developed in which frames are generated according to the compression pattern of the captured video stream. For each frame type, the number of cells per frame is fitted by a lognormal distribution whose parameters are determined by the frame type. The appropriateness and limitations of the model are examined by studying the multiplexing performance of MPEG streams. Simulations of an ATM multiplexer are conducted, in which traffic sources are derived from the measured VBR trace as well as the proposed model. The queueing performance in both cases is found to be relatively close.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {47--55},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223587.223592},
 doi = {http://doi.acm.org/10.1145/223587.223592},
 acmid = {223592},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krunz:1995:TMV:223586.223592,
 author = {Krunz, Marwan and Hughes, Herman},
 title = {A traffic for MPEG-coded VBR streams},
 abstract = {Compression of digital video is the only viable means to transport real-time full-motion video over BISDN/ATM networks. Traffic streams generated by video compressors exhibit complicated patterns which vary from one compression scheme to another. In this paper we investigate the traffic characteristics of video streams which are compressed based on the MPEG standard. Our study is based on 23 minutes of video obtained from an entertainment movie. A particular significance of our data is that it contains all types of coded frames, namely: Intra-coded (I), Prediction (P), and Bidirectional (B) MPEG frames. We describe the statistical behavior of the VBR stream using histograms and autocorrelation functions. A procedure is developed to determine the instants of a scene change based on the changes in the size of successive I</i> frames. It is found that the length of a scene can be modeled by a geometric distribution.A model for an MPEG traffic source is developed in which frames are generated according to the compression pattern of the captured video stream. For each frame type, the number of cells per frame is fitted by a lognormal distribution whose parameters are determined by the frame type. The appropriateness and limitations of the model are examined by studying the multiplexing performance of MPEG streams. Simulations of an ATM multiplexer are conducted, in which traffic sources are derived from the measured VBR trace as well as the proposed model. The queueing performance in both cases is found to be relatively close.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {47--55},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/223586.223592},
 doi = {http://doi.acm.org/10.1145/223586.223592},
 acmid = {223592},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Williamson:1995:NTM:223587.223593,
 author = {Williamson, Carey L.},
 title = {Network traffic measurement and modeling},
 abstract = {Network traffic measurement and workload characterization are key steps in the workload modeling process. Much has been learned through network measurement and workload modeling in the last ten years, but new challenges are now at the forefront: measuring network traffic in the Internet environment, understanding the implications of network traffic structure (e.g., self-similarity, autocorrelation, long range dependence), and accurate modeling of network traffic workloads for high speed network environments.This "hot topic" session brings together three prominent speakers to address each of these topics, in turn.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {56--57},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223593},
 doi = {http://doi.acm.org/10.1145/223587.223593},
 acmid = {223593},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Williamson:1995:NTM:223586.223593,
 author = {Williamson, Carey L.},
 title = {Network traffic measurement and modeling},
 abstract = {Network traffic measurement and workload characterization are key steps in the workload modeling process. Much has been learned through network measurement and workload modeling in the last ten years, but new challenges are now at the forefront: measuring network traffic in the Internet environment, understanding the implications of network traffic structure (e.g., self-similarity, autocorrelation, long range dependence), and accurate modeling of network traffic workloads for high speed network environments.This "hot topic" session brings together three prominent speakers to address each of these topics, in turn.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {56--57},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223593},
 doi = {http://doi.acm.org/10.1145/223586.223593},
 acmid = {223593},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gelenbe:1995:GNQ:223586.376966,
 author = {Gelenbe, Erol},
 title = {G-networks: new queueing models with additional control capabilities},
 abstract = {This Hot-Topics Session on G-Networks aims at bringing these relatively new models which we introduced for the first time in 1989 and 1990, to the attention of the performance evaluation and modeling community. The session includes presentations by <b>Peter Harrison, Onno Boxma, Jean-Michel Fourneau</b> and myself. We will cover the basic concepts, some examples of potential applications, as well as recent research efforts in this area.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {58--59},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.376966},
 doi = {http://doi.acm.org/10.1145/223586.376966},
 acmid = {376966},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelenbe:1995:GNQ:223587.376966,
 author = {Gelenbe, Erol},
 title = {G-networks: new queueing models with additional control capabilities},
 abstract = {This Hot-Topics Session on G-Networks aims at bringing these relatively new models which we introduced for the first time in 1989 and 1990, to the attention of the performance evaluation and modeling community. The session includes presentations by <b>Peter Harrison, Onno Boxma, Jean-Michel Fourneau</b> and myself. We will cover the basic concepts, some examples of potential applications, as well as recent research efforts in this area.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {58--59},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.376966},
 doi = {http://doi.acm.org/10.1145/223587.376966},
 acmid = {376966},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tridandapani:1995:FPF:223587.223594,
 author = {Tridandapani, Srini and Dahbura, Anton T. and Martel, Charles U. and Matthews, John and Somani, Arun K.},
 title = {Free performance and fault tolerance (extended abstract): using system idle capacity efficiently},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {60--61},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223594},
 doi = {http://doi.acm.org/10.1145/223587.223594},
 acmid = {223594},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tridandapani:1995:FPF:223586.223594,
 author = {Tridandapani, Srini and Dahbura, Anton T. and Martel, Charles U. and Matthews, John and Somani, Arun K.},
 title = {Free performance and fault tolerance (extended abstract): using system idle capacity efficiently},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {60--61},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223594},
 doi = {http://doi.acm.org/10.1145/223586.223594},
 acmid = {223594},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Malony:1995:DIE:223586.223595,
 author = {Malony, Allen D.},
 title = {Data interpretation and experiment planning in performance tools},
 abstract = {The parallel scientific computing community is placing increasing emphasis on portability and scalability of programs, languages, and architectures. This creates new challenges for developers of parallel performance analysis tools, who will have to deal with increasing volumes of performance data drawn from diverse platforms. One way to meet this challenge is to incorporate sophisticated facilities for data interpretation and experiment planning within the tools themselves, giving them increased flexibility and autonomy in gathering and selecting performance data. This panel discussion brings together four research groups that have made advances in this direction.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {62--63},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223595},
 doi = {http://doi.acm.org/10.1145/223586.223595},
 acmid = {223595},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Malony:1995:DIE:223587.223595,
 author = {Malony, Allen D.},
 title = {Data interpretation and experiment planning in performance tools},
 abstract = {The parallel scientific computing community is placing increasing emphasis on portability and scalability of programs, languages, and architectures. This creates new challenges for developers of parallel performance analysis tools, who will have to deal with increasing volumes of performance data drawn from diverse platforms. One way to meet this challenge is to incorporate sophisticated facilities for data interpretation and experiment planning within the tools themselves, giving them increased flexibility and autonomy in gathering and selecting performance data. This panel discussion brings together four research groups that have made advances in this direction.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {62--63},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223595},
 doi = {http://doi.acm.org/10.1145/223587.223595},
 acmid = {223595},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vaidya:1995:CTD:223586.223596,
 author = {Vaidya, Nitin H.},
 title = {A case for two-level distributed recovery schemes},
 abstract = {Most distributed and multiprocessor recovery schemes proposed in the literature are designed to tolerate arbitrary number of failures. In this paper, we demonstrate that, it is often advantageous to use "two-level" recovery schemes. A two-level</i> recovery scheme tolerates the more probable</i> failures with low performance overhead, while the less probable failures may be tolerated with a higher overhead. By minimizing the overhead for the more frequently occurring failure scenarios, our approach is expected to achieve lower performance overhead (on average) as compared to existing recovery schemes.To demonstrate the advantages of two-level recovery, we evaluate the performance of a recovery scheme that takes two different types of checkpoints, namely, 1-checkpoints and N</i>-checkpoints. A single failure can be tolerated by rolling the system back to a 1-checkpoint, while multiple failure recovery is possible by rolling back to an N</i>-checkpoint. For such a system, we demonstrate that to minimize the average overhead, it is often necessary to take both</i> 1-checkpoints and N</i>-checkpoints.While the conclusions of this paper are intuitive, the work on design of appropriate recovery schemes is lacking. The objective of this paper is to motivate research into recovery schemes that can provide multiple levels of fault tolerance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {64--73},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223596},
 doi = {http://doi.acm.org/10.1145/223586.223596},
 acmid = {223596},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vaidya:1995:CTD:223587.223596,
 author = {Vaidya, Nitin H.},
 title = {A case for two-level distributed recovery schemes},
 abstract = {Most distributed and multiprocessor recovery schemes proposed in the literature are designed to tolerate arbitrary number of failures. In this paper, we demonstrate that, it is often advantageous to use "two-level" recovery schemes. A two-level</i> recovery scheme tolerates the more probable</i> failures with low performance overhead, while the less probable failures may be tolerated with a higher overhead. By minimizing the overhead for the more frequently occurring failure scenarios, our approach is expected to achieve lower performance overhead (on average) as compared to existing recovery schemes.To demonstrate the advantages of two-level recovery, we evaluate the performance of a recovery scheme that takes two different types of checkpoints, namely, 1-checkpoints and N</i>-checkpoints. A single failure can be tolerated by rolling the system back to a 1-checkpoint, while multiple failure recovery is possible by rolling back to an N</i>-checkpoint. For such a system, we demonstrate that to minimize the average overhead, it is often necessary to take both</i> 1-checkpoints and N</i>-checkpoints.While the conclusions of this paper are intuitive, the work on design of appropriate recovery schemes is lacking. The objective of this paper is to motivate research into recovery schemes that can provide multiple levels of fault tolerance.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {64--73},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223596},
 doi = {http://doi.acm.org/10.1145/223587.223596},
 acmid = {223596},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Epema:1995:ADS:223586.223597,
 author = {Epema, D. H. J.},
 title = {An analysis of decay-usage scheduling in multiprocessors},
 abstract = {Priority-ageing or decay-usage scheduling is a time-sharing scheduling policy capable of dealing with a workload of both interactive and batch jobs by decreasing the priority of a job when it acquires CPU time, and by increasing its priority when it does not use the (a) CPU. In this paper we deal with a decay-usage scheduling policy in multiprocessor systems modeled after widely used systems. The priority of a job consists of a base priority and a time-dependent part based on processor usage. Because the priorities in our model are time dependent, a queueing-theoretic analysis, for instance for the mean response time, seems impossible. Still, it turns out that as a consequence of the scheduling policy, the shares of available CPU time obtained by jobs converge, and a deterministic analysis for these shares is feasible: for a fixed set of jobs with very large (infinite) processing demands, we derive the relation between their base priorities and their steady-state shares. In addition, we analyze the relation between the values of the parameters of the scheduler and the level of control it can exercise over the steady-state shares. We validate the model by simulations and by measurements of actual systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223597},
 doi = {http://doi.acm.org/10.1145/223586.223597},
 acmid = {223597},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Epema:1995:ADS:223587.223597,
 author = {Epema, D. H. J.},
 title = {An analysis of decay-usage scheduling in multiprocessors},
 abstract = {Priority-ageing or decay-usage scheduling is a time-sharing scheduling policy capable of dealing with a workload of both interactive and batch jobs by decreasing the priority of a job when it acquires CPU time, and by increasing its priority when it does not use the (a) CPU. In this paper we deal with a decay-usage scheduling policy in multiprocessor systems modeled after widely used systems. The priority of a job consists of a base priority and a time-dependent part based on processor usage. Because the priorities in our model are time dependent, a queueing-theoretic analysis, for instance for the mean response time, seems impossible. Still, it turns out that as a consequence of the scheduling policy, the shares of available CPU time obtained by jobs converge, and a deterministic analysis for these shares is feasible: for a fixed set of jobs with very large (infinite) processing demands, we derive the relation between their base priorities and their steady-state shares. In addition, we analyze the relation between the values of the parameters of the scheduler and the level of control it can exercise over the steady-state shares. We validate the model by simulations and by measurements of actual systems.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223597},
 doi = {http://doi.acm.org/10.1145/223587.223597},
 acmid = {223597},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Elwalid:1995:FRP:223586.223598,
 author = {Elwalid, Anwar and Heyman, Daniel and Lakshman, T. V. and Mitra, Debasis and Weiss, Alan},
 title = {Fundamental results on the performance of ATM multiplexers with applications to video teleconferencing},
 abstract = {The main contributions of this paper are two-fold. First, we prove fundamental, similarly behaving lower and upper bounds, and give an approximation based on the bounds, which is effective for analyzing ATM multiplexers, even when the traffic has many, possibly heterogeneous, sources and their models are of high dimension. Second, we apply our analytic approximation to statistical models of video teleconference traffic, obtain the multiplexing system's capacity as determined by the number of admissible sources for given cell loss probability, buffer size and trunk bandwidth, and, finally, compare with results from simulations, which are driven by actual data from coders. The results are surprisingly close. Our bounds are based on Large Deviations theory. Our approximation has two easily calculated parameters, one is from Chernoff's theorem and the other is the system's dominant eigenvalue. A broad range of systems are analyzed and the time for analysis in each case is a fraction of a second.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223598},
 doi = {http://doi.acm.org/10.1145/223586.223598},
 acmid = {223598},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Elwalid:1995:FRP:223587.223598,
 author = {Elwalid, Anwar and Heyman, Daniel and Lakshman, T. V. and Mitra, Debasis and Weiss, Alan},
 title = {Fundamental results on the performance of ATM multiplexers with applications to video teleconferencing},
 abstract = {The main contributions of this paper are two-fold. First, we prove fundamental, similarly behaving lower and upper bounds, and give an approximation based on the bounds, which is effective for analyzing ATM multiplexers, even when the traffic has many, possibly heterogeneous, sources and their models are of high dimension. Second, we apply our analytic approximation to statistical models of video teleconference traffic, obtain the multiplexing system's capacity as determined by the number of admissible sources for given cell loss probability, buffer size and trunk bandwidth, and, finally, compare with results from simulations, which are driven by actual data from coders. The results are surprisingly close. Our bounds are based on Large Deviations theory. Our approximation has two easily calculated parameters, one is from Chernoff's theorem and the other is the system's dominant eigenvalue. A broad range of systems are analyzed and the time for analysis in each case is a fraction of a second.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223598},
 doi = {http://doi.acm.org/10.1145/223587.223598},
 acmid = {223598},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Knightly:1995:FLT:223587.223599,
 author = {Knightly, Edward W. and Wrege, Dallas E. and Liebeherr, J\"{o}rg and Zhang, Hui},
 title = {Fundamental limits and tradeoffs of providing deterministic guarantees to VBR video traffic},
 abstract = {Compressed digital video is one of the most important traffic types in future integrated services networks. However, a network service that supports delay-sensitive video imposes many problems since compressed video sources are variable bit rate (VBR) with a high degree of burstiness. In this paper, we consider a network service that can provide deterministic guarantees on the minimum throughput and the maximum delay of VBR video traffic. A common belief is that due to the burstiness of VBR traffic, such a service will not be efficient and will necessarily result in low network utilization. We investigate the fundamental limits and tradeoffs in providing deterministic performance guarantees to video and use a set of 10 to 90 minute long MPEG-compressed video traces for evaluation. Contrary to conventional wisdom, we are able to show that, in many cases, a deterministic service can be provided to video traffic while maintaining a reasonable level of network utilization. We first consider an ideal network environment that employs the most accurate deterministic, time-invariant video traffic characterizations, Earliest-Deadline-First packet schedulers, and exact admission control conditions. The utilization achievable in this situation provides the fundamental limits of a deterministic service. We then investigate the utilization limits in a network environment that takes into account practical constraints, such as the need for fast policing mechanisms, simple packet scheduling algorithms, and efficient admission control tests.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223599},
 doi = {http://doi.acm.org/10.1145/223587.223599},
 acmid = {223599},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Knightly:1995:FLT:223586.223599,
 author = {Knightly, Edward W. and Wrege, Dallas E. and Liebeherr, J\"{o}rg and Zhang, Hui},
 title = {Fundamental limits and tradeoffs of providing deterministic guarantees to VBR video traffic},
 abstract = {Compressed digital video is one of the most important traffic types in future integrated services networks. However, a network service that supports delay-sensitive video imposes many problems since compressed video sources are variable bit rate (VBR) with a high degree of burstiness. In this paper, we consider a network service that can provide deterministic guarantees on the minimum throughput and the maximum delay of VBR video traffic. A common belief is that due to the burstiness of VBR traffic, such a service will not be efficient and will necessarily result in low network utilization. We investigate the fundamental limits and tradeoffs in providing deterministic performance guarantees to video and use a set of 10 to 90 minute long MPEG-compressed video traces for evaluation. Contrary to conventional wisdom, we are able to show that, in many cases, a deterministic service can be provided to video traffic while maintaining a reasonable level of network utilization. We first consider an ideal network environment that employs the most accurate deterministic, time-invariant video traffic characterizations, Earliest-Deadline-First packet schedulers, and exact admission control conditions. The utilization achievable in this situation provides the fundamental limits of a deterministic service. We then investigate the utilization limits in a network environment that takes into account practical constraints, such as the need for fast policing mechanisms, simple packet scheduling algorithms, and efficient admission control tests.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223599},
 doi = {http://doi.acm.org/10.1145/223586.223599},
 acmid = {223599},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Fang:1995:EBW:223586.223600,
 author = {Fang, Youjian and Devetsikiotis, Michael and Lambadaris, Ioannis and Kaye, A. Roger},
 title = {Exponential bounds for the waiting time distribution in Markovian queues, with applications to <italic>TES</italic>/<italic>GI</italic>/1 systems},
 abstract = {Several services to be supported by emerging high-speed networks are expected to result in highly bursty</i> (autocorrelated) traffic streams. A typical example is variable bit-rate (VBR) compressed video. Therefore, traffic modeling and performance evaluation techniques geared towards autocorrelated streams are extremely important for the design of practical networks.The TES</i> (Transform - Expand - Sample) technique has emerged as a general methodology for modeling autocorrelated random processes with arbitrary marginal distributions. Because of their generality and practical applicability, TES models can be readily used to accurately characterize bursty traffic streams in ATM networks.Although TES models can be easily implemented for simulation studies, the need still exists for analytical</i> results on the performance of queueing systems driven by autocorrelated traffic. Of particular interest are the tails of the waiting time distribution in queues driven by TES-modeled bursty traffic. Such tail probabilities, when they become exceedingly small, may be difficult to obtain via conventional simulation.In order to extend existing results, based on Large Deviations theory, to TES processes, the main difficulty is posed by the continuous state-space of the TES time-series. In this paper, we develop a general result concerning exponential bounds for the waiting time under continuous state-space</i> Markov arrivals. We apply this result to TES/GI</i>/1 queues, show numerical examples, and compare our bound with simulation results. Accurate estimates of extremely low probabilities are obtained by employing fast simulation techniques based on importance sampling.</i>},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {108--115},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/223586.223600},
 doi = {http://doi.acm.org/10.1145/223586.223600},
 acmid = {223600},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Fang:1995:EBW:223587.223600,
 author = {Fang, Youjian and Devetsikiotis, Michael and Lambadaris, Ioannis and Kaye, A. Roger},
 title = {Exponential bounds for the waiting time distribution in Markovian queues, with applications to <italic>TES</italic>/<italic>GI</italic>/1 systems},
 abstract = {Several services to be supported by emerging high-speed networks are expected to result in highly bursty</i> (autocorrelated) traffic streams. A typical example is variable bit-rate (VBR) compressed video. Therefore, traffic modeling and performance evaluation techniques geared towards autocorrelated streams are extremely important for the design of practical networks.The TES</i> (Transform - Expand - Sample) technique has emerged as a general methodology for modeling autocorrelated random processes with arbitrary marginal distributions. Because of their generality and practical applicability, TES models can be readily used to accurately characterize bursty traffic streams in ATM networks.Although TES models can be easily implemented for simulation studies, the need still exists for analytical</i> results on the performance of queueing systems driven by autocorrelated traffic. Of particular interest are the tails of the waiting time distribution in queues driven by TES-modeled bursty traffic. Such tail probabilities, when they become exceedingly small, may be difficult to obtain via conventional simulation.In order to extend existing results, based on Large Deviations theory, to TES processes, the main difficulty is posed by the continuous state-space of the TES time-series. In this paper, we develop a general result concerning exponential bounds for the waiting time under continuous state-space</i> Markov arrivals. We apply this result to TES/GI</i>/1 queues, show numerical examples, and compare our bound with simulation results. Accurate estimates of extremely low probabilities are obtained by employing fast simulation techniques based on importance sampling.</i>},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {108--115},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/223587.223600},
 doi = {http://doi.acm.org/10.1145/223587.223600},
 acmid = {223600},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Borst:1995:OPA:223586.223601,
 author = {Borst, S. C.},
 title = {Optimal probabilistic allocation of customer types to servers},
 abstract = {The model under consideration consists of n</i> customer types attended by m</i> parallel non-identical servers. Customers are allocated to the servers in a probabilistic manner; upon arrival customers are sent to one of the servers according to an m \&amp;times; n</i> matrix of routing probabilities. We consider the problem of finding an allocation that minimizes a weighted sum of the mean waiting times. We expose the structure of an optimal allocation and describe for some special cases in detail how the structure may be exploited in actually determining an optimal allocation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {116--125},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223601},
 doi = {http://doi.acm.org/10.1145/223586.223601},
 acmid = {223601},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Borst:1995:OPA:223587.223601,
 author = {Borst, S. C.},
 title = {Optimal probabilistic allocation of customer types to servers},
 abstract = {The model under consideration consists of n</i> customer types attended by m</i> parallel non-identical servers. Customers are allocated to the servers in a probabilistic manner; upon arrival customers are sent to one of the servers according to an m \&amp;times; n</i> matrix of routing probabilities. We consider the problem of finding an allocation that minimizes a weighted sum of the mean waiting times. We expose the structure of an optimal allocation and describe for some special cases in detail how the structure may be exploited in actually determining an optimal allocation.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {116--125},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223601},
 doi = {http://doi.acm.org/10.1145/223587.223601},
 acmid = {223601},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Matta:1995:ZIS:223586.223602,
 author = {Matta, Ibrahim and Shankar, A. Udaya},
 title = {<italic>Z</italic>-iteration: a simple method for throughput estimation in time-dependent multi-class systems},
 abstract = {Multiple-class multiple-resource (MCMR) systems, where each class of customers requires a particular set of resources, are common. These systems are often analyzed under steady-state conditions. We describe a simple method, referred to as Z-iteration</i>, to estimate both transient and steady-state performances of such systems. The method makes use of results and techniques available from queueing theory, network analysis, dynamic flow theory, and numerical analysis. We show the generality of the Z-iteration by applying it to an ATM network, a parallel disk system, and a distributed batch system. Validations against discrete-event simulations show the accuracy and computational advantages of the Z-iteration.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {126--135},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223602},
 doi = {http://doi.acm.org/10.1145/223586.223602},
 acmid = {223602},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Matta:1995:ZIS:223587.223602,
 author = {Matta, Ibrahim and Shankar, A. Udaya},
 title = {<italic>Z</italic>-iteration: a simple method for throughput estimation in time-dependent multi-class systems},
 abstract = {Multiple-class multiple-resource (MCMR) systems, where each class of customers requires a particular set of resources, are common. These systems are often analyzed under steady-state conditions. We describe a simple method, referred to as Z-iteration</i>, to estimate both transient and steady-state performances of such systems. The method makes use of results and techniques available from queueing theory, network analysis, dynamic flow theory, and numerical analysis. We show the generality of the Z-iteration by applying it to an ATM network, a parallel disk system, and a distributed batch system. Validations against discrete-event simulations show the accuracy and computational advantages of the Z-iteration.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {126--135},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223602},
 doi = {http://doi.acm.org/10.1145/223587.223602},
 acmid = {223602},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1995:SRL:223586.223603,
 author = {Chen, Peter M. and Lee, Edward K.},
 title = {Striping in a RAID level 5 disk array},
 abstract = {Redundant disk arrays are an increasingly popular way to improve I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to stripe data in redundant disk arrays such as RAID Level 5, or on how the choice of striping unit varies with the number of disks. Using synthetic workloads, we derive simple design rules for striping data in RAID Level 5 disk arrays given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems.We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read-modify writes or reconstruct writes. This additional factor causes the optimal striping unit for RAID Level 5 to be four times smaller for write-intensive workloads than for read-intensive workloads.We next investigate how the optimal striping unit varies with the number of disks in an array. We find that the optimal striping unit for reads in a RAID Level 5 varies inversely</i> to the number of disks, but that the optimal striping unit for writes varies with</i> the number of disks. Overall, we find that the optimal striping unit for workloads with an unspecified mix of reads and writes is independent</i> of the number of disks.Together, these trends lead us to recommend (in the absence of specific workload information) that the striping unit over a wide range of RAID Level 5 disk array sizes be equal to 1/2 * average positioning time * disk transfer rate.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223603},
 doi = {http://doi.acm.org/10.1145/223586.223603},
 acmid = {223603},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1995:SRL:223587.223603,
 author = {Chen, Peter M. and Lee, Edward K.},
 title = {Striping in a RAID level 5 disk array},
 abstract = {Redundant disk arrays are an increasingly popular way to improve I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to stripe data in redundant disk arrays such as RAID Level 5, or on how the choice of striping unit varies with the number of disks. Using synthetic workloads, we derive simple design rules for striping data in RAID Level 5 disk arrays given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems.We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read-modify writes or reconstruct writes. This additional factor causes the optimal striping unit for RAID Level 5 to be four times smaller for write-intensive workloads than for read-intensive workloads.We next investigate how the optimal striping unit varies with the number of disks in an array. We find that the optimal striping unit for reads in a RAID Level 5 varies inversely</i> to the number of disks, but that the optimal striping unit for writes varies with</i> the number of disks. Overall, we find that the optimal striping unit for workloads with an unspecified mix of reads and writes is independent</i> of the number of disks.Together, these trends lead us to recommend (in the absence of specific workload information) that the striping unit over a wide range of RAID Level 5 disk array sizes be equal to 1/2 * average positioning time * disk transfer rate.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223603},
 doi = {http://doi.acm.org/10.1145/223587.223603},
 acmid = {223603},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Worthington:1995:OES:223586.223604,
 author = {Worthington, Bruce L. and Ganger, Gregory R. and Patt, Yale N. and Wilkes, John},
 title = {On-line extraction of SCSI disk drive parameters},
 abstract = {Sophisticated disk scheduling algorithms require accurate, detailed disk drive specifications, including data about mechanical delays, on-board caching and prefetching algorithms, command and protocol overheads, and logical-to-physical block mappings. Comprehensive disk models used in storage subsystem design require similar levels of detail. We describe a suite of general-purpose algorithms and techniques for acquiring the necessary information from a SCSI disk drive. Using only the ANSI-standard interface, we demonstrate how the important parameter values of a modern SCSI drive can be determined accurately and efficiently.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {146--156},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223586.223604},
 doi = {http://doi.acm.org/10.1145/223586.223604},
 acmid = {223604},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Worthington:1995:OES:223587.223604,
 author = {Worthington, Bruce L. and Ganger, Gregory R. and Patt, Yale N. and Wilkes, John},
 title = {On-line extraction of SCSI disk drive parameters},
 abstract = {Sophisticated disk scheduling algorithms require accurate, detailed disk drive specifications, including data about mechanical delays, on-board caching and prefetching algorithms, command and protocol overheads, and logical-to-physical block mappings. Comprehensive disk models used in storage subsystem design require similar levels of detail. We describe a suite of general-purpose algorithms and techniques for acquiring the necessary information from a SCSI disk drive. Using only the ANSI-standard interface, we demonstrate how the important parameter values of a modern SCSI drive can be determined accurately and efficiently.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {146--156},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223587.223604},
 doi = {http://doi.acm.org/10.1145/223587.223604},
 acmid = {223604},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolf:1995:DDD:223586.223605,
 author = {Wolf, Joel L. and Yu, Philip S. and Shachnai, Hadas},
 title = {DASD dancing: a disk load balancing optimization scheme for video-on-demand computer systems},
 abstract = {For a video-on-demand computer system we propose a scheme which balances the load on the disks, thereby helping to solve a performance problem crucial to achieving maximal video throughput. Our load balancing scheme consists of two stages. The static stage determines good assignments of videos to groups of striped disks. The dynamic phase uses these assignments, and features a DASD dancing algorithm which performs real-time disk scheduling in an effective manner. Our scheme works synergistically with disk striping. We examine the performance of the DASD dancing algorithm via simulation experiments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {157--166},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223605},
 doi = {http://doi.acm.org/10.1145/223586.223605},
 acmid = {223605},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolf:1995:DDD:223587.223605,
 author = {Wolf, Joel L. and Yu, Philip S. and Shachnai, Hadas},
 title = {DASD dancing: a disk load balancing optimization scheme for video-on-demand computer systems},
 abstract = {For a video-on-demand computer system we propose a scheme which balances the load on the disks, thereby helping to solve a performance problem crucial to achieving maximal video throughput. Our load balancing scheme consists of two stages. The static stage determines good assignments of videos to groups of striped disks. The dynamic phase uses these assignments, and features a DASD dancing algorithm which performs real-time disk scheduling in an effective manner. Our scheme works synergistically with disk striping. We examine the performance of the DASD dancing algorithm via simulation experiments.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {157--166},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223605},
 doi = {http://doi.acm.org/10.1145/223587.223605},
 acmid = {223605},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sandhu:1995:ASD:223586.223606,
 author = {Sandhu, Harjinder S. and Sevcik, Kenneth C.},
 title = {An analytic study of dynamic hardware and software cache coherence strategies},
 abstract = {Dynamic software cache coherence strategies use information about program sharing behaviour to manage caches at run-time and at a granularity defined by the application. The program-level information is obtained through annotations placed into the application by the user or the compiler. The coherence protocols may range from simple static algorithms to dynamic algorithms that use run-time data structures similar to the directories used in hardware strategies. In this paper, we present an analytic study of five dynamic software cache coherence algorithms and compare these to a representative hardware coherence strategy. The analytic model is constructed using four input parameters --- write probability, locality, granularity, and system size --- and solved by analysis of a Markov chain. We show that the fundamental tradeoffs between the different hardware and software strategies are captured in this model. The results of the study show that hardware schemes perform better for fine-grained data structures for much of the parameter space that we study. However, for coarse-grained data structures, various software algorithms are dominant over most of the parameter space. Further, hardware strategies are found to be more susceptible to the effects of contention, and also perform worse for the asymmetric workload that we study.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {167--177},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223586.223606},
 doi = {http://doi.acm.org/10.1145/223586.223606},
 acmid = {223606},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sandhu:1995:ASD:223587.223606,
 author = {Sandhu, Harjinder S. and Sevcik, Kenneth C.},
 title = {An analytic study of dynamic hardware and software cache coherence strategies},
 abstract = {Dynamic software cache coherence strategies use information about program sharing behaviour to manage caches at run-time and at a granularity defined by the application. The program-level information is obtained through annotations placed into the application by the user or the compiler. The coherence protocols may range from simple static algorithms to dynamic algorithms that use run-time data structures similar to the directories used in hardware strategies. In this paper, we present an analytic study of five dynamic software cache coherence algorithms and compare these to a representative hardware coherence strategy. The analytic model is constructed using four input parameters --- write probability, locality, granularity, and system size --- and solved by analysis of a Markov chain. We show that the fundamental tradeoffs between the different hardware and software strategies are captured in this model. The results of the study show that hardware schemes perform better for fine-grained data structures for much of the parameter space that we study. However, for coarse-grained data structures, various software algorithms are dominant over most of the parameter space. Further, hardware strategies are found to be more susceptible to the effects of contention, and also perform worse for the asymmetric workload that we study.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {167--177},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223587.223606},
 doi = {http://doi.acm.org/10.1145/223587.223606},
 acmid = {223606},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brorsson:1995:STV:223587.223607,
 author = {Brorsson, Mats},
 title = {SM-prof: a tool to visualise and find cache coherence performance bottlenecks in multiprocessor programs},
 abstract = {Cache misses due to coherence actions are often the major source for performance degradation in cache coherent multiprocessors. It is often difficult for the programmer to take cache coherence into account when writing the program since the resulting access pattern is not apparent until the program is executed.SM-prof is a performance analysis tool that addresses this problem by visualising the shared data access pattern in a diagram with links to the source code lines causing performance degrading access patterns. The execution of a program is divided into time slots and each data block is classified based on the accesses made to the block during a time slot. This enables the programmer to follow the execution over time and it is possible to track the exact position responsible for accesses causing many cache misses related to coherence actions.Matrix multiplication and the MP3D application from SPLASH are used to illustrate the use of SM-prof. For MP3D, SM-prof revealed performance limitations that resulted in a performance improvement of over 75\%.The current implementation is based on program-driven simulation in order to achieve non-intrusive profiling. If a small perturbation of the program execution is acceptable, it is also possible to use software tracing techniques given that a data address can be related to the originating instruction.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223607},
 doi = {http://doi.acm.org/10.1145/223587.223607},
 acmid = {223607},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brorsson:1995:STV:223586.223607,
 author = {Brorsson, Mats},
 title = {SM-prof: a tool to visualise and find cache coherence performance bottlenecks in multiprocessor programs},
 abstract = {Cache misses due to coherence actions are often the major source for performance degradation in cache coherent multiprocessors. It is often difficult for the programmer to take cache coherence into account when writing the program since the resulting access pattern is not apparent until the program is executed.SM-prof is a performance analysis tool that addresses this problem by visualising the shared data access pattern in a diagram with links to the source code lines causing performance degrading access patterns. The execution of a program is divided into time slots and each data block is classified based on the accesses made to the block during a time slot. This enables the programmer to follow the execution over time and it is possible to track the exact position responsible for accesses causing many cache misses related to coherence actions.Matrix multiplication and the MP3D application from SPLASH are used to illustrate the use of SM-prof. For MP3D, SM-prof revealed performance limitations that resulted in a performance improvement of over 75\%.The current implementation is based on program-driven simulation in order to achieve non-intrusive profiling. If a small perturbation of the program execution is acceptable, it is also possible to use software tracing techniques given that a data address can be related to the originating instruction.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223607},
 doi = {http://doi.acm.org/10.1145/223586.223607},
 acmid = {223607},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cao:1995:SIP:223587.223608,
 author = {Cao, Pei and Felten, Edward W. and Karlin, Anna R. and Li, Kai},
 title = {A study of integrated prefetching and caching strategies},
 abstract = {Prefetching and caching are effective techniques for improving the performance of file systems, but they have not been studied in an integrated fashion. This paper proposes four properties that optimal integrated strategies for prefetching and caching must satisfy, and then presents and studies two such integrated strategies, called aggressive</i> and conservative.</i> We prove that the performance of the conservative</i> approach is within a factor of two of optimal and that the performance of the aggressive</i> strategy is a factor significantly less than twice that of the optimal case. We have evaluated these two approaches by trace-driven simulation with a collection of file access traces. Our results show that the two integrated prefetching and caching strategies are indeed close to optimal and that these strategies can reduce the running time of applications by up to 50\%.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {188--197},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223608},
 doi = {http://doi.acm.org/10.1145/223587.223608},
 acmid = {223608},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cao:1995:SIP:223586.223608,
 author = {Cao, Pei and Felten, Edward W. and Karlin, Anna R. and Li, Kai},
 title = {A study of integrated prefetching and caching strategies},
 abstract = {Prefetching and caching are effective techniques for improving the performance of file systems, but they have not been studied in an integrated fashion. This paper proposes four properties that optimal integrated strategies for prefetching and caching must satisfy, and then presents and studies two such integrated strategies, called aggressive</i> and conservative.</i> We prove that the performance of the conservative</i> approach is within a factor of two of optimal and that the performance of the aggressive</i> strategy is a factor significantly less than twice that of the optimal case. We have evaluated these two approaches by trace-driven simulation with a collection of file access traces. Our results show that the two integrated prefetching and caching strategies are indeed close to optimal and that these strategies can reduce the running time of applications by up to 50\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {188--197},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223608},
 doi = {http://doi.acm.org/10.1145/223586.223608},
 acmid = {223608},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sivasubramaniam:1995:CBR:223586.223609,
 author = {Sivasubramaniam, Anand and Singla, Aman and Ramachandran, Umakishore and Venkateswaran, H.},
 title = {On characterizing bandwidth requirements of parallel applications},
 abstract = {Synthesizing architectural requirements from an application viewpoint can help in making important architectural design decisions towards building large scale parallel machines. In this paper, we quantify the link bandwidth requirement on a binary hypercube topology for a set of five parallel applications. We use an execution-driven simulator called SPASM to collect data points for system sizes that are feasible to be simulated. These data points are then used in a regression analysis for projecting the link bandwidth requirements for larger systems. The requirements are projected as a function of the following system parameters: number of processors, CPU clock speed, and problem size. These results are also used to project the link bandwidths for other network topologies. Our study quantifies the link bandwidth that has to be made available to limit the network overhead in an application to a specified tolerance level. The results show that typical link bandwidths (200-300 MBytes/sec) found in current commercial parallel architectures (such as Intel Paragon and Cray T3D) would have fairly low network overhead for the applications considered in this study. For two of the applications, this overhead is negligible. For the other applications, this overhead can be limited to about 30\% of the execution time provided the problem sizes are increased commensurate with the processor clock speed. The technique presented can be useful to a system architect to synthesize the bandwidth requirements for realizing well-balanced parallel architectures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {198--207},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223609},
 doi = {http://doi.acm.org/10.1145/223586.223609},
 acmid = {223609},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sivasubramaniam:1995:CBR:223587.223609,
 author = {Sivasubramaniam, Anand and Singla, Aman and Ramachandran, Umakishore and Venkateswaran, H.},
 title = {On characterizing bandwidth requirements of parallel applications},
 abstract = {Synthesizing architectural requirements from an application viewpoint can help in making important architectural design decisions towards building large scale parallel machines. In this paper, we quantify the link bandwidth requirement on a binary hypercube topology for a set of five parallel applications. We use an execution-driven simulator called SPASM to collect data points for system sizes that are feasible to be simulated. These data points are then used in a regression analysis for projecting the link bandwidth requirements for larger systems. The requirements are projected as a function of the following system parameters: number of processors, CPU clock speed, and problem size. These results are also used to project the link bandwidths for other network topologies. Our study quantifies the link bandwidth that has to be made available to limit the network overhead in an application to a specified tolerance level. The results show that typical link bandwidths (200-300 MBytes/sec) found in current commercial parallel architectures (such as Intel Paragon and Cray T3D) would have fairly low network overhead for the applications considered in this study. For two of the applications, this overhead is negligible. For the other applications, this overhead can be limited to about 30\% of the execution time provided the problem sizes are increased commensurate with the processor clock speed. The technique presented can be useful to a system architect to synthesize the bandwidth requirements for realizing well-balanced parallel architectures.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {198--207},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223609},
 doi = {http://doi.acm.org/10.1145/223587.223609},
 acmid = {223609},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McCann:1995:SMC:223586.223610,
 author = {McCann, Cathy and Zahorjan, John},
 title = {Scheduling memory constrained jobs on distributed memory parallel computers},
 abstract = {We consider the problem of multiprocessor scheduling of jobs whose memory requirements place lower bounds on the fraction of the machine required in order to execute. We address three primary questions in this work:1. How can a parallel machine be multiprogrammed with minimal overhead when jobs have minimum memory requirements?2. To what extent does the inability of an application to repartition its workload during runtime affect the choice of processor allocation policy?3. How rigid should the system be in attempting to provide equal resource allocation to each runnable job in order to minimize average response time?This work is applicable both to parallel machines and to networks of workstations supporting parallel applications.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {208--219},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223610},
 doi = {http://doi.acm.org/10.1145/223586.223610},
 acmid = {223610},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McCann:1995:SMC:223587.223610,
 author = {McCann, Cathy and Zahorjan, John},
 title = {Scheduling memory constrained jobs on distributed memory parallel computers},
 abstract = {We consider the problem of multiprocessor scheduling of jobs whose memory requirements place lower bounds on the fraction of the machine required in order to execute. We address three primary questions in this work:1. How can a parallel machine be multiprogrammed with minimal overhead when jobs have minimum memory requirements?2. To what extent does the inability of an application to repartition its workload during runtime affect the choice of processor allocation policy?3. How rigid should the system be in attempting to provide equal resource allocation to each runnable job in order to minimize average response time?This work is applicable both to parallel machines and to networks of workstations supporting parallel applications.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {208--219},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223610},
 doi = {http://doi.acm.org/10.1145/223587.223610},
 acmid = {223610},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lebeck:1995:AMN:223586.223611,
 author = {Lebeck, Alvin R. and Wood, David A.},
 title = {Active memory: a new abstraction for memory-system simulation},
 abstract = {This paper describes the active memory</i> abstraction for memory-system simulation. In this abstraction---designed specifically for on-the-fly simulation, memory references logically invoke a user-specified function depending upon the reference's type and accessed memory block state. Active memory allows simulator writers to specify the appropriate action on each reference, including "no action" for the common case of cache hits. Because the abstraction hides implementation details, implementations can be carefully tuned for particular platforms, permitting much more efficient on-the-fly simulation than the traditional trace-driven abstraction.Our SPARC implementation, Fast-Cache,</i> executes simple data cache simulations two or three times faster than a highly-tuned trace-driven simulator and only 2 to 7 times slower than the original program. Fast-Cache implements active memory by performing a fast table look up of the memory block state, taking as few as 3 cycles on a SuperSPARC for the no-action case. Modeling the effects of Fast-Cache's additional lookup instructions qualitatively shows that Fast-Cache is likely to be the most efficient simulator for miss ratios between 3\% and 40\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {220--230},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223586.223611},
 doi = {http://doi.acm.org/10.1145/223586.223611},
 acmid = {223611},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lebeck:1995:AMN:223587.223611,
 author = {Lebeck, Alvin R. and Wood, David A.},
 title = {Active memory: a new abstraction for memory-system simulation},
 abstract = {This paper describes the active memory</i> abstraction for memory-system simulation. In this abstraction---designed specifically for on-the-fly simulation, memory references logically invoke a user-specified function depending upon the reference's type and accessed memory block state. Active memory allows simulator writers to specify the appropriate action on each reference, including "no action" for the common case of cache hits. Because the abstraction hides implementation details, implementations can be carefully tuned for particular platforms, permitting much more efficient on-the-fly simulation than the traditional trace-driven abstraction.Our SPARC implementation, Fast-Cache,</i> executes simple data cache simulations two or three times faster than a highly-tuned trace-driven simulator and only 2 to 7 times slower than the original program. Fast-Cache implements active memory by performing a fast table look up of the memory block state, taking as few as 3 cycles on a SuperSPARC for the no-action case. Modeling the effects of Fast-Cache's additional lookup instructions qualitatively shows that Fast-Cache is likely to be the most efficient simulator for miss ratios between 3\% and 40\%.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {220--230},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/223587.223611},
 doi = {http://doi.acm.org/10.1145/223587.223611},
 acmid = {223611},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{de Souza e Silva:1995:CTD:223586.223612,
 author = {de Souza e Silva, Edmundo and Gail, H. Richard and Campos, Reinaldo Vallejos},
 title = {Calculating transient distributions of cumulative reward},
 abstract = {Markov reward models have been employed to obtain performability measures of computer and communication systems. In these models, a continuous time Markov chain is used to represent changes in the system structure, usually caused by faults and repairs of its components, and reward rates are assigned to states of the model to indicate some measure of accomplishment at each structure. A procedure to calculate numerically the distribution of the reward accumulated over a finite observation period is presented. The development is based solely on probabilistic arguments, and the final recursion is quite simple. The algorithm has a low computational cost in terms of model parameters. In fact, the number of operations is linear in a parameter that is smaller than the number of rewards, while the storage required is independent of the number of rewards. We also consider the calculation of the distribution of cumulative reward for models in which impulse based rewards are associated with transitions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {231--240},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223612},
 doi = {http://doi.acm.org/10.1145/223586.223612},
 acmid = {223612},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{de Souza e Silva:1995:CTD:223587.223612,
 author = {de Souza e Silva, Edmundo and Gail, H. Richard and Campos, Reinaldo Vallejos},
 title = {Calculating transient distributions of cumulative reward},
 abstract = {Markov reward models have been employed to obtain performability measures of computer and communication systems. In these models, a continuous time Markov chain is used to represent changes in the system structure, usually caused by faults and repairs of its components, and reward rates are assigned to states of the model to indicate some measure of accomplishment at each structure. A procedure to calculate numerically the distribution of the reward accumulated over a finite observation period is presented. The development is based solely on probabilistic arguments, and the final recursion is quite simple. The algorithm has a low computational cost in terms of model parameters. In fact, the number of operations is linear in a parameter that is smaller than the number of rewards, while the storage required is independent of the number of rewards. We also consider the calculation of the distribution of cumulative reward for models in which impulse based rewards are associated with transitions.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {231--240},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223612},
 doi = {http://doi.acm.org/10.1145/223587.223612},
 acmid = {223612},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carrasco:1995:RRT:223586.223613,
 author = {Carrasco, Juan A. and Calder\'{o}n, Angel},
 title = {Regenerative randomization: theory and application examples},
 abstract = {Randomization is a popular method for the transient solution of continuous-time Markov models. Its primary advantages over other methods (i.e., ODE solvers) are robustness and ease of implementation. It is however well-known that the performance of the method deteriorates with the "stiffness" of the model: the number of required steps to solve the model up to time t</i> tends to \&amp;Lambda;t</i> for \&amp;Lambda;t</i> \&amp;rarr; \&amp;infin;. In this paper we present a new method called regenerative randomization and apply it to the computation of two transient measures for rewarded irreducible Markov models. Regarding the number of steps required in regenerative randomization we prove that: 1) it is smaller than the number of steps required in standard randomization when the initial distribution is concentrated in a single state, 2) for \&amp;Lambda;t</i> \&amp;rarr; \&amp;infin;, it is upper bounded by a function O</i>(log(\&amp;Lambda;t</i>/\&amp;epsilon;)), where \&amp;epsilon; is the desired relative approximation error bound. Using dependability and performability examples we analyze the performance of the method.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223613},
 doi = {http://doi.acm.org/10.1145/223586.223613},
 acmid = {223613},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Carrasco:1995:RRT:223587.223613,
 author = {Carrasco, Juan A. and Calder\'{o}n, Angel},
 title = {Regenerative randomization: theory and application examples},
 abstract = {Randomization is a popular method for the transient solution of continuous-time Markov models. Its primary advantages over other methods (i.e., ODE solvers) are robustness and ease of implementation. It is however well-known that the performance of the method deteriorates with the "stiffness" of the model: the number of required steps to solve the model up to time t</i> tends to \&amp;Lambda;t</i> for \&amp;Lambda;t</i> \&amp;rarr; \&amp;infin;. In this paper we present a new method called regenerative randomization and apply it to the computation of two transient measures for rewarded irreducible Markov models. Regarding the number of steps required in regenerative randomization we prove that: 1) it is smaller than the number of steps required in standard randomization when the initial distribution is concentrated in a single state, 2) for \&amp;Lambda;t</i> \&amp;rarr; \&amp;infin;, it is upper bounded by a function O</i>(log(\&amp;Lambda;t</i>/\&amp;epsilon;)), where \&amp;epsilon; is the desired relative approximation error bound. Using dependability and performability examples we analyze the performance of the method.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223613},
 doi = {http://doi.acm.org/10.1145/223587.223613},
 acmid = {223613},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Greenberg:1995:CTA:223587.223614,
 author = {Greenberg, Albert G. and Srikant, R.},
 title = {Computational techniques for accurate performance evaluation of multirate, multihop communication networks},
 abstract = {Computational techniques are presented for connection-level performance evaluation of communication networks, with stochastic multirate traffic, state dependent admission control, alternate routing, and general topology --- all characteristics of emerging integrated service networks. The techniques involve solutions of systems of fixed point equations, which estimate equilibrium network behavior. Though similar techniques have been applied with success to single-rate fully connected networks, the curse of dimensionality arises when the techniques are extended to multirate, multihop networks, and the cost of solving the fixed point equations exactly is exponential. This exponential barrier is skirted by exploiting, in particular, a close relationship with the network reliability problem, and by borrowing effective heuristics from the reliability domain. A series of experiments are reported on, comparing the estimates from the new techniques to the results of discrete event simulations.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {253--260},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/223587.223614},
 doi = {http://doi.acm.org/10.1145/223587.223614},
 acmid = {223614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Greenberg:1995:CTA:223586.223614,
 author = {Greenberg, Albert G. and Srikant, R.},
 title = {Computational techniques for accurate performance evaluation of multirate, multihop communication networks},
 abstract = {Computational techniques are presented for connection-level performance evaluation of communication networks, with stochastic multirate traffic, state dependent admission control, alternate routing, and general topology --- all characteristics of emerging integrated service networks. The techniques involve solutions of systems of fixed point equations, which estimate equilibrium network behavior. Though similar techniques have been applied with success to single-rate fully connected networks, the curse of dimensionality arises when the techniques are extended to multirate, multihop networks, and the cost of solving the fixed point equations exactly is exponential. This exponential barrier is skirted by exploiting, in particular, a close relationship with the network reliability problem, and by borrowing effective heuristics from the reliability domain. A series of experiments are reported on, comparing the estimates from the new techniques to the results of discrete event simulations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {253--260},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/223586.223614},
 doi = {http://doi.acm.org/10.1145/223586.223614},
 acmid = {223614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ott:1995:IET:223586.223615,
 note = {Chairman-Ott, Teun},
 title = {The Internet in evolution, and TCP over ATM (panel session)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {261--262},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223615},
 doi = {http://doi.acm.org/10.1145/223586.223615},
 acmid = {223615},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ott:1995:IET:223587.223615,
 note = {Chairman-Ott, Teun},
 title = {The Internet in evolution, and TCP over ATM (panel session)},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {261--262},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223615},
 doi = {http://doi.acm.org/10.1145/223587.223615},
 acmid = {223615},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Trivedi:1995:NPN:223586.223616,
 author = {Trivedi, Kishor S. and Bobbio, A. and Ciardo, G. and German, R. and Puliafito, A. and Telek, M.},
 title = {Non-Markovian Petri nets},
 abstract = {Non-Markovian models allow us to capture a very wide range of circumstances in which it is necessary to model phenomena whose times to occurrence is not exponentially distributed. Events such as timeouts in a protocol, service times at a machine performing the same task on each part, and memory access or instruction execution in a low-level h/w or s/w model, have durations which are constant or with a very low variance. Phase-type distributions can be used to approximate a non-exponential, but they increase the size of the state space.The analysis of stochastic systems with non-exponential timing is of increasing interest in the literature and requires the development of suitable modeling tools. Recently, some effort has been devoted to generalize the concept of Stochastic Petri Nets</i> (SPN), by allowing the firing times to be generally distributed.A particular case of non-Markovian SPN,</i> is the class of Deterministic and SPN (DSPN)</i> [1]. A DSPN</i> is a non-Markovian SPN</i> where, in each marking, at most one transition is allowed to have a deterministic firing time with enabling memory policy.A new class of stochastic Petri nets has recently been defined [2, 3] by generalizing the deterministic firing times of the DSPN to generally distributed firing times. The underlying stochastic process for these classes of Petri nets is a Markov Regenerative Process</i> (MRGP). This observation has opened a very fertile line of research aimed at the definition of solvable classes of models whose underlying marking process is an MRGP, and therefore referred to as Markov Regenerative Stochastic Petri Nets (MRSPN).</i>Some of the results in this filed will be described in the session. In particular, Ciardo investigates stochastic confusion by defining the selection probability for transitions attempting to fire at the same time. German introduces the "method of supplementary variables" for the derivation of state equations describing the transient behavior of the marking process. Puliafito describes how, under some constraints, concurrent enabling of several generally distributed timed transitions is allowed. Bobbio and Telek discuss how age memory policy can be included to capture preemptive mechanisms of the resume (prs)</i> type.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {263--264},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223616},
 doi = {http://doi.acm.org/10.1145/223586.223616},
 acmid = {223616},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Trivedi:1995:NPN:223587.223616,
 author = {Trivedi, Kishor S. and Bobbio, A. and Ciardo, G. and German, R. and Puliafito, A. and Telek, M.},
 title = {Non-Markovian Petri nets},
 abstract = {Non-Markovian models allow us to capture a very wide range of circumstances in which it is necessary to model phenomena whose times to occurrence is not exponentially distributed. Events such as timeouts in a protocol, service times at a machine performing the same task on each part, and memory access or instruction execution in a low-level h/w or s/w model, have durations which are constant or with a very low variance. Phase-type distributions can be used to approximate a non-exponential, but they increase the size of the state space.The analysis of stochastic systems with non-exponential timing is of increasing interest in the literature and requires the development of suitable modeling tools. Recently, some effort has been devoted to generalize the concept of Stochastic Petri Nets</i> (SPN), by allowing the firing times to be generally distributed.A particular case of non-Markovian SPN,</i> is the class of Deterministic and SPN (DSPN)</i> [1]. A DSPN</i> is a non-Markovian SPN</i> where, in each marking, at most one transition is allowed to have a deterministic firing time with enabling memory policy.A new class of stochastic Petri nets has recently been defined [2, 3] by generalizing the deterministic firing times of the DSPN to generally distributed firing times. The underlying stochastic process for these classes of Petri nets is a Markov Regenerative Process</i> (MRGP). This observation has opened a very fertile line of research aimed at the definition of solvable classes of models whose underlying marking process is an MRGP, and therefore referred to as Markov Regenerative Stochastic Petri Nets (MRSPN).</i>Some of the results in this filed will be described in the session. In particular, Ciardo investigates stochastic confusion by defining the selection probability for transitions attempting to fire at the same time. German introduces the "method of supplementary variables" for the derivation of state equations describing the transient behavior of the marking process. Puliafito describes how, under some constraints, concurrent enabling of several generally distributed timed transitions is allowed. Bobbio and Telek discuss how age memory policy can be included to capture preemptive mechanisms of the resume (prs)</i> type.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {263--264},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223616},
 doi = {http://doi.acm.org/10.1145/223587.223616},
 acmid = {223616},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Erramilli:1995:PIS:223586.223617,
 author = {Erramilli, Ashok},
 title = {Performance impacts of self-similarity in traffic},
 abstract = {Recent measurement studies in Bellcore and elsewhere have convincingly established the presence of statistical self similarity in high-speed network traffic. What is less clear --- and as such the subject of intense current research --- is the impact of the self-similarity on network performance. Given that traditional queueing models of network performance do not model self-similarity, the validity of traditional models to predict network performance would be supported if it is shown that self-similarity does not have measurable impacts on performance. On the other hand, if the converse of this assertion were true, it would have significant impacts on the way networks are designed and analyzed, as well as open up new areas of research in mathematical modeling, queueing analysis, network design and control. The issues addressed in this session are therefore of fundamental importance in high-speed network research.Given that queueing behavior is dominated by traffic characteristics over the time scales of busy periods, it has been argued that phenomena that span many time scales, such as self-similarity, should not be relevant for queueing performance. However, the paper by Narayan, Erramilli and Willinger presents evidence that for data traffic, the long range dependence (which is related to the self-similarity in traffic) can dominate queueing behavior under a variety of conditions. Specifically, it is shown based on a series of carefully designed simulation experiments with actual traffic traces, that the queueing behavior with actual traces is considerably heavier than that predicted by traditional theory, and that these differences are attributable to long range dependence. The paper by Heyman and Lakshman investigates modeling of video traffic to predict cell loss performance with finite buffer systems, and they conclude that long-range dependence is not a crucial property in determining the finite buffer behavior of video conferences. In particular, a Markov chain model that does not model long-range dependence is nevertheless able to reproduce various operating characteristics over a wide range of loadings obtained with the actual video trace. Mukherjee, Adas, Klivansky and Song investigate the performance impacts of short-range and long-range correlation components using simulations with a fractional ARIMA model. They also discuss a strategy to provide quality of service guarantees with long range dependent traffic, as well as recent results on NSFNET traffic. Finally, the paper by Li describes a frequency-domain based analytical tool that matches a special class of Markov chains with traces exhibiting a variety of characteristics, including long-range dependence. Good agreement is reported between analytical queueing solutions of the matched Markov chains, and simulation results obtained video and data traffic traces.This session therefore brings together a wide range of viewpoints on this issue. Resolution of such seemingly conflicting conclusions lies in the fact that in performance analysis, answers sensitively depend on the specific details of a problem. Thus the proper question to ask is not whether or not self-similarity matters in queueing; but under what conditions it matters. Likewise, the question to ask is not whether a class of models is invalid; but to identify the conditions under which traditional Markov or self-similar traffic models are expected to be valid. Finally, given an understanding of statistical features that are relevant to a given problem, the challenge is to model these accurately and parsimoniously so that the model is useful in practical performance analysis. The work outlined in the abstracts below adds significantly to our understanding of these issues.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {265--266},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223617},
 doi = {http://doi.acm.org/10.1145/223586.223617},
 acmid = {223617},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Erramilli:1995:PIS:223587.223617,
 author = {Erramilli, Ashok},
 title = {Performance impacts of self-similarity in traffic},
 abstract = {Recent measurement studies in Bellcore and elsewhere have convincingly established the presence of statistical self similarity in high-speed network traffic. What is less clear --- and as such the subject of intense current research --- is the impact of the self-similarity on network performance. Given that traditional queueing models of network performance do not model self-similarity, the validity of traditional models to predict network performance would be supported if it is shown that self-similarity does not have measurable impacts on performance. On the other hand, if the converse of this assertion were true, it would have significant impacts on the way networks are designed and analyzed, as well as open up new areas of research in mathematical modeling, queueing analysis, network design and control. The issues addressed in this session are therefore of fundamental importance in high-speed network research.Given that queueing behavior is dominated by traffic characteristics over the time scales of busy periods, it has been argued that phenomena that span many time scales, such as self-similarity, should not be relevant for queueing performance. However, the paper by Narayan, Erramilli and Willinger presents evidence that for data traffic, the long range dependence (which is related to the self-similarity in traffic) can dominate queueing behavior under a variety of conditions. Specifically, it is shown based on a series of carefully designed simulation experiments with actual traffic traces, that the queueing behavior with actual traces is considerably heavier than that predicted by traditional theory, and that these differences are attributable to long range dependence. The paper by Heyman and Lakshman investigates modeling of video traffic to predict cell loss performance with finite buffer systems, and they conclude that long-range dependence is not a crucial property in determining the finite buffer behavior of video conferences. In particular, a Markov chain model that does not model long-range dependence is nevertheless able to reproduce various operating characteristics over a wide range of loadings obtained with the actual video trace. Mukherjee, Adas, Klivansky and Song investigate the performance impacts of short-range and long-range correlation components using simulations with a fractional ARIMA model. They also discuss a strategy to provide quality of service guarantees with long range dependent traffic, as well as recent results on NSFNET traffic. Finally, the paper by Li describes a frequency-domain based analytical tool that matches a special class of Markov chains with traces exhibiting a variety of characteristics, including long-range dependence. Good agreement is reported between analytical queueing solutions of the matched Markov chains, and simulation results obtained video and data traffic traces.This session therefore brings together a wide range of viewpoints on this issue. Resolution of such seemingly conflicting conclusions lies in the fact that in performance analysis, answers sensitively depend on the specific details of a problem. Thus the proper question to ask is not whether or not self-similarity matters in queueing; but under what conditions it matters. Likewise, the question to ask is not whether a class of models is invalid; but to identify the conditions under which traditional Markov or self-similar traffic models are expected to be valid. Finally, given an understanding of statistical features that are relevant to a given problem, the challenge is to model these accurately and parsimoniously so that the model is useful in practical performance analysis. The work outlined in the abstracts below adds significantly to our understanding of these issues.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {265--266},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223617},
 doi = {http://doi.acm.org/10.1145/223587.223617},
 acmid = {223617},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arpaci:1995:IPS:223587.223618,
 author = {Arpaci, Remzi H. and Dusseau, Andrea C. and Vahdat, Amin M. and Liu, Lok T. and Anderson, Thomas E. and Patterson, David A.},
 title = {The interaction of parallel and sequential workloads on a network of workstations},
 abstract = {This paper examines the plausibility of using a network of workstations (NOW) for a mixture of parallel and sequential jobs. Through simulations, our study examines issues that arise when combining these two workloads on a single platform. Starting from a dedicated NOW just for parallel programs, we incrementally relax uniprogramming restrictions until we have a multi-programmed, multi-user NOW for both interactive sequential users and parallel programs. We show that a number of issues associated with the distributed NOW environment (e.g., daemon activity, coscheduling skew) can have a small but noticeable effect on parallel program performance. We also find that efficient migration to idle workstations is necessary to maintain acceptable parallel application performance. Furthermore, we present a methodology for deriving an optimal delay time for recruiting idle machines for use by parallel programs; this recruitment threshold</i> was just 3 minutes for the research cluster we measured. Finally, we quantify the effects of the additional parallel load upon interactive users by keeping track of the potential number of user delays</i> in our simulations. When we limit the maximum number of delays per user, we can still maintain acceptable parallel program performance. In summary, we find that for our workloads a 2:1 rule applies: a NOW cluster of approximately 60 machines can sustain a 32-node parallel workload in addition to the sequential load placed upon it by interactive users.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {267--278},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223618},
 doi = {http://doi.acm.org/10.1145/223587.223618},
 acmid = {223618},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arpaci:1995:IPS:223586.223618,
 author = {Arpaci, Remzi H. and Dusseau, Andrea C. and Vahdat, Amin M. and Liu, Lok T. and Anderson, Thomas E. and Patterson, David A.},
 title = {The interaction of parallel and sequential workloads on a network of workstations},
 abstract = {This paper examines the plausibility of using a network of workstations (NOW) for a mixture of parallel and sequential jobs. Through simulations, our study examines issues that arise when combining these two workloads on a single platform. Starting from a dedicated NOW just for parallel programs, we incrementally relax uniprogramming restrictions until we have a multi-programmed, multi-user NOW for both interactive sequential users and parallel programs. We show that a number of issues associated with the distributed NOW environment (e.g., daemon activity, coscheduling skew) can have a small but noticeable effect on parallel program performance. We also find that efficient migration to idle workstations is necessary to maintain acceptable parallel application performance. Furthermore, we present a methodology for deriving an optimal delay time for recruiting idle machines for use by parallel programs; this recruitment threshold</i> was just 3 minutes for the research cluster we measured. Finally, we quantify the effects of the additional parallel load upon interactive users by keeping track of the potential number of user delays</i> in our simulations. When we limit the maximum number of delays per user, we can still maintain acceptable parallel program performance. In summary, we find that for our workloads a 2:1 rule applies: a NOW cluster of approximately 60 machines can sustain a 32-node parallel workload in addition to the sequential load placed upon it by interactive users.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {267--278},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223618},
 doi = {http://doi.acm.org/10.1145/223586.223618},
 acmid = {223618},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Myllymaki:1995:DJS:223587.223619,
 author = {Myllymaki, Jussi and Livny, Miron},
 title = {Disk-tape joins: synchronizing disk and tape access},
 abstract = {Today large amounts of data are stored on tertiary storage media such as magnetic tapes and optical disks. DBMSs typically operate only on magnetic disks since they know how to maneuver disks and how to optimize accesses on them. Tertiary devices present a problem for DBMSs since these devices have dismountable media and have very different operational characteristics compared to magnetic disks. For instance, most tape drives offer very high capacity at low cost but are accessed sequentially, involve lengthy latencies, and deliver lower bandwidth. Typically, the scope of a DBMS's query optimizer does not include tertiary devices, and the DBMS might not even know how to control and operate upon tertiary-resident data. In a three-level hierarchy of storage devices (main memory, disk, tape), the typical solution is to elevate tape-resident data to disk devices, thus bringing such data into the DBMS' control, and then to perform the required operations on disk. This requires additional space on disk and may not give the lowest response time possible. With this challenge in mind, we studied the trade-offs between memory and disk requirements and the execution time of a join with the help of two well-known join methods. The conventional, disk-based Nested Block Join and Hybrid Hash Join were modified to operate directly on tapes. An experimental implementation of the modified algorithms gave us more insight into how the algorithms perform in practice. Our performance analysis shows that a DBMS desiring to operate on tertiary storage will benefit from special algorithms that operate directly on tape-resident data and take into account and exploit the mismatch in disk and tape characteristics.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223587.223619},
 doi = {http://doi.acm.org/10.1145/223587.223619},
 acmid = {223619},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent I/O, join methods, tertiary storage},
} 

@article{Myllymaki:1995:DJS:223586.223619,
 author = {Myllymaki, Jussi and Livny, Miron},
 title = {Disk-tape joins: synchronizing disk and tape access},
 abstract = {Today large amounts of data are stored on tertiary storage media such as magnetic tapes and optical disks. DBMSs typically operate only on magnetic disks since they know how to maneuver disks and how to optimize accesses on them. Tertiary devices present a problem for DBMSs since these devices have dismountable media and have very different operational characteristics compared to magnetic disks. For instance, most tape drives offer very high capacity at low cost but are accessed sequentially, involve lengthy latencies, and deliver lower bandwidth. Typically, the scope of a DBMS's query optimizer does not include tertiary devices, and the DBMS might not even know how to control and operate upon tertiary-resident data. In a three-level hierarchy of storage devices (main memory, disk, tape), the typical solution is to elevate tape-resident data to disk devices, thus bringing such data into the DBMS' control, and then to perform the required operations on disk. This requires additional space on disk and may not give the lowest response time possible. With this challenge in mind, we studied the trade-offs between memory and disk requirements and the execution time of a join with the help of two well-known join methods. The conventional, disk-based Nested Block Join and Hybrid Hash Join were modified to operate directly on tapes. An experimental implementation of the modified algorithms gave us more insight into how the algorithms perform in practice. Our performance analysis shows that a DBMS desiring to operate on tertiary storage will benefit from special algorithms that operate directly on tape-resident data and take into account and exploit the mismatch in disk and tape characteristics.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/223586.223619},
 doi = {http://doi.acm.org/10.1145/223586.223619},
 acmid = {223619},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent I/O, join methods, tertiary storage},
} 

@inproceedings{Phalke:1995:IGM:223587.223620,
 author = {Phalke, Vidyadhar and Gopinath, Bhaskarpillai},
 title = {An inter-reference gap model for temporal locality in program behavior},
 abstract = {The property of locality in program behavior has been studied and modelled extensively because of its application to memory design, code optimization, multiprogramming etc. We propose a k</i> order Markov chain based scheme to model the sequence of time intervals between successive references to the same address in memory during program execution. Each unique address in a program is modelled separately. To validate our model, which we call the Inter-Reference Gap (IRG) model, we show substantial improvements in three different areas where it is applied. (1) We improve upon the miss ratio for the Least Recently Used (LRU) memory replacement algorithm by up to 37\%. (2) We achieve up to 22\% space-time product improvement over the Working Set (WS) algorithm for dynamic memory management. (3) A new trace compression technique is proposed which compresses up to 2.5\% with zero error in WS simulations and up to 3.7\% error in the LRU simulations. All these results are obtained experimentally, via trace driven simulations over a wide range of cache traces, page reference traces, object traces and database traces.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223587.223620},
 doi = {http://doi.acm.org/10.1145/223587.223620},
 acmid = {223620},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chains, dynamic memory management, locality of reference, memory replacement, prediction, trace compaction, trace driven simulation},
} 

@article{Phalke:1995:IGM:223586.223620,
 author = {Phalke, Vidyadhar and Gopinath, Bhaskarpillai},
 title = {An inter-reference gap model for temporal locality in program behavior},
 abstract = {The property of locality in program behavior has been studied and modelled extensively because of its application to memory design, code optimization, multiprogramming etc. We propose a k</i> order Markov chain based scheme to model the sequence of time intervals between successive references to the same address in memory during program execution. Each unique address in a program is modelled separately. To validate our model, which we call the Inter-Reference Gap (IRG) model, we show substantial improvements in three different areas where it is applied. (1) We improve upon the miss ratio for the Least Recently Used (LRU) memory replacement algorithm by up to 37\%. (2) We achieve up to 22\% space-time product improvement over the Working Set (WS) algorithm for dynamic memory management. (3) A new trace compression technique is proposed which compresses up to 2.5\% with zero error in WS simulations and up to 3.7\% error in the LRU simulations. All these results are obtained experimentally, via trace driven simulations over a wide range of cache traces, page reference traces, object traces and database traces.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {291--300},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/223586.223620},
 doi = {http://doi.acm.org/10.1145/223586.223620},
 acmid = {223620},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov chains, dynamic memory management, locality of reference, memory replacement, prediction, trace compaction, trace driven simulation},
} 

@article{Braams:1995:BCP:223586.223621,
 author = {Braams, Jan},
 title = {Batch class process scheduler for Unix SVR4},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {301--302},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223621},
 doi = {http://doi.acm.org/10.1145/223586.223621},
 acmid = {223621},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Braams:1995:BCP:223587.223621,
 author = {Braams, Jan},
 title = {Batch class process scheduler for Unix SVR4},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {301--302},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223621},
 doi = {http://doi.acm.org/10.1145/223587.223621},
 acmid = {223621},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Donatelli:1995:SSR:223587.223622,
 author = {Donatelli, S. and Franceschinis, G.},
 title = {State space reductions using stochastic well-formed net simplifications: an application to random polling systems},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {303--304},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223622},
 doi = {http://doi.acm.org/10.1145/223587.223622},
 acmid = {223622},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Donatelli:1995:SSR:223586.223622,
 author = {Donatelli, S. and Franceschinis, G.},
 title = {State space reductions using stochastic well-formed net simplifications: an application to random polling systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {303--304},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223622},
 doi = {http://doi.acm.org/10.1145/223586.223622},
 acmid = {223622},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Balsamo:1995:ART:223586.223623,
 author = {Balsamo, S. and Mura, I.},
 title = {Approximate response time distribution in Fork and Join systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {305--306},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223623},
 doi = {http://doi.acm.org/10.1145/223586.223623},
 acmid = {223623},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Balsamo:1995:ART:223587.223623,
 author = {Balsamo, S. and Mura, I.},
 title = {Approximate response time distribution in Fork and Join systems},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {305--306},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223623},
 doi = {http://doi.acm.org/10.1145/223587.223623},
 acmid = {223623},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhang:1995:SAS:223586.223624,
 author = {Zhang, Xiaodong and Xu, Zhichen},
 title = {A semi-empirical approach to scalability study},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {307--308},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223624},
 doi = {http://doi.acm.org/10.1145/223586.223624},
 acmid = {223624},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhang:1995:SAS:223587.223624,
 author = {Zhang, Xiaodong and Xu, Zhichen},
 title = {A semi-empirical approach to scalability study},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {307--308},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223624},
 doi = {http://doi.acm.org/10.1145/223587.223624},
 acmid = {223624},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hughes:1995:PFP:223587.223625,
 author = {Hughes, Eric and Winslett, Marianne},
 title = {PEDCAD: a framework for performance evaluation of object database applications},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {309--310},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223625},
 doi = {http://doi.acm.org/10.1145/223587.223625},
 acmid = {223625},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hughes:1995:PFP:223586.223625,
 author = {Hughes, Eric and Winslett, Marianne},
 title = {PEDCAD: a framework for performance evaluation of object database applications},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {309--310},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223625},
 doi = {http://doi.acm.org/10.1145/223586.223625},
 acmid = {223625},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Salehi:1995:SCA:223587.223626,
 author = {Salehi, James D. and Kurose, James F. and Towsley, Don},
 title = {Scheduling for cache affinity in parallelized communication protocols},
 abstract = {We explore processor-cache affinity scheduling of parallel network protocol processing in a setting in which protocol processing executes on a shared-memory multiprocessor concurrently with a general workload of non-protocol activity. We find that affinity scheduling can significantly reduce the communication delay associated with protocol processing, enabling the host to support a greater number of concurrent streams and to provide a higher maximum throughput to individual streams. In addition, we compare implementations of two parallelization approaches (Locking</i> and Independent Protocol Stacks</i>) with very different caching behaviors.},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {311--312},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223626},
 doi = {http://doi.acm.org/10.1145/223587.223626},
 acmid = {223626},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Salehi:1995:SCA:223586.223626,
 author = {Salehi, James D. and Kurose, James F. and Towsley, Don},
 title = {Scheduling for cache affinity in parallelized communication protocols},
 abstract = {We explore processor-cache affinity scheduling of parallel network protocol processing in a setting in which protocol processing executes on a shared-memory multiprocessor concurrently with a general workload of non-protocol activity. We find that affinity scheduling can significantly reduce the communication delay associated with protocol processing, enabling the host to support a greater number of concurrent streams and to provide a higher maximum throughput to individual streams. In addition, we compare implementations of two parallelization approaches (Locking</i> and Independent Protocol Stacks</i>) with very different caching behaviors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {311--312},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223626},
 doi = {http://doi.acm.org/10.1145/223586.223626},
 acmid = {223626},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chatterjee:1995:MAM:223586.223627,
 author = {Chatterjee, Amit K. and Konangi, Vijay K.},
 title = {Modeling and analysis of multi channel asymmetric packet switch modules in a bursty and nonuniform traffic environment},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {313--314},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223627},
 doi = {http://doi.acm.org/10.1145/223586.223627},
 acmid = {223627},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chatterjee:1995:MAM:223587.223627,
 author = {Chatterjee, Amit K. and Konangi, Vijay K.},
 title = {Modeling and analysis of multi channel asymmetric packet switch modules in a bursty and nonuniform traffic environment},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {313--314},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223627},
 doi = {http://doi.acm.org/10.1145/223587.223627},
 acmid = {223627},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shah:1995:TNT:223587.223628,
 author = {Shah, Gautam and Ramachandran, Umakishore and Fujimoto, Richard},
 title = {Timepatch: a novel technique for the parallel simulation of multiprocessor caches},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {315--316},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223628},
 doi = {http://doi.acm.org/10.1145/223587.223628},
 acmid = {223628},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shah:1995:TNT:223586.223628,
 author = {Shah, Gautam and Ramachandran, Umakishore and Fujimoto, Richard},
 title = {Timepatch: a novel technique for the parallel simulation of multiprocessor caches},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {315--316},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223628},
 doi = {http://doi.acm.org/10.1145/223586.223628},
 acmid = {223628},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sundaram:1995:FAB:223586.223629,
 author = {Sundaram, C. R. M. and Eager, Derek L.},
 title = {Future applicability of bus-based shared memory multiprocessors},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {317--318},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223629},
 doi = {http://doi.acm.org/10.1145/223586.223629},
 acmid = {223629},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sundaram:1995:FAB:223587.223629,
 author = {Sundaram, C. R. M. and Eager, Derek L.},
 title = {Future applicability of bus-based shared memory multiprocessors},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {317--318},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223629},
 doi = {http://doi.acm.org/10.1145/223587.223629},
 acmid = {223629},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ciardo:1995:MFC:223586.223630,
 author = {Ciardo, Gianfranco and Cherkasova, Ludmila and Kotov, Vadim and Rokicki, Tomas},
 title = {Modeling a fibre channel switch with stochastic Petri nets},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {319--320},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223630},
 doi = {http://doi.acm.org/10.1145/223586.223630},
 acmid = {223630},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ciardo:1995:MFC:223587.223630,
 author = {Ciardo, Gianfranco and Cherkasova, Ludmila and Kotov, Vadim and Rokicki, Tomas},
 title = {Modeling a fibre channel switch with stochastic Petri nets},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {319--320},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223630},
 doi = {http://doi.acm.org/10.1145/223587.223630},
 acmid = {223630},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arunachalam:1995:PPP:223586.223631,
 author = {Arunachalam, Meenakshi and Choudhary, Alok},
 title = {A prefetching prototype for the parallel file systems on the Paragon},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {23},
 issue = {1},
 month = {May},
 year = {1995},
 issn = {0163-5999},
 pages = {321--322},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223586.223631},
 doi = {http://doi.acm.org/10.1145/223586.223631},
 acmid = {223631},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arunachalam:1995:PPP:223587.223631,
 author = {Arunachalam, Meenakshi and Choudhary, Alok},
 title = {A prefetching prototype for the parallel file systems on the Paragon},
 abstract = {},
 booktitle = {Proceedings of the 1995 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '95/PERFORMANCE '95},
 year = {1995},
 isbn = {0-89791-695-6},
 location = {Ottawa, Ontario, Canada},
 pages = {321--322},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/223587.223631},
 doi = {http://doi.acm.org/10.1145/223587.223631},
 acmid = {223631},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gopalakrishnan:1996:BRS:233008.233017,
 author = {Gopalakrishnan, R. and Parulkar, Gurudatta M.},
 title = {Bringing real-time scheduling theory and practice closer for multimedia computing},
 abstract = {This paper seeks to bridge the gap between theory and practice of real-time scheduling in the domain of high speed multimedia networking. We show that the strict preemptive nature of real-time scheduling leads to more context switching, and requires system calls for concurrency control. We present our scheduling scheme called rate-monotonic with delayed preemption (<sc>rmdp</sc>) and show how it reduces both these overheads. We then develop the analytical framework to analyze <sc>rmdp</sc> and other scheduling schemes that lie in the region between strict (immediate) preemption and no preemption. Our idealized scheduler simulation</i> methodology accounts for the blocking introduced by these schemes under the usual assumption that the time for context switching and preemption is zero. We derive simpler schedulability tests for non-preemptive scheduling, and prove a variant of rate-monotonic scheduling that has fewer preemptions. Our measurements on Sparc and Pentium platforms, show that for the workloads we considered, <sc>Rmdp</sc> increases useful utilization by as much as 8\%. Thus our scheduling policies have the potential to improve performance over existing methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233017},
 doi = {http://doi.acm.org/10.1145/233008.233017},
 acmid = {233017},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gopalakrishnan:1996:BRS:233013.233017,
 author = {Gopalakrishnan, R. and Parulkar, Gurudatta M.},
 title = {Bringing real-time scheduling theory and practice closer for multimedia computing},
 abstract = {This paper seeks to bridge the gap between theory and practice of real-time scheduling in the domain of high speed multimedia networking. We show that the strict preemptive nature of real-time scheduling leads to more context switching, and requires system calls for concurrency control. We present our scheduling scheme called rate-monotonic with delayed preemption (<sc>rmdp</sc>) and show how it reduces both these overheads. We then develop the analytical framework to analyze <sc>rmdp</sc> and other scheduling schemes that lie in the region between strict (immediate) preemption and no preemption. Our idealized scheduler simulation</i> methodology accounts for the blocking introduced by these schemes under the usual assumption that the time for context switching and preemption is zero. We derive simpler schedulability tests for non-preemptive scheduling, and prove a variant of rate-monotonic scheduling that has fewer preemptions. Our measurements on Sparc and Pentium platforms, show that for the workloads we considered, <sc>Rmdp</sc> increases useful utilization by as much as 8\%. Thus our scheduling policies have the potential to improve performance over existing methods.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233017},
 doi = {http://doi.acm.org/10.1145/233013.233017},
 acmid = {233017},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harchol-Balter:1996:EPL:233008.233019,
 author = {Harchol-Balter, Mor and Downey, Allen B.},
 title = {Exploiting process lifetime distributions for dynamic load balancing},
 abstract = {We measure the distribution of lifetimes for UNIX processes and propose a functional form that fits this distribution well. We use this functional form to derive a policy for preemptive migration, and then use a trace-driven simulator to compare our proposed policy with other preemptive migration policies, and with a non-preemptive load balancing strategy. We find that, contrary to previous reports, the performance benefits of preemptive migration are significantly greater than those of non-preemptive migration, even when the memory-transfer cost is high. Using a model of migration costs representative of current systems, we find that preemptive migration reduces the mean delay (queueing and migration) by 35 - 50\%, compared to non-preemptive migration.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233019},
 doi = {http://doi.acm.org/10.1145/233008.233019},
 acmid = {233019},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harchol-Balter:1996:EPL:233013.233019,
 author = {Harchol-Balter, Mor and Downey, Allen B.},
 title = {Exploiting process lifetime distributions for dynamic load balancing},
 abstract = {We measure the distribution of lifetimes for UNIX processes and propose a functional form that fits this distribution well. We use this functional form to derive a policy for preemptive migration, and then use a trace-driven simulator to compare our proposed policy with other preemptive migration policies, and with a non-preemptive load balancing strategy. We find that, contrary to previous reports, the performance benefits of preemptive migration are significantly greater than those of non-preemptive migration, even when the memory-transfer cost is high. Using a model of migration costs representative of current systems, we find that preemptive migration reduces the mean delay (queueing and migration) by 35 - 50\%, compared to non-preemptive migration.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233019},
 doi = {http://doi.acm.org/10.1145/233013.233019},
 acmid = {233019},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dusseau:1996:EDS:233008.233020,
 author = {Dusseau, Andrea C. and Arpaci, Remzi H. and Culler, David E.},
 title = {Effective distributed scheduling of parallel workloads},
 abstract = {We present a distributed algorithm for time-sharing parallel workloads that is competitive with coscheduling. Implicit scheduling</i> allows each local scheduler in the system to make independent decisions that dynamically coordinate the scheduling of cooperating processes across processors. Of particular importance is the blocking algorithm which decides the action of a process waiting for a communication or synchronization event to complete. Through simulation of bulk-synchronous parallel applications, we find that a simple two-phase fixed-spin blocking algorithm performs well; a two-phase adaptive algorithm that gathers run-time data on barrier wait-times performs slightly better. Our results hold for a range of machine parameters and parallel program characteristics. These findings are in direct contrast to the literature that states explicit coscheduling is necessary for fine-grained programs. We show that the choice of the local scheduler is crucial, with a priority-based scheduler performing two to three times better than a round-robin scheduler. Overall, we find that the performance of implicit scheduling is near that of coscheduling (+/- 35\%), without the requirement of explicit, global coordination.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233020},
 doi = {http://doi.acm.org/10.1145/233008.233020},
 acmid = {233020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dusseau:1996:EDS:233013.233020,
 author = {Dusseau, Andrea C. and Arpaci, Remzi H. and Culler, David E.},
 title = {Effective distributed scheduling of parallel workloads},
 abstract = {We present a distributed algorithm for time-sharing parallel workloads that is competitive with coscheduling. Implicit scheduling</i> allows each local scheduler in the system to make independent decisions that dynamically coordinate the scheduling of cooperating processes across processors. Of particular importance is the blocking algorithm which decides the action of a process waiting for a communication or synchronization event to complete. Through simulation of bulk-synchronous parallel applications, we find that a simple two-phase fixed-spin blocking algorithm performs well; a two-phase adaptive algorithm that gathers run-time data on barrier wait-times performs slightly better. Our results hold for a range of machine parameters and parallel program characteristics. These findings are in direct contrast to the literature that states explicit coscheduling is necessary for fine-grained programs. We show that the choice of the local scheduler is crucial, with a priority-based scheduler performing two to three times better than a round-robin scheduler. Overall, we find that the performance of implicit scheduling is near that of coscheduling (+/- 35\%), without the requirement of explicit, global coordination.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233020},
 doi = {http://doi.acm.org/10.1145/233013.233020},
 acmid = {233020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lim:1996:LPB:233013.233021,
 author = {Lim, Beng-Hong and Bianchini, Ricardo},
 title = {Limits on the performance benefits of multithreading and prefetching},
 abstract = {This paper presents new analytical models of the performance benefits of multithreading and prefetching, and experimental measurements of parallel applications on the MIT Alewife multiprocessor. For the first time, both techniques are evaluated on a real machine as opposed to simulations. The models determine the region in the parameter space where the techniques are most effective, while the measurements determine the region where the applications lie. We find that these regions do not always overlap significantly.The multithreading model shows that only 2-4 contexts are necessary to maximize this technique's potential benefit in current multiprocessors. Multithreading improves execution time by less than 10\% for most of the applications that we examined. The model also shows that multithreading can significantly improve the performance of the same applications in multiprocessors with longer latencies. Reducing context-switch overhead is not crucial.The software prefetching model shows that allowing 4 outstanding prefetches is sufficient to achieve most of this technique's potential benefit on current multiprocessors. Prefetching improves performance over a wide range of parameters, and improves execution time by as much as 20-50\% even on current multiprocessors. The two models show that prefetching has a significant advantage over multithreading for machines with low memory latencies and/or applications with high cache miss rates because a prefetch instruction consumes less time than a context-switch.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233021},
 doi = {http://doi.acm.org/10.1145/233013.233021},
 acmid = {233021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lim:1996:LPB:233008.233021,
 author = {Lim, Beng-Hong and Bianchini, Ricardo},
 title = {Limits on the performance benefits of multithreading and prefetching},
 abstract = {This paper presents new analytical models of the performance benefits of multithreading and prefetching, and experimental measurements of parallel applications on the MIT Alewife multiprocessor. For the first time, both techniques are evaluated on a real machine as opposed to simulations. The models determine the region in the parameter space where the techniques are most effective, while the measurements determine the region where the applications lie. We find that these regions do not always overlap significantly.The multithreading model shows that only 2-4 contexts are necessary to maximize this technique's potential benefit in current multiprocessors. Multithreading improves execution time by less than 10\% for most of the applications that we examined. The model also shows that multithreading can significantly improve the performance of the same applications in multiprocessors with longer latencies. Reducing context-switch overhead is not crucial.The software prefetching model shows that allowing 4 outstanding prefetches is sufficient to achieve most of this technique's potential benefit on current multiprocessors. Prefetching improves performance over a wide range of parameters, and improves execution time by as much as 20-50\% even on current multiprocessors. The two models show that prefetching has a significant advantage over multithreading for machines with low memory latencies and/or applications with high cache miss rates because a prefetch instruction consumes less time than a context-switch.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233021},
 doi = {http://doi.acm.org/10.1145/233008.233021},
 acmid = {233021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dinda:1996:FMA:233013.233022,
 author = {Dinda, Peter A. and O'Hallaron, David R.},
 title = {Fast message assembly using compact address relations},
 abstract = {Message assembly and disassembly represent a significant fraction of total communication time in many parallel systems. We introduce a run-time approach for fast message assembly and disassembly. The approach is based on generating addresses by decoding a precomputed and compactly stored address relation that describes the mapping of addresses on the source node to addresses on the destination node. The main result is that relations induced by redistributions of regular block-cyclic distributed arrays can be encoded in an extremely compact form that facilitates high throughput message assembly and disassembly. We measure the throughput of decoding-based message assembly and disassembly on several systems and find performance on par with copy throughput.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233022},
 doi = {http://doi.acm.org/10.1145/233013.233022},
 acmid = {233022},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dinda:1996:FMA:233008.233022,
 author = {Dinda, Peter A. and O'Hallaron, David R.},
 title = {Fast message assembly using compact address relations},
 abstract = {Message assembly and disassembly represent a significant fraction of total communication time in many parallel systems. We introduce a run-time approach for fast message assembly and disassembly. The approach is based on generating addresses by decoding a precomputed and compactly stored address relation that describes the mapping of addresses on the source node to addresses on the destination node. The main result is that relations induced by redistributions of regular block-cyclic distributed arrays can be encoded in an extremely compact form that facilitates high throughput message assembly and disassembly. We measure the throughput of decoding-based message assembly and disassembly on several systems and find performance on par with copy throughput.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233022},
 doi = {http://doi.acm.org/10.1145/233008.233022},
 acmid = {233022},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Parsons:1996:CAM:233008.233023,
 author = {Parsons, Eric W. and Sevcik, Kenneth C.},
 title = {Coordinated allocation of memory and processors in multiprocessors},
 abstract = {An important issue in multiprogrammed multiprocessor systems is the scheduling of parallel jobs. Most research in the area has focussed solely on the allocation of processors to jobs. However, since memory is also a critical resource for many parallel jobs, the allocation of memory and processors must be coordinated to allow the system to operate most effectively.To understand how to design such coordinated scheduling disciplines, it is important to have a theoretical foundation. To this end, we develop bounds on the achievable system throughput when both memory and processing time are in demand. We then propose and simulate a simple discipline and relate its performance to the throughput bounds. An important result of our work is for the situation in which the workload speedup is convex (from above), but the speedup characteristics of individual jobs are unknown. It shows that an equi-allocation strategy for processors can achieve near-maximum throughput, yet offer good mean response times, when both memory and processors are considered.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {57--67},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/233008.233023},
 doi = {http://doi.acm.org/10.1145/233008.233023},
 acmid = {233023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Parsons:1996:CAM:233013.233023,
 author = {Parsons, Eric W. and Sevcik, Kenneth C.},
 title = {Coordinated allocation of memory and processors in multiprocessors},
 abstract = {An important issue in multiprogrammed multiprocessor systems is the scheduling of parallel jobs. Most research in the area has focussed solely on the allocation of processors to jobs. However, since memory is also a critical resource for many parallel jobs, the allocation of memory and processors must be coordinated to allow the system to operate most effectively.To understand how to design such coordinated scheduling disciplines, it is important to have a theoretical foundation. To this end, we develop bounds on the achievable system throughput when both memory and processing time are in demand. We then propose and simulate a simple discipline and relate its performance to the throughput bounds. An important result of our work is for the situation in which the workload speedup is convex (from above), but the speedup characteristics of individual jobs are unknown. It shows that an equi-allocation strategy for processors can achieve near-maximum throughput, yet offer good mean response times, when both memory and processors are considered.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {57--67},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/233013.233023},
 doi = {http://doi.acm.org/10.1145/233013.233023},
 acmid = {233023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Witchel:1996:EFF:233008.233025,
 author = {Witchel, Emmett and Rosenblum, Mendel},
 title = {Embra: fast and flexible machine simulation},
 abstract = {This paper describes Embra, a simulator for the processors, caches, and memory systems of uniprocessors and cache-coherent multiprocessors. When running as part of the SimOS simulation environment, Embra models the processors of a MIPS R3000/R4000 machine faithfully enough to run a commercial operating system and arbitrary user applications. To achieve high simulation speed, Embra uses dynamic binary translation to generate code sequences which simulate the workload. It is the first machine simulator to use this technique. Embra can simulate real workloads such as multiprocess compiles and the SPEC92 benchmarks running on Silicon Graphic's IRIX 5.3 at speeds only 3 to 9 times slower than native execution of the workload, making Embra the fastest reported complete machine simulator. Dynamic binary translation also gives Embra the flexibility to dynamically control both the simulation statistics reported and the simulation model accuracy with low performance overheads. For example, Embra can customize its generated code to include a processor cache model which allows it to compute the cache misses and memory stall time of a workload. Customized code generation allows Embra to simulate a machine with caches at slowdowns of only a factor of 7 to 20. Most of the statistics generated at this speed match those produced by a slower reference simulator to within 1\%. This paper describes the techniques used by Embra to achieve high performance, focusing on the requirements unique to machine simulation, including modeling the processor, memory management unit, and caches. In order to study Embra's memory system performance we use the SimOS simulation system to examine Embra itself. We present a detailed breakdown of Embra's memory system performance for two cache hierarchies to understand Embra's current performance and to show that Embra's implementation techniques benefit significantly from the larger cache hierarchies that are becoming available. Embra has been used for operating system development and testing as well as for studies of computer architecture. In this capacity it has simulated large, commercial workloads including IRIX running a relational database system and a CAD system for billions of simulated machine cycles.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {68--79},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233025},
 doi = {http://doi.acm.org/10.1145/233008.233025},
 acmid = {233025},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Witchel:1996:EFF:233013.233025,
 author = {Witchel, Emmett and Rosenblum, Mendel},
 title = {Embra: fast and flexible machine simulation},
 abstract = {This paper describes Embra, a simulator for the processors, caches, and memory systems of uniprocessors and cache-coherent multiprocessors. When running as part of the SimOS simulation environment, Embra models the processors of a MIPS R3000/R4000 machine faithfully enough to run a commercial operating system and arbitrary user applications. To achieve high simulation speed, Embra uses dynamic binary translation to generate code sequences which simulate the workload. It is the first machine simulator to use this technique. Embra can simulate real workloads such as multiprocess compiles and the SPEC92 benchmarks running on Silicon Graphic's IRIX 5.3 at speeds only 3 to 9 times slower than native execution of the workload, making Embra the fastest reported complete machine simulator. Dynamic binary translation also gives Embra the flexibility to dynamically control both the simulation statistics reported and the simulation model accuracy with low performance overheads. For example, Embra can customize its generated code to include a processor cache model which allows it to compute the cache misses and memory stall time of a workload. Customized code generation allows Embra to simulate a machine with caches at slowdowns of only a factor of 7 to 20. Most of the statistics generated at this speed match those produced by a slower reference simulator to within 1\%. This paper describes the techniques used by Embra to achieve high performance, focusing on the requirements unique to machine simulation, including modeling the processor, memory management unit, and caches. In order to study Embra's memory system performance we use the SimOS simulation system to examine Embra itself. We present a detailed breakdown of Embra's memory system performance for two cache hierarchies to understand Embra's current performance and to show that Embra's implementation techniques benefit significantly from the larger cache hierarchies that are becoming available. Embra has been used for operating system development and testing as well as for studies of computer architecture. In this capacity it has simulated large, commercial workloads including IRIX running a relational database system and a CAD system for billions of simulated machine cycles.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {68--79},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233025},
 doi = {http://doi.acm.org/10.1145/233013.233025},
 acmid = {233025},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brakmo:1996:ENS:233008.233027,
 author = {Brakmo, Lawrence S. and Peterson, Larry L.},
 title = {Experiences with network simulation},
 abstract = {Simulation is a critical tool in developing, testing, and evaluating network protocols and architectures. This paper describes x</i>-Sim, a network simulator based on the x</i>-kernel, that is able to fully simulate the topologies and traffic patterns of large scale networks. It also illustrates the capabilities and usefulness of the simulator with case studies. Finally, based on our experiences using x</i>-Sim, we identify a set of principles (guidelines) for network simulation, and present concrete examples that quantify the value of these principles, along with the cost of ignoring them.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {80--90},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/233008.233027},
 doi = {http://doi.acm.org/10.1145/233008.233027},
 acmid = {233027},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brakmo:1996:ENS:233013.233027,
 author = {Brakmo, Lawrence S. and Peterson, Larry L.},
 title = {Experiences with network simulation},
 abstract = {Simulation is a critical tool in developing, testing, and evaluating network protocols and architectures. This paper describes x</i>-Sim, a network simulator based on the x</i>-kernel, that is able to fully simulate the topologies and traffic patterns of large scale networks. It also illustrates the capabilities and usefulness of the simulator with case studies. Finally, based on our experiences using x</i>-Sim, we identify a set of principles (guidelines) for network simulation, and present concrete examples that quantify the value of these principles, along with the cost of ignoring them.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {80--90},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/233013.233027},
 doi = {http://doi.acm.org/10.1145/233013.233027},
 acmid = {233027},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Greenberg:1996:AUL:233013.233028,
 author = {Greenberg, Albert G. and Shenker, S. and Stolyar, Alexander L.},
 title = {Asynchronous updates in large parallel systems},
 abstract = {Lubachevsky [5] introduced a new parallel simulation technique intended for systems with limited interactions between their many components or sites. Each site has a local simulation time, and the states of the sites are updated asynchronously. This asynchronous updating appears to allow the simulation to achieve a high degree of parallelism, with very low overhead in processor synchronization. The key issue for this asynchronous updating technique is: how fast do the local times make progress in the large system limit? We show that in a simple K</i>-random interaction model the local times progress at a rate 1/(K</i> + 1). More importantly, we find that the asymptotic distribution of local times is described by a traveling wave</i> solution with exponentially decaying tails. In terms of the parallel simulation, though the interactions are local, a very high degree of global synchronization results, and this synchronization is succinctly described by the traveling wave solution. Moreover, we report on experiments that suggest that the traveling wave solution is universal;</i> i.e., it holds in realistic scenarios (out of reach of our analysis) where interactions among sites are not random.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {91--103},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/233013.233028},
 doi = {http://doi.acm.org/10.1145/233013.233028},
 acmid = {233028},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Greenberg:1996:AUL:233008.233028,
 author = {Greenberg, Albert G. and Shenker, S. and Stolyar, Alexander L.},
 title = {Asynchronous updates in large parallel systems},
 abstract = {Lubachevsky [5] introduced a new parallel simulation technique intended for systems with limited interactions between their many components or sites. Each site has a local simulation time, and the states of the sites are updated asynchronously. This asynchronous updating appears to allow the simulation to achieve a high degree of parallelism, with very low overhead in processor synchronization. The key issue for this asynchronous updating technique is: how fast do the local times make progress in the large system limit? We show that in a simple K</i>-random interaction model the local times progress at a rate 1/(K</i> + 1). More importantly, we find that the asymptotic distribution of local times is described by a traveling wave</i> solution with exponentially decaying tails. In terms of the parallel simulation, though the interactions are local, a very high degree of global synchronization results, and this synchronization is succinctly described by the traveling wave solution. Moreover, we report on experiments that suggest that the traveling wave solution is universal;</i> i.e., it holds in realistic scenarios (out of reach of our analysis) where interactions among sites are not random.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {91--103},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/233008.233028},
 doi = {http://doi.acm.org/10.1145/233008.233028},
 acmid = {233028},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stiliadis:1996:DAF:233013.233030,
 author = {Stiliadis, Dimitrios and Varma, Anujan},
 title = {Design and analysis of frame-based fair queueing: a new traffic scheduling algorithm for packet-switched networks},
 abstract = {In this paper we introduce and analyze frame-based fair queueing</i>, a novel traffic scheduling algorithm for packet-switched networks. The algorithm provides end-to-end delay bounds identical to those of PGPS (packet-level generalized processor sharing), without the complexity of simulating the fluid-model system in the background as required in PGPS. The algorithm is therefore ideally suited for implementation in packet switches supporting a large number of sessions. We present a simple implementation of the algorithm for a general packet switch. In addition, we prove that the algorithm is fair in the sense that sessions are not penalized for excess bandwidth they received while other sessions were idle. Frame-based fair queueing belongs to a general class of scheduling algorithms, which we call Rate-Proportional Servers</i>. This class of algorithms provides the same end-to-end delay and burstiness bounds as PGPS, but allows more flexibility in the design and implementation of the algorithm. We provide a systematic analysis of this class of schedulers and obtain bounds on their fairness.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {104--115},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233030},
 doi = {http://doi.acm.org/10.1145/233013.233030},
 acmid = {233030},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stiliadis:1996:DAF:233008.233030,
 author = {Stiliadis, Dimitrios and Varma, Anujan},
 title = {Design and analysis of frame-based fair queueing: a new traffic scheduling algorithm for packet-switched networks},
 abstract = {In this paper we introduce and analyze frame-based fair queueing</i>, a novel traffic scheduling algorithm for packet-switched networks. The algorithm provides end-to-end delay bounds identical to those of PGPS (packet-level generalized processor sharing), without the complexity of simulating the fluid-model system in the background as required in PGPS. The algorithm is therefore ideally suited for implementation in packet switches supporting a large number of sessions. We present a simple implementation of the algorithm for a general packet switch. In addition, we prove that the algorithm is fair in the sense that sessions are not penalized for excess bandwidth they received while other sessions were idle. Frame-based fair queueing belongs to a general class of scheduling algorithms, which we call Rate-Proportional Servers</i>. This class of algorithms provides the same end-to-end delay and burstiness bounds as PGPS, but allows more flexibility in the design and implementation of the algorithm. We provide a systematic analysis of this class of schedulers and obtain bounds on their fairness.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {104--115},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233030},
 doi = {http://doi.acm.org/10.1145/233008.233030},
 acmid = {233030},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yates:1996:NSL:233008.233032,
 author = {Yates, David J. and Nahum, Erich M. and Kurose, James F. and Towsley, Don},
 title = {Networking support for large scale multiprocessor servers},
 abstract = {Over the next several years the performance demands on globally available information servers are expected to increase dramatically. These servers must be capable of sending and receiving data over hundreds or even thousands of simultaneous connections. In this paper, we show that connection-level parallel protocols (where different connections are processed in parallel) running on a shared-memory multiprocessor can deliver high network bandwidth across a large number of connections.We experimentally evaluate connection-level parallel implementations of both TCP/IP and UDP/IP protocol stacks. We focus on three questions in our performance evaluation: how throughput scales with the number of processors, how throughput changes as the number of connections increases, and how fairly the aggregate bandwidth is distributed across connections. We show how several factors impact performance: the number of processors used, the number of threads in the system, the number of connections assigned to each thread, and the type of protocols in the stack (i.e., TCP versus UDP).Our results show that with careful implementation connection-level parallel protocol stacks scale well with the number of processors, and deliver high throughput which is, for the most part, sustained as the number of connections increases. Maximizing the number of threads in the system yields the best overall throughput. However, the best fairness behavior is achieved by matching the number of threads to the number of processors and scheduling connections assigned to threads in a round-robin manner.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {116--125},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233032},
 doi = {http://doi.acm.org/10.1145/233008.233032},
 acmid = {233032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yates:1996:NSL:233013.233032,
 author = {Yates, David J. and Nahum, Erich M. and Kurose, James F. and Towsley, Don},
 title = {Networking support for large scale multiprocessor servers},
 abstract = {Over the next several years the performance demands on globally available information servers are expected to increase dramatically. These servers must be capable of sending and receiving data over hundreds or even thousands of simultaneous connections. In this paper, we show that connection-level parallel protocols (where different connections are processed in parallel) running on a shared-memory multiprocessor can deliver high network bandwidth across a large number of connections.We experimentally evaluate connection-level parallel implementations of both TCP/IP and UDP/IP protocol stacks. We focus on three questions in our performance evaluation: how throughput scales with the number of processors, how throughput changes as the number of connections increases, and how fairly the aggregate bandwidth is distributed across connections. We show how several factors impact performance: the number of processors used, the number of threads in the system, the number of connections assigned to each thread, and the type of protocols in the stack (i.e., TCP versus UDP).Our results show that with careful implementation connection-level parallel protocol stacks scale well with the number of processors, and deliver high throughput which is, for the most part, sustained as the number of connections increases. Maximizing the number of threads in the system yields the best overall throughput. However, the best fairness behavior is achieved by matching the number of threads to the number of processors and scheduling connections assigned to threads in a round-robin manner.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {116--125},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233032},
 doi = {http://doi.acm.org/10.1145/233013.233032},
 acmid = {233032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arlitt:1996:WSW:233013.233034,
 author = {Arlitt, Martin F. and Williamson, Carey L.},
 title = {Web server workload characterization: the search for invariants},
 abstract = {The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction.Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic (i.e., university) environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity.Throughout the study, emphasis is placed on finding workload invariants</i>: observations that apply across all the data sets studied. Ten invariants are identified. These invariants are deemed important since they (potentially) represent universal truths for all Internet Web servers. The paper concludes with a discussion of caching and performance issues, using the invariants to suggest performance enhancements that seem most promising for Internet Web servers.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {126--137},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233034},
 doi = {http://doi.acm.org/10.1145/233013.233034},
 acmid = {233034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arlitt:1996:WSW:233008.233034,
 author = {Arlitt, Martin F. and Williamson, Carey L.},
 title = {Web server workload characterization: the search for invariants},
 abstract = {The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction.Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic (i.e., university) environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity.Throughout the study, emphasis is placed on finding workload invariants</i>: observations that apply across all the data sets studied. Ten invariants are identified. These invariants are deemed important since they (potentially) represent universal truths for all Internet Web servers. The paper concludes with a discussion of caching and performance issues, using the invariants to suggest performance enhancements that seem most promising for Internet Web servers.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {126--137},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233034},
 doi = {http://doi.acm.org/10.1145/233008.233034},
 acmid = {233034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martonosi:1996:IPM:233008.233035,
 author = {Martonosi, Margaret and Ofelt, David and Heinrich, Mark},
 title = {Integrating performance monitoring and communication in parallel computers},
 abstract = {A large and increasing gap exists between processor and memory speeds in scalable cache-coherent multiprocessors. To cope with this situation, programmers and compiler writers must increasingly be aware of the memory hierarchy as they implement software. Tools to support memory performance tuning have, however, been hobbled by the fact that it is difficult to observe the caching behavior of a running program. Little hardware support exists specifically for observing caching behavior; furthermore, what support does exist is often difficult to use for making fine-grained observations about program memory behavior.Our work observes that in a multiprocessor, the actions required for memory performance monitoring are similar to those required for enforcing cache coherence. In fact, we argue that on several machines, the coherence/communication system itself can be used as machine support for performance monitoring. We have demonstrated this idea by implementing the FlashPoint memory performance monitoring tool. FlashPoint is implemented as a special performance-monitoring coherence protocol for the Stanford FLASH Multiprocessor. By embedding performance monitoring into a cache-coherence scheme based on a programmable controller, we can gather detailed, per-data-structure, memory statistics with less than a 10\% slowdown compared to unmonitored program executions. We present results on the accuracy of the data collected, and on how FlashPoint performance scales with the number of processors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233035},
 doi = {http://doi.acm.org/10.1145/233008.233035},
 acmid = {233035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Martonosi:1996:IPM:233013.233035,
 author = {Martonosi, Margaret and Ofelt, David and Heinrich, Mark},
 title = {Integrating performance monitoring and communication in parallel computers},
 abstract = {A large and increasing gap exists between processor and memory speeds in scalable cache-coherent multiprocessors. To cope with this situation, programmers and compiler writers must increasingly be aware of the memory hierarchy as they implement software. Tools to support memory performance tuning have, however, been hobbled by the fact that it is difficult to observe the caching behavior of a running program. Little hardware support exists specifically for observing caching behavior; furthermore, what support does exist is often difficult to use for making fine-grained observations about program memory behavior.Our work observes that in a multiprocessor, the actions required for memory performance monitoring are similar to those required for enforcing cache coherence. In fact, we argue that on several machines, the coherence/communication system itself can be used as machine support for performance monitoring. We have demonstrated this idea by implementing the FlashPoint memory performance monitoring tool. FlashPoint is implemented as a special performance-monitoring coherence protocol for the Stanford FLASH Multiprocessor. By embedding performance monitoring into a cache-coherence scheme based on a programmable controller, we can gather detailed, per-data-structure, memory statistics with less than a 10\% slowdown compared to unmonitored program executions. We present results on the accuracy of the data collected, and on how FlashPoint performance scales with the number of processors.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {138--147},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233035},
 doi = {http://doi.acm.org/10.1145/233013.233035},
 acmid = {233035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krishnaswamy:1996:MEU:233013.233037,
 author = {Krishnaswamy, Umesh and Scherson, Isaac D.},
 title = {Micro-architecture evaluation using performance vectors},
 abstract = {Benchmarking is a widely used approach to measure computer performance. Current use of benchmarks only provides running times to describe the performance of a tested system. Glancing through these execution times provides little or no information about system strengths and weaknesses. A novel benchmarking methodology is proposed to identify key performance parameters; the methodology is based on measuring performance vectors. A performance vector is a vector of ratings that represents delivered performance of primitive operations of a system. Measuring the performance vector of a system in a typical user workload can be a tough problem. We show how the performance vector falls out of an equation consisting of dynamic instruction counts and execution times of benchmarks. We present a non-linear approach for computing the performance vector. The efficacy of the methodology is ascertained by evaluating the micro-architecture of the Sun SuperSPARC superscalar processor using SPEC benchmarks. Results show interesting tradeoffs in the SuperSPARC and speak favorably of our methodology.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {148--159},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233037},
 doi = {http://doi.acm.org/10.1145/233013.233037},
 acmid = {233037},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krishnaswamy:1996:MEU:233008.233037,
 author = {Krishnaswamy, Umesh and Scherson, Isaac D.},
 title = {Micro-architecture evaluation using performance vectors},
 abstract = {Benchmarking is a widely used approach to measure computer performance. Current use of benchmarks only provides running times to describe the performance of a tested system. Glancing through these execution times provides little or no information about system strengths and weaknesses. A novel benchmarking methodology is proposed to identify key performance parameters; the methodology is based on measuring performance vectors. A performance vector is a vector of ratings that represents delivered performance of primitive operations of a system. Measuring the performance vector of a system in a typical user workload can be a tough problem. We show how the performance vector falls out of an equation consisting of dynamic instruction counts and execution times of benchmarks. We present a non-linear approach for computing the performance vector. The efficacy of the methodology is ascertained by evaluating the micro-architecture of the Sun SuperSPARC superscalar processor using SPEC benchmarks. Results show interesting tradeoffs in the SuperSPARC and speak favorably of our methodology.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {148--159},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233037},
 doi = {http://doi.acm.org/10.1145/233008.233037},
 acmid = {233037},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Crovella:1996:SWW:233013.233038,
 author = {Crovella, Mark E. and Bestavros, Azer},
 title = {Self-similarity in World Wide Web traffic: evidence and possible causes},
 abstract = {Recently the notion of self-similarity</i> has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to the self-similarity of network traffic. We present a hypothesized explanation for the possible self-similarity of traffic by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we examine the dependence structure of WWW traffic. While our measurements are not conclusive, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user "think time", and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {160--169},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233038},
 doi = {http://doi.acm.org/10.1145/233013.233038},
 acmid = {233038},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Crovella:1996:SWW:233008.233038,
 author = {Crovella, Mark E. and Bestavros, Azer},
 title = {Self-similarity in World Wide Web traffic: evidence and possible causes},
 abstract = {Recently the notion of self-similarity</i> has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to the self-similarity of network traffic. We present a hypothesized explanation for the possible self-similarity of traffic by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we examine the dependence structure of WWW traffic. While our measurements are not conclusive, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user "think time", and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {160--169},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233038},
 doi = {http://doi.acm.org/10.1145/233008.233038},
 acmid = {233038},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hillyer:1996:MPC:233008.233039,
 author = {Hillyer, Bruce K. and Silberschatz, Avi},
 title = {On the modeling and performance characteristics of a serpentine tape drive},
 abstract = {New applications require online access to many terabytes of data, but a magnetic disk storage system this large requires thousands of drives. Magnetic tape is be a good alternative, except that the application demand for transparent data retrieval is not met by current tape systems because of their high access latency. This latency can be significantly improved by good retrieval scheduling. A fundamental prerequisite to efficient scheduling is the ability to estimate the amount of time required for tape positioning operations (the locate time</i>). For serpentine tape, which is the most common mass storage tape technology, this estimation is subtle and complex. The main contribution of this paper is a locate-time model for a DLT4000 tape drive. The accuracy of the model is evaluated by measurements, and the utility of the model is demonstrated through a model-driven simulation of retrieval scheduling, validated by measurements and sensitivity testing. In brief, the locate-time model is accurate to within a few percent, which enables the production of efficient schedules.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233039},
 doi = {http://doi.acm.org/10.1145/233008.233039},
 acmid = {233039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hillyer:1996:MPC:233013.233039,
 author = {Hillyer, Bruce K. and Silberschatz, Avi},
 title = {On the modeling and performance characteristics of a serpentine tape drive},
 abstract = {New applications require online access to many terabytes of data, but a magnetic disk storage system this large requires thousands of drives. Magnetic tape is be a good alternative, except that the application demand for transparent data retrieval is not met by current tape systems because of their high access latency. This latency can be significantly improved by good retrieval scheduling. A fundamental prerequisite to efficient scheduling is the ability to estimate the amount of time required for tape positioning operations (the locate time</i>). For serpentine tape, which is the most common mass storage tape technology, this estimation is subtle and complex. The main contribution of this paper is a locate-time model for a DLT4000 tape drive. The accuracy of the model is evaluated by measurements, and the utility of the model is demonstrated through a model-driven simulation of retrieval scheduling, validated by measurements and sensitivity testing. In brief, the locate-time model is accurate to within a few percent, which enables the production of efficient schedules.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233039},
 doi = {http://doi.acm.org/10.1145/233013.233039},
 acmid = {233039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Menasce:1996:AMH:233008.233041,
 author = {Menasc\'{e}, Daniel A. and Pentakalos, Odysseas I. and Yesha, Yelena},
 title = {An analytic model of hierarchical mass storage systems with network-attached storage devices},
 abstract = {Network attached storage devices improve I/O performance by separating control and data paths and eliminating host intervention during data transfer. Devices are attached to a high speed network for data transfer and to a slower network for control messages. Hierarchical mass storage systems use disks to cache the most recently used files and tapes (robotic and manually mounted) to store the bulk of the files in the file system. This paper shows how queuing network models can be used to assess the performance of hierarchical mass storage systems that use network attached storage devices. The analytic model validated through simulation was used to analyze many different scenarios.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {180--189},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233041},
 doi = {http://doi.acm.org/10.1145/233008.233041},
 acmid = {233041},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Menasce:1996:AMH:233013.233041,
 author = {Menasc\'{e}, Daniel A. and Pentakalos, Odysseas I. and Yesha, Yelena},
 title = {An analytic model of hierarchical mass storage systems with network-attached storage devices},
 abstract = {Network attached storage devices improve I/O performance by separating control and data paths and eliminating host intervention during data transfer. Devices are attached to a high speed network for data transfer and to a slower network for control messages. Hierarchical mass storage systems use disks to cache the most recently used files and tapes (robotic and manually mounted) to store the bulk of the files in the file system. This paper shows how queuing network models can be used to assess the performance of hierarchical mass storage systems that use network attached storage devices. The analytic model validated through simulation was used to analyze many different scenarios.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {180--189},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233041},
 doi = {http://doi.acm.org/10.1145/233013.233041},
 acmid = {233041},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1996:AAW:233008.233043,
 author = {Chen, Ken and Decreusefond, Laurent},
 title = {An approximate analysis of waiting time in multi-class <italic>M/G</italic>/1/./<italic>EDF</italic> queues},
 abstract = {The Earliest-Deadline-First (EDF) queueing discipline is being more and more widely used for handling time-sensitive applications in computer systems and networks. In this paper, we consider an arbitrary number of traffic classes with class-specific soft-deadline. A soft-deadline is a target waiting-time limit that can be missed. EDF queueing has been proved to minimize the maximum delay overflow related to this limit. We propose a quantitative analysis, through the metric of mean waiting time, on the behavior of EDF queueing. This analysis gives also insight on the correlation between traffic classes with different time-constraints. Technically speaking, we have proven that the mean waiting times for an arbitrary set of N</i> classes of traffic streams with soft deadlines are the unique solution of a system of non-linear equations under the constraint of the Kleinrock's conservation law. We then provide an O</i> (N</i><sup>2</sup>) algorithm to get the solution. Simulation suggests that the theoretical approximation we made is quite acceptable.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233043},
 doi = {http://doi.acm.org/10.1145/233008.233043},
 acmid = {233043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication networks, computer architecture, multimedia systems, real-time systems, stochastic modeling},
} 

@inproceedings{Chen:1996:AAW:233013.233043,
 author = {Chen, Ken and Decreusefond, Laurent},
 title = {An approximate analysis of waiting time in multi-class <italic>M/G</italic>/1/./<italic>EDF</italic> queues},
 abstract = {The Earliest-Deadline-First (EDF) queueing discipline is being more and more widely used for handling time-sensitive applications in computer systems and networks. In this paper, we consider an arbitrary number of traffic classes with class-specific soft-deadline. A soft-deadline is a target waiting-time limit that can be missed. EDF queueing has been proved to minimize the maximum delay overflow related to this limit. We propose a quantitative analysis, through the metric of mean waiting time, on the behavior of EDF queueing. This analysis gives also insight on the correlation between traffic classes with different time-constraints. Technically speaking, we have proven that the mean waiting times for an arbitrary set of N</i> classes of traffic streams with soft deadlines are the unique solution of a system of non-linear equations under the constraint of the Kleinrock's conservation law. We then provide an O</i> (N</i><sup>2</sup>) algorithm to get the solution. Simulation suggests that the theoretical approximation we made is quite acceptable.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233043},
 doi = {http://doi.acm.org/10.1145/233013.233043},
 acmid = {233043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication networks, computer architecture, multimedia systems, real-time systems, stochastic modeling},
} 

@inproceedings{Aggarwal:1996:OPM:233013.233044,
 author = {Aggarwal, Charu and Wolf, Joel and Yu, Philip S.},
 title = {On optimal piggyback merging policies for video-on-demand systems},
 abstract = {A critical issue in the performance of a video-on-demand system is the I/O bandwidth required in order to satisfy client requests. A number of techniques have been proposed in order to reduce these bandwidth requirements. In this paper we concentrate on one such technique, known as adaptive piggybacking. We develop and analyze piggyback merging policies which are optimal over large classes of reasonable methods.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {200--209},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233044},
 doi = {http://doi.acm.org/10.1145/233013.233044},
 acmid = {233044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Aggarwal:1996:OPM:233008.233044,
 author = {Aggarwal, Charu and Wolf, Joel and Yu, Philip S.},
 title = {On optimal piggyback merging policies for video-on-demand systems},
 abstract = {A critical issue in the performance of a video-on-demand system is the I/O bandwidth required in order to satisfy client requests. A number of techniques have been proposed in order to reduce these bandwidth requirements. In this paper we concentrate on one such technique, known as adaptive piggybacking. We develop and analyze piggyback merging policies which are optimal over large classes of reasonable methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {200--209},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233044},
 doi = {http://doi.acm.org/10.1145/233008.233044},
 acmid = {233044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gerber:1996:EDV:233008.233046,
 author = {Gerber, Richard and Gharai, Ladan},
 title = {Experiments with digital video playback},
 abstract = {In this paper we describe our experiments on digital video applications, concentrating on the static and dynamic tradeoffs involved in video playback. Our results were extracted from a controlled series of 272 tests, which we ran in three stages.In the first stage of 120 tests, we used a simple player-monitor tool to evaluate the effects of various static parameters: compression type, frame size, digitized rate, spatial quality</i> and keyframe distribution.</i> The tests were carried out on two Apple Macintosh platforms: at the lower end a Quadra 950, and at the higher end, a Power PC 7100/80. Our quantitative metrics included average playback rate, as well as the rate's variance over one-second intervals.The first set of experiments unveiled several anomalous latencies. To track them down we ran an additional 120 tests, from which we concluded that the video and IO operations were insufficiently tuned to each other.In the next step we attempted to correct this problem, by implementing our own video playback software and accompanying device-level handlers. Our emphasis was on achieving a controlled, deterministic coordination between the various system components. An additional set of 32 experiments were carried out on our platforms, which showed frame-rate increases of up to 325\%, with associated reductions in rate variance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {210--221},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233008.233046},
 doi = {http://doi.acm.org/10.1145/233008.233046},
 acmid = {233046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gerber:1996:EDV:233013.233046,
 author = {Gerber, Richard and Gharai, Ladan},
 title = {Experiments with digital video playback},
 abstract = {In this paper we describe our experiments on digital video applications, concentrating on the static and dynamic tradeoffs involved in video playback. Our results were extracted from a controlled series of 272 tests, which we ran in three stages.In the first stage of 120 tests, we used a simple player-monitor tool to evaluate the effects of various static parameters: compression type, frame size, digitized rate, spatial quality</i> and keyframe distribution.</i> The tests were carried out on two Apple Macintosh platforms: at the lower end a Quadra 950, and at the higher end, a Power PC 7100/80. Our quantitative metrics included average playback rate, as well as the rate's variance over one-second intervals.The first set of experiments unveiled several anomalous latencies. To track them down we ran an additional 120 tests, from which we concluded that the video and IO operations were insufficiently tuned to each other.In the next step we attempted to correct this problem, by implementing our own video playback software and accompanying device-level handlers. Our emphasis was on achieving a controlled, deterministic coordination between the various system components. An additional set of 32 experiments were carried out on our platforms, which showed frame-rate increases of up to 325\%, with associated reductions in rate variance.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {210--221},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/233013.233046},
 doi = {http://doi.acm.org/10.1145/233013.233046},
 acmid = {233046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Salehi:1996:SSV:233008.233047,
 author = {Salehi, James D. and Zhang, Zhi-Li and Kurose, James F. and Towsley, Don},
 title = {Supporting stored video: reducing rate variability and end-to-end resource requirements through optimal smoothing},
 abstract = {VBR compressed video is known to exhibit significant, multiple-time-scale bit rate variability. In this paper, we consider the transmission of stored video from a server to a client across a high speed network, and explore how the client buffer space can be used most effectively toward reducing the variability of the transmitted bit rate.We present two basic results. First, we present an optimal smoothing algorithm for achieving the greatest possible reduction in rate variability</i> when transmitting stored video to a client with given buffer size. We provide a formal proof of optimality, and demonstrate the performance of the algorithm on a set of long MPEG-1 encoded video traces. Second, we evaluate the impact of optimal smoothing on the network resources needed for video transport, under two network service models: Deterministic Guaranteed service [1, 9] and Renegotiated CBR (RCBR) service [8, 7]. Under both models, we find the impact of optimal smoothing to be dramatic.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {222--231},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233047},
 doi = {http://doi.acm.org/10.1145/233008.233047},
 acmid = {233047},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Salehi:1996:SSV:233013.233047,
 author = {Salehi, James D. and Zhang, Zhi-Li and Kurose, James F. and Towsley, Don},
 title = {Supporting stored video: reducing rate variability and end-to-end resource requirements through optimal smoothing},
 abstract = {VBR compressed video is known to exhibit significant, multiple-time-scale bit rate variability. In this paper, we consider the transmission of stored video from a server to a client across a high speed network, and explore how the client buffer space can be used most effectively toward reducing the variability of the transmitted bit rate.We present two basic results. First, we present an optimal smoothing algorithm for achieving the greatest possible reduction in rate variability</i> when transmitting stored video to a client with given buffer size. We provide a formal proof of optimality, and demonstrate the performance of the algorithm on a set of long MPEG-1 encoded video traces. Second, we evaluate the impact of optimal smoothing on the network resources needed for video transport, under two network service models: Deterministic Guaranteed service [1, 9] and Renegotiated CBR (RCBR) service [8, 7]. Under both models, we find the impact of optimal smoothing to be dramatic.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {222--231},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233047},
 doi = {http://doi.acm.org/10.1145/233013.233047},
 acmid = {233047},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Varki:1996:ABF:233013.233048,
 author = {Varki, Elizabeth and Dowdy, Lawrence W.},
 title = {Analysis of balanced fork-join queueing networks},
 abstract = {This paper presents an analysis of closed, balanced, fork-join queueing networks with exponential service time distributions. The fork-join queue is mapped onto two non-parallel networks, namely, a serial-join model and a state-dependent model. Using these models, it is proven that the proportion of the number of jobs in the different subsystems of the fork-join queueing network remains constant, irrespective of the multiprogramming level. This property of balanced fork-join networks is used to compute quick, inexpensive bounds for arbitrary fork-join networks.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {232--241},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233048},
 doi = {http://doi.acm.org/10.1145/233013.233048},
 acmid = {233048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Varki:1996:ABF:233008.233048,
 author = {Varki, Elizabeth and Dowdy, Lawrence W.},
 title = {Analysis of balanced fork-join queueing networks},
 abstract = {This paper presents an analysis of closed, balanced, fork-join queueing networks with exponential service time distributions. The fork-join queue is mapped onto two non-parallel networks, namely, a serial-join model and a state-dependent model. Using these models, it is proven that the proportion of the number of jobs in the different subsystems of the fork-join queueing network remains constant, irrespective of the multiprogramming level. This property of balanced fork-join networks is used to compute quick, inexpensive bounds for arbitrary fork-join networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {232--241},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233048},
 doi = {http://doi.acm.org/10.1145/233008.233048},
 acmid = {233048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Carrasco:1996:EEA:233013.233049,
 author = {Carrasco, Juan A. and Escrib\'{a}, Javier and Calder\'{o}n, Angel},
 title = {Efficient exploration of availability models guided by failure distances},
 abstract = {Recently, a method to bound the steady-state availability using the failure distance concept has been proposed. In this paper we refine that method by introducing state space exploration techniques. In the methods proposed here, the state space is incrementally generated based on the contributions to the steady-state availability band of the states in the frontier of the currently generated state space. Several state space exploration algorithms are evaluated in terms of bounds quality and memory and CPU time requirements. The more efficient seems to be a waved algorithm which expands transition groups. We compare our new methods with the method based on the failure distance concept without state exploration and a method proposed by Souza e Silva and Ochoa which uses state space exploration but does not use the failure distance concept. Using typical examples we show that the methods proposed here can be significantly more efficient than any of the previous methods.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {242--251},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233049},
 doi = {http://doi.acm.org/10.1145/233013.233049},
 acmid = {233049},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carrasco:1996:EEA:233008.233049,
 author = {Carrasco, Juan A. and Escrib\'{a}, Javier and Calder\'{o}n, Angel},
 title = {Efficient exploration of availability models guided by failure distances},
 abstract = {Recently, a method to bound the steady-state availability using the failure distance concept has been proposed. In this paper we refine that method by introducing state space exploration techniques. In the methods proposed here, the state space is incrementally generated based on the contributions to the steady-state availability band of the states in the frontier of the currently generated state space. Several state space exploration algorithms are evaluated in terms of bounds quality and memory and CPU time requirements. The more efficient seems to be a waved algorithm which expands transition groups. We compare our new methods with the method based on the failure distance concept without state exploration and a method proposed by Souza e Silva and Ochoa which uses state space exploration but does not use the failure distance concept. Using typical examples we show that the methods proposed here can be significantly more efficient than any of the previous methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {242--251},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233049},
 doi = {http://doi.acm.org/10.1145/233008.233049},
 acmid = {233049},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Garg:1996:MCT:233008.233050,
 author = {Garg, Sachin and Huang, Yennun and Kintala, Chandra and Trivedi, Kishor S.},
 title = {Minimizing completion time of a program by checkpointing and rejuvenation},
 abstract = {Checkpointing with rollback-recovery is a well known technique to reduce the completion time of a program in the presence of failures. While checkpointing is corrective in nature, rejuvenation refers to preventive maintenance of software aimed to reduce unexpected failures mostly resulting from the "aging" phenomenon. In this paper, we show how both these techniques may be used together to further reduce the expected completion time of a program. The idea of using checkpoints to reduce the amount of rollback upon a failure is taken a step further by combining it with rejuvenation. We derive the equations for expected completion time of a program with finite failure free running time for the following three cases when; (a) neither checkpointing nor rejuvenation is employed, (b) only checkpointing is employed, and finally (c) both checkpointing and rejuvenation are employed.We also present numerical results for Weibull failure time distribution for the above three cases and discuss optimal checkpointing and rejuvenation that minimizes the expected completion time. Using the numerical results, some interesting conclusions are drawn about benefits of these techniques in relation to the nature of failure distribution.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {252--261},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233008.233050},
 doi = {http://doi.acm.org/10.1145/233008.233050},
 acmid = {233050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Garg:1996:MCT:233013.233050,
 author = {Garg, Sachin and Huang, Yennun and Kintala, Chandra and Trivedi, Kishor S.},
 title = {Minimizing completion time of a program by checkpointing and rejuvenation},
 abstract = {Checkpointing with rollback-recovery is a well known technique to reduce the completion time of a program in the presence of failures. While checkpointing is corrective in nature, rejuvenation refers to preventive maintenance of software aimed to reduce unexpected failures mostly resulting from the "aging" phenomenon. In this paper, we show how both these techniques may be used together to further reduce the expected completion time of a program. The idea of using checkpoints to reduce the amount of rollback upon a failure is taken a step further by combining it with rejuvenation. We derive the equations for expected completion time of a program with finite failure free running time for the following three cases when; (a) neither checkpointing nor rejuvenation is employed, (b) only checkpointing is employed, and finally (c) both checkpointing and rejuvenation are employed.We also present numerical results for Weibull failure time distribution for the above three cases and discuss optimal checkpointing and rejuvenation that minimizes the expected completion time. Using the numerical results, some interesting conclusions are drawn about benefits of these techniques in relation to the nature of failure distribution.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {252--261},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/233013.233050},
 doi = {http://doi.acm.org/10.1145/233013.233050},
 acmid = {233050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kimbrel:1996:IPP:233008.233052,
 author = {Kimbrel, Tracy and Cao, Pei and Felten, Edward W. and Karlin, Anna R. and Li, Kai},
 title = {Integrated parallel prefetching and caching},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {262--263},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233008.233052},
 doi = {http://doi.acm.org/10.1145/233008.233052},
 acmid = {233052},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kimbrel:1996:IPP:233013.233052,
 author = {Kimbrel, Tracy and Cao, Pei and Felten, Edward W. and Karlin, Anna R. and Li, Kai},
 title = {Integrated parallel prefetching and caching},
 abstract = {},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {262--263},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233013.233052},
 doi = {http://doi.acm.org/10.1145/233013.233052},
 acmid = {233052},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Leutenegger:1996:BME:233013.233054,
 author = {Leutenegger, Scott T. and Lopez, Mario A.},
 title = {A buffer model for evaluating R-tree performance},
 abstract = {},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233013.233054},
 doi = {http://doi.acm.org/10.1145/233013.233054},
 acmid = {233054},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Leutenegger:1996:BME:233008.233054,
 author = {Leutenegger, Scott T. and Lopez, Mario A.},
 title = {A buffer model for evaluating R-tree performance},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233008.233054},
 doi = {http://doi.acm.org/10.1145/233008.233054},
 acmid = {233054},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hellerstein:1996:ASM:233008.233055,
 author = {Hellerstein, Joseph L.},
 title = {An approach to selecting metrics for detecting performance problems in information systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233008.233055},
 doi = {http://doi.acm.org/10.1145/233008.233055},
 acmid = {233055},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hellerstein:1996:ASM:233013.233055,
 author = {Hellerstein, Joseph L.},
 title = {An approach to selecting metrics for detecting performance problems in information systems},
 abstract = {},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233013.233055},
 doi = {http://doi.acm.org/10.1145/233013.233055},
 acmid = {233055},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Courtright:1996:RRP:233008.233057,
 author = {Courtright,II, William V. and Gibson, Garth and Holland, Mark and Zelenka, Jim},
 title = {RAIDframe: rapid prototyping for disk arrays},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233008.233057},
 doi = {http://doi.acm.org/10.1145/233008.233057},
 acmid = {233057},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Courtright:1996:RRP:233013.233057,
 author = {Courtright,II, William V. and Gibson, Garth and Holland, Mark and Zelenka, Jim},
 title = {RAIDframe: rapid prototyping for disk arrays},
 abstract = {},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233013.233057},
 doi = {http://doi.acm.org/10.1145/233013.233057},
 acmid = {233057},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramany:1996:QAR:233013.233059,
 author = {Ramany, Swaminathan and Eager, Derek},
 title = {Quantifying achievable routing performance in multiprocessor interconnection networks},
 abstract = {},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233013.233059},
 doi = {http://doi.acm.org/10.1145/233013.233059},
 acmid = {233059},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramany:1996:QAR:233008.233059,
 author = {Ramany, Swaminathan and Eager, Derek},
 title = {Quantifying achievable routing performance in multiprocessor interconnection networks},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233008.233059},
 doi = {http://doi.acm.org/10.1145/233008.233059},
 acmid = {233059},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hotovy:1996:AEW:233013.233060,
 author = {Hotovy, Steven and Schneider, David and O'Donnell, Timothy},
 title = {Analysis of the early workload on the Cornell Theory Center IBM SP2},
 abstract = {Parallel computers have matured to the point where they are capable of running a significant production workload. Characterizing this workload, however, is far more complicated than for the single-processor case. Besides the varying number of processors that may be invoked, the nodes themselves may provide differing computational resources (memory size, for example). In addition, the batch schedulers may introduce further categories of service which must be considered in the analysis.The Cornell Theory Center (CTC) put a 512-node IBM SP2 system into production in early 1995. Extended traces of batch jobs began to be collected in mid-1995 when the usage base became sufficiently large. This paper offers an analysis of this early batch workload.},
 booktitle = {Proceedings of the 1996 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '96},
 year = {1996},
 isbn = {0-89791-793-6},
 location = {Philadelphia, Pennsylvania, United States},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233013.233060},
 doi = {http://doi.acm.org/10.1145/233013.233060},
 acmid = {233060},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hotovy:1996:AEW:233008.233060,
 author = {Hotovy, Steven and Schneider, David and O'Donnell, Timothy},
 title = {Analysis of the early workload on the Cornell Theory Center IBM SP2},
 abstract = {Parallel computers have matured to the point where they are capable of running a significant production workload. Characterizing this workload, however, is far more complicated than for the single-processor case. Besides the varying number of processors that may be invoked, the nodes themselves may provide differing computational resources (memory size, for example). In addition, the batch schedulers may introduce further categories of service which must be considered in the analysis.The Cornell Theory Center (CTC) put a 512-node IBM SP2 system into production in early 1995. Extended traces of batch jobs began to be collected in mid-1995 when the usage base became sufficiently large. This paper offers an analysis of this early batch workload.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {24},
 issue = {1},
 month = {May},
 year = {1996},
 issn = {0163-5999},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/233008.233060},
 doi = {http://doi.acm.org/10.1145/233008.233060},
 acmid = {233060},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Denning:1994:FL:183018.183020,
 author = {Denning, Peter J.},
 title = {The fifteenth level (keynote address)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/183018.183020},
 doi = {http://doi.acm.org/10.1145/183018.183020},
 acmid = {183020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Denning:1994:FL:183019.183020,
 author = {Denning, Peter J.},
 title = {The fifteenth level (keynote address)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/183019.183020},
 doi = {http://doi.acm.org/10.1145/183019.183020},
 acmid = {183020},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peris:1994:AIM:183019.183021,
 author = {Peris, Vinod G. J. and Squillante, Mark S. and Naik, Vijay K.},
 title = {Analysis of the impact of memory in distributed parallel processing systems},
 abstract = {We consider an important tradeoff between processor and memory allocation in distributed parallel processing systems. To study this tradeoff, we formulate stochastic models of parallel program behavior, distributed parallel processing environments and memory overheads incurred by parallel programs as a function of their processor allocation. A mathematical analysis of the models is developed, which includes the effects of contention for shared resources caused by paging activity. We conduct a detailed analysis of real large-scale scientific applications and use these results to parameterize our models. Our results show that memory overhead resulting from processor allocation decisions can have a significant effect on system performance in distributed parallel environments, strongly suggesting that memory considerations must be incorporated in the resource allocation policies for parallel systems. We also demonstrate the importance of the inter-locality miss ratio, which is introduced in this paper and analyzed for the first time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {5--18},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/183019.183021},
 doi = {http://doi.acm.org/10.1145/183019.183021},
 acmid = {183021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peris:1994:AIM:183018.183021,
 author = {Peris, Vinod G. J. and Squillante, Mark S. and Naik, Vijay K.},
 title = {Analysis of the impact of memory in distributed parallel processing systems},
 abstract = {We consider an important tradeoff between processor and memory allocation in distributed parallel processing systems. To study this tradeoff, we formulate stochastic models of parallel program behavior, distributed parallel processing environments and memory overheads incurred by parallel programs as a function of their processor allocation. A mathematical analysis of the models is developed, which includes the effects of contention for shared resources caused by paging activity. We conduct a detailed analysis of real large-scale scientific applications and use these results to parameterize our models. Our results show that memory overhead resulting from processor allocation decisions can have a significant effect on system performance in distributed parallel environments, strongly suggesting that memory considerations must be incorporated in the resource allocation policies for parallel systems. We also demonstrate the importance of the inter-locality miss ratio, which is introduced in this paper and analyzed for the first time.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {5--18},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/183018.183021},
 doi = {http://doi.acm.org/10.1145/183018.183021},
 acmid = {183021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McCann:1994:PAP:183018.183022,
 author = {McCann, Cathy and Zahorjan, John},
 title = {Processor allocation policies for message-passing parallel computers},
 abstract = {When multiple jobs compete for processing resources on a parallel computer, the operating system kernel's processor allocation policy determines how many and which processors to allocate to each. In this paper we investigate the issues involved in constructing a processor allocation policy for large scale, message-passing parallel computers supporting a scientific workload.
},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {19--32},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/183018.183022},
 doi = {http://doi.acm.org/10.1145/183018.183022},
 acmid = {183022},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McCann:1994:PAP:183019.183022,
 author = {McCann, Cathy and Zahorjan, John},
 title = {Processor allocation policies for message-passing parallel computers},
 abstract = {When multiple jobs compete for processing resources on a parallel computer, the operating system kernel's processor allocation policy determines how many and which processors to allocate to each. In this paper we investigate the issues involved in constructing a processor allocation policy for large scale, message-passing parallel computers supporting a scientific workload.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {19--32},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/183019.183022},
 doi = {http://doi.acm.org/10.1145/183019.183022},
 acmid = {183022},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiang:1994:UAC:183018.183023,
 author = {Chiang, Su-Hui and Mansharamani, Rajesh K. and Vernon, Mary K.},
 title = {Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies},
 abstract = {The performance potential of run-to-completion (RTC) parallel processor scheduling policies is investigated by examining whether (1) application execution rate characteristics such as average parallelism (avg) and processor working set (PWS) and/or (2) limited preemption can be used to improve the performance of these policies. We address the first question by comparing policies (previous as well as new) that differ only in whether or not they use execution rate characteristics and by examining a wider range of the workload parameter space than previous studies. We address the second question by comparing a simple two-level queueing policy with RTC scheduling in the second level queue against RTC policies that don't allow any preemption and against dynamic equiallocation(EQ).Using simulation to estimate mean response times we find that for promising RTC policies such as adaptive static partitioning (ASP) and shortest demand first (SDF), a maximum allocation constraint that is for all practical purposes independent of avg and pws provides greater and more consistent improvement in policy performance than using avg or pws. Also, under the assumption that job demand information is unavailable to the scheduler we show that the ASP-max policy outperforms all previous high performance RTC policies for workloads with coefficient of variation in processing requirement greater than one. Furthermore, a two-level queue that allows at most one preemption per job outperforms ASP-max but is not competitive with EQ.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {33--44},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/183018.183023},
 doi = {http://doi.acm.org/10.1145/183018.183023},
 acmid = {183023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chiang:1994:UAC:183019.183023,
 author = {Chiang, Su-Hui and Mansharamani, Rajesh K. and Vernon, Mary K.},
 title = {Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies},
 abstract = {The performance potential of run-to-completion (RTC) parallel processor scheduling policies is investigated by examining whether (1) application execution rate characteristics such as average parallelism (avg) and processor working set (PWS) and/or (2) limited preemption can be used to improve the performance of these policies. We address the first question by comparing policies (previous as well as new) that differ only in whether or not they use execution rate characteristics and by examining a wider range of the workload parameter space than previous studies. We address the second question by comparing a simple two-level queueing policy with RTC scheduling in the second level queue against RTC policies that don't allow any preemption and against dynamic equiallocation(EQ).Using simulation to estimate mean response times we find that for promising RTC policies such as adaptive static partitioning (ASP) and shortest demand first (SDF), a maximum allocation constraint that is for all practical purposes independent of avg and pws provides greater and more consistent improvement in policy performance than using avg or pws. Also, under the assumption that job demand information is unavailable to the scheduler we show that the ASP-max policy outperforms all previous high performance RTC policies for workloads with coefficient of variation in processing requirement greater than one. Furthermore, a two-level queue that allows at most one preemption per job outperforms ASP-max but is not competitive with EQ.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {33--44},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/183019.183023},
 doi = {http://doi.acm.org/10.1145/183019.183023},
 acmid = {183023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolf:1994:SMQ:183018.183024,
 author = {Wolf, Joel L. and Turek, John and Chen, Ming-Syan and Yu, Philip S.},
 title = {Scheduling multiple queries on a parallel machine},
 abstract = {There has been a good deal of progress made recently towards the efficient parallelization of individual phases of single queries in multiprocessor database systems. In this paper we devise and evaluate a number of scheduling algorithms designed to handle multiple parallel queries. One of these algorithms emerges as a clear winner. This algorithm  is hierarchical in nature: In the first phase, a good quality precedence-based schedule is created for each individual query and each possible number of processors. This component employs dynamic programming. In the second phase, the results of the first phase are used to create an overall schedule of the full set of queries. This component is based on previously published work on nonprecedence-based malleable scheduling. Even though the  problem we are considering is NP-hard in the strong sense, the multiple query schedules generated by our hierarchical algorithm are seen experimentally to achieve results which are close to optimal.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {45--55},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183018.183024},
 doi = {http://doi.acm.org/10.1145/183018.183024},
 acmid = {183024},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolf:1994:SMQ:183019.183024,
 author = {Wolf, Joel L. and Turek, John and Chen, Ming-Syan and Yu, Philip S.},
 title = {Scheduling multiple queries on a parallel machine},
 abstract = {There has been a good deal of progress made recently towards the efficient parallelization of individual phases of single queries in multiprocessor database systems. In this paper we devise and evaluate a number of scheduling algorithms designed to handle multiple parallel queries. One of these algorithms emerges as a clear winner. This algorithm  is hierarchical in nature: In the first phase, a good quality precedence-based schedule is created for each individual query and each possible number of processors. This component employs dynamic programming. In the second phase, the results of the first phase are used to create an overall schedule of the full set of queries. This component is based on previously published work on nonprecedence-based malleable scheduling. Even though the  problem we are considering is NP-hard in the strong sense, the multiple query schedules generated by our hierarchical algorithm are seen experimentally to achieve results which are close to optimal.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {45--55},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183019.183024},
 doi = {http://doi.acm.org/10.1145/183019.183024},
 acmid = {183024},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Patel:1994:AMH:183018.183025,
 author = {Patel, Jignesh M. and Carey, Michael J. and Vernon, Mary K.},
 title = {Accurate modeling of the hybrid hash join algorithm},
 abstract = {The join of two relations is an important operation in database systems. It occurs frequently in relational queries, and join performance is a significant factor in overall system performance. Cost models for join algorithms are used by query optimizers to choose efficient query execution strategies. This paper presents an efficient analytical model of an important join method, the hybrid hash join algorithm, that captures several key features of the algorithm's performance\&mdash;including its intra-operator parallelism, interference between disk reads and writes, caching of disk pages, and placement of data on disk(s). Validation of the model against a detailed simulation of a database system shows that the response time estimates produced by the model are quite accurate.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {56--66},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183018.183025},
 doi = {http://doi.acm.org/10.1145/183018.183025},
 acmid = {183025},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Patel:1994:AMH:183019.183025,
 author = {Patel, Jignesh M. and Carey, Michael J. and Vernon, Mary K.},
 title = {Accurate modeling of the hybrid hash join algorithm},
 abstract = {The join of two relations is an important operation in database systems. It occurs frequently in relational queries, and join performance is a significant factor in overall system performance. Cost models for join algorithms are used by query optimizers to choose efficient query execution strategies. This paper presents an efficient analytical model of an important join method, the hybrid hash join algorithm, that captures several key features of the algorithm's performance\&mdash;including its intra-operator parallelism, interference between disk reads and writes, caching of disk pages, and placement of data on disk(s). Validation of the model against a detailed simulation of a database system shows that the response time estimates produced by the model are quite accurate.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {56--66},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183019.183025},
 doi = {http://doi.acm.org/10.1145/183019.183025},
 acmid = {183025},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bittan:1994:APB:183018.183026,
 author = {Bittan, Avi and Kogan, Yaakov and Yu, Philip S.},
 title = {Asymptotic performance of a buffer model in a data sharing environment},
 abstract = {The performance of a transaction processing system is very sensitive to the buffer hit probability. In a data sharing environment where multiple computing nodes are coupled together with direct access to shared data on disks, buffer coherency needs to be maintained such that if a data granule is updated by a node, the old copies of this granule present in the buffer of other nodes must be invalidated. The buffer invalidation phenomenon reduces the buffer hit probability in a multi-node environment. After the buffer reaches a certain size, the buffer hit probability will remain constant regardless of further increase in buffer size due to the buffer invalidation effect. This puts an upper limit on the achievable buffer hit probability. Thus the selection of appropriate buffer size is  one of the critical issues in a data sharing environment. In this paper, we develop an asymptotic analysis of the Markov model for a buffer in the data sharing environment. Important relations between buffer size, number of nodes, write-probability and the size of the database to the buffer hit probability had been found in all range of system parameters. A simple expression is obtained for the maximum achievable buffer hit probability and also for the maximum usable buffer size. Various properties of the maximum achievable buffer hit probability and usable buffer size are derived for a skewed access workload. The accuracy of the asymptotic method is validated by numerous case studies.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {67--76},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183026},
 doi = {http://doi.acm.org/10.1145/183018.183026},
 acmid = {183026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bittan:1994:APB:183019.183026,
 author = {Bittan, Avi and Kogan, Yaakov and Yu, Philip S.},
 title = {Asymptotic performance of a buffer model in a data sharing environment},
 abstract = {The performance of a transaction processing system is very sensitive to the buffer hit probability. In a data sharing environment where multiple computing nodes are coupled together with direct access to shared data on disks, buffer coherency needs to be maintained such that if a data granule is updated by a node, the old copies of this granule present in the buffer of other nodes must be invalidated. The buffer invalidation phenomenon reduces the buffer hit probability in a multi-node environment. After the buffer reaches a certain size, the buffer hit probability will remain constant regardless of further increase in buffer size due to the buffer invalidation effect. This puts an upper limit on the achievable buffer hit probability. Thus the selection of appropriate buffer size is  one of the critical issues in a data sharing environment. In this paper, we develop an asymptotic analysis of the Markov model for a buffer in the data sharing environment. Important relations between buffer size, number of nodes, write-probability and the size of the database to the buffer hit probability had been found in all range of system parameters. A simple expression is obtained for the maximum achievable buffer hit probability and also for the maximum usable buffer size. Various properties of the maximum achievable buffer hit probability and usable buffer size are derived for a skewed access workload. The accuracy of the asymptotic method is validated by numerous case studies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {67--76},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183026},
 doi = {http://doi.acm.org/10.1145/183019.183026},
 acmid = {183026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Petriu:1994:AMV:183019.183027,
 author = {Petriu, Dorina C.},
 title = {Approximate mean value analysis of client-server systems with multi-class requests},
 abstract = {Stochastic Rendezvous Networks (SRVNs) are performance models for multitasking parallel software with intertask communication via rendezvous introduced in [1], which are very appropriate to model client-server systems. SRVNs differ from Queueing Networks (QNs) in two ways: nodes act as both clients and servers (allowing for nested service), and servers have two distinct phases of service\&mdash;the first one ``in RV" with the client, and the second ``after RV", executed in parallel with the client. Early work on solving SRVN models has used a kind of approximate Mean Value Analysis based on heuristic ad hoc assumptions to determine the task queue properties at the instant of RV request arrivals. Approximation are necessary since  SRVN violates product form. Recently, a more rigorous approach was proposed in [2] for the solution of SRVN models, based on a special aggregation (named ``Task-Directed Aggregation" TDA) of the Markov chain model describing the interference of different clients that contend for a single server with FIFO queueing discipline and different service times. The algorithm derived in [2] has the limitation that each client may require only a single class of service. In general, a software server offers a range of services with different workloads and functionalities, and a client may need more than one service. The present paper uses the TDA approach to derive an extended algorithm which allows a client to require any number of services from a server by changing randomly the request  class. The new algorithm is incorporated into a decomposition method for models with any number of servers. The SRVN modelling technique is applied to a large case study of a distributed database system, giving insight into the behaviour of the system and helping to identify performance problems such as software bottle-neck.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {77--86},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183027},
 doi = {http://doi.acm.org/10.1145/183019.183027},
 acmid = {183027},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Petriu:1994:AMV:183018.183027,
 author = {Petriu, Dorina C.},
 title = {Approximate mean value analysis of client-server systems with multi-class requests},
 abstract = {Stochastic Rendezvous Networks (SRVNs) are performance models for multitasking parallel software with intertask communication via rendezvous introduced in [1], which are very appropriate to model client-server systems. SRVNs differ from Queueing Networks (QNs) in two ways: nodes act as both clients and servers (allowing for nested service), and servers have two distinct phases of service\&mdash;the first one ``in RV" with the client, and the second ``after RV", executed in parallel with the client. Early work on solving SRVN models has used a kind of approximate Mean Value Analysis based on heuristic ad hoc assumptions to determine the task queue properties at the instant of RV request arrivals. Approximation are necessary since  SRVN violates product form. Recently, a more rigorous approach was proposed in [2] for the solution of SRVN models, based on a special aggregation (named ``Task-Directed Aggregation" TDA) of the Markov chain model describing the interference of different clients that contend for a single server with FIFO queueing discipline and different service times. The algorithm derived in [2] has the limitation that each client may require only a single class of service. In general, a software server offers a range of services with different workloads and functionalities, and a client may need more than one service. The present paper uses the TDA approach to derive an extended algorithm which allows a client to require any number of services from a server by changing randomly the request  class. The new algorithm is incorporated into a decomposition method for models with any number of servers. The SRVN modelling technique is applied to a large case study of a distributed database system, giving insight into the behaviour of the system and helping to identify performance problems such as software bottle-neck.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {77--86},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183027},
 doi = {http://doi.acm.org/10.1145/183018.183027},
 acmid = {183027},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Balbo:1994:ATP:183019.183028,
 author = {Balbo, G. and Bruell, S. C. and Sereno, M.},
 title = {Arrival theorems for product-form stochastic Petri nets},
 abstract = {We consider a particular class of Stochastic Petri Nets whose stationary probabilities at arbitrary instants exhibit a product form. We study these nets at specific instants in the steady state that occur directly after the firing of a transition. We focus our attention on the instant after tokens are removed from the places specified by a transition's input bag and just before tokens are entered into the places specified by the same transition's output bag. We show that the stationary probabilities at ``arrival instants" are related to corresponding stationary probabilities at arbitrary instants in net(s) with lower load. We then show how one of the ``arrival" theorems can be applied to the derivation of a formula for the mean  sojourn time of a token in a place at steady state. This is the basis for the development of a Mean Value Analysis algorithm for the computation of performance indices for Product-Form Stochastic Petri Nets.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {87--97},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183019.183028},
 doi = {http://doi.acm.org/10.1145/183019.183028},
 acmid = {183028},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Balbo:1994:ATP:183018.183028,
 author = {Balbo, G. and Bruell, S. C. and Sereno, M.},
 title = {Arrival theorems for product-form stochastic Petri nets},
 abstract = {We consider a particular class of Stochastic Petri Nets whose stationary probabilities at arbitrary instants exhibit a product form. We study these nets at specific instants in the steady state that occur directly after the firing of a transition. We focus our attention on the instant after tokens are removed from the places specified by a transition's input bag and just before tokens are entered into the places specified by the same transition's output bag. We show that the stationary probabilities at ``arrival instants" are related to corresponding stationary probabilities at arbitrary instants in net(s) with lower load. We then show how one of the ``arrival" theorems can be applied to the derivation of a formula for the mean  sojourn time of a token in a place at steady state. This is the basis for the development of a Mean Value Analysis algorithm for the computation of performance indices for Product-Form Stochastic Petri Nets.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {87--97},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183018.183028},
 doi = {http://doi.acm.org/10.1145/183018.183028},
 acmid = {183028},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Somani:1994:PSA:183018.183029,
 author = {Somani, Arun K. and Trivedi, Kishor S.},
 title = {Phased-mission system analysis using Boolean algebraic methods},
 abstract = {Most reliability analysis techniques and tools assume that a system is used for a mission consisting of a single phase. However, multiple phases are natural in many missions. The failure rates of components, system configuration, and success criteria may vary from phase to phase. In addition, the duration of a phase may be deterministic or random. Recently, several researchers have addressed the problem of reliability analysis of such systems using a variety of methods. We describe a new technique for phased-mission system reliability analysis based on Boolean algebraic methods. Our technique is computationally efficient and is applicable to a large class of systems for which the failure criterion in each phase can be expressed as a fault tree (or an equivalent representation). Our technique avoids state space explosion that commonly plague Markov chain-based analysis. We develop a phase algebra to account for the effects of variable configurations and success criteria from phase to phase. Our technique yields exact (as opposed to approximate) results. We demonstrate the use of our technique by means of an example and present numerical results to show the effects of mission phases on the system reliability.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183029},
 doi = {http://doi.acm.org/10.1145/183018.183029},
 acmid = {183029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Boolean algebraic methods, fault trees, phased-mission systems, random phase duration, reconfiguration, reliability analysis, ultra-reliable computer system, variable success criteria},
} 

@article{Somani:1994:PSA:183019.183029,
 author = {Somani, Arun K. and Trivedi, Kishor S.},
 title = {Phased-mission system analysis using Boolean algebraic methods},
 abstract = {Most reliability analysis techniques and tools assume that a system is used for a mission consisting of a single phase. However, multiple phases are natural in many missions. The failure rates of components, system configuration, and success criteria may vary from phase to phase. In addition, the duration of a phase may be deterministic or random. Recently, several researchers have addressed the problem of reliability analysis of such systems using a variety of methods. We describe a new technique for phased-mission system reliability analysis based on Boolean algebraic methods. Our technique is computationally efficient and is applicable to a large class of systems for which the failure criterion in each phase can be expressed as a fault tree (or an equivalent representation). Our technique avoids state space explosion that commonly plague Markov chain-based analysis. We develop a phase algebra to account for the effects of variable configurations and success criteria from phase to phase. Our technique yields exact (as opposed to approximate) results. We demonstrate the use of our technique by means of an example and present numerical results to show the effects of mission phases on the system reliability.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183029},
 doi = {http://doi.acm.org/10.1145/183019.183029},
 acmid = {183029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Boolean algebraic methods, fault trees, phased-mission systems, random phase duration, reconfiguration, reliability analysis, ultra-reliable computer system, variable success criteria},
} 

@article{Ebling:1994:SEF:183019.183030,
 author = {Ebling, Maria R. and Satyanarayanan, M.},
 title = {SynRGen: an extensible file reference generator},
 abstract = {SynRGen, a synthetic file reference generator operating at the system call level, is capable of modeling a wide variety of usage environments. It achieves realism through trace-inspired micromodels and flexibility by combining these micromodels stochastically. A micromodel is a parameterized piece of code that captures the distinctive signature of an application. We have used SynRGen extensively for stress testing the Coda File System. We have also performed a controlled experiment that demonstrates SynRGen's ability to closely emulate real users\&mdash;within 20\% of many key system variables. In this paper we present the rationale, detailed design, and evaluation of SynRGen, and mention its applicability to broader uses such as performance evaluation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183030},
 doi = {http://doi.acm.org/10.1145/183019.183030},
 acmid = {183030},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ebling:1994:SEF:183018.183030,
 author = {Ebling, Maria R. and Satyanarayanan, M.},
 title = {SynRGen: an extensible file reference generator},
 abstract = {SynRGen, a synthetic file reference generator operating at the system call level, is capable of modeling a wide variety of usage environments. It achieves realism through trace-inspired micromodels and flexibility by combining these micromodels stochastically. A micromodel is a parameterized piece of code that captures the distinctive signature of an application. We have used SynRGen extensively for stress testing the Coda File System. We have also performed a controlled experiment that demonstrates SynRGen's ability to closely emulate real users\&mdash;within 20\% of many key system variables. In this paper we present the rationale, detailed design, and evaluation of SynRGen, and mention its applicability to broader uses such as performance evaluation.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183030},
 doi = {http://doi.acm.org/10.1145/183018.183030},
 acmid = {183030},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raghavan:1994:GNM:183019.183031,
 author = {Raghavan, S. V. and Vasukiammaiyar, D. and Haring, Gunter},
 title = {Generative networkload models for a single server environment},
 abstract = {Any performance evaluation study requires a concise description of the workload under which the performance of the system is to be evaluated. Also, the repeatability of the experiments for different workload profiles, requires that the workload models generate the workload profiles parametrically. Such a model, should preferably be time-invariant, consistent and generative. We view the networkload as a sequence that can be generated from the rules of a Context Free Grammar (CFG). Our approach combines the established practice of viewing the workload as ``consisting of a hierarchy" and the CFG description, to produce a generative networkload model. The networkload model is applied to a SingleServer-MultipleClients network by deriving the networkload model parameters from an operational SingleServer network of personal computers. The time-invariance and generative nature are verified experimentally. The usefulness of such a description of the networkload to study the resource management problems of a network, like the optimal allocation of clients to servers, is explored by using the generative model as input descriptor to a queueing network model of SingleServer network.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {118--127},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183031},
 doi = {http://doi.acm.org/10.1145/183019.183031},
 acmid = {183031},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Raghavan:1994:GNM:183018.183031,
 author = {Raghavan, S. V. and Vasukiammaiyar, D. and Haring, Gunter},
 title = {Generative networkload models for a single server environment},
 abstract = {Any performance evaluation study requires a concise description of the workload under which the performance of the system is to be evaluated. Also, the repeatability of the experiments for different workload profiles, requires that the workload models generate the workload profiles parametrically. Such a model, should preferably be time-invariant, consistent and generative. We view the networkload as a sequence that can be generated from the rules of a Context Free Grammar (CFG). Our approach combines the established practice of viewing the workload as ``consisting of a hierarchy" and the CFG description, to produce a generative networkload model. The networkload model is applied to a SingleServer-MultipleClients network by deriving the networkload model parameters from an operational SingleServer network of personal computers. The time-invariance and generative nature are verified experimentally. The usefulness of such a description of the networkload to study the resource management problems of a network, like the optimal allocation of clients to servers, is explored by using the generative model as input descriptor to a queueing network model of SingleServer network.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {118--127},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183031},
 doi = {http://doi.acm.org/10.1145/183018.183031},
 acmid = {183031},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cmelik:1994:SFI:183019.183032,
 author = {Cmelik, Bob and Keppel, David},
 title = {Shade: a fast instruction-set simulator for execution profiling},
 abstract = {Tracing tools are used widely to help analyze, design, and tune both hardware and software systems. This paper describes a tool called Shade which combines efficient instruction-set simulation with a flexible, extensible trace generation capability. Efficiency is achieved by dynamically compiling and caching code to simulate and trace the application program. The user may control the extent of tracing in a variety of ways; arbitrarily detailed application state information may be collected during the simulation, but tracing less translates directly into greater efficiency. Current Shade implementations run on SPARC systems and simulate the SPARC (Versions 8 and 9) and MIPS I instruction sets. This paper describes the capabilities, design, implementation, and performance of Shade, and discusses instruction set emulation in general.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {128--137},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183032},
 doi = {http://doi.acm.org/10.1145/183019.183032},
 acmid = {183032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cmelik:1994:SFI:183018.183032,
 author = {Cmelik, Bob and Keppel, David},
 title = {Shade: a fast instruction-set simulator for execution profiling},
 abstract = {Tracing tools are used widely to help analyze, design, and tune both hardware and software systems. This paper describes a tool called Shade which combines efficient instruction-set simulation with a flexible, extensible trace generation capability. Efficiency is achieved by dynamically compiling and caching code to simulate and trace the application program. The user may control the extent of tracing in a variety of ways; arbitrarily detailed application state information may be collected during the simulation, but tracing less translates directly into greater efficiency. Current Shade implementations run on SPARC systems and simulate the SPARC (Versions 8 and 9) and MIPS I instruction sets. This paper describes the capabilities, design, implementation, and performance of Shade, and discusses instruction set emulation in general.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {128--137},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183032},
 doi = {http://doi.acm.org/10.1145/183018.183032},
 acmid = {183032},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Noble:1994:ESH:183019.183033,
 author = {Noble, Brian D. and Satyanarayanan, M.},
 title = {An empirical study of a highly available file system},
 abstract = {In this paper we present results from a six-month empirical study of the high availability aspects of the Coda File System. We report on the service failures experienced by Coda clients, and show that such failures are masked successfully. We also explore the effectiveness and resource costs of key aspects of server replication and disconnected operation, the two high availability mechanisms of Coda. Wherever possible, we compare our measurements to simulation-based predictions from earlier papers and to anecdotal evidence from users. Finally, we explore how users take advantage of the support provided by Coda for mobile computing.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {138--149},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/183019.183033},
 doi = {http://doi.acm.org/10.1145/183019.183033},
 acmid = {183033},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Noble:1994:ESH:183018.183033,
 author = {Noble, Brian D. and Satyanarayanan, M.},
 title = {An empirical study of a highly available file system},
 abstract = {In this paper we present results from a six-month empirical study of the high availability aspects of the Coda File System. We report on the service failures experienced by Coda clients, and show that such failures are masked successfully. We also explore the effectiveness and resource costs of key aspects of server replication and disconnected operation, the two high availability mechanisms of Coda. Wherever possible, we compare our measurements to simulation-based predictions from earlier papers and to anecdotal evidence from users. Finally, we explore how users take advantage of the support provided by Coda for mobile computing.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {138--149},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/183018.183033},
 doi = {http://doi.acm.org/10.1145/183018.183033},
 acmid = {183033},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dahlin:1994:QAC:183018.183034,
 author = {Dahlin, Michael D. and Mather, Clifford J. and Wang, Randolph Y. and Anderson, Thomas E. and Patterson, David A.},
 title = {A quantitative analysis of cache policies for scalable network file systems},
 abstract = {Current network file system protocols rely heavily on a central server to coordinate file activity among client workstations. This central server can become a bottleneck that limits scalability for environments with large numbers of clients. In central server systems such as NFS and AFS, all client writes, cache misses, and coherence messages are handled by the server. To keep up with this workload, expensive server machines are needed, configured with high-performance CPUs, memory systems, and I/O channels. Since the server stores all data, it must be physically capable of connecting to many disks. This reliance on a central server also makes current systems inappropriate for wide area network use where the network bandwidth to the server may be limited.In this paper, we  investigate the quantitative performance effect of moving as many of the server responsibilities as possible to client workstations to reduce the need for high-performance server machines. We have devised a cache protocol in which all data reside on clients and all data transfers proceed directly from client to client. The server is used only to coordinate these data transfers. This protocol is being incorporated as part of our experimental file system, xFS. We present results from a trace-driven simulation study of the protocol using traces from a 237 client NFS installation. We find that the xFS protocol reduces server load by more than a factor of six compared to AFS without significantly affecting response time or file availability.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {150--160},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183018.183034},
 doi = {http://doi.acm.org/10.1145/183018.183034},
 acmid = {183034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dahlin:1994:QAC:183019.183034,
 author = {Dahlin, Michael D. and Mather, Clifford J. and Wang, Randolph Y. and Anderson, Thomas E. and Patterson, David A.},
 title = {A quantitative analysis of cache policies for scalable network file systems},
 abstract = {Current network file system protocols rely heavily on a central server to coordinate file activity among client workstations. This central server can become a bottleneck that limits scalability for environments with large numbers of clients. In central server systems such as NFS and AFS, all client writes, cache misses, and coherence messages are handled by the server. To keep up with this workload, expensive server machines are needed, configured with high-performance CPUs, memory systems, and I/O channels. Since the server stores all data, it must be physically capable of connecting to many disks. This reliance on a central server also makes current systems inappropriate for wide area network use where the network bandwidth to the server may be limited.In this paper, we  investigate the quantitative performance effect of moving as many of the server responsibilities as possible to client workstations to reduce the need for high-performance server machines. We have devised a cache protocol in which all data reside on clients and all data transfers proceed directly from client to client. The server is used only to coordinate these data transfers. This protocol is being incorporated as part of our experimental file system, xFS. We present results from a trace-driven simulation study of the protocol using traces from a 237 client NFS installation. We find that the xFS protocol reduces server load by more than a factor of six compared to AFS without significantly affecting response time or file availability.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {150--160},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183019.183034},
 doi = {http://doi.acm.org/10.1145/183019.183034},
 acmid = {183034},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kotz:1994:ELL:183019.183036,
 author = {Kotz, David and Crow, Preston},
 title = {The expected lifetime of \&ldquo;single-address-space\&rdquo; operating systems},
 abstract = {Trends toward shared-memory programming paradigms, large (64-bit) address spaces, and memory-mapped files have led some to propose the use of a single virtual-address space, shared by all processes and processors. Typical proposals require the single address space to contain all process-private data, shared data, and stored files. To simplify management of an address space where stable pointers make it difficult to re-use addresses, some have claimed that a 64-bit address space is sufficiently large that there is no need to ever re-use addresses. Unfortunately, there has been no data to either support or refute these claims, or to aid in the design of appropriate address-space management policies. In this paper, we present the results of extensive kernel-level tracing of the  workstations in our department, and discuss the implications for single-address-space operating systems. We found that single-address-space systems will not outgrow the available address space, but only if reasonable space-allocation policies are used, and only if the system can adapt as larger address space becomes available.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183036},
 doi = {http://doi.acm.org/10.1145/183019.183036},
 acmid = {183036},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kotz:1994:ELL:183018.183036,
 author = {Kotz, David and Crow, Preston},
 title = {The expected lifetime of \&ldquo;single-address-space\&rdquo; operating systems},
 abstract = {Trends toward shared-memory programming paradigms, large (64-bit) address spaces, and memory-mapped files have led some to propose the use of a single virtual-address space, shared by all processes and processors. Typical proposals require the single address space to contain all process-private data, shared data, and stored files. To simplify management of an address space where stable pointers make it difficult to re-use addresses, some have claimed that a 64-bit address space is sufficiently large that there is no need to ever re-use addresses. Unfortunately, there has been no data to either support or refute these claims, or to aid in the design of appropriate address-space management policies. In this paper, we present the results of extensive kernel-level tracing of the  workstations in our department, and discuss the implications for single-address-space operating systems. We found that single-address-space systems will not outgrow the available address space, but only if reasonable space-allocation policies are used, and only if the system can adapt as larger address space becomes available.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183036},
 doi = {http://doi.acm.org/10.1145/183018.183036},
 acmid = {183036},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sivasubramaniam:1994:ASS:183019.183038,
 author = {Sivasubramaniam, Anand and Singla, Aman and Ramachandran, Umakishore and Venkateswaran, H.},
 title = {An approach to scalability study of shared memory parallel systems},
 abstract = {The overheads in a parallel system that limit its scalability need to be identified and separated in order to enable parallel algorithm design and the development of parallel machines. Such overheads may be broadly classified into two components. The first one is intrinsic to the algorithm and arises due to factors such as the work-imbalance and the serial fraction. The second one is due to the interaction between the algorithm and the architecture and arises due to latency and contention in the network. A top-down approach to scalability study of shared memory parallel systems is proposed in this research. We define the notion of overhead functions associated with the different algorithmic and architectural characteristics to quantify the scalability of parallel systems; we isolate the algorithmic overhead and the overheads due to network latency and contention from the overall execution time of an application; we design and implement an execution-driven simulation platform that incorporates these methods for quantifying the overhead functions; and we use this simulator to study the scalability characteristics of five applications on shared memory platforms with different communication topologies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183038},
 doi = {http://doi.acm.org/10.1145/183019.183038},
 acmid = {183038},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sivasubramaniam:1994:ASS:183018.183038,
 author = {Sivasubramaniam, Anand and Singla, Aman and Ramachandran, Umakishore and Venkateswaran, H.},
 title = {An approach to scalability study of shared memory parallel systems},
 abstract = {The overheads in a parallel system that limit its scalability need to be identified and separated in order to enable parallel algorithm design and the development of parallel machines. Such overheads may be broadly classified into two components. The first one is intrinsic to the algorithm and arises due to factors such as the work-imbalance and the serial fraction. The second one is due to the interaction between the algorithm and the architecture and arises due to latency and contention in the network. A top-down approach to scalability study of shared memory parallel systems is proposed in this research. We define the notion of overhead functions associated with the different algorithmic and architectural characteristics to quantify the scalability of parallel systems; we isolate the algorithmic overhead and the overheads due to network latency and contention from the overall execution time of an application; we design and implement an execution-driven simulation platform that incorporates these methods for quantifying the overhead functions; and we use this simulator to study the scalability characteristics of five applications on shared memory platforms with different communication topologies.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183038},
 doi = {http://doi.acm.org/10.1145/183018.183038},
 acmid = {183038},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mehra:1994:CTM:183018.183039,
 author = {Mehra, Pankaj and Schulbach, Catherine H. and Yan, Jerry C.},
 title = {A comparison of two model-based performance-prediction techniques for message-passing parallel programs},
 abstract = {This paper describes our experience in modeling two significant parallel applications: ARC2D, a 2-dimensional Euler solver; and, Xtrid, a tridiagonal linear solver. Both of these models were expressed in BDL (Behavior Description language) and simulated on an iPSC/860 Hypercube modeled using Axe (Abstract eXecution Environment). BDL models consist of abstract communicating objects: blocks of sequential code are modeled by single RUN statements; all communication operations in the original code are mirrored by corresponding BDL operations in the model. Our ARC2D model was built by first profiling the program to locate the significant loops and then timing the basic blocks within those loops. Simulated completion times were (except in one case) within 8\% of measured execution times. Lengthy simulations were necessary for predicting the performance of large-scale runs. For Xtrid, only the loops surrounding communications were modeled; other loops were absorbed into large sequential blocks whose complexity was estimated using statistical regression. This approach yielded a much smaller model whose computation and communication complexities were clearly manifest. Analysis of complexity allowed rapid prediction of large-scale performance without lengthy simulations! Analytically predicted speed-ups were within 7\% of those predicted by simulation. Simulated completion times were within 5\% of measured execution times. The second approach provides a more effective methodology for simulation-based performance-tuning.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183039},
 doi = {http://doi.acm.org/10.1145/183018.183039},
 acmid = {183039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mehra:1994:CTM:183019.183039,
 author = {Mehra, Pankaj and Schulbach, Catherine H. and Yan, Jerry C.},
 title = {A comparison of two model-based performance-prediction techniques for message-passing parallel programs},
 abstract = {This paper describes our experience in modeling two significant parallel applications: ARC2D, a 2-dimensional Euler solver; and, Xtrid, a tridiagonal linear solver. Both of these models were expressed in BDL (Behavior Description language) and simulated on an iPSC/860 Hypercube modeled using Axe (Abstract eXecution Environment). BDL models consist of abstract communicating objects: blocks of sequential code are modeled by single RUN statements; all communication operations in the original code are mirrored by corresponding BDL operations in the model. Our ARC2D model was built by first profiling the program to locate the significant loops and then timing the basic blocks within those loops. Simulated completion times were (except in one case) within 8\% of measured execution times. Lengthy simulations were necessary for predicting the performance of large-scale runs. For Xtrid, only the loops surrounding communications were modeled; other loops were absorbed into large sequential blocks whose complexity was estimated using statistical regression. This approach yielded a much smaller model whose computation and communication complexities were clearly manifest. Analysis of complexity allowed rapid prediction of large-scale performance without lengthy simulations! Analytically predicted speed-ups were within 7\% of those predicted by simulation. Simulated completion times were within 5\% of measured execution times. The second approach provides a more effective methodology for simulation-based performance-tuning.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183039},
 doi = {http://doi.acm.org/10.1145/183019.183039},
 acmid = {183039},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Horton:1994:MSA:183018.183040,
 author = {Horton, Graham and Leutenegger, Scott T.},
 title = {A multi-level solution algorithm for steady-state Markov chains},
 abstract = {A new iterative algorithm, the multi-level algorithm, for the numerical solution of steady state Markov chains is presented. The method utilizes a set of recursively coarsened representations of the original system to achieve accelerated convergence. It is motivated by multigrid methods, which are widely used for fast solution of partial differential equations. Initial results of numerical experiments are reported, showing significant reductions in computation time, often an order of magnitude or more, relative to the Gauss-Seidel and optimal SOR algorithms for a variety of test problems. It is shown how the well-known iterative aggregation-disaggregation algorithm of Takahashi can be interpreted as a special case of the new method.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {191--200},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183040},
 doi = {http://doi.acm.org/10.1145/183018.183040},
 acmid = {183040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Horton:1994:MSA:183019.183040,
 author = {Horton, Graham and Leutenegger, Scott T.},
 title = {A multi-level solution algorithm for steady-state Markov chains},
 abstract = {A new iterative algorithm, the multi-level algorithm, for the numerical solution of steady state Markov chains is presented. The method utilizes a set of recursively coarsened representations of the original system to achieve accelerated convergence. It is motivated by multigrid methods, which are widely used for fast solution of partial differential equations. Initial results of numerical experiments are reported, showing significant reductions in computation time, often an order of magnitude or more, relative to the Gauss-Seidel and optimal SOR algorithms for a variety of test problems. It is shown how the well-known iterative aggregation-disaggregation algorithm of Takahashi can be interpreted as a special case of the new method.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {191--200},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183040},
 doi = {http://doi.acm.org/10.1145/183019.183040},
 acmid = {183040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Das:1994:AMM:183018.183041,
 author = {Das, Samir R. and Fujimoto, Richard M.},
 title = {An adaptive memory management protocol for Time Warp parallel simulation},
 abstract = {It is widely believed that Time Warp is prone to two potential problems: an excessive amount of wasted, rolled back computation resulting from ``rollback thrashing" behaviors, and inefficient use of memory, leading to poor performance of virtual memory and/or multiprocessor cache systems. An adaptive mechanism is proposed based on the Cancelback memory management protocol that dynamically controls the amount of memory used in the simulation in order to maximize performance. The proposed mechanism is adaptive in the sense that it monitors the execution of the Time Warp program, automatically adjusts the amount of memory used to reduce Time Warp overheads (fossil collection, Cancelback, the amount of rolled back computation, etc.) to a manageable level. The mechanism is based on a model that characterizes the behavior of Time Warp programs in terms of the flow of memory buffers among different buffer pools. We demonstrate that an implementation of the adaptive mechanism on a Kendall Square Research KSR-1 multiprocessor is effective in automatically maximizing performance while minimizing memory utilization of Time Warp programs, even for dynamically changing simulation models.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {201--210},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183041},
 doi = {http://doi.acm.org/10.1145/183018.183041},
 acmid = {183041},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Das:1994:AMM:183019.183041,
 author = {Das, Samir R. and Fujimoto, Richard M.},
 title = {An adaptive memory management protocol for Time Warp parallel simulation},
 abstract = {It is widely believed that Time Warp is prone to two potential problems: an excessive amount of wasted, rolled back computation resulting from ``rollback thrashing" behaviors, and inefficient use of memory, leading to poor performance of virtual memory and/or multiprocessor cache systems. An adaptive mechanism is proposed based on the Cancelback memory management protocol that dynamically controls the amount of memory used in the simulation in order to maximize performance. The proposed mechanism is adaptive in the sense that it monitors the execution of the Time Warp program, automatically adjusts the amount of memory used to reduce Time Warp overheads (fossil collection, Cancelback, the amount of rolled back computation, etc.) to a manageable level. The mechanism is based on a model that characterizes the behavior of Time Warp programs in terms of the flow of memory buffers among different buffer pools. We demonstrate that an implementation of the adaptive mechanism on a Kendall Square Research KSR-1 multiprocessor is effective in automatically maximizing performance while minimizing memory utilization of Time Warp programs, even for dynamically changing simulation models.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {201--210},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183041},
 doi = {http://doi.acm.org/10.1145/183019.183041},
 acmid = {183041},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhang:1994:PES:183019.183042,
 author = {Zhang, Hui and Knightly, Edward W.},
 title = {Providing end-to-end statistical performance guarantees with bounding interval dependent stochastic models},
 abstract = {This paper demonstrates a new, efficient, and general approach for providing end-to-end performance guarantees in integrated services networks. This is achieved by modeling a traffic source with a family of bounding interval-dependent (BIND) random variables and by using a rate-controlled service discipline inside the network. The traffic model stochastically bounds the number of bits sent over time intervals of different length. The model captures different source behavior over different time scales by making the bounding distribution an explicit function of the interval length. The service discipline, RCSP, has the priority queueing mechanisms necessary to provide performance guarantees in integrated services networks. In addition, RCSP provides the means for efficiently extending the results from a single switch to a network of arbitrary topology. These techniques are derived analytically and then demonstrated with numerical examples.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {211--220},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183042},
 doi = {http://doi.acm.org/10.1145/183019.183042},
 acmid = {183042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhang:1994:PES:183018.183042,
 author = {Zhang, Hui and Knightly, Edward W.},
 title = {Providing end-to-end statistical performance guarantees with bounding interval dependent stochastic models},
 abstract = {This paper demonstrates a new, efficient, and general approach for providing end-to-end performance guarantees in integrated services networks. This is achieved by modeling a traffic source with a family of bounding interval-dependent (BIND) random variables and by using a rate-controlled service discipline inside the network. The traffic model stochastically bounds the number of bits sent over time intervals of different length. The model captures different source behavior over different time scales by making the bounding distribution an explicit function of the interval length. The service discipline, RCSP, has the priority queueing mechanisms necessary to provide performance guarantees in integrated services networks. In addition, RCSP provides the means for efficiently extending the results from a single switch to a network of arbitrary topology. These techniques are derived analytically and then demonstrated with numerical examples.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {211--220},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183042},
 doi = {http://doi.acm.org/10.1145/183018.183042},
 acmid = {183042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pingali:1994:CSR:183019.183043,
 author = {Pingali, Sridhar and Towsley, Don and Kurose, James F.},
 title = {A comparison of sender-initiated and receiver-initiated reliable multicast protocols},
 abstract = {Sender-initiated reliable multicast protocols, based on the use of positive acknowledgments (ACKs), lead to an ACK implosion problem at the sender as the number of receivers increases. Briefly, the ACK implosion problem refers to the significant overhead incurred by the sending host due to the processing of ACKs from each receiver. A potential solution to this problem is to shift the burden of providing reliable data transfer to the receivers\&mdash;thus resulting in a receiver-initiated multicast error control protocol based on the use of negative acknowledgments (NAKs). In this paper we determine the maximum throughputs of the sending and receiving hosts for generic sender-initiated and receiver-initiated protocols. We show that the receiver-initiated error control protocols provide substantially higher throughputs than their sender-initiated counterparts. We further demonstrate that the introduction of random delays prior to generating NAKs coupled with the multicasting of NAKs to all receivers has the potential for an additional substantial increase in the throughput of receiver-initiated error control protocols over sender-initiated protocols.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {221--230},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183043},
 doi = {http://doi.acm.org/10.1145/183019.183043},
 acmid = {183043},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pingali:1994:CSR:183018.183043,
 author = {Pingali, Sridhar and Towsley, Don and Kurose, James F.},
 title = {A comparison of sender-initiated and receiver-initiated reliable multicast protocols},
 abstract = {Sender-initiated reliable multicast protocols, based on the use of positive acknowledgments (ACKs), lead to an ACK implosion problem at the sender as the number of receivers increases. Briefly, the ACK implosion problem refers to the significant overhead incurred by the sending host due to the processing of ACKs from each receiver. A potential solution to this problem is to shift the burden of providing reliable data transfer to the receivers\&mdash;thus resulting in a receiver-initiated multicast error control protocol based on the use of negative acknowledgments (NAKs). In this paper we determine the maximum throughputs of the sending and receiving hosts for generic sender-initiated and receiver-initiated protocols. We show that the receiver-initiated error control protocols provide substantially higher throughputs than their sender-initiated counterparts. We further demonstrate that the introduction of random delays prior to generating NAKs coupled with the multicasting of NAKs to all receivers has the potential for an additional substantial increase in the throughput of receiver-initiated error control protocols over sender-initiated protocols.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {221--230},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183043},
 doi = {http://doi.acm.org/10.1145/183018.183043},
 acmid = {183043},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nikolaidis:1994:TSC:183018.183044,
 author = {Nikolaidis, Ioanis and Fujimoto, Richard and Cooper, C. Anthony},
 title = {Time-parallel simulation of cascaded statistical multiplexers},
 abstract = {The multiplexing of several lightly loaded links onto a more heavily loaded output link is a problem of considerable importance to the design and traffic engineering of many types of packet-oriented telecommunications equipment, including that used in Asynchronous Transfer Mode (ATM) networks. Network configurations generally require the cascaded operation of such multiplexers and switches. Important objectives to achieve small cell loss ratios while maintaining efficient utilization of the transmission links. The small cell loss ratio objective results in extremely long simulation runs. To address this problem, we propose a new technique that relies on a compact description for the arriving/departing traffic at the multiplexers and a time-parallel scheme without fix-up phases for effective parallelization. The technique does not make assumptions about the analytical nature of the arrival process, thereby allowing trace-driven simulations to be performed as well. We demonstrate the method for a number of configurations and traffic scenarios, and observe that it yields one to two orders of magnitude speedup on a 32 processor Kendall Square Research KSR-1 multiprocessor compared to an efficient cell-level simulation executing on a Sparc-10 workstation.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {231--240},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183018.183044},
 doi = {http://doi.acm.org/10.1145/183018.183044},
 acmid = {183044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nikolaidis:1994:TSC:183019.183044,
 author = {Nikolaidis, Ioanis and Fujimoto, Richard and Cooper, C. Anthony},
 title = {Time-parallel simulation of cascaded statistical multiplexers},
 abstract = {The multiplexing of several lightly loaded links onto a more heavily loaded output link is a problem of considerable importance to the design and traffic engineering of many types of packet-oriented telecommunications equipment, including that used in Asynchronous Transfer Mode (ATM) networks. Network configurations generally require the cascaded operation of such multiplexers and switches. Important objectives to achieve small cell loss ratios while maintaining efficient utilization of the transmission links. The small cell loss ratio objective results in extremely long simulation runs. To address this problem, we propose a new technique that relies on a compact description for the arriving/departing traffic at the multiplexers and a time-parallel scheme without fix-up phases for effective parallelization. The technique does not make assumptions about the analytical nature of the arrival process, thereby allowing trace-driven simulations to be performed as well. We demonstrate the method for a number of configurations and traffic scenarios, and observe that it yields one to two orders of magnitude speedup on a 32 processor Kendall Square Research KSR-1 multiprocessor compared to an efficient cell-level simulation executing on a Sparc-10 workstation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {231--240},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/183019.183044},
 doi = {http://doi.acm.org/10.1145/183019.183044},
 acmid = {183044},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Worthington:1994:SAM:183019.183045,
 author = {Worthington, Bruce L. and Ganger, Gregory R. and Patt, Yale N.},
 title = {Scheduling algorithms for modern disk drives},
 abstract = {Disk subsystem performance can be dramatically improved by dynamically ordering, or scheduling, pending requests. Via strongly validated simulation, we examine the impact of complex logical-to-physical mappings and large prefetching caches on scheduling effectiveness. Using both synthetic workloads and traces captured from six different user environments, we arrive at three main conclusions: (1) Incorporating complex mapping information into the scheduler provides only a marginal (less than 2\%) decrease in response times for seek-reducing algorithms. (2) Algorithms which effectively utilize prefetching disk caches provide significant performance improvements for workloads with read sequentiality. The cyclical scan algorithm (C-LOOK), which always schedules requests in ascending logical order, achieves the highest performance among seek-reducing algorithms for such workloads. (3) Algorithms that reduce overall positioning delays produce the highest performance provided that they recognize and exploit a prefetching cache.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183019.183045},
 doi = {http://doi.acm.org/10.1145/183019.183045},
 acmid = {183045},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Worthington:1994:SAM:183018.183045,
 author = {Worthington, Bruce L. and Ganger, Gregory R. and Patt, Yale N.},
 title = {Scheduling algorithms for modern disk drives},
 abstract = {Disk subsystem performance can be dramatically improved by dynamically ordering, or scheduling, pending requests. Via strongly validated simulation, we examine the impact of complex logical-to-physical mappings and large prefetching caches on scheduling effectiveness. Using both synthetic workloads and traces captured from six different user environments, we arrive at three main conclusions: (1) Incorporating complex mapping information into the scheduler provides only a marginal (less than 2\%) decrease in response times for seek-reducing algorithms. (2) Algorithms which effectively utilize prefetching disk caches provide significant performance improvements for workloads with read sequentiality. The cyclical scan algorithm (C-LOOK), which always schedules requests in ascending logical order, achieves the highest performance among seek-reducing algorithms for such workloads. (3) Algorithms that reduce overall positioning delays produce the highest performance provided that they recognize and exploit a prefetching cache.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183018.183045},
 doi = {http://doi.acm.org/10.1145/183018.183045},
 acmid = {183045},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nicol:1994:OMC:183018.183046,
 author = {Nicol, David M. and Bokhari, Shahid H.},
 title = {Optimal multiphase complete exchange on circuit-switched hypercube architectures},
 abstract = {The complete-exchange communication primitive on a distributed memory multiprocessor calls for every processor to send a message to every other processor, each such message being unique. For circuit-switched hypercube networks there are two well-known schemes for implementing this primitive. Direct exchange minimizes communication volume but maximizes startup costs, while Standard Exchange minimizes startup costs at the price of higher communication volume. This paper analyzes a hybrid, which can be thought of as a sequence of Direct Echange phases, applied to variable-sized subcubes. This paper examines the problem of determining the optimal subcube dimension sizes d<subscrpt>i</subscrpt> for every phase. We show that optimal performance is achieved using some equi-partition, where |d<subscrpt>i</subscrpt>\&minus;d<subscrpt>j</subscrpt>|\&le;1 for all phases i and j. We study the behavior of the optimal partition as a function of machine communication parameters, hypercube dimension, and message size, and show that the optimal partition can be determined with no more than<inline-equation> <f> 2<fen lp="par"><rad><rcd><it>d</it></rcd></rad>+<rm>1</rm><rp post="par"></fen> </f> </inline-equation> comparisons. Finally we validate the model empirically, and for certain problem instances observe as much as a factor of two improvement over the other methods.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {252--260},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/183018.183046},
 doi = {http://doi.acm.org/10.1145/183018.183046},
 acmid = {183046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nicol:1994:OMC:183019.183046,
 author = {Nicol, David M. and Bokhari, Shahid H.},
 title = {Optimal multiphase complete exchange on circuit-switched hypercube architectures},
 abstract = {The complete-exchange communication primitive on a distributed memory multiprocessor calls for every processor to send a message to every other processor, each such message being unique. For circuit-switched hypercube networks there are two well-known schemes for implementing this primitive. Direct exchange minimizes communication volume but maximizes startup costs, while Standard Exchange minimizes startup costs at the price of higher communication volume. This paper analyzes a hybrid, which can be thought of as a sequence of Direct Echange phases, applied to variable-sized subcubes. This paper examines the problem of determining the optimal subcube dimension sizes d<subscrpt>i</subscrpt> for every phase. We show that optimal performance is achieved using some equi-partition, where |d<subscrpt>i</subscrpt>\&minus;d<subscrpt>j</subscrpt>|\&le;1 for all phases i and j. We study the behavior of the optimal partition as a function of machine communication parameters, hypercube dimension, and message size, and show that the optimal partition can be determined with no more than<inline-equation> <f> 2<fen lp="par"><rad><rcd><it>d</it></rcd></rad>+<rm>1</rm><rp post="par"></fen> </f> </inline-equation> comparisons. Finally we validate the model empirically, and for certain problem instances observe as much as a factor of two improvement over the other methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {252--260},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/183019.183046},
 doi = {http://doi.acm.org/10.1145/183019.183046},
 acmid = {183046},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Temam:1994:CIP:183018.183047,
 author = {Temam, O. and Fricker, C. and Jalby, W.},
 title = {Cache interference phenomena},
 abstract = {The impact of cache interferences on program performance (particularly numerical codes, which heavily use the memory hierarchy) remains unknown. The general knowledge is that cache interferences are highly irregular, in terms of occurrence and intensity. In this paper, the different types of cache interferences that can occur in numerical loop nests are identified. An analytical method is developed for detecting the occurrence of interferences and, more important, for computing the number of cache misses due to interferences. Simulations and experiments on real machines show that the model is generally accurate and that most interference phenomena are captured. Experiments also show that cache interferences can be intense and frequent. Certain parameters such as array base addresses or dimensions can have a strong impact on the occurrence of interferences. Modifying these parameters only can induce global execution time variations of 30\% and more. Applications of these modeling techniques are numerous and range from performance evaluation and prediction to enhancement of data locality optimizations techniques.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {261--271},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183018.183047},
 doi = {http://doi.acm.org/10.1145/183018.183047},
 acmid = {183047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache interferences or conflicts, data locality, modeling, numerical codes, performance evaluation},
} 

@article{Temam:1994:CIP:183019.183047,
 author = {Temam, O. and Fricker, C. and Jalby, W.},
 title = {Cache interference phenomena},
 abstract = {The impact of cache interferences on program performance (particularly numerical codes, which heavily use the memory hierarchy) remains unknown. The general knowledge is that cache interferences are highly irregular, in terms of occurrence and intensity. In this paper, the different types of cache interferences that can occur in numerical loop nests are identified. An analytical method is developed for detecting the occurrence of interferences and, more important, for computing the number of cache misses due to interferences. Simulations and experiments on real machines show that the model is generally accurate and that most interference phenomena are captured. Experiments also show that cache interferences can be intense and frequent. Certain parameters such as array base addresses or dimensions can have a strong impact on the occurrence of interferences. Modifying these parameters only can induce global execution time variations of 30\% and more. Applications of these modeling techniques are numerous and range from performance evaluation and prediction to enhancement of data locality optimizations techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {261--271},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/183019.183047},
 doi = {http://doi.acm.org/10.1145/183019.183047},
 acmid = {183047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache interferences or conflicts, data locality, modeling, numerical codes, performance evaluation},
} 

@article{Danskin:1994:PXP:183019.183048,
 author = {Danskin, John and Hanrahan, Pat},
 title = {Profiling the X protocol (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183048},
 doi = {http://doi.acm.org/10.1145/183019.183048},
 acmid = {183048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Danskin:1994:PXP:183018.183048,
 author = {Danskin, John and Hanrahan, Pat},
 title = {Profiling the X protocol (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {272--273},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183048},
 doi = {http://doi.acm.org/10.1145/183018.183048},
 acmid = {183048},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Drapeau:1994:TWC:183018.183049,
 author = {Drapeau, Ann L. and Patterson, David A. and Katz, Randy H.},
 title = {Toward workload characterization of video server and digital library applications (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {274--275},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183049},
 doi = {http://doi.acm.org/10.1145/183018.183049},
 acmid = {183049},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Drapeau:1994:TWC:183019.183049,
 author = {Drapeau, Ann L. and Patterson, David A. and Katz, Randy H.},
 title = {Toward workload characterization of video server and digital library applications (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {274--275},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183049},
 doi = {http://doi.acm.org/10.1145/183019.183049},
 acmid = {183049},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gill:1994:CSF:183018.183050,
 author = {Gill, Deepinder S. and Zhou, Songnian and Sandhu, Harjinder S.},
 title = {A case study of file system workload in a large-scale distributed environment},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {276--277},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183050},
 doi = {http://doi.acm.org/10.1145/183018.183050},
 acmid = {183050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gill:1994:CSF:183019.183050,
 author = {Gill, Deepinder S. and Zhou, Songnian and Sandhu, Harjinder S.},
 title = {A case study of file system workload in a large-scale distributed environment},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {276--277},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183050},
 doi = {http://doi.acm.org/10.1145/183019.183050},
 acmid = {183050},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hellerstein:1994:CTD:183019.183051,
 author = {Hellerstein, Joseph L.},
 title = {A comparison of techniques for diagnosing performance problems in information systems (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {278--279},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183051},
 doi = {http://doi.acm.org/10.1145/183019.183051},
 acmid = {183051},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hellerstein:1994:CTD:183018.183051,
 author = {Hellerstein, Joseph L.},
 title = {A comparison of techniques for diagnosing performance problems in information systems (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {278--279},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183051},
 doi = {http://doi.acm.org/10.1145/183018.183051},
 acmid = {183051},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1994:EUC:183018.183052,
 author = {Lee, J. William},
 title = {Efficient user-level communication on multicomputers with an optimistic flow-control protocol (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {280--281},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183052},
 doi = {http://doi.acm.org/10.1145/183018.183052},
 acmid = {183052},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1994:EUC:183019.183052,
 author = {Lee, J. William},
 title = {Efficient user-level communication on multicomputers with an optimistic flow-control protocol (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {280--281},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183052},
 doi = {http://doi.acm.org/10.1145/183019.183052},
 acmid = {183052},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rolia:1994:MRP:183018.183053,
 author = {Rolia, J. A. and Starkey, M. and Boersma, G.},
 title = {Modeling RPC performance},
 abstract = {Distributed computing applications are collections of processes allocated across a network that cooperate to accomplish common goals. The applications require the support of a distributed computing runtime environment that provides services to help manage process concurrency and interprocess communication. This support helps to hide much of the inherent complexity of distributed environments via industry standard interfaces and permits developers to create more portable applications. The resource requirements of the runtime services can be significant and may impact application performance and system throughput. This paper describes work done to study the potential benefits of redesigning some aspects of the DCE RPC and its current implementation on a specific platform.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {282--283},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183053},
 doi = {http://doi.acm.org/10.1145/183018.183053},
 acmid = {183053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rolia:1994:MRP:183019.183053,
 author = {Rolia, J. A. and Starkey, M. and Boersma, G.},
 title = {Modeling RPC performance},
 abstract = {Distributed computing applications are collections of processes allocated across a network that cooperate to accomplish common goals. The applications require the support of a distributed computing runtime environment that provides services to help manage process concurrency and interprocess communication. This support helps to hide much of the inherent complexity of distributed environments via industry standard interfaces and permits developers to create more portable applications. The resource requirements of the runtime services can be significant and may impact application performance and system throughput. This paper describes work done to study the potential benefits of redesigning some aspects of the DCE RPC and its current implementation on a specific platform.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {282--283},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183053},
 doi = {http://doi.acm.org/10.1145/183019.183053},
 acmid = {183053},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tayyab:1994:SPM:183018.183054,
 author = {Tayyab, Athar B. and Kuhl, Jon G.},
 title = {Stochastic performance models of parallel task systems (extended abstract)},
 abstract = {This paper considers the class of parallel computations represented by directed, acyclic task graphs. These include parallel loops, multiphase algorithms, partitioning and merging algorithms, as well as any arbitrary parallel computation that can be structured by a task graph. The paper reviews the current state of the art in stochastic bound models of parallel programs and presents new stochastic bound performance models that predict the expected execution time of parallel programs on a given shared-memory multiprocessor system; and provide qualitative and quantitative description of the relationships between the structure of parallel programs, computation and synchronization behavior of the program, and architectural features of the underlying multiprocessor system.The models use a new formulation based on stochastic bound analysis and are solvable for a number of distribution functions. They are applicable to shared-memory multiprocessors with significantly different architectural and synchronization performance characteristics. The accuracy of the models is validated via several measurements on two different shared-memory multiprocessor systems, the Alliant FX/2800 and the Encore Multimax. The results show the models to be quite accurate, even when some of the modeling assumptions are violated. The maximum error of prediction ranges from about 10\% to under 1\%.},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {284--285},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183054},
 doi = {http://doi.acm.org/10.1145/183018.183054},
 acmid = {183054},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tayyab:1994:SPM:183019.183054,
 author = {Tayyab, Athar B. and Kuhl, Jon G.},
 title = {Stochastic performance models of parallel task systems (extended abstract)},
 abstract = {This paper considers the class of parallel computations represented by directed, acyclic task graphs. These include parallel loops, multiphase algorithms, partitioning and merging algorithms, as well as any arbitrary parallel computation that can be structured by a task graph. The paper reviews the current state of the art in stochastic bound models of parallel programs and presents new stochastic bound performance models that predict the expected execution time of parallel programs on a given shared-memory multiprocessor system; and provide qualitative and quantitative description of the relationships between the structure of parallel programs, computation and synchronization behavior of the program, and architectural features of the underlying multiprocessor system.The models use a new formulation based on stochastic bound analysis and are solvable for a number of distribution functions. They are applicable to shared-memory multiprocessors with significantly different architectural and synchronization performance characteristics. The accuracy of the models is validated via several measurements on two different shared-memory multiprocessor systems, the Alliant FX/2800 and the Encore Multimax. The results show the models to be quite accurate, even when some of the modeling assumptions are violated. The maximum error of prediction ranges from about 10\% to under 1\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {284--285},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183054},
 doi = {http://doi.acm.org/10.1145/183019.183054},
 acmid = {183054},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Uhlig:1994:KMS:183018.183056,
 author = {Uhlig, Richard and Nagle, David and Mudge, Trevor and Sechrest, Stuart},
 title = {Kernel-based memory simulation (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {286--287},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183056},
 doi = {http://doi.acm.org/10.1145/183018.183056},
 acmid = {183056},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Uhlig:1994:KMS:183019.183056,
 author = {Uhlig, Richard and Nagle, David and Mudge, Trevor and Sechrest, Stuart},
 title = {Kernel-based memory simulation (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {286--287},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183056},
 doi = {http://doi.acm.org/10.1145/183019.183056},
 acmid = {183056},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wabnig:1994:PPP:183018.183057,
 author = {Wabnig, Harald and Haring, G\"{u}nter},
 title = {Performance prediction of parallel systems with scalable specifications\&mdash;methodology and case study},
 abstract = {},
 booktitle = {Proceedings of the 1994 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '94},
 year = {1994},
 isbn = {0-89791-659-X},
 location = {Nashville, Tennessee, United States},
 pages = {288--289},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183018.183057},
 doi = {http://doi.acm.org/10.1145/183018.183057},
 acmid = {183057},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wabnig:1994:PPP:183019.183057,
 author = {Wabnig, Harald and Haring, G\"{u}nter},
 title = {Performance prediction of parallel systems with scalable specifications\&mdash;methodology and case study},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {May 1994},
 volume = {22},
 issue = {1},
 month = {May},
 year = {1994},
 issn = {0163-5999},
 pages = {288--289},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/183019.183057},
 doi = {http://doi.acm.org/10.1145/183019.183057},
 acmid = {183057},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martonosi:1992:MAM:149439.133079,
 author = {Martonosi, Margaret and Gupta, Anoop and Anderson, Thomas},
 title = {MemSpy: analyzing memory system bottlenecks in programs},
 abstract = {To cope with the increasing difference between processor and main memory speeds, modern computer systems use deep memory hierarchies. In the presence of such hierarchies, the performance attained by an application is largely determined by its memory reference behavior\&mdash;if most references hit in the cache, the performance is significantly higher than if most references have to go to main memory. Frequently, it is possible for the programmer to restructure the data or code to achieve better memory reference behavior. Unfortunately, most existing performance debugging tools do not assist the programmer in this component of the overall performance tuning task.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133079},
 doi = {http://doi.acm.org/10.1145/149439.133079},
 acmid = {133079},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Martonosi:1992:MAM:133057.133079,
 author = {Martonosi, Margaret and Gupta, Anoop and Anderson, Thomas},
 title = {MemSpy: analyzing memory system bottlenecks in programs},
 abstract = {To cope with the increasing difference between processor and main memory speeds, modern computer systems use deep memory hierarchies. In the presence of such hierarchies, the performance attained by an application is largely determined by its memory reference behavior\&mdash;if most references hit in the cache, the performance is significantly higher than if most references have to go to main memory. Frequently, it is possible for the programmer to restructure the data or code to achieve better memory reference behavior. Unfortunately, most existing performance debugging tools do not assist the programmer in this component of the overall performance tuning task.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133079},
 doi = {http://doi.acm.org/10.1145/133057.133079},
 acmid = {133079},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Whalley:1992:FIC:149439.133081,
 author = {Whalley, David B.},
 title = {Fast instruction cache performance evaluation using compile-time analysis},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/149439.133081},
 doi = {http://doi.acm.org/10.1145/149439.133081},
 acmid = {133081},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache simulation, instruction cache, trace analysis, trace generation},
} 

@inproceedings{Whalley:1992:FIC:133057.133081,
 author = {Whalley, David B.},
 title = {Fast instruction cache performance evaluation using compile-time analysis},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/133057.133081},
 doi = {http://doi.acm.org/10.1145/133057.133081},
 acmid = {133081},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache simulation, instruction cache, trace analysis, trace generation},
} 

@inproceedings{LaRowe:1992:ADP:133057.133082,
 author = {LaRowe,Jr., Richard P. and Holliday, Mark A. and Ellis, Carla Schlatter},
 title = {An analysis of dynamic page placement on a NUMA multiprocessor},
 abstract = {The class of NUMA (nonuniform memory access time) shared memory architectures is becoming increasingly important with the desire for larger scale multiprocessors. In such machines, the placement and movement of code and data are crucial to performance. The operating system can play a role in managing placement through the policies and mechanisms of the virtual memory subsystem. In this paper, we develop an analytic model of memory system performance of a Local/Remote NUMA architecture based on approximate mean-value analysis techniques. The model assumes that a simple workload model based on a few parameters can often provide insight into the general behavior of real applications. The model is validated against experimental data obtained with the DUnX operating system kernel for the BBN GP1000 while running a synthetic workload. The results of this validation show that in general, model predictions are quite good, though in some cases the model fails to include the effect of unexpected behaviors in the implementation. Experiments investigate the effectiveness of dynamic multiple-copy page placement. We investigate the cost of incorrect policy decisions by introducing different percentages of policy error and measuring their effect on performance.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133082},
 doi = {http://doi.acm.org/10.1145/133057.133082},
 acmid = {133082},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{LaRowe:1992:ADP:149439.133082,
 author = {LaRowe,Jr., Richard P. and Holliday, Mark A. and Ellis, Carla Schlatter},
 title = {An analysis of dynamic page placement on a NUMA multiprocessor},
 abstract = {The class of NUMA (nonuniform memory access time) shared memory architectures is becoming increasingly important with the desire for larger scale multiprocessors. In such machines, the placement and movement of code and data are crucial to performance. The operating system can play a role in managing placement through the policies and mechanisms of the virtual memory subsystem. In this paper, we develop an analytic model of memory system performance of a Local/Remote NUMA architecture based on approximate mean-value analysis techniques. The model assumes that a simple workload model based on a few parameters can often provide insight into the general behavior of real applications. The model is validated against experimental data obtained with the DUnX operating system kernel for the BBN GP1000 while running a synthetic workload. The results of this validation show that in general, model predictions are quite good, though in some cases the model fails to include the effect of unexpected behaviors in the implementation. Experiments investigate the effectiveness of dynamic multiple-copy page placement. We investigate the cost of incorrect policy decisions by introducing different percentages of policy error and measuring their effect on performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133082},
 doi = {http://doi.acm.org/10.1145/149439.133082},
 acmid = {133082},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nicola:1992:AGC:133057.133084,
 author = {Nicola, Victor F. and Dan, Asit and Dias, Daniel M.},
 title = {Analysis of the generalized clock buffer replacement scheme for database transaction processing},
 abstract = {The CLOCK algorithm is a popular buffer replacement algorithm because of its simplicity and its ability to approximate the performance of the Least Recently Used (LRU) replacement policy. The Generalized Clock (GCLOCK) buffer replacement policy uses a circular buffer and a weight associated with each page brought in buffer to decide on which page to replace. We develop an approximate analysis for the GCLOCK policy under the Independent Reference Model (IRM) that applies to many database transaction processing workloads. We validate the analysis for various workloads with data access skew. Comparison with simulations shows that in all cases examined the error is extremely small (less than 1\%). To show the usefulness of the model we apply it to a Transaction Processing Council benchmark  A (TPC-A) like workload. If knowledge of the different data partitions in this workload is assumed, the analysis shows that, with appropriate choice of weights, the performance of the GCLOCK algorithm can be better than the LRU policy. Performance very close to that for optimal (static) buffer allocation can be achieved by assigning sufficiently high weights, and can be implemented with a reasonably low overhead. Finally, we outline how the model can be extended to capture the effect of page invalidation in a multinode system.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133084},
 doi = {http://doi.acm.org/10.1145/133057.133084},
 acmid = {133084},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nicola:1992:AGC:149439.133084,
 author = {Nicola, Victor F. and Dan, Asit and Dias, Daniel M.},
 title = {Analysis of the generalized clock buffer replacement scheme for database transaction processing},
 abstract = {The CLOCK algorithm is a popular buffer replacement algorithm because of its simplicity and its ability to approximate the performance of the Least Recently Used (LRU) replacement policy. The Generalized Clock (GCLOCK) buffer replacement policy uses a circular buffer and a weight associated with each page brought in buffer to decide on which page to replace. We develop an approximate analysis for the GCLOCK policy under the Independent Reference Model (IRM) that applies to many database transaction processing workloads. We validate the analysis for various workloads with data access skew. Comparison with simulations shows that in all cases examined the error is extremely small (less than 1\%). To show the usefulness of the model we apply it to a Transaction Processing Council benchmark  A (TPC-A) like workload. If knowledge of the different data partitions in this workload is assumed, the analysis shows that, with appropriate choice of weights, the performance of the GCLOCK algorithm can be better than the LRU policy. Performance very close to that for optimal (static) buffer allocation can be achieved by assigning sufficiently high weights, and can be implemented with a reasonably low overhead. Finally, we outline how the model can be extended to capture the effect of page invalidation in a multinode system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133084},
 doi = {http://doi.acm.org/10.1145/149439.133084},
 acmid = {133084},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Borst:1992:CCC:149439.133085,
 author = {Borst, S. C. and Boxma, O. J. and Comb\'{e}, M. B.},
 title = {Collection of customers: a correlated <italic>M</italic>/<italic>G</italic>/1 queue},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {47--59},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/149439.133085},
 doi = {http://doi.acm.org/10.1145/149439.133085},
 acmid = {133085},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Borst:1992:CCC:133057.133085,
 author = {Borst, S. C. and Boxma, O. J. and Comb\'{e}, M. B.},
 title = {Collection of customers: a correlated <italic>M</italic>/<italic>G</italic>/1 queue},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {47--59},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/133057.133085},
 doi = {http://doi.acm.org/10.1145/133057.133085},
 acmid = {133085},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jacquet:1992:STD:133057.133087,
 author = {Jacquet, Philippe},
 title = {Subexponential tail distribution in LaPalice queues},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/133057.133087},
 doi = {http://doi.acm.org/10.1145/133057.133087},
 acmid = {133087},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jacquet:1992:STD:149439.133087,
 author = {Jacquet, Philippe},
 title = {Subexponential tail distribution in LaPalice queues},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/149439.133087},
 doi = {http://doi.acm.org/10.1145/149439.133087},
 acmid = {133087},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1992:RBC:133057.133088,
 author = {Lee, Duan-Shin and Sengupta, Bhaskar},
 title = {A reservation based cyclic server queue with limited service},
 abstract = {In this paper, we examine a problem which is an extension of the limited service in a queueing system with a cyclic server. In this service mechanism, each queue, after receiving service in cycle j, makes a reservation for its service requirement in cycle j + 1. In this paper, we consider symmetric case only, i.e., the arrival rates to all the queues are the same. The main contribution to queueing theory is that we propose an approximation for the queue length and sojourn-time distributions for this discipline. Most approximate studies on cyclic queues, which have been considered before, examine the means only. Our method is an iterative one, which we prove to be convergent by using stochastic dominance arguments. We examine the performance of our algorithm by comparing it to simulations and show that the results are very good.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {70--77},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/133057.133088},
 doi = {http://doi.acm.org/10.1145/133057.133088},
 acmid = {133088},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1992:RBC:149439.133088,
 author = {Lee, Duan-Shin and Sengupta, Bhaskar},
 title = {A reservation based cyclic server queue with limited service},
 abstract = {In this paper, we examine a problem which is an extension of the limited service in a queueing system with a cyclic server. In this service mechanism, each queue, after receiving service in cycle j, makes a reservation for its service requirement in cycle j + 1. In this paper, we consider symmetric case only, i.e., the arrival rates to all the queues are the same. The main contribution to queueing theory is that we propose an approximation for the queue length and sojourn-time distributions for this discipline. Most approximate studies on cyclic queues, which have been considered before, examine the means only. Our method is an iterative one, which we prove to be convergent by using stochastic dominance arguments. We examine the performance of our algorithm by comparing it to simulations and show that the results are very good.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {70--77},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/149439.133088},
 doi = {http://doi.acm.org/10.1145/149439.133088},
 acmid = {133088},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramakrishnan:1992:AFI:133057.133090,
 author = {Ramakrishnan, K. K. and Biswas, Prabuddha and Karedla, Ramakrishna},
 title = {Analysis of file I/O traces in commercial computing environments},
 abstract = {Improving the performance of the file system is becoming increasingly important to alleviate the effect of I/O bottlenecks in computer systems. To design changes to an existing file system or to architect a new file system it is important to understand current usage patterns. In this paper we analyze file I/O traces of several existing production computer sytems to understand file access behavior.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {78--90},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/133057.133090},
 doi = {http://doi.acm.org/10.1145/133057.133090},
 acmid = {133090},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramakrishnan:1992:AFI:149439.133090,
 author = {Ramakrishnan, K. K. and Biswas, Prabuddha and Karedla, Ramakrishna},
 title = {Analysis of file I/O traces in commercial computing environments},
 abstract = {Improving the performance of the file system is becoming increasingly important to alleviate the effect of I/O bottlenecks in computer systems. To design changes to an existing file system or to architect a new file system it is important to understand current usage patterns. In this paper we analyze file I/O traces of several existing production computer sytems to understand file access behavior.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {78--90},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/149439.133090},
 doi = {http://doi.acm.org/10.1145/149439.133090},
 acmid = {133090},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sandhu:1992:CFR:149439.133092,
 author = {Sandhu, Harjinder S. and Zhou, Songnian},
 title = {Cluster-based file replication in large-scale distributed systems},
 abstract = {The increasing need for data sharing in large-scale distributed systems may place a heavy burden on critical resources such as file servers and networks. Our examination of the workload in one large commercial engineering environment shows that wide-spread sharing of unstable files among tens to hundreds of users is common. Traditional client-based file cacheing techniques are not scalable in such environments.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133092},
 doi = {http://doi.acm.org/10.1145/149439.133092},
 acmid = {133092},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sandhu:1992:CFR:133057.133092,
 author = {Sandhu, Harjinder S. and Zhou, Songnian},
 title = {Cluster-based file replication in large-scale distributed systems},
 abstract = {The increasing need for data sharing in large-scale distributed systems may place a heavy burden on critical resources such as file servers and networks. Our examination of the workload in one large commercial engineering environment shows that wide-spread sharing of unstable files among tens to hundreds of users is common. Traditional client-based file cacheing techniques are not scalable in such environments.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {91--102},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133092},
 doi = {http://doi.acm.org/10.1145/133057.133092},
 acmid = {133092},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Merchant:1992:PAD:149439.133094,
 author = {Merchant, Arif and Wu, Kun-Lung and Yu, Philip S. and Chen, Ming-Syan},
 title = {Performance analysis of dynamic finite versioning for concurrent transaction and query processing},
 abstract = {In this paper, we analyze the performance of dynamic finite versioning (DFV) schemes for concurrent transaction and query processing, where a finite number of consistent snapshots can be derived for query access. We develop analytical models based on a renewal process approximation to evaluate the performance of DFV using M \&ge; 2 snapshots. The storage overhead and obsolescence faced by queries are measured. Simulation is used to validate the analytical models and to evaluate the trade-offs between various starategies for advancing snapshots when M \&gt; 2.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133094},
 doi = {http://doi.acm.org/10.1145/149439.133094},
 acmid = {133094},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Merchant:1992:PAD:133057.133094,
 author = {Merchant, Arif and Wu, Kun-Lung and Yu, Philip S. and Chen, Ming-Syan},
 title = {Performance analysis of dynamic finite versioning for concurrent transaction and query processing},
 abstract = {In this paper, we analyze the performance of dynamic finite versioning (DFV) schemes for concurrent transaction and query processing, where a finite number of consistent snapshots can be derived for query access. We develop analytical models based on a renewal process approximation to evaluate the performance of DFV using M \&ge; 2 snapshots. The storage overhead and obsolescence faced by queries are measured. Simulation is used to validate the analytical models and to evaluate the trade-offs between various starategies for advancing snapshots when M \&gt; 2.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {103--114},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133094},
 doi = {http://doi.acm.org/10.1145/133057.133094},
 acmid = {133094},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thomasian:1992:PAL:149439.133095,
 author = {Thomasian, Alexander},
 title = {Performance analysis of locking policies with limited wait depth},
 abstract = {The performance of a transaction processing system with the standard two-phase locking (2PL) concurrency control (CC) method (with the general waiting policy upon a lock conflict) may be degraded significantly due to transaction blocking in a high lock contention environment. In the limit this effect leads to the thrashing phenomenon, i.e., the majority of the transactions in the system become blocked. Limiting the wait depth of blocked transactions is an effective method to increase the number of active transactions in the system and to prevent thrashing, but this is at the cost of additional processing due to transaction restarts. The no-waiting (or immediate restart) policy limits the wait-depth to zero, while cautious waiting and the running priority policies use  different methods to limit the wait depth to one. A variant of the wait depth limited (WDL) policy [8] also limits the wait depth to one, while attempting to minimize the wasted processing incurred by transaction aborts. A unified methodology to analyze the performance of the 2PL CC method with limited wait depth policies in a system with multiple transaction classes is described in this paper. The analysis is based on Markov chains representing the execution steps of each transaction in isolation, but as affected by hardware resource and data contention with other transactions in the system. Since the transition rates of the Markov chain are not known a priori, an iterative solution method is developed, which is then applied to the running priority and WDL policies. Simulation is used for  validating the accuracy of the approximate analytic solutions. Of interest are the conservation laws governing the rate at which locks are transferred among transactions, which can be used to verify the correctness of the analysis.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {115--127},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/149439.133095},
 doi = {http://doi.acm.org/10.1145/149439.133095},
 acmid = {133095},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thomasian:1992:PAL:133057.133095,
 author = {Thomasian, Alexander},
 title = {Performance analysis of locking policies with limited wait depth},
 abstract = {The performance of a transaction processing system with the standard two-phase locking (2PL) concurrency control (CC) method (with the general waiting policy upon a lock conflict) may be degraded significantly due to transaction blocking in a high lock contention environment. In the limit this effect leads to the thrashing phenomenon, i.e., the majority of the transactions in the system become blocked. Limiting the wait depth of blocked transactions is an effective method to increase the number of active transactions in the system and to prevent thrashing, but this is at the cost of additional processing due to transaction restarts. The no-waiting (or immediate restart) policy limits the wait-depth to zero, while cautious waiting and the running priority policies use  different methods to limit the wait depth to one. A variant of the wait depth limited (WDL) policy [8] also limits the wait depth to one, while attempting to minimize the wasted processing incurred by transaction aborts. A unified methodology to analyze the performance of the 2PL CC method with limited wait depth policies in a system with multiple transaction classes is described in this paper. The analysis is based on Markov chains representing the execution steps of each transaction in isolation, but as affected by hardware resource and data contention with other transactions in the system. Since the transition rates of the Markov chain are not known a priori, an iterative solution method is developed, which is then applied to the running priority and WDL policies. Simulation is used for  validating the accuracy of the approximate analytic solutions. Of interest are the conservation laws governing the rate at which locks are transferred among transactions, which can be used to verify the correctness of the analysis.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {115--127},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/133057.133095},
 doi = {http://doi.acm.org/10.1145/133057.133095},
 acmid = {133095},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kurose:1992:CPP:149439.133097,
 author = {Kurose, Jim},
 title = {On computing per-session performance bounds in high-speed multi-hop computer networks},
 abstract = {We present a technique for computing upper bounds on the distribution of individual per-session performance measures such as delay and buffer occupancy for networks in which sessions may be routed over several ``hops." Our approach is based on first stochastically bounding the distribution of the number of packets (or cells) which can be generated by each traffic source over various lengths of time and then ``pushing" these bounds (which are then shown to hold over new time interval lengths at various network queues) through the network on a per-session basis. Session performance bounds can then be computed once the stochastic bounds on the arrival process have been characterized for each session at all network nodes. A numerical example is presented and the resulting distributional bounds compared with simulation as well as with a point-valued worst-case performance bound.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133097},
 doi = {http://doi.acm.org/10.1145/149439.133097},
 acmid = {133097},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kurose:1992:CPP:133057.133097,
 author = {Kurose, Jim},
 title = {On computing per-session performance bounds in high-speed multi-hop computer networks},
 abstract = {We present a technique for computing upper bounds on the distribution of individual per-session performance measures such as delay and buffer occupancy for networks in which sessions may be routed over several ``hops." Our approach is based on first stochastically bounding the distribution of the number of packets (or cells) which can be generated by each traffic source over various lengths of time and then ``pushing" these bounds (which are then shown to hold over new time interval lengths at various network queues) through the network on a per-session basis. Session performance bounds can then be computed once the stochastic bounds on the arrival process have been characterized for each session at all network nodes. A numerical example is presented and the resulting distributional bounds compared with simulation as well as with a point-valued worst-case performance bound.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133097},
 doi = {http://doi.acm.org/10.1145/133057.133097},
 acmid = {133097},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lui:1992:AAB:133057.133099,
 author = {Lui, John C. S. and Muntz, Richard R.},
 title = {Algorithmic approach to bounding the mean response time of a minimum expected delay routing system},
 abstract = {In this paper we present an algorithmic approach to bounding the mean response time of a multi-server system in which the minimum expected delay routing policy issued, i.e., an arriving job will join the queue which has the minimal expected value of unfinished work. We assume the queueing system to have K servers, each with an infinite capacity queue. The arrival process is Poisson with parameter \&lgr;, and the service time of server i is exponentially distributed with mean 1/\&mgr;<subscrpt>i</subscrpt>, 1 \&le; i \&le; K. The computation algorithm we present allows one to tradeoff accuracy and computational cost. Upper and lower bounds on the expected response time and expected number of customers  are computed; the spread between the bounds can be reduced with additional space and time complexity. Examples are presented which illustrate the excellent relative accuracy attainable with relatively little computation.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {140--151},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133099},
 doi = {http://doi.acm.org/10.1145/133057.133099},
 acmid = {133099},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lui:1992:AAB:149439.133099,
 author = {Lui, John C. S. and Muntz, Richard R.},
 title = {Algorithmic approach to bounding the mean response time of a minimum expected delay routing system},
 abstract = {In this paper we present an algorithmic approach to bounding the mean response time of a multi-server system in which the minimum expected delay routing policy issued, i.e., an arriving job will join the queue which has the minimal expected value of unfinished work. We assume the queueing system to have K servers, each with an infinite capacity queue. The arrival process is Poisson with parameter \&lgr;, and the service time of server i is exponentially distributed with mean 1/\&mgr;<subscrpt>i</subscrpt>, 1 \&le; i \&le; K. The computation algorithm we present allows one to tradeoff accuracy and computational cost. Upper and lower bounds on the expected response time and expected number of customers  are computed; the spread between the bounds can be reduced with additional space and time complexity. Examples are presented which illustrate the excellent relative accuracy attainable with relatively little computation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {140--151},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133099},
 doi = {http://doi.acm.org/10.1145/149439.133099},
 acmid = {133099},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{de Souza e Silva:1992:SSE:149439.133100,
 author = {de Souza e Silva, Edmundo and Ochoa, Pedro Meji\'{a}},
 title = {State space exploration in Markov models},
 abstract = {Performance and dependability analysis is usually based on Markov models. One of the main problems faced by the analyst is the large state space cardinality of the Markov chain associated with the model, which precludes not only the model solution, but also the generation of the transition rate matrix. However, in many real system models, most of the probability mass is concentrated in a small number of states in comparison with the whole state space. Therefore, performability measures may be accurately evaluated from these ``high probable" states. In this paper, we present an algorithm to generate the most probable states that is more efficient than previous algorithms in the literature. We also address the problem of calculating measures of interest and show how bounds on some measures can be efficiently calculated.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {152--166},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/149439.133100},
 doi = {http://doi.acm.org/10.1145/149439.133100},
 acmid = {133100},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{de Souza e Silva:1992:SSE:133057.133100,
 author = {de Souza e Silva, Edmundo and Ochoa, Pedro Meji\'{a}},
 title = {State space exploration in Markov models},
 abstract = {Performance and dependability analysis is usually based on Markov models. One of the main problems faced by the analyst is the large state space cardinality of the Markov chain associated with the model, which precludes not only the model solution, but also the generation of the transition rate matrix. However, in many real system models, most of the probability mass is concentrated in a small number of states in comparison with the whole state space. Therefore, performability measures may be accurately evaluated from these ``high probable" states. In this paper, we present an algorithm to generate the most probable states that is more efficient than previous algorithms in the literature. We also address the problem of calculating measures of interest and show how bounds on some measures can be efficiently calculated.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {152--166},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/133057.133100},
 doi = {http://doi.acm.org/10.1145/133057.133100},
 acmid = {133100},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Owicki:1992:FPA:133057.133102,
 author = {Owicki, Susan S. and Karlin, Anna R.},
 title = {Factors in the performance of the AN1 computer network},
 abstract = {AN1 (formerly known as Autonet) is a local area network composed of crossbar switches interconnected by 100Mbit/second, full-duplex links. In this paper, we evaluate the performance impact of certain choices in the AN1 design. These include the use of FIFO input buffering in the crossbar switch, the deadlock-avoidance mechanism, cut-through routing, back-pressure for flow control, and multi-path routing. AN1's performance goals were to provide low latency and high bandwidth in a lightly loaded network. In this it is successful. Under heavy load, the most serious impediment to good performance is the use of FIFO input buffers. The deadlock-avoidance technique has an adverse effect on the performance of some topologies, but it seems to be the best alternative, given the goals and  constraints of the AN1 design. Cut-through switching performs well relative to store-and-forward switching, even under heavy load. Back-pressure deals adequately with congestion in a lightly-loaded network; under moderate load, performance is acceptable when coupled with end-to-end flow control for bursts. Multi-path routing successfully exploits redundant paths between hosts to improve performance in the face of congestion.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {167--180},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/133057.133102},
 doi = {http://doi.acm.org/10.1145/133057.133102},
 acmid = {133102},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Owicki:1992:FPA:149439.133102,
 author = {Owicki, Susan S. and Karlin, Anna R.},
 title = {Factors in the performance of the AN1 computer network},
 abstract = {AN1 (formerly known as Autonet) is a local area network composed of crossbar switches interconnected by 100Mbit/second, full-duplex links. In this paper, we evaluate the performance impact of certain choices in the AN1 design. These include the use of FIFO input buffering in the crossbar switch, the deadlock-avoidance mechanism, cut-through routing, back-pressure for flow control, and multi-path routing. AN1's performance goals were to provide low latency and high bandwidth in a lightly loaded network. In this it is successful. Under heavy load, the most serious impediment to good performance is the use of FIFO input buffers. The deadlock-avoidance technique has an adverse effect on the performance of some topologies, but it seems to be the best alternative, given the goals and  constraints of the AN1 design. Cut-through switching performs well relative to store-and-forward switching, even under heavy load. Back-pressure deals adequately with congestion in a lightly-loaded network; under moderate load, performance is acceptable when coupled with end-to-end flow control for bursts. Multi-path routing successfully exploits redundant paths between hosts to improve performance in the face of congestion.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {167--180},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/149439.133102},
 doi = {http://doi.acm.org/10.1145/149439.133102},
 acmid = {133102},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shankar:1992:PCR:149439.133103,
 author = {Shankar, A. Udaya and Alaettino\u{g}lu, Cengiz and Matta, Ibrahim and Dussa-Zieger, Klaudia},
 title = {Performance comparison of routing protocols using MaRS: distance-vector versus link-state},
 abstract = {There are two approaches to adaptive routing protocols for wide-area store-and-forward networks: distance-vector and link-state. Distance-vector algorithms use O(N x e) storage at each node, whereas link-state algorithms use O(N<supscrpt>2</supscrpt>), where N is the number of nodes in the network and e is the average degree of a node. The ARPANET started with a distance-vector algorithm (Distributed Bellman-Ford), but because of long-lived loops, changed to a link-state algorithm (SPF). We show, using a recently developed network simulator, MaRS, that a newly proposed distance-vector algorithm (ExBF) performs as well as SPF. This suggests that distance-vector algorithms are appropriate for very large wide-area  networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133103},
 doi = {http://doi.acm.org/10.1145/149439.133103},
 acmid = {133103},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shankar:1992:PCR:133057.133103,
 author = {Shankar, A. Udaya and Alaettino\u{g}lu, Cengiz and Matta, Ibrahim and Dussa-Zieger, Klaudia},
 title = {Performance comparison of routing protocols using MaRS: distance-vector versus link-state},
 abstract = {There are two approaches to adaptive routing protocols for wide-area store-and-forward networks: distance-vector and link-state. Distance-vector algorithms use O(N x e) storage at each node, whereas link-state algorithms use O(N<supscrpt>2</supscrpt>), where N is the number of nodes in the network and e is the average degree of a node. The ARPANET started with a distance-vector algorithm (Distributed Bellman-Ford), but because of long-lived loops, changed to a link-state algorithm (SPF). We show, using a recently developed network simulator, MaRS, that a newly proposed distance-vector algorithm (ExBF) performs as well as SPF. This suggests that distance-vector algorithms are appropriate for very large wide-area  networks.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133103},
 doi = {http://doi.acm.org/10.1145/133057.133103},
 acmid = {133103},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Altman:1992:CCD:133057.133106,
 author = {Altman, Eitan and Nain, Philippe},
 title = {Closed-loop control with delayed information},
 abstract = {The theory of Markov Control Model with Perfect State Information (MCM-PSI) requires that the current state of the system is known to the decision maker at decision instants. Otherwise, one speaks of Markov Control Model with Imperfect State Information (MCM-ISI). In this article, we introduce a new class of MCM-ISI, where the information on the state of the system is delayed. Such an information structure is encountered, for instance, in high-speed data networks.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133106},
 doi = {http://doi.acm.org/10.1145/133057.133106},
 acmid = {133106},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Altman:1992:CCD:149439.133106,
 author = {Altman, Eitan and Nain, Philippe},
 title = {Closed-loop control with delayed information},
 abstract = {The theory of Markov Control Model with Perfect State Information (MCM-PSI) requires that the current state of the system is known to the decision maker at decision instants. Otherwise, one speaks of Markov Control Model with Imperfect State Information (MCM-ISI). In this article, we introduce a new class of MCM-ISI, where the information on the state of the system is delayed. Such an information structure is encountered, for instance, in high-speed data networks.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133106},
 doi = {http://doi.acm.org/10.1145/149439.133106},
 acmid = {133106},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Merchant:1992:AMC:149439.133107,
 author = {Merchant, Arif},
 title = {Analytical models of combining Banyan networks},
 abstract = {We present in this paper an analytical model of a multistage combining Banyan network with output buffered switches, in hot-sport traffic. In a combining network, packets bound for the same destination are combined into one if they meet at a switch; this alleviates the problem of tree-saturation caused by hot-spot traffic. We model the flow processes in the network as Markov chains and recursively approximate the departure processes of each stage of the network in terms of the departure processes of the preceding stage. This model is used to predict the throughput of the combining network, and comparison with simulation results shows the prediction to be accurate.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {205--212},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/149439.133107},
 doi = {http://doi.acm.org/10.1145/149439.133107},
 acmid = {133107},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Merchant:1992:AMC:133057.133107,
 author = {Merchant, Arif},
 title = {Analytical models of combining Banyan networks},
 abstract = {We present in this paper an analytical model of a multistage combining Banyan network with output buffered switches, in hot-sport traffic. In a combining network, packets bound for the same destination are combined into one if they meet at a switch; this alleviates the problem of tree-saturation caused by hot-spot traffic. We model the flow processes in the network as Markov chains and recursively approximate the departure processes of each stage of the network in terms of the departure processes of the preceding stage. This model is used to predict the throughput of the combining network, and comparison with simulation results shows the prediction to be accurate.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {205--212},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/133057.133107},
 doi = {http://doi.acm.org/10.1145/133057.133107},
 acmid = {133107},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Akyildiz:1992:PAL:149439.133109,
 author = {Akyildiz, Ian F. and Chen, Liang and Das, Samir R. and Fujimoto, Richard M. and Serfozo, Richard F.},
 title = {Performance analysis of \&ldquo;Time Warp\&rdquo; with limited memory},
 abstract = {The behavior of n interacting processes synchronized by the ``Time Warp" rollback mechanism is analyzed under the constraint that the total amount of memory to execute the program is limited. In Time Warp, a protocol called ``cancelback" has been proposed to reclaim storage when the system runs out of memory. A discrete state, continuous time Markov chain model for Time Warp augmented with the cancelback protocol is developed for a shared memory system with n homogeneous processors and homogeneous workload. The model allows one to predict speedup as the amount of available memory is varied. To our knowledge, this is the first model to achieve this result. The performance predicted by the model is validated through direct  performance measurements on an operational Time Warp system executing on a shared-memory multiprocessor using a workload similar to that in the model. It is observed that Time Warp with only a few additional message buffers per processor over that required in the corresponding sequential execution can achieve approximately the same or even greater performance than Time Warp with unlimited memory, if GVT computation and fossil collection can be efficiently implemented.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {213--224},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133109},
 doi = {http://doi.acm.org/10.1145/149439.133109},
 acmid = {133109},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Akyildiz:1992:PAL:133057.133109,
 author = {Akyildiz, Ian F. and Chen, Liang and Das, Samir R. and Fujimoto, Richard M. and Serfozo, Richard F.},
 title = {Performance analysis of \&ldquo;Time Warp\&rdquo; with limited memory},
 abstract = {The behavior of n interacting processes synchronized by the ``Time Warp" rollback mechanism is analyzed under the constraint that the total amount of memory to execute the program is limited. In Time Warp, a protocol called ``cancelback" has been proposed to reclaim storage when the system runs out of memory. A discrete state, continuous time Markov chain model for Time Warp augmented with the cancelback protocol is developed for a shared memory system with n homogeneous processors and homogeneous workload. The model allows one to predict speedup as the amount of available memory is varied. To our knowledge, this is the first model to achieve this result. The performance predicted by the model is validated through direct  performance measurements on an operational Time Warp system executing on a shared-memory multiprocessor using a workload similar to that in the model. It is observed that Time Warp with only a few additional message buffers per processor over that required in the corresponding sequential execution can achieve approximately the same or even greater performance than Time Warp with unlimited memory, if GVT computation and fossil collection can be efficiently implemented.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {213--224},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133109},
 doi = {http://doi.acm.org/10.1145/133057.133109},
 acmid = {133109},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Turek:1992:SPT:149439.133111,
 author = {Turek, John and Wolf, Joel L. and Pattipati, Krishna R. and Yu, Philip S.},
 title = {Scheduling parallelizable tasks: putting it all on the shelf},
 abstract = {In this paper we formulate the following natural multiprocessor scheduling problem: Consider a parallel system with P processors. Suppose that there are Ntasks to be scheduled on this system, and that the execution time of each task j \&egr; 1,\&hellip;,N is a nonincreasing function t<subscrpt>j</subscrpt>(\&bgr;<subscrpt>j</subscrpt>) of the number of processors \&\&bgr;<subscrpt>j</subscrpt> \&egr; 1,\&hellip;,P allotted to it. The goal is to find, for each task j, an allotment of processors \&bgr;<subscrpt>j</subscrpt>, and, overall, a schedule assigning the tasks to the processors which minimizes the makespan, or latest task completion time.  The so-called shelf strategy is commonly used for orthogonal rectangle packing, a related and classic optimization problem. The prime difference between the orthogonal rectangle problem and our own is that in our case the rectangles are, in some sense, malleable: The height of each rectangle is a nonincreasing function of its width. In this paper, we solve our multiprocessor scheduling problem exactly in the context of a shelf-based paradigm. The algorithm we give uses techniques from resource allocation theory and employs a variety of other combinatorial optimization techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {225--236},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/149439.133111},
 doi = {http://doi.acm.org/10.1145/149439.133111},
 acmid = {133111},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Turek:1992:SPT:133057.133111,
 author = {Turek, John and Wolf, Joel L. and Pattipati, Krishna R. and Yu, Philip S.},
 title = {Scheduling parallelizable tasks: putting it all on the shelf},
 abstract = {In this paper we formulate the following natural multiprocessor scheduling problem: Consider a parallel system with P processors. Suppose that there are Ntasks to be scheduled on this system, and that the execution time of each task j \&egr; 1,\&hellip;,N is a nonincreasing function t<subscrpt>j</subscrpt>(\&bgr;<subscrpt>j</subscrpt>) of the number of processors \&\&bgr;<subscrpt>j</subscrpt> \&egr; 1,\&hellip;,P allotted to it. The goal is to find, for each task j, an allotment of processors \&bgr;<subscrpt>j</subscrpt>, and, overall, a schedule assigning the tasks to the processors which minimizes the makespan, or latest task completion time.  The so-called shelf strategy is commonly used for orthogonal rectangle packing, a related and classic optimization problem. The prime difference between the orthogonal rectangle problem and our own is that in our case the rectangles are, in some sense, malleable: The height of each rectangle is a nonincreasing function of its width. In this paper, we solve our multiprocessor scheduling problem exactly in the context of a shelf-based paradigm. The algorithm we give uses techniques from resource allocation theory and employs a variety of other combinatorial optimization techniques.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {225--236},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/133057.133111},
 doi = {http://doi.acm.org/10.1145/133057.133111},
 acmid = {133111},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bremaud:1992:SLR:133057.114676,
 author = {Br\'{e}maud, P. and Gong, W.-B.},
 title = {Stationary likelihood ratios and smoothed perturbation analysis gradient estimates for the routing problem},
 abstract = {We present stationary and regenerative form estimates for the gradients of the cycle variables with respect to a thinning parameter in the arrival process of G/G/1 queueing systems. Our estimates belong to the category of the likelihood ratio method (LRM) and smoothed perturbation analysis (SPA) estimates. The results are useful in adaptive routing design.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {237--238},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.114676},
 doi = {http://doi.acm.org/10.1145/133057.114676},
 acmid = {114676},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bremaud:1992:SLR:149439.114676,
 author = {Br\'{e}maud, P. and Gong, W.-B.},
 title = {Stationary likelihood ratios and smoothed perturbation analysis gradient estimates for the routing problem},
 abstract = {We present stationary and regenerative form estimates for the gradients of the cycle variables with respect to a thinning parameter in the arrival process of G/G/1 queueing systems. Our estimates belong to the category of the likelihood ratio method (LRM) and smoothed perturbation analysis (SPA) estimates. The results are useful in adaptive routing design.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {237--238},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.114676},
 doi = {http://doi.acm.org/10.1145/149439.114676},
 acmid = {114676},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Candlin:1992:SPP:133057.133138,
 author = {Candlin, Rosemary and Fisk, Peter and Phillips, Joe and Skilling, Neil},
 title = {Studying the performance properties of concurrent programs by simulation experiments on synthetic programs},
 abstract = {We have developed a methodology for constructing performance models of different types of concurrent programs, and hence obtaining estimates of execution times on different multiprocessor machines. A given class of program is characterized in terms of a small set of parameters which summarise the behaviour of the program over time. Synthetic programs with selected sets of parameters can then be generated and their execution simulated on a model of some given parallel machine. By varying the parameters systematically, we can discover which factors most affect performance.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {239--240},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133138},
 doi = {http://doi.acm.org/10.1145/133057.133138},
 acmid = {133138},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Candlin:1992:SPP:149439.133138,
 author = {Candlin, Rosemary and Fisk, Peter and Phillips, Joe and Skilling, Neil},
 title = {Studying the performance properties of concurrent programs by simulation experiments on synthetic programs},
 abstract = {We have developed a methodology for constructing performance models of different types of concurrent programs, and hence obtaining estimates of execution times on different multiprocessor machines. A given class of program is characterized in terms of a small set of parameters which summarise the behaviour of the program over time. Synthetic programs with selected sets of parameters can then be generated and their execution simulated on a model of some given parallel machine. By varying the parameters systematically, we can discover which factors most affect performance.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {239--240},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133138},
 doi = {http://doi.acm.org/10.1145/149439.133138},
 acmid = {133138},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Berry:1992:CIP:133057.133139,
 author = {Berry, Robert F. and Hellerstein, Joseph L.},
 title = {Characterizing and interpreting periodic behavior in computer systems},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {241--242},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133139},
 doi = {http://doi.acm.org/10.1145/133057.133139},
 acmid = {133139},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berry:1992:CIP:149439.133139,
 author = {Berry, Robert F. and Hellerstein, Joseph L.},
 title = {Characterizing and interpreting periodic behavior in computer systems},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {241--242},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133139},
 doi = {http://doi.acm.org/10.1145/149439.133139},
 acmid = {133139},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rahm:1992:HPC:149439.133141,
 author = {Rahm, Erhard and Ferguson, Donald},
 title = {High performance cache management for sequential data access},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {243--244},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133141},
 doi = {http://doi.acm.org/10.1145/149439.133141},
 acmid = {133141},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rahm:1992:HPC:133057.133141,
 author = {Rahm, Erhard and Ferguson, Donald},
 title = {High performance cache management for sequential data access},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {243--244},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133141},
 doi = {http://doi.acm.org/10.1145/133057.133141},
 acmid = {133141},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chakka:1992:MSG:133057.133143,
 author = {Chakka, Ram and Mitrani, Isi},
 title = {Multiprocessor systems with general breakdowns and repairs (extended abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {245--246},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133143},
 doi = {http://doi.acm.org/10.1145/133057.133143},
 acmid = {133143},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chakka:1992:MSG:149439.133143,
 author = {Chakka, Ram and Mitrani, Isi},
 title = {Multiprocessor systems with general breakdowns and repairs (extended abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {245--246},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133143},
 doi = {http://doi.acm.org/10.1145/149439.133143},
 acmid = {133143},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brewer:1992:PHP:149439.133146,
 author = {Brewer, Eric A. and Dellarocas, Chrysanthos N. and Colbrook, Adrian and Weihl, William E.},
 title = {PROTEUS: a high-performance parallel-architecture simulator},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {247--248},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133146},
 doi = {http://doi.acm.org/10.1145/149439.133146},
 acmid = {133146},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brewer:1992:PHP:133057.133146,
 author = {Brewer, Eric A. and Dellarocas, Chrysanthos N. and Colbrook, Adrian and Weihl, William E.},
 title = {PROTEUS: a high-performance parallel-architecture simulator},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {247--248},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133146},
 doi = {http://doi.acm.org/10.1145/133057.133146},
 acmid = {133146},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Meliksetian:1992:PAC:149439.133148,
 author = {Meliksetian, Dikran S. and Chen, C. Y. Roger},
 title = {Performance analysis of communications in static interconnection networks},
 abstract = {We present a model, based on a network of D<supscrpt>X</supscrpt>/D/1 queues, to predict the communication performance of static interconnection networks under various communication patterns. Our model predicts delay time distributions in the links as well as the first and second moments of the overall delay time of messages in the system. These predictions are verified by the results of simulations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {249--250},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133148},
 doi = {http://doi.acm.org/10.1145/149439.133148},
 acmid = {133148},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Meliksetian:1992:PAC:133057.133148,
 author = {Meliksetian, Dikran S. and Chen, C. Y. Roger},
 title = {Performance analysis of communications in static interconnection networks},
 abstract = {We present a model, based on a network of D<supscrpt>X</supscrpt>/D/1 queues, to predict the communication performance of static interconnection networks under various communication patterns. Our model predicts delay time distributions in the links as well as the first and second moments of the overall delay time of messages in the system. These predictions are verified by the results of simulations.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {249--250},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133148},
 doi = {http://doi.acm.org/10.1145/133057.133148},
 acmid = {133148},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dan:1992:CDA:133057.133150,
 author = {Dan, Asit and Yu, Philip S. and Chung, Jen-Yao},
 title = {Characterization of database access skew in a transaction processing environment},
 abstract = {The knowledge of access skew (non-uniform access) in each database relation is useful for both workload management (buffer pool allocation, transaction routing, etc.), as well as capacity planning for changing workload mix. However, it is a challenging problem to characterize the access skew of a real database workload in a simple manner that can easily be used to compute the buffer hit probability under the LRU replacement policy. A concise way to characterize the access skew is proposed by assuming that the large number of data pages may be logically grouped into a small number of partitions such that the frequency of accessing each page within a partition can be treated as equal. Based on this approach, a recursive binary partitioning algorithm is presented that can infer the access skew from the buffer hit probabilities for a subset of the buffer sizes. This avoids explicit estimation of individual access frequencies for the large number of database pages. The method is validated of its ability to predict buffer hit from the skew characterization using production database traces.},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {251--252},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133150},
 doi = {http://doi.acm.org/10.1145/133057.133150},
 acmid = {133150},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dan:1992:CDA:149439.133150,
 author = {Dan, Asit and Yu, Philip S. and Chung, Jen-Yao},
 title = {Characterization of database access skew in a transaction processing environment},
 abstract = {The knowledge of access skew (non-uniform access) in each database relation is useful for both workload management (buffer pool allocation, transaction routing, etc.), as well as capacity planning for changing workload mix. However, it is a challenging problem to characterize the access skew of a real database workload in a simple manner that can easily be used to compute the buffer hit probability under the LRU replacement policy. A concise way to characterize the access skew is proposed by assuming that the large number of data pages may be logically grouped into a small number of partitions such that the frequency of accessing each page within a partition can be treated as equal. Based on this approach, a recursive binary partitioning algorithm is presented that can infer the access skew from the buffer hit probabilities for a subset of the buffer sizes. This avoids explicit estimation of individual access frequencies for the large number of database pages. The method is validated of its ability to predict buffer hit from the skew characterization using production database traces.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {251--252},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133150},
 doi = {http://doi.acm.org/10.1145/149439.133150},
 acmid = {133150},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gupta:1992:XPE:133057.133152,
 author = {Gupta, Aloke and Hwu, Wen-Mei W.},
 title = {Xprof: profiling the execution of X Window programs},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {253--254},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133152},
 doi = {http://doi.acm.org/10.1145/133057.133152},
 acmid = {133152},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gupta:1992:XPE:149439.133152,
 author = {Gupta, Aloke and Hwu, Wen-Mei W.},
 title = {Xprof: profiling the execution of X Window programs},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {253--254},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133152},
 doi = {http://doi.acm.org/10.1145/149439.133152},
 acmid = {133152},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shoham:1992:EPS:149439.133154,
 author = {Shoham, Ruth and Yechiali, Uri},
 title = {Elevator-type polling systems (abstract)},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {255--257},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/149439.133154},
 doi = {http://doi.acm.org/10.1145/149439.133154},
 acmid = {133154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shoham:1992:EPS:133057.133154,
 author = {Shoham, Ruth and Yechiali, Uri},
 title = {Elevator-type polling systems (abstract)},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {255--257},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/133057.133154},
 doi = {http://doi.acm.org/10.1145/133057.133154},
 acmid = {133154},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Baccelli:1992:PSS:149439.133156,
 author = {Baccelli, Fran\c{c}ois and Canales, Miguel},
 title = {Parallel simulation of stochastic petri nets using recurrence equations},
 abstract = {Petri nets provide a powerful modeling formalism, which allows one to describe and study various classes of systems, such as synchronous and asynchronous processes, and/or parallel or sequential ones.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {257--258},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133156},
 doi = {http://doi.acm.org/10.1145/149439.133156},
 acmid = {133156},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Baccelli:1992:PSS:133057.133156,
 author = {Baccelli, Fran\c{c}ois and Canales, Miguel},
 title = {Parallel simulation of stochastic petri nets using recurrence equations},
 abstract = {Petri nets provide a powerful modeling formalism, which allows one to describe and study various classes of systems, such as synchronous and asynchronous processes, and/or parallel or sequential ones.
},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {257--258},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133156},
 doi = {http://doi.acm.org/10.1145/133057.133156},
 acmid = {133156},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jobmann:1992:PAP:149439.133158,
 author = {Jobmann, Manfred R. and Schumann, Johann},
 title = {Performance analysis of a parallel theorem prover},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {20},
 issue = {1},
 month = {June},
 year = {1992},
 issn = {0163-5999},
 pages = {259--260},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/149439.133158},
 doi = {http://doi.acm.org/10.1145/149439.133158},
 acmid = {133158},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jobmann:1992:PAP:133057.133158,
 author = {Jobmann, Manfred R. and Schumann, Johann},
 title = {Performance analysis of a parallel theorem prover},
 abstract = {},
 booktitle = {Proceedings of the 1992 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '92/PERFORMANCE '92},
 year = {1992},
 isbn = {0-89791-507-0},
 location = {Newport, Rhode Island, United States},
 pages = {259--260},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/133057.133158},
 doi = {http://doi.acm.org/10.1145/133057.133158},
 acmid = {133158},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Melliar-Smith:1991:PAB:107971.107973,
 author = {Melliar-Smith, P. M. and Moser, Louise E.},
 title = {Performance analysis of a broadcast communications protocol},
 abstract = {The Trans protocol is a communications protocol that exploits the broadcast capability of local area networks. Classical Markov models and queueing theory are used to analyze the performance of components of this protocol, but cannot be applied directly to determine the performance of the protocol as a whole. Instead, Laplace transforms of the distributions for the components are first derived and then combined into a transform for the entire protocol. This transform is evaluated by contour integration to yield the latency for the protocol.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107973},
 doi = {http://doi.acm.org/10.1145/107971.107973},
 acmid = {107973},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Melliar-Smith:1991:PAB:107972.107973,
 author = {Melliar-Smith, P. M. and Moser, Louise E.},
 title = {Performance analysis of a broadcast communications protocol},
 abstract = {The Trans protocol is a communications protocol that exploits the broadcast capability of local area networks. Classical Markov models and queueing theory are used to analyze the performance of components of this protocol, but cannot be applied directly to determine the performance of the protocol as a whole. Instead, Laplace transforms of the distributions for the components are first derived and then combined into a transform for the entire protocol. This transform is evaluated by contour integration to yield the latency for the protocol.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107973},
 doi = {http://doi.acm.org/10.1145/107972.107973},
 acmid = {107973},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Danzig:1991:AMO:107972.107974,
 author = {Danzig, Peter B.},
 title = {An analytical model of operating system protocol processing including effects of multiprogramming},
 abstract = {We model the limited buffer queueing process that occurs within the UNIX operating system's protocol processing layers. Our model accounts for the effects of user process multiprogramming and preemptive, priority scheduling of interrupt, operating system, and user tasks. After developing the model, we use it to predict message loss that occurs during local area network (LAN) multicast. Our service time model can be applied to window-and rate-based stream flow control.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107974},
 doi = {http://doi.acm.org/10.1145/107972.107974},
 acmid = {107974},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Danzig:1991:AMO:107971.107974,
 author = {Danzig, Peter B.},
 title = {An analytical model of operating system protocol processing including effects of multiprogramming},
 abstract = {We model the limited buffer queueing process that occurs within the UNIX operating system's protocol processing layers. Our model accounts for the effects of user process multiprogramming and preemptive, priority scheduling of interrupt, operating system, and user tasks. After developing the model, we use it to predict message loss that occurs during local area network (LAN) multicast. Our service time model can be applied to window-and rate-based stream flow control.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107974},
 doi = {http://doi.acm.org/10.1145/107971.107974},
 acmid = {107974},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harinarayan:1991:LSL:107972.107975,
 author = {Harinarayan, Venkatesh and Kleinrock, Leonard},
 title = {Load sharing in limited access distributed systems},
 abstract = {In this paper we examine dynamic load sharing in limited access distributed systems. In this class of distributed systems all servers are not accessible to all sources, and there exist many different accessibility topologies. We focus our attention on the ring topology and provide an analytic model to derive the approximate mean waiting time (our metric of performance). We then consider other limited access topologies and find that rather different interconnection patterns give similar performance measurements. We conjecture that the number of servers accessible to a source is the parameter with the greatest performance impact, in a limited access topology with load sharing. We also introduce another variable called diversity that is indicative of the degree of load sharing and speculate that performance is reasonably insensitive to diversity so long as it is non-zero. Using these conjectures we show how a reasonable estimate of the mean waiting time can be analytically derived in many limited access topologies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107975},
 doi = {http://doi.acm.org/10.1145/107972.107975},
 acmid = {107975},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harinarayan:1991:LSL:107971.107975,
 author = {Harinarayan, Venkatesh and Kleinrock, Leonard},
 title = {Load sharing in limited access distributed systems},
 abstract = {In this paper we examine dynamic load sharing in limited access distributed systems. In this class of distributed systems all servers are not accessible to all sources, and there exist many different accessibility topologies. We focus our attention on the ring topology and provide an analytic model to derive the approximate mean waiting time (our metric of performance). We then consider other limited access topologies and find that rather different interconnection patterns give similar performance measurements. We conjecture that the number of servers accessible to a source is the parameter with the greatest performance impact, in a limited access topology with load sharing. We also introduce another variable called diversity that is indicative of the degree of load sharing and speculate that performance is reasonably insensitive to diversity so long as it is non-zero. Using these conjectures we show how a reasonable estimate of the mean waiting time can be analytically derived in many limited access topologies.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107975},
 doi = {http://doi.acm.org/10.1145/107971.107975},
 acmid = {107975},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lin:1991:SPA:107972.107976,
 author = {Lin, Tein-Hsiang and Tarng, Wernhuar},
 title = {Scheduling periodic and aperiodic tasks in hard real-time computing systems},
 abstract = {Scheduling periodic and aperiodic tasks to meet their time constraints has been an important issue in the design of real-time computing systems. Usually, the task scheduling algorithms in such systems must satisfy the deadlines of periodic tasks and provide fast response times for aperiodic tasks. A simple and efficient approach to scheduling real-time tasks is the use of a periodic server in a static preemptive scheduling algorithm. Periodic tasks, including the server, are scheduled at priori</i> to meet their deadlines according to the knowledge of their periods and computation times. The scheduling of aperiodic tasks is then managed by the periodic server during its service time. In this paper, a new scheduling algorithm is proposed. The new algorithm creates a periodic server which will have the highest priority but not necessarily the shortest period. The server is suspended to reduce the overhead if there are no aperiodic tasks waiting, and is activated immediately upon the arrival of the next aperiodic task. After activated, the server performs its duty periodically until all waiting aperiodic tasks are completed. For a set of tasks scheduled by this algorithm, the deadlines of periodic tasks are guaranteed by a deterministic feasibility check, and the mean response time of aperiodic tasks are estimated using a queueing model. Based on the analytical results, we can determine the period and service time of the server producing the minimum mean response time for aperiodic tasks. The analytical results are compared with simulation results to demonstrate the correctness of our model.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {31--38},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/107972.107976},
 doi = {http://doi.acm.org/10.1145/107972.107976},
 acmid = {107976},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lin:1991:SPA:107971.107976,
 author = {Lin, Tein-Hsiang and Tarng, Wernhuar},
 title = {Scheduling periodic and aperiodic tasks in hard real-time computing systems},
 abstract = {Scheduling periodic and aperiodic tasks to meet their time constraints has been an important issue in the design of real-time computing systems. Usually, the task scheduling algorithms in such systems must satisfy the deadlines of periodic tasks and provide fast response times for aperiodic tasks. A simple and efficient approach to scheduling real-time tasks is the use of a periodic server in a static preemptive scheduling algorithm. Periodic tasks, including the server, are scheduled at priori</i> to meet their deadlines according to the knowledge of their periods and computation times. The scheduling of aperiodic tasks is then managed by the periodic server during its service time. In this paper, a new scheduling algorithm is proposed. The new algorithm creates a periodic server which will have the highest priority but not necessarily the shortest period. The server is suspended to reduce the overhead if there are no aperiodic tasks waiting, and is activated immediately upon the arrival of the next aperiodic task. After activated, the server performs its duty periodically until all waiting aperiodic tasks are completed. For a set of tasks scheduled by this algorithm, the deadlines of periodic tasks are guaranteed by a deterministic feasibility check, and the mean response time of aperiodic tasks are estimated using a queueing model. Based on the analytical results, we can determine the period and service time of the server producing the minimum mean response time for aperiodic tasks. The analytical results are compared with simulation results to demonstrate the correctness of our model.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {31--38},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/107971.107976},
 doi = {http://doi.acm.org/10.1145/107971.107976},
 acmid = {107976},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Berry:1991:ADC:107972.107977,
 author = {Berry, Robert and Hellerstein, Joseph},
 title = {An approach to detecting changes in the factors affecting the performance of computer systems},
 abstract = {Resolving intermittent performance problems in computer systems is made easier by pinpointing when a change occurs in the system's perforrnance-determinin g factors (e.g., workload composition, configuration). Since we often lack direct measurements of performance factors, this paper presents a procedure for indirectly detecting such changes by analyzing performance characteristics (e.g., response times, queue lengths). Our procedure employs a widely used clustering algorithm to identify candidate change points (the times at which performance factors change), and a newly developed statistical test (based on an AR(1) time series model) to determine the signficance of candidate change points. We evaluate our procedure by using simulations of M/M/1, FCFS queueing systems and by applying our procedure to measurements of a mainframe computer system at a large telephone company. These evaluations suggest that our procedure is effective in practice, especially for larger sample sizes and smaller utilizations. We further conclude that indirectly detecting changes in performance factors appears to be inherently difficult in that the sensitivity of a detection procedure depends on the magnitude of the change in performance characteristics, which often has a nonlinear relationship with the change in performance factors. Thus, a change in performance factors (e.g., increased service times) may be more readily detected in some situations (e.g., very low or very high utilizations) than in others (e.g., moderate utilizations). A key insight here is that the sensitivity of the detection procedure can be improved by choosing appropriate measures of performance characteristics. For example, our experience and analysis suggest that queue lengths can be more sensitive than response times to changes in arrival rates.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {39--49},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107972.107977},
 doi = {http://doi.acm.org/10.1145/107972.107977},
 acmid = {107977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Berry:1991:ADC:107971.107977,
 author = {Berry, Robert and Hellerstein, Joseph},
 title = {An approach to detecting changes in the factors affecting the performance of computer systems},
 abstract = {Resolving intermittent performance problems in computer systems is made easier by pinpointing when a change occurs in the system's perforrnance-determinin g factors (e.g., workload composition, configuration). Since we often lack direct measurements of performance factors, this paper presents a procedure for indirectly detecting such changes by analyzing performance characteristics (e.g., response times, queue lengths). Our procedure employs a widely used clustering algorithm to identify candidate change points (the times at which performance factors change), and a newly developed statistical test (based on an AR(1) time series model) to determine the signficance of candidate change points. We evaluate our procedure by using simulations of M/M/1, FCFS queueing systems and by applying our procedure to measurements of a mainframe computer system at a large telephone company. These evaluations suggest that our procedure is effective in practice, especially for larger sample sizes and smaller utilizations. We further conclude that indirectly detecting changes in performance factors appears to be inherently difficult in that the sensitivity of a detection procedure depends on the magnitude of the change in performance characteristics, which often has a nonlinear relationship with the change in performance factors. Thus, a change in performance factors (e.g., increased service times) may be more readily detected in some situations (e.g., very low or very high utilizations) than in others (e.g., moderate utilizations). A key insight here is that the sensitivity of the detection procedure can be improved by choosing appropriate measures of performance characteristics. For example, our experience and analysis suggest that queue lengths can be more sensitive than response times to changes in arrival rates.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {39--49},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107971.107977},
 doi = {http://doi.acm.org/10.1145/107971.107977},
 acmid = {107977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bodnarchuk:1991:SWM:107972.107978,
 author = {Bodnarchuk, Robert and Bunt, Richard},
 title = {A synthetic workload model for a distributed system file server},
 abstract = {The accuracy of the results of any performance study depends largely on the quality of the workload model driving it. Not surprisingly then, workload modelling is an area of great interest to those involved in the study of computer system performance. While a significant amount of research has focussed on the modelling of workloads in a centralized computer system, little has been done in the context of distributed systems. The goal of this research was to model the workload of a distributed system file server in a UNIX/NFS environment. The resulting model is distribution-driven and generates workload components in real time. It runs externally to the system it drives, thus eliminating any interference at the server. The model was validated for different workload intensities to ensure that it provides the flexibility to vary the workload intensity without loss of accuracy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107978},
 doi = {http://doi.acm.org/10.1145/107972.107978},
 acmid = {107978},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bodnarchuk:1991:SWM:107971.107978,
 author = {Bodnarchuk, Robert and Bunt, Richard},
 title = {A synthetic workload model for a distributed system file server},
 abstract = {The accuracy of the results of any performance study depends largely on the quality of the workload model driving it. Not surprisingly then, workload modelling is an area of great interest to those involved in the study of computer system performance. While a significant amount of research has focussed on the modelling of workloads in a centralized computer system, little has been done in the context of distributed systems. The goal of this research was to model the workload of a distributed system file server in a UNIX/NFS environment. The resulting model is distribution-driven and generates workload components in real time. It runs externally to the system it drives, thus eliminating any interference at the server. The model was validated for different workload intensities to ensure that it provides the flexibility to vary the workload intensity without loss of accuracy.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {50--59},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107978},
 doi = {http://doi.acm.org/10.1145/107971.107978},
 acmid = {107978},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Merchant:1991:MCA:107971.107979,
 author = {Merchant, Arif},
 title = {A Markov chain approximation for the analysis of banyan networks},
 abstract = {This paper analyzes the delay suffered by messages in a clocked, packet-switched, square Banyan network with k</i> x k</i> output-buffered switches by approximating the flow processes in the network with Markov chains. We recursively approximate the departure process of buffers of the n<sup>th</sup> stage in terms of thqt at the n</i> -- l<sup>st</sup> stage. We show how to construct the transition matrix for the Markov chain at each stage of the network and how to solve for the stationary distribution of the delay in the queues of that stage. The analytical results are compared with simulation results for several cases. Finally, we give a method based on this approximation and the technique of coupling</i> to compute upper bounds on the time for the system to approach steady state.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {60--67},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/107971.107979},
 doi = {http://doi.acm.org/10.1145/107971.107979},
 acmid = {107979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Merchant:1991:MCA:107972.107979,
 author = {Merchant, Arif},
 title = {A Markov chain approximation for the analysis of banyan networks},
 abstract = {This paper analyzes the delay suffered by messages in a clocked, packet-switched, square Banyan network with k</i> x k</i> output-buffered switches by approximating the flow processes in the network with Markov chains. We recursively approximate the departure process of buffers of the n<sup>th</sup> stage in terms of thqt at the n</i> -- l<sup>st</sup> stage. We show how to construct the transition matrix for the Markov chain at each stage of the network and how to solve for the stationary distribution of the delay in the queues of that stage. The analytical results are compared with simulation results for several cases. Finally, we give a method based on this approximation and the technique of coupling</i> to compute upper bounds on the time for the system to approach steady state.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {60--67},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/107972.107979},
 doi = {http://doi.acm.org/10.1145/107972.107979},
 acmid = {107979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lin:1991:PAF:107972.107980,
 author = {Lin, T. and Kleinrock, L.},
 title = {Performance analysis of finite-buffered multistage interconnection networks with a general traffic pattern},
 abstract = {We present an analytical model for evaluating the performance of finite-buffered packet switching multistage interconnection networks using blocking switches under any general traffic pattern. Most of the previous research work has assumed unbuffered, single buffer or infinite buffer cases, and all of them assumed that every processing element had the same traffic pattern (either a uniform traffic pattern or a specific hot spot pattern). However, their models cannot be applied very generally. There is a need for an analytical model to evaluate the performance under more general conditions.We first present a description of a decomposition \&amp;amp; iteration model which we propose for a specific hot spot pattern. This model is then extended to handle more general traffic patterns using a transformation method. For an even more general traffic condition where each processing element can have its own traffic pattern, we propose a superposition method to be used with the iteration model and the transformation method. We can extend the model to account for processing elements having different input rates by adding weighting factors in the analytical model.An approximation method is also proposed to refine the analytical model to account for the memory characteristic of a blocking switch which causes persistent blocking of packets contending for the same output ports. The analytical model is used to evaluate the uniform traffic pattern and a very general traffic pattern " EFOS". Comparison with simulation indicates that the analytical model is very accurate.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107972.107980},
 doi = {http://doi.acm.org/10.1145/107972.107980},
 acmid = {107980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lin:1991:PAF:107971.107980,
 author = {Lin, T. and Kleinrock, L.},
 title = {Performance analysis of finite-buffered multistage interconnection networks with a general traffic pattern},
 abstract = {We present an analytical model for evaluating the performance of finite-buffered packet switching multistage interconnection networks using blocking switches under any general traffic pattern. Most of the previous research work has assumed unbuffered, single buffer or infinite buffer cases, and all of them assumed that every processing element had the same traffic pattern (either a uniform traffic pattern or a specific hot spot pattern). However, their models cannot be applied very generally. There is a need for an analytical model to evaluate the performance under more general conditions.We first present a description of a decomposition \&amp;amp; iteration model which we propose for a specific hot spot pattern. This model is then extended to handle more general traffic patterns using a transformation method. For an even more general traffic condition where each processing element can have its own traffic pattern, we propose a superposition method to be used with the iteration model and the transformation method. We can extend the model to account for processing elements having different input rates by adding weighting factors in the analytical model.An approximation method is also proposed to refine the analytical model to account for the memory characteristic of a blocking switch which causes persistent blocking of packets contending for the same output ports. The analytical model is used to evaluate the uniform traffic pattern and a very general traffic pattern " EFOS". Comparison with simulation indicates that the analytical model is very accurate.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107971.107980},
 doi = {http://doi.acm.org/10.1145/107971.107980},
 acmid = {107980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wood:1991:MET:107972.107981,
 author = {Wood, David A. and Hill, Mark D. and Kessler, R. E.},
 title = {A model for estimating trace-sample miss ratios},
 abstract = {Unknown references, also known as cold-start misses, arise during trace-driven simulation of uniprocessor caches because of the unknown initial conditions. Accurately estimating the miss ratio of unknown references, denoted by \&amp;mu;, is particularly important when simulating large caches with short trace samples, since many references may be unknown.In this paper we make three contributions regarding \&amp;mu;. First, we provide empirical evidence that \&amp;mu; is much larger than the overall miss ratio (e.g., 0.40 vs. 0.02). Prior work suggests that they should be the same. Second, we develop a model that explains our empirical results for long trace samples. In our model, each block frame is either live</i>, if its next reference will hit, or dead, if its next reference will miss. We model each block frame as an alternating renewal process, and use the renewal-reward theorem to show that \&amp;mu; is simply the fraction of time block frames are dead. Finally, we extend the model to handle short trace samples and use it to develop several estimators of \&amp;mu;. Trace-driven simulation results show these estimators lead to better estimates of overall miss ratios than do previous methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {79--89},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107972.107981},
 doi = {http://doi.acm.org/10.1145/107972.107981},
 acmid = {107981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wood:1991:MET:107971.107981,
 author = {Wood, David A. and Hill, Mark D. and Kessler, R. E.},
 title = {A model for estimating trace-sample miss ratios},
 abstract = {Unknown references, also known as cold-start misses, arise during trace-driven simulation of uniprocessor caches because of the unknown initial conditions. Accurately estimating the miss ratio of unknown references, denoted by \&amp;mu;, is particularly important when simulating large caches with short trace samples, since many references may be unknown.In this paper we make three contributions regarding \&amp;mu;. First, we provide empirical evidence that \&amp;mu; is much larger than the overall miss ratio (e.g., 0.40 vs. 0.02). Prior work suggests that they should be the same. Second, we develop a model that explains our empirical results for long trace samples. In our model, each block frame is either live</i>, if its next reference will hit, or dead, if its next reference will miss. We model each block frame as an alternating renewal process, and use the renewal-reward theorem to show that \&amp;mu; is simply the fraction of time block frames are dead. Finally, we extend the model to handle short trace samples and use it to develop several estimators of \&amp;mu;. Trace-driven simulation results show these estimators lead to better estimates of overall miss ratios than do previous methods.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {79--89},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107971.107981},
 doi = {http://doi.acm.org/10.1145/107971.107981},
 acmid = {107981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chiang:1991:EMV:107972.107982,
 author = {Chiang, Mee-Chow and Sohi, Gurindar S.},
 title = {Experience with mean value analysis model for evaluating shared bus, throughput-oriented multiprocessors},
 abstract = {We report on our experience with the accuracy of mean value analysis analytical models for evaluating shared bus multiprocessors operating in a throughput-oriented environment. Having developed separate models for multiprocessors with circuit switched and split transaction, pipelined (packet switched) buses, wc compare the results of the models with those of an actual trace-driven simulation for 5,376 multiprocessor configurations.We find that the analytical models are accurate in predicting the individual processor throughputs and partial bus utilizations. For processor throughput, the difference between the results of the models and simulation are within 1\% for 75\% of the cases and within 3\% in 94\% of all cases. For partial bus utilization the model results are with 1\% of simulation results in 70\% of all cases and within 3\% in 92\% of all cases. The models are less accurate in predicting cache miss latency.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {90--100},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107972.107982},
 doi = {http://doi.acm.org/10.1145/107972.107982},
 acmid = {107982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiang:1991:EMV:107971.107982,
 author = {Chiang, Mee-Chow and Sohi, Gurindar S.},
 title = {Experience with mean value analysis model for evaluating shared bus, throughput-oriented multiprocessors},
 abstract = {We report on our experience with the accuracy of mean value analysis analytical models for evaluating shared bus multiprocessors operating in a throughput-oriented environment. Having developed separate models for multiprocessors with circuit switched and split transaction, pipelined (packet switched) buses, wc compare the results of the models with those of an actual trace-driven simulation for 5,376 multiprocessor configurations.We find that the analytical models are accurate in predicting the individual processor throughputs and partial bus utilizations. For processor throughput, the difference between the results of the models and simulation are within 1\% for 75\% of the cases and within 3\% in 94\% of all cases. For partial bus utilization the model results are with 1\% of simulation results in 70\% of all cases and within 3\% in 92\% of all cases. The models are less accurate in predicting cache miss latency.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {90--100},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/107971.107982},
 doi = {http://doi.acm.org/10.1145/107971.107982},
 acmid = {107982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gupta:1991:PAT:107972.107983,
 author = {Gupta, Anurag and Akyildiz, Ian and Fujimoto, Richard M.},
 title = {Performance analysis of Time Warp with homogeneous processors and exponential task times},
 abstract = {The behavior of n interacting processors synchronized by the "Time Warp" protocol is analyzed using a discrete state continuous time Markov chain model. The performance and dynamics of the processes are analyzed under the following assumptions: exponential task times and times-tamp increments on messages, each event message generates one new message that is sent to a randomly selected process, negligible rollback, state saving, and communication delay, unbounded message buffers, and homogeneous processors that are never idle. We determine the fraction of processed events that commit, speedup, rollback probability, expected length of rollback, the probability mass function for the number of uncommitted processed events, and the probability distribution function for the virtual time of a process. The analysis is approximate, so the results have been validated through performance measurements of a Time Warp testbed (PHOLD workload model) executing on a shared memory multiprocessor.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {101--110},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107983},
 doi = {http://doi.acm.org/10.1145/107972.107983},
 acmid = {107983},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gupta:1991:PAT:107971.107983,
 author = {Gupta, Anurag and Akyildiz, Ian and Fujimoto, Richard M.},
 title = {Performance analysis of Time Warp with homogeneous processors and exponential task times},
 abstract = {The behavior of n interacting processors synchronized by the "Time Warp" protocol is analyzed using a discrete state continuous time Markov chain model. The performance and dynamics of the processes are analyzed under the following assumptions: exponential task times and times-tamp increments on messages, each event message generates one new message that is sent to a randomly selected process, negligible rollback, state saving, and communication delay, unbounded message buffers, and homogeneous processors that are never idle. We determine the fraction of processed events that commit, speedup, rollback probability, expected length of rollback, the probability mass function for the number of uncommitted processed events, and the probability distribution function for the virtual time of a process. The analysis is approximate, so the results have been validated through performance measurements of a Time Warp testbed (PHOLD workload model) executing on a shared memory multiprocessor.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {101--110},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107983},
 doi = {http://doi.acm.org/10.1145/107971.107983},
 acmid = {107983},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:1991:SDH:107972.107984,
 author = {Kim, Jong and Das, Chita R.},
 title = {On subcube dependability in a hypercube},
 abstract = {In this paper, we present an analytical model for computing the dependability of hypercube systems. The model, referred to as task-based dependability (TBD), is developed under the assumption that a task needs at least an m</i>-cube (m</i> \&amp;lt; n</i>) in an n-cube for its execution. Two probabilistic terms are required for computing this dependability. The first is the probability of any x</i> nodes working out of 2<sup>n</sup> nodes. The second term is a conditional probability that at least a connected m-cube exists among those x</i> working nodes. This term is computed using a recursive expression. Two dependability measures, reliability and availability, are analyzed in this paper. A combinatorial enumeration is used in the reliability analysis, and a machine repairman model is used in the availability analysis to find the first probability. The machine repairman model is modified to capture imperfect coverage and imprecise repair. The TBD model is also extended to find multitask dependability. Numerical results are presented for n-cubes with different task requirements and are validated through extensive simulation. It is observed that an m-cube requirement is highly restrictive compared to the simple 2<sup>m</sup>-connected node requirement.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {111--119},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/107972.107984},
 doi = {http://doi.acm.org/10.1145/107972.107984},
 acmid = {107984},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:1991:SDH:107971.107984,
 author = {Kim, Jong and Das, Chita R.},
 title = {On subcube dependability in a hypercube},
 abstract = {In this paper, we present an analytical model for computing the dependability of hypercube systems. The model, referred to as task-based dependability (TBD), is developed under the assumption that a task needs at least an m</i>-cube (m</i> \&amp;lt; n</i>) in an n-cube for its execution. Two probabilistic terms are required for computing this dependability. The first is the probability of any x</i> nodes working out of 2<sup>n</sup> nodes. The second term is a conditional probability that at least a connected m-cube exists among those x</i> working nodes. This term is computed using a recursive expression. Two dependability measures, reliability and availability, are analyzed in this paper. A combinatorial enumeration is used in the reliability analysis, and a machine repairman model is used in the availability analysis to find the first probability. The machine repairman model is modified to capture imperfect coverage and imprecise repair. The TBD model is also extended to find multitask dependability. Numerical results are presented for n-cubes with different task requirements and are validated through extensive simulation. It is observed that an m-cube requirement is highly restrictive compared to the simple 2<sup>m</sup>-connected node requirement.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {111--119},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/107971.107984},
 doi = {http://doi.acm.org/10.1145/107971.107984},
 acmid = {107984},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gupta:1991:IOS:107972.107985,
 author = {Gupta, Anoop and Tucker, Andrew and Urushibara, Shigeru},
 title = {The impact of operating system scheduling policies and synchronization methods of performance of parallel applications},
 abstract = {Shared-memory multiprocessors are frequently used as compute servers with multiple parallel applications executing at the same time. In such environments, the efficiency of a parallel application can be significantly affected by the operating system scheduling policy. In this paper, we use detailed simulation studies to evaluate the performance of several different scheduling strategies, These include regular priority scheduling, coscheduling or gang scheduling, process control with processor partitioning, handoff scheduling, and affinity-based scheduling. We also explore tradeoffs between the use of busy-waiting and blocking synchronization primitives and their interactions with the scheduling strategies. Since effective use of caches is essential to achieving high performance, a key focus is on the impact of the scheduling strategies on the caching behavior of the applications.Our results show that in situations where the number of processes exceeds the number of processors, regular priority-based scheduling in conjunction with busy-waiting synchronization primitives results in extremely poor processor utilization. In such situations, use of blocking synchronization primitives can significantly improve performance. Process control and gang scheduling strategies are shown to offer the highest performance, and their performance is relatively independent of the synchronization method used. However, for applications that have sizable working sets that fit into the cache, process control performs better than gang scheduling. For the applications considered, the performance gains due to handoff scheduling and processor affinity are shown to be small.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {120--132},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/107972.107985},
 doi = {http://doi.acm.org/10.1145/107972.107985},
 acmid = {107985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gupta:1991:IOS:107971.107985,
 author = {Gupta, Anoop and Tucker, Andrew and Urushibara, Shigeru},
 title = {The impact of operating system scheduling policies and synchronization methods of performance of parallel applications},
 abstract = {Shared-memory multiprocessors are frequently used as compute servers with multiple parallel applications executing at the same time. In such environments, the efficiency of a parallel application can be significantly affected by the operating system scheduling policy. In this paper, we use detailed simulation studies to evaluate the performance of several different scheduling strategies, These include regular priority scheduling, coscheduling or gang scheduling, process control with processor partitioning, handoff scheduling, and affinity-based scheduling. We also explore tradeoffs between the use of busy-waiting and blocking synchronization primitives and their interactions with the scheduling strategies. Since effective use of caches is essential to achieving high performance, a key focus is on the impact of the scheduling strategies on the caching behavior of the applications.Our results show that in situations where the number of processes exceeds the number of processors, regular priority-based scheduling in conjunction with busy-waiting synchronization primitives results in extremely poor processor utilization. In such situations, use of blocking synchronization primitives can significantly improve performance. Process control and gang scheduling strategies are shown to offer the highest performance, and their performance is relatively independent of the synchronization method used. However, for applications that have sizable working sets that fit into the cache, process control performs better than gang scheduling. For the applications considered, the performance gains due to handoff scheduling and processor affinity are shown to be small.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {120--132},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/107971.107985},
 doi = {http://doi.acm.org/10.1145/107971.107985},
 acmid = {107985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhou:1991:PSL:107971.107986,
 author = {Zhou, Songnian and Brecht, Timothy},
 title = {Processor-pool-based scheduling for large-scale NUMA multiprocessors},
 abstract = {Large-scale Non-Uniform Memory Access (NUMA) multiprocessors are gaining increased attention due to their potential for achieving high performance through the replication of relatively simple components. Because of the complexity of such systems, scheduling algorithms for parallel applications are crucial in realizing the performance potential of these systems. In particular, scheduling methods must consider the scale of the system, with the increased likelihood of creating bottlenecks, along with the NUMA characteristics of the system, and the benefits to be gained by placing threads close to their code and data.We propose a class of scheduling algorithms based on processor pools</i>. A processor pool is a software construct for organizing and managing a large number of processors by dividing them into groups called pools. The parallel threads of a job are run in a single processor pool, unless there are performance advantages for a job to span multiple pools. Several jobs may share one pool. Our simulation experiments show that processor pool-based scheduling may effectively reduce the average job response time. The performance improvements attained by using processor pools increase with the average parallelism of the jobs, the load level of the system, the differentials in memory access costs, and the likelihood of having system bottlenecks. As the system size increasesr, while maintaining the workload composition and intensity, we observed that processor pools can be used to provide significant performance improvements. We therefore conclude that processor pool-based scheduling may be an effective and efficient technique for scalable systems.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {133--142},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107986},
 doi = {http://doi.acm.org/10.1145/107971.107986},
 acmid = {107986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zhou:1991:PSL:107972.107986,
 author = {Zhou, Songnian and Brecht, Timothy},
 title = {Processor-pool-based scheduling for large-scale NUMA multiprocessors},
 abstract = {Large-scale Non-Uniform Memory Access (NUMA) multiprocessors are gaining increased attention due to their potential for achieving high performance through the replication of relatively simple components. Because of the complexity of such systems, scheduling algorithms for parallel applications are crucial in realizing the performance potential of these systems. In particular, scheduling methods must consider the scale of the system, with the increased likelihood of creating bottlenecks, along with the NUMA characteristics of the system, and the benefits to be gained by placing threads close to their code and data.We propose a class of scheduling algorithms based on processor pools</i>. A processor pool is a software construct for organizing and managing a large number of processors by dividing them into groups called pools. The parallel threads of a job are run in a single processor pool, unless there are performance advantages for a job to span multiple pools. Several jobs may share one pool. Our simulation experiments show that processor pool-based scheduling may effectively reduce the average job response time. The performance improvements attained by using processor pools increase with the average parallelism of the jobs, the load level of the system, the differentials in memory access costs, and the likelihood of having system bottlenecks. As the system size increasesr, while maintaining the workload composition and intensity, we observed that processor pools can be used to provide significant performance improvements. We therefore conclude that processor pool-based scheduling may be an effective and efficient technique for scalable systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {133--142},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107986},
 doi = {http://doi.acm.org/10.1145/107972.107986},
 acmid = {107986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Squillante:1991:ATM:107972.107987,
 author = {Squillante, Mark S. and Nelson, Randolph D.},
 title = {Analysis of task migration in shared-memory multiprocessor scheduling},
 abstract = {In shared-memory multiprocessor systems it may be more efficient to schedule a task on one processor than on mother. Due to the inevitability of idle processors in these environments, there exists an important tradeoff between keeping the workload balanced and scheduling tasks where they run most efficiently. The purpose of an adaptive task migration policy is to determine the appropriate balance between the extremes of this load sharing tradeoff.We make the observation that there are considerable differences between this load sharing problem in distributed and shared-memory multiprocessor systems, and we formulate a queueing theoretic model of task migration to study the problem. A detailed mathematical analysis of the model is developed, which includes the effects of increased contention for system resources induced by the task migration policy. Our objective is to provide a better understanding of task migration in shared-memory multiprocessor environments. In particular, we illustrate the potential for significant improvements in system performance, and we show that even when migration costs are large it may still be beneficial to migrate waiting tasks to idle processors. We further demonstrate the potential for unstable behavior under migratory scheduling policies, and we provide optimal policy thresholds that yield the best performance and avoid this form of processor thrashing.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {143--155},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/107972.107987},
 doi = {http://doi.acm.org/10.1145/107972.107987},
 acmid = {107987},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Squillante:1991:ATM:107971.107987,
 author = {Squillante, Mark S. and Nelson, Randolph D.},
 title = {Analysis of task migration in shared-memory multiprocessor scheduling},
 abstract = {In shared-memory multiprocessor systems it may be more efficient to schedule a task on one processor than on mother. Due to the inevitability of idle processors in these environments, there exists an important tradeoff between keeping the workload balanced and scheduling tasks where they run most efficiently. The purpose of an adaptive task migration policy is to determine the appropriate balance between the extremes of this load sharing tradeoff.We make the observation that there are considerable differences between this load sharing problem in distributed and shared-memory multiprocessor systems, and we formulate a queueing theoretic model of task migration to study the problem. A detailed mathematical analysis of the model is developed, which includes the effects of increased contention for system resources induced by the task migration policy. Our objective is to provide a better understanding of task migration in shared-memory multiprocessor environments. In particular, we illustrate the potential for significant improvements in system performance, and we show that even when migration costs are large it may still be beneficial to migrate waiting tasks to idle processors. We further demonstrate the potential for unstable behavior under migratory scheduling policies, and we provide optimal policy thresholds that yield the best performance and avoid this form of processor thrashing.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {143--155},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/107971.107987},
 doi = {http://doi.acm.org/10.1145/107971.107987},
 acmid = {107987},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dan:1991:AMH:107972.107988,
 author = {Dan, Asit and Dias, Daniel M. and Yu, Philip S.},
 title = {Analytical modelling of a hierarchical buffer for a data sharing environment},
 abstract = {In a data sharing environment, where a number of loosely coupled computing nodes share a common storage subsystem, the effectiveness of a private buffer at each node is limited due to the multi-system invalidation effect, particularly under a non-uniform data access pattern. A global shared buffer can be introduced to alleviate this problem either as a disk cache or shared memory. In this paper we developed an approximate analytic model to evaluate different shared buffer management policies (SBMPs) which differ in their choice of data granules to be put into the shared buffer. The analytic model can be used to study the trade-offs of different SBMPs and the impact of different buffer allocations between shared and private buffers. The effects of various parameters, such as, the probability of update, the number of nodes, the sizes of private and shared buffer, etc., on the performance of SBMPS are captured in the analytic model. A detailed simulation model is also developed to validate the analytic model. We show that dependency between the contents of the private and shared buffers can play an important role in determining the effectiveness of the shared buffer particularly for a small number of nodes.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {156--167},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/107972.107988},
 doi = {http://doi.acm.org/10.1145/107972.107988},
 acmid = {107988},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dan:1991:AMH:107971.107988,
 author = {Dan, Asit and Dias, Daniel M. and Yu, Philip S.},
 title = {Analytical modelling of a hierarchical buffer for a data sharing environment},
 abstract = {In a data sharing environment, where a number of loosely coupled computing nodes share a common storage subsystem, the effectiveness of a private buffer at each node is limited due to the multi-system invalidation effect, particularly under a non-uniform data access pattern. A global shared buffer can be introduced to alleviate this problem either as a disk cache or shared memory. In this paper we developed an approximate analytic model to evaluate different shared buffer management policies (SBMPs) which differ in their choice of data granules to be put into the shared buffer. The analytic model can be used to study the trade-offs of different SBMPs and the impact of different buffer allocations between shared and private buffers. The effects of various parameters, such as, the probability of update, the number of nodes, the sizes of private and shared buffer, etc., on the performance of SBMPS are captured in the analytic model. A detailed simulation model is also developed to validate the analytic model. We show that dependency between the contents of the private and shared buffers can play an important role in determining the effectiveness of the shared buffer particularly for a small number of nodes.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {156--167},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/107971.107988},
 doi = {http://doi.acm.org/10.1145/107971.107988},
 acmid = {107988},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reiman:1991:PAC:107971.107989,
 author = {Reiman, Martin and Wright, Paul E.},
 title = {Performance analysis of concurrent-read exclusive-write},
 abstract = {We analyze the concurrent-read exclusive-write protocol for access to a shared resource, such as occurs in database and distributed operating systems. Readers arrive according to a Poisson process and acquire shareable i.e., non-exclusive, locks which, once granted, are released after a generally distributed random period. Writers arrive according to an arbitrary renewal process and acquire exclusive locks which, once granted, are held for a random time which is also generally distributed. Locks are granted in the order in which requests are received.We derive necessary and sufficient conditions under which the queue is stable i.e., the Iatencies for reader/writer lock acquisition have a limiting distribution. In the unstable case, the delays of successive readers/writers become unbounded. The stability condition is sensitive to the interarrival-time distribution of the writers and the lock holding-time distribution of the readers but depends only on the mean lock holding-time of the writers.Distributional and moment bounds are given for the latencies of read/write requests.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {168--177},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107989},
 doi = {http://doi.acm.org/10.1145/107971.107989},
 acmid = {107989},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reiman:1991:PAC:107972.107989,
 author = {Reiman, Martin and Wright, Paul E.},
 title = {Performance analysis of concurrent-read exclusive-write},
 abstract = {We analyze the concurrent-read exclusive-write protocol for access to a shared resource, such as occurs in database and distributed operating systems. Readers arrive according to a Poisson process and acquire shareable i.e., non-exclusive, locks which, once granted, are released after a generally distributed random period. Writers arrive according to an arbitrary renewal process and acquire exclusive locks which, once granted, are held for a random time which is also generally distributed. Locks are granted in the order in which requests are received.We derive necessary and sufficient conditions under which the queue is stable i.e., the Iatencies for reader/writer lock acquisition have a limiting distribution. In the unstable case, the delays of successive readers/writers become unbounded. The stability condition is sensitive to the interarrival-time distribution of the writers and the lock holding-time distribution of the readers but depends only on the mean lock holding-time of the writers.Distributional and moment bounds are given for the latencies of read/write requests.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {168--177},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107989},
 doi = {http://doi.acm.org/10.1145/107972.107989},
 acmid = {107989},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{French:1991:PMP:107972.107990,
 author = {French, James C. and Pratt, Terrence W. and Das, Mriganka},
 title = {Performance measurement of a parallel Input/Output system for the Intel iPSC/2 Hypercube},
 abstract = {The Intel Concurrent File System (CFS) for the iPSC/2 hypercube is one of the first production file systems to utilize the declustering of large files across numbers of disks to improve I/O performance. The CFS also makes use of dedicated I/O nodes, operating asynchronously, which provide file caching and prefetching. Processing of I/O requests is distributed between the compute node that initiates the request and the I/O nodes that service the request. The effects of the various design decisions in the Intel CFS are difficult to determine without measurements of an actual system. We present performance measurements of the CFS for a hypercube with 32 compute nodes and four I/0 nodes (four disks). Measurement of read/write rates for one compute node to one I/O node, one compute node to multiple I/O nodes, and multiple compute nodes to multiple I/O nodes form the basis for the study. Additional measurements show the effects of different buffer sizes, caching, prefetching, and file preallocation on system performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107990},
 doi = {http://doi.acm.org/10.1145/107972.107990},
 acmid = {107990},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{French:1991:PMP:107971.107990,
 author = {French, James C. and Pratt, Terrence W. and Das, Mriganka},
 title = {Performance measurement of a parallel Input/Output system for the Intel iPSC/2 Hypercube},
 abstract = {The Intel Concurrent File System (CFS) for the iPSC/2 hypercube is one of the first production file systems to utilize the declustering of large files across numbers of disks to improve I/O performance. The CFS also makes use of dedicated I/O nodes, operating asynchronously, which provide file caching and prefetching. Processing of I/O requests is distributed between the compute node that initiates the request and the I/O nodes that service the request. The effects of the various design decisions in the Intel CFS are difficult to determine without measurements of an actual system. We present performance measurements of the CFS for a hypercube with 32 compute nodes and four I/0 nodes (four disks). Measurement of read/write rates for one compute node to one I/O node, one compute node to multiple I/O nodes, and multiple compute nodes to multiple I/O nodes form the basis for the study. Additional measurements show the effects of different buffer sizes, caching, prefetching, and file preallocation on system performance.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {178--187},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107990},
 doi = {http://doi.acm.org/10.1145/107971.107990},
 acmid = {107990},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chervenak:1991:PDA:107971.107991,
 author = {Chervenak, Ann L. and Katz, Randy H.},
 title = {Performance of a disk array protype},
 abstract = {The RAID group at U.C. Berkeley recently built a prototype disk array. This paper examines the performance limits of each component of the array usiug SCSI bus traces, Sprite operating system traces and user programs.The array performs successfully for a workload of small, random I/O operations, achieving 275 I/Os per second on 14 disks before the Sun4/280 host becomes CPU-limited. The prototype is less successful in delivering high throughput for large, sequential operations. Memory system contention on the Sun4/280 host limits throughput to 2.3 MBytes/sec under the Sprite Operating System. Throughput is also limited by the bandwidth supported by the VME backplane, disk controller and disks, and overheads associated with the SCSI protocol.We conclude that merely using a powerful host CPU and many disks will not provide the full bandwidth possible from disk arrays. Host memory bandwidth and throughput of disk controllers are equally important. In addition, operating systems should avoid unnecessary copy and cache flush operations that can saturate the host memory system.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {188--197},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107991},
 doi = {http://doi.acm.org/10.1145/107971.107991},
 acmid = {107991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chervenak:1991:PDA:107972.107991,
 author = {Chervenak, Ann L. and Katz, Randy H.},
 title = {Performance of a disk array protype},
 abstract = {The RAID group at U.C. Berkeley recently built a prototype disk array. This paper examines the performance limits of each component of the array usiug SCSI bus traces, Sprite operating system traces and user programs.The array performs successfully for a workload of small, random I/O operations, achieving 275 I/Os per second on 14 disks before the Sun4/280 host becomes CPU-limited. The prototype is less successful in delivering high throughput for large, sequential operations. Memory system contention on the Sun4/280 host limits throughput to 2.3 MBytes/sec under the Sprite Operating System. Throughput is also limited by the bandwidth supported by the VME backplane, disk controller and disks, and overheads associated with the SCSI protocol.We conclude that merely using a powerful host CPU and many disks will not provide the full bandwidth possible from disk arrays. Host memory bandwidth and throughput of disk controllers are equally important. In addition, operating systems should avoid unnecessary copy and cache flush operations that can saturate the host memory system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {188--197},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107991},
 doi = {http://doi.acm.org/10.1145/107972.107991},
 acmid = {107991},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1991:PMD:107972.107992,
 author = {Chen, Shenze and Towsley, Don},
 title = {Performance of a mirrored disk in a real-time transaction system},
 abstract = {Disk mirroring has found widespread use in computer systems as a method for providing fault tolerance. In addition to increasing reliability, a mirrored disk can also reduce I/O response time by supporting the execution of parallel I/O requests. The improvement in I/O efficiency is extremely important in a real-time system, where each computational entity carries a deadline. In this paper, we present two classes of real-time disk scheduling policies, RT-DMQ and RT-CMQ, for a mirrored disk I/O subsystem and examine their performance in an integrated real-time transaction system. The real-time transaction system model is validated on a real-time database testbed, called RT-CARAT. The performance results show that a mirrored disk I/O subsystem can decrease the fraction of transactions that miss their deadlines over a single disk system by 68\%. Our results also reveal the importance of real-time scheduling policies, which can lead up to a 17\% performance improvement over non-real-time policies in terms of minimizing the transaction loss ratio.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {198--207},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107972.107992},
 doi = {http://doi.acm.org/10.1145/107972.107992},
 acmid = {107992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1991:PMD:107971.107992,
 author = {Chen, Shenze and Towsley, Don},
 title = {Performance of a mirrored disk in a real-time transaction system},
 abstract = {Disk mirroring has found widespread use in computer systems as a method for providing fault tolerance. In addition to increasing reliability, a mirrored disk can also reduce I/O response time by supporting the execution of parallel I/O requests. The improvement in I/O efficiency is extremely important in a real-time system, where each computational entity carries a deadline. In this paper, we present two classes of real-time disk scheduling policies, RT-DMQ and RT-CMQ, for a mirrored disk I/O subsystem and examine their performance in an integrated real-time transaction system. The real-time transaction system model is validated on a real-time database testbed, called RT-CARAT. The performance results show that a mirrored disk I/O subsystem can decrease the fraction of transactions that miss their deadlines over a single disk system by 68\%. Our results also reveal the importance of real-time scheduling policies, which can lead up to a 17\% performance improvement over non-real-time policies in terms of minimizing the transaction loss ratio.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {198--207},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/107971.107992},
 doi = {http://doi.acm.org/10.1145/107971.107992},
 acmid = {107992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Glenn:1991:IMP:107972.107993,
 author = {Glenn, R. R. and Pryor, D. V.},
 title = {Instrumentation for a massively parallel MIMD application},
 abstract = {This paper describes an application implemented on a simulated machine called Horizon. One purpose of this study is to investigate some of the features of a possible future machine (or class of machines) with a view toward deciding, early on in the research cycle, where problems may come up, what features should be added or strengthened, and what proposed features seem to be unnecessary. Another purpose is to learn more about how to program, instrument and debug a shared memory, massively parallel MIMD computer, and to begin to answer some of the questions: What tools does a programmer need to debug this type of machine? How can a programmer know if the machine is performing well? How can bottlenecks be identified? How can the massive amount of instrumentation information be condensed and presented to a user in a way that makes sense?},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {208--209},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107993},
 doi = {http://doi.acm.org/10.1145/107972.107993},
 acmid = {107993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Glenn:1991:IMP:107971.107993,
 author = {Glenn, R. R. and Pryor, D. V.},
 title = {Instrumentation for a massively parallel MIMD application},
 abstract = {This paper describes an application implemented on a simulated machine called Horizon. One purpose of this study is to investigate some of the features of a possible future machine (or class of machines) with a view toward deciding, early on in the research cycle, where problems may come up, what features should be added or strengthened, and what proposed features seem to be unnecessary. Another purpose is to learn more about how to program, instrument and debug a shared memory, massively parallel MIMD computer, and to begin to answer some of the questions: What tools does a programmer need to debug this type of machine? How can a programmer know if the machine is performing well? How can bottlenecks be identified? How can the massive amount of instrumentation information be condensed and presented to a user in a way that makes sense?},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {208--209},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107993},
 doi = {http://doi.acm.org/10.1145/107971.107993},
 acmid = {107993},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goldberg:1991:MMD:107971.107994,
 author = {Goldberg, Aaron and Hennessy, John},
 title = {MTOOL: a method for detecting memory bottlenecks},
 abstract = {This paper presents a new, relatively inexpensive method for detecting regions (e.g. loops and procedures) in a program where the memory hierarchy is performing poorly. By observing where actual measured execution time differs from the time predicted given a perfect memory system, we can isolate memory bottlenecks. MTOOL, an implementation of the approach aimed at applications programs running on MIPS-chip based workstations is described and results for some of the Perfect Club and SPEC benchmarks are summarized.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {210--211},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107994},
 doi = {http://doi.acm.org/10.1145/107971.107994},
 acmid = {107994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goldberg:1991:MMD:107972.107994,
 author = {Goldberg, Aaron and Hennessy, John},
 title = {MTOOL: a method for detecting memory bottlenecks},
 abstract = {This paper presents a new, relatively inexpensive method for detecting regions (e.g. loops and procedures) in a program where the memory hierarchy is performing poorly. By observing where actual measured execution time differs from the time predicted given a perfect memory system, we can isolate memory bottlenecks. MTOOL, an implementation of the approach aimed at applications programs running on MIPS-chip based workstations is described and results for some of the Perfect Club and SPEC benchmarks are summarized.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {210--211},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107994},
 doi = {http://doi.acm.org/10.1145/107972.107994},
 acmid = {107994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kim:1991:ISS:107972.107995,
 author = {Kim, Yul H. and Hill, Mark D. and Wood, David A.},
 title = {Implementing stack simulation for highly-associative memories},
 abstract = {Prior to this work, all implementations of stack simulation [MGS70] required more than linear time to process an address trace. In particular these implementations are often slow for highly-associative memories and traces with poor locality, as can be found in simulations of tile systems. We describe a new implementation of stack simulation where the refrrenced block and its stack distance are found using a hash table rather than by traversing the stack. The key to this implementation is that designers are rarely interested in a continuum of memory sizes, but instead desire metrics for only a discrete set of alternatives (e.g., powers of two). Our experimental evaluation shows the run-time of the new implementation to be linear in address trace length and independent of trace locality. Kim, et al., [KHW91] present the results of this research in more detail.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {212--213},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107995},
 doi = {http://doi.acm.org/10.1145/107972.107995},
 acmid = {107995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kim:1991:ISS:107971.107995,
 author = {Kim, Yul H. and Hill, Mark D. and Wood, David A.},
 title = {Implementing stack simulation for highly-associative memories},
 abstract = {Prior to this work, all implementations of stack simulation [MGS70] required more than linear time to process an address trace. In particular these implementations are often slow for highly-associative memories and traces with poor locality, as can be found in simulations of tile systems. We describe a new implementation of stack simulation where the refrrenced block and its stack distance are found using a hash table rather than by traversing the stack. The key to this implementation is that designers are rarely interested in a continuum of memory sizes, but instead desire metrics for only a discrete set of alternatives (e.g., powers of two). Our experimental evaluation shows the run-time of the new implementation to be linear in address trace length and independent of trace locality. Kim, et al., [KHW91] present the results of this research in more detail.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {212--213},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107995},
 doi = {http://doi.acm.org/10.1145/107971.107995},
 acmid = {107995},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Newman:1991:PAC:107972.107996,
 author = {Newman, Robb},
 title = {Performance analysis case study (abstract): application of experimental design \&amp; statistical data analysis techniques},
 abstract = {A common requirement of computer vendor's competitive performance analysis departments is to measure and report on the performance characteristics of another vendor's system. In many cases the amount of prior knowledge concerning the competitor's systcm is limited to sales brochures and non-technical publications. Availability of the system for benchmarking is minimal; there is little choice concerning memory and I/O configurations; and time to complete the project is short. A project of this nature is not, however, unique to computer vendors. Many users of computer systems that want to better understand a system's performance characteristics before deciding on a purchase, are also faced with similar restrictions.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {214--215},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107996},
 doi = {http://doi.acm.org/10.1145/107972.107996},
 acmid = {107996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Newman:1991:PAC:107971.107996,
 author = {Newman, Robb},
 title = {Performance analysis case study (abstract): application of experimental design \&amp; statistical data analysis techniques},
 abstract = {A common requirement of computer vendor's competitive performance analysis departments is to measure and report on the performance characteristics of another vendor's system. In many cases the amount of prior knowledge concerning the competitor's systcm is limited to sales brochures and non-technical publications. Availability of the system for benchmarking is minimal; there is little choice concerning memory and I/O configurations; and time to complete the project is short. A project of this nature is not, however, unique to computer vendors. Many users of computer systems that want to better understand a system's performance characteristics before deciding on a purchase, are also faced with similar restrictions.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {214--215},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107996},
 doi = {http://doi.acm.org/10.1145/107971.107996},
 acmid = {107996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Park:1991:MPB:107972.107997,
 author = {Park, Arvin and Becker, Jeffrey C.},
 title = {Measurements of the paging behavior of UNIX},
 abstract = {This paper analyzes measurements of paging activity from several different versions of UNIX. We set out to characterize paging activity by first taking measurements of it, and then writing programs to analyze it. In doing so, we were interested in answering several questions:1. What is the magnitude of paging traffic and how much of I/O system activity is paging related?2. What are the characteristics of paging activity, and how can paging system implementations be tuned to match them?3. How does paging activity vary across different machines, operating systems, and job mixes?4. How well does paging activity correlate with system load average and number of users?},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {216--217},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107997},
 doi = {http://doi.acm.org/10.1145/107972.107997},
 acmid = {107997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Park:1991:MPB:107971.107997,
 author = {Park, Arvin and Becker, Jeffrey C.},
 title = {Measurements of the paging behavior of UNIX},
 abstract = {This paper analyzes measurements of paging activity from several different versions of UNIX. We set out to characterize paging activity by first taking measurements of it, and then writing programs to analyze it. In doing so, we were interested in answering several questions:1. What is the magnitude of paging traffic and how much of I/O system activity is paging related?2. What are the characteristics of paging activity, and how can paging system implementations be tuned to match them?3. How does paging activity vary across different machines, operating systems, and job mixes?4. How well does paging activity correlate with system load average and number of users?},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {216--217},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107997},
 doi = {http://doi.acm.org/10.1145/107971.107997},
 acmid = {107997},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pasquale:1991:SDW:107971.107998,
 author = {Pasquale, Joseph and Bittel, Barbara and Kraiman, Daniel},
 title = {A static and dynamic workload characterization study of the San Diego Supercomputer center Cray X-MP},
 abstract = {The San Diego Supercomputer Center is one of four NSF sponsored national supercomputer centers. Up until January of 1990, its workhorse was a Cray X-MP, which served 2700 researchers from 170 institutions, spanning 44 states. In order to better understand how this supercomputer was utilized by its diverse community of users, we undertook a workload characterization study of the Cray X-MP. The goals of our study were twofold. First, we wished to characterize the workload at both the functional and resource levels. The functional level represents the user point of view: what types of programs users are running on the system. The resource level represents the system point of view: how the systems resources (CPU, memory, I/O bandwidth) are being used. Second, we wanted to see how the workload changed over an average weekday. Thus, we conducted a static characterization to understand its global attributes over the entire measurement period, as well as a dynamic workload characterization to understand the time behavior of the workload over a weekday cycle.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {218--219},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107998},
 doi = {http://doi.acm.org/10.1145/107971.107998},
 acmid = {107998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pasquale:1991:SDW:107972.107998,
 author = {Pasquale, Joseph and Bittel, Barbara and Kraiman, Daniel},
 title = {A static and dynamic workload characterization study of the San Diego Supercomputer center Cray X-MP},
 abstract = {The San Diego Supercomputer Center is one of four NSF sponsored national supercomputer centers. Up until January of 1990, its workhorse was a Cray X-MP, which served 2700 researchers from 170 institutions, spanning 44 states. In order to better understand how this supercomputer was utilized by its diverse community of users, we undertook a workload characterization study of the Cray X-MP. The goals of our study were twofold. First, we wished to characterize the workload at both the functional and resource levels. The functional level represents the user point of view: what types of programs users are running on the system. The resource level represents the system point of view: how the systems resources (CPU, memory, I/O bandwidth) are being used. Second, we wanted to see how the workload changed over an average weekday. Thus, we conducted a static characterization to understand its global attributes over the entire measurement period, as well as a dynamic workload characterization to understand the time behavior of the workload over a weekday cycle.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {218--219},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107998},
 doi = {http://doi.acm.org/10.1145/107972.107998},
 acmid = {107998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pu:1991:EMA:107972.107999,
 author = {Pu, Calton and Korz, Frederick and Lehman, Robert C.},
 title = {An experiment on measuring application performance over the Internet},
 abstract = {The use of wide area networks (WANs) such as the Internet is growing at a tremendous rate.Such networks hold great promise for new types of distributed applications, which will be widely distributed, highly replicated, intensely interactive, and adaptive to many types of network conditions. Developing such applications will require a solid understanding of the performance and availability characteristics of WANs as they evolve. The ability to measure the effect of these conditions will, for example, be important for large-volume applications such as digital libraries, and for near-real-time applications such as collaborative research and teleconferencing.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {220--221},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.107999},
 doi = {http://doi.acm.org/10.1145/107972.107999},
 acmid = {107999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pu:1991:EMA:107971.107999,
 author = {Pu, Calton and Korz, Frederick and Lehman, Robert C.},
 title = {An experiment on measuring application performance over the Internet},
 abstract = {The use of wide area networks (WANs) such as the Internet is growing at a tremendous rate.Such networks hold great promise for new types of distributed applications, which will be widely distributed, highly replicated, intensely interactive, and adaptive to many types of network conditions. Developing such applications will require a solid understanding of the performance and availability characteristics of WANs as they evolve. The ability to measure the effect of these conditions will, for example, be important for large-volume applications such as digital libraries, and for near-real-time applications such as collaborative research and teleconferencing.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {220--221},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.107999},
 doi = {http://doi.acm.org/10.1145/107971.107999},
 acmid = {107999},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yang:1991:PBA:107971.108000,
 author = {Yang, Myung K. and Das, Chita R.},
 title = {A parallel branch-and-bound algorithm for MIN-based multiprocessors},
 abstract = {A parallel "Decomposite Best-First" search Branch-and-Bound algorithm (pdbsbb</i>) for MIN-based multiprocessor systems is proposed in this paper. A conflict free mapping scheme, known as step-by-step spread</i>, is used to map the algorithm efficiently on to a MIN-based system for reducing communication overhead. It is shown that the proposed algorithm provides better speed-up than other reported schemes when communication overhead is taken into consideration.},
 booktitle = {Proceedings of the 1991 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '91},
 year = {1991},
 isbn = {0-89791-392-2},
 location = {San Diego, California, United States},
 pages = {222--223},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107971.108000},
 doi = {http://doi.acm.org/10.1145/107971.108000},
 acmid = {108000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yang:1991:PBA:107972.108000,
 author = {Yang, Myung K. and Das, Chita R.},
 title = {A parallel branch-and-bound algorithm for MIN-based multiprocessors},
 abstract = {A parallel "Decomposite Best-First" search Branch-and-Bound algorithm (pdbsbb</i>) for MIN-based multiprocessor systems is proposed in this paper. A conflict free mapping scheme, known as step-by-step spread</i>, is used to map the algorithm efficiently on to a MIN-based system for reducing communication overhead. It is shown that the proposed algorithm provides better speed-up than other reported schemes when communication overhead is taken into consideration.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {19},
 issue = {1},
 month = {April},
 year = {1991},
 issn = {0163-5999},
 pages = {222--223},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/107972.108000},
 doi = {http://doi.acm.org/10.1145/107972.108000},
 acmid = {108000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1993:NAI:166955.166966,
 author = {Chen, Peter M. and Patterson, David A.},
 title = {A new approach to I/O performance evaluation: self-scaling I/O benchmarks, predicted I/O performance},
 abstract = {Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of fie workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to quickly estimate the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a Sprite LFS DECstation 5000/200 with a three-disk disk array, a Convex C240 minisupercomputer with a four-disk disk array, and a Solbourne 5E/905 fileserver with a two-disk disk array.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166966},
 doi = {http://doi.acm.org/10.1145/166955.166966},
 acmid = {166966},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1993:NAI:166962.166966,
 author = {Chen, Peter M. and Patterson, David A.},
 title = {A new approach to I/O performance evaluation: self-scaling I/O benchmarks, predicted I/O performance},
 abstract = {Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of fie workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to quickly estimate the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a Sprite LFS DECstation 5000/200 with a three-disk disk array, a Convex C240 minisupercomputer with a four-disk disk array, and a Solbourne 5E/905 fileserver with a two-disk disk array.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166966},
 doi = {http://doi.acm.org/10.1145/166962.166966},
 acmid = {166966},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Biswas:1993:TDA:166962.166971,
 author = {Biswas, Prabuddha and Ramakrishnan, K. K. and Towsley, Don},
 title = {Trace driven analysis of write caching policies for disks},
 abstract = {The I/O subsystem in a computer system is becoming the bottleneck as a result of recent dramatic improvements in processor speeds. Disk caches have been effective in closing this gap but the benefit is restricted to the read operations as the write I/Os are usually committed to disk to maintain consistency and to allow for crash recovery. As a result, write I/O traffic is becoming dominant and solutions to alleviate this problem are becoming increasingly important. A simple solution which can easily work with existing tile systems is to use non-volatile disk caches together with a write-behind strategy. In this study, we look at the issues around managing such a cache using a detailed trace driven simulation. Traces from three different commercial sites are used in the analysis of various policies for managing the write cache.We observe that even a simple write-behind policy for the write cache is effective in reducing the total number of writes by over 50\%. We further observe that the use of hysteresis in the policy to purge the write cache, with two thresholds, yields substantial improvement over a single threshold scheme. The inclusion of a mechanism to piggyback blocks from the write cache with read miss I/Os further reduces the number of writes to only about 15\% of the original total number of write operations. We compare two piggybacking options and also study the impact of varying the write cache size. We briefly looked at the case of a single non-volatile disk cache to estimate the performance impact of statically partitioning the cache for reads and writes.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/166962.166971},
 doi = {http://doi.acm.org/10.1145/166962.166971},
 acmid = {166971},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Biswas:1993:TDA:166955.166971,
 author = {Biswas, Prabuddha and Ramakrishnan, K. K. and Towsley, Don},
 title = {Trace driven analysis of write caching policies for disks},
 abstract = {The I/O subsystem in a computer system is becoming the bottleneck as a result of recent dramatic improvements in processor speeds. Disk caches have been effective in closing this gap but the benefit is restricted to the read operations as the write I/Os are usually committed to disk to maintain consistency and to allow for crash recovery. As a result, write I/O traffic is becoming dominant and solutions to alleviate this problem are becoming increasingly important. A simple solution which can easily work with existing tile systems is to use non-volatile disk caches together with a write-behind strategy. In this study, we look at the issues around managing such a cache using a detailed trace driven simulation. Traces from three different commercial sites are used in the analysis of various policies for managing the write cache.We observe that even a simple write-behind policy for the write cache is effective in reducing the total number of writes by over 50\%. We further observe that the use of hysteresis in the policy to purge the write cache, with two thresholds, yields substantial improvement over a single threshold scheme. The inclusion of a mechanism to piggyback blocks from the write cache with read miss I/Os further reduces the number of writes to only about 15\% of the original total number of write operations. We compare two piggybacking options and also study the impact of varying the write cache size. We briefly looked at the case of a single non-volatile disk cache to estimate the performance impact of statically partitioning the cache for reads and writes.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/166955.166971},
 doi = {http://doi.acm.org/10.1145/166955.166971},
 acmid = {166971},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sugumar:1993:ESC:166955.166974,
 author = {Sugumar, Rabin A. and Abraham, Santosh G.},
 title = {Efficient simulation of caches under optimal replacement with applications to miss characterization},
 abstract = {Cache miss characterization models such as the three Cs model are useful in developing schemes to reduce cache misses and their penalty. In this paper we propose the OPT model that uses cache simulation under optimal (OPT) replacement to obtain a finer and more accurate characterization of misses than the three Cs model. However, current methods for optimal cache simulation are slow and difficult to use. We present three new techniques for optimal cache simulation. First, we propose a limited lookahead strategy with error fixing, which allows one pass simulation of multiple optimal caches. Second, we propose a scheme to group entries in the OPT stack, which allows efficient tree based fully-associative cache simulation under OPT. Third, we propose a scheme for exploiting partial inclusion in set-associative cache simulation under OPT. Simulators based on these algorithms were used to obtain cache miss characterizations using the OPT model for nine SPEC benchmarks. The results indicate that miss ratios under OPT are substantially lower than those under LRU replacement, by up to 70\% in fully-associative caches, and up to 32\% in two-way set-associative caches.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {24--35},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166974},
 doi = {http://doi.acm.org/10.1145/166955.166974},
 acmid = {166974},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sugumar:1993:ESC:166962.166974,
 author = {Sugumar, Rabin A. and Abraham, Santosh G.},
 title = {Efficient simulation of caches under optimal replacement with applications to miss characterization},
 abstract = {Cache miss characterization models such as the three Cs model are useful in developing schemes to reduce cache misses and their penalty. In this paper we propose the OPT model that uses cache simulation under optimal (OPT) replacement to obtain a finer and more accurate characterization of misses than the three Cs model. However, current methods for optimal cache simulation are slow and difficult to use. We present three new techniques for optimal cache simulation. First, we propose a limited lookahead strategy with error fixing, which allows one pass simulation of multiple optimal caches. Second, we propose a scheme to group entries in the OPT stack, which allows efficient tree based fully-associative cache simulation under OPT. Third, we propose a scheme for exploiting partial inclusion in set-associative cache simulation under OPT. Simulators based on these algorithms were used to obtain cache miss characterizations using the OPT model for nine SPEC benchmarks. The results indicate that miss ratios under OPT are substantially lower than those under LRU replacement, by up to 70\% in fully-associative caches, and up to 32\% in two-way set-associative caches.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {24--35},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166974},
 doi = {http://doi.acm.org/10.1145/166962.166974},
 acmid = {166974},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chame:1993:CIP:166955.166977,
 author = {Cha Jacqueline and Dubois, Michel},
 title = {Cache inclusion and processor sampling in multiprocessor simulations},
 abstract = {The evaluation of cache-based systems demands careful simulations of entire benchmarks. Simulation efficiency is essential to realistic evaluations. For systems with large caches and large number of processors, simulation is often too slow to be practical. In particular, the optimized design of a cache for a multiprocessor is very complex with current techniques.This paper addresses these problems. First we introduce necessary and sufficient conditions for cache inclusion in systems with invalidations. Second, under cache inclusion, we show that an accurate trace for a given processor or for a cluster of processors can be extracted from a multiprocessor trace. With this methodology, possible cache architectures for a processor or for a cluster of processors are evaluated independently of the rest of the system, resulting in a drastic reduction of the trace length and simulation complexity. Moreover, many important system-wide metrics can be estimated with good accuracy by extracting the traces of a set of randomly selected processors, an approach we call processor sampling. We demonstrate the accuracy and efficiency of these techniques by applying them to three 64-processor traces.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166977},
 doi = {http://doi.acm.org/10.1145/166955.166977},
 acmid = {166977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chame:1993:CIP:166962.166977,
 author = {Cha Jacqueline and Dubois, Michel},
 title = {Cache inclusion and processor sampling in multiprocessor simulations},
 abstract = {The evaluation of cache-based systems demands careful simulations of entire benchmarks. Simulation efficiency is essential to realistic evaluations. For systems with large caches and large number of processors, simulation is often too slow to be practical. In particular, the optimized design of a cache for a multiprocessor is very complex with current techniques.This paper addresses these problems. First we introduce necessary and sufficient conditions for cache inclusion in systems with invalidations. Second, under cache inclusion, we show that an accurate trace for a given processor or for a cluster of processors can be extracted from a multiprocessor trace. With this methodology, possible cache architectures for a processor or for a cluster of processors are evaluated independently of the rest of the system, resulting in a drastic reduction of the trace length and simulation complexity. Moreover, many important system-wide metrics can be estimated with good accuracy by extracting the traces of a set of randomly selected processors, an approach we call processor sampling. We demonstrate the accuracy and efficiency of these techniques by applying them to three 64-processor traces.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166977},
 doi = {http://doi.acm.org/10.1145/166962.166977},
 acmid = {166977},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reinhardt:1993:WWT:166962.166979,
 author = {Reinhardt, Steven K. and Hill, Mark D. and Larus, James R. and Lebeck, Alvin R. and Lewis, James C. and Wood, David A.},
 title = {The Wisconsin Wind Tunnel: virtual prototyping of parallel computers},
 abstract = {We have developed a new technique for evaluating cache coherent, shared-memory computers. The Wisconsin Wind Tunnel (WWT) runs a parallel shared-memory program on a parallel computer (CM-5) and uses execution-driven, distributed, discrete-event simulation to accurately calculate program execution time. WWT is a virtual prototype that exploits similarities between the system under design (the target) and an existing evaluation platform (the host). The host directly executes all target program instructions and memory references that hit in the target cache. WWT's shared memory uses the CM-5 memory's error-correcting code (ECC) as valid bits for a fine-grained extension of shared virtual memory. Only memory references that miss in the target cache trap to WWT, which simulates a cache-coherence protocol. WWT correctly interleaves target machine events and calculates target program execution time. WWT runs on parallel computers with greater speed and memory capacity than uniprocessors. WWT's simulation time decreases as target system size increases for fixed-size problems and holds roughly constant as the target system and problem scale.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {48--60},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166962.166979},
 doi = {http://doi.acm.org/10.1145/166962.166979},
 acmid = {166979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reinhardt:1993:WWT:166955.166979,
 author = {Reinhardt, Steven K. and Hill, Mark D. and Larus, James R. and Lebeck, Alvin R. and Lewis, James C. and Wood, David A.},
 title = {The Wisconsin Wind Tunnel: virtual prototyping of parallel computers},
 abstract = {We have developed a new technique for evaluating cache coherent, shared-memory computers. The Wisconsin Wind Tunnel (WWT) runs a parallel shared-memory program on a parallel computer (CM-5) and uses execution-driven, distributed, discrete-event simulation to accurately calculate program execution time. WWT is a virtual prototype that exploits similarities between the system under design (the target) and an existing evaluation platform (the host). The host directly executes all target program instructions and memory references that hit in the target cache. WWT's shared memory uses the CM-5 memory's error-correcting code (ECC) as valid bits for a fine-grained extension of shared virtual memory. Only memory references that miss in the target cache trap to WWT, which simulates a cache-coherence protocol. WWT correctly interleaves target machine events and calculates target program execution time. WWT runs on parallel computers with greater speed and memory capacity than uniprocessors. WWT's simulation time decreases as target system size increases for fixed-size problems and holds roughly constant as the target system and problem scale.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {48--60},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166955.166979},
 doi = {http://doi.acm.org/10.1145/166955.166979},
 acmid = {166979},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Adve:1993:IRD:166955.166982,
 author = {Adve, Vikram S. and Vernon, Mary K.},
 title = {The influence of random delays on parallel execution times},
 abstract = {Stochastic models are widely used for the performance evaluation of parallel programs and systems. The stochastic assumptions in such models exe intended to represent non-deterministic processing requirements as well as random delays due to inter-process communication end resource contention. In this paper, we provide compelling analytical and experimental evidence that in current and foreseeable shared-memory programs, communication delays introduce negligible variance into the execution time between synchronization points. Furthermore, we show using direct measurements of variance that other sources of randomness, particularly non-deterministic computational requirements, also do not introduce significant variance in many programs. We then use two examples to demonstrate the implications of these results for parallel program performance prediction models, as well as for general stochastic models of parallel systems.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {61--73},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166955.166982},
 doi = {http://doi.acm.org/10.1145/166955.166982},
 acmid = {166982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Adve:1993:IRD:166962.166982,
 author = {Adve, Vikram S. and Vernon, Mary K.},
 title = {The influence of random delays on parallel execution times},
 abstract = {Stochastic models are widely used for the performance evaluation of parallel programs and systems. The stochastic assumptions in such models exe intended to represent non-deterministic processing requirements as well as random delays due to inter-process communication end resource contention. In this paper, we provide compelling analytical and experimental evidence that in current and foreseeable shared-memory programs, communication delays introduce negligible variance into the execution time between synchronization points. Furthermore, we show using direct measurements of variance that other sources of randomness, particularly non-deterministic computational requirements, also do not introduce significant variance in many programs. We then use two examples to demonstrate the implications of these results for parallel program performance prediction models, as well as for general stochastic models of parallel systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {61--73},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166962.166982},
 doi = {http://doi.acm.org/10.1145/166962.166982},
 acmid = {166982},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rosti:1993:KEM:166962.166985,
 author = {Rosti, E. and Smirni, E. and Wagner, T. D. and Apon, A. W. and Dowdy, L. W.},
 title = {The KSR1: experimentation and modeling of poststore},
 abstract = {Kendall Square Research introduced the KSR1 system in 1991. The architecture is based on a ring of rings of 64-bit microprocessora. It is a distributed, shared memory system and is scalable. The memory structure is unique and is the key to understanding the system. Different levels of caching eliminates physical memory addressing and leads to the ALLCACHE\&amp;trade; scheme. Since requested data may be found in any of several caches, the initial access time is variable. Once pulled into the local (sub) cache, subsequent access times are fixed and minimal. Thus, the KSR1 is a Cache-Only Memory Architecture (COMA) system.This paper describes experimentation and an analytic model of the KSR1. The focus is on the poststore programmer option. With the poststore option, the programm er can elect to broadcast the updated value of a variable to all processors that might have a copy. This may save time for threads on other processors, but delays the broadcasting thread and places additional traffic on the ring. The specific issue addressed is to determine under what conditions poststore is beneficial. The analytic model and the experimental observations are in good agreement. They indicate that the decision to use poststore depends both on the application and the current system load.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166985},
 doi = {http://doi.acm.org/10.1145/166962.166985},
 acmid = {166985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rosti:1993:KEM:166955.166985,
 author = {Rosti, E. and Smirni, E. and Wagner, T. D. and Apon, A. W. and Dowdy, L. W.},
 title = {The KSR1: experimentation and modeling of poststore},
 abstract = {Kendall Square Research introduced the KSR1 system in 1991. The architecture is based on a ring of rings of 64-bit microprocessora. It is a distributed, shared memory system and is scalable. The memory structure is unique and is the key to understanding the system. Different levels of caching eliminates physical memory addressing and leads to the ALLCACHE\&amp;trade; scheme. Since requested data may be found in any of several caches, the initial access time is variable. Once pulled into the local (sub) cache, subsequent access times are fixed and minimal. Thus, the KSR1 is a Cache-Only Memory Architecture (COMA) system.This paper describes experimentation and an analytic model of the KSR1. The focus is on the poststore programmer option. With the poststore option, the programm er can elect to broadcast the updated value of a variable to all processors that might have a copy. This may save time for threads on other processors, but delays the broadcasting thread and places additional traffic on the ring. The specific issue addressed is to determine under what conditions poststore is beneficial. The analytic model and the experimental observations are in good agreement. They indicate that the decision to use poststore depends both on the application and the current system load.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166985},
 doi = {http://doi.acm.org/10.1145/166955.166985},
 acmid = {166985},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ganger:1993:PME:166962.166989,
 author = {Ganger, Gregory R. and Patt, Yale N.},
 title = {The process-flow model: examining I/O performance from the system's point of view},
 abstract = {Input/output subsystem performance is currently receiving considerable research attention. Significant effort has been focused on reducing average I/O response times and increasing throughput for a given workload. This work has resulted in tremendous advances in I/O subsystem performance. It is unclear, however, how these improvements will be reflected in overall system performance. The central problem lies in the fact that the current method of study tends to treat all I/O requests aa equally important. We introduce a three class taxonomy of I/O requests based on their effects on system performance. We denote the three classes time-critical, time-limited, and time-noncritical</i>. A system-level, trace-driven simulation model has been developed for the purpose of studying disk scheduling algorithms. By incorporating knowledge of I/O classes, algorithms tuned for system performance rather than I/O subsystem performance may be developed. Traditional I/O subsystem simulators would rate such algorithms unfavorably because they produce suboptimal subsystem performance. By studying the I/O subsystem via global, system-level simulation, one can more easily identify changes that will improve overall system performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166989},
 doi = {http://doi.acm.org/10.1145/166962.166989},
 acmid = {166989},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ganger:1993:PME:166955.166989,
 author = {Ganger, Gregory R. and Patt, Yale N.},
 title = {The process-flow model: examining I/O performance from the system's point of view},
 abstract = {Input/output subsystem performance is currently receiving considerable research attention. Significant effort has been focused on reducing average I/O response times and increasing throughput for a given workload. This work has resulted in tremendous advances in I/O subsystem performance. It is unclear, however, how these improvements will be reflected in overall system performance. The central problem lies in the fact that the current method of study tends to treat all I/O requests aa equally important. We introduce a three class taxonomy of I/O requests based on their effects on system performance. We denote the three classes time-critical, time-limited, and time-noncritical</i>. A system-level, trace-driven simulation model has been developed for the purpose of studying disk scheduling algorithms. By incorporating knowledge of I/O classes, algorithms tuned for system performance rather than I/O subsystem performance may be developed. Traditional I/O subsystem simulators would rate such algorithms unfavorably because they produce suboptimal subsystem performance. By studying the I/O subsystem via global, system-level simulation, one can more easily identify changes that will improve overall system performance.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {86--97},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166989},
 doi = {http://doi.acm.org/10.1145/166955.166989},
 acmid = {166989},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1993:APM:166962.166994,
 author = {Lee, Edward K. and Katz, Randy H.},
 title = {An analytic performance model of disk arrays},
 abstract = {As disk arrays become widely used, tools for understanding and analyzing their performance become increasingly important. In particular, performance models can be invaluable in both configuring and designing disk arrays. Accurate analytic performance models are preferable to other types of models because they can be quickly evaluated, are applicable under a wide range of system and workload parameters, and can be manipulated by a range of mathematical techniques. Unfortunately, analytic performance models of disk arrays are difficult to formulate due to the presence of queueing</i> and fork-join synchronization</i>; a disk array request is broken up into independent disk requests which must all complete to satisfy the original request. In this paper, we develop and validate an analytic performance model for disk arrays. We derive simple equations for approximating their utilization, response time and throughput. We validate the analytic model via simulation, investigate the error introduced by each approximation used in deriving the analytic model, and examine the validity of some of the conclusions that can be drawn from the model.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166994},
 doi = {http://doi.acm.org/10.1145/166962.166994},
 acmid = {166994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1993:APM:166955.166994,
 author = {Lee, Edward K. and Katz, Randy H.},
 title = {An analytic performance model of disk arrays},
 abstract = {As disk arrays become widely used, tools for understanding and analyzing their performance become increasingly important. In particular, performance models can be invaluable in both configuring and designing disk arrays. Accurate analytic performance models are preferable to other types of models because they can be quickly evaluated, are applicable under a wide range of system and workload parameters, and can be manipulated by a range of mathematical techniques. Unfortunately, analytic performance models of disk arrays are difficult to formulate due to the presence of queueing</i> and fork-join synchronization</i>; a disk array request is broken up into independent disk requests which must all complete to satisfy the original request. In this paper, we develop and validate an analytic performance model for disk arrays. We derive simple equations for approximating their utilization, response time and throughput. We validate the analytic model via simulation, investigate the error introduced by each approximation used in deriving the analytic model, and examine the validity of some of the conclusions that can be drawn from the model.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {98--109},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166994},
 doi = {http://doi.acm.org/10.1145/166955.166994},
 acmid = {166994},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tang:1993:MMD:166962.166996,
 author = {Tang, Dong and Iyer, Ravishankar K.},
 title = {MEASURE+: a measurement-based dependability analysis package},
 abstract = {Most existing dependability modeling and evaluation tools are designed for building and solving commonly used models with emphasis on solution techniques, not for identifying realistic models from measurements. In this paper, a measurement-based dependability analysis package, MEASURE+, is introduced. Given measured data from real systems in a specified format MEASURE+ can generate appropriate dependability models and measures including Markov and semi-Markov models, k</i>-out-of-n</i> availability models, failure distribution and hazard functions, and correlation parameters. These models and measures obtained from data are valuable for understanding actual error/failure characteristics, identifying system bottlenecks, evaluating dependability for real systems, and verifying assumptions made in analytical models. The paper illustrates MEASURE+ by applying it to the data from a VAXcluster multicomputer system. Models of field failure behavior identified by MEASURE+ indicate that both traditional models assuming failure independence and those few taking correlation into account are not representative of the actual occurrence process of correlated failures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {110--121},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.166996},
 doi = {http://doi.acm.org/10.1145/166962.166996},
 acmid = {166996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tang:1993:MMD:166955.166996,
 author = {Tang, Dong and Iyer, Ravishankar K.},
 title = {MEASURE+: a measurement-based dependability analysis package},
 abstract = {Most existing dependability modeling and evaluation tools are designed for building and solving commonly used models with emphasis on solution techniques, not for identifying realistic models from measurements. In this paper, a measurement-based dependability analysis package, MEASURE+, is introduced. Given measured data from real systems in a specified format MEASURE+ can generate appropriate dependability models and measures including Markov and semi-Markov models, k</i>-out-of-n</i> availability models, failure distribution and hazard functions, and correlation parameters. These models and measures obtained from data are valuable for understanding actual error/failure characteristics, identifying system bottlenecks, evaluating dependability for real systems, and verifying assumptions made in analytical models. The paper illustrates MEASURE+ by applying it to the data from a VAXcluster multicomputer system. Models of field failure behavior identified by MEASURE+ indicate that both traditional models assuming failure independence and those few taking correlation into account are not representative of the actual occurrence process of correlated failures.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {110--121},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.166996},
 doi = {http://doi.acm.org/10.1145/166955.166996},
 acmid = {166996},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ramesh:1993:STS:166955.166998,
 author = {Ramesh, A. V. and Trivedi, Kishor},
 title = {On the sensitivity of transient solutions of Markov models},
 abstract = {We consider the sensitivity of transient solutions of Markov models to perturbations in their generator matrices. The perturbations can either be of a certain structure or can be very general. We consider two different measures of sensitivity and derive upper bounds on them. The derived bounds are sharper than previously reported bounds in the literature. Since the sensitivity analysis of transient solutions is intimately related to the condition of the exponential of the CTMC matrix, we derive an expression for the condition number of the CTMC matrix exponential which leads to some interesting implications. We compare the derived sensitivity bounds both numerically and analytically with those reported in the literature.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {122--134},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166955.166998},
 doi = {http://doi.acm.org/10.1145/166955.166998},
 acmid = {166998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ramesh:1993:STS:166962.166998,
 author = {Ramesh, A. V. and Trivedi, Kishor},
 title = {On the sensitivity of transient solutions of Markov models},
 abstract = {We consider the sensitivity of transient solutions of Markov models to perturbations in their generator matrices. The perturbations can either be of a certain structure or can be very general. We consider two different measures of sensitivity and derive upper bounds on them. The derived bounds are sharper than previously reported bounds in the literature. Since the sensitivity analysis of transient solutions is intimately related to the condition of the exponential of the CTMC matrix, we derive an expression for the condition number of the CTMC matrix exponential which leads to some interesting implications. We compare the derived sensitivity bounds both numerically and analytically with those reported in the literature.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {122--134},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166962.166998},
 doi = {http://doi.acm.org/10.1145/166962.166998},
 acmid = {166998},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nicol:1993:PSM:166962.167000,
 author = {Nicol, David M. and Heidelberger, Philip},
 title = {Parallel simulation of Markovian queueing networks using adaptive uniformization},
 abstract = {This paper describes a method for simulating a large class of queueing network models with Markovian phase-type distributions on parallel architectures. The method, which is based on uniformization, exploits Markovian properties that permit one to first build schedules of simulation times at which processors ought to synchronize, and then simulate a mathematically correct sample path through the pre-chosen schedule. While the technique eliminates many of the overheads incurred by other synchronization methods, it may suffer when the maximum rate (in simulation time) at which one processor might possibly ever send jobs to another is much larger than the average rate at which it actually does. We show how to reduce these overheads, sometimes doubling the execution rate as a result. We discuss experiments performed on the Intel iPSC/2 and Touchstone Delta architectures, where speedups in excess of 155 are observed on 256 processors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {135--145},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/166962.167000},
 doi = {http://doi.acm.org/10.1145/166962.167000},
 acmid = {167000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nicol:1993:PSM:166955.167000,
 author = {Nicol, David M. and Heidelberger, Philip},
 title = {Parallel simulation of Markovian queueing networks using adaptive uniformization},
 abstract = {This paper describes a method for simulating a large class of queueing network models with Markovian phase-type distributions on parallel architectures. The method, which is based on uniformization, exploits Markovian properties that permit one to first build schedules of simulation times at which processors ought to synchronize, and then simulate a mathematically correct sample path through the pre-chosen schedule. While the technique eliminates many of the overheads incurred by other synchronization methods, it may suffer when the maximum rate (in simulation time) at which one processor might possibly ever send jobs to another is much larger than the average rate at which it actually does. We show how to reduce these overheads, sometimes doubling the execution rate as a result. We discuss experiments performed on the Intel iPSC/2 and Touchstone Delta architectures, where speedups in excess of 155 are observed on 256 processors.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {135--145},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/166955.167000},
 doi = {http://doi.acm.org/10.1145/166955.167000},
 acmid = {167000},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goldschmidt:1993:ATS:166955.167001,
 author = {Goldschmidt, Stephen R. and Hennessy, John L.},
 title = {The accuracy of trace-driven simulations of multiprocessors},
 abstract = {In trace-driven simulation, traces generated for one set of system characteristics are used to simulate a system with different characteristics. However, the execution path of a multiprocessor workload may depend on the order of events occurring on different processing elements. The event order, in turn, depends on system charcteristics such as memory-system latencies and buffer-sizes. Trace-driven simulations of multiprocessor workloads are inaccurate unless the dependencies are eliminated from the traces.We have measured the effects of these inaccuracies by comparing trace-driven simulations to direct simulations of the same workloads. The simulators predicted identical performance only for workloads whose traces were timing-independent. Workloads that used first-come first-served scheduling and/or non-deterministic algorithms produced timing-dependent traces, and simulation of these traces produced inaccurate performance predictions. Two types of performance metrics were particularly affected: those related to synchronization latency and those derived from relatively small numbers of events. To accurately predict such performance metrics, timing-independent traces or direct simulation should be used.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {146--157},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167001},
 doi = {http://doi.acm.org/10.1145/166955.167001},
 acmid = {167001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goldschmidt:1993:ATS:166962.167001,
 author = {Goldschmidt, Stephen R. and Hennessy, John L.},
 title = {The accuracy of trace-driven simulations of multiprocessors},
 abstract = {In trace-driven simulation, traces generated for one set of system characteristics are used to simulate a system with different characteristics. However, the execution path of a multiprocessor workload may depend on the order of events occurring on different processing elements. The event order, in turn, depends on system charcteristics such as memory-system latencies and buffer-sizes. Trace-driven simulations of multiprocessor workloads are inaccurate unless the dependencies are eliminated from the traces.We have measured the effects of these inaccuracies by comparing trace-driven simulations to direct simulations of the same workloads. The simulators predicted identical performance only for workloads whose traces were timing-independent. Workloads that used first-come first-served scheduling and/or non-deterministic algorithms produced timing-dependent traces, and simulation of these traces produced inaccurate performance predictions. Two types of performance metrics were particularly affected: those related to synchronization latency and those derived from relatively small numbers of events. To accurately predict such performance metrics, timing-independent traces or direct simulation should be used.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {146--157},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167001},
 doi = {http://doi.acm.org/10.1145/166962.167001},
 acmid = {167001},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Setia:1993:PSM:166955.167002,
 author = {Setia, Sanjeev K. and Squillante, Mark S. and Tripathi, Satish K.},
 title = {Processor scheduling on multiprogrammed, distributed memory parallel computers},
 abstract = {Multicomputers, consisting of many processing nodes connected through a high speed interconnection network, have become an important and common platform for a large body of scientific computations. These parallel systems have traditionally executed programs in batch mode, or have at most space-shared the processors among multiple programs using a static partitioning policy. This, however, can result in relatively low system utilization and throughput for important classes of scientific applications.In this paper we consider "a class of scheduling policies that attempt to increase processor utilization and system throughput by timesharing a partition of processors among multiple programs. We compare the system performance under this multiprogramming policy with that of static partitioning for a variety of workloads via both analytic and simulation modeling. Our results show that timesharing a partition can provide significant improvements in performance, particularly at moderate to heavy loads. The performance gains of the multiprogrammed policy depend upon the inherent efficiency of the parallel programs that comprise the workload, decreasing with increasing program efficiency. Our analysis also provides the regions over which one scheduling policy outperforms the other, as a function of system load.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {158--170},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166955.167002},
 doi = {http://doi.acm.org/10.1145/166955.167002},
 acmid = {167002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Setia:1993:PSM:166962.167002,
 author = {Setia, Sanjeev K. and Squillante, Mark S. and Tripathi, Satish K.},
 title = {Processor scheduling on multiprogrammed, distributed memory parallel computers},
 abstract = {Multicomputers, consisting of many processing nodes connected through a high speed interconnection network, have become an important and common platform for a large body of scientific computations. These parallel systems have traditionally executed programs in batch mode, or have at most space-shared the processors among multiple programs using a static partitioning policy. This, however, can result in relatively low system utilization and throughput for important classes of scientific applications.In this paper we consider "a class of scheduling policies that attempt to increase processor utilization and system throughput by timesharing a partition of processors among multiple programs. We compare the system performance under this multiprogramming policy with that of static partitioning for a variety of workloads via both analytic and simulation modeling. Our results show that timesharing a partition can provide significant improvements in performance, particularly at moderate to heavy loads. The performance gains of the multiprogrammed policy depend upon the inherent efficiency of the parallel programs that comprise the workload, decreasing with increasing program efficiency. Our analysis also provides the regions over which one scheduling policy outperforms the other, as a function of system load.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {158--170},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/166962.167002},
 doi = {http://doi.acm.org/10.1145/166962.167002},
 acmid = {167002},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wu:1993:PCT:166962.167003,
 author = {Wu, Kun-Lung and Yu, Philip S. and Teng, James Z.},
 title = {Performance comparison of thrashing control policies for concurrent Mergesorts with parallel prefetching},
 abstract = {We study the performance of various run-time thrashing control policies for the merge phase of concurrent mergesorts using parallel prefetching, where initial sorted runs are stored on multiple disks and the final sorted run is written back to another dedicated disk. Parallel prefetching via multiple disks can be attractive in reducing the response times for concurrent mergesorts. However, severe thrashing</i> may develop due to imbalances between input and output rates, thus a large number of prefetched pages in the buffer can be replaced before referenced. We evaluate through detailed simulations three run-time thrashing control policies: (a) disabling prefetching, (b) forcing synchronous writes and (c) lowering the prefetch quantity in addition to forcing synchronous writes. The results show that (1) thrashing resulted from parallel prefetching can severely degrade the system response time; (2) though effective in reducing the degree of thrashing, disabling prefetching may worsen the response time since more synchronous reads are needed; (3) forcing synchronous writes can both reduce thrashing and improve the response time; (4) lowering the prefetch quantity in addition to forcing synchronous writes is most effective in reducing thrashing and improving the response time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167003},
 doi = {http://doi.acm.org/10.1145/166962.167003},
 acmid = {167003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wu:1993:PCT:166955.167003,
 author = {Wu, Kun-Lung and Yu, Philip S. and Teng, James Z.},
 title = {Performance comparison of thrashing control policies for concurrent Mergesorts with parallel prefetching},
 abstract = {We study the performance of various run-time thrashing control policies for the merge phase of concurrent mergesorts using parallel prefetching, where initial sorted runs are stored on multiple disks and the final sorted run is written back to another dedicated disk. Parallel prefetching via multiple disks can be attractive in reducing the response times for concurrent mergesorts. However, severe thrashing</i> may develop due to imbalances between input and output rates, thus a large number of prefetched pages in the buffer can be replaced before referenced. We evaluate through detailed simulations three run-time thrashing control policies: (a) disabling prefetching, (b) forcing synchronous writes and (c) lowering the prefetch quantity in addition to forcing synchronous writes. The results show that (1) thrashing resulted from parallel prefetching can severely degrade the system response time; (2) though effective in reducing the degree of thrashing, disabling prefetching may worsen the response time since more synchronous reads are needed; (3) forcing synchronous writes can both reduce thrashing and improve the response time; (4) lowering the prefetch quantity in addition to forcing synchronous writes is most effective in reducing thrashing and improving the response time.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {171--182},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167003},
 doi = {http://doi.acm.org/10.1145/166955.167003},
 acmid = {167003},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Meliksetian:1993:MBP:166962.167005,
 author = {Meliksetian, Dikran S. and Chen, C. Y. Roger},
 title = {A Markov-modulated Bernoulli process approximation for the analysis of Banyan Networks},
 abstract = {The Markov-Modulated Bernoulli Process (MMBP) model is used to analyze the delay experienced by messages in clocked, packed-switched Banyan networks with k</i> x k</i> output-buffered switches. This approach allows us to analyze both single packet messages and multipacket messages with general traffic pattern including uniform traffic, hot-spot traffic, locality of reference, etc. The ability to analyze multipacket messages is very important for multimedia applications. Previous work, which is only applicable to restricted message and traffic patterns, resorts to either heuristic correction factors to artificially tune the model or tedious computational efforts. In contrast, the proposed model, which is applicable to much more general message and traffic patterns, not only is an application of a theoretically complete model but also requires a minimal amount of computational effort. In all cases, the analytical results are compared with results obtained by simulation and are shown to be very accurate.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167005},
 doi = {http://doi.acm.org/10.1145/166962.167005},
 acmid = {167005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Meliksetian:1993:MBP:166955.167005,
 author = {Meliksetian, Dikran S. and Chen, C. Y. Roger},
 title = {A Markov-modulated Bernoulli process approximation for the analysis of Banyan Networks},
 abstract = {The Markov-Modulated Bernoulli Process (MMBP) model is used to analyze the delay experienced by messages in clocked, packed-switched Banyan networks with k</i> x k</i> output-buffered switches. This approach allows us to analyze both single packet messages and multipacket messages with general traffic pattern including uniform traffic, hot-spot traffic, locality of reference, etc. The ability to analyze multipacket messages is very important for multimedia applications. Previous work, which is only applicable to restricted message and traffic patterns, resorts to either heuristic correction factors to artificially tune the model or tedious computational efforts. In contrast, the proposed model, which is applicable to much more general message and traffic patterns, not only is an application of a theoretically complete model but also requires a minimal amount of computational effort. In all cases, the analytical results are compared with results obtained by simulation and are shown to be very accurate.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167005},
 doi = {http://doi.acm.org/10.1145/166955.167005},
 acmid = {167005},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Arakawa:1993:MVR:166955.167008,
 author = {Arakawa, Hiroshi and Katcher, Daniel I. and Strosnider, Jay K. and Tokuda, Hideyuki},
 title = {Modeling and validation of the real-time Mach scheduler},
 abstract = {Real-time scheduling theory is designed to provide a priori</i> verification that all real-time tasks meet their timing requirements. However, this body of theory generally assumes that resources are instantaneously pre-emptable and ignores the costs of systems services. In previous work [1, 2] we provided a theoretical foundation for including the costs of the operating system scheduler in the real-time scheduling framework. In this paper, we apply that theory to the Real-Time (RT) Mach scheduler. We describe a methodology for measuring the components of the RT Mach scheduler in user space. We analyze the predicted performance of different real-time task sets on the target system using the scheduling model and the measured characteristics. We then verify the model experimentally by measuring the performance of the real-time task sets, consisting of RT Mach threads, on the target system, The experimental measurements verify the analytical model to within a small percentage of error. Thus, using the model we have successfully predicted the performance of real-time task sets using system services, and developed consistent methodologies to accomplish that prediction.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167008},
 doi = {http://doi.acm.org/10.1145/166955.167008},
 acmid = {167008},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Arakawa:1993:MVR:166962.167008,
 author = {Arakawa, Hiroshi and Katcher, Daniel I. and Strosnider, Jay K. and Tokuda, Hideyuki},
 title = {Modeling and validation of the real-time Mach scheduler},
 abstract = {Real-time scheduling theory is designed to provide a priori</i> verification that all real-time tasks meet their timing requirements. However, this body of theory generally assumes that resources are instantaneously pre-emptable and ignores the costs of systems services. In previous work [1, 2] we provided a theoretical foundation for including the costs of the operating system scheduler in the real-time scheduling framework. In this paper, we apply that theory to the Real-Time (RT) Mach scheduler. We describe a methodology for measuring the components of the RT Mach scheduler in user space. We analyze the predicted performance of different real-time task sets on the target system using the scheduling model and the measured characteristics. We then verify the model experimentally by measuring the performance of the real-time task sets, consisting of RT Mach threads, on the target system, The experimental measurements verify the analytical model to within a small percentage of error. Thus, using the model we have successfully predicted the performance of real-time task sets using system services, and developed consistent methodologies to accomplish that prediction.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {195--206},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167008},
 doi = {http://doi.acm.org/10.1145/166962.167008},
 acmid = {167008},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Baruah:1993:RHS:166955.167010,
 author = {Baruah, Sanjoy and Haritsa, Jayant R.},
 title = {ROBUST: a hardware solution to real-time overload},
 abstract = {No on-line scheduling algorithm operating in a uniprocessor environment can guarantee to obtain an effective processor utilization greater than 25\% under conditions of overload. This result holds in the most generad case, where incoming tasks may have arbitrary slack times. We address here the issue of improving overload performance in environments where the slack-time charactersitics of all incoming tasks satisfy certain constraints. In particular, we present a new scheduling algorithm, ROBUST, that efficiently takes advantage of these task slack constraints to provide improved overload performance and is asymptotically optimal.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/166955.167010},
 doi = {http://doi.acm.org/10.1145/166955.167010},
 acmid = {167010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Baruah:1993:RHS:166962.167010,
 author = {Baruah, Sanjoy and Haritsa, Jayant R.},
 title = {ROBUST: a hardware solution to real-time overload},
 abstract = {No on-line scheduling algorithm operating in a uniprocessor environment can guarantee to obtain an effective processor utilization greater than 25\% under conditions of overload. This result holds in the most generad case, where incoming tasks may have arbitrary slack times. We address here the issue of improving overload performance in environments where the slack-time charactersitics of all incoming tasks satisfy certain constraints. In particular, we present a new scheduling algorithm, ROBUST, that efficiently takes advantage of these task slack constraints to provide improved overload performance and is asymptotically optimal.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/166962.167010},
 doi = {http://doi.acm.org/10.1145/166962.167010},
 acmid = {167010},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dey:1993:EOP:166962.167013,
 author = {Dey, Jayanta K. and Kurose, James F. and Towsley, Don and Krishna, C. M. and Girkar, Mahesh},
 title = {Efficient on-line processor scheduling for a class of IRIS (<italic>increasing reward with increasing service</italic>) real-time tasks},
 abstract = {In this paper we consider the problem of on-line scheduling of real-time tasks which receive a "reward" that depends on the amount of service received. In our model, tasks have associated deadlines at which they must depart the system. The task computations are such that the longer they are able to execute before their deadline, the greater the value of their computations, i.e., the tasks have the property that they receive increasing reward with increasing service (IRIS)</i>. We focus on the problem of scheduling IRIS tasks in a system in which tasks arrive randomly over time, with the goal of maximizing the average reward accrued per task and per unit time. We describe and evaluate a two-level policy for this system. A top-level algorithm executes each time a task arrives and determines the amount of service to allocate to each task in the absence of future arrivals. A lower-level algorithm, an earliest deadline first (EDF) policy in our case, is responsible for the actual selection of tasks to execute. This two-level policy is evaluated through a combination of analysis and simulation, We observe that it provides nearly optimal performance when the variance in the interarrival times and/or laxities is low and that the performance is more sensitive to changes in the arrival process than the deadline distribution.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167013},
 doi = {http://doi.acm.org/10.1145/166962.167013},
 acmid = {167013},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dey:1993:EOP:166955.167013,
 author = {Dey, Jayanta K. and Kurose, James F. and Towsley, Don and Krishna, C. M. and Girkar, Mahesh},
 title = {Efficient on-line processor scheduling for a class of IRIS (<italic>increasing reward with increasing service</italic>) real-time tasks},
 abstract = {In this paper we consider the problem of on-line scheduling of real-time tasks which receive a "reward" that depends on the amount of service received. In our model, tasks have associated deadlines at which they must depart the system. The task computations are such that the longer they are able to execute before their deadline, the greater the value of their computations, i.e., the tasks have the property that they receive increasing reward with increasing service (IRIS)</i>. We focus on the problem of scheduling IRIS tasks in a system in which tasks arrive randomly over time, with the goal of maximizing the average reward accrued per task and per unit time. We describe and evaluate a two-level policy for this system. A top-level algorithm executes each time a task arrives and determines the amount of service to allocate to each task in the absence of future arrivals. A lower-level algorithm, an earliest deadline first (EDF) policy in our case, is responsible for the actual selection of tasks to execute. This two-level policy is evaluated through a combination of analysis and simulation, We observe that it provides nearly optimal performance when the variance in the interarrival times and/or laxities is low and that the performance is more sensitive to changes in the arrival process than the deadline distribution.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167013},
 doi = {http://doi.acm.org/10.1145/166955.167013},
 acmid = {167013},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Morris:1993:ASS:166962.167016,
 author = {Morris, Robert J. T.},
 title = {Analysis of superposition of streams into a cache buffer},
 abstract = {We consider the superposition of address streams into a cache buffer which is managed according to a Least Recently Used (LRU) replacement policy. Each of the streams is characterized by a stack depth distribution, i.e., the cache hit ratio as a function of the cache size, if that individual stream were applied to a LRU cache. We seek the cache hit ratio for each stream, when the combined stream is applied to a shared LRU cache. This problem arises in a number of branches of computer science, particularly in database systems and processor architecture.We provide two techniques to solve this problem and demonstrate their effectiveness using database I/O request streams. The first technique is extremely simple and relies on an assumption that the buffer is "well-mixed". The second technique relaxes this assumption and provides more accurate results. We evaluate the performance of the two techniques on realistic data, both in a lab environment and a large database installation. We find that the first simple technique provides accuracy which is sufficient for most practical purposes. By investigating sources of error and trying various improvements in the model we obtain some insight into the nature of database I/O request streams.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {229--235},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/166962.167016},
 doi = {http://doi.acm.org/10.1145/166962.167016},
 acmid = {167016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Morris:1993:ASS:166955.167016,
 author = {Morris, Robert J. T.},
 title = {Analysis of superposition of streams into a cache buffer},
 abstract = {We consider the superposition of address streams into a cache buffer which is managed according to a Least Recently Used (LRU) replacement policy. Each of the streams is characterized by a stack depth distribution, i.e., the cache hit ratio as a function of the cache size, if that individual stream were applied to a LRU cache. We seek the cache hit ratio for each stream, when the combined stream is applied to a shared LRU cache. This problem arises in a number of branches of computer science, particularly in database systems and processor architecture.We provide two techniques to solve this problem and demonstrate their effectiveness using database I/O request streams. The first technique is extremely simple and relies on an assumption that the buffer is "well-mixed". The second technique relaxes this assumption and provides more accurate results. We evaluate the performance of the two techniques on realistic data, both in a lab environment and a large database installation. We find that the first simple technique provides accuracy which is sufficient for most practical purposes. By investigating sources of error and trying various improvements in the model we obtain some insight into the nature of database I/O request streams.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {229--235},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/166955.167016},
 doi = {http://doi.acm.org/10.1145/166955.167016},
 acmid = {167016},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tsai:1993:AMC:166962.167021,
 author = {Tsai, Jory and Agarwal, Anant},
 title = {Analyzing multiprocessor cache behavior through data reference modeling},
 abstract = {This paper develops a data reference modeling</i> technique to estimate with high accuracy the cache miss ratio in cache-coherent multiproeessors. The technique involves analyzing the dynamic data referencing behavior of parallel algorithms. Data referenee modeling first identifies different types of shared data blocks accessed during the execution of a parallel algorithm, then captures in a few parameters the cache behavior of each shared block as a function of the problem size, number of processors, and cache line size, and finally constructs an analytical expression for each algorithm to estimate the cache miss ratio. Because the number of processors, problem size, and cache line size are included as parameters, the expression for the each miss ratio can be used to predict the performance of systems with different configurations. Six parallel algorithms are studied, and the analytical results compared against previously published simulation results, to establish the confidence level of the data reference modeling technique. It is found that the average prediction error for four out of six algorithms is within five percent and within ten percent for the other two. The paper also derives from the model several results on how cache miss rates scale with system size.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {236--247},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167021},
 doi = {http://doi.acm.org/10.1145/166962.167021},
 acmid = {167021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tsai:1993:AMC:166955.167021,
 author = {Tsai, Jory and Agarwal, Anant},
 title = {Analyzing multiprocessor cache behavior through data reference modeling},
 abstract = {This paper develops a data reference modeling</i> technique to estimate with high accuracy the cache miss ratio in cache-coherent multiproeessors. The technique involves analyzing the dynamic data referencing behavior of parallel algorithms. Data referenee modeling first identifies different types of shared data blocks accessed during the execution of a parallel algorithm, then captures in a few parameters the cache behavior of each shared block as a function of the problem size, number of processors, and cache line size, and finally constructs an analytical expression for each algorithm to estimate the cache miss ratio. Because the number of processors, problem size, and cache line size are included as parameters, the expression for the each miss ratio can be used to predict the performance of systems with different configurations. Six parallel algorithms are studied, and the analytical results compared against previously published simulation results, to establish the confidence level of the data reference modeling technique. It is found that the average prediction error for four out of six algorithms is within five percent and within ten percent for the other two. The paper also derives from the model several results on how cache miss rates scale with system size.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {236--247},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167021},
 doi = {http://doi.acm.org/10.1145/166955.167021},
 acmid = {167021},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Martonosi:1993:ETS:166955.167023,
 author = {Martonosi, Margaret and Gupta, Anoop and Anderson, Thomas},
 title = {Effectiveness of trace sampling for performance debugging tools},
 abstract = {Recently there has been a surge of interest in developing performance debugging tools to help programmers tune their applications for better memory performance [2, 4, 10]. These tools vary both in the detail of feedback provided to the user, and in the run-time overbead of using them. MemSpy [10] is a simulation-based tool which gives programmers detailed statistics on the memory system behavior of applications. It provides information on the frequency and causes of cache misses, and presents it in terms of source-level data and code objects with which the programmer is familiar. However, using MemSpy increases a program's execution time by roughly 10 to 40 fold. This overhead is generally acceptable for applications with execution times of several minutes or less, but it can be inconvenient when tuning applications with very long execution times.This paper examines the use of trace sampling techniques to reduce the execution time overhead of tools like MemSpy. When simulating one tenth of the references, we find that MemSpy's execution time overhead is improved by a factor of 4 to 6. That is, the execution time when using MemSpy is generally within a factor of 3 to 8 times the normal exwution time. With this improved performance, we observe only small errors in the performance statistics reported by MemSpy. On moderate sized caches of 16KB to 128KB, simulating as few as one tenth of the references (in samples of 0.5M references each) allows us to estimate the program's actual cache miss rate with an absolute error no greater than 0.3\% on our five benchmarks. These errors are quite tolerable within the context of performance bugging. With larger caches we can also obtain good accuracy by using longer sample lengths. We conclude that, used with care, trace sampling is a powerful technique that makes possible performance debugging tools which provide both</i> detailed memory statistics and</i> low execution time overheads.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166955.167023},
 doi = {http://doi.acm.org/10.1145/166955.167023},
 acmid = {167023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Martonosi:1993:ETS:166962.167023,
 author = {Martonosi, Margaret and Gupta, Anoop and Anderson, Thomas},
 title = {Effectiveness of trace sampling for performance debugging tools},
 abstract = {Recently there has been a surge of interest in developing performance debugging tools to help programmers tune their applications for better memory performance [2, 4, 10]. These tools vary both in the detail of feedback provided to the user, and in the run-time overbead of using them. MemSpy [10] is a simulation-based tool which gives programmers detailed statistics on the memory system behavior of applications. It provides information on the frequency and causes of cache misses, and presents it in terms of source-level data and code objects with which the programmer is familiar. However, using MemSpy increases a program's execution time by roughly 10 to 40 fold. This overhead is generally acceptable for applications with execution times of several minutes or less, but it can be inconvenient when tuning applications with very long execution times.This paper examines the use of trace sampling techniques to reduce the execution time overhead of tools like MemSpy. When simulating one tenth of the references, we find that MemSpy's execution time overhead is improved by a factor of 4 to 6. That is, the execution time when using MemSpy is generally within a factor of 3 to 8 times the normal exwution time. With this improved performance, we observe only small errors in the performance statistics reported by MemSpy. On moderate sized caches of 16KB to 128KB, simulating as few as one tenth of the references (in samples of 0.5M references each) allows us to estimate the program's actual cache miss rate with an absolute error no greater than 0.3\% on our five benchmarks. These errors are quite tolerable within the context of performance bugging. With larger caches we can also obtain good accuracy by using longer sample lengths. We conclude that, used with care, trace sampling is a powerful technique that makes possible performance debugging tools which provide both</i> detailed memory statistics and</i> low execution time overheads.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {248--259},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/166962.167023},
 doi = {http://doi.acm.org/10.1145/166962.167023},
 acmid = {167023},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ahn:1993:HTS:166955.167026,
 author = {Ahn, Jong-Suk and Danzig, Peter B. and Estrin, Deborah and Timmerman, Brenda},
 title = {Hybrid technique for simulating high bandwidth delay computer networks},
 abstract = {Researchers evaluate and contrast new network routing, admission control, congestion control and flow control algorithms through simulation. Analytically de-rived arguments justifiably lack credibility because, in the attempt to model the underlying physical system, the analyst is forced to make compromising approximations. However, unlike analytical techniques like Jackson Queueing Networks, simulations require significant computation and a simulation's state can consume a great deal of memory.This paper describes a technique that we developed to reduce the memory consumption of communication network simulators. Reduced memory makes simulations of larger and higher bandwidth-delay networks possible, but introduces an adjustable degree of approximation in the simulation. The higher the memory savings, the less accurate the computed measures. We call our technique Flowsim</i>. The paper motivates the need to simulate computer networks rather than model them analytically, motivates why a simulator's state can grow quickly, and explains why analytical techniques have failed to model modern communication networks.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {260--261},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167026},
 doi = {http://doi.acm.org/10.1145/166955.167026},
 acmid = {167026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ahn:1993:HTS:166962.167026,
 author = {Ahn, Jong-Suk and Danzig, Peter B. and Estrin, Deborah and Timmerman, Brenda},
 title = {Hybrid technique for simulating high bandwidth delay computer networks},
 abstract = {Researchers evaluate and contrast new network routing, admission control, congestion control and flow control algorithms through simulation. Analytically de-rived arguments justifiably lack credibility because, in the attempt to model the underlying physical system, the analyst is forced to make compromising approximations. However, unlike analytical techniques like Jackson Queueing Networks, simulations require significant computation and a simulation's state can consume a great deal of memory.This paper describes a technique that we developed to reduce the memory consumption of communication network simulators. Reduced memory makes simulations of larger and higher bandwidth-delay networks possible, but introduces an adjustable degree of approximation in the simulation. The higher the memory savings, the less accurate the computed measures. We call our technique Flowsim</i>. The paper motivates the need to simulate computer networks rather than model them analytically, motivates why a simulator's state can grow quickly, and explains why analytical techniques have failed to model modern communication networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {260--261},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167026},
 doi = {http://doi.acm.org/10.1145/166962.167026},
 acmid = {167026},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Becker:1993:AIC:166955.167028,
 author = {Becker, Jeffrey C. and Park, Arvin},
 title = {An analysis of the information content of address and data reference streams},
 abstract = {Recent increases in VLSI processor speed and transistor density have not been matched by a proportionate increase in the number of I/O pins used to communicate information on and off chip. Since the number of I/O pins is limited by packaging technology and switching constraints, this trend is likely to continue, and I/O bandwidth will become the primary VLSI system performance bottleneck. This paper analyzes the potential of address and data stream coding in order to reduce bandwidth requirements},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {262--263},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167028},
 doi = {http://doi.acm.org/10.1145/166955.167028},
 acmid = {167028},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Becker:1993:AIC:166962.167028,
 author = {Becker, Jeffrey C. and Park, Arvin},
 title = {An analysis of the information content of address and data reference streams},
 abstract = {Recent increases in VLSI processor speed and transistor density have not been matched by a proportionate increase in the number of I/O pins used to communicate information on and off chip. Since the number of I/O pins is limited by packaging technology and switching constraints, this trend is likely to continue, and I/O bandwidth will become the primary VLSI system performance bottleneck. This paper analyzes the potential of address and data stream coding in order to reduce bandwidth requirements},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {262--263},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167028},
 doi = {http://doi.acm.org/10.1145/166962.167028},
 acmid = {167028},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ghandeharizadeh:1993:EAV:166955.167030,
 author = {Ghandeharizadeh, Shahram and Ramos, Luis},
 title = {An evaluation of alternative virtual replication strategies for continuous retrieval of multimedia data},
 abstract = {During the past decade, information technology has evolved to store and retrieve multimedia data (e.g., audio, video). Multimedia information systems utilize a variety of human senses to provide an effective means of conveying information. Already, these systems play a major role in educational applications, entertainment technology, and library information systems. A challenging task when implementing these systems is to support a continuous retrieval of an object at the bandwidth required by its media type. This is challenging because certain media types, in particular video, require very high bandwidths. For example, the bandwidth required by NTSC (the US standard established by the National Television System Committee) for "network-quality" video is about 45 megabits per second (mbps). Recommendation 601 of the International Radio Consultative Committee (CCIR) calls for a 216 mbps bandwidth for video objects. A video object based on the HDTV (High Definition Television) quality images requires approximately a 700 mbps bandwidth. Compare these bandwidth requirements with the typical 10 mbps bandwidth of a magnetic disk drive, which is not expected to increase significantly in the near future. Currently, there are several ways to support continuous display of these objects: 1) sacrifice the quality of the data by using either a lossy compression technique or a low resolution device, 2) employ the aggregate bandwidth of several disk drives by declustering an object across multiple disks [2], and 3) use a combination of these two techniques. Lossy compression techniques encode data into a form that consumes a relatively small amount of space, however, when the data is decoded, it yields a representation similar to the original (some loss of data). While it is effective, there are applications that cannot tolerate loss of data. As an example consider the video signals collected from space. This data may not be compressed using a lossy compression technique. Otherwise, the scientists who later uncompress and analyze the data run the risk of either observing phenomena that may not exist due to a slight change in data or miss important observations due to some loss of data.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167030},
 doi = {http://doi.acm.org/10.1145/166955.167030},
 acmid = {167030},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghandeharizadeh:1993:EAV:166962.167030,
 author = {Ghandeharizadeh, Shahram and Ramos, Luis},
 title = {An evaluation of alternative virtual replication strategies for continuous retrieval of multimedia data},
 abstract = {During the past decade, information technology has evolved to store and retrieve multimedia data (e.g., audio, video). Multimedia information systems utilize a variety of human senses to provide an effective means of conveying information. Already, these systems play a major role in educational applications, entertainment technology, and library information systems. A challenging task when implementing these systems is to support a continuous retrieval of an object at the bandwidth required by its media type. This is challenging because certain media types, in particular video, require very high bandwidths. For example, the bandwidth required by NTSC (the US standard established by the National Television System Committee) for "network-quality" video is about 45 megabits per second (mbps). Recommendation 601 of the International Radio Consultative Committee (CCIR) calls for a 216 mbps bandwidth for video objects. A video object based on the HDTV (High Definition Television) quality images requires approximately a 700 mbps bandwidth. Compare these bandwidth requirements with the typical 10 mbps bandwidth of a magnetic disk drive, which is not expected to increase significantly in the near future. Currently, there are several ways to support continuous display of these objects: 1) sacrifice the quality of the data by using either a lossy compression technique or a low resolution device, 2) employ the aggregate bandwidth of several disk drives by declustering an object across multiple disks [2], and 3) use a combination of these two techniques. Lossy compression techniques encode data into a form that consumes a relatively small amount of space, however, when the data is decoded, it yields a representation similar to the original (some loss of data). While it is effective, there are applications that cannot tolerate loss of data. As an example consider the video signals collected from space. This data may not be compressed using a lossy compression technique. Otherwise, the scientists who later uncompress and analyze the data run the risk of either observing phenomena that may not exist due to a slight change in data or miss important observations due to some loss of data.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167030},
 doi = {http://doi.acm.org/10.1145/166962.167030},
 acmid = {167030},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kay:1993:STN:166962.167033,
 author = {Kay, Jonathan and Pasquale, Joseph},
 title = {A summary of TCP/IP networking software performance for the DECstation 5000},
 abstract = {Network software speed is not increasing as rapidly as that of work-station CPUs. The goal of this study is to determine how various components of network software contribute to this bottleneck. In this extended abstract, we summarize the performance of TCPIP and UDPIP networking software for the DECstation 5000/200 workstations connected by an FDDI LAN.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167033},
 doi = {http://doi.acm.org/10.1145/166962.167033},
 acmid = {167033},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kay:1993:STN:166955.167033,
 author = {Kay, Jonathan and Pasquale, Joseph},
 title = {A summary of TCP/IP networking software performance for the DECstation 5000},
 abstract = {Network software speed is not increasing as rapidly as that of work-station CPUs. The goal of this study is to determine how various components of network software contribute to this bottleneck. In this extended abstract, we summarize the performance of TCPIP and UDPIP networking software for the DECstation 5000/200 workstations connected by an FDDI LAN.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {266--267},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167033},
 doi = {http://doi.acm.org/10.1145/166955.167033},
 acmid = {167033},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lewandowski:1993:AAP:166962.167035,
 author = {Lewandowski, Gary and Condon, Anne and Bach, Eric},
 title = {Asynchronous analysis of parallel dynamic programming},
 abstract = {We examine a very simple asynchronous model of parallel computation that assumes the time to compute a task is random, following some probability distribution. The goal of this model is to capture the effects of unexpected delays on processors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167035},
 doi = {http://doi.acm.org/10.1145/166962.167035},
 acmid = {167035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lewandowski:1993:AAP:166955.167035,
 author = {Lewandowski, Gary and Condon, Anne and Bach, Eric},
 title = {Asynchronous analysis of parallel dynamic programming},
 abstract = {We examine a very simple asynchronous model of parallel computation that assumes the time to compute a task is random, following some probability distribution. The goal of this model is to capture the effects of unexpected delays on processors.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {268--269},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167035},
 doi = {http://doi.acm.org/10.1145/166955.167035},
 acmid = {167035},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shin:1993:ELS:166955.167037,
 author = {Shin, Kang G. and Hou, Chao-Ju},
 title = {Evaluation of load sharing in HARTS while considering message routing and broadcasting},
 abstract = {In this paper, we apply the load sharing (LS) mechanism proposed in [1, 2] to HARTS, an experimental distributed realtime system [3] currently being built at the Real-Time Computing Laboratory of the University of Michigan.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167037},
 doi = {http://doi.acm.org/10.1145/166955.167037},
 acmid = {167037},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shin:1993:ELS:166962.167037,
 author = {Shin, Kang G. and Hou, Chao-Ju},
 title = {Evaluation of load sharing in HARTS while considering message routing and broadcasting},
 abstract = {In this paper, we apply the load sharing (LS) mechanism proposed in [1, 2] to HARTS, an experimental distributed realtime system [3] currently being built at the Real-Time Computing Laboratory of the University of Michigan.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {270--271},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167037},
 doi = {http://doi.acm.org/10.1145/166962.167037},
 acmid = {167037},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Torrellas:1993:BCS:166962.167038,
 author = {Torrellas, Josep and Tucker, Andrew and Gupta, Anoop},
 title = {Benefits of cache-affinity scheduling in shared-memory multiprocessors: a summary},
 abstract = {An interesting and common class of workloads for shared-memory multiprocessors is multiprogrammed workloads. Because these workloads generally contain more processes than there are processors in the machine, there are two factors that increase the number of cache misses. First, several processes are forced to time-share the same cache, resulting in one process displacing the cache state previously built up by a second one. Consequently, when the second process runs again, it generates a stream of misses as it rebuilds ita cache state. Second since an idle processor simply selects the highest priority runnable process, a given process often moves from one CPU to another. This frequent migration results in the process having to continuously reload its state into new caches, producing streams of cache misses. To reduce the number of misses in these workloads, processes should reuse their cached state more. One way to encourage this is to schedule each process based on its affinity to individual caches, that is, based on the amount of state that the process has accumulated in an individual cache. This technique is called cache affinity scheduling</i>.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {272--274},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/166962.167038},
 doi = {http://doi.acm.org/10.1145/166962.167038},
 acmid = {167038},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Torrellas:1993:BCS:166955.167038,
 author = {Torrellas, Josep and Tucker, Andrew and Gupta, Anoop},
 title = {Benefits of cache-affinity scheduling in shared-memory multiprocessors: a summary},
 abstract = {An interesting and common class of workloads for shared-memory multiprocessors is multiprogrammed workloads. Because these workloads generally contain more processes than there are processors in the machine, there are two factors that increase the number of cache misses. First, several processes are forced to time-share the same cache, resulting in one process displacing the cache state previously built up by a second one. Consequently, when the second process runs again, it generates a stream of misses as it rebuilds ita cache state. Second since an idle processor simply selects the highest priority runnable process, a given process often moves from one CPU to another. This frequent migration results in the process having to continuously reload its state into new caches, producing streams of cache misses. To reduce the number of misses in these workloads, processes should reuse their cached state more. One way to encourage this is to schedule each process based on its affinity to individual caches, that is, based on the amount of state that the process has accumulated in an individual cache. This technique is called cache affinity scheduling</i>.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {272--274},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/166955.167038},
 doi = {http://doi.acm.org/10.1145/166955.167038},
 acmid = {167038},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vetland:1993:CMA:166962.167040,
 author = {Vetland, Vidar and Hughes, Peter and S{\o}lvberg, Arne},
 title = {A composite modelling approach to software performance measurement},
 abstract = {Traditionally performance modellers have tended to ignore the difficulty of obtaining parameter vaules which represent the resource demands of multi-layered software. In practice the use of performance engineering in large-scale systems development is limited by the cost of acquiring appropriate performance information about the various software components. However, if this information cart be reused when components are combined in different ways, then the cost of measurement can be more easily justified. Such reuse can be achieved by means of a composite work model.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {275--276},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167040},
 doi = {http://doi.acm.org/10.1145/166962.167040},
 acmid = {167040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vetland:1993:CMA:166955.167040,
 author = {Vetland, Vidar and Hughes, Peter and S{\o}lvberg, Arne},
 title = {A composite modelling approach to software performance measurement},
 abstract = {Traditionally performance modellers have tended to ignore the difficulty of obtaining parameter vaules which represent the resource demands of multi-layered software. In practice the use of performance engineering in large-scale systems development is limited by the cost of acquiring appropriate performance information about the various software components. However, if this information cart be reused when components are combined in different ways, then the cost of measurement can be more easily justified. Such reuse can be achieved by means of a composite work model.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {275--276},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167040},
 doi = {http://doi.acm.org/10.1145/166955.167040},
 acmid = {167040},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wagner:1993:AMV:166955.167042,
 author = {Wagner, David B.},
 title = {Approximate mean value analysis of interconnection networks with deflection routing},
 abstract = {This paper presents an Approximate Mean Value Analysis model of deflection routing in Shuffle-Loop interconnection networks. (The methodology is readily extended to other network topologies.) In contrast to most previous work on deflection routing, the model makes no assumptions about traffic patterns, nor does it assume that messages that cannot be admitted to the network are lost. The technique allows the network to be modeled in its entirety: all processors, switches, and memory modules, and their steady-state interactions, are modeled explicitly. The results of the model are found to be in close agreement with the results of simulation experiments.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {277--278},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167042},
 doi = {http://doi.acm.org/10.1145/166955.167042},
 acmid = {167042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wagner:1993:AMV:166962.167042,
 author = {Wagner, David B.},
 title = {Approximate mean value analysis of interconnection networks with deflection routing},
 abstract = {This paper presents an Approximate Mean Value Analysis model of deflection routing in Shuffle-Loop interconnection networks. (The methodology is readily extended to other network topologies.) In contrast to most previous work on deflection routing, the model makes no assumptions about traffic patterns, nor does it assume that messages that cannot be admitted to the network are lost. The technique allows the network to be modeled in its entirety: all processors, switches, and memory modules, and their steady-state interactions, are modeled explicitly. The results of the model are found to be in close agreement with the results of simulation experiments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {277--278},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167042},
 doi = {http://doi.acm.org/10.1145/166962.167042},
 acmid = {167042},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Williamson:1993:OFT:166955.167043,
 author = {Williamson, Carey L.},
 title = {Optimizing file transfer response time using the loss-load curve congestion control mechanism},
 abstract = {Loss-load curves are a recently proposed feedback mechanism for rate-based congestion control in datagram computer networks. In the loss-load model, packet loss inside the network is a direct function of sender transmission rates, and senders choose their own transmission rate based on the loss-load tradeoff curve provided by the network. Earlier work [1] has provided the mathematical basis for the loss-load model and provided preliminary simulation results demonstrating its responsiveness, fairness, and stability. The loss-load model works Well for simple network environments where each source has a large number of packets to transmit, and wishes to maximize raw throughput.},
 booktitle = {Proceedings of the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '93},
 year = {1993},
 isbn = {0-89791-580-1},
 location = {Santa Clara, California, United States},
 pages = {279--280},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166955.167043},
 doi = {http://doi.acm.org/10.1145/166955.167043},
 acmid = {167043},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Williamson:1993:OFT:166962.167043,
 author = {Williamson, Carey L.},
 title = {Optimizing file transfer response time using the loss-load curve congestion control mechanism},
 abstract = {Loss-load curves are a recently proposed feedback mechanism for rate-based congestion control in datagram computer networks. In the loss-load model, packet loss inside the network is a direct function of sender transmission rates, and senders choose their own transmission rate based on the loss-load tradeoff curve provided by the network. Earlier work [1] has provided the mathematical basis for the loss-load model and provided preliminary simulation results demonstrating its responsiveness, fairness, and stability. The loss-load model works Well for simple network environments where each source has a large number of packets to transmit, and wishes to maximize raw throughput.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {21},
 issue = {1},
 month = {June},
 year = {1993},
 issn = {0163-5999},
 pages = {279--280},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/166962.167043},
 doi = {http://doi.acm.org/10.1145/166962.167043},
 acmid = {167043},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Greenberg:1990:UPS:98460.98492,
 author = {Greenberg, Albert G. and Lubachevsky, Boris D. and Mitrani, Isi},
 title = {Unboundedly parallel simulations via recurrence relations},
 abstract = {New methods are presented for parallel simulation of discrete event systems that, when applicable, can usefully employ a number of processors much larger than the number of objects in the system being simulated. Abandoning the distributed event list approach, the simulation problem is posed using recurrence relations. We bring three algorithmic ideas to bear on parallel simulation: parallel prefix computation, parallel merging, and iterative folding. Efficient parallel simulations are given for (in turn) the G/G/1 queue, a variety of queueing networks having a global first come first served structure (e.g., a series of queues with finite buffers), acyclic networks of queues, and networks of queues with feedbacks and cycles. In particular, the problem of simulating the arrival and departure times for the first N jobs to a single G/G/1 queue is solved in time proportional to N/P + log P using P processors.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/98460.98492},
 doi = {http://doi.acm.org/10.1145/98460.98492},
 acmid = {98492},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Greenberg:1990:UPS:98457.98492,
 author = {Greenberg, Albert G. and Lubachevsky, Boris D. and Mitrani, Isi},
 title = {Unboundedly parallel simulations via recurrence relations},
 abstract = {New methods are presented for parallel simulation of discrete event systems that, when applicable, can usefully employ a number of processors much larger than the number of objects in the system being simulated. Abandoning the distributed event list approach, the simulation problem is posed using recurrence relations. We bring three algorithmic ideas to bear on parallel simulation: parallel prefix computation, parallel merging, and iterative folding. Efficient parallel simulations are given for (in turn) the G/G/1 queue, a variety of queueing networks having a global first come first served structure (e.g., a series of queues with finite buffers), acyclic networks of queues, and networks of queues with feedbacks and cycles. In particular, the problem of simulating the arrival and departure times for the first N jobs to a single G/G/1 queue is solved in time proportional to N/P + log P using P processors.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/98457.98492},
 doi = {http://doi.acm.org/10.1145/98457.98492},
 acmid = {98492},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nelson:1990:PEG:98457.98495,
 author = {Nelson, Randolph},
 title = {A performance evaluation of a general parallel processing model},
 abstract = {In this paper we analyze a model of a parallel processing system. In our model there is a single queue which is K \&ge; 1 identical processors. Jobs are assumed to consist of a sequence of barrier synchronizations where, at each step, the number of tasks that must be synchronized is random with a known distribution. An exact analysis of the model is derived. The model leads to a rich set of results characterizing the performance of parallel processing systems. We show that the number of jobs concurrently in execution, as well as the number of synchronization variables, grows linearly with the load of the system and strongly depends on the average number of parallel tasks found in the workload. Properties of expected response time or such systems are extensively analyzed and, in particular, we report on some non-obvious response time behavior that arises as a function of the variance of parallelism found in the workload. Based on exact response time analysis, we propose a simple calculation that can be used as a rule of thumb to predict speedups. This can be viewed as a generalization of Amdahl's law that includes queueing effects. This generalization is reformulated when precise workloads cannot be characterized, but rather when only the fraction or sequential work and the average number of parallel tasks arc assumed to be known.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {13--26},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/98457.98495},
 doi = {http://doi.acm.org/10.1145/98457.98495},
 acmid = {98495},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nelson:1990:PEG:98460.98495,
 author = {Nelson, Randolph},
 title = {A performance evaluation of a general parallel processing model},
 abstract = {In this paper we analyze a model of a parallel processing system. In our model there is a single queue which is K \&ge; 1 identical processors. Jobs are assumed to consist of a sequence of barrier synchronizations where, at each step, the number of tasks that must be synchronized is random with a known distribution. An exact analysis of the model is derived. The model leads to a rich set of results characterizing the performance of parallel processing systems. We show that the number of jobs concurrently in execution, as well as the number of synchronization variables, grows linearly with the load of the system and strongly depends on the average number of parallel tasks found in the workload. Properties of expected response time or such systems are extensively analyzed and, in particular, we report on some non-obvious response time behavior that arises as a function of the variance of parallelism found in the workload. Based on exact response time analysis, we propose a simple calculation that can be used as a rule of thumb to predict speedups. This can be viewed as a generalization of Amdahl's law that includes queueing effects. This generalization is reformulated when precise workloads cannot be characterized, but rather when only the fraction or sequential work and the average number of parallel tasks arc assumed to be known.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {13--26},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/98460.98495},
 doi = {http://doi.acm.org/10.1145/98460.98495},
 acmid = {98495},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:1990:ETS:98457.98497,
 author = {Wang, Wen-Hann and Baer, Jean-Loup},
 title = {Efficient trace-driven simulation method for cache performance analysis},
 abstract = {We propose improvements to current trace-driven cache simulation methods to make them faster and more economical. We attack the large time and space demands of cache simulation in two ways. First, we reduce the program traces to the extent that exact performance can still be obtained from the reduced traces. Second, we devise an algorithm that can produce performance results for a variety of metrics (hit ratio, write-back counts, bus traffic) for a large number of set-associative write-back caches in just a single simulation run. The trace reduction and the efficient simulation techniques are extended to parallel multiprocessor cache simulations. Our simulation results show that our approach substantially reduces the disk space needed to store the program traces and can dramatically speedup cache simulations and still produce the exact results.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {27--36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98497},
 doi = {http://doi.acm.org/10.1145/98457.98497},
 acmid = {98497},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:1990:ETS:98460.98497,
 author = {Wang, Wen-Hann and Baer, Jean-Loup},
 title = {Efficient trace-driven simulation method for cache performance analysis},
 abstract = {We propose improvements to current trace-driven cache simulation methods to make them faster and more economical. We attack the large time and space demands of cache simulation in two ways. First, we reduce the program traces to the extent that exact performance can still be obtained from the reduced traces. Second, we devise an algorithm that can produce performance results for a variety of metrics (hit ratio, write-back counts, bus traffic) for a large number of set-associative write-back caches in just a single simulation run. The trace reduction and the efficient simulation techniques are extended to parallel multiprocessor cache simulations. Our simulation results show that our approach substantially reduces the disk space needed to store the program traces and can dramatically speedup cache simulations and still produce the exact results.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {27--36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98497},
 doi = {http://doi.acm.org/10.1145/98460.98497},
 acmid = {98497},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eggers:1990:TEI:98460.98501,
 author = {Eggers, S. J. and Keppel, David R. and Koldinger, Eric J. and Levy, Henry M.},
 title = {Techniques for efficient inline tracing on a shared-memory multiprocessor},
 abstract = {While much current research concerns multiprocessor design, few traces of parallel programs are available for analyzing the effect of design trade-offs. Existing trace collection methods have serious drawbacks: trap-driven methods often slow down program execution by more than 1000 times, significantly perturbing program behavior; microcode modification is faster, but the technique is neither general nor portable.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98460.98501},
 doi = {http://doi.acm.org/10.1145/98460.98501},
 acmid = {98501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eggers:1990:TEI:98457.98501,
 author = {Eggers, S. J. and Keppel, David R. and Koldinger, Eric J. and Levy, Henry M.},
 title = {Techniques for efficient inline tracing on a shared-memory multiprocessor},
 abstract = {While much current research concerns multiprocessor design, few traces of parallel programs are available for analyzing the effect of design trade-offs. Existing trace collection methods have serious drawbacks: trap-driven methods often slow down program execution by more than 1000 times, significantly perturbing program behavior; microcode modification is faster, but the technique is neither general nor portable.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98457.98501},
 doi = {http://doi.acm.org/10.1145/98457.98501},
 acmid = {98501},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agarwal:1990:BES:98457.98503,
 author = {Agarwal, Anant and Huffman, Minor},
 title = {Blocking: exploiting spatial locality for trace compaction},
 abstract = {Trace-driven simulation is a popular method of estimating the performance of cache memories, translation lookaside buffers, and paging schemes. Because the cost of trace-driven simulation is directly proportional to trace length, reducing the number of references in the trace significantly impacts simulation time. This paper concentrates on trace driven simulation for cache miss rate analysis. Previous schemes, such as cache filtering, exploited temporal locality for compressing traces and could yield an order of magnitude reduction in trace length. A technique called blocking and a variant called blocking with temporal data are presented that compress traces by exploiting spatial locality. Experimental results show that blocking filtering combined with cache filtering can reduce trace length by nearly two orders of magnitude while introducing about 10\% error in cache miss rate estimates.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {48--57},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98503},
 doi = {http://doi.acm.org/10.1145/98457.98503},
 acmid = {98503},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agarwal:1990:BES:98460.98503,
 author = {Agarwal, Anant and Huffman, Minor},
 title = {Blocking: exploiting spatial locality for trace compaction},
 abstract = {Trace-driven simulation is a popular method of estimating the performance of cache memories, translation lookaside buffers, and paging schemes. Because the cost of trace-driven simulation is directly proportional to trace length, reducing the number of references in the trace significantly impacts simulation time. This paper concentrates on trace driven simulation for cache miss rate analysis. Previous schemes, such as cache filtering, exploited temporal locality for compressing traces and could yield an order of magnitude reduction in trace length. A technique called blocking and a variant called blocking with temporal data are presented that compress traces by exploiting spatial locality. Experimental results show that blocking filtering combined with cache filtering can reduce trace length by nearly two orders of magnitude while introducing about 10\% error in cache miss rate estimates.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {48--57},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98503},
 doi = {http://doi.acm.org/10.1145/98460.98503},
 acmid = {98503},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lin:1990:BAF:98460.98505,
 author = {Lin, Tein-Hsiang and Shin, Kang G.},
 title = {A Bayesian approach to fault classification},
 abstract = {According to their temporal behavior, faults in computer systems are classified into permanent, intermittent, and transient faults. Since it is impossible to identify the type of a fault upon its first detection, the common practice is to retry the failed instruction one or more times and then use other fault recovery methods, such as rollback or restart, if the retry is not successful. To determine an ``optimal" (in some sense) number of retries, we need to know several fault parameters, which can be estimated only after classifying all the faults detected in the past.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {58--66},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98460.98505},
 doi = {http://doi.acm.org/10.1145/98460.98505},
 acmid = {98505},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lin:1990:BAF:98457.98505,
 author = {Lin, Tein-Hsiang and Shin, Kang G.},
 title = {A Bayesian approach to fault classification},
 abstract = {According to their temporal behavior, faults in computer systems are classified into permanent, intermittent, and transient faults. Since it is impossible to identify the type of a fault upon its first detection, the common practice is to retry the failed instruction one or more times and then use other fault recovery methods, such as rollback or restart, if the retry is not successful. To determine an ``optimal" (in some sense) number of retries, we need to know several fault parameters, which can be estimated only after classifying all the faults detected in the past.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {58--66},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98457.98505},
 doi = {http://doi.acm.org/10.1145/98457.98505},
 acmid = {98505},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Moser:1990:PLA:98460.98507,
 author = {Moser, Louise E. and Kapur, Vikas and Melliar-Smith, P. M.},
 title = {Probabilistic language analysis of weighted voting algorithms},
 abstract = {We present a method of analyzing the performance of weighted voting algorithms in a fault-tolerant distributed system. In many distributed systems, some processors send messages more frequently than others and all processors share a common communication medium, such as an Ethernet. Typical fault-tolerant voting algorithms require that a certain minimum number of votes be collected from different processors. System performance is significantly affected by the time required to collect those votes. We formulate the problem of weighted voting in terms of probabilistic languages and then use the calculus of generating functions to compute the expected delay to collect that number of votes. An application of the method to a particular voting algorithm, the Total protocol, is given.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {67--73},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/98460.98507},
 doi = {http://doi.acm.org/10.1145/98460.98507},
 acmid = {98507},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Moser:1990:PLA:98457.98507,
 author = {Moser, Louise E. and Kapur, Vikas and Melliar-Smith, P. M.},
 title = {Probabilistic language analysis of weighted voting algorithms},
 abstract = {We present a method of analyzing the performance of weighted voting algorithms in a fault-tolerant distributed system. In many distributed systems, some processors send messages more frequently than others and all processors share a common communication medium, such as an Ethernet. Typical fault-tolerant voting algorithms require that a certain minimum number of votes be collected from different processors. System performance is significantly affected by the time required to collect those votes. We formulate the problem of weighted voting in terms of probabilistic languages and then use the calculus of generating functions to compute the expected delay to collect that number of votes. An application of the method to a particular voting algorithm, the Total protocol, is given.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {67--73},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/98457.98507},
 doi = {http://doi.acm.org/10.1145/98457.98507},
 acmid = {98507},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chen:1990:ERA:98460.98509,
 author = {Chen, Peter M. and Gibson, Garth A. and Katz, Randy H. and Patterson, David A.},
 title = {An evaluation of redundant arrays of disks using an Amdahl 5890},
 abstract = {Recently we presented several disk array architectures designed to increase the data rate and I/O rate of supercomputing applications, transaction processing, and file systems [Patterson 88]. In this paper we present a hardware performance measurement of two of these architectures, mirroring and rotated parity. We see how throughput for these two architectures is affected by response time requirements, request sizes, and read to write ratios. We find that for applications with large accesses, such as many supercomputing applications, a rotated parity disk array far outperforms traditional mirroring architecture. For applications dominated by small accesses, such as transaction processing, mirroring architectures have higher performance per disk than rotated parity architectures.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/98460.98509},
 doi = {http://doi.acm.org/10.1145/98460.98509},
 acmid = {98509},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chen:1990:ERA:98457.98509,
 author = {Chen, Peter M. and Gibson, Garth A. and Katz, Randy H. and Patterson, David A.},
 title = {An evaluation of redundant arrays of disks using an Amdahl 5890},
 abstract = {Recently we presented several disk array architectures designed to increase the data rate and I/O rate of supercomputing applications, transaction processing, and file systems [Patterson 88]. In this paper we present a hardware performance measurement of two of these architectures, mirroring and rotated parity. We see how throughput for these two architectures is affected by response time requirements, request sizes, and read to write ratios. We find that for applications with large accesses, such as many supercomputing applications, a rotated parity disk array far outperforms traditional mirroring architecture. For applications dominated by small accesses, such as transaction processing, mirroring architectures have higher performance per disk than rotated parity architectures.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {74--85},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/98457.98509},
 doi = {http://doi.acm.org/10.1145/98457.98509},
 acmid = {98509},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mukherjee:1990:SAF:98457.98510,
 author = {Mukherjee, Amarnath and Landweber, Lawrence H. and Strikwerda, John C.},
 title = {Simultaneous analysis of flow and error control strategies with congestion-dependent errors},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98510},
 doi = {http://doi.acm.org/10.1145/98457.98510},
 acmid = {98510},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukherjee:1990:SAF:98460.98510,
 author = {Mukherjee, Amarnath and Landweber, Lawrence H. and Strikwerda, John C.},
 title = {Simultaneous analysis of flow and error control strategies with congestion-dependent errors},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {86--95},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98510},
 doi = {http://doi.acm.org/10.1145/98460.98510},
 acmid = {98510},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lin:1990:QAA:98460.98514,
 author = {Lin, Arthur Y. M. and Silvester, John A.},
 title = {Queueing analysis of an ATM switch with multichannel transmission groups},
 abstract = {The discrete-time D<supscrpt>[A]</supscrpt>/D/c/B queueing system is studied. We consider both a bulk arrival process with constant bulk inter-arrival time (D) and general bulk-size distribution (A) and a periodic arrival process (D<subscrpt>1</subscrpt> + \&middot;\&middot;\&middot; + D<subscrpt>N</subscrpt>). The service/transmission times are deterministic (D) and the system provides for a maximum of c servers with a buffer size B. The motivation for studying this queueing system is its application in performance modeling and analysis of an asynchronous transfer mode (ATM) switch with multichannel transmission groups.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {96--105},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98514},
 doi = {http://doi.acm.org/10.1145/98460.98514},
 acmid = {98514},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lin:1990:QAA:98457.98514,
 author = {Lin, Arthur Y. M. and Silvester, John A.},
 title = {Queueing analysis of an ATM switch with multichannel transmission groups},
 abstract = {The discrete-time D<supscrpt>[A]</supscrpt>/D/c/B queueing system is studied. We consider both a bulk arrival process with constant bulk inter-arrival time (D) and general bulk-size distribution (A) and a periodic arrival process (D<subscrpt>1</subscrpt> + \&middot;\&middot;\&middot; + D<subscrpt>N</subscrpt>). The service/transmission times are deterministic (D) and the system provides for a maximum of c servers with a buffer size B. The motivation for studying this queueing system is its application in performance modeling and analysis of an asynchronous transfer mode (ATM) switch with multichannel transmission groups.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {96--105},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98514},
 doi = {http://doi.acm.org/10.1145/98457.98514},
 acmid = {98514},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Johnson:1990:AAR:98460.98517,
 author = {Johnson, Theodore},
 title = {Approximate analysis of reader and writer access to a shared resource},
 abstract = {In this paper we present a queue that has two classes of customers: readers and writers. Readers access the resource concurrently and writers access the resource serially. The queue discipline is FCFS: readers must wait until all writers that arrived earlier have completed service, and vice versa. The approximation can predict both the expected waiting times for readers and writers and the capacity of the queue. The queue can be used for the analysis of operating system and software resources that can be accessed both serially and concurrently, such as shared files. We have used the queue to analyze the performance of concurrent B-tree algorithms.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {106--114},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98460.98517},
 doi = {http://doi.acm.org/10.1145/98460.98517},
 acmid = {98517},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Johnson:1990:AAR:98457.98517,
 author = {Johnson, Theodore},
 title = {Approximate analysis of reader and writer access to a shared resource},
 abstract = {In this paper we present a queue that has two classes of customers: readers and writers. Readers access the resource concurrently and writers access the resource serially. The queue discipline is FCFS: readers must wait until all writers that arrived earlier have completed service, and vice versa. The approximation can predict both the expected waiting times for readers and writers and the capacity of the queue. The queue can be used for the analysis of operating system and software resources that can be accessed both serially and concurrently, such as shared files. We have used the queue to analyze the performance of concurrent B-tree algorithms.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {106--114},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98457.98517},
 doi = {http://doi.acm.org/10.1145/98457.98517},
 acmid = {98517},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1990:QTT:98460.98518,
 author = {Anderson, Thomas E. and Lazowska, Edward D.},
 title = {Quartz: a tool for tuning parallel program performance},
 abstract = {Initial implementations of parallel programs typically yield disappointing performance. Tuning to improve performance is thus a significant part of the parallel programming process. The effort required to tune a parallel program, and the level of performance that eventually is achieved, both depend heavily on the quality of the instrumentation that is available to the programmer.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {115--125},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98460.98518},
 doi = {http://doi.acm.org/10.1145/98460.98518},
 acmid = {98518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1990:QTT:98457.98518,
 author = {Anderson, Thomas E. and Lazowska, Edward D.},
 title = {Quartz: a tool for tuning parallel program performance},
 abstract = {Initial implementations of parallel programs typically yield disappointing performance. Tuning to improve performance is thus a significant part of the parallel programming process. The effort required to tune a parallel program, and the level of performance that eventually is achieved, both depend heavily on the quality of the instrumentation that is available to the programmer.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {115--125},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98457.98518},
 doi = {http://doi.acm.org/10.1145/98457.98518},
 acmid = {98518},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pattipati:1990:CVA:98457.98522,
 author = {Pattipati, Krishna R. and Wolf, Joel and Deb, Somnath},
 title = {A calculus of variations approach to file allocation problems in computer systems},
 abstract = {This paper is concerned with the parameter optimization in closed product-form queueing networks. Our approach is to combine the techniques of the calculus of variations with the mean value analysis (MVA) recursion of closed queueing networks. We view the MVA recursion as nonlinear difference equations describing a multi-stage system, wherein a stage corresponds to the network population, and the response times at each node constitute the state variables of the multi-stage system. This viewpoint leads to a two-point boundary value problem , in which the forward system corresponds to the MVA recursion and the backward system corresponds to an MVA-like adjoint recursion. The method allows for a very general class of objective functions, and the adjoint equations provide the necessary information to compute the gradient of the cost function. The optimization problem can then be solved by any of the gradient-based methods. For the special case when the objective function is the network delay function, the gradient vector is shown to be related to the moments of the queue lengths. In addition, the adjoint vector offers the potential for the on-line adaptive control of queueing networks based on the state information (e.g., actual degree of multi-programming, response times at the devices.) The theory is illustrated via application to the problem of determining the optimal disk routing probabilities in a large scale, modern I/O (Input/Output) subsystem. A subsequent paper will deal with extensions of the theory to multi-class networks.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {126--133},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/98457.98522},
 doi = {http://doi.acm.org/10.1145/98457.98522},
 acmid = {98522},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pattipati:1990:CVA:98460.98522,
 author = {Pattipati, Krishna R. and Wolf, Joel and Deb, Somnath},
 title = {A calculus of variations approach to file allocation problems in computer systems},
 abstract = {This paper is concerned with the parameter optimization in closed product-form queueing networks. Our approach is to combine the techniques of the calculus of variations with the mean value analysis (MVA) recursion of closed queueing networks. We view the MVA recursion as nonlinear difference equations describing a multi-stage system, wherein a stage corresponds to the network population, and the response times at each node constitute the state variables of the multi-stage system. This viewpoint leads to a two-point boundary value problem , in which the forward system corresponds to the MVA recursion and the backward system corresponds to an MVA-like adjoint recursion. The method allows for a very general class of objective functions, and the adjoint equations provide the necessary information to compute the gradient of the cost function. The optimization problem can then be solved by any of the gradient-based methods. For the special case when the objective function is the network delay function, the gradient vector is shown to be related to the moments of the queue lengths. In addition, the adjoint vector offers the potential for the on-line adaptive control of queueing networks based on the state information (e.g., actual degree of multi-programming, response times at the devices.) The theory is illustrated via application to the problem of determining the optimal disk routing probabilities in a large scale, modern I/O (Input/Output) subsystem. A subsequent paper will deal with extensions of the theory to multi-class networks.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {126--133},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/98460.98522},
 doi = {http://doi.acm.org/10.1145/98460.98522},
 acmid = {98522},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Robinson:1990:DCM:98457.98523,
 author = {Robinson, John T. and Devarakonda, Murthy V.},
 title = {Data cache management using frequency-based replacement},
 abstract = {We propose a new frequency-based replacement algorithm for managing caches used for disk blocks by a file system, database management system, or disk control unit, which we refer to here as data caches. Previously, LRU replacement has usually been used for such caches. We describe a replacement algorithm based on the concept of maintaining reference counts in which locality has been ``factored out". In this algorithm replacement choices are made using a combination of reference frequency and block age. Simulation results based on traces of file system and I/O activity from actual systems show that this algorithm can offer up to 34\% performance improvement over LRU replacement, where the improvement is expressed as the fraction of the performance gain achieved between LRU replacement and the theoretically optimal policy in which the reference string must be known in advance. Furthermore, the implementation complexity and efficiency of this algorithm is comparable to one using LRU replacement.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {134--142},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98457.98523},
 doi = {http://doi.acm.org/10.1145/98457.98523},
 acmid = {98523},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Robinson:1990:DCM:98460.98523,
 author = {Robinson, John T. and Devarakonda, Murthy V.},
 title = {Data cache management using frequency-based replacement},
 abstract = {We propose a new frequency-based replacement algorithm for managing caches used for disk blocks by a file system, database management system, or disk control unit, which we refer to here as data caches. Previously, LRU replacement has usually been used for such caches. We describe a replacement algorithm based on the concept of maintaining reference counts in which locality has been ``factored out". In this algorithm replacement choices are made using a combination of reference frequency and block age. Simulation results based on traces of file system and I/O activity from actual systems show that this algorithm can offer up to 34\% performance improvement over LRU replacement, where the improvement is expressed as the fraction of the performance gain achieved between LRU replacement and the theoretically optimal policy in which the reference string must be known in advance. Furthermore, the implementation complexity and efficiency of this algorithm is comparable to one using LRU replacement.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {134--142},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98460.98523},
 doi = {http://doi.acm.org/10.1145/98460.98523},
 acmid = {98523},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dan:1990:AAL:98460.98525,
 author = {Dan, Asit and Towsley, Don},
 title = {An approximate analysis of the LRU and FIFO buffer replacement schemes},
 abstract = {In this paper, we develop approximate analytical models for predicting the buffer hit probability under the Least Recently Used (LRU) and First In First Out (FIFO) buffer replacement policies under the independent reference model. In the case of the analysis of the LRU policy, the computational complexity for estimating the buffer hit probability is O(KB) where B is the size of the buffer and K denotes the number of items having distinct access probabilities. In the case of the FIFO policy, the solution algorithm is iterative and the computational complexity of each iteration is O(K). Results from these models are compared to exact results for models originally developed by King [KING71] for small values of the buffer size, B, and the total number of items sharing the buffer, D. Results are also compared with results from a simulation for large values of B and D. In most cases, the error is extremely small (less than 0.1\%) for both LRU and FIFO, and a maximum error of 3\% is observed for very small buffer size (less than 5) when the access probabilities are extremely skewed. To demonstrate the usefulness of the model, we consider two applications. In our first application, we compare the LRU and FIFO policies to an optimal static buffer allocation policy for a database consisting of two classes of data items. We observe that the performance of LRU is close to that of the optimal allocation. As the optimal allocation requires knowledge of the access probabilities, the LRU policy is preferred when this information is unavailable. We also observe that the LRU policy always performs better than the FIFO policy in our experiments. In our second application, we show that if multiple independent reference streams on mutually disjoint sets of data compete for the same buffer, it is better to partition the buffer using an optimal allocation policy than to share a common buffer.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {143--152},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98525},
 doi = {http://doi.acm.org/10.1145/98460.98525},
 acmid = {98525},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dan:1990:AAL:98457.98525,
 author = {Dan, Asit and Towsley, Don},
 title = {An approximate analysis of the LRU and FIFO buffer replacement schemes},
 abstract = {In this paper, we develop approximate analytical models for predicting the buffer hit probability under the Least Recently Used (LRU) and First In First Out (FIFO) buffer replacement policies under the independent reference model. In the case of the analysis of the LRU policy, the computational complexity for estimating the buffer hit probability is O(KB) where B is the size of the buffer and K denotes the number of items having distinct access probabilities. In the case of the FIFO policy, the solution algorithm is iterative and the computational complexity of each iteration is O(K). Results from these models are compared to exact results for models originally developed by King [KING71] for small values of the buffer size, B, and the total number of items sharing the buffer, D. Results are also compared with results from a simulation for large values of B and D. In most cases, the error is extremely small (less than 0.1\%) for both LRU and FIFO, and a maximum error of 3\% is observed for very small buffer size (less than 5) when the access probabilities are extremely skewed. To demonstrate the usefulness of the model, we consider two applications. In our first application, we compare the LRU and FIFO policies to an optimal static buffer allocation policy for a database consisting of two classes of data items. We observe that the performance of LRU is close to that of the optimal allocation. As the optimal allocation requires knowledge of the access probabilities, the LRU policy is preferred when this information is unavailable. We also observe that the LRU policy always performs better than the FIFO policy in our experiments. In our second application, we show that if multiple independent reference streams on mutually disjoint sets of data compete for the same buffer, it is better to partition the buffer using an optimal allocation policy than to share a common buffer.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {143--152},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98525},
 doi = {http://doi.acm.org/10.1145/98457.98525},
 acmid = {98525},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Alonso:1990:AFW:98460.98753,
 author = {Alonso, Raphael and Appel, Andrew W.},
 title = {An advisor for flexible working sets},
 abstract = {The traditional model of virtual memory working sets does not account for programs that can adjust their working sets on demand. Examples of such programs are garbage-collected systems and databases with block cache buffers. We present a memory-use model of such systems, and propose a method that may be used by virtual memory managers to advise programs on how to adjust their working sets. Our method tries to minimize memory contention and ensure better overall system response time. We have implemented a memory ``advice server" that runs as a non-privileged process under Berkeley Unix. User processes may ask this server for advice about working set sizes, so as to take maximum advantage of memory resources. Our implementation is quite simple, and has negligible overhead, and experimental results show that it results in sizable performance improvements.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {153--162},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98753},
 doi = {http://doi.acm.org/10.1145/98460.98753},
 acmid = {98753},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Alonso:1990:AFW:98457.98753,
 author = {Alonso, Raphael and Appel, Andrew W.},
 title = {An advisor for flexible working sets},
 abstract = {The traditional model of virtual memory working sets does not account for programs that can adjust their working sets on demand. Examples of such programs are garbage-collected systems and databases with block cache buffers. We present a memory-use model of such systems, and propose a method that may be used by virtual memory managers to advise programs on how to adjust their working sets. Our method tries to minimize memory contention and ensure better overall system response time. We have implemented a memory ``advice server" that runs as a non-privileged process under Berkeley Unix. User processes may ask this server for advice about working set sizes, so as to take maximum advantage of memory resources. Our implementation is quite simple, and has negligible overhead, and experimental results show that it results in sizable performance improvements.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {153--162},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98753},
 doi = {http://doi.acm.org/10.1145/98457.98753},
 acmid = {98753},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Torrellas:1990:ACA:98457.98754,
 author = {Torrellas, Joseph and Hennessy, John and Weil, Thierry},
 title = {Analysis of critical architectural and programming parameters in a hierarchical},
 abstract = {Scalable shared-memory multiprocessors are the subject of much current research, but little is known about the performance behavior of these machines. This paper studies the performance effects of two machine characteristics and two program characteristics that seem to be major factors in determining the performance of a hierarchical shared-memory machine. We develop an analytical model of the traffic in a machine loosely based on Stanford's DASH multiprocessor and use program parameters extracted from multiprocessor traces to study its performance. It is shown that both locality in the data reference stream and the amount of data sharing in a program have an important impact on performance. Although less obvious, the bandwidth within each cluster in the hierarchy also has a significant performance effect. Optimizations that improve the intracluster cache coherence protocol or increase the bandwidth within a cluster can be quite effective.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98754},
 doi = {http://doi.acm.org/10.1145/98457.98754},
 acmid = {98754},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Torrellas:1990:ACA:98460.98754,
 author = {Torrellas, Joseph and Hennessy, John and Weil, Thierry},
 title = {Analysis of critical architectural and programming parameters in a hierarchical},
 abstract = {Scalable shared-memory multiprocessors are the subject of much current research, but little is known about the performance behavior of these machines. This paper studies the performance effects of two machine characteristics and two program characteristics that seem to be major factors in determining the performance of a hierarchical shared-memory machine. We develop an analytical model of the traffic in a machine loosely based on Stanford's DASH multiprocessor and use program parameters extracted from multiprocessor traces to study its performance. It is shown that both locality in the data reference stream and the amount of data sharing in a program have an important impact on performance. Although less obvious, the bandwidth within each cluster in the hierarchy also has a significant performance effect. Optimizations that improve the intracluster cache coherence protocol or increase the bandwidth within a cluster can be quite effective.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98754},
 doi = {http://doi.acm.org/10.1145/98460.98754},
 acmid = {98754},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jog:1990:PEC:98457.98756,
 author = {Jog, Rajeev and Vitale, Philip L. and Callister, James R.},
 title = {Performance evaluation of a commercial cache-coherent shared memory multiprocessor},
 abstract = {This paper describes an approximate Mean Value Analysis (MVA) model developed to project the performance of a small-scale shared-memory commercial symmetric multiprocessor system. The system, based on Hewlett Packard Precision Architecture processors, supports multiple active user processes and multiple execution threads within the operating system.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {173--182},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98457.98756},
 doi = {http://doi.acm.org/10.1145/98457.98756},
 acmid = {98756},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jog:1990:PEC:98460.98756,
 author = {Jog, Rajeev and Vitale, Philip L. and Callister, James R.},
 title = {Performance evaluation of a commercial cache-coherent shared memory multiprocessor},
 abstract = {This paper describes an approximate Mean Value Analysis (MVA) model developed to project the performance of a small-scale shared-memory commercial symmetric multiprocessor system. The system, based on Hewlett Packard Precision Architecture processors, supports multiple active user processes and multiple execution threads within the operating system.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {173--182},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/98460.98756},
 doi = {http://doi.acm.org/10.1145/98460.98756},
 acmid = {98756},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gelenbe:1990:PAC:98460.98757,
 author = {Gelenbe, Erol},
 title = {Performance analysis of the connection machine},
 abstract = {This paper presents an analysis of the performance of the Connection Machine, with special emphasis on estimating the effect of its interprocessor communication architecture. A queueing model of the network architecture, including the NEWS and ROUTER networks, is used to compute the slow-down induced by message exchange between processors. Locality of the message exchanges is modelled by message sending probabilities which depend on whether a message is sent by a processor to another processor placed on the same NEWS network, or on the same ROUTER, or at a ``remote" location which is only accessible via the ROUTER network. The specific slotted TDMA structure of the ROUTER Network communications is taken into account. The performance degradation of the Connection Machine as a function of the communication and architecture parameters is derived.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {183--191},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98460.98757},
 doi = {http://doi.acm.org/10.1145/98460.98757},
 acmid = {98757},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelenbe:1990:PAC:98457.98757,
 author = {Gelenbe, Erol},
 title = {Performance analysis of the connection machine},
 abstract = {This paper presents an analysis of the performance of the Connection Machine, with special emphasis on estimating the effect of its interprocessor communication architecture. A queueing model of the network architecture, including the NEWS and ROUTER networks, is used to compute the slow-down induced by message exchange between processors. Locality of the message exchanges is modelled by message sending probabilities which depend on whether a message is sent by a processor to another processor placed on the same NEWS network, or on the same ROUTER, or at a ``remote" location which is only accessible via the ROUTER network. The specific slotted TDMA structure of the ROUTER Network communications is taken into account. The performance degradation of the Connection Machine as a function of the communication and architecture parameters is derived.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {183--191},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/98457.98757},
 doi = {http://doi.acm.org/10.1145/98457.98757},
 acmid = {98757},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Willick:1990:AMM:98457.98758,
 author = {Willick, Darryl L. and Eager, D. L.},
 title = {An analytic model of multistage interconnection networks},
 abstract = {Multiprocessors require an interconnection network to connect processors with memory modules. The performance of the interconnection network can have a large effect upon overall system performance, and, therefore, methods are needed to model and compare alternative network architectures.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {192--202},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98457.98758},
 doi = {http://doi.acm.org/10.1145/98457.98758},
 acmid = {98758},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Willick:1990:AMM:98460.98758,
 author = {Willick, Darryl L. and Eager, D. L.},
 title = {An analytic model of multistage interconnection networks},
 abstract = {Multiprocessors require an interconnection network to connect processors with memory modules. The performance of the interconnection network can have a large effect upon overall system performance, and, therefore, methods are needed to model and compare alternative network architectures.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {192--202},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98460.98758},
 doi = {http://doi.acm.org/10.1145/98460.98758},
 acmid = {98758},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dussa:1990:DPT:98460.98759,
 author = {Dussa, K. and Carlson, B. and Dowdy, L. and Park, K.-H.},
 title = {Dynamic partitioning in a transputer environment},
 abstract = {Parallel programs are characterized by their speedup behavior. As more processors are allocated to a particular parallel program, the program (potentially) executes faster. However, there is often a point of diminishing returns, beyond which extra allocated processors cannot be used effectively. Extra processors would be better utilized by allocating them to another program. Thus, given a set of processors in a multiprocessor system, and a set of parallel programs, a partitioning problem naturally arises which seeks to allocate processors to programs optimally.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {203--213},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98460.98759},
 doi = {http://doi.acm.org/10.1145/98460.98759},
 acmid = {98759},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dussa:1990:DPT:98457.98759,
 author = {Dussa, K. and Carlson, B. and Dowdy, L. and Park, K.-H.},
 title = {Dynamic partitioning in a transputer environment},
 abstract = {Parallel programs are characterized by their speedup behavior. As more processors are allocated to a particular parallel program, the program (potentially) executes faster. However, there is often a point of diminishing returns, beyond which extra allocated processors cannot be used effectively. Extra processors would be better utilized by allocating them to another program. Thus, given a set of processors in a multiprocessor system, and a set of parallel programs, a partitioning problem naturally arises which seeks to allocate processors to programs optimally.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {203--213},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98457.98759},
 doi = {http://doi.acm.org/10.1145/98457.98759},
 acmid = {98759},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zahorjan:1990:PSS:98457.98760,
 author = {Zahorjan, John and McCann, Cathy},
 title = {Processor scheduling in shared memory multiprocessors},
 abstract = {Existing work indicates that the commonly used ``single queue of runnable tasks" approach to scheduling shared memory multiprocessors can perform very poorly in a multiprogrammed parallel processing environment. A more promising approach is the class of ``two-level schedulers" in which the operating system deals solely with allocating processors to jobs while the individual jobs themselves perform task dispatching on those processors.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {214--225},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/98457.98760},
 doi = {http://doi.acm.org/10.1145/98457.98760},
 acmid = {98760},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahorjan:1990:PSS:98460.98760,
 author = {Zahorjan, John and McCann, Cathy},
 title = {Processor scheduling in shared memory multiprocessors},
 abstract = {Existing work indicates that the commonly used ``single queue of runnable tasks" approach to scheduling shared memory multiprocessors can perform very poorly in a multiprogrammed parallel processing environment. A more promising approach is the class of ``two-level schedulers" in which the operating system deals solely with allocating processors to jobs while the individual jobs themselves perform task dispatching on those processors.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {214--225},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/98460.98760},
 doi = {http://doi.acm.org/10.1145/98460.98760},
 acmid = {98760},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Leutenegger:1990:PMM:98460.98761,
 author = {Leutenegger, Scott T. and Vernon, Mary K.},
 title = {The performance of multiprogrammed multiprocessor scheduling algorithms},
 abstract = {Scheduling policies for general purpose multiprogrammed multiprocessors are not well understood. This paper examines various policies to determine which properties of a scheduling policy are the most significant determinants of performance. We compare a more comprehensive set of policies than previous work, including one important scheduling policy that has not previously been examined. We also compare the policies under workloads that we feel are more realistic than previous studies have used. Using these new workloads, we arrive at different conclusions than reported in earlier work. In particular, we find that the ``smallest number of processes first" (SNPF) scheduling discipline performs poorly, even when the number of processes in a job is positively correlated with the total service demand of the job. We also find that policies that allocate an equal fraction of the processing power to each job in the system perform better, on the whole, than policies that allocate processing power unequally. Finally, we find that for lock access synchronization, dividing processing power equally among all jobs in the system is a more effective property of a scheduling policy than the property of minimizing synchronization spin-waiting, unless demand for synchronization is extremely high. (The latter property is implemented by coscheduling processes within a job, or by using a thread management package that avoids preemption of processes that hold spinlocks.) Our studies are done by simulating abstract models of the system and the workloads.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {226--236},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98460.98761},
 doi = {http://doi.acm.org/10.1145/98460.98761},
 acmid = {98761},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Leutenegger:1990:PMM:98457.98761,
 author = {Leutenegger, Scott T. and Vernon, Mary K.},
 title = {The performance of multiprogrammed multiprocessor scheduling algorithms},
 abstract = {Scheduling policies for general purpose multiprogrammed multiprocessors are not well understood. This paper examines various policies to determine which properties of a scheduling policy are the most significant determinants of performance. We compare a more comprehensive set of policies than previous work, including one important scheduling policy that has not previously been examined. We also compare the policies under workloads that we feel are more realistic than previous studies have used. Using these new workloads, we arrive at different conclusions than reported in earlier work. In particular, we find that the ``smallest number of processes first" (SNPF) scheduling discipline performs poorly, even when the number of processes in a job is positively correlated with the total service demand of the job. We also find that policies that allocate an equal fraction of the processing power to each job in the system perform better, on the whole, than policies that allocate processing power unequally. Finally, we find that for lock access synchronization, dividing processing power equally among all jobs in the system is a more effective property of a scheduling policy than the property of minimizing synchronization spin-waiting, unless demand for synchronization is extremely high. (The latter property is implemented by coscheduling processes within a job, or by using a thread management package that avoids preemption of processes that hold spinlocks.) Our studies are done by simulating abstract models of the system and the workloads.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {226--236},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/98457.98761},
 doi = {http://doi.acm.org/10.1145/98457.98761},
 acmid = {98761},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dawkins:1990:ESM:98460.98762,
 author = {Dawkins, W. P. and Debbad, V. and Jump, J. R. and Sinclair, J. B.},
 title = {Efficient simulation of multiprogramming},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {237--238},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98762},
 doi = {http://doi.acm.org/10.1145/98460.98762},
 acmid = {98762},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dawkins:1990:ESM:98457.98762,
 author = {Dawkins, W. P. and Debbad, V. and Jump, J. R. and Sinclair, J. B.},
 title = {Efficient simulation of multiprogramming},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {237--238},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98762},
 doi = {http://doi.acm.org/10.1145/98457.98762},
 acmid = {98762},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shenker:1990:MFC:98460.98763,
 author = {Shenker, Scott},
 title = {Making flow control work in networks: a control-theoretic analysis of gateway service disciplines},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {239--240},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98763},
 doi = {http://doi.acm.org/10.1145/98460.98763},
 acmid = {98763},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shenker:1990:MFC:98457.98763,
 author = {Shenker, Scott},
 title = {Making flow control work in networks: a control-theoretic analysis of gateway service disciplines},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {239--240},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98763},
 doi = {http://doi.acm.org/10.1145/98457.98763},
 acmid = {98763},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shenker:1990:MGW:98457.98764,
 author = {Shenker, Scott},
 title = {Making greed work in networks: a game-theoretic analysis of gateway service disciplines},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {241--242},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98764},
 doi = {http://doi.acm.org/10.1145/98457.98764},
 acmid = {98764},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shenker:1990:MGW:98460.98764,
 author = {Shenker, Scott},
 title = {Making greed work in networks: a game-theoretic analysis of gateway service disciplines},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {241--242},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98764},
 doi = {http://doi.acm.org/10.1145/98460.98764},
 acmid = {98764},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ghandeharizadeh:1990:FAP:98460.98765,
 author = {Ghandeharizadeh, Shahram and DeWitt, David J.},
 title = {Factors affecting the performance of multiuser database management systems},
 abstract = {While in the past 20 years database management systems (DBMS) have become a critical component of almost all organizations, their behavior in a multiuser environment has surprisingly not been studied carefully. In order to help us understand the multiuser performance of the multiprocessor Gamma database machine [DEWI90], we began by studying the performance of a single processor version of this system. In this paper, we describe some of the factors that affect the performance of DBMS in a multiuser environment. We refer the interested reader to [GHAN90] for more details.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {243--244},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98765},
 doi = {http://doi.acm.org/10.1145/98460.98765},
 acmid = {98765},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ghandeharizadeh:1990:FAP:98457.98765,
 author = {Ghandeharizadeh, Shahram and DeWitt, David J.},
 title = {Factors affecting the performance of multiuser database management systems},
 abstract = {While in the past 20 years database management systems (DBMS) have become a critical component of almost all organizations, their behavior in a multiuser environment has surprisingly not been studied carefully. In order to help us understand the multiuser performance of the multiprocessor Gamma database machine [DEWI90], we began by studying the performance of a single processor version of this system. In this paper, we describe some of the factors that affect the performance of DBMS in a multiuser environment. We refer the interested reader to [GHAN90] for more details.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {243--244},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98765},
 doi = {http://doi.acm.org/10.1145/98457.98765},
 acmid = {98765},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Englert:1990:BNS:98457.98766,
 author = {Englert, Susanne and Gray, Jim and Kocher, Terrye and Shah, Praful},
 title = {A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {245--246},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98766},
 doi = {http://doi.acm.org/10.1145/98457.98766},
 acmid = {98766},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Englert:1990:BNS:98460.98766,
 author = {Englert, Susanne and Gray, Jim and Kocher, Terrye and Shah, Praful},
 title = {A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {245--246},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98766},
 doi = {http://doi.acm.org/10.1145/98460.98766},
 acmid = {98766},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Somani:1990:PMR:98457.98768,
 author = {Somani, Arun K. and Ritcey, James A. and Au, Stephen H. L.},
 title = {Phased mission reliability analysis},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {247--248},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98768},
 doi = {http://doi.acm.org/10.1145/98457.98768},
 acmid = {98768},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Somani:1990:PMR:98460.98768,
 author = {Somani, Arun K. and Ritcey, James A. and Au, Stephen H. L.},
 title = {Phased mission reliability analysis},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {247--248},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98768},
 doi = {http://doi.acm.org/10.1145/98460.98768},
 acmid = {98768},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mitchell:1990:PAF:98457.98769,
 author = {Mitchell, Lionel C.},
 title = {Performance analysis of a fault tolerant computer system},
 abstract = {This paper presents the description of an analytical queueing network model of a Tandem computer system in the FAA Remote Maintenance Monitoring environment and a performance analysis of the Maintenance Processor Subsystem for the 1990s time frame. The approach was to use measurement data to quantify application service demands and performance contributions of the fault-tolerant software in the Tandem environment in an analytical queueing network model. Sensitivity analyses were conducted using the model to examine alternative configurations, workload growth, and system overhead among others. The model framework and performance analysis methodology can be used for capacity planning purposes during the operational phase of the system.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {249--250},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98769},
 doi = {http://doi.acm.org/10.1145/98457.98769},
 acmid = {98769},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mitchell:1990:PAF:98460.98769,
 author = {Mitchell, Lionel C.},
 title = {Performance analysis of a fault tolerant computer system},
 abstract = {This paper presents the description of an analytical queueing network model of a Tandem computer system in the FAA Remote Maintenance Monitoring environment and a performance analysis of the Maintenance Processor Subsystem for the 1990s time frame. The approach was to use measurement data to quantify application service demands and performance contributions of the fault-tolerant software in the Tandem environment in an analytical queueing network model. Sensitivity analyses were conducted using the model to examine alternative configurations, workload growth, and system overhead among others. The model framework and performance analysis methodology can be used for capacity planning purposes during the operational phase of the system.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {249--250},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98769},
 doi = {http://doi.acm.org/10.1145/98460.98769},
 acmid = {98769},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jensen:1990:RTD:98460.98770,
 author = {Jensen, David W. and Reed, Daniel A.},
 title = {Ray tracing on distributed memory parallel systems},
 abstract = {Among the many techniques in computer graphics, ray tracing is prized because it can render realistic images, albeit at great computational expense. In this note we explore the performance of several approaches to ray tracing on a distributed memory parallel system. A set of performance instrumentation tools and their associated visualization software are used to identify the underlying causes of performance differences.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {251--252},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98770},
 doi = {http://doi.acm.org/10.1145/98460.98770},
 acmid = {98770},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jensen:1990:RTD:98457.98770,
 author = {Jensen, David W. and Reed, Daniel A.},
 title = {Ray tracing on distributed memory parallel systems},
 abstract = {Among the many techniques in computer graphics, ray tracing is prized because it can render realistic images, albeit at great computational expense. In this note we explore the performance of several approaches to ray tracing on a distributed memory parallel system. A set of performance instrumentation tools and their associated visualization software are used to identify the underlying causes of performance differences.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {251--252},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98770},
 doi = {http://doi.acm.org/10.1145/98457.98770},
 acmid = {98770},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mirchandani:1990:CME:98460.98771,
 author = {Mirchandani, Dinesh and Biswas, Prabuddha},
 title = {Characterizing and modeling Ethernet performance of distributed DECwindows applications},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {253--254},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98771},
 doi = {http://doi.acm.org/10.1145/98460.98771},
 acmid = {98771},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mirchandani:1990:CME:98457.98771,
 author = {Mirchandani, Dinesh and Biswas, Prabuddha},
 title = {Characterizing and modeling Ethernet performance of distributed DECwindows applications},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {253--254},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98771},
 doi = {http://doi.acm.org/10.1145/98457.98771},
 acmid = {98771},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McGehearty:1990:COP:98460.98773,
 author = {McGehearty, Patrick F.},
 title = {Challenges in obtaining peak parallel performance with a Convex C240, parallel vector processor},
 abstract = {This report examines the behavior of the Linpack 300\&times;300 benchmark [Dongarra] on a parallel vector machine. It is observed that the performance of several parallel vector machines on this application is far below their nominal peak performance. Dissection of the internals of the algorithms shows how peak performance is limited. The insights gained provide guidance to algorithm developers as to ways to make maximum use of architectural strengths. System architects may gain insight about which system characteristics to optimize to increase the performance of future designs for this class of application.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {255--256},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98773},
 doi = {http://doi.acm.org/10.1145/98460.98773},
 acmid = {98773},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{McGehearty:1990:COP:98457.98773,
 author = {McGehearty, Patrick F.},
 title = {Challenges in obtaining peak parallel performance with a Convex C240, parallel vector processor},
 abstract = {This report examines the behavior of the Linpack 300\&times;300 benchmark [Dongarra] on a parallel vector machine. It is observed that the performance of several parallel vector machines on this application is far below their nominal peak performance. Dissection of the internals of the algorithms shows how peak performance is limited. The insights gained provide guidance to algorithm developers as to ways to make maximum use of architectural strengths. System architects may gain insight about which system characteristics to optimize to increase the performance of future designs for this class of application.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {255--256},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98773},
 doi = {http://doi.acm.org/10.1145/98457.98773},
 acmid = {98773},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heimlich:1990:TCN:98460.98774,
 author = {Heimlich, Steven A.},
 title = {Traffic characterization of the NSFNET national backbone},
 abstract = {Traditionally, models of packet arrival in communication networks have assumed either Poisson or compound Poisson arrival patterns. A study of a token ring local area network (LAN) at MIT [5] found that packet arrival followed neither of these models. Instead, traffic followed a more general model dubbed the ``packet train," which describes network traffic as a collection of packet streams traveling between pairs of nodes. A packet train consists of a number of packets travelling between a particular node pair.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {257--258},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98774},
 doi = {http://doi.acm.org/10.1145/98460.98774},
 acmid = {98774},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heimlich:1990:TCN:98457.98774,
 author = {Heimlich, Steven A.},
 title = {Traffic characterization of the NSFNET national backbone},
 abstract = {Traditionally, models of packet arrival in communication networks have assumed either Poisson or compound Poisson arrival patterns. A study of a token ring local area network (LAN) at MIT [5] found that packet arrival followed neither of these models. Instead, traffic followed a more general model dubbed the ``packet train," which describes network traffic as a collection of packet streams traveling between pairs of nodes. A packet train consists of a number of packets travelling between a particular node pair.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {257--258},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98774},
 doi = {http://doi.acm.org/10.1145/98457.98774},
 acmid = {98774},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Davidson:1990:EEA:98457.98775,
 author = {Davidson, Jack W. and Whalley, David B.},
 title = {Ease: an environment for architecture study and experimentation},
 abstract = {Gathering detailed measurements of the execution behavior of an instruction set architecture is difficult. There are two major problems that must be solved. First, for meaningful measurements to be obtained, programs that represent typical work load and instruction mixes must be used. This means that high-level language compilers for the target architecture are required. This problem is further compounded as most architectures require an optimizing compiler to exploit their capabilities. Building such a compiler can be a formidable task.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {259--260},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98775},
 doi = {http://doi.acm.org/10.1145/98457.98775},
 acmid = {98775},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Davidson:1990:EEA:98460.98775,
 author = {Davidson, Jack W. and Whalley, David B.},
 title = {Ease: an environment for architecture study and experimentation},
 abstract = {Gathering detailed measurements of the execution behavior of an instruction set architecture is difficult. There are two major problems that must be solved. First, for meaningful measurements to be obtained, programs that represent typical work load and instruction mixes must be used. This means that high-level language compilers for the target architecture are required. This problem is further compounded as most architectures require an optimizing compiler to exploit their capabilities. Building such a compiler can be a formidable task.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {259--260},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98775},
 doi = {http://doi.acm.org/10.1145/98460.98775},
 acmid = {98775},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Waclawsky:1990:DQB:98460.98777,
 author = {Waclawsky, John G. and Agrawala, Ashok K.},
 title = {Dynamic queue behavior in networks with window protocols},
 abstract = {In this paper we employ a deterministic analysis technique to characterize the dynamic queueing aspects of window protocols. The deterministic behavior of these protocols and the deterministic influence of the resources along the physical path are explicitly considered in the evaluation of path queue behavior. Transient and steady state queue behavior of fixed and sliding window protocols are investigated. We discover the existence of significant nonlinearities in the dynamics of queue activity.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {261--262},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98777},
 doi = {http://doi.acm.org/10.1145/98460.98777},
 acmid = {98777},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Waclawsky:1990:DQB:98457.98777,
 author = {Waclawsky, John G. and Agrawala, Ashok K.},
 title = {Dynamic queue behavior in networks with window protocols},
 abstract = {In this paper we employ a deterministic analysis technique to characterize the dynamic queueing aspects of window protocols. The deterministic behavior of these protocols and the deterministic influence of the resources along the physical path are explicitly considered in the evaluation of path queue behavior. Transient and steady state queue behavior of fixed and sliding window protocols are investigated. We discover the existence of significant nonlinearities in the dynamics of queue activity.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {261--262},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98777},
 doi = {http://doi.acm.org/10.1145/98457.98777},
 acmid = {98777},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Garofalakis:1990:PMI:98460.98779,
 author = {Garofalakis, John D. and Spirakis, Paul G.},
 title = {The performance of multistage interconnection networks with finite buffers},
 abstract = {Multistage interconnection networks with crossbar switches are a major component of parallel machines. In this paper we analyze Banyan networks of k by k switches and with finite buffers. The exact solution of the steady state distribution of the first stage is derived in the situation where packets are lost when they encounter a full buffer (Assumption A). The solution is a linear combination of k-1 geometrics. We use this to get an approximation for the steady state distributions in the second stage and beyond. As a side effect, the infinite buffer case is solved, confirming known results. Our results are validated by extensive simulations. An alternate situation of networks where full buffers may block previous switches is also analyzed through an approximation technique (Assumption B).
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {263--264},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98779},
 doi = {http://doi.acm.org/10.1145/98460.98779},
 acmid = {98779},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Garofalakis:1990:PMI:98457.98779,
 author = {Garofalakis, John D. and Spirakis, Paul G.},
 title = {The performance of multistage interconnection networks with finite buffers},
 abstract = {Multistage interconnection networks with crossbar switches are a major component of parallel machines. In this paper we analyze Banyan networks of k by k switches and with finite buffers. The exact solution of the steady state distribution of the first stage is derived in the situation where packets are lost when they encounter a full buffer (Assumption A). The solution is a linear combination of k-1 geometrics. We use this to get an approximation for the steady state distributions in the second stage and beyond. As a side effect, the infinite buffer case is solved, confirming known results. Our results are validated by extensive simulations. An alternate situation of networks where full buffers may block previous switches is also analyzed through an approximation technique (Assumption B).
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {263--264},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98779},
 doi = {http://doi.acm.org/10.1145/98457.98779},
 acmid = {98779},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vasilakos:1990:AWF:98457.98780,
 author = {Vasilakos, Athanasios V. and Moschonas, Christos A. and Paximadis, Constantinos T.},
 title = {Adaptive window flow control and learning algorithms for adaptive routing in data networks},
 abstract = {We present a new adaptive flow control algorithm together with learning routing algorithms.
},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {265--266},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98457.98780},
 doi = {http://doi.acm.org/10.1145/98457.98780},
 acmid = {98780},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vasilakos:1990:AWF:98460.98780,
 author = {Vasilakos, Athanasios V. and Moschonas, Christos A. and Paximadis, Constantinos T.},
 title = {Adaptive window flow control and learning algorithms for adaptive routing in data networks},
 abstract = {We present a new adaptive flow control algorithm together with learning routing algorithms.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {265--266},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/98460.98780},
 doi = {http://doi.acm.org/10.1145/98460.98780},
 acmid = {98780},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nussbaum:1990:MCS:98457.98781,
 author = {Nussbaum, Daniel and Vuong-Adlerberg, Ingmar and Agarwal, Anant},
 title = {Modeling a circuit switched multiprocessor interconnect},
 abstract = {},
 booktitle = {Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '90},
 year = {1990},
 isbn = {0-89791-359-0},
 location = {Univ. of Colorado, Boulder, Colorado, United States},
 pages = {267--269},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/98457.98781},
 doi = {http://doi.acm.org/10.1145/98457.98781},
 acmid = {98781},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nussbaum:1990:MCS:98460.98781,
 author = {Nussbaum, Daniel and Vuong-Adlerberg, Ingmar and Agarwal, Anant},
 title = {Modeling a circuit switched multiprocessor interconnect},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {18},
 issue = {1},
 month = {April},
 year = {1990},
 issn = {0163-5999},
 pages = {267--269},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/98460.98781},
 doi = {http://doi.acm.org/10.1145/98460.98781},
 acmid = {98781},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolf:1989:POP:75108.75373,
 author = {Wolf, J.},
 title = {The placement optimization program: a practical solution to the disk file assignment problem},
 abstract = {In this paper we describe a practical mathematical formulation and solution of the so-called ``File Assignment Problem" (FAP) for computer disks. Our FAP solution has been implemented in a PL/I program known as the Placement Optimization Program (POP). The algorithm consists of three major components \&mdash; two heuristic optimization models and a queueing network model. POP has been used in validation studies to assign files to disks in two IBM MVS complexes. The resulting savings in I/O response times were 22\% and 25\%, respectively. Throughout the paper we shall emphasize the real-world nature of our approach to the disk FAP, which we believe sets it apart from previous attempts.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75373},
 doi = {http://doi.acm.org/10.1145/75108.75373},
 acmid = {75373},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wolf:1989:POP:75372.75373,
 author = {Wolf, J.},
 title = {The placement optimization program: a practical solution to the disk file assignment problem},
 abstract = {In this paper we describe a practical mathematical formulation and solution of the so-called ``File Assignment Problem" (FAP) for computer disks. Our FAP solution has been implemented in a PL/I program known as the Placement Optimization Program (POP). The algorithm consists of three major components \&mdash; two heuristic optimization models and a queueing network model. POP has been used in validation studies to assign files to disks in two IBM MVS complexes. The resulting savings in I/O response times were 22\% and 25\%, respectively. Throughout the paper we shall emphasize the real-world nature of our approach to the disk FAP, which we believe sets it apart from previous attempts.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75373},
 doi = {http://doi.acm.org/10.1145/75372.75373},
 acmid = {75373},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kearns:1989:DDR:75372.75374,
 author = {Kearns, J. P. and DeFazio, S.},
 title = {Diversity in database reference behavior},
 abstract = {Over the past fifteen years, empirical studies of the reference behavior of a number of database systems have produced seemingly contradictory results. The presence or absence of locality of reference and sequentiality have both been reported (or denied) in various papers. As such, the performance analyst or database implementor is left with little concrete guidance in the form of expected reference behavior of a database system under a realistic workload. We present empirical evidence that all of the previous results about database reference behavior are correct (or incorrect). That is, if the database reference sequence is viewed on a per-transaction instance or per-database basis, almost any reference behavior is discernible. Previous results which report the absolute absence or presence of a certain form of reference behavior were almost certainly derived from reference traces which were dominated by transactions or databases which exhibited a certain behavior. Our sample consists of roughly twenty-five million block references, from 350,000 transaction executions, directed at 175 operational on-line databases at two major corporations. As such, the sample is an order of magnitude more comprehensive than any other reported in the literature.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {11--19},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75374},
 doi = {http://doi.acm.org/10.1145/75372.75374},
 acmid = {75374},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kearns:1989:DDR:75108.75374,
 author = {Kearns, J. P. and DeFazio, S.},
 title = {Diversity in database reference behavior},
 abstract = {Over the past fifteen years, empirical studies of the reference behavior of a number of database systems have produced seemingly contradictory results. The presence or absence of locality of reference and sequentiality have both been reported (or denied) in various papers. As such, the performance analyst or database implementor is left with little concrete guidance in the form of expected reference behavior of a database system under a realistic workload. We present empirical evidence that all of the previous results about database reference behavior are correct (or incorrect). That is, if the database reference sequence is viewed on a per-transaction instance or per-database basis, almost any reference behavior is discernible. Previous results which report the absolute absence or presence of a certain form of reference behavior were almost certainly derived from reference traces which were dominated by transactions or databases which exhibited a certain behavior. Our sample consists of roughly twenty-five million block references, from 350,000 transaction executions, directed at 175 operational on-line databases at two major corporations. As such, the sample is an order of magnitude more comprehensive than any other reported in the literature.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {11--19},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75374},
 doi = {http://doi.acm.org/10.1145/75108.75374},
 acmid = {75374},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hellerstein:1989:SAD:75372.75375,
 author = {Hellerstein, J.},
 title = {A statistical approach to diagnosing intermittent performance-problems using monotone relationships},
 abstract = {Managing a computer system requires that good performance (e.g., large throughputs, small response times) be maintained in order to meet business objectives. Rarely is performance consistently bad. More frequently, performance is good one day and bad the next. Diagnosing such intermittent performance-problems involves determining what distinguishes bad days from good days, such as larger paging rates. Once this is understood, an appropriate remedy can be found, such as buying more memory. This paper describes a statistical approach to diagnosing intermittent performance-problems when the relationships among measurement variables are expressed qualitatively as monotone relationships (e.g., paging delays increase with the number of logged-on users). We present a non-parametric test for monotonicity (NTM) that evaluates monotone relationships based on F<subscrpt>A</subscrpt>, the fraction of observation-pairs that agree with the monotone relationship. An interpretation of F<subscrpt>A</subscrpt> in terms of statistical significance levels is presented, and NTM is compared to least-squares regression. Based on NTM, an algorithm for diagnosing intermittent performance-problems is presented. NTM and our diagnosis algorithm are applied to measurements of four similarly configured IBM 9370 model 60s running IBM's operating-system Virtual Machine System Product (VM SP).
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {20--28},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75375},
 doi = {http://doi.acm.org/10.1145/75372.75375},
 acmid = {75375},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hellerstein:1989:SAD:75108.75375,
 author = {Hellerstein, J.},
 title = {A statistical approach to diagnosing intermittent performance-problems using monotone relationships},
 abstract = {Managing a computer system requires that good performance (e.g., large throughputs, small response times) be maintained in order to meet business objectives. Rarely is performance consistently bad. More frequently, performance is good one day and bad the next. Diagnosing such intermittent performance-problems involves determining what distinguishes bad days from good days, such as larger paging rates. Once this is understood, an appropriate remedy can be found, such as buying more memory. This paper describes a statistical approach to diagnosing intermittent performance-problems when the relationships among measurement variables are expressed qualitatively as monotone relationships (e.g., paging delays increase with the number of logged-on users). We present a non-parametric test for monotonicity (NTM) that evaluates monotone relationships based on F<subscrpt>A</subscrpt>, the fraction of observation-pairs that agree with the monotone relationship. An interpretation of F<subscrpt>A</subscrpt> in terms of statistical significance levels is presented, and NTM is compared to least-squares regression. Based on NTM, an algorithm for diagnosing intermittent performance-problems is presented. NTM and our diagnosis algorithm are applied to measurements of four similarly configured IBM 9370 model 60s running IBM's operating-system Virtual Machine System Product (VM SP).
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {20--28},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75375},
 doi = {http://doi.acm.org/10.1145/75108.75375},
 acmid = {75375},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Muntz:1989:BAR:75108.75376,
 author = {Muntz, R. R. and de Souza e Silva, E. and Goyal, A.},
 title = {Bounding availability of repairable computer systems},
 abstract = {Markov models are widely used for the analysis of availability of computer/communication systems. Realistic models often involve state space cardinalities that are so large that it is impractical to generate the transition rate matrix let alone solve for availability measures. Various state space reduction methods have been developed, particularly for transient analysis. In this paper we present an approximation technique for determining steady state availability. Of particular interest is that the method also provides bounds on the error. Examples are given to illustrate the method.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {29--38},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75376},
 doi = {http://doi.acm.org/10.1145/75108.75376},
 acmid = {75376},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Muntz:1989:BAR:75372.75376,
 author = {Muntz, R. R. and de Souza e Silva, E. and Goyal, A.},
 title = {Bounding availability of repairable computer systems},
 abstract = {Markov models are widely used for the analysis of availability of computer/communication systems. Realistic models often involve state space cardinalities that are so large that it is impractical to generate the transition rate matrix let alone solve for availability measures. Various state space reduction methods have been developed, particularly for transient analysis. In this paper we present an approximation technique for determining steady state availability. Of particular interest is that the method also provides bounds on the error. Examples are given to illustrate the method.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {29--38},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75376},
 doi = {http://doi.acm.org/10.1145/75372.75376},
 acmid = {75376},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bubenik:1989:POM:75108.75377,
 author = {Bubenik, R. and Zwaenepoel, W.},
 title = {Performance of optimistic make},
 abstract = {Optimistic make is a version of make that executes the commands necessary to bring targets up-to-date prior to the time the user types a make request. Side effects of these optimistic computations (such as file or screen updates) are concealed until the make request is issued. If the inputs read by the optimistic computations are identical to the inputs the computation would read at the time the make request is issued, the results of the optimistic computations are used immediately, resulting in improved response time. Otherwise, the necessary computations are reexecuted.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {39--48},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75377},
 doi = {http://doi.acm.org/10.1145/75108.75377},
 acmid = {75377},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bubenik:1989:POM:75372.75377,
 author = {Bubenik, R. and Zwaenepoel, W.},
 title = {Performance of optimistic make},
 abstract = {Optimistic make is a version of make that executes the commands necessary to bring targets up-to-date prior to the time the user types a make request. Side effects of these optimistic computations (such as file or screen updates) are concealed until the make request is issued. If the inputs read by the optimistic computations are identical to the inputs the computation would read at the time the make request is issued, the results of the optimistic computations are used immediately, resulting in improved response time. Otherwise, the necessary computations are reexecuted.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {39--48},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75377},
 doi = {http://doi.acm.org/10.1145/75372.75377},
 acmid = {75377},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1989:PIT:75108.75378,
 author = {Anderson, T. E. and Lazowska, D. D. and Levy, H. M.},
 title = {The performance implications of thread management alternatives for shared-memory multiprocessors},
 abstract = {Threads (``lightweight" processes) have become a common element of new languages and operating systems. This paper examines the performance implications of several data structure and algorithm alternatives for thread management in shared-memory multiprocessors. Both experimental measurements and analytical model projections are presented.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/75108.75378},
 doi = {http://doi.acm.org/10.1145/75108.75378},
 acmid = {75378},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Anderson:1989:PIT:75372.75378,
 author = {Anderson, T. E. and Lazowska, D. D. and Levy, H. M.},
 title = {The performance implications of thread management alternatives for shared-memory multiprocessors},
 abstract = {Threads (``lightweight" processes) have become a common element of new languages and operating systems. This paper examines the performance implications of several data structure and algorithm alternatives for thread management in shared-memory multiprocessors. Both experimental measurements and analytical model projections are presented.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {49--60},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/75372.75378},
 doi = {http://doi.acm.org/10.1145/75372.75378},
 acmid = {75378},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Carter:1989:OIB:75372.75379,
 author = {Carter, J. B. and Zwaenepoel, W.},
 title = {Optimistic implementation of bulk data transfer protocols},
 abstract = {During a bulk data transfer over a high speed network, there is a high probability that the next packet received from the network by the destination host is the next packet in the transfer. An optimistic implementation of a bulk data transfer protocol takes advantage of this observation by instructing the network interface on the destination host to deposit the data of the next packet immediately into its anticipated final location. No copying of the data is required in the common case, and overhead is greatly reduced.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {61--69},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75379},
 doi = {http://doi.acm.org/10.1145/75372.75379},
 acmid = {75379},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Carter:1989:OIB:75108.75379,
 author = {Carter, J. B. and Zwaenepoel, W.},
 title = {Optimistic implementation of bulk data transfer protocols},
 abstract = {During a bulk data transfer over a high speed network, there is a high probability that the next packet received from the network by the destination host is the next packet in the transfer. An optimistic implementation of a bulk data transfer protocol takes advantage of this observation by instructing the network interface on the destination host to deposit the data of the next packet immediately into its anticipated final location. No copying of the data is required in the common case, and overhead is greatly reduced.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {61--69},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75379},
 doi = {http://doi.acm.org/10.1145/75108.75379},
 acmid = {75379},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stunkel:1989:TPT:75108.75380,
 author = {Stunkel, C. B. and Fuchs, W. K.},
 title = {TRAPEDS: producing traces for multicomputers via execution driven simulation},
 abstract = {Trace-driven simulation is an important aid in performance analysis of computer systems. Capturing address traces for these simulations is a difficult problem for single processors and particularly for multicomputers. Even when existing trace methods can be used on multicomputers, the amount of collected data typically grows with the number of processors, so I/O and trace storage costs increase. A new technique is presented in this paper which modifies the executable code to dynamically collect the address trace from the user code and analyzes this trace during the execution of the program. This method helps resolve the I/O and storage problems and facilitates parallel analysis of the address trace. If a trace stored on disk is desired, the generated trace information can also be written to files during execution, with a resultant drop in program execution speed. An initial implementation on the Intel iPSC/2 hypercube multicomputer is detailed, and sample simulation results are presented. The effect of this trace collection method on execution time is illustrated.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {70--78},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75380},
 doi = {http://doi.acm.org/10.1145/75108.75380},
 acmid = {75380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stunkel:1989:TPT:75372.75380,
 author = {Stunkel, C. B. and Fuchs, W. K.},
 title = {TRAPEDS: producing traces for multicomputers via execution driven simulation},
 abstract = {Trace-driven simulation is an important aid in performance analysis of computer systems. Capturing address traces for these simulations is a difficult problem for single processors and particularly for multicomputers. Even when existing trace methods can be used on multicomputers, the amount of collected data typically grows with the number of processors, so I/O and trace storage costs increase. A new technique is presented in this paper which modifies the executable code to dynamically collect the address trace from the user code and analyzes this trace during the execution of the program. This method helps resolve the I/O and storage problems and facilitates parallel analysis of the address trace. If a trace stored on disk is desired, the generated trace information can also be written to files during execution, with a resultant drop in program execution speed. An initial implementation on the Intel iPSC/2 hypercube multicomputer is detailed, and sample simulation results are presented. The effect of this trace collection method on execution time is illustrated.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {70--78},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75380},
 doi = {http://doi.acm.org/10.1145/75372.75380},
 acmid = {75380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gallivan:1989:BCM:75372.75381,
 author = {Gallivan, K. and Gannon, D. and Jalby, W. and Malony, A. and Wijshoff, H.},
 title = {Behavioral characterization of multiprocessor memory systems: a case study},
 abstract = {The speed and efficiency of the memory system is a key limiting factor in the performance of supercomputers. Consequently, one of the major concerns when developing a high-performance code, either manually or automatically, is determining and characterizing the influence of the memory system on performance in terms of algorithmic parameters. Unfortunately, the performance data available to an algorithm designer such as various benchmarks and, occasionally, manufacturer-supplied information, e.g. instruction timings and architecture component characteristics, are rarely sufficient for this task. In this paper, we discuss a systematic methodology for probing the performance characteristics of a memory system via a hierarchy of data-movement kernels. We present and analyze the results obtained by such a methodology on a cache-based multi-vector processor (Alliant FX/8). Finally, we indicate how these experimental results can be used for predicting the performance of simple Fortran codes by a combination of empirical observations, architectural models and analytical techniques.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {79--88},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75381},
 doi = {http://doi.acm.org/10.1145/75372.75381},
 acmid = {75381},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gallivan:1989:BCM:75108.75381,
 author = {Gallivan, K. and Gannon, D. and Jalby, W. and Malony, A. and Wijshoff, H.},
 title = {Behavioral characterization of multiprocessor memory systems: a case study},
 abstract = {The speed and efficiency of the memory system is a key limiting factor in the performance of supercomputers. Consequently, one of the major concerns when developing a high-performance code, either manually or automatically, is determining and characterizing the influence of the memory system on performance in terms of algorithmic parameters. Unfortunately, the performance data available to an algorithm designer such as various benchmarks and, occasionally, manufacturer-supplied information, e.g. instruction timings and architecture component characteristics, are rarely sufficient for this task. In this paper, we discuss a systematic methodology for probing the performance characteristics of a memory system via a hierarchy of data-movement kernels. We present and analyze the results obtained by such a methodology on a cache-based multi-vector processor (Alliant FX/8). Finally, we indicate how these experimental results can be used for predicting the performance of simple Fortran codes by a combination of empirical observations, architectural models and analytical techniques.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {79--88},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75381},
 doi = {http://doi.acm.org/10.1145/75108.75381},
 acmid = {75381},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Samples:1989:MNT:75108.75382,
 author = {Samples, A. D.},
 title = {Mache: no-loss trace compaction},
 abstract = {Execution traces can be significantly compressed using their referencing locality. A simple observation leads to a technique capable of compressing execution traces by an order of magnitude; instruction-only traces are compressed by two orders of magnitude. This technique is unlike previously reported trace compression techniques in that it compresses without loss of information and, therefore, does not affect trace-driven simulation time or accuracy.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {89--97},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75382},
 doi = {http://doi.acm.org/10.1145/75108.75382},
 acmid = {75382},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Samples:1989:MNT:75372.75382,
 author = {Samples, A. D.},
 title = {Mache: no-loss trace compaction},
 abstract = {Execution traces can be significantly compressed using their referencing locality. A simple observation leads to a technique capable of compressing execution traces by an order of magnitude; instruction-only traces are compressed by two orders of magnitude. This technique is unlike previously reported trace compression techniques in that it compresses without loss of information and, therefore, does not affect trace-driven simulation time or accuracy.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {89--97},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75382},
 doi = {http://doi.acm.org/10.1145/75372.75382},
 acmid = {75382},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mukherjee:1989:ERS:75108.75383,
 author = {Mukherjee, A. and Landweber, L. H. and Strikwerda, J. C.},
 title = {Evaluation of retransmission strategies in a local area network environment},
 abstract = {We present an evaluation of retransmission strategies over local area networks. Expressions are derived for the expectation and the variance of the transmission time of the go-back-n and the selective repeat protocols in the presence of errors. These are compared to the expressions for blast with full retransmission on error (BFRE) derived by Zwaenepoel [Zwa 85]. We conclude that go-back-n performs almost as well as selective repeat and is very much simpler to implement while BFRE is stable only for a limited range of messages sizes and error rates. We also present a variant of BFRE which optimally checkpoints the transmission of a large message. This is shown to overcome the instability of ordinary BFRE. It has a simple state machine and seems to take full advantage of the low error rates of local area networks. We further investigate go-back-n by generalizing the analysis to an upper layer transport protocol, which is likely to encounter among other things, variable delays due to protocol overhead, multiple connections, process switches and operating system scheduling priorities.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75383},
 doi = {http://doi.acm.org/10.1145/75108.75383},
 acmid = {75383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukherjee:1989:ERS:75372.75383,
 author = {Mukherjee, A. and Landweber, L. H. and Strikwerda, J. C.},
 title = {Evaluation of retransmission strategies in a local area network environment},
 abstract = {We present an evaluation of retransmission strategies over local area networks. Expressions are derived for the expectation and the variance of the transmission time of the go-back-n and the selective repeat protocols in the presence of errors. These are compared to the expressions for blast with full retransmission on error (BFRE) derived by Zwaenepoel [Zwa 85]. We conclude that go-back-n performs almost as well as selective repeat and is very much simpler to implement while BFRE is stable only for a limited range of messages sizes and error rates. We also present a variant of BFRE which optimally checkpoints the transmission of a large message. This is shown to overcome the instability of ordinary BFRE. It has a simple state machine and seems to take full advantage of the low error rates of local area networks. We further investigate go-back-n by generalizing the analysis to an upper layer transport protocol, which is likely to encounter among other things, variable delays due to protocol overhead, multiple connections, process switches and operating system scheduling priorities.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75383},
 doi = {http://doi.acm.org/10.1145/75372.75383},
 acmid = {75383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Danzig:1989:FBF:75108.75384,
 author = {Danzig, P. B.},
 title = {Finite buffers for fast multicast},
 abstract = {When many or all of the recipients of a multicast message respond to the multicast's sender, their responses may overflow the sender's available buffer space. Buffer overflow is a serious, known problem of broadcast-based protocols, and can be troublesome when as few as three or four recipients respond. We develop analytical models that calculate the expected number of buffer overflows that can be used to estimate the number of buffers necessary for an application. The common cure for buffer overflow requires that recipients delay their responses by some random amount of time in order to increase the minimum spacing between response messages, eliminate collisions on the network, and decrease the peak processing demand at the sender. In our table driven algorithm, the sender tries to minimize the multicast's latency, the elapsed time between its initial transmission of the multicast and its reception of the final response, given the number of times (rounds) it is willing to retransmit the multicast. It includes in the multicast the time interval over which it anticipates receiving the response, the round timeout. We demonstrate that the latency of single round multicasts exceeds the latency of multiple round multicasts. We show how recipients minimize the sender's buffer overflows by independently choosing their response times as a function of the round's timeout, sender's buffer size, and the number of other recipients.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75384},
 doi = {http://doi.acm.org/10.1145/75108.75384},
 acmid = {75384},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Danzig:1989:FBF:75372.75384,
 author = {Danzig, P. B.},
 title = {Finite buffers for fast multicast},
 abstract = {When many or all of the recipients of a multicast message respond to the multicast's sender, their responses may overflow the sender's available buffer space. Buffer overflow is a serious, known problem of broadcast-based protocols, and can be troublesome when as few as three or four recipients respond. We develop analytical models that calculate the expected number of buffer overflows that can be used to estimate the number of buffers necessary for an application. The common cure for buffer overflow requires that recipients delay their responses by some random amount of time in order to increase the minimum spacing between response messages, eliminate collisions on the network, and decrease the peak processing demand at the sender. In our table driven algorithm, the sender tries to minimize the multicast's latency, the elapsed time between its initial transmission of the multicast and its reception of the final response, given the number of times (rounds) it is willing to retransmit the multicast. It includes in the multicast the time interval over which it anticipates receiving the response, the round timeout. We demonstrate that the latency of single round multicasts exceeds the latency of multiple round multicasts. We show how recipients minimize the sender's buffer overflows by independently choosing their response times as a function of the round's timeout, sender's buffer size, and the number of other recipients.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75384},
 doi = {http://doi.acm.org/10.1145/75372.75384},
 acmid = {75384},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukherjee:1989:PDU:75372.75385,
 author = {Mukherjee, B.},
 title = {Performance of a dual-bus unidirectional broadcast network operating under probabilistic scheduling strategy},
 abstract = {Recent advances in fiber optic technology (viz. its promise to provide information-carrying capacity in the Gpbs range over long repeater-free distances) has triggered tremendous activity in the study of unidirectional bus networks (because signal flow in the fiber is unidirectional). A popular network structure that has received significant attention is the Dual-bus Unidirectional Broadcast System (DUBS) network topology. Most of the access mechanism studied on this structure are based on round-robin scheduling (or some variation thereof). However since round-robin schemes suffer a loss of channel capacity because of their inter-round overhead (which can be significant for long high-speed buses), a probabilistic scheduling strategy, called p<subscrpt>i</subscrpt>-persistent protocol, has recently been proposed and studied for single channel unidirectional bus systems. Our concern here is to apply this probabilistic scheduling strategy to each bus in DUBS, and study the corresponding network performance. In so doing, we allow stations to buffer multiple packets, represent a station's queue size by a Markov chain model, and employ an independence assumption. We find that the average packet delay is bounded and the maximum network throughput approaches two pkt/slot with increasing buffer size. Further, the protocol's performance is insensitive to bus characteristics, and it appears to be a particularly well suited for fiber-optic network application requiring long distances and high bandwidth. Simulation results, which verify the analytical model, are also included.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {118--126},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75385},
 doi = {http://doi.acm.org/10.1145/75372.75385},
 acmid = {75385},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mukherjee:1989:PDU:75108.75385,
 author = {Mukherjee, B.},
 title = {Performance of a dual-bus unidirectional broadcast network operating under probabilistic scheduling strategy},
 abstract = {Recent advances in fiber optic technology (viz. its promise to provide information-carrying capacity in the Gpbs range over long repeater-free distances) has triggered tremendous activity in the study of unidirectional bus networks (because signal flow in the fiber is unidirectional). A popular network structure that has received significant attention is the Dual-bus Unidirectional Broadcast System (DUBS) network topology. Most of the access mechanism studied on this structure are based on round-robin scheduling (or some variation thereof). However since round-robin schemes suffer a loss of channel capacity because of their inter-round overhead (which can be significant for long high-speed buses), a probabilistic scheduling strategy, called p<subscrpt>i</subscrpt>-persistent protocol, has recently been proposed and studied for single channel unidirectional bus systems. Our concern here is to apply this probabilistic scheduling strategy to each bus in DUBS, and study the corresponding network performance. In so doing, we allow stations to buffer multiple packets, represent a station's queue size by a Markov chain model, and employ an independence assumption. We find that the average packet delay is bounded and the maximum network throughput approaches two pkt/slot with increasing buffer size. Further, the protocol's performance is insensitive to bus characteristics, and it appears to be a particularly well suited for fiber-optic network application requiring long distances and high bandwidth. Simulation results, which verify the analytical model, are also included.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {118--126},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75385},
 doi = {http://doi.acm.org/10.1145/75108.75385},
 acmid = {75385},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Greenberg:1989:SCP:75372.75386,
 author = {Greenberg, A. G. and McKenna, J.},
 title = {Solution of closed, product form, queueing networks via the RECAL and tree-RECAL methods on a shared memory multiprocessor},
 abstract = {RECAL is a new recurrence relation for calculating the partition function and various queue length moments for closed, product form networks. In this paper we discuss a number of the issues involved in the software implementation of RECAL on both sequential computers and parallel, shared memory computers. After a brief description of RECAL, we describe software implementing RECAL on a sequential computer. In particular, we discuss the problems involved in indexing and data storage. Next we describe code implementing RECAL on a parallel, shared memory computer. Special attention is given to designing a special buffer for temporary data storage and several other important features of the parallel code. Finally, we touch on software for serial and parallel implementations of a tree algorithm for RECAL.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {127--135},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75386},
 doi = {http://doi.acm.org/10.1145/75372.75386},
 acmid = {75386},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Greenberg:1989:SCP:75108.75386,
 author = {Greenberg, A. G. and McKenna, J.},
 title = {Solution of closed, product form, queueing networks via the RECAL and tree-RECAL methods on a shared memory multiprocessor},
 abstract = {RECAL is a new recurrence relation for calculating the partition function and various queue length moments for closed, product form networks. In this paper we discuss a number of the issues involved in the software implementation of RECAL on both sequential computers and parallel, shared memory computers. After a brief description of RECAL, we describe software implementing RECAL on a sequential computer. In particular, we discuss the problems involved in indexing and data storage. Next we describe code implementing RECAL on a parallel, shared memory computer. Special attention is given to designing a special buffer for temporary data storage and several other important features of the parallel code. Finally, we touch on software for serial and parallel implementations of a tree algorithm for RECAL.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {127--135},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75386},
 doi = {http://doi.acm.org/10.1145/75108.75386},
 acmid = {75386},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Paterok:1989:FQP:75372.75387,
 author = {Paterok, M. and Fischer, O. and Opta, L.},
 title = {Feedback queues with preemption-distance priorities},
 abstract = {The method of moments is used to derive exact analytical solutions for an open priority queueing system with preemption-distance priorities and feedback. Customers enter from outside in a Poisson stream. They can feed back for several times, changing priorities and service demands in an arbitrary manner. During feedback they can fork and branch according to user-defined probabilities. The service demands of the different classes are pairwise independent and can be arbitrarily distributed. A customer who has been interrupted resumes his service from the point where he was interrupted (preemptive resume). A system of linear equations is to be solved to obtain the mean sojourn times of each customer class in the system.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75387},
 doi = {http://doi.acm.org/10.1145/75372.75387},
 acmid = {75387},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Paterok:1989:FQP:75108.75387,
 author = {Paterok, M. and Fischer, O. and Opta, L.},
 title = {Feedback queues with preemption-distance priorities},
 abstract = {The method of moments is used to derive exact analytical solutions for an open priority queueing system with preemption-distance priorities and feedback. Customers enter from outside in a Poisson stream. They can feed back for several times, changing priorities and service demands in an arbitrary manner. During feedback they can fork and branch according to user-defined probabilities. The service demands of the different classes are pairwise independent and can be arbitrarily distributed. A customer who has been interrupted resumes his service from the point where he was interrupted (preemptive resume). A system of linear equations is to be solved to obtain the mean sojourn times of each customer class in the system.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {136--145},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75387},
 doi = {http://doi.acm.org/10.1145/75108.75387},
 acmid = {75387},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wagner:1989:PSQ:75372.75388,
 author = {Wagner, D. B. and Lazowska, E. D.},
 title = {Parallel simulation of queueing networks: limitations and potentials},
 abstract = {This paper concerns the parallel simulation of queueing network models (QNMs) using the conservative (Chandy-Misra) paradigm. Most empirical studies of conservative parallel simulation have used QNMs as benchmarks. For the most part, these studies concluded that the conservative paradigm is unsuitable for speeding up the simulation of QNMs, or that it is only suitable for simulating a very limited subclass of these models (e.g., those containing only FCFS servers). In this paper we argue that these are unnecessarily pessimistic conclusions. On the one hand, we show that the structure of some QNMs inherently limits the attainable simulation speedup. On the other hand, we show that QNMs without such limitations can be efficiently simulated using some recently introduced implementation techniques.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75388},
 doi = {http://doi.acm.org/10.1145/75372.75388},
 acmid = {75388},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wagner:1989:PSQ:75108.75388,
 author = {Wagner, D. B. and Lazowska, E. D.},
 title = {Parallel simulation of queueing networks: limitations and potentials},
 abstract = {This paper concerns the parallel simulation of queueing network models (QNMs) using the conservative (Chandy-Misra) paradigm. Most empirical studies of conservative parallel simulation have used QNMs as benchmarks. For the most part, these studies concluded that the conservative paradigm is unsuitable for speeding up the simulation of QNMs, or that it is only suitable for simulating a very limited subclass of these models (e.g., those containing only FCFS servers). In this paper we argue that these are unnecessarily pessimistic conclusions. On the one hand, we show that the structure of some QNMs inherently limits the attainable simulation speedup. On the other hand, we show that QNMs without such limitations can be efficiently simulated using some recently introduced implementation techniques.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {146--155},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75388},
 doi = {http://doi.acm.org/10.1145/75108.75388},
 acmid = {75388},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mitra:1989:CCP:75372.75389,
 author = {Mitra, D. and Mitrani, I.},
 title = {Control and coordination policies for systems with buffers},
 abstract = {We study systems consisting of a number of service cells in tandem, each containing a finite buffer. Several policies governing the operation of such systems are described and compared. These include traditional and novel blocking schemes, with applications to computer communications and production lines. In particular, it is shown that kanban, a novel discipline for coordinating cells in a manufacturing context, is obtained by combining two, more basic, concepts: a blocking policy introduced here as minimal blocking, and shared buffers. The Kanban discipline is superior in terms of throughput to the ordinary transfer blocking policy.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {156--164},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75389},
 doi = {http://doi.acm.org/10.1145/75372.75389},
 acmid = {75389},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mitra:1989:CCP:75108.75389,
 author = {Mitra, D. and Mitrani, I.},
 title = {Control and coordination policies for systems with buffers},
 abstract = {We study systems consisting of a number of service cells in tandem, each containing a finite buffer. Several policies governing the operation of such systems are described and compared. These include traditional and novel blocking schemes, with applications to computer communications and production lines. In particular, it is shown that kanban, a novel discipline for coordinating cells in a manufacturing context, is obtained by combining two, more basic, concepts: a blocking policy introduced here as minimal blocking, and shared buffers. The Kanban discipline is superior in terms of throughput to the ordinary transfer blocking policy.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {156--164},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75389},
 doi = {http://doi.acm.org/10.1145/75108.75389},
 acmid = {75389},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nicol:1989:AMP:75372.75390,
 author = {Nicol, D. M. and Townsend, J. C.},
 title = {Accurate modeling of parallel scientific computations},
 abstract = {Scientific codes are usually parallelized by partitioning a grid among processors. To achieve top performance it is necessary to partition the grid so as to balance workload and minimize communication/synchronization costs. This problem is particularly acute when the grid is irregular, changes over the course of the computation, and is not known until load-time. Critical mapping and remapping decisions rest on our ability to accurately predict performance, given a description of a grid and its partition. This paper discusses one approach to this problem, and illustrates its use on a one-dimensional fluids code. The models we construct are shown empirically to be accurate, and are used to find optimal remapping schedules.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {165--170},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/75372.75390},
 doi = {http://doi.acm.org/10.1145/75372.75390},
 acmid = {75390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nicol:1989:AMP:75108.75390,
 author = {Nicol, D. M. and Townsend, J. C.},
 title = {Accurate modeling of parallel scientific computations},
 abstract = {Scientific codes are usually parallelized by partitioning a grid among processors. To achieve top performance it is necessary to partition the grid so as to balance workload and minimize communication/synchronization costs. This problem is particularly acute when the grid is irregular, changes over the course of the computation, and is not known until load-time. Critical mapping and remapping decisions rest on our ability to accurately predict performance, given a description of a grid and its partition. This paper discusses one approach to this problem, and illustrates its use on a one-dimensional fluids code. The models we construct are shown empirically to be accurate, and are used to find optimal remapping schedules.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {165--170},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/75108.75390},
 doi = {http://doi.acm.org/10.1145/75108.75390},
 acmid = {75390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sevcik:1989:CPA:75372.75391,
 author = {Sevcik, K. C.},
 title = {Characterizations of parallelism in applications and their use in scheduling},
 abstract = {As multiprocessors with large numbers of processors become more prevalent, we face the task of developing scheduling algorithms for the multiprogrammed use of such machines. The scheduling decisions must take into account the number of processors available, the overall system load, and the ability of each application awaiting activation to make use of a given number of processors.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75391},
 doi = {http://doi.acm.org/10.1145/75372.75391},
 acmid = {75391},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sevcik:1989:CPA:75108.75391,
 author = {Sevcik, K. C.},
 title = {Characterizations of parallelism in applications and their use in scheduling},
 abstract = {As multiprocessors with large numbers of processors become more prevalent, we face the task of developing scheduling algorithms for the multiprogrammed use of such machines. The scheduling decisions must take into account the number of processors available, the overall system load, and the ability of each application awaiting activation to make use of a given number of processors.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75391},
 doi = {http://doi.acm.org/10.1145/75108.75391},
 acmid = {75391},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nelson:1989:ART:75108.75392,
 author = {Nelson, R. D. and Philips, T. K.},
 title = {An approximation to the response time for shortest queue routing},
 abstract = {In this paper we derive an approximation for the mean response time of a multiple queue system in which shortest queue routing is used. We assume there are \&Kgr; identical queues with infinite capacity and service times that are exponentially distributed. Arrivals of jobs to this system are Poisson and are routed to a queue of minimal length. We develop an approximation which is based on both theoretical and experimental considerations and, for \&Kgr; \&le; 8, has an relative error of less than one half of one percent when compared to simulation. For \&Kgr; = 16, the relative error is still acceptable, being less than 2 percent. An application to a model of parallel processing and a comparison of static and dynamic load balancing schemes are presented.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {181--189},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75392},
 doi = {http://doi.acm.org/10.1145/75108.75392},
 acmid = {75392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nelson:1989:ART:75372.75392,
 author = {Nelson, R. D. and Philips, T. K.},
 title = {An approximation to the response time for shortest queue routing},
 abstract = {In this paper we derive an approximation for the mean response time of a multiple queue system in which shortest queue routing is used. We assume there are \&Kgr; identical queues with infinite capacity and service times that are exponentially distributed. Arrivals of jobs to this system are Poisson and are routed to a queue of minimal length. We develop an approximation which is based on both theoretical and experimental considerations and, for \&Kgr; \&le; 8, has an relative error of less than one half of one percent when compared to simulation. For \&Kgr; = 16, the relative error is still acceptable, being less than 2 percent. An application to a model of parallel processing and a comparison of static and dynamic load balancing schemes are presented.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {181--189},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75392},
 doi = {http://doi.acm.org/10.1145/75372.75392},
 acmid = {75392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Raatikainen:1989:ART:75108.75393,
 author = {Raatikainen, K. E. E.},
 title = {Approximating response time distributions},
 abstract = {The response time is the most visible performance index to users of computer systems. End-users see individual response times, not the average. Therefore the distribution of response times is important in performance evaluation and capacity planning studies. However, the analytic results cannot be obtained in practical cases. A new method is proposed to approximate the response-time distribution. Unlike the previous methods the proposed one takes into account the service-time distributions and routing behaviour. The reported results indicate that the method provides reasonable approximations in many cases.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75108.75393},
 doi = {http://doi.acm.org/10.1145/75108.75393},
 acmid = {75393},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raatikainen:1989:ART:75372.75393,
 author = {Raatikainen, K. E. E.},
 title = {Approximating response time distributions},
 abstract = {The response time is the most visible performance index to users of computer systems. End-users see individual response times, not the average. Therefore the distribution of response times is important in performance evaluation and capacity planning studies. However, the analytic results cannot be obtained in practical cases. A new method is proposed to approximate the response-time distribution. Unlike the previous methods the proposed one takes into account the service-time distributions and routing behaviour. The reported results indicate that the method provides reasonable approximations in many cases.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {190--199},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/75372.75393},
 doi = {http://doi.acm.org/10.1145/75372.75393},
 acmid = {75393},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mitra:1989:CND:75372.75394,
 author = {Mitra, D. and Weiss, A.},
 title = {A closed network with a discriminatory processor-sharing server},
 abstract = {This paper gives a simple, accurate first order asymptotic analysis of the transient and steady state behavior of a network which is closed, not product-form and has multiple classes. One of the two nodes of the network is an infinite server and the discipline in the other node is discriminatory processor-sharing. Specifically, if there are n<subscrpt>j</subscrpt> jobs of class j at the latter node, then each class j job receives a fraction w<subscrpt>j</subscrpt>/(\&Sgr;w<subscrpt>i</subscrpt>n<subscrpt>i</subscrpt>) of the processor capacity. This work has applications to data networks. For the asymptotic regime of high loading of the processor and high processing capacity, we derive the explicit first order transient behavior of the means of queue lengths. We also give explicit expressions for the steady state mean values and a simple procedure for finding the time constants (eigenvalues) that govern the approach to steady state. The results are based on an extension of Kurtz's theorem concerning the fluid limit of Markov processes. Some numerical experiments show that the analysis is quite accurate.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {17},
 issue = {1},
 month = {April},
 year = {1989},
 issn = {0163-5999},
 pages = {200--208},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75372.75394},
 doi = {http://doi.acm.org/10.1145/75372.75394},
 acmid = {75394},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mitra:1989:CND:75108.75394,
 author = {Mitra, D. and Weiss, A.},
 title = {A closed network with a discriminatory processor-sharing server},
 abstract = {This paper gives a simple, accurate first order asymptotic analysis of the transient and steady state behavior of a network which is closed, not product-form and has multiple classes. One of the two nodes of the network is an infinite server and the discipline in the other node is discriminatory processor-sharing. Specifically, if there are n<subscrpt>j</subscrpt> jobs of class j at the latter node, then each class j job receives a fraction w<subscrpt>j</subscrpt>/(\&Sgr;w<subscrpt>i</subscrpt>n<subscrpt>i</subscrpt>) of the processor capacity. This work has applications to data networks. For the asymptotic regime of high loading of the processor and high processing capacity, we derive the explicit first order transient behavior of the means of queue lengths. We also give explicit expressions for the steady state mean values and a simple procedure for finding the time constants (eigenvalues) that govern the approach to steady state. The results are based on an extension of Kurtz's theorem concerning the fluid limit of Markov processes. Some numerical experiments show that the analysis is quite accurate.
},
 booktitle = {Proceedings of the 1989 ACM SIGMETRICS international conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '89},
 year = {1989},
 isbn = {0-89791-315-9},
 location = {Oakland, California, United States},
 pages = {200--208},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/75108.75394},
 doi = {http://doi.acm.org/10.1145/75108.75394},
 acmid = {75394},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gray:1987:VDS:29903.29905,
 author = {Gray, Jim},
 title = {A view of database system performance measures},
 abstract = {Database systems allow quick creation of performance problems. The goal of database systems is to allow the computer-illiterate to write complex and complete applications. It is the job of the system to translate a high-level description of data and procedures into efficient algorithms. The REAL performance metric of a system is how successfully it meets these goals.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {3--4},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/29903.29905},
 doi = {http://doi.acm.org/10.1145/29903.29905},
 acmid = {29905},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gray:1987:VDS:29904.29905,
 author = {Gray, Jim},
 title = {A view of database system performance measures},
 abstract = {Database systems allow quick creation of performance problems. The goal of database systems is to allow the computer-illiterate to write complex and complete applications. It is the job of the system to translate a high-level description of data and procedures into efficient algorithms. The REAL performance metric of a system is how successfully it meets these goals.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {3--4},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/29904.29905},
 doi = {http://doi.acm.org/10.1145/29904.29905},
 acmid = {29905},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Heidelberger:1987:PCM:29904.29906,
 author = {Heidelberger, Philip and Lakshmi, Seetha},
 title = {A performance comparison of multi-micro and mainframe database architectures},
 abstract = {Database machine architectures consisting of multiple microprocessors or mini-computers are attracting wide attention. There have been several proposals and prototypes (see, e.g., DeWitt, Gerber, Graefe, Heytens, Kumar and Muralikrishna (1986), Fishman, Lai and Wilkinson (1984), Hsiao (1983), or the 1983 and 1985 Proceedings of the International Workshop on Database Machines). There is also a commercially available system based on multiple microprocessors (Teradata (1984)). With these architectures it is possible to exploit parallelism at three levels: within a single query, within a single transaction, and by simultaneously executing multiple independent transactions. The rationale behind these multiple microprocessor architectures is primarily to take advantage of the potential lower cost per MIPS (Millions of Instructions per Second, a measure of processing power) of microprocessors as opposed to mainframes. In addition, database machines may offer incremental capacity growth as well as improved performance for large queries by exploiting parallelism within a single query. However, it is not clear if database machines made of multiple microprocessors indeed have any cost/performance advantage over a more conventional mainframe based database management systems. Several papers on the performance analysis of database machines can be found in the literature (e.g., Salza, Terranova and Velardi (1983) or Bit and Hartman (1985)). Most of these studies have focused on determining the execution time of a single query in a particular database machine architecture.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {5--6},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/29904.29906},
 doi = {http://doi.acm.org/10.1145/29904.29906},
 acmid = {29906},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Heidelberger:1987:PCM:29903.29906,
 author = {Heidelberger, Philip and Lakshmi, Seetha},
 title = {A performance comparison of multi-micro and mainframe database architectures},
 abstract = {Database machine architectures consisting of multiple microprocessors or mini-computers are attracting wide attention. There have been several proposals and prototypes (see, e.g., DeWitt, Gerber, Graefe, Heytens, Kumar and Muralikrishna (1986), Fishman, Lai and Wilkinson (1984), Hsiao (1983), or the 1983 and 1985 Proceedings of the International Workshop on Database Machines). There is also a commercially available system based on multiple microprocessors (Teradata (1984)). With these architectures it is possible to exploit parallelism at three levels: within a single query, within a single transaction, and by simultaneously executing multiple independent transactions. The rationale behind these multiple microprocessor architectures is primarily to take advantage of the potential lower cost per MIPS (Millions of Instructions per Second, a measure of processing power) of microprocessors as opposed to mainframes. In addition, database machines may offer incremental capacity growth as well as improved performance for large queries by exploiting parallelism within a single query. However, it is not clear if database machines made of multiple microprocessors indeed have any cost/performance advantage over a more conventional mainframe based database management systems. Several papers on the performance analysis of database machines can be found in the literature (e.g., Salza, Terranova and Velardi (1983) or Bit and Hartman (1985)). Most of these studies have focused on determining the execution time of a single query in a particular database machine architecture.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {5--6},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/29903.29906},
 doi = {http://doi.acm.org/10.1145/29903.29906},
 acmid = {29906},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reed:1987:PRA:29903.29907,
 author = {Reed, Daniel A. and Kim, Chong-kwon},
 title = {Packet routing algorithms for integrated switching networks},
 abstract = {Repeated studies have shown that a single switching technique, either circuit or packet switching, cannot optimally support a heterogeneous traffic mix composed of voice, video and data. Integrated networks support such heterogeneous traffic by combining circuit and packet switching in a single network. To manage the statistical variations of network traffic, we introduce a new, adaptive routing algorithm called hybrid, weighted routing. Simulations show that hybrid, weighted routing is preferable to other adaptive routing techniques for both packet switched networks and integrated networks.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {7--15},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29903.29907},
 doi = {http://doi.acm.org/10.1145/29903.29907},
 acmid = {29907},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reed:1987:PRA:29904.29907,
 author = {Reed, Daniel A. and Kim, Chong-kwon},
 title = {Packet routing algorithms for integrated switching networks},
 abstract = {Repeated studies have shown that a single switching technique, either circuit or packet switching, cannot optimally support a heterogeneous traffic mix composed of voice, video and data. Integrated networks support such heterogeneous traffic by combining circuit and packet switching in a single network. To manage the statistical variations of network traffic, we introduce a new, adaptive routing algorithm called hybrid, weighted routing. Simulations show that hybrid, weighted routing is preferable to other adaptive routing techniques for both packet switched networks and integrated networks.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {7--15},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29904.29907},
 doi = {http://doi.acm.org/10.1145/29904.29907},
 acmid = {29907},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gonsalves:1987:PEV:29904.29908,
 author = {Gonsalves, Timothy A. and Tobagi, Fouad A.},
 title = {Performance of the Expressnet with voice/data traffic},
 abstract = {In the past few years, local area networks have come into widespread use for the interconnection of computers. Together with the trend towards digital transmission in voice telephony, this has spurred interest in integrated voice/data networks. The Expressnet, an implicit-token round-robin scheme using unidirectional busses, achieves high performance even at bandwidths of 100 Mb/s. Other features that make the protocol attractive for voice/data traffic are bounded delays and priorities. The latter is achieved by devoting alternate rounds to one or the other of the two traffic types. By the use of accurate simulation, the performance of the Expressnet with voice/data traffic is characterized. It is shown that the Expressnet satisfies the real-time constraints of voice traffic adequately even at bandwidths of 100 Mb/s. Data traffic is able to effectively utilize bandwidth unused by voice traffic. The trade-offs in the alternating round priority mechanism are quantified. Loss of voice samples under overload is shown to occur regularly in small, frequent clips, subjectively preferable to irregular clips. In a comparison of the Expressnet, the contention-based Ethernet and the round-robin Token Bus protocols, the two round-robin protocols are found to perform better than the Ethernet under heavy load owing to the more deterministic mode of operation. The comparison of the two round-robin protocols highlights the importance of minimizing scheduling overhead at high bandwidths.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {16--26},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29908},
 doi = {http://doi.acm.org/10.1145/29904.29908},
 acmid = {29908},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gonsalves:1987:PEV:29903.29908,
 author = {Gonsalves, Timothy A. and Tobagi, Fouad A.},
 title = {Performance of the Expressnet with voice/data traffic},
 abstract = {In the past few years, local area networks have come into widespread use for the interconnection of computers. Together with the trend towards digital transmission in voice telephony, this has spurred interest in integrated voice/data networks. The Expressnet, an implicit-token round-robin scheme using unidirectional busses, achieves high performance even at bandwidths of 100 Mb/s. Other features that make the protocol attractive for voice/data traffic are bounded delays and priorities. The latter is achieved by devoting alternate rounds to one or the other of the two traffic types. By the use of accurate simulation, the performance of the Expressnet with voice/data traffic is characterized. It is shown that the Expressnet satisfies the real-time constraints of voice traffic adequately even at bandwidths of 100 Mb/s. Data traffic is able to effectively utilize bandwidth unused by voice traffic. The trade-offs in the alternating round priority mechanism are quantified. Loss of voice samples under overload is shown to occur regularly in small, frequent clips, subjectively preferable to irregular clips. In a comparison of the Expressnet, the contention-based Ethernet and the round-robin Token Bus protocols, the two round-robin protocols are found to perform better than the Ethernet under heavy load owing to the more deterministic mode of operation. The comparison of the two round-robin protocols highlights the importance of minimizing scheduling overhead at high bandwidths.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {16--26},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29908},
 doi = {http://doi.acm.org/10.1145/29903.29908},
 acmid = {29908},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1987:ARD:29904.29909,
 author = {Agrawal, Subhash and Ramaswamy, Ravi},
 title = {Analysis of the resequencing delay for M/M/m systems},
 abstract = {Many virtual circuit service communications networks such as SNA employ virtual circuit transmission method inside the subnet. An essential feature of such networks is that the sequence in which messages are transmitted is maintained throughout the route from source node to the destination node. When there are multiple links connecting two intermediate nodes in the route and the messages are of different lengths, then it is possible that the messages complete transmission at the next node out of sequence. These messages then have to be resquenced, i.e. put in the right order, in order to provide a virtual circuit service. The resequencing operation introduces an additional delay in transmission which may be significant. In this paper the probability distribution of the resequencing delay is obtained for the M/M/m system. Simple expressions for the mean and coefficient of variation of the resequencing delay are also provided. It is shown through a variety of numerical examples that the resequencing delay is likely to be a significant component of the overall response time. Some interesting aspects of dependence of the mean resequencing delay on system parameters are studied analytically.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {27--35},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29904.29909},
 doi = {http://doi.acm.org/10.1145/29904.29909},
 acmid = {29909},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agrawal:1987:ARD:29903.29909,
 author = {Agrawal, Subhash and Ramaswamy, Ravi},
 title = {Analysis of the resequencing delay for M/M/m systems},
 abstract = {Many virtual circuit service communications networks such as SNA employ virtual circuit transmission method inside the subnet. An essential feature of such networks is that the sequence in which messages are transmitted is maintained throughout the route from source node to the destination node. When there are multiple links connecting two intermediate nodes in the route and the messages are of different lengths, then it is possible that the messages complete transmission at the next node out of sequence. These messages then have to be resquenced, i.e. put in the right order, in order to provide a virtual circuit service. The resequencing operation introduces an additional delay in transmission which may be significant. In this paper the probability distribution of the resequencing delay is obtained for the M/M/m system. Simple expressions for the mean and coefficient of variation of the resequencing delay are also provided. It is shown through a variety of numerical examples that the resequencing delay is likely to be a significant component of the overall response time. Some interesting aspects of dependence of the mean resequencing delay on system parameters are studied analytically.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {27--35},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29903.29909},
 doi = {http://doi.acm.org/10.1145/29903.29909},
 acmid = {29909},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reed:1987:PDE:29903.29910,
 author = {Reed, Daniel A. and Malony, Allen D. and McCredie, Bradley D.},
 title = {Parallel discrete event simulation: a shared memory approach},
 abstract = {},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {36--38},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/29903.29910},
 doi = {http://doi.acm.org/10.1145/29903.29910},
 acmid = {29910},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reed:1987:PDE:29904.29910,
 author = {Reed, Daniel A. and Malony, Allen D. and McCredie, Bradley D.},
 title = {Parallel discrete event simulation: a shared memory approach},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {36--38},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/29904.29910},
 doi = {http://doi.acm.org/10.1145/29904.29910},
 acmid = {29910},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bucher:1987:CLV:29904.29911,
 author = {Bucher, Ingrid Y. and Simmons, Margaret L.},
 title = {A close look at vector performance of register-to-register vector computers and a new model},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {39--45},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/29904.29911},
 doi = {http://doi.acm.org/10.1145/29904.29911},
 acmid = {29911},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bucher:1987:CLV:29903.29911,
 author = {Bucher, Ingrid Y. and Simmons, Margaret L.},
 title = {A close look at vector performance of register-to-register vector computers and a new model},
 abstract = {},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {39--45},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/29903.29911},
 doi = {http://doi.acm.org/10.1145/29903.29911},
 acmid = {29911},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Darema-Rogers:1987:MAP:29903.29912,
 author = {Darema-Rogers, F. and Pfister, G. F. and So, K.},
 title = {Memory access patterns of parallel scientific programs},
 abstract = {A parallel simulator, PSIMUL, has been used to collect information on the memory access patterns and synchronization overheads of several scientific applications. The parallel simulation method we use is very efficient and it allows us to simulate execution of an entire application program, amounting to hundreds of millions of instructions. We present our measurements on the memory access characteristics of these applications; particularly our observations on shared and private data, their frequency of access and locality. We have found that, even though the shared data comprise the largest portion of the data in the application program, on the average a small fraction of the memory references are to shared data. The low averages do not preclude bursts of traffic to shared memory nor does it rule out positive benefits from caching shared data. We also discuss issues of synchronization overheads and their effect on performance.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {46--58},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/29903.29912},
 doi = {http://doi.acm.org/10.1145/29903.29912},
 acmid = {29912},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Darema-Rogers:1987:MAP:29904.29912,
 author = {Darema-Rogers, F. and Pfister, G. F. and So, K.},
 title = {Memory access patterns of parallel scientific programs},
 abstract = {A parallel simulator, PSIMUL, has been used to collect information on the memory access patterns and synchronization overheads of several scientific applications. The parallel simulation method we use is very efficient and it allows us to simulate execution of an entire application program, amounting to hundreds of millions of instructions. We present our measurements on the memory access characteristics of these applications; particularly our observations on shared and private data, their frequency of access and locality. We have found that, even though the shared data comprise the largest portion of the data in the application program, on the average a small fraction of the memory references are to shared data. The low averages do not preclude bursts of traffic to shared memory nor does it rule out positive benefits from caching shared data. We also discuss issues of synchronization overheads and their effect on performance.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {46--58},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/29904.29912},
 doi = {http://doi.acm.org/10.1145/29904.29912},
 acmid = {29912},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Geist:1987:DSS:29904.29913,
 author = {Geist, Robert and Reynolds, Robert and Pittard, Eve},
 title = {Disk scheduling in System V},
 abstract = {A variety of disk scheduling algorithms, including some newly defined ones, are compared both in simulation and in tests on a real machine running UNIX<supscrpt>*</supscrpt> System V. In the real system tests, first-come first-served (FCFS), shortest seek time first (SSTF), and the standard System V algorithm (SVS) are all seen to yield relatively poor mean waiting time performance when compared to the VSCAN(0.2) algorithm and modifications thereof suggested by Coffman. Nevertheless, each is seen to excel along a particular performance dimension. The adequacy of open, Poisson arrival simulation models in predicting disk scheduling performance is questioned, and an alternative arrival model is suggested which offers improved predictions in the System V environment.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {59--68},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29904.29913},
 doi = {http://doi.acm.org/10.1145/29904.29913},
 acmid = {29913},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Geist:1987:DSS:29903.29913,
 author = {Geist, Robert and Reynolds, Robert and Pittard, Eve},
 title = {Disk scheduling in System V},
 abstract = {A variety of disk scheduling algorithms, including some newly defined ones, are compared both in simulation and in tests on a real machine running UNIX<supscrpt>*</supscrpt> System V. In the real system tests, first-come first-served (FCFS), shortest seek time first (SSTF), and the standard System V algorithm (SVS) are all seen to yield relatively poor mean waiting time performance when compared to the VSCAN(0.2) algorithm and modifications thereof suggested by Coffman. Nevertheless, each is seen to excel along a particular performance dimension. The adequacy of open, Poisson arrival simulation models in predicting disk scheduling performance is questioned, and an alternative arrival model is suggested which offers improved predictions in the System V environment.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {59--68},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29903.29913},
 doi = {http://doi.acm.org/10.1145/29903.29913},
 acmid = {29913},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Livny:1987:MMA:29903.29914,
 author = {Livny, Miron and Khoshafian, Setrag and Boral, Haran},
 title = {Multi-disk management algorithms},
 abstract = {We investigate two schemes for placing data on multiple disks. We show that declustering (spreading each file across several disks) is inherently better than clustering (placing each file on a single disk) due to a number of reasons including parallelism and uniform load on all disks.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {69--77},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29903.29914},
 doi = {http://doi.acm.org/10.1145/29903.29914},
 acmid = {29914},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Livny:1987:MMA:29904.29914,
 author = {Livny, Miron and Khoshafian, Setrag and Boral, Haran},
 title = {Multi-disk management algorithms},
 abstract = {We investigate two schemes for placing data on multiple disks. We show that declustering (spreading each file across several disks) is inherently better than clustering (placing each file on a single disk) due to a number of reasons including parallelism and uniform load on all disks.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {69--77},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29904.29914},
 doi = {http://doi.acm.org/10.1145/29904.29914},
 acmid = {29914},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Buzen:1987:UOT:29903.29915,
 author = {Buzen, Jeffrey P. and Shum, Annie W.},
 title = {A unified operational treatment of RPS reconnect delays},
 abstract = {Expressions are presented for RPS reconnect delays in three basic cases: single path, multiple path with static reconnect, multiple path with dynamic reconnect. The assumption of homogeneous reconnects, which is introduced in the analysis, is shown to be implicit in many prior analyses. This assumption simplifies the resulting equations, but more general equations are also presented for the case where homogeneous reconnects are not assumed. These general results have not appeared previously. This paper also uses the assumption of constrained independence to derive a result for static reconnect which has only been derived previously using the maximum entropy principle. In the case of dynamic reconnect, constrained independence yields an entirely new closed form result. In addition to being a consistent extension of the static reconnect case, this new result is the only closed form expression for dynamic reconnect that yields a correct solution in certain saturated cases. Constrained independence can provide a useful alternative assumption in many other cases where complete independence is known to be only approximately correct.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {78--92},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/29903.29915},
 doi = {http://doi.acm.org/10.1145/29903.29915},
 acmid = {29915},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Buzen:1987:UOT:29904.29915,
 author = {Buzen, Jeffrey P. and Shum, Annie W.},
 title = {A unified operational treatment of RPS reconnect delays},
 abstract = {Expressions are presented for RPS reconnect delays in three basic cases: single path, multiple path with static reconnect, multiple path with dynamic reconnect. The assumption of homogeneous reconnects, which is introduced in the analysis, is shown to be implicit in many prior analyses. This assumption simplifies the resulting equations, but more general equations are also presented for the case where homogeneous reconnects are not assumed. These general results have not appeared previously. This paper also uses the assumption of constrained independence to derive a result for static reconnect which has only been derived previously using the maximum entropy principle. In the case of dynamic reconnect, constrained independence yields an entirely new closed form result. In addition to being a consistent extension of the static reconnect case, this new result is the only closed form expression for dynamic reconnect that yields a correct solution in certain saturated cases. Constrained independence can provide a useful alternative assumption in many other cases where complete independence is known to be only approximately correct.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {78--92},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/29904.29915},
 doi = {http://doi.acm.org/10.1145/29904.29915},
 acmid = {29915},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nelson:1987:PAP:29903.29916,
 author = {Nelson, R. and Towsley, D. and Tantawi, A. N.},
 title = {Performance analysis of parallel processing systems},
 abstract = {A centralized parallel processing system with job splitting is considered. In such a system, jobs wait in a central queue, which is accessible by all the processors, and are split into independent tasks that can be executed on separate processors. This parallel processing system is modeled as a bulk arrival M<supscrpt>X</supscrpt>/M/c queueing system where customers and bulks correspond to tasks and jobs, respectively. Such a system has been studied in [1, 3] and an expression for the mean response time of a random customer is obtained. However, since we are interested in the time that a job spends in the system, including synchronization delay, we must evaluate the bulk response time rather than simply the customer response time. The job response time is the sum of the job waiting time and the job service time. By analyzing the bulk queueing system we obtain an expression for the mean job waiting time. The mean job service time is given by a set of recurrence equations.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {93--94},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/29903.29916},
 doi = {http://doi.acm.org/10.1145/29903.29916},
 acmid = {29916},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nelson:1987:PAP:29904.29916,
 author = {Nelson, R. and Towsley, D. and Tantawi, A. N.},
 title = {Performance analysis of parallel processing systems},
 abstract = {A centralized parallel processing system with job splitting is considered. In such a system, jobs wait in a central queue, which is accessible by all the processors, and are split into independent tasks that can be executed on separate processors. This parallel processing system is modeled as a bulk arrival M<supscrpt>X</supscrpt>/M/c queueing system where customers and bulks correspond to tasks and jobs, respectively. Such a system has been studied in [1, 3] and an expression for the mean response time of a random customer is obtained. However, since we are interested in the time that a job spends in the system, including synchronization delay, we must evaluate the bulk response time rather than simply the customer response time. The job response time is the sum of the job waiting time and the job service time. By analyzing the bulk queueing system we obtain an expression for the mean job waiting time. The mean job service time is given by a set of recurrence equations.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {93--94},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/29904.29916},
 doi = {http://doi.acm.org/10.1145/29904.29916},
 acmid = {29916},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tan:1987:RDR:29904.29917,
 author = {Tan, Ziao-Nan and Sevcik, Kenneth C.},
 title = {Reduced distance routing in single-state shuffle-exchange interconnection networks},
 abstract = {In multiprocessor architectures, it is frequently necessary to provide parallel communication among a potentially large number of processors and memories. Among the many interconnection schemes that have been proposed and analyzed, shuffle-exchange networks have received much attention due to their ability to allow a message to pass from any node to any other node in a number of steps that grows only logarithmically with the number of interconnected nodes (in the absence of contention) while keeping the number of hardware connections per node independent of the number of nodes.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {95--110},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/29904.29917},
 doi = {http://doi.acm.org/10.1145/29904.29917},
 acmid = {29917},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tan:1987:RDR:29903.29917,
 author = {Tan, Ziao-Nan and Sevcik, Kenneth C.},
 title = {Reduced distance routing in single-state shuffle-exchange interconnection networks},
 abstract = {In multiprocessor architectures, it is frequently necessary to provide parallel communication among a potentially large number of processors and memories. Among the many interconnection schemes that have been proposed and analyzed, shuffle-exchange networks have received much attention due to their ability to allow a message to pass from any node to any other node in a number of steps that grows only logarithmically with the number of interconnected nodes (in the absence of contention) while keeping the number of hardware connections per node independent of the number of nodes.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {95--110},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/29903.29917},
 doi = {http://doi.acm.org/10.1145/29903.29917},
 acmid = {29917},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bouras:1987:QDB:29904.29918,
 author = {Bouras, Christos and Garofalakis, John},
 title = {Queueing delays in buffered multistage interconnection networks},
 abstract = {Our work deals with the analysis of the queueing delays of buffered multistage Banyan networks of multiprocessors. We provide tight upper bounds on the mean delays of the second stage and beyond, in the case of infinite buffers. Our results are validated by simulations performed on a network simulator constructed by us. The analytic work for network stages beyond the first, provides a partial answer to open problems posed by previous research.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {111--121},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29918},
 doi = {http://doi.acm.org/10.1145/29904.29918},
 acmid = {29918},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bouras:1987:QDB:29903.29918,
 author = {Bouras, Christos and Garofalakis, John},
 title = {Queueing delays in buffered multistage interconnection networks},
 abstract = {Our work deals with the analysis of the queueing delays of buffered multistage Banyan networks of multiprocessors. We provide tight upper bounds on the mean delays of the second stage and beyond, in the case of infinite buffers. Our results are validated by simulations performed on a network simulator constructed by us. The analytic work for network stages beyond the first, provides a partial answer to open problems posed by previous research.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {111--121},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29918},
 doi = {http://doi.acm.org/10.1145/29903.29918},
 acmid = {29918},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Garcia-Molina:1987:PTM:29903.29919,
 author = {Garcia-Molina, Hector and Rogers, Lawrence R.},
 title = {Performance through memory},
 abstract = {Two of the most important parameters of a computer are its processor speed and physical memory size. We study the relationship between these two parameters by experimentally evaluating the intrinsic memory and processor requirements of various applications. We also explore how hardware prices are changing the cost effectiveness of these two resources. Our results indicate that several important applications are ``memory-bound," i.e., can benefit more from increased memory than from a faster processor.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29903.29919},
 doi = {http://doi.acm.org/10.1145/29903.29919},
 acmid = {29919},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Garcia-Molina:1987:PTM:29904.29919,
 author = {Garcia-Molina, Hector and Rogers, Lawrence R.},
 title = {Performance through memory},
 abstract = {Two of the most important parameters of a computer are its processor speed and physical memory size. We study the relationship between these two parameters by experimentally evaluating the intrinsic memory and processor requirements of various applications. We also explore how hardware prices are changing the cost effectiveness of these two resources. Our results indicate that several important applications are ``memory-bound," i.e., can benefit more from increased memory than from a faster processor.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {122--131},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29904.29919},
 doi = {http://doi.acm.org/10.1145/29904.29919},
 acmid = {29919},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jipping:1987:PPC:29904.29920,
 author = {Jipping, Michael J. and Ford, Ray},
 title = {Predicting performance of concurrency control designs},
 abstract = {Performance is a high-priority consideration when designing concurrent or distributed systems. The process of designing such a system is complicated by two factors: (1) the current state-of-the-art in concurrent system design is very ad hoc \&mdash; software design principles for concurrent systems are still in their infancy, and (2) performance evaluation of concurrent systems is quite difficult and it is especially difficult to relate aspects of the design to aspects of the implementation. This paper reports on work with a performance modeling technique for concurrent or distributed systems that allows structured design to be related to the implementation of the concurrency control component of the system. First, a General Process Model (GPM) is used to organize system design information into a six level hierarchy. The abstract performance properties of each level in the hierarchy have been established using concurrency control theory. Next, we describe how to translate the structured system design into efficient concurrency control techniques, using elements of this theory. Finally, a prototype automated design evaluation tool which serves as a central component of the design methodology is described.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {132--142},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29920},
 doi = {http://doi.acm.org/10.1145/29904.29920},
 acmid = {29920},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jipping:1987:PPC:29903.29920,
 author = {Jipping, Michael J. and Ford, Ray},
 title = {Predicting performance of concurrency control designs},
 abstract = {Performance is a high-priority consideration when designing concurrent or distributed systems. The process of designing such a system is complicated by two factors: (1) the current state-of-the-art in concurrent system design is very ad hoc \&mdash; software design principles for concurrent systems are still in their infancy, and (2) performance evaluation of concurrent systems is quite difficult and it is especially difficult to relate aspects of the design to aspects of the implementation. This paper reports on work with a performance modeling technique for concurrent or distributed systems that allows structured design to be related to the implementation of the concurrency control component of the system. First, a General Process Model (GPM) is used to organize system design information into a six level hierarchy. The abstract performance properties of each level in the hierarchy have been established using concurrency control theory. Next, we describe how to translate the structured system design into efficient concurrency control techniques, using elements of this theory. Finally, a prototype automated design evaluation tool which serves as a central component of the design methodology is described.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {132--142},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29920},
 doi = {http://doi.acm.org/10.1145/29903.29920},
 acmid = {29920},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dahbura:1987:PAF:29903.29921,
 author = {Dahbura, Anton T. and Sabnani, Krishan K. and Hery, William J.},
 title = {Performance analysis of a fault detection scheme in multiprocessor systems},
 abstract = {A technique is described for detecting and diagnosing faults at the processor level in a multiprocessor system. In this method, a process is assigned whenever possible to two processors: the processor that it would normally be assigned to (primary) and an additional processor which would otherwise be idle (secondary). Two strategies will be described and analyzed: one which is preemptive and another which is non-preemptive. It is shown that for moderately loaded systems, a sufficient percentage of processes can be performed redundantly using the system's spare capacity to provide a basis for fault detection and diagnosis with virtually no degradation of response time.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/29903.29921},
 doi = {http://doi.acm.org/10.1145/29903.29921},
 acmid = {29921},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dahbura:1987:PAF:29904.29921,
 author = {Dahbura, Anton T. and Sabnani, Krishan K. and Hery, William J.},
 title = {Performance analysis of a fault detection scheme in multiprocessor systems},
 abstract = {A technique is described for detecting and diagnosing faults at the processor level in a multiprocessor system. In this method, a process is assigned whenever possible to two processors: the processor that it would normally be assigned to (primary) and an additional processor which would otherwise be idle (secondary). Two strategies will be described and analyzed: one which is preemptive and another which is non-preemptive. It is shown that for moderately loaded systems, a sufficient percentage of processes can be performed redundantly using the system's spare capacity to provide a basis for fault detection and diagnosis with virtually no degradation of response time.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {143--154},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/29904.29921},
 doi = {http://doi.acm.org/10.1145/29904.29921},
 acmid = {29921},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Salsburg:1987:SAC:29904.29922,
 author = {Salsburg, Michael A.},
 title = {A statistical approach to computer performance modeling},
 abstract = {Models of discrete systems are often utilized to assist in computer engineering and procurement. The tools for modeling have been traditionally developed using either analytic methods or discrete event simulation. The research presented here explores the use of statistical techniques to augment and assist this basic set of tools.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {155--162},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/29904.29922},
 doi = {http://doi.acm.org/10.1145/29904.29922},
 acmid = {29922},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Salsburg:1987:SAC:29903.29922,
 author = {Salsburg, Michael A.},
 title = {A statistical approach to computer performance modeling},
 abstract = {Models of discrete systems are often utilized to assist in computer engineering and procurement. The tools for modeling have been traditionally developed using either analytic methods or discrete event simulation. The research presented here explores the use of statistical techniques to augment and assist this basic set of tools.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {155--162},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/29903.29922},
 doi = {http://doi.acm.org/10.1145/29903.29922},
 acmid = {29922},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kerola:1987:MPM:29904.29923,
 author = {Kerola, Teemu and Schwetman, Herb},
 title = {Monit: a performance monitoring tool for parallel and pseudo-parallel programs},
 abstract = {This paper describes a performance monitoring system, Monit, developed for performance evaluation of parallel systems. Monit uses trace files that are generated during the execution of parallel programs. Monit analyzes these trace files and produces time-oriented graphs of resource usage and system queues. Users interactively select the displayed items, resolution, and time intervals of interest. The current implementation of Monit is for SUN-3 workstation, but the program is easily adaptable to other devices.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {163--174},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/29904.29923},
 doi = {http://doi.acm.org/10.1145/29904.29923},
 acmid = {29923},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kerola:1987:MPM:29903.29923,
 author = {Kerola, Teemu and Schwetman, Herb},
 title = {Monit: a performance monitoring tool for parallel and pseudo-parallel programs},
 abstract = {This paper describes a performance monitoring system, Monit, developed for performance evaluation of parallel systems. Monit uses trace files that are generated during the execution of parallel programs. Monit analyzes these trace files and produces time-oriented graphs of resource usage and system queues. Users interactively select the displayed items, resolution, and time intervals of interest. The current implementation of Monit is for SUN-3 workstation, but the program is easily adaptable to other devices.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {163--174},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/29903.29923},
 doi = {http://doi.acm.org/10.1145/29903.29923},
 acmid = {29923},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marsan:1987:MSA:29903.29924,
 author = {Marsan, M. Ajmone and Balbo, G. and Chiola, G. and Conte, G.},
 title = {Modeling the software architecture of a prototype parallel machine},
 abstract = {A high-level Petri net model of the software architecture of an experimental MIMD multiprocessor system for Artificial Intelligence applications is derived by direct translation of the code corresponding to the assumed workload. Hardware architectural constraints are then easily added, and formal reduction rules are used to simplify the model, which is then further approximated to obtain a performance model of the system based on generalized stochastic Petri nets. From the latter model it is possible to estimate the optimal multiprogramming level of each processor so as to achieve the maximum performance in terms of overall throughput (number of tasks completed per unit time).
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {175--185},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29924},
 doi = {http://doi.acm.org/10.1145/29903.29924},
 acmid = {29924},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marsan:1987:MSA:29904.29924,
 author = {Marsan, M. Ajmone and Balbo, G. and Chiola, G. and Conte, G.},
 title = {Modeling the software architecture of a prototype parallel machine},
 abstract = {A high-level Petri net model of the software architecture of an experimental MIMD multiprocessor system for Artificial Intelligence applications is derived by direct translation of the code corresponding to the assumed workload. Hardware architectural constraints are then easily added, and formal reduction rules are used to simplify the model, which is then further approximated to obtain a performance model of the system based on generalized stochastic Petri nets. From the latter model it is possible to estimate the optimal multiprogramming level of each processor so as to achieve the maximum performance in terms of overall throughput (number of tasks completed per unit time).
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {175--185},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29924},
 doi = {http://doi.acm.org/10.1145/29904.29924},
 acmid = {29924},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Alexander:1987:WCP:29903.29925,
 author = {Alexander, William and Keller, Tom W. and Boughter, Ellen E.},
 title = {A workload characterization pipeline for models of parallel systems},
 abstract = {The same application implemented on different systems will necessarily present different workloads to the systems. Characterizations of workloads intended to represent the same application, but input to models of different systems, must also differ in analogous ways. We present a hierarchical method for characterizing a workload at increasing levels of detail such that every characterization at a lower level still accurately represents the workload at higher levels.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {186--194},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29903.29925},
 doi = {http://doi.acm.org/10.1145/29903.29925},
 acmid = {29925},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Alexander:1987:WCP:29904.29925,
 author = {Alexander, William and Keller, Tom W. and Boughter, Ellen E.},
 title = {A workload characterization pipeline for models of parallel systems},
 abstract = {The same application implemented on different systems will necessarily present different workloads to the systems. Characterizations of workloads intended to represent the same application, but input to models of different systems, must also differ in analogous ways. We present a hierarchical method for characterizing a workload at increasing levels of detail such that every characterization at a lower level still accurately represents the workload at higher levels.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {186--194},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/29904.29925},
 doi = {http://doi.acm.org/10.1145/29904.29925},
 acmid = {29925},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Graf:1987:TDL:29903.29926,
 author = {Graf, Ingrid M.},
 title = {Transformation between different levels of workload characterization for capacity planning: fundamentals and case study},
 abstract = {Queueing network models are effective tools for capacity planning of computer systems. The base of all performance oriented questions is the characterization of the computer system workload. At the capacity planning level the workload is described in user-oriented terms. At the system level the queueing network model requires input parameters, which differ from the workload description at the capacity planning level. In this paper a general procedure to transform the parameters between these two levels is presented and applied to a case study. The effect on system performance of an increase in the use of an existing application system is analysed.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {195--204},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29903.29926},
 doi = {http://doi.acm.org/10.1145/29903.29926},
 acmid = {29926},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Graf:1987:TDL:29904.29926,
 author = {Graf, Ingrid M.},
 title = {Transformation between different levels of workload characterization for capacity planning: fundamentals and case study},
 abstract = {Queueing network models are effective tools for capacity planning of computer systems. The base of all performance oriented questions is the characterization of the computer system workload. At the capacity planning level the workload is described in user-oriented terms. At the system level the queueing network model requires input parameters, which differ from the workload description at the capacity planning level. In this paper a general procedure to transform the parameters between these two levels is presented and applied to a case study. The effect on system performance of an increase in the use of an existing application system is analysed.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {195--204},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29904.29926},
 doi = {http://doi.acm.org/10.1145/29904.29926},
 acmid = {29926},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ruan:1987:PAF:29903.29927,
 author = {Ruan, Zuwang and Tichy, Walter F.},
 title = {Performance analysis of file replication schemes in distributed systems},
 abstract = {In distributed systems the efficiency of the network file system is a key performance issue. Replication of files and directories can enhance file system efficiency, but the choice of replication techniques is crucial. This paper studies a number of replication techniques, including remote access, prereplication, weighted voting, and two demand replication schemes: polling and staling. It develops a Markov chain model, which is capable of characterizing properties of file access sequences, including access locality and access bias. The paper compares the replication techniques under three different network file system architectures. The results show that, under reasonable assumptions, demand replication requires fewer file transfers than remote access, especially for files that have a high degree of access locality. Among the demand replication schemes, staling requires fewer auxiliary messages than polling.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {205--215},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29927},
 doi = {http://doi.acm.org/10.1145/29903.29927},
 acmid = {29927},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ruan:1987:PAF:29904.29927,
 author = {Ruan, Zuwang and Tichy, Walter F.},
 title = {Performance analysis of file replication schemes in distributed systems},
 abstract = {In distributed systems the efficiency of the network file system is a key performance issue. Replication of files and directories can enhance file system efficiency, but the choice of replication techniques is crucial. This paper studies a number of replication techniques, including remote access, prereplication, weighted voting, and two demand replication schemes: polling and staling. It develops a Markov chain model, which is capable of characterizing properties of file access sequences, including access locality and access bias. The paper compares the replication techniques under three different network file system architectures. The results show that, under reasonable assumptions, demand replication requires fewer file transfers than remote access, especially for files that have a high degree of access locality. Among the demand replication schemes, staling requires fewer auxiliary messages than polling.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {205--215},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29927},
 doi = {http://doi.acm.org/10.1145/29904.29927},
 acmid = {29927},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cheriton:1987:NMV:29904.29928,
 author = {Cheriton, david R. and Williamson, Carey L.},
 title = {Network measurement of the VMTP request-response protocol in the V distributed system},
 abstract = {Communication systems are undergoing a change in use from stream to request-response or transaction communication. In addition, communication systems are becoming increasingly based on high-speed, low delay, low error rate channels. These changes call for a new generation of networks, network interfaces, and transport protocol design. The performance characteristics of request-response protocols on these high-performance networks should guide the design of this new generation, yet relatively little data of this nature is available.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {216--225},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29904.29928},
 doi = {http://doi.acm.org/10.1145/29904.29928},
 acmid = {29928},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cheriton:1987:NMV:29903.29928,
 author = {Cheriton, david R. and Williamson, Carey L.},
 title = {Network measurement of the VMTP request-response protocol in the V distributed system},
 abstract = {Communication systems are undergoing a change in use from stream to request-response or transaction communication. In addition, communication systems are becoming increasingly based on high-speed, low delay, low error rate channels. These changes call for a new generation of networks, network interfaces, and transport protocol design. The performance characteristics of request-response protocols on these high-performance networks should guide the design of this new generation, yet relatively little data of this nature is available.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {216--225},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/29903.29928},
 doi = {http://doi.acm.org/10.1145/29903.29928},
 acmid = {29928},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Salehmohamed:1987:PEL:29903.29929,
 author = {Salehmohamed, Mohamed and Luk, W. S. and Peters, Joseph G.},
 title = {Performance evaluation of LAN sorting algorithms},
 abstract = {We adapt several parallel sorting algorithms (block sorting algorithms) and distributed sorting algorithms for implementation on an Ethernet network with diskless Sun workstations. We argue that the performance of sorting algorithms on local area networks (LANs) should be analyzed in a manner that is different from the ways that parallel and distributed sorting algorithms are usually analyzed. Consequently, we propose an empirical approach which will provide more insight into the performance of the algorithms. We obtain data on communication time, local processing time, and response time (i.e. total running time) of each algorithm for various file sizes and different numbers of processors. Comparing the performance data with our theoretical analysis, we attempt to provide rationale for the behaviour of the algorithms and project the future behaviour of the algorithms as file size, number of processors, or interprocessor communication facilities change.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {226--233},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/29903.29929},
 doi = {http://doi.acm.org/10.1145/29903.29929},
 acmid = {29929},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Salehmohamed:1987:PEL:29904.29929,
 author = {Salehmohamed, Mohamed and Luk, W. S. and Peters, Joseph G.},
 title = {Performance evaluation of LAN sorting algorithms},
 abstract = {We adapt several parallel sorting algorithms (block sorting algorithms) and distributed sorting algorithms for implementation on an Ethernet network with diskless Sun workstations. We argue that the performance of sorting algorithms on local area networks (LANs) should be analyzed in a manner that is different from the ways that parallel and distributed sorting algorithms are usually analyzed. Consequently, we propose an empirical approach which will provide more insight into the performance of the algorithms. We obtain data on communication time, local processing time, and response time (i.e. total running time) of each algorithm for various file sizes and different numbers of processors. Comparing the performance data with our theoretical analysis, we attempt to provide rationale for the behaviour of the algorithms and project the future behaviour of the algorithms as file size, number of processors, or interprocessor communication facilities change.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {226--233},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/29904.29929},
 doi = {http://doi.acm.org/10.1145/29904.29929},
 acmid = {29929},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Polyzos:1987:DAW:29904.29930,
 author = {Polyzos, George C. and Molle, Mart L.},
 title = {Delay analysis of a window tree conflict resolution algorithm in a local area network environment},
 abstract = {Expressions are found for the throughput and delay performance of a Tree Conflict Resolution Algorithm that is used in a Local Area Network with carrier sensing (and possibly also collision detection). We assume that Massey's constant size window algorithm is used to control access to the channel, and that the resulting conflicts (if any) are resolved using a Capetanakis-like preorder traversal tree algorithm with d-ary splitting. We develop and solve functional equations for various performance metrics of the system and apply the ``Moving Server" technique to calculate the main component of the delay. Our results compare very favorably with those for CSMA protocols, which are commonly used in Local Area Networks that support sensing.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {234--244},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29930},
 doi = {http://doi.acm.org/10.1145/29904.29930},
 acmid = {29930},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Polyzos:1987:DAW:29903.29930,
 author = {Polyzos, George C. and Molle, Mart L.},
 title = {Delay analysis of a window tree conflict resolution algorithm in a local area network environment},
 abstract = {Expressions are found for the throughput and delay performance of a Tree Conflict Resolution Algorithm that is used in a Local Area Network with carrier sensing (and possibly also collision detection). We assume that Massey's constant size window algorithm is used to control access to the channel, and that the resulting conflicts (if any) are resolved using a Capetanakis-like preorder traversal tree algorithm with d-ary splitting. We develop and solve functional equations for various performance metrics of the system and apply the ``Moving Server" technique to calculate the main component of the delay. Our results compare very favorably with those for CSMA protocols, which are commonly used in Local Area Networks that support sensing.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {234--244},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29930},
 doi = {http://doi.acm.org/10.1145/29903.29930},
 acmid = {29930},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shenker:1987:CBA:29904.29931,
 author = {Shenker, Scott},
 title = {Some conjectures on the behavior of acknowledgement-based transmission control of random access communication channels},
 abstract = {A class of acknowledgment-based transmission control algorithms is considered. In the finite population case, we claim that algorithms based on backoff functions which increase faster than linearly but slower than exponentially are stable up to full channel capacity, whereas sublinear, exponential, and superexponential algorithms are not. In addition, comments are made about the nature of the quasistationary behavior in the infinite population case, and about how systems interpolate between the finite and infinite number of station cases. The treatment presented here is nonrigorous, consisting of approximate analytic arguments confirmed by detailed numerical simulations.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {245--255},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29904.29931},
 doi = {http://doi.acm.org/10.1145/29904.29931},
 acmid = {29931},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shenker:1987:CBA:29903.29931,
 author = {Shenker, Scott},
 title = {Some conjectures on the behavior of acknowledgement-based transmission control of random access communication channels},
 abstract = {A class of acknowledgment-based transmission control algorithms is considered. In the finite population case, we claim that algorithms based on backoff functions which increase faster than linearly but slower than exponentially are stable up to full channel capacity, whereas sublinear, exponential, and superexponential algorithms are not. In addition, comments are made about the nature of the quasistationary behavior in the infinite population case, and about how systems interpolate between the finite and infinite number of station cases. The treatment presented here is nonrigorous, consisting of approximate analytic arguments confirmed by detailed numerical simulations.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {245--255},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/29903.29931},
 doi = {http://doi.acm.org/10.1145/29903.29931},
 acmid = {29931},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mathys:1987:ECP:29903.29932,
 author = {Mathys, Peter and Faltings, Boi V.},
 title = {The effect of channel-exit protocols on the performance of finite population radom-access systems},
 abstract = {Random-access systems (RAS) for collision-type channels have been studied extensively under the assumption of an infinite population which generates a Poisson arrival process. If the population is finite and if the (practically desirable) free-access channel-access protocol is used, then it is shown that the specification of a channel-exit protocol is crucial for the stability and the fairness of the RAS. Free-exit and blocked-exit protocols are analyzed and it is concluded that the p-persistent blocked-exit protocol provides the mechanisms to assure stability and fairness for a wide range of arrival process models.
},
 booktitle = {Proceedings of the 1987 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '87},
 year = {1987},
 isbn = {0-89791-225-X},
 location = {Banff, Alberta, Canada},
 pages = {256--267},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/29903.29932},
 doi = {http://doi.acm.org/10.1145/29903.29932},
 acmid = {29932},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mathys:1987:ECP:29904.29932,
 author = {Mathys, Peter and Faltings, Boi V.},
 title = {The effect of channel-exit protocols on the performance of finite population radom-access systems},
 abstract = {Random-access systems (RAS) for collision-type channels have been studied extensively under the assumption of an infinite population which generates a Poisson arrival process. If the population is finite and if the (practically desirable) free-access channel-access protocol is used, then it is shown that the specification of a channel-exit protocol is crucial for the stability and the fairness of the RAS. Free-exit and blocked-exit protocols are analyzed and it is concluded that the p-persistent blocked-exit protocol provides the mechanisms to assure stability and fairness for a wide range of arrival process models.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {15},
 issue = {1},
 month = {May},
 year = {1987},
 issn = {0163-5999},
 pages = {256--267},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/29904.29932},
 doi = {http://doi.acm.org/10.1145/29904.29932},
 acmid = {29932},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Covington:1988:RPP:1007771.55596,
 author = {Covington, R. C. and Madala, S. and Mehta, V. and Jump, J. R. and Sinclair, J. B.},
 title = {The rice parallel processing testbed},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {4--11},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1007771.55596},
 doi = {http://doi.acm.org/10.1145/1007771.55596},
 acmid = {55596},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Covington:1988:RPP:55595.55596,
 author = {Covington, R. C. and Madala, S. and Mehta, V. and Jump, J. R. and Sinclair, J. B.},
 title = {The rice parallel processing testbed},
 abstract = {},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {4--11},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/55595.55596},
 doi = {http://doi.acm.org/10.1145/55595.55596},
 acmid = {55596},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lubachevsky:1988:EDE:1007771.55597,
 author = {Lubachevsky, B. D.},
 title = {Efficient distributed event driven simulations of multiple-loop networks},
 abstract = {Simulating asynchronous multiple-loop networks is commonly considered a difficult task for parallel programming. This paper presents two examples of asynchronous multiple-loop networks: a stylized queuing system and an Ising model. The network topology in both cases is an nX n grid on a torus. A new distributed simulation algorithm is demonstrated on these two examples. The algorithm combines three elements: 1) the bounded lag restriction, 2) precomputed minimal propagation delays, and 3) the so-called opaque periods. Theoretical performance evaluation suggests that if N processing elements (PEs) execute the algorithm in parallel and the simulated system exhibits sufficient density of events, then, in average, processing one event would require \&Ogr;(logN) instructions of one PE. In practice, the algorithm has achieved substantial speed-ups: the speed-up is greater than 16 using 25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2<supscrpt>14</supscrpt> PEs on a SIMD computer.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {12--24},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1007771.55597},
 doi = {http://doi.acm.org/10.1145/1007771.55597},
 acmid = {55597},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lubachevsky:1988:EDE:55595.55597,
 author = {Lubachevsky, B. D.},
 title = {Efficient distributed event driven simulations of multiple-loop networks},
 abstract = {Simulating asynchronous multiple-loop networks is commonly considered a difficult task for parallel programming. This paper presents two examples of asynchronous multiple-loop networks: a stylized queuing system and an Ising model. The network topology in both cases is an nX n grid on a torus. A new distributed simulation algorithm is demonstrated on these two examples. The algorithm combines three elements: 1) the bounded lag restriction, 2) precomputed minimal propagation delays, and 3) the so-called opaque periods. Theoretical performance evaluation suggests that if N processing elements (PEs) execute the algorithm in parallel and the simulated system exhibits sufficient density of events, then, in average, processing one event would require \&Ogr;(logN) instructions of one PE. In practice, the algorithm has achieved substantial speed-ups: the speed-up is greater than 16 using 25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2<supscrpt>14</supscrpt> PEs on a SIMD computer.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {12--24},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/55595.55597},
 doi = {http://doi.acm.org/10.1145/55595.55597},
 acmid = {55597},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lucier:1988:PEM:55595.55598,
 author = {Lucier, B. J.},
 title = {Performance evaluation for multiprocessors programmed using monitors},
 abstract = {We present a classification of synchronization delays inherent in multiprocessor systems programmed using the monitor paradigm. This characterization is useful in relating performance of such systems to algorithmic parameters in subproblems such as domain decomposition. We apply this approach to a parallel, adaptive grid code for solving the equations of one-dimensional gas dynamics implemented on shared memory multiprocessors such as the Encore Multimax.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {22--29},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/55595.55598},
 doi = {http://doi.acm.org/10.1145/55595.55598},
 acmid = {55598},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lucier:1988:PEM:1007771.55598,
 author = {Lucier, B. J.},
 title = {Performance evaluation for multiprocessors programmed using monitors},
 abstract = {We present a classification of synchronization delays inherent in multiprocessor systems programmed using the monitor paradigm. This characterization is useful in relating performance of such systems to algorithmic parameters in subproblems such as domain decomposition. We apply this approach to a parallel, adaptive grid code for solving the equations of one-dimensional gas dynamics implemented on shared memory multiprocessors such as the Encore Multimax.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {22--29},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1007771.55598},
 doi = {http://doi.acm.org/10.1145/1007771.55598},
 acmid = {55598},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ganz:1988:QAF:1007771.55599,
 author = {Ganz, A. and Chlamtac, I.},
 title = {Queueing analysis of finite buffer token networks},
 abstract = {This paper introduces analytic models for evaluating demand assignment protocols in realistic finite buffer/finite station network configurations. We present a solution for implicit and explicit token passing systems enabling us to model local area networks, such as Token Bus. We provide, for the first time, a tractable approximate solution by using an approach based on restricted occupancy urn models. The presented approximation involves the solving of linear equations whose number is linear and equal only to the number of buffers in the system. It is demonstrated that in addition to its simplicity, the presented approximation is also highly accurate.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {30--36},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1007771.55599},
 doi = {http://doi.acm.org/10.1145/1007771.55599},
 acmid = {55599},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ganz:1988:QAF:55595.55599,
 author = {Ganz, A. and Chlamtac, I.},
 title = {Queueing analysis of finite buffer token networks},
 abstract = {This paper introduces analytic models for evaluating demand assignment protocols in realistic finite buffer/finite station network configurations. We present a solution for implicit and explicit token passing systems enabling us to model local area networks, such as Token Bus. We provide, for the first time, a tractable approximate solution by using an approach based on restricted occupancy urn models. The presented approximation involves the solving of linear equations whose number is linear and equal only to the number of buffers in the system. It is demonstrated that in addition to its simplicity, the presented approximation is also highly accurate.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {30--36},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/55595.55599},
 doi = {http://doi.acm.org/10.1145/55595.55599},
 acmid = {55599},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zafirovic-Vukotic:1988:PMH:1007771.55600,
 author = {Zafirovic-Vukotic, M. and Niemegeers, I. G. M. M.},
 title = {Performance modelling of a HSLAN slotted ring protocol},
 abstract = {The slotted ring protocol which is evaluated in this paper is suitable for use at very large transmission rates. In terms of modelling it is a multiple cyclic server system. A few approximative analytical models of this protocol are presented and evaluated vs the simulation in this paper. The cyclic server model shows to be the most accurate and usable over a wide range of parameters. A performance analysis based on this model is presented.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55600},
 doi = {http://doi.acm.org/10.1145/1007771.55600},
 acmid = {55600},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zafirovic-Vukotic:1988:PMH:55595.55600,
 author = {Zafirovic-Vukotic, M. and Niemegeers, I. G. M. M.},
 title = {Performance modelling of a HSLAN slotted ring protocol},
 abstract = {The slotted ring protocol which is evaluated in this paper is suitable for use at very large transmission rates. In terms of modelling it is a multiple cyclic server system. A few approximative analytical models of this protocol are presented and evaluated vs the simulation in this paper. The cyclic server model shows to be the most accurate and usable over a wide range of parameters. A performance analysis based on this model is presented.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {37--46},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55600},
 doi = {http://doi.acm.org/10.1145/55595.55600},
 acmid = {55600},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chiu:1988:CSD:55595.55602,
 author = {Chiu, D.-M. and Sudama, R.},
 title = {A case study of DECnet applications and protocol performance},
 abstract = {This paper is a study based on measurements of network activities of a major site of Digital's world-wide corporate network. The study yields two kinds of results: (1) DECnet protocol performance information and (2) DECnet session statistics. Protocol performance is measured in terms of the various network overhead (non-data) packets in routing, transport and session layers. From these protocol performance data, we are able to review how effective various network protocol optimizations are; for example the on/off flow control scheme and the delayed acknowledgement scheme in the transport protocol. DECnet session statistics characterizes the workload in such a large network. The attributes of a session include the user who started it, the application invoked, the distance between the user and the application, the time span, the number of packets and bytes in each direction, and the various reasons if a session is not successfully established. Based on a large sample of such sessions, we generate distributions based on various attributes of sessions; for example the application mix, the visit count distribution and various packet number and size distributions.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {47--55},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/55595.55602},
 doi = {http://doi.acm.org/10.1145/55595.55602},
 acmid = {55602},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chiu:1988:CSD:1007771.55602,
 author = {Chiu, D.-M. and Sudama, R.},
 title = {A case study of DECnet applications and protocol performance},
 abstract = {This paper is a study based on measurements of network activities of a major site of Digital's world-wide corporate network. The study yields two kinds of results: (1) DECnet protocol performance information and (2) DECnet session statistics. Protocol performance is measured in terms of the various network overhead (non-data) packets in routing, transport and session layers. From these protocol performance data, we are able to review how effective various network protocol optimizations are; for example the on/off flow control scheme and the delayed acknowledgement scheme in the transport protocol. DECnet session statistics characterizes the workload in such a large network. The attributes of a session include the user who started it, the application invoked, the distance between the user and the application, the time span, the number of packets and bytes in each direction, and the various reasons if a session is not successfully established. Based on a large sample of such sessions, we generate distributions based on various attributes of sessions; for example the application mix, the visit count distribution and various packet number and size distributions.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {47--55},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007771.55602},
 doi = {http://doi.acm.org/10.1145/1007771.55602},
 acmid = {55602},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Shenker:1988:SAL:1007771.55603,
 author = {Shenker, S. and Weinrib, A.},
 title = {A symptotic analysis of large heterogeneous queueing systems},
 abstract = {As a simple example of a large heterogeneous queueing system, we consider a single queue with many servers with differing service rates. In the limit of infinitely many servers, we identify a queue control policy that minimizes the average system delay. When there are only two possible server speeds, we can analyze the convergence of this policy to optimality. Based on this result, we propose policies for large but finite systems with a general distribution of server speeds.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {56--62},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1007771.55603},
 doi = {http://doi.acm.org/10.1145/1007771.55603},
 acmid = {55603},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Shenker:1988:SAL:55595.55603,
 author = {Shenker, S. and Weinrib, A.},
 title = {A symptotic analysis of large heterogeneous queueing systems},
 abstract = {As a simple example of a large heterogeneous queueing system, we consider a single queue with many servers with differing service rates. In the limit of infinitely many servers, we identify a queue control policy that minimizes the average system delay. When there are only two possible server speeds, we can analyze the convergence of this policy to optimality. Based on this result, we propose policies for large but finite systems with a general distribution of server speeds.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {56--62},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/55595.55603},
 doi = {http://doi.acm.org/10.1145/55595.55603},
 acmid = {55603},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eager:1988:LPB:1007771.55604,
 author = {Eager, D. L. and Lazowska, E. D. and Zahorjan, J.},
 title = {The limited performance benefits of migrating active processes for load sharing},
 abstract = {Load sharing in a distributed system is the process of transparently sharing workload among the nodes in the system to achieve improved performance. In non-migratory load sharing, jobs may not be transferred once they have commenced execution. In load sharing with migration, on the other hand, jobs in execution may be interrupted, moved to other nodes, and then resumed.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {63--72},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55604},
 doi = {http://doi.acm.org/10.1145/1007771.55604},
 acmid = {55604},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eager:1988:LPB:55595.55604,
 author = {Eager, D. L. and Lazowska, E. D. and Zahorjan, J.},
 title = {The limited performance benefits of migrating active processes for load sharing},
 abstract = {Load sharing in a distributed system is the process of transparently sharing workload among the nodes in the system to achieve improved performance. In non-migratory load sharing, jobs may not be transferred once they have commenced execution. In load sharing with migration, on the other hand, jobs in execution may be interrupted, moved to other nodes, and then resumed.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {63--72},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55604},
 doi = {http://doi.acm.org/10.1145/55595.55604},
 acmid = {55604},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hong:1988:LGA:1007771.55605,
 author = {Hong, J. and Tan, X. and Chen, M.},
 title = {From local to global: an analysis of nearest neighbor balancing on hypercube},
 abstract = {This paper will focus on the issue of load balancing on a hypercube network of N processors. We will investigate a typical nearest neighbor balancing strategy - in which workloads among neighboring processors are averaged at discrete time steps. The computation model allows tasks, described by independent random variables, to be generated and terminated at all times.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {73--82},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55605},
 doi = {http://doi.acm.org/10.1145/1007771.55605},
 acmid = {55605},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hong:1988:LGA:55595.55605,
 author = {Hong, J. and Tan, X. and Chen, M.},
 title = {From local to global: an analysis of nearest neighbor balancing on hypercube},
 abstract = {This paper will focus on the issue of load balancing on a hypercube network of N processors. We will investigate a typical nearest neighbor balancing strategy - in which workloads among neighboring processors are averaged at discrete time steps. The computation model allows tasks, described by independent random variables, to be generated and terminated at all times.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {73--82},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55605},
 doi = {http://doi.acm.org/10.1145/55595.55605},
 acmid = {55605},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kant:1988:ALM:1007771.55606,
 author = {Kant, K.},
 title = {Application level modeling of parallel machines},
 abstract = {In this paper, we consider the application level performance modeling of parallel machines consisting of a large number of processing elements (PE's) connected in some regular structure such as mesh, tree, hypercube, etc. There are K problem types, each arriving according to a Poisson process, and each of which needs a PE substructure of some given size and topology. Thus several problems can run on the machine simultaneously. It is desired to characterize the performance of such a system under various types of allocation schemes.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {83--93},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007771.55606},
 doi = {http://doi.acm.org/10.1145/1007771.55606},
 acmid = {55606},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kant:1988:ALM:55595.55606,
 author = {Kant, K.},
 title = {Application level modeling of parallel machines},
 abstract = {In this paper, we consider the application level performance modeling of parallel machines consisting of a large number of processing elements (PE's) connected in some regular structure such as mesh, tree, hypercube, etc. There are K problem types, each arriving according to a Poisson process, and each of which needs a PE substructure of some given size and topology. Thus several problems can run on the machine simultaneously. It is desired to characterize the performance of such a system under various types of allocation schemes.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {83--93},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/55595.55606},
 doi = {http://doi.acm.org/10.1145/55595.55606},
 acmid = {55606},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Born:1988:ADP:1007771.55607,
 author = {Born, R. G. and Kenevan, J. R.},
 title = {Analytic derivation of processor potential utilization in straight line, ring, square mesh, and hypercube networks},
 abstract = {In multicomputer architectures, in which processors communicate through message-passing, the overhead encountered because of the need to relay messages can significantly affect performance. Based upon some simplifying assumptions including the rate at which a processor generates messages being proportional to its current potential utilization, processor utilizations are analytically derived in matrix form for a bidirectional straight line and square mesh. In addition, closed form derivations are provided for a unidirectional ring and an n-dimensional hypercube. Finally, the theoretical results are found to be in close agreement with discrete-event simulations of the four architectures.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {94--103},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55607},
 doi = {http://doi.acm.org/10.1145/1007771.55607},
 acmid = {55607},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Born:1988:ADP:55595.55607,
 author = {Born, R. G. and Kenevan, J. R.},
 title = {Analytic derivation of processor potential utilization in straight line, ring, square mesh, and hypercube networks},
 abstract = {In multicomputer architectures, in which processors communicate through message-passing, the overhead encountered because of the need to relay messages can significantly affect performance. Based upon some simplifying assumptions including the rate at which a processor generates messages being proportional to its current potential utilization, processor utilizations are analytically derived in matrix form for a bidirectional straight line and square mesh. In addition, closed form derivations are provided for a unidirectional ring and an n-dimensional hypercube. Finally, the theoretical results are found to be in close agreement with discrete-event simulations of the four architectures.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {94--103},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55607},
 doi = {http://doi.acm.org/10.1145/55595.55607},
 acmid = {55607},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Majumdar:1988:SMP:1007771.55608,
 author = {Majumdar, S. and Eager, D. L. and Bunt, R. B.},
 title = {Scheduling in multiprogrammed parallel systems},
 abstract = {Processor scheduling on multiprocessor systems that simultaneously run concurrent applications is currently not well-understood. This paper reports a preliminary investigation of a number of fundamental issues which are important in the context of scheduling concurrent jobs on multiprogrammed parallel systems. The major motivation for this research is to gain insight into system behaviour and understand the basic principles underlying the performance of scheduling strategies in such parallel systems. Based on abstract models of systems and scheduling disciplines, several high level issues that are important in this context have been analysed.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {104--113},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55608},
 doi = {http://doi.acm.org/10.1145/1007771.55608},
 acmid = {55608},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Majumdar:1988:SMP:55595.55608,
 author = {Majumdar, S. and Eager, D. L. and Bunt, R. B.},
 title = {Scheduling in multiprogrammed parallel systems},
 abstract = {Processor scheduling on multiprocessor systems that simultaneously run concurrent applications is currently not well-understood. This paper reports a preliminary investigation of a number of fundamental issues which are important in the context of scheduling concurrent jobs on multiprogrammed parallel systems. The major motivation for this research is to gain insight into system behaviour and understand the basic principles underlying the performance of scheduling strategies in such parallel systems. Based on abstract models of systems and scheduling disciplines, several high level issues that are important in this context have been analysed.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {104--113},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55608},
 doi = {http://doi.acm.org/10.1145/55595.55608},
 acmid = {55608},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Patel:1988:HCI:1007771.55609,
 author = {Patel, N. M. and Harrison, P. G.},
 title = {On hot-spot contention in interconnection networks},
 abstract = {A major component of a parallel machine is its interconnection network, which provides concurrent communication between the processing elements. It is common to use a multi-stage interconnection network (MIN) which is constructed using crossbar switches and introduces not only contention for destination addresses but also additional contention for internal switches. Both types of contention are increased when non-local communication across a MIN becomes concentrated on a certain destination address, for example when a frequently-accessed data structure is stored entirely in one element of a distributed memory. Such an address, often called a hot-spot, affects the blocking probability of paths to other destination addresses because of the shared internal switches. This paper describes an analytical model of hot-spot contention and quantifies its effect on the performance of a MIN with a circuit switching communication protocol. We obtain performance measures for a MIN in which partial paths are held during path building and one destination address is more frequently chosen by incoming traffic than other addresses.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {114--123},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55609},
 doi = {http://doi.acm.org/10.1145/1007771.55609},
 acmid = {55609},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Patel:1988:HCI:55595.55609,
 author = {Patel, N. M. and Harrison, P. G.},
 title = {On hot-spot contention in interconnection networks},
 abstract = {A major component of a parallel machine is its interconnection network, which provides concurrent communication between the processing elements. It is common to use a multi-stage interconnection network (MIN) which is constructed using crossbar switches and introduces not only contention for destination addresses but also additional contention for internal switches. Both types of contention are increased when non-local communication across a MIN becomes concentrated on a certain destination address, for example when a frequently-accessed data structure is stored entirely in one element of a distributed memory. Such an address, often called a hot-spot, affects the blocking probability of paths to other destination addresses because of the shared internal switches. This paper describes an analytical model of hot-spot contention and quantifies its effect on the performance of a MIN with a circuit switching communication protocol. We obtain performance measures for a MIN in which partial paths are held during path building and one destination address is more frequently chosen by incoming traffic than other addresses.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {114--123},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55609},
 doi = {http://doi.acm.org/10.1145/55595.55609},
 acmid = {55609},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kothari:1988:PAM:1007771.55610,
 author = {Kothari, S. C. and Jhunjhunwala, A. and Mukherjee, A.},
 title = {Performance analysis of multipath multistage interconnection networks},
 abstract = {This paper closely examines the performance analysis for unbuffered multipath multistage interconnection networks. A critical discussion of commonly used analysis is provided to identify a basic flaw in the model. A new analysis based on the grouping of alternate links is proposed as an alternative to rectify the error. The results based on the new analysis and extensive simulation are presented for three representative networks. The simulation study strongly supports the results of the new analysis.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {124--132},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007771.55610},
 doi = {http://doi.acm.org/10.1145/1007771.55610},
 acmid = {55610},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kothari:1988:PAM:55595.55610,
 author = {Kothari, S. C. and Jhunjhunwala, A. and Mukherjee, A.},
 title = {Performance analysis of multipath multistage interconnection networks},
 abstract = {This paper closely examines the performance analysis for unbuffered multipath multistage interconnection networks. A critical discussion of commonly used analysis is provided to identify a basic flaw in the model. A new analysis based on the grouping of alternate links is proposed as an alternative to rectify the error. The results based on the new analysis and extensive simulation are presented for three representative networks. The simulation study strongly supports the results of the new analysis.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {124--132},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/55595.55610},
 doi = {http://doi.acm.org/10.1145/55595.55610},
 acmid = {55610},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Melus:1988:MPE:55595.55611,
 author = {Mel\'{u}s, J. L. and Sanvicente, E. and Magri\~{n}\'{a}, J.},
 title = {Modelling and performance evaluation of multiprocessor based packet switches},
 abstract = {This paper presents an approximate analytic model for the performance analysis of a class of multiprocessor based packet switches. For these systems, processors and common memory modules are grouped in clusters, each of them composed of several processor-memory pairs that communicate through a multiple bus interconnection network. Intercluster communication is also achieved using one or more busses. The whole network operates in a circuit-switched mode.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {133--140},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/55595.55611},
 doi = {http://doi.acm.org/10.1145/55595.55611},
 acmid = {55611},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Melus:1988:MPE:1007771.55611,
 author = {Mel\'{u}s, J. L. and Sanvicente, E. and Magri\~{n}\'{a}, J.},
 title = {Modelling and performance evaluation of multiprocessor based packet switches},
 abstract = {This paper presents an approximate analytic model for the performance analysis of a class of multiprocessor based packet switches. For these systems, processors and common memory modules are grouped in clusters, each of them composed of several processor-memory pairs that communicate through a multiple bus interconnection network. Intercluster communication is also achieved using one or more busses. The whole network operates in a circuit-switched mode.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {133--140},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1007771.55611},
 doi = {http://doi.acm.org/10.1145/1007771.55611},
 acmid = {55611},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1988:MCP:55595.55612,
 author = {Lee, T. P.},
 title = {A manufacturing capacity planning experiment through functional workload decomposition},
 abstract = {In this paper, we describe an experiment to evaluate a distributed architecture via functional database workload decomposition. A workload in a circuit pack assembly environment was decomposed and mapped onto a frontend/backend distributed computer architecture. To evaluate this distributed architecture, an operational model for capacity planning was devised, and performance and cost-effectiveness measures were chosen. Model parameters were estimated through benchmark experiments in a distributed system consisting of various super-microcomputers connected by a CSMA/CD local area network with INGRES as the database management system.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55612},
 doi = {http://doi.acm.org/10.1145/55595.55612},
 acmid = {55612},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1988:MCP:1007771.55612,
 author = {Lee, T. P.},
 title = {A manufacturing capacity planning experiment through functional workload decomposition},
 abstract = {In this paper, we describe an experiment to evaluate a distributed architecture via functional database workload decomposition. A workload in a circuit pack assembly environment was decomposed and mapped onto a frontend/backend distributed computer architecture. To evaluate this distributed architecture, an operational model for capacity planning was devised, and performance and cost-effectiveness measures were chosen. Model parameters were estimated through benchmark experiments in a distributed system consisting of various super-microcomputers connected by a CSMA/CD local area network with INGRES as the database management system.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55612},
 doi = {http://doi.acm.org/10.1145/1007771.55612},
 acmid = {55612},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Irgon:1988:FLS:1007771.55613,
 author = {Irgon, A. E. and Dragoni,Jr., A. H. and Huleatt, T. O.},
 title = {FAST: A large scale expert system for application and system software performance tuning},
 abstract = {},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {151--156},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1007771.55613},
 doi = {http://doi.acm.org/10.1145/1007771.55613},
 acmid = {55613},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Irgon:1988:FLS:55595.55613,
 author = {Irgon, A. E. and Dragoni,Jr., A. H. and Huleatt, T. O.},
 title = {FAST: A large scale expert system for application and system software performance tuning},
 abstract = {},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {151--156},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/55595.55613},
 doi = {http://doi.acm.org/10.1145/55595.55613},
 acmid = {55613},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Alexander:1988:CDC:1007771.55614,
 author = {Alexander, W. and Copeland, G.},
 title = {Comparison of dataflow control techniques in distributed data-intensive systems},
 abstract = {In dataflow architectures, each dataflow node (i.e., operation) is typically executed on a single physical node. We are concerned with distributed data-intensive systems, in which each base (i.e., persistent) set of data has been declustered over many physical nodes to achieve load balancing. Because of large base set size, each operation is executed where the base set resides, and intermediate results are transferred between physical nodes. In such systems, each dataflow node is typically executed on many physical nodes. Furthermore, because computations are data-dependent, we cannot know until run time which subset of the physical nodes containing a particular base set will be involved in a given dataflow node. This uncertainty affects program loading, task activation and termination, and data transfer among the nodes.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {157--166},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55614},
 doi = {http://doi.acm.org/10.1145/1007771.55614},
 acmid = {55614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Alexander:1988:CDC:55595.55614,
 author = {Alexander, W. and Copeland, G.},
 title = {Comparison of dataflow control techniques in distributed data-intensive systems},
 abstract = {In dataflow architectures, each dataflow node (i.e., operation) is typically executed on a single physical node. We are concerned with distributed data-intensive systems, in which each base (i.e., persistent) set of data has been declustered over many physical nodes to achieve load balancing. Because of large base set size, each operation is executed where the base set resides, and intermediate results are transferred between physical nodes. In such systems, each dataflow node is typically executed on many physical nodes. Furthermore, because computations are data-dependent, we cannot know until run time which subset of the physical nodes containing a particular base set will be involved in a given dataflow node. This uncertainty affects program loading, task activation and termination, and data transfer among the nodes.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {157--166},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55614},
 doi = {http://doi.acm.org/10.1145/55595.55614},
 acmid = {55614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Leutenegger:1988:MPA:55595.55615,
 author = {Leutenegger, S. T. and Vernon, M. K.},
 title = {A mean-value performance analysis of a new multiprocessor architecture},
 abstract = {This paper presents a preliminary performance analysis of a new large-scale multiprocessor: the Wisconsin Multicube. A key characteristic of the machine is that it is based on shared buses and a snooping cache coherence protocol. The organization of the shared buses and shared memory is unique and non-hierarchical. The two-dimensional version of the architecture is envisioned as scaling to 1024 processors.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55615},
 doi = {http://doi.acm.org/10.1145/55595.55615},
 acmid = {55615},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Leutenegger:1988:MPA:1007771.55615,
 author = {Leutenegger, S. T. and Vernon, M. K.},
 title = {A mean-value performance analysis of a new multiprocessor architecture},
 abstract = {This paper presents a preliminary performance analysis of a new large-scale multiprocessor: the Wisconsin Multicube. A key characteristic of the machine is that it is based on shared buses and a snooping cache coherence protocol. The organization of the shared buses and shared memory is unique and non-hierarchical. The two-dimensional version of the architecture is envisioned as scaling to 1024 processors.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {167--176},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55615},
 doi = {http://doi.acm.org/10.1145/1007771.55615},
 acmid = {55615},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Blake:1988:SAR:1007771.55616,
 author = {Blake, J. T. and Reibman, A. L. and Trivedi, K. S.},
 title = {Sensitivity analysis of reliability and performability measures for multiprocessor systems},
 abstract = {Traditional evaluation techniques for multiprocessor systems use Markov chains and Markov reward models to compute measures such as mean time to failure, reliability, performance, and performability. In this paper, we discuss the extension of Markov models to include parametric sensitivity analysis. Using such analysis, we can guide system optimization, identify parts of a system model sensitive to error, and find system reliability and performability bottlenecks.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55616},
 doi = {http://doi.acm.org/10.1145/1007771.55616},
 acmid = {55616},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blake:1988:SAR:55595.55616,
 author = {Blake, J. T. and Reibman, A. L. and Trivedi, K. S.},
 title = {Sensitivity analysis of reliability and performability measures for multiprocessor systems},
 abstract = {Traditional evaluation techniques for multiprocessor systems use Markov chains and Markov reward models to compute measures such as mean time to failure, reliability, performance, and performability. In this paper, we discuss the extension of Markov models to include parametric sensitivity analysis. Using such analysis, we can guide system optimization, identify parts of a system model sensitive to error, and find system reliability and performability bottlenecks.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55616},
 doi = {http://doi.acm.org/10.1145/55595.55616},
 acmid = {55616},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mukkamala:1988:DPR:55595.55617,
 author = {Mukkamala, R. and Bruell, S. C. and Shultz, R. K.},
 title = {Design of partially replicated distributed database systems: an integrated methodology},
 abstract = {The objective of this research is to develop and integrate tools for the design of partially replicated distributed database systems. Many existing tools are inappropriate for designing large-scale distributed databases due to their large computational requirements. Our goal is to develop tools that solve the design problems reasonably quickly, typically by using heuristic algorithms that provide approximate or near-optimal solutions.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {187--196},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55617},
 doi = {http://doi.acm.org/10.1145/55595.55617},
 acmid = {55617},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mukkamala:1988:DPR:1007771.55617,
 author = {Mukkamala, R. and Bruell, S. C. and Shultz, R. K.},
 title = {Design of partially replicated distributed database systems: an integrated methodology},
 abstract = {The objective of this research is to develop and integrate tools for the design of partially replicated distributed database systems. Many existing tools are inappropriate for designing large-scale distributed databases due to their large computational requirements. Our goal is to develop tools that solve the design problems reasonably quickly, typically by using heuristic algorithms that provide approximate or near-optimal solutions.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {187--196},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55617},
 doi = {http://doi.acm.org/10.1145/1007771.55617},
 acmid = {55617},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wybranietz:1988:MPM:1007771.55618,
 author = {Wybranietz, D. and Haban, D.},
 title = {Monitoring and performance measuring distributed systems during operation},
 abstract = {This paper describes an integrated tool for monitoring distributed systems continuously during operation. A hybrid monitoring approach is used. As special hardware support a test and measurement processor (TMP) was designed, which is part of each node in an experimental multicomputer system. Each TMP runs local parts of the monitoring software for its node, while all the TMPs are connected to a central test station via a separate TMP interconnection network. The monitoring system is transparent to users. It permanently observes system behavior, measures system performance and records system information. The immense amount of information is graphically displayed in easy-to-read-charts and graphs in an application-oriented manner. The tools promote an improved understanding of run time behavior and performance measurements to derive qualitative and even quantitative assessments about distributed systems. A prototype of the monitoring facility is operational and currently experiments are being conducted in our distributed system consisting of several MC68000 microcomputers.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {197--206},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1007771.55618},
 doi = {http://doi.acm.org/10.1145/1007771.55618},
 acmid = {55618},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wybranietz:1988:MPM:55595.55618,
 author = {Wybranietz, D. and Haban, D.},
 title = {Monitoring and performance measuring distributed systems during operation},
 abstract = {This paper describes an integrated tool for monitoring distributed systems continuously during operation. A hybrid monitoring approach is used. As special hardware support a test and measurement processor (TMP) was designed, which is part of each node in an experimental multicomputer system. Each TMP runs local parts of the monitoring software for its node, while all the TMPs are connected to a central test station via a separate TMP interconnection network. The monitoring system is transparent to users. It permanently observes system behavior, measures system performance and records system information. The immense amount of information is graphically displayed in easy-to-read-charts and graphs in an application-oriented manner. The tools promote an improved understanding of run time behavior and performance measurements to derive qualitative and even quantitative assessments about distributed systems. A prototype of the monitoring facility is operational and currently experiments are being conducted in our distributed system consisting of several MC68000 microcomputers.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {197--206},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/55595.55618},
 doi = {http://doi.acm.org/10.1145/55595.55618},
 acmid = {55618},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Melvin:1988:UMI:1007771.55619,
 author = {Melvin, S. W. and Patt, Y. N.},
 title = {The use of microcode instrumentation for development, debugging and tuning of operating system kernels},
 abstract = {We have developed a tool based on microcode modifications to a VAX 8600 which allows a wide variety of operating system measurements to be taken with minimal perturbation and without the need to modify any operating system software. A trace of interrupts, exceptions, system calls and context switches is generated as a side-effect to normal execution. In this paper we describe the tool we have developed and present some results we have gathered under both UNIX 4.3 BSD and VAX/VMS V4.5. We compare the process fork behavior of two different command shells under UNIX, look at context switch rates for interactive and batch workloads and generate a histogram for network interrupt service time.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {207--214},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1007771.55619},
 doi = {http://doi.acm.org/10.1145/1007771.55619},
 acmid = {55619},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Melvin:1988:UMI:55595.55619,
 author = {Melvin, S. W. and Patt, Y. N.},
 title = {The use of microcode instrumentation for development, debugging and tuning of operating system kernels},
 abstract = {We have developed a tool based on microcode modifications to a VAX 8600 which allows a wide variety of operating system measurements to be taken with minimal perturbation and without the need to modify any operating system software. A trace of interrupts, exceptions, system calls and context switches is generated as a side-effect to normal execution. In this paper we describe the tool we have developed and present some results we have gathered under both UNIX 4.3 BSD and VAX/VMS V4.5. We compare the process fork behavior of two different command shells under UNIX, look at context switch rates for interactive and batch workloads and generate a histogram for network interrupt service time.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {207--214},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/55595.55619},
 doi = {http://doi.acm.org/10.1145/55595.55619},
 acmid = {55619},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agawal:1988:MCM:1007771.55620,
 author = {Agawal, A. and Gupta, A.},
 title = {Memory-reference characteristics of multiprocessor applications under MACH},
 abstract = {Shared-memory multiprocessors have received wide attention in recent times as a means of achieving high-performance cost-effectively. Their viability requires a thorough understanding of the memory access patterns of parallel processing applications and operating systems. This paper reports on the memory reference behavior of several parallel applications running under the MACH operating system on a shared-memory multiprocessor. The data used for this study is derived from multiprocessor address traces obtained from an extended ATUM address tracing scheme implemented on a 4-CPU DEC VAX 8350. The applications include parallel OPS5, logic simulation, and a VSLI wire routing program. Among the important issues addressed in this paper are the amount of sharing in user programs and in the operating system, comparing the characteristics of user and system reference patterns, sharing related to process migration, and the temporal, spatial, and processor locality of shared blocks. We also analyze the impact of shared references on cache coherence in shared-memory multiprocessors.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {215--225},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1007771.55620},
 doi = {http://doi.acm.org/10.1145/1007771.55620},
 acmid = {55620},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agawal:1988:MCM:55595.55620,
 author = {Agawal, A. and Gupta, A.},
 title = {Memory-reference characteristics of multiprocessor applications under MACH},
 abstract = {Shared-memory multiprocessors have received wide attention in recent times as a means of achieving high-performance cost-effectively. Their viability requires a thorough understanding of the memory access patterns of parallel processing applications and operating systems. This paper reports on the memory reference behavior of several parallel applications running under the MACH operating system on a shared-memory multiprocessor. The data used for this study is derived from multiprocessor address traces obtained from an extended ATUM address tracing scheme implemented on a 4-CPU DEC VAX 8350. The applications include parallel OPS5, logic simulation, and a VSLI wire routing program. Among the important issues addressed in this paper are the amount of sharing in user programs and in the operating system, comparing the characteristics of user and system reference patterns, sharing related to process migration, and the temporal, spatial, and processor locality of shared blocks. We also analyze the impact of shared references on cache coherence in shared-memory multiprocessors.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {215--225},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/55595.55620},
 doi = {http://doi.acm.org/10.1145/55595.55620},
 acmid = {55620},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Murphy:1988:CPB:1007771.55621,
 author = {Murphy, J. M. and Bunt, R. B.},
 title = {Characterising program behaviour with phases and transitions},
 abstract = {A detailed quantitative study of program behaviour is described. Reference strings from a representative set of programs were decomposed into phases and transitions. Referencing behaviour is studied at both the macro level (program-wide) and the micro level (within the phases and transitions). Quantitative data, suitable for the parameterization of program behaviour models, is presented.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {226--234},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007771.55621},
 doi = {http://doi.acm.org/10.1145/1007771.55621},
 acmid = {55621},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Murphy:1988:CPB:55595.55621,
 author = {Murphy, J. M. and Bunt, R. B.},
 title = {Characterising program behaviour with phases and transitions},
 abstract = {A detailed quantitative study of program behaviour is described. Reference strings from a representative set of programs were decomposed into phases and transitions. Referencing behaviour is studied at both the macro level (program-wide) and the micro level (within the phases and transitions). Quantitative data, suitable for the parameterization of program behaviour models, is presented.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {226--234},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/55595.55621},
 doi = {http://doi.acm.org/10.1145/55595.55621},
 acmid = {55621},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yoshizawa:1988:ASC:1007771.55622,
 author = {Yoshizawa, Y. and Arai, T.},
 title = {Adaptive storage control for page frame supply in large scale computer systems},
 abstract = {A real storage management algorithm called Adaptive Control of Page-frame Supply (ACPS) is described. ACPS employees three strategies: prediction of the demand for real page frames, page replacement based on the prediction, and working set control. Together, these strategies constitute the real page frame allocation method, and contribute to short and stable response times in conversational processing environments.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {235--243},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007771.55622},
 doi = {http://doi.acm.org/10.1145/1007771.55622},
 acmid = {55622},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yoshizawa:1988:ASC:55595.55622,
 author = {Yoshizawa, Y. and Arai, T.},
 title = {Adaptive storage control for page frame supply in large scale computer systems},
 abstract = {A real storage management algorithm called Adaptive Control of Page-frame Supply (ACPS) is described. ACPS employees three strategies: prediction of the demand for real page frames, page replacement based on the prediction, and working set control. Together, these strategies constitute the real page frame allocation method, and contribute to short and stable response times in conversational processing environments.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {235--243},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/55595.55622},
 doi = {http://doi.acm.org/10.1145/55595.55622},
 acmid = {55622},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Pattipati:1988:PAM:1007771.55623,
 author = {Pattipati, K. R. and Kostreva, M. M.},
 title = {On the properties of approximate mean value analysis algorithms for queueing networks},
 abstract = {This paper presents new formulations of the approximate mean value analysis (MVA) algorithms for the performance evaluation of closed product-form queueing networks. The key to the development of the algorithms is the derivation of vector nonlinear equations for the approximate network throughput. We solve this set of throughput equations using a nonlinear Gauss-Seidel type distributed algorithms, coupled with a quadratically convergent Newton's method for scalar nonlinear equations. The throughput equations have enabled us to: (a) derive bounds on the approximate throughput; (b) prove the existence, uniqueness, and convergence of the Schweitzer-Bard (S-B) approximation algorithm for a wide class of monotone, single class networks, (c) establish the existence of the S-B solution for multi-class, monotone networks, and (d) prove the asymptotic (i.e., as the number of customers of each class tends to \&infin;) uniqueness of the S-B throughput solution, and the asymptotic convergence of the various versions of the distributed algorithms in multi-class networks with single server and infinite server nodes. The asymptotic convergence is established using results from convex programming and convex duality theory. Extension of our algorithms to mixed networks is straighfoward. Only multi-class results are presented in this paper.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {244--252},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007771.55623},
 doi = {http://doi.acm.org/10.1145/1007771.55623},
 acmid = {55623},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Pattipati:1988:PAM:55595.55623,
 author = {Pattipati, K. R. and Kostreva, M. M.},
 title = {On the properties of approximate mean value analysis algorithms for queueing networks},
 abstract = {This paper presents new formulations of the approximate mean value analysis (MVA) algorithms for the performance evaluation of closed product-form queueing networks. The key to the development of the algorithms is the derivation of vector nonlinear equations for the approximate network throughput. We solve this set of throughput equations using a nonlinear Gauss-Seidel type distributed algorithms, coupled with a quadratically convergent Newton's method for scalar nonlinear equations. The throughput equations have enabled us to: (a) derive bounds on the approximate throughput; (b) prove the existence, uniqueness, and convergence of the Schweitzer-Bard (S-B) approximation algorithm for a wide class of monotone, single class networks, (c) establish the existence of the S-B solution for multi-class, monotone networks, and (d) prove the asymptotic (i.e., as the number of customers of each class tends to \&infin;) uniqueness of the S-B throughput solution, and the asymptotic convergence of the various versions of the distributed algorithms in multi-class networks with single server and infinite server nodes. The asymptotic convergence is established using results from convex programming and convex duality theory. Extension of our algorithms to mixed networks is straighfoward. Only multi-class results are presented in this paper.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {244--252},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/55595.55623},
 doi = {http://doi.acm.org/10.1145/55595.55623},
 acmid = {55623},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tantawi:1988:OAM:1007771.55624,
 author = {Tantawi, A. N. and Towsley, G. and Wolf, J.},
 title = {Optimal allocation of multiple class resources in computer systems},
 abstract = {A class-constrained resource allocation problem is considered. In this problem, a set of M heterogeneous resources is to be allocated optimally among a set of L users belonging to K user classes. A set of class allocation constraints, which limit the number of users of a given class that could be allocated to a given resource, is imposed. An algorithm with worst case time complexity O(M (LM + M<supscrpt>2</supscrpt> + LK)) is presented along with a proof of its correctness. This problem arises in many areas of resource management in computer systems, such as load balancing in distributed systems, transaction processing in distributed database systems, and session allocation in time-shared computer systems. We illustrate the behavior of this algorithm with an example where file servers are to be allocated to workstations of multiple classes.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {253--260},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1007771.55624},
 doi = {http://doi.acm.org/10.1145/1007771.55624},
 acmid = {55624},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tantawi:1988:OAM:55595.55624,
 author = {Tantawi, A. N. and Towsley, G. and Wolf, J.},
 title = {Optimal allocation of multiple class resources in computer systems},
 abstract = {A class-constrained resource allocation problem is considered. In this problem, a set of M heterogeneous resources is to be allocated optimally among a set of L users belonging to K user classes. A set of class allocation constraints, which limit the number of users of a given class that could be allocated to a given resource, is imposed. An algorithm with worst case time complexity O(M (LM + M<supscrpt>2</supscrpt> + LK)) is presented along with a proof of its correctness. This problem arises in many areas of resource management in computer systems, such as load balancing in distributed systems, transaction processing in distributed database systems, and session allocation in time-shared computer systems. We illustrate the behavior of this algorithm with an example where file servers are to be allocated to workstations of multiple classes.
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {253--260},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/55595.55624},
 doi = {http://doi.acm.org/10.1145/55595.55624},
 acmid = {55624},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hsieh:1988:PNA:55595.55625,
 author = {Hsieh, C.-H. and Lam, S. S.},
 title = {PAM-a noniterative approximate solution method for closed multichain queueing networks},
 abstract = {Approximate MVA algorithms for separable queueing networks are based upon an iterative solution of a set of modified MVA formulas. Although each iteration has a computational time requirement of O(MK<supscrpt>2</supscrpt>) or less, many iterations are typically needed for convergence to a solution. (M denotes the number of queues and K the number of closed chains or customer classes.) We present some faster approximate solution algorithms that are noniterative. They are suitable for the analysis and design of communication networks which may require tens to hundreds, perhaps thousands, of closed chains to model flow-controlled virtual channels. Three PAM algorithms of increasing accuracy are presented. Two of them have time and space requirements of O(MK). The third algorithm has a time requirement of O(MK<supscrpt>2</supscrpt>) and a space requirement of O(MK).
},
 booktitle = {Proceedings of the 1988 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '88},
 year = {1988},
 isbn = {0-89791-254-3},
 location = {Santa Fe, New Mexico, United States},
 pages = {261--269},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/55595.55625},
 doi = {http://doi.acm.org/10.1145/55595.55625},
 acmid = {55625},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hsieh:1988:PNA:1007771.55625,
 author = {Hsieh, C.-H. and Lam, S. S.},
 title = {PAM-a noniterative approximate solution method for closed multichain queueing networks},
 abstract = {Approximate MVA algorithms for separable queueing networks are based upon an iterative solution of a set of modified MVA formulas. Although each iteration has a computational time requirement of O(MK<supscrpt>2</supscrpt>) or less, many iterations are typically needed for convergence to a solution. (M denotes the number of queues and K the number of closed chains or customer classes.) We present some faster approximate solution algorithms that are noniterative. They are suitable for the analysis and design of communication networks which may require tens to hundreds, perhaps thousands, of closed chains to model flow-controlled virtual channels. Three PAM algorithms of increasing accuracy are presented. Two of them have time and space requirements of O(MK). The third algorithm has a time requirement of O(MK<supscrpt>2</supscrpt>) and a space requirement of O(MK).
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {16},
 issue = {1},
 month = {May},
 year = {1988},
 issn = {0163-5999},
 pages = {261--269},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1007771.55625},
 doi = {http://doi.acm.org/10.1145/1007771.55625},
 acmid = {55625},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Buzen:1986:MIS:317531.317532,
 author = {Buzen, Jeffrey P.},
 title = {Modeling I/O subsystems (tutorial)},
 abstract = {This tutorial will present techniques for modeling the performance of I/O subsystems that incorporate channels, control units, string controllers and direct access devices. The presentation will focus on the general principles involved in analyses of this type, and will explore the strengths and weaknesses of alternative assumptions. Attendees should gain an overall understanding of basic analysis procedures so they can deal with alternative I/O architectures that are not treated explicitly in the presentation. 
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/317531.317532},
 doi = {http://doi.acm.org/10.1145/317531.317532},
 acmid = {317532},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Buzen:1986:MIS:317499.317532,
 author = {Buzen, Jeffrey P.},
 title = {Modeling I/O subsystems (tutorial)},
 abstract = {This tutorial will present techniques for modeling the performance of I/O subsystems that incorporate channels, control units, string controllers and direct access devices. The presentation will focus on the general principles involved in analyses of this type, and will explore the strengths and weaknesses of alternative assumptions. Attendees should gain an overall understanding of basic analysis procedures so they can deal with alternative I/O architectures that are not treated explicitly in the presentation. 
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/317499.317532},
 doi = {http://doi.acm.org/10.1145/317499.317532},
 acmid = {317532},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ferrari:1986:WCI:317531.317900,
 author = {Ferrari, Domenico},
 title = {Workload characterization (tutorial): issues and approaches},
 abstract = {Workload characterization is that branch of performance evaluation which concerns itself with the measurement and modeling of the workloads to be processed by the system being evaluated. Since all performance indices of interest are workload-dependent, there is no evaluation study that does not require the characterization of one or more workloads. In spite of the importance of the problem, our knowledge in this area leaves much to be desired.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/317531.317900},
 doi = {http://doi.acm.org/10.1145/317531.317900},
 acmid = {317900},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ferrari:1986:WCI:317499.317900,
 author = {Ferrari, Domenico},
 title = {Workload characterization (tutorial): issues and approaches},
 abstract = {Workload characterization is that branch of performance evaluation which concerns itself with the measurement and modeling of the workloads to be processed by the system being evaluated. Since all performance indices of interest are workload-dependent, there is no evaluation study that does not require the characterization of one or more workloads. In spite of the importance of the problem, our knowledge in this area leaves much to be desired.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/317499.317900},
 doi = {http://doi.acm.org/10.1145/317499.317900},
 acmid = {317900},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Goel:1986:SRM:317499.317901,
 author = {Goel, Amrit L.},
 title = {Software reliability modeling (tutorial)},
 abstract = {There are a number of views as to what software reliability is and how it should be quantified. Some people believe that this measure should be binary in nature so that an imperfect program would have zero reliability while a perfect one would have a reliability value of one. This view parallels that of program proving whereby the program is either correct or incorrect. Others, however, feel that software reliability should be defined as the relative frequency of the times that the program works as intended by the user. This view is similar to that taken in testing where a percentage of the successful ewes is used as a measure of program quality. According to the latter viewpoint, software reliability is a probabilistic measure and can be defined as follows:
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {2--},
 url = {http://doi.acm.org/10.1145/317499.317901},
 doi = {http://doi.acm.org/10.1145/317499.317901},
 acmid = {317901},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Goel:1986:SRM:317531.317901,
 author = {Goel, Amrit L.},
 title = {Software reliability modeling (tutorial)},
 abstract = {There are a number of views as to what software reliability is and how it should be quantified. Some people believe that this measure should be binary in nature so that an imperfect program would have zero reliability while a perfect one would have a reliability value of one. This view parallels that of program proving whereby the program is either correct or incorrect. Others, however, feel that software reliability should be defined as the relative frequency of the times that the program works as intended by the user. This view is similar to that taken in testing where a percentage of the successful ewes is used as a measure of program quality. According to the latter viewpoint, software reliability is a probabilistic measure and can be defined as follows:
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {2--},
 url = {http://doi.acm.org/10.1145/317531.317901},
 doi = {http://doi.acm.org/10.1145/317531.317901},
 acmid = {317901},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hedlund:1986:PMI:317499.317902,
 author = {Hedlund, Kye},
 title = {Performance modeling in integrated curcuit design (tutorial)},
 abstract = {This tutorial is an introduction to performance modeling in the design of integrated circuits (ICs). It assumes no background in either electrical engineering or VLSI design; all relevant concepts and terminology will be introduced. The goal is to give an overview of the role of performance modeling in IC design, the current state of the art, central problems and research challenges.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {2--},
 url = {http://doi.acm.org/10.1145/317499.317902},
 doi = {http://doi.acm.org/10.1145/317499.317902},
 acmid = {317902},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hedlund:1986:PMI:317531.317902,
 author = {Hedlund, Kye},
 title = {Performance modeling in integrated curcuit design (tutorial)},
 abstract = {This tutorial is an introduction to performance modeling in the design of integrated circuits (ICs). It assumes no background in either electrical engineering or VLSI design; all relevant concepts and terminology will be introduced. The goal is to give an overview of the role of performance modeling in IC design, the current state of the art, central problems and research challenges.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {2--},
 url = {http://doi.acm.org/10.1145/317531.317902},
 doi = {http://doi.acm.org/10.1145/317531.317902},
 acmid = {317902},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Artis:1986:ESP:317499.317903,
 author = {Artis, H. Pat},
 title = {Expert systems for performance analysis (tutorial)},
 abstract = {A great portion of the formal practice called computer performance evaluation is the application of rules of thumb and proceduralized analysis of model results, specific reports, and data elements based on the experience and knowledge of the practitioner. Expert systems provide a technique to support the analyst in such mundane analyses and allow them to study more complex problems that cannot easily be proceduralized. Rather than replacing performance analysts expert systems provide an opportunity to increase their productivity.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {3--},
 url = {http://doi.acm.org/10.1145/317499.317903},
 doi = {http://doi.acm.org/10.1145/317499.317903},
 acmid = {317903},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Artis:1986:ESP:317531.317903,
 author = {Artis, H. Pat},
 title = {Expert systems for performance analysis (tutorial)},
 abstract = {A great portion of the formal practice called computer performance evaluation is the application of rules of thumb and proceduralized analysis of model results, specific reports, and data elements based on the experience and knowledge of the practitioner. Expert systems provide a technique to support the analyst in such mundane analyses and allow them to study more complex problems that cannot easily be proceduralized. Rather than replacing performance analysts expert systems provide an opportunity to increase their productivity.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {3--},
 url = {http://doi.acm.org/10.1145/317531.317903},
 doi = {http://doi.acm.org/10.1145/317531.317903},
 acmid = {317903},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tripathi:1986:PIL:317531.317904,
 author = {Tripathi, Satish K.},
 title = {Performance issues in local area networks (tutorial)},
 abstract = {This tutorial addresses performance problems in Local Area Networks (LAN). User level performance measures are affected both by the software as well as communication bottlenecks. Techniques for modeling the key components of the performance of a LAN will be presented. Models will be presented to discuss the throughput and response time characteristics of LANs. We also present some measurement data obtained from a LAN performance experiment.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {3--},
 url = {http://doi.acm.org/10.1145/317531.317904},
 doi = {http://doi.acm.org/10.1145/317531.317904},
 acmid = {317904},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tripathi:1986:PIL:317499.317904,
 author = {Tripathi, Satish K.},
 title = {Performance issues in local area networks (tutorial)},
 abstract = {This tutorial addresses performance problems in Local Area Networks (LAN). User level performance measures are affected both by the software as well as communication bottlenecks. Techniques for modeling the key components of the performance of a LAN will be presented. Models will be presented to discuss the throughput and response time characteristics of LANs. We also present some measurement data obtained from a LAN performance experiment.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {3--},
 url = {http://doi.acm.org/10.1145/317499.317904},
 doi = {http://doi.acm.org/10.1145/317499.317904},
 acmid = {317904},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stone:1986:FC:317531.317533,
 author = {Stone, Harold S. and Thibaut, Dominique},
 title = {Footprints in the cache},
 abstract = {This paper develops an analytical model for a cache-reload transient. When an interrupt program or system program runs periodically in a cache-based computer, a short cache-reload transient occurs each time the interrupt program is invoked. That transient depends on the size of the cache, the fraction of the cache used by the interrupt program, and the fraction of the cache used by background programs that run between interrupts. We call the portion of a cache used by a program its footprint in the cache, and we show that the reload transient is related to the area in the tail of a normal distribution whose mean is a function of the footprints of the programs that compete for the cache. We believe that the model may be useful as well for predicting paging behavior in virtual-memory systems with round-robin scheduling.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {4--8},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/317531.317533},
 doi = {http://doi.acm.org/10.1145/317531.317533},
 acmid = {317533},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stone:1986:FC:317499.317533,
 author = {Stone, Harold S. and Thibaut, Dominique},
 title = {Footprints in the cache},
 abstract = {This paper develops an analytical model for a cache-reload transient. When an interrupt program or system program runs periodically in a cache-based computer, a short cache-reload transient occurs each time the interrupt program is invoked. That transient depends on the size of the cache, the fraction of the cache used by the interrupt program, and the fraction of the cache used by background programs that run between interrupts. We call the portion of a cache used by a program its footprint in the cache, and we show that the reload transient is related to the area in the tail of a normal distribution whose mean is a function of the footprints of the programs that compete for the cache. We believe that the model may be useful as well for predicting paging behavior in virtual-memory systems with round-robin scheduling.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {4--8},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/317499.317533},
 doi = {http://doi.acm.org/10.1145/317499.317533},
 acmid = {317533},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Vernon:1986:PAM:317531.317534,
 author = {Vernon, Mary K. and Holliday, Mark A.},
 title = {Performance analysis of multiprocessor cache consistency protocols using generalized timed Petri nets},
 abstract = {We use an exact analytical technique, based on Generalized Timed Petri Nets (GTPNs), to study the performance of shared bus cache consistency protocols for multiprocessors. We develop a general framework within which the key characteristics of the Write-Once protocol and four enhancements that have been combined in various ways in the literature can be identified and evaluated. We then quantitatively assess the performance gains for each of the four enhancements. We consider three levels of data sharing in our workload models. One of the enhancements substantially improves system performance in all cases. Two enhancements are shown to have negligible effect over the range of workloads analyzed. The fourth enhancement shows a small improvement for low levels of sharing, but shows more substantial improvement as sharing is increased, if we assume a ``good access pattern". The effects of two architectural parameters, the blocksize and the main memory cycle time are also considered.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {9--17},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317531.317534},
 doi = {http://doi.acm.org/10.1145/317531.317534},
 acmid = {317534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vernon:1986:PAM:317499.317534,
 author = {Vernon, Mary K. and Holliday, Mark A.},
 title = {Performance analysis of multiprocessor cache consistency protocols using generalized timed Petri nets},
 abstract = {We use an exact analytical technique, based on Generalized Timed Petri Nets (GTPNs), to study the performance of shared bus cache consistency protocols for multiprocessors. We develop a general framework within which the key characteristics of the Write-Once protocol and four enhancements that have been combined in various ways in the literature can be identified and evaluated. We then quantitatively assess the performance gains for each of the four enhancements. We consider three levels of data sharing in our workload models. One of the enhancements substantially improves system performance in all cases. Two enhancements are shown to have negligible effect over the range of workloads analyzed. The fourth enhancement shows a small improvement for low levels of sharing, but shows more substantial improvement as sharing is increased, if we assume a ``good access pattern". The effects of two architectural parameters, the blocksize and the main memory cycle time are also considered.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {9--17},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317499.317534},
 doi = {http://doi.acm.org/10.1145/317499.317534},
 acmid = {317534},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harrison:1986:PMP:317499.317535,
 author = {Harrison, P. G. and Field, A. J.},
 title = {Performance modelling of parallel computer architectures},
 abstract = {In this paper we describe two types of complex server aggregations which can be used to model collections of components in certain types of parallel computer systems and give a case study showing how the aggregations may be applied in practice. Analytical models of such systems are becoming increasingly important as a means of guiding the often complex design processes, particularly since recent developments in VLSI technology now make it possible to fabricate many paper-designs hitherto impractical for reasons of cost. We argue that aggregations of the type described are essential in the modelling of parallel systems; using the proposed techniques, large numbers of components can be modelled as queue-length-dependent servers within a queueing network in which the number of servers is the same as the number of distinct types of processing element in the system being modelled. Because the number of severs in the model is fixed i.e. is independent of the number of processors, very large multiprocessor systems can be modelled efficiently with no explosion in the size of the state space.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {18--27},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317499.317535},
 doi = {http://doi.acm.org/10.1145/317499.317535},
 acmid = {317535},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harrison:1986:PMP:317531.317535,
 author = {Harrison, P. G. and Field, A. J.},
 title = {Performance modelling of parallel computer architectures},
 abstract = {In this paper we describe two types of complex server aggregations which can be used to model collections of components in certain types of parallel computer systems and give a case study showing how the aggregations may be applied in practice. Analytical models of such systems are becoming increasingly important as a means of guiding the often complex design processes, particularly since recent developments in VLSI technology now make it possible to fabricate many paper-designs hitherto impractical for reasons of cost. We argue that aggregations of the type described are essential in the modelling of parallel systems; using the proposed techniques, large numbers of components can be modelled as queue-length-dependent servers within a queueing network in which the number of servers is the same as the number of distinct types of processing element in the system being modelled. Because the number of severs in the model is fixed i.e. is independent of the number of processors, very large multiprocessor systems can be modelled efficiently with no explosion in the size of the state space.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {18--27},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317531.317535},
 doi = {http://doi.acm.org/10.1145/317531.317535},
 acmid = {317535},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Madnick:1986:MMC:317499.317536,
 author = {Madnick, Stuart and Wang, Y. Richard},
 title = {Modeling multiprocessor computer systems with unbalanced flows},
 abstract = {A performance analysis methodology using certain aspects of queueing theory to evaluate computer system speed performance is presented. This methodology specifically focuses on modeling multiprocessor computer systems with unbalanced flows (i.e., number of transactions leaving a server is not the same as number of transactions entering that server) due to asynchronously spawned parallel tasks. This unbalanced flow phenomenon, which has a significant effect on performance, cannot be solved analytically by classical queueing network models.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {28--34},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/317499.317536},
 doi = {http://doi.acm.org/10.1145/317499.317536},
 acmid = {317536},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Madnick:1986:MMC:317531.317536,
 author = {Madnick, Stuart and Wang, Y. Richard},
 title = {Modeling multiprocessor computer systems with unbalanced flows},
 abstract = {A performance analysis methodology using certain aspects of queueing theory to evaluate computer system speed performance is presented. This methodology specifically focuses on modeling multiprocessor computer systems with unbalanced flows (i.e., number of transactions leaving a server is not the same as number of transactions entering that server) due to asynchronously spawned parallel tasks. This unbalanced flow phenomenon, which has a significant effect on performance, cannot be solved analytically by classical queueing network models.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {28--34},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/317531.317536},
 doi = {http://doi.acm.org/10.1145/317531.317536},
 acmid = {317536},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kleeman:1986:APB:317531.317537,
 author = {Kleeman, Lindsay and Cantoni, Antonio},
 title = {The analysis and performance of batching arbiters},
 abstract = {A class of arbiters, known as batching arbiters, is introduced and defined. A particularly simple decentralised example of a batching arbiter is described, with motivation given for the batching arbiter model adopted. It is shown that under reasonable assumptions, batching arbiters can be described by a finite state Markov chain. The key steps in the analysis of the arbiter performance are the method of assigning states, evaluation of state transition probabilities and showing that the Markov chain is irreducible. Arbiter performance parameters are defined, such as proportion of time allocated to each requester and mean waiting time for each requester. Apart from results describing the steady state behavior of the arbiter for general system parameters, a number of limiting results are also obtained corresponding to light and heavy request loading.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {35--43},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317531.317537},
 doi = {http://doi.acm.org/10.1145/317531.317537},
 acmid = {317537},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kleeman:1986:APB:317499.317537,
 author = {Kleeman, Lindsay and Cantoni, Antonio},
 title = {The analysis and performance of batching arbiters},
 abstract = {A class of arbiters, known as batching arbiters, is introduced and defined. A particularly simple decentralised example of a batching arbiter is described, with motivation given for the batching arbiter model adopted. It is shown that under reasonable assumptions, batching arbiters can be described by a finite state Markov chain. The key steps in the analysis of the arbiter performance are the method of assigning states, evaluation of state transition probabilities and showing that the Markov chain is irreducible. Arbiter performance parameters are defined, such as proportion of time allocated to each requester and mean waiting time for each requester. Apart from results describing the steady state behavior of the arbiter for general system parameters, a number of limiting results are also obtained corresponding to light and heavy request loading.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {35--43},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317499.317537},
 doi = {http://doi.acm.org/10.1145/317499.317537},
 acmid = {317537},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lehoczky:1986:PRB:317499.317538,
 author = {Lehoczky, John P. and Sha, Lui},
 title = {Performance of real-time bus scheduling algorithms},
 abstract = {When periodic tasks with hard deadlines communicate over a bus, the problem of hard real-time bus scheduling arises. This paper addresses several problems of hard real-time bus scheduling, including the evaluation of scheduling algorithms and the issues of message packet pacing, preemption, priority granularity and buffering.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {44--53},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317499.317538},
 doi = {http://doi.acm.org/10.1145/317499.317538},
 acmid = {317538},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lehoczky:1986:PRB:317531.317538,
 author = {Lehoczky, John P. and Sha, Lui},
 title = {Performance of real-time bus scheduling algorithms},
 abstract = {When periodic tasks with hard deadlines communicate over a bus, the problem of hard real-time bus scheduling arises. This paper addresses several problems of hard real-time bus scheduling, including the evaluation of scheduling algorithms and the issues of message packet pacing, preemption, priority granularity and buffering.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {44--53},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317531.317538},
 doi = {http://doi.acm.org/10.1145/317531.317538},
 acmid = {317538},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Leland:1986:LHP:317531.317539,
 author = {Leland, Will and Ott, Teunis J.},
 title = {Load-balancing heuristics and process behavior},
 abstract = {Dynamic load balancing in a system of loosely-coupled homogeneous processors may employ both judicious initial placement of processes and migration of existing processes to processors with fewer resident processes. In order to predict the possible benefits of these dynamic assignment techniques, we analyzed the behavior (CPU, disk, and memory use) of 9.5 million Unix* processes during normal use. The observed process behavior was then used to drive simulation studies of particular dynamic assignment heuristics.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {54--69},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/317531.317539},
 doi = {http://doi.acm.org/10.1145/317531.317539},
 acmid = {317539},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Leland:1986:LHP:317499.317539,
 author = {Leland, Will and Ott, Teunis J.},
 title = {Load-balancing heuristics and process behavior},
 abstract = {Dynamic load balancing in a system of loosely-coupled homogeneous processors may employ both judicious initial placement of processes and migration of existing processes to processors with fewer resident processes. In order to predict the possible benefits of these dynamic assignment techniques, we analyzed the behavior (CPU, disk, and memory use) of 9.5 million Unix* processes during normal use. The observed process behavior was then used to drive simulation studies of particular dynamic assignment heuristics.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {54--69},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/317499.317539},
 doi = {http://doi.acm.org/10.1145/317499.317539},
 acmid = {317539},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lee:1986:CPD:317531.317540,
 author = {Lee, Kyoo Jeong and Towsley, Don},
 title = {A comparison of priority-based decentralized load balancing policies},
 abstract = {Load balancing policies in distributed systems divide jobs into two classes; those processed at their of origination (local jobs) and those processed at some other site in the system after being transfered through a communication network (remote jobs). This paper considers a class of decentralized load balancing policies that use a threshold on the local job queue length at each host in making decisions for remote processing. They differ from each other according to how they assign priorities to each of these job classes, ranging from one providing favorable treatment to local jobs to one providing favorable treatment to remote jobs. Under each policy, the optimal load balancing problem is formulated as an optimization problem with respect to the threshold parameter. The optimal threshold is obtained numerically using matrix-geometric formulation and an iteration method. Last, we consider the effects that the job arrival process can have on performance. One expects that load balancing for systems operating in an environment of bursty job arrivals should be more beneficial than for an environment with random job arrivals. This fact is observed through numerical examples.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {70--77},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317531.317540},
 doi = {http://doi.acm.org/10.1145/317531.317540},
 acmid = {317540},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lee:1986:CPD:317499.317540,
 author = {Lee, Kyoo Jeong and Towsley, Don},
 title = {A comparison of priority-based decentralized load balancing policies},
 abstract = {Load balancing policies in distributed systems divide jobs into two classes; those processed at their of origination (local jobs) and those processed at some other site in the system after being transfered through a communication network (remote jobs). This paper considers a class of decentralized load balancing policies that use a threshold on the local job queue length at each host in making decisions for remote processing. They differ from each other according to how they assign priorities to each of these job classes, ranging from one providing favorable treatment to local jobs to one providing favorable treatment to remote jobs. Under each policy, the optimal load balancing problem is formulated as an optimization problem with respect to the threshold parameter. The optimal threshold is obtained numerically using matrix-geometric formulation and an iteration method. Last, we consider the effects that the job arrival process can have on performance. One expects that load balancing for systems operating in an environment of bursty job arrivals should be more beneficial than for an environment with random job arrivals. This fact is observed through numerical examples.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {70--77},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317499.317540},
 doi = {http://doi.acm.org/10.1145/317499.317540},
 acmid = {317540},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Le Boudec:1986:BEM:317499.317541,
 author = {Le Boudec, Jean-Yves},
 title = {A BCMP extension to multiserver stations with concurrent classes of customers},
 abstract = {We consider a multiclass service station with B identical exponential servers, with constant service rate \&mgr;. At a station, the classes of customers are sorted into M concurrent groups ; the discipline of service is on a first come first served basis, but two customers of the same group cannot be served simultaneously. We show that product form is maintained when such stations are inserted in BCMP networks, and give closed form expressions for the steady-state probabilities.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {78--91},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/317499.317541},
 doi = {http://doi.acm.org/10.1145/317499.317541},
 acmid = {317541},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Le Boudec:1986:BEM:317531.317541,
 author = {Le Boudec, Jean-Yves},
 title = {A BCMP extension to multiserver stations with concurrent classes of customers},
 abstract = {We consider a multiclass service station with B identical exponential servers, with constant service rate \&mgr;. At a station, the classes of customers are sorted into M concurrent groups ; the discipline of service is on a first come first served basis, but two customers of the same group cannot be served simultaneously. We show that product form is maintained when such stations are inserted in BCMP networks, and give closed form expressions for the steady-state probabilities.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {78--91},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/317531.317541},
 doi = {http://doi.acm.org/10.1145/317531.317541},
 acmid = {317541},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Conway:1986:EAS:317531.317542,
 author = {Conway, A. E. and Georganas, N. D.},
 title = {An efficient algorithm for semi-homogeneous queueing network models},
 abstract = {The class of product-form semi-homogeneous queueing networks is introduced as a generalization of the class of homogeneous networks, which has been considered by Balbo et al for the performance modeling of local area networks. In semi-homogeneous networks, the relative traffic intensity at the various shared resources may depend on the routing chain to which a customer belongs. We develop an efficient algorithm for the exact analysis of this class of networks. It is based on the equations which form the foundation of RECAL, a general purpose exact algorithm for multiple-chain closed queueing networks. The complexity of the algorithm is shown to be of order less than exponential in (P-1)<supscrpt>1/2</supscrpt>, where P is the number of processors (workstations) in the network. It is therefore, in general, more efficient than a direct application of either convolution, MVA or RECAL to the class of semi-homogeneous queueing networks. The algorithm presented here may be situated between the algorithms of Balbo et al and the general purpose algorithms, both in terms of its generality and efficiency.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {92--99},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317531.317542},
 doi = {http://doi.acm.org/10.1145/317531.317542},
 acmid = {317542},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Conway:1986:EAS:317499.317542,
 author = {Conway, A. E. and Georganas, N. D.},
 title = {An efficient algorithm for semi-homogeneous queueing network models},
 abstract = {The class of product-form semi-homogeneous queueing networks is introduced as a generalization of the class of homogeneous networks, which has been considered by Balbo et al for the performance modeling of local area networks. In semi-homogeneous networks, the relative traffic intensity at the various shared resources may depend on the routing chain to which a customer belongs. We develop an efficient algorithm for the exact analysis of this class of networks. It is based on the equations which form the foundation of RECAL, a general purpose exact algorithm for multiple-chain closed queueing networks. The complexity of the algorithm is shown to be of order less than exponential in (P-1)<supscrpt>1/2</supscrpt>, where P is the number of processors (workstations) in the network. It is therefore, in general, more efficient than a direct application of either convolution, MVA or RECAL to the class of semi-homogeneous queueing networks. The algorithm presented here may be situated between the algorithms of Balbo et al and the general purpose algorithms, both in terms of its generality and efficiency.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {92--99},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317499.317542},
 doi = {http://doi.acm.org/10.1145/317499.317542},
 acmid = {317542},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nain:1986:OMH:317531.317543,
 author = {Nain, Philippe and Ross, Keith},
 title = {Optimal multiplexing of heterogeneous traffic with hard constraint},
 abstract = {Considered are optimal dynamic policies for multiplexing \&Kgr; + 1 heterogeneous traffic types onto a single communication channel. The packet types arrive to the channel according to independent Poisson processes. The service requirements are exponential with type dependent means. The optimization criterion is to minimize a linear combination of the average delays for packet types 1 to \&Kgr;, while simultaneously subjecting the average delay of type-0 packets to a hard constraint. The optimal multiplexing policy is shown to be a randomized modification of the ``\&mgr;c rule". The optimization problem is thereby reduced to a problem of finding the optimal randomization factor; an algorithm, which can be implemented in real time, is given to do this for two particular cases.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {100--108},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317531.317543},
 doi = {http://doi.acm.org/10.1145/317531.317543},
 acmid = {317543},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nain:1986:OMH:317499.317543,
 author = {Nain, Philippe and Ross, Keith},
 title = {Optimal multiplexing of heterogeneous traffic with hard constraint},
 abstract = {Considered are optimal dynamic policies for multiplexing \&Kgr; + 1 heterogeneous traffic types onto a single communication channel. The packet types arrive to the channel according to independent Poisson processes. The service requirements are exponential with type dependent means. The optimization criterion is to minimize a linear combination of the average delays for packet types 1 to \&Kgr;, while simultaneously subjecting the average delay of type-0 packets to a hard constraint. The optimal multiplexing policy is shown to be a randomized modification of the ``\&mgr;c rule". The optimization problem is thereby reduced to a problem of finding the optimal randomization factor; an algorithm, which can be implemented in real time, is given to do this for two particular cases.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {100--108},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317499.317543},
 doi = {http://doi.acm.org/10.1145/317499.317543},
 acmid = {317543},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sevcik:1986:CTP:317499.317544,
 author = {Sevcik, Kenneth and Johnson, Marjory J.},
 title = {Cycle time properties of the FDDI token ring protocol (extended abstract)},
 abstract = {Communication technology now makes it possible to support high data transmission rates at relatively low cost. In particular, optical fiber can be used as the medium in local area networks with data rates in the range of 100 megabits per second. Unfortunately, local area network topologies and communication protocols that work well with lower speed media are not necessarily appropriate when the data transmission rate is scaled up by approximately an order of magnitude. Recognizing this fact, an ANSI sub-committee (ANSIX3T9) has been working for the past two years on a proposed standard for a token ring protocol tailored to a transmission medium with transmission rate in the 100 megabits per second range. The protocol is referred to as the FDDI (Fiber Distributed Data Interface) Token Ring protocol. The proposal for the standard is now quite mature and nearly stable.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {109--110},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/317499.317544},
 doi = {http://doi.acm.org/10.1145/317499.317544},
 acmid = {317544},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sevcik:1986:CTP:317531.317544,
 author = {Sevcik, Kenneth and Johnson, Marjory J.},
 title = {Cycle time properties of the FDDI token ring protocol (extended abstract)},
 abstract = {Communication technology now makes it possible to support high data transmission rates at relatively low cost. In particular, optical fiber can be used as the medium in local area networks with data rates in the range of 100 megabits per second. Unfortunately, local area network topologies and communication protocols that work well with lower speed media are not necessarily appropriate when the data transmission rate is scaled up by approximately an order of magnitude. Recognizing this fact, an ANSI sub-committee (ANSIX3T9) has been working for the past two years on a proposed standard for a token ring protocol tailored to a transmission medium with transmission rate in the 100 megabits per second range. The protocol is referred to as the FDDI (Fiber Distributed Data Interface) Token Ring protocol. The proposal for the standard is now quite mature and nearly stable.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {109--110},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/317531.317544},
 doi = {http://doi.acm.org/10.1145/317531.317544},
 acmid = {317544},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dallery:1986:ADP:317531.317545,
 author = {Dallery, Yves and Suri, Rajan},
 title = {Approximate disaggregation and performance bounds for queueing networks with multiple-server stations},
 abstract = {We introduce the concept of approximate disaggregation which enables us to replace a station by a subnetwork, i.e. a set of stations, such that the performance of the derived network is close to the performance of the initial network. We use this concept to disaggregate any multiple-server station into a set single-server stations. Using two different disaggregations, we are able to bound the performance of the initial network by the performance of a ``lower" and an ``upper" network each consisting of single-server stations, whose performance can in turn be bounded by the Balanced Job Bounds (or other known bounds). Several examples show the useful information provided by these bounds at a very low cost: for \&Kgr; stations and \&Ngr; customers, the computational complexity here is \&Ogr;(\&Kgr;) which is significantly less than the \&Ogr;(\&Kgr;\&Ngr;<supscrpt>2</supscrpt>) operations required for exact solution. Indeed, despite the multiple server stations, the computational complexity of our bounds is the same as that of Balanced Job Bounds.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {111--128},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/317531.317545},
 doi = {http://doi.acm.org/10.1145/317531.317545},
 acmid = {317545},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate disaggregation, closed queueing networks, performance bounds, product form networks},
} 

@inproceedings{Dallery:1986:ADP:317499.317545,
 author = {Dallery, Yves and Suri, Rajan},
 title = {Approximate disaggregation and performance bounds for queueing networks with multiple-server stations},
 abstract = {We introduce the concept of approximate disaggregation which enables us to replace a station by a subnetwork, i.e. a set of stations, such that the performance of the derived network is close to the performance of the initial network. We use this concept to disaggregate any multiple-server station into a set single-server stations. Using two different disaggregations, we are able to bound the performance of the initial network by the performance of a ``lower" and an ``upper" network each consisting of single-server stations, whose performance can in turn be bounded by the Balanced Job Bounds (or other known bounds). Several examples show the useful information provided by these bounds at a very low cost: for \&Kgr; stations and \&Ngr; customers, the computational complexity here is \&Ogr;(\&Kgr;) which is significantly less than the \&Ogr;(\&Kgr;\&Ngr;<supscrpt>2</supscrpt>) operations required for exact solution. Indeed, despite the multiple server stations, the computational complexity of our bounds is the same as that of Balanced Job Bounds.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {111--128},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/317499.317545},
 doi = {http://doi.acm.org/10.1145/317499.317545},
 acmid = {317545},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate disaggregation, closed queueing networks, performance bounds, product form networks},
} 

@article{Strelen:1986:GMV:317531.317546,
 author = {Strelen, Johann},
 title = {A generalization of mean value analysis to higher moments: moment analysis},
 abstract = {Closed product-form queueing networks are considered. Recursive schemata are proposed for the higher moments of the number of customers in the queues, called ``moment analysis". As with mean value analysis (MVA), in general no state probabilities are needed. Approximation techniques for these schemata similar to those existing for MVA are introduced.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317531.317546},
 doi = {http://doi.acm.org/10.1145/317531.317546},
 acmid = {317546},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Strelen:1986:GMV:317499.317546,
 author = {Strelen, Johann},
 title = {A generalization of mean value analysis to higher moments: moment analysis},
 abstract = {Closed product-form queueing networks are considered. Recursive schemata are proposed for the higher moments of the number of customers in the queues, called ``moment analysis". As with mean value analysis (MVA), in general no state probabilities are needed. Approximation techniques for these schemata similar to those existing for MVA are introduced.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {129--140},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317499.317546},
 doi = {http://doi.acm.org/10.1145/317499.317546},
 acmid = {317546},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Massey:1986:PAD:317499.317547,
 author = {Massey, William A.},
 title = {A probabilistic analysis of a database system},
 abstract = {In Gray, Homan, Obermarck, and Korth [GHOK], the authors give many conjectures based on simulation for the probabilistic analysis of transaction lock-waits and transaction deadlocks. In this paper, we introduce a probabilistic model to explain their observations.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {141--146},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/317499.317547},
 doi = {http://doi.acm.org/10.1145/317499.317547},
 acmid = {317547},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Massey:1986:PAD:317531.317547,
 author = {Massey, William A.},
 title = {A probabilistic analysis of a database system},
 abstract = {In Gray, Homan, Obermarck, and Korth [GHOK], the authors give many conjectures based on simulation for the probabilistic analysis of transaction lock-waits and transaction deadlocks. In this paper, we introduce a probabilistic model to explain their observations.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {141--146},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/317531.317547},
 doi = {http://doi.acm.org/10.1145/317531.317547},
 acmid = {317547},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Witkowski:1986:PEM:317499.317548,
 author = {Witkowski, Andrew},
 title = {Performance evaluation of multiversion with the Oracle synchronization},
 abstract = {In this paper we present a new analytical model for performance measurements of timestamp driven databases. The model is based on two-dimensional Poisson processes where one coordinate represents the real arrival time and the other the timestamp of an arriving messages. The notion of preemption is defined which serves as a model for synchronization. Preemption naturally implies such performance measures as response time and amount of abortion in the system. The concept of oracle is introduced which allows evaluation of a lower bound on the synchronization cost. Preemption and the oracle are then used to evaluate performance of the Multiversion synchronization. We present the distribution and the expectation of the synchronization cost. The analysis is then applied to a database with exponential communication delays (\&agr;) and the intensity of transaction \&lgr;. It is shown that for Multiversion, this cost depends linearly on l/\&agr; and logarithmically on \&lgr;.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {147--158},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317499.317548},
 doi = {http://doi.acm.org/10.1145/317499.317548},
 acmid = {317548},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Witkowski:1986:PEM:317531.317548,
 author = {Witkowski, Andrew},
 title = {Performance evaluation of multiversion with the Oracle synchronization},
 abstract = {In this paper we present a new analytical model for performance measurements of timestamp driven databases. The model is based on two-dimensional Poisson processes where one coordinate represents the real arrival time and the other the timestamp of an arriving messages. The notion of preemption is defined which serves as a model for synchronization. Preemption naturally implies such performance measures as response time and amount of abortion in the system. The concept of oracle is introduced which allows evaluation of a lower bound on the synchronization cost. Preemption and the oracle are then used to evaluate performance of the Multiversion synchronization. We present the distribution and the expectation of the synchronization cost. The analysis is then applied to a database with exponential communication delays (\&agr;) and the intensity of transaction \&lgr;. It is shown that for Multiversion, this cost depends linearly on l/\&agr; and logarithmically on \&lgr;.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {147--158},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317531.317548},
 doi = {http://doi.acm.org/10.1145/317531.317548},
 acmid = {317548},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Swinghal:1986:PAA:317499.317549,
 author = {Swinghal, Mukesh and Agrawala, A. K.},
 title = {Performance analysis of an algorithm for concurrency control in replicated database systems},
 abstract = {In this paper, we analyze the performance of a concurrency control algorithm for replicated database systems. We present a model of a distributed database system which provides a framework to study the performance of different concurrency control algorithms. We discuss performance criteria to evaluate different algorithms. We use the model to analyze the performance of an algorithm for concurrency control in replicated database systems. The technique used in analysis is iterative and approximate. We plot a set of performance measures for several values of the model parameters. The results of analysis are compared against a simulation study.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317499.317549},
 doi = {http://doi.acm.org/10.1145/317499.317549},
 acmid = {317549},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solutions, error analysis, mean value analysis, moment analysis, multiclass queueing networks, product-form solutions},
} 

@article{Swinghal:1986:PAA:317531.317549,
 author = {Swinghal, Mukesh and Agrawala, A. K.},
 title = {Performance analysis of an algorithm for concurrency control in replicated database systems},
 abstract = {In this paper, we analyze the performance of a concurrency control algorithm for replicated database systems. We present a model of a distributed database system which provides a framework to study the performance of different concurrency control algorithms. We discuss performance criteria to evaluate different algorithms. We use the model to analyze the performance of an algorithm for concurrency control in replicated database systems. The technique used in analysis is iterative and approximate. We plot a set of performance measures for several values of the model parameters. The results of analysis are compared against a simulation study.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {159--169},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317531.317549},
 doi = {http://doi.acm.org/10.1145/317531.317549},
 acmid = {317549},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solutions, error analysis, mean value analysis, moment analysis, multiclass queueing networks, product-form solutions},
} 

@article{Haikala:1986:AMP:317531.317550,
 author = {Haikala, Ilkka},
 title = {ARMA models of program behaviour},
 abstract = {In models of virtual memory computer systems, it is generally assumed that the time intervals between the page (or segment) faults, often called lifetimes, are independent from each other. Due to the phase-transition behaviour in many real programs this is not always true, and strong correlations may exist between successive lifetimes. These correlations may have a notable effect on the system behaviour. This paper describes a series of experiments where autoregressive -moving average (ARMA) models are used to describe the correlation structure in sequences of lifetimes. It is shown that many real program executions can be described with models having four parameters only, i.e. with the ARMA(1,1) models. The models can be used as parts of simulation models for instance, and they also give us better understanding about the program behaviour in general.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317531.317550},
 doi = {http://doi.acm.org/10.1145/317531.317550},
 acmid = {317550},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Haikala:1986:AMP:317499.317550,
 author = {Haikala, Ilkka},
 title = {ARMA models of program behaviour},
 abstract = {In models of virtual memory computer systems, it is generally assumed that the time intervals between the page (or segment) faults, often called lifetimes, are independent from each other. Due to the phase-transition behaviour in many real programs this is not always true, and strong correlations may exist between successive lifetimes. These correlations may have a notable effect on the system behaviour. This paper describes a series of experiments where autoregressive -moving average (ARMA) models are used to describe the correlation structure in sequences of lifetimes. It is shown that many real program executions can be described with models having four parameters only, i.e. with the ARMA(1,1) models. The models can be used as parts of simulation models for instance, and they also give us better understanding about the program behaviour in general.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {170--179},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317499.317550},
 doi = {http://doi.acm.org/10.1145/317499.317550},
 acmid = {317550},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Majumdar:1986:MAL:317531.317551,
 author = {Majumdar, Shikharesh and Bunt, Richard B.},
 title = {Measurement and analysis of locality phases in file referencing behaviour},
 abstract = {Recent research has demonstrated the existence of locality in short-term file referencing behaviour. A detailed study of the dynamic characteristics of file referencing is presented in this paper. The concept of Bounded Locality Intervals from the field of program behaviour has been used to model the locality phases of file referencing behaviour. The model is found to be powerful both from a descriptive point of view and from the perspective of understanding the performance implications of locality properties of file referencing behaviour on file system management.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {180--192},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/317531.317551},
 doi = {http://doi.acm.org/10.1145/317531.317551},
 acmid = {317551},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Majumdar:1986:MAL:317499.317551,
 author = {Majumdar, Shikharesh and Bunt, Richard B.},
 title = {Measurement and analysis of locality phases in file referencing behaviour},
 abstract = {Recent research has demonstrated the existence of locality in short-term file referencing behaviour. A detailed study of the dynamic characteristics of file referencing is presented in this paper. The concept of Bounded Locality Intervals from the field of program behaviour has been used to model the locality phases of file referencing behaviour. The model is found to be powerful both from a descriptive point of view and from the perspective of understanding the performance implications of locality properties of file referencing behaviour on file system management.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {180--192},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/317499.317551},
 doi = {http://doi.acm.org/10.1145/317499.317551},
 acmid = {317551},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Razouk:1986:MOS:317531.317552,
 author = {Razouk, Rami R. and Stewart, Terri and Wilson, Michael},
 title = {Measuring operating system performance on modern micro-processors},
 abstract = {The use of micro-processors and commercial operating systems in real-time applications demands a good understanding of factors which influence software performance. Advances in micro-processor design (e.g. pipelining) make performance prediction based on instruction cycle counts difficult. In addition, the increasing complexity of operating systems raises doubts about our ability to ensure that their performance will meet system requirements. Performance measurement is more important than ever.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {193--202},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317531.317552},
 doi = {http://doi.acm.org/10.1145/317531.317552},
 acmid = {317552},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Razouk:1986:MOS:317499.317552,
 author = {Razouk, Rami R. and Stewart, Terri and Wilson, Michael},
 title = {Measuring operating system performance on modern micro-processors},
 abstract = {The use of micro-processors and commercial operating systems in real-time applications demands a good understanding of factors which influence software performance. Advances in micro-processor design (e.g. pipelining) make performance prediction based on instruction cycle counts difficult. In addition, the increasing complexity of operating systems raises doubts about our ability to ensure that their performance will meet system requirements. Performance measurement is more important than ever.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {193--202},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317499.317552},
 doi = {http://doi.acm.org/10.1145/317499.317552},
 acmid = {317552},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Nicola:1986:QAF:317499.317553,
 author = {Nicola, Victor F. and Kulkarni, V. G. and Trivedi, Kishor S.},
 title = {Queueing analysis of fault-tolerant computer systems (extended abstract)},
 abstract = {Queueing models provide a useful tool for predicting the performance of many service systems including computer systems, telecommunication systems, computer/communication networks and flexible manufacturing systems. Traditional queueing models predict system performance under the assumption that all service facilities provide failure-free service. It must, however, be acknowledged that service facilities do experience failures and that they get repaired. In recent years, it has been increasingly recognized that this separation of performance and reliability/availability models is no longer adequate.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {203--},
 url = {http://doi.acm.org/10.1145/317499.317553},
 doi = {http://doi.acm.org/10.1145/317499.317553},
 acmid = {317553},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Nicola:1986:QAF:317531.317553,
 author = {Nicola, Victor F. and Kulkarni, V. G. and Trivedi, Kishor S.},
 title = {Queueing analysis of fault-tolerant computer systems (extended abstract)},
 abstract = {Queueing models provide a useful tool for predicting the performance of many service systems including computer systems, telecommunication systems, computer/communication networks and flexible manufacturing systems. Traditional queueing models predict system performance under the assumption that all service facilities provide failure-free service. It must, however, be acknowledged that service facilities do experience failures and that they get repaired. In recent years, it has been increasingly recognized that this separation of performance and reliability/availability models is no longer adequate.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {203--},
 url = {http://doi.acm.org/10.1145/317531.317553},
 doi = {http://doi.acm.org/10.1145/317531.317553},
 acmid = {317553},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coffman:1986:ACQ:317499.317554,
 author = {Coffman,Jr., E. G. and Gelenbe, E. and Gilbert, E. N.},
 title = {Analysis of a conveyor queue in a flexible manufacturing system},
 abstract = {In a flexible manufacturing system stations are arranged along a common conveyor that brings items for processing to the stations and also carries away the processed items. At each station specialized robots automatically load and unload items on and off the conveyor. We examine here a single station in such a system. A new kind of queueing problem arises, with input-output dependencies that result because the same conveyor transports items both to and from the station. The paper analyzes two models of a station. Model 1 has one robot that cannot return a processed item to the conveyor while unloading a new item for processing. Model 2 has two robots to allow simultaneous loading and unloading of the conveyor. A principal goal of the analysis is the proper choice of the distance separating the two points at which items leave and rejoin the conveyor.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {204--223},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/317499.317554},
 doi = {http://doi.acm.org/10.1145/317499.317554},
 acmid = {317554},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Coffman:1986:ACQ:317531.317554,
 author = {Coffman,Jr., E. G. and Gelenbe, E. and Gilbert, E. N.},
 title = {Analysis of a conveyor queue in a flexible manufacturing system},
 abstract = {In a flexible manufacturing system stations are arranged along a common conveyor that brings items for processing to the stations and also carries away the processed items. At each station specialized robots automatically load and unload items on and off the conveyor. We examine here a single station in such a system. A new kind of queueing problem arises, with input-output dependencies that result because the same conveyor transports items both to and from the station. The paper analyzes two models of a station. Model 1 has one robot that cannot return a processed item to the conveyor while unloading a new item for processing. Model 2 has two robots to allow simultaneous loading and unloading of the conveyor. A principal goal of the analysis is the proper choice of the distance separating the two points at which items leave and rejoin the conveyor.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {204--223},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/317531.317554},
 doi = {http://doi.acm.org/10.1145/317531.317554},
 acmid = {317554},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kouvatsos:1986:MEQ:317531.317555,
 author = {Kouvatsos, Demetres D.},
 title = {A maximum entropy queue length distribution for the G/G/1 finite capacity queue},
 abstract = {A new ``hybrid" analytic framework, based on the principle of maximum entropy, is used to approximate the queue length distribution of a G/G/1 finite buffer queue. Robust recursive relations are derived and asymptotic connections to the infinite capacity queue are established. Furthermore, ``equivalence" principles are applied to analyse two-stage cyclic queues with general service times and favourable comparisons with global balance solutions are made. Numerical examples provide useful information on how critically system behaviour is affected by the distributional form of interarrival and service patterns. It is shown that the maximum entropy solution predicts the bottleneck ``anomaly" and also it defines bounds on system performance. Comments on the implication of the work to the analysis and aggregation of computer systems are included.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {224--236},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/317531.317555},
 doi = {http://doi.acm.org/10.1145/317531.317555},
 acmid = {317555},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kouvatsos:1986:MEQ:317499.317555,
 author = {Kouvatsos, Demetres D.},
 title = {A maximum entropy queue length distribution for the G/G/1 finite capacity queue},
 abstract = {A new ``hybrid" analytic framework, based on the principle of maximum entropy, is used to approximate the queue length distribution of a G/G/1 finite buffer queue. Robust recursive relations are derived and asymptotic connections to the infinite capacity queue are established. Furthermore, ``equivalence" principles are applied to analyse two-stage cyclic queues with general service times and favourable comparisons with global balance solutions are made. Numerical examples provide useful information on how critically system behaviour is affected by the distributional form of interarrival and service patterns. It is shown that the maximum entropy solution predicts the bottleneck ``anomaly" and also it defines bounds on system performance. Comments on the implication of the work to the analysis and aggregation of computer systems are included.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {224--236},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/317499.317555},
 doi = {http://doi.acm.org/10.1145/317499.317555},
 acmid = {317555},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Takagi:1986:QAN:317531.317556,
 author = {Takagi, Hideaki and Murata, Masayuki},
 title = {Queueing analysis of nonpreemptive reservation priority discipline},
 abstract = {Analysis is given to a nonpreemptive priority queueing system with P classes of messages where the class of message to be served next is the highest priority class waiting at the time of service start. (If this were the highest priority class waiting at the service completion epoch, we would have a classical nonpreemptive head-of-line priority queueing system.) We assume that the message service time distribution is identical for all classes. The mean message waiting time is obtained explicitly for each class, and numerically compared to the values in the corresponding head-of-line system. We have also proposed and evaluated a fairness measure to demonstrate the degree of discrimination. This model can be applied to the performance analysis of the prioritized token-ring scheme in local area computer networks when the propagation delay and bit latency are negligible compared to the frame transmission time.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {237--244},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317531.317556},
 doi = {http://doi.acm.org/10.1145/317531.317556},
 acmid = {317556},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Takagi:1986:QAN:317499.317556,
 author = {Takagi, Hideaki and Murata, Masayuki},
 title = {Queueing analysis of nonpreemptive reservation priority discipline},
 abstract = {Analysis is given to a nonpreemptive priority queueing system with P classes of messages where the class of message to be served next is the highest priority class waiting at the time of service start. (If this were the highest priority class waiting at the service completion epoch, we would have a classical nonpreemptive head-of-line priority queueing system.) We assume that the message service time distribution is identical for all classes. The mean message waiting time is obtained explicitly for each class, and numerically compared to the values in the corresponding head-of-line system. We have also proposed and evaluated a fairness measure to demonstrate the degree of discrimination. This model can be applied to the performance analysis of the prioritized token-ring scheme in local area computer networks when the propagation delay and bit latency are negligible compared to the frame transmission time.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {237--244},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317499.317556},
 doi = {http://doi.acm.org/10.1145/317499.317556},
 acmid = {317556},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hofri:1986:QSP:317499.317557,
 author = {Hofri, Micha},
 title = {Queueing systems with a procrastinating server},
 abstract = {Two related problems are analyzed and discussed:
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {245--253},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317499.317557},
 doi = {http://doi.acm.org/10.1145/317499.317557},
 acmid = {317557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hofri:1986:QSP:317531.317557,
 author = {Hofri, Micha},
 title = {Queueing systems with a procrastinating server},
 abstract = {Two related problems are analyzed and discussed:
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {245--253},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317531.317557},
 doi = {http://doi.acm.org/10.1145/317531.317557},
 acmid = {317557},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Boxma:1986:WAC:317499.317558,
 author = {Boxma, O. J. and Meister, B.},
 title = {Waiting-time approximations for cyclic-service systems with switch-over times},
 abstract = {Mean waiting-time approximations are derived for a single-server multi-queue system with nonexhaustive cyclic service. Non-zero switch-over times of the server between consecutive queues are assumed. The main tool used in the derivation is a pseudo-conservation law recently found by Watson. The approximation is simpler and, as extensive simulations show, more accurate than existing approximations. Moreover, it gives very good insight into the qualitative behavior of cyclic-service queueing systems.
},
 booktitle = {Proceedings of the 1986 ACM SIGMETRICS joint international conference on Computer performance modelling, measurement and evaluation},
 series = {SIGMETRICS '86/PERFORMANCE '86},
 year = {1986},
 isbn = {0-89791-184-9},
 location = {Raleigh, North Carolina, United States},
 pages = {254--262},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317499.317558},
 doi = {http://doi.acm.org/10.1145/317499.317558},
 acmid = {317558},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Boxma:1986:WAC:317531.317558,
 author = {Boxma, O. J. and Meister, B.},
 title = {Waiting-time approximations for cyclic-service systems with switch-over times},
 abstract = {Mean waiting-time approximations are derived for a single-server multi-queue system with nonexhaustive cyclic service. Non-zero switch-over times of the server between consecutive queues are assumed. The main tool used in the derivation is a pseudo-conservation law recently found by Watson. The approximation is simpler and, as extensive simulations show, more accurate than existing approximations. Moreover, it gives very good insight into the qualitative behavior of cyclic-service queueing systems.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {14},
 issue = {1},
 month = {May},
 year = {1986},
 issn = {0163-5999},
 pages = {254--262},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317531.317558},
 doi = {http://doi.acm.org/10.1145/317531.317558},
 acmid = {317558},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eager:1985:CRS:317795.317802,
 author = {Eager, Derek L. and Lazowska, Edward D. and Zahorjan, John},
 title = {A comparison of receiver-initiated and sender-initiated adaptive load sharing (extended abstract)},
 abstract = {One goal of locally distributed systems is to facilitate resource sharing. Most current locally distributed systems, however, share primarily data, data storage devices, and output devices; there is little sharing of computational resources. Load sharing is the process of sharing computational resources by transparently distributing the system workload. System performance can be improved by transferring work from nodes that are heavily loaded to nodes that are lightly loaded.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {1--3},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/317795.317802},
 doi = {http://doi.acm.org/10.1145/317795.317802},
 acmid = {317802},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Eager:1985:CRS:317786.317802,
 author = {Eager, Derek L. and Lazowska, Edward D. and Zahorjan, John},
 title = {A comparison of receiver-initiated and sender-initiated adaptive load sharing (extended abstract)},
 abstract = {One goal of locally distributed systems is to facilitate resource sharing. Most current locally distributed systems, however, share primarily data, data storage devices, and output devices; there is little sharing of computational resources. Load sharing is the process of sharing computational resources by transparently distributing the system workload. System performance can be improved by transferring work from nodes that are heavily loaded to nodes that are lightly loaded.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {1--3},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/317786.317802},
 doi = {http://doi.acm.org/10.1145/317786.317802},
 acmid = {317802},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelernter:1985:ACP:317795.317803,
 author = {Gelernter, David and Podar, Sunil and Badr, Hussein G.},
 title = {An adaptive communications protocol for network computers (extended abstract)},
 abstract = {A network computer is a collection of computers designed to function as one machine. On a network computer, as opposed to a multiprocessor, constituent subcomputers are memory-disjoint and communicate only by some form of message exchange. Ensemble architectures like multiprocessors and network computers are of growing interest because of their capacity to support parallel programs, where a parallel program is one that is made up of many simultaneously-active, communicating processes. Parallel programs should, on an appropriate architecture, run faster than sequential programs, and, indeed, good speed-ups have been reported in parallel programming experiments in several domains, amongst which are AI, numerical problems, and system simulation. Our interest lies in network computers, particularly ones that range in size from several hundred nodes to several thousand.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {4--5},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/317795.317803},
 doi = {http://doi.acm.org/10.1145/317795.317803},
 acmid = {317803},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gelernter:1985:ACP:317786.317803,
 author = {Gelernter, David and Podar, Sunil and Badr, Hussein G.},
 title = {An adaptive communications protocol for network computers (extended abstract)},
 abstract = {A network computer is a collection of computers designed to function as one machine. On a network computer, as opposed to a multiprocessor, constituent subcomputers are memory-disjoint and communicate only by some form of message exchange. Ensemble architectures like multiprocessors and network computers are of growing interest because of their capacity to support parallel programs, where a parallel program is one that is made up of many simultaneously-active, communicating processes. Parallel programs should, on an appropriate architecture, run faster than sequential programs, and, indeed, good speed-ups have been reported in parallel programming experiments in several domains, amongst which are AI, numerical problems, and system simulation. Our interest lies in network computers, particularly ones that range in size from several hundred nodes to several thousand.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {4--5},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/317786.317803},
 doi = {http://doi.acm.org/10.1145/317786.317803},
 acmid = {317803},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gelenbe:1985:ADC:317786.317804,
 author = {Gelenbe, Erol and Finkel, David and Tripathi, Satish K.},
 title = {On the availability of a distributed computer system with failing components},
 abstract = {We present a model for distributed systems with failing components. Each node may fail and during its recovery the load is distributed to other nodes that are operational. The model assumes periodic checkpointing for error recovery and testing of the status of other nodes for the distribution of load.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {6--13},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317786.317804},
 doi = {http://doi.acm.org/10.1145/317786.317804},
 acmid = {317804},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelenbe:1985:ADC:317795.317804,
 author = {Gelenbe, Erol and Finkel, David and Tripathi, Satish K.},
 title = {On the availability of a distributed computer system with failing components},
 abstract = {We present a model for distributed systems with failing components. Each node may fail and during its recovery the load is distributed to other nodes that are operational. The model assumes periodic checkpointing for error recovery and testing of the status of other nodes for the distribution of load.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {6--13},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317795.317804},
 doi = {http://doi.acm.org/10.1145/317795.317804},
 acmid = {317804},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Conway:1985:RNE:317786.317805,
 author = {Conway, A. E. and Georganas, N. D.},
 title = {RECAL\&mdash;a new efficient algorithm for the exact analysis of multiple-chain closed queueing networks (abstract)},
 abstract = {RECAL, a Recursion by Chain Algorithm for computing the mean performance measures of product-form multiple-chain closed queueing networks, is presented. It is based on a new recursive expression which relates the normalization constant of a network with r closed routing chains to those of a set of networks having (r-l) chains. It relies on the artifice of breaking down each chain into constituent sub-chains that each have a population of one. The time and space requirements of the algorithm are shown to be polynomial in the number of chains. When the network contains many routing chains the proposed algorithm is substantially more efficient than the convolution or mean value analysis algorithms. The algorithm therefore extends the range of queueing networks which can be analyzed efficiently by exact means. A numerical example is given.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {14--},
 url = {http://doi.acm.org/10.1145/317786.317805},
 doi = {http://doi.acm.org/10.1145/317786.317805},
 acmid = {317805},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Conway:1985:RNE:317795.317805,
 author = {Conway, A. E. and Georganas, N. D.},
 title = {RECAL\&mdash;a new efficient algorithm for the exact analysis of multiple-chain closed queueing networks (abstract)},
 abstract = {RECAL, a Recursion by Chain Algorithm for computing the mean performance measures of product-form multiple-chain closed queueing networks, is presented. It is based on a new recursive expression which relates the normalization constant of a network with r closed routing chains to those of a set of networks having (r-l) chains. It relies on the artifice of breaking down each chain into constituent sub-chains that each have a population of one. The time and space requirements of the algorithm are shown to be polynomial in the number of chains. When the network contains many routing chains the proposed algorithm is substantially more efficient than the convolution or mean value analysis algorithms. The algorithm therefore extends the range of queueing networks which can be analyzed efficiently by exact means. A numerical example is given.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {14--},
 url = {http://doi.acm.org/10.1145/317795.317805},
 doi = {http://doi.acm.org/10.1145/317795.317805},
 acmid = {317805},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Balbo:1985:MPS:317795.317806,
 author = {Balbo, G. and Bruell, S. C. and Ghanta, S.},
 title = {Modeling priority schemes},
 abstract = {We develop Generalized Stochastic Petri Net models for several priority queueing disciplines. The building blocks of these models are explained and many variants are easily derivable from them. We then combine these building blocks with product-form queueing network models. Numerical results are provided that illustrate the effectiveness of the method.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317795.317806},
 doi = {http://doi.acm.org/10.1145/317795.317806},
 acmid = {317806},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {generalized stochastic Petri nets, head-of-the-line, preemptive resume, priorities, product-form queueing networks, reorientation, time-out},
} 

@article{Balbo:1985:MPS:317786.317806,
 author = {Balbo, G. and Bruell, S. C. and Ghanta, S.},
 title = {Modeling priority schemes},
 abstract = {We develop Generalized Stochastic Petri Net models for several priority queueing disciplines. The building blocks of these models are explained and many variants are easily derivable from them. We then combine these building blocks with product-form queueing network models. Numerical results are provided that illustrate the effectiveness of the method.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317786.317806},
 doi = {http://doi.acm.org/10.1145/317786.317806},
 acmid = {317806},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {generalized stochastic Petri nets, head-of-the-line, preemptive resume, priorities, product-form queueing networks, reorientation, time-out},
} 

@article{Walstra:1985:NNQ:317786.317807,
 author = {Walstra, Robbe J.},
 title = {Nonexponential networks of queues: a maximum entropy analysis},
 abstract = {We will propose a new, iterative method for approximately analyzing closed networks of queues with nonexponential service time distributions and FCFS scheduling. Our method is based on the Principle of Maximum Entropy and produces results which, first, are consistent with the fundamental Work Rate Theorem and, second, are exact for separable networks of queues. Considering accuracy and execution time characteristics, our method offers a viable alternative to Marie's homogeneous approximation method.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {27--37},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317786.317807},
 doi = {http://doi.acm.org/10.1145/317786.317807},
 acmid = {317807},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Walstra:1985:NNQ:317795.317807,
 author = {Walstra, Robbe J.},
 title = {Nonexponential networks of queues: a maximum entropy analysis},
 abstract = {We will propose a new, iterative method for approximately analyzing closed networks of queues with nonexponential service time distributions and FCFS scheduling. Our method is based on the Principle of Maximum Entropy and produces results which, first, are consistent with the fundamental Work Rate Theorem and, second, are exact for separable networks of queues. Considering accuracy and execution time characteristics, our method offers a viable alternative to Marie's homogeneous approximation method.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {27--37},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317795.317807},
 doi = {http://doi.acm.org/10.1145/317795.317807},
 acmid = {317807},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Calzarossa:1985:SSC:317795.317808,
 author = {Calzarossa, Maria and Ferrari, Domenico},
 title = {A sensitivity study of the clustering approach to workload modeling (extended abstract)},
 abstract = {In a paper published in 1984 [Ferr84], the validity of applying clustering techniques to the design of an executable model for an interactive workload was discussed. The following assumptions, intended not to be necessarily realistic but to provide sufficient conditions for the applicability of clustering techniques, were made:
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {38--39},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/317795.317808},
 doi = {http://doi.acm.org/10.1145/317795.317808},
 acmid = {317808},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Calzarossa:1985:SSC:317786.317808,
 author = {Calzarossa, Maria and Ferrari, Domenico},
 title = {A sensitivity study of the clustering approach to workload modeling (extended abstract)},
 abstract = {In a paper published in 1984 [Ferr84], the validity of applying clustering techniques to the design of an executable model for an interactive workload was discussed. The following assumptions, intended not to be necessarily realistic but to provide sufficient conditions for the applicability of clustering techniques, were made:
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {38--39},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/317786.317808},
 doi = {http://doi.acm.org/10.1145/317786.317808},
 acmid = {317808},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Raghavan:1985:CIU:317795.317809,
 author = {Raghavan, S. V. and Kalyanakrishnan, R.},
 title = {On the classification of interactive user behaviour indices},
 abstract = {The concepts of user behaviour entropy and user behaviour mobility are proposed as indices for the description of user behaviour. The user behaviour indices are derivable from the mode probability vector and the mode transition matrix which adequately describe the behaviour dynamics of an interactive user. The user behaviour indices reduce the ((n*n)+n) dimensional parameter space to two dimensions only for classification, without loss of information related to the user behaviour dynamics. The classification of the users in an interactive educational environment using the user behaviour indices is presented as a case study.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {40--48},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317795.317809},
 doi = {http://doi.acm.org/10.1145/317795.317809},
 acmid = {317809},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raghavan:1985:CIU:317786.317809,
 author = {Raghavan, S. V. and Kalyanakrishnan, R.},
 title = {On the classification of interactive user behaviour indices},
 abstract = {The concepts of user behaviour entropy and user behaviour mobility are proposed as indices for the description of user behaviour. The user behaviour indices are derivable from the mode probability vector and the mode transition matrix which adequately describe the behaviour dynamics of an interactive user. The user behaviour indices reduce the ((n*n)+n) dimensional parameter space to two dimensions only for classification, without loss of information related to the user behaviour dynamics. The classification of the users in an interactive educational environment using the user behaviour indices is presented as a case study.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {40--48},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317786.317809},
 doi = {http://doi.acm.org/10.1145/317786.317809},
 acmid = {317809},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Verkamo:1985:ERL:317786.317810,
 author = {Verkamo, A. Inkeri},
 title = {Empirical results on locality in database referencing},
 abstract = {Database referencing behaviour is analyzed with respect to locality features. The analysis is based on database reference strings collected from several runs of typical batch programs accessing a real database. Locality of reference is measured by the stack distance probability distribution, the number of block faults, and a locality measure based on the memory reservation size.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {49--58},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317786.317810},
 doi = {http://doi.acm.org/10.1145/317786.317810},
 acmid = {317810},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Verkamo:1985:ERL:317795.317810,
 author = {Verkamo, A. Inkeri},
 title = {Empirical results on locality in database referencing},
 abstract = {Database referencing behaviour is analyzed with respect to locality features. The analysis is based on database reference strings collected from several runs of typical batch programs accessing a real database. Locality of reference is measured by the stack distance probability distribution, the number of block faults, and a locality measure based on the memory reservation size.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {49--58},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317795.317810},
 doi = {http://doi.acm.org/10.1145/317795.317810},
 acmid = {317810},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Khelalfa:1985:DCS:317786.317811,
 author = {Khelalfa, Halin M. and von Mayrhauser, Anneliese K.},
 title = {Degradable computer systems with dependent subsystems},
 abstract = {When building a model for degradable computer systems, it is not sufficient to merely quantify reliability and performance measures. These indices must be mathematically sound if they are to be used to design such systems in an optimal way. The paper presents an analysis of design optimisation for degradable computer systems and shows how this particular application leads to a system model with interdepedent subsystems. A procedure is presented on how to solve the resulting Markov model. Its computational complexity is compared to another solution method and shown to be largely more efficient.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {59--68},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317786.317811},
 doi = {http://doi.acm.org/10.1145/317786.317811},
 acmid = {317811},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Khelalfa:1985:DCS:317795.317811,
 author = {Khelalfa, Halin M. and von Mayrhauser, Anneliese K.},
 title = {Degradable computer systems with dependent subsystems},
 abstract = {When building a model for degradable computer systems, it is not sufficient to merely quantify reliability and performance measures. These indices must be mathematically sound if they are to be used to design such systems in an optimal way. The paper presents an analysis of design optimisation for degradable computer systems and shows how this particular application leads to a system model with interdepedent subsystems. A procedure is presented on how to solve the resulting Markov model. Its computational complexity is compared to another solution method and shown to be largely more efficient.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {59--68},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317795.317811},
 doi = {http://doi.acm.org/10.1145/317795.317811},
 acmid = {317811},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chillarege:1985:ESW:317795.317812,
 author = {Chillarege, Ram and Lyer, Ravishankar K.},
 title = {The effect of system workload on error latency: an experimental study},
 abstract = {In this paper, a methodology for determining and characterizing error latency is developed. The method is based on real workload data, gathered by an experiment instrumented on a VAX 11/780 during the normal workload cycle of the installation. This is the first attempt at jointly studying error latency and workload variations in a full production system. Distributions of error latency were generated by simulating the occurrence of faults under varying workload conditions. A family of error latency distributions so generated illustrate that error latency is not so much a function of when in time a fault occurred but rather a function of the workload that followed the failure. The study finds that the mean error latency varies by a 1 to 8 (hours) ratio between high and low workloads. The method is general and can be applied to any system.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {69--77},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317795.317812},
 doi = {http://doi.acm.org/10.1145/317795.317812},
 acmid = {317812},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chillarege:1985:ESW:317786.317812,
 author = {Chillarege, Ram and Lyer, Ravishankar K.},
 title = {The effect of system workload on error latency: an experimental study},
 abstract = {In this paper, a methodology for determining and characterizing error latency is developed. The method is based on real workload data, gathered by an experiment instrumented on a VAX 11/780 during the normal workload cycle of the installation. This is the first attempt at jointly studying error latency and workload variations in a full production system. Distributions of error latency were generated by simulating the occurrence of faults under varying workload conditions. A family of error latency distributions so generated illustrate that error latency is not so much a function of when in time a fault occurred but rather a function of the workload that followed the failure. The study finds that the mean error latency varies by a 1 to 8 (hours) ratio between high and low workloads. The method is general and can be applied to any system.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {69--77},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317786.317812},
 doi = {http://doi.acm.org/10.1145/317786.317812},
 acmid = {317812},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gonsalves:1985:PCT:317795.317813,
 author = {Gonsalves, Timothy A.},
 title = {Performance characteristics of two Ethernets: an experimental study},
 abstract = {Local computer networks are increasing in popularity for the interconnection of computers for a variety of applications. One such network that has been implemented on a large scale is the Ethernet. This paper describes an experimental performance evaluation of a 3 and a 10 Mb/s Ethernet. The effects of varying packet length and transmission speed on throughput, mean delay and delay distribution are quantified. The protocols are seen to be fair and stable. These measurements span the range from the region of high performance of the CSMA/CD protocol to the upper limits of its utility where performance is degraded. The measurements are compared to the predictions of existing analytical models. The correlation is found to range from good to poor, with more sophisticated models yielding better results than a simple one.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {78--86},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317795.317813},
 doi = {http://doi.acm.org/10.1145/317795.317813},
 acmid = {317813},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gonsalves:1985:PCT:317786.317813,
 author = {Gonsalves, Timothy A.},
 title = {Performance characteristics of two Ethernets: an experimental study},
 abstract = {Local computer networks are increasing in popularity for the interconnection of computers for a variety of applications. One such network that has been implemented on a large scale is the Ethernet. This paper describes an experimental performance evaluation of a 3 and a 10 Mb/s Ethernet. The effects of varying packet length and transmission speed on throughput, mean delay and delay distribution are quantified. The protocols are seen to be fair and stable. These measurements span the range from the region of high performance of the CSMA/CD protocol to the upper limits of its utility where performance is degraded. The measurements are compared to the predictions of existing analytical models. The correlation is found to range from good to poor, with more sophisticated models yielding better results than a simple one.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {78--86},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317786.317813},
 doi = {http://doi.acm.org/10.1145/317786.317813},
 acmid = {317813},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chlamtac:1985:PIS:317786.317814,
 author = {Chlamtac, I. and Eisinger, M.},
 title = {Performance of integrated services (voice/data) CSMA/CD networks},
 abstract = {We consider a voice/data integrated local area communication system. Due to the high suitability of CSMA/CD protocols for data communication and the existence of real time voice delay constraints we consider a hybrid TDM/CSMA/CD protocol. This model fundamentally differs from the very well documented voice/data integrated systems in point to point networks in which both voice and data users are assigned fixed duration time slots for transmission. The TDM/CSMA/CD integrated system performance is analysed and basic performance tradeoffs in the system design are manifested.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {87--93},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/317786.317814},
 doi = {http://doi.acm.org/10.1145/317786.317814},
 acmid = {317814},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chlamtac:1985:PIS:317795.317814,
 author = {Chlamtac, I. and Eisinger, M.},
 title = {Performance of integrated services (voice/data) CSMA/CD networks},
 abstract = {We consider a voice/data integrated local area communication system. Due to the high suitability of CSMA/CD protocols for data communication and the existence of real time voice delay constraints we consider a hybrid TDM/CSMA/CD protocol. This model fundamentally differs from the very well documented voice/data integrated systems in point to point networks in which both voice and data users are assigned fixed duration time slots for transmission. The TDM/CSMA/CD integrated system performance is analysed and basic performance tradeoffs in the system design are manifested.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {87--93},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/317795.317814},
 doi = {http://doi.acm.org/10.1145/317795.317814},
 acmid = {317814},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Chlamtac:1985:AMH:317795.317815,
 author = {Chlamtac, I. and Eisinger, M.},
 title = {An analytic model of the hyperchannel network using multiple channel architecture},
 abstract = {The HYPERchannel communication network configured around one to four channels is considered. We develop a queueing model which characterizes the network performance as a function of the number of channels, the channel load and the number of stations in the network. The model is used to analyze the multichannel system performance and to evaluate the effect of the channel selection mechanism, as implemented by the HYPERchannel station interface units, on the performance.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {94--104},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317795.317815},
 doi = {http://doi.acm.org/10.1145/317795.317815},
 acmid = {317815},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Chlamtac:1985:AMH:317786.317815,
 author = {Chlamtac, I. and Eisinger, M.},
 title = {An analytic model of the hyperchannel network using multiple channel architecture},
 abstract = {The HYPERchannel communication network configured around one to four channels is considered. We develop a queueing model which characterizes the network performance as a function of the number of channels, the channel load and the number of stations in the network. The model is used to analyze the multichannel system performance and to evaluate the effect of the channel selection mechanism, as implemented by the HYPERchannel station interface units, on the performance.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {94--104},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317786.317815},
 doi = {http://doi.acm.org/10.1145/317786.317815},
 acmid = {317815},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bleistein:1985:APM:317795.317816,
 author = {Bleistein, Sandra and Cho, Shin-Sun and Goettge, Robert T.},
 title = {Analytic performance model of the U.S. en route air traffic control computer systems},
 abstract = {An analytic performance modeling case study of a complex command and control computer system is presented. A queueing network model of the system was developed and validated. Features of the model found to be critical to its accuracy were detailed software models, general service time distributions, and models of transient response time behavior. Response time prediction accuracy of the model was validated to 20 percent for moderate device utilizations. The study shows that analytic techniques can be successfully applied to performance modeling of complex systems. Prediction of response time percentile values and modeling of transient effects are identified as two areas where improved analytic techniques would enhance performance engineering of such systems.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {105--115},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317795.317816},
 doi = {http://doi.acm.org/10.1145/317795.317816},
 acmid = {317816},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bleistein:1985:APM:317786.317816,
 author = {Bleistein, Sandra and Cho, Shin-Sun and Goettge, Robert T.},
 title = {Analytic performance model of the U.S. en route air traffic control computer systems},
 abstract = {An analytic performance modeling case study of a complex command and control computer system is presented. A queueing network model of the system was developed and validated. Features of the model found to be critical to its accuracy were detailed software models, general service time distributions, and models of transient response time behavior. Response time prediction accuracy of the model was validated to 20 percent for moderate device utilizations. The study shows that analytic techniques can be successfully applied to performance modeling of complex systems. Prediction of response time percentile values and modeling of transient effects are identified as two areas where improved analytic techniques would enhance performance engineering of such systems.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {105--115},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317786.317816},
 doi = {http://doi.acm.org/10.1145/317786.317816},
 acmid = {317816},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dowdy:1985:AUM:317795.317817,
 author = {Dowdy, Lawrence W. and Chopra, Manvinder S.},
 title = {On the applicability of using multiprogramming level distributions},
 abstract = {A computer system's workload is represented by its multiprogramming level, which is defined as the number of tasks (jobs, customers) which actively compete for resources within the system. In a product-form queuing network model of the system, the workload is modeled by assuming that the multiprogramming level is either fixed (i.e., closed model) or that the multiprogramming level depends upon an outside arrival process (i.e., open model). However, in many actual systems, closed and open models are both inappropriate since the multiprogramming level is neither fixed nor governed by an outside arrival process.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317795.317817},
 doi = {http://doi.acm.org/10.1145/317795.317817},
 acmid = {317817},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blocking, multiprogramming level distributions, open and closed queuing networks, subsystem modeling},
} 

@article{Dowdy:1985:AUM:317786.317817,
 author = {Dowdy, Lawrence W. and Chopra, Manvinder S.},
 title = {On the applicability of using multiprogramming level distributions},
 abstract = {A computer system's workload is represented by its multiprogramming level, which is defined as the number of tasks (jobs, customers) which actively compete for resources within the system. In a product-form queuing network model of the system, the workload is modeled by assuming that the multiprogramming level is either fixed (i.e., closed model) or that the multiprogramming level depends upon an outside arrival process (i.e., open model). However, in many actual systems, closed and open models are both inappropriate since the multiprogramming level is neither fixed nor governed by an outside arrival process.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {116--127},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317786.317817},
 doi = {http://doi.acm.org/10.1145/317786.317817},
 acmid = {317817},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blocking, multiprogramming level distributions, open and closed queuing networks, subsystem modeling},
} 

@article{Krzesinski:1985:MQN:317786.317818,
 author = {Krzesinski, A. E. and Teunissen, P.},
 title = {Multiclass queueing networks with population constrainted subnetworks},
 abstract = {A Multiclass Queueing Network model (MQN) is partitioned into a set of disjoint subnetworks. Population constraints are applied to each subnetwork such that within each subnetwork each population chain is either subject to an individual population constraint, or a group of chains may be subject to a common (shared) population constraint. Such population constraints are necessary in order to model multiprogramming level constraints in mainframe computer systems and window flow control mechanisms in computer communication networks. A computationally efficient approximate solution method is developed for solving MQN's with population constraints. Each subnetwork is reduced to a single approximately flow equivalent composite centre by assuming that the effect of other chains on a given chain can be adequately represented by their average customer populations. The accuracy of the population constraint approximation is compared against previous techniques by applying it to a set of test cases for which simulation solutions have previously been reported. The accuracy of the approximation technique is found to be good and in general is an improvement over previously published concurrency constraint approximations.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317786.317818},
 doi = {http://doi.acm.org/10.1145/317786.317818},
 acmid = {317818},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solution, mean value analysis, multiclass queueing networks, product form solutions},
} 

@inproceedings{Krzesinski:1985:MQN:317795.317818,
 author = {Krzesinski, A. E. and Teunissen, P.},
 title = {Multiclass queueing networks with population constrainted subnetworks},
 abstract = {A Multiclass Queueing Network model (MQN) is partitioned into a set of disjoint subnetworks. Population constraints are applied to each subnetwork such that within each subnetwork each population chain is either subject to an individual population constraint, or a group of chains may be subject to a common (shared) population constraint. Such population constraints are necessary in order to model multiprogramming level constraints in mainframe computer systems and window flow control mechanisms in computer communication networks. A computationally efficient approximate solution method is developed for solving MQN's with population constraints. Each subnetwork is reduced to a single approximately flow equivalent composite centre by assuming that the effect of other chains on a given chain can be adequately represented by their average customer populations. The accuracy of the population constraint approximation is compared against previous techniques by applying it to a set of test cases for which simulation solutions have previously been reported. The accuracy of the approximation technique is found to be good and in general is an improvement over previously published concurrency constraint approximations.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {128--139},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317795.317818},
 doi = {http://doi.acm.org/10.1145/317795.317818},
 acmid = {317818},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solution, mean value analysis, multiclass queueing networks, product form solutions},
} 

@inproceedings{Branwajn:1985:NSI:317795.317986,
 author = {Branwajn, Alexandre and Jow, Yung-Li Lily},
 title = {A note on service interruptions},
 abstract = {This note is devoted to a few remarks on the performance evaluation of systems with service interruptions such as priority queues for lower priority customers, systems subject to breakdowns, etc. Recent work on priority queues has shown that a popular approximation method, the ``reduced occupancy approximation", can be exceedingly inaccurate for a range of parameter values. We identify a cause of inaccuracy and, hence, propose a simple correction that provides a substantial improvement in the results. Using the example of a simple model with service interruptions, we show also that conditional probabilities can be of value in deriving recurrent solutions to some problems.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {140--148},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317795.317986},
 doi = {http://doi.acm.org/10.1145/317795.317986},
 acmid = {317986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Branwajn:1985:NSI:317786.317986,
 author = {Branwajn, Alexandre and Jow, Yung-Li Lily},
 title = {A note on service interruptions},
 abstract = {This note is devoted to a few remarks on the performance evaluation of systems with service interruptions such as priority queues for lower priority customers, systems subject to breakdowns, etc. Recent work on priority queues has shown that a popular approximation method, the ``reduced occupancy approximation", can be exceedingly inaccurate for a range of parameter values. We identify a cause of inaccuracy and, hence, propose a simple correction that provides a substantial improvement in the results. Using the example of a simple model with service interruptions, we show also that conditional probabilities can be of value in deriving recurrent solutions to some problems.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {140--148},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317786.317986},
 doi = {http://doi.acm.org/10.1145/317786.317986},
 acmid = {317986},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Plateau:1985:SSP:317786.317819,
 author = {Plateau, Brigitte},
 title = {On the stochastic structure of parallelism and synchronization models for distributed algorithms},
 abstract = {In this paper a new technique to handle complex Markov models is presented. This method is based on a description using stochastic automatas and is dedicated to distributed algorithms modelling. One example of a mutual exclusion algorithm in a distributed environment is extensively analysed. The mathematical analysis is based on tensor algebra for matrices.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {147--154},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317786.317819},
 doi = {http://doi.acm.org/10.1145/317786.317819},
 acmid = {317819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Plateau:1985:SSP:317795.317819,
 author = {Plateau, Brigitte},
 title = {On the stochastic structure of parallelism and synchronization models for distributed algorithms},
 abstract = {In this paper a new technique to handle complex Markov models is presented. This method is based on a description using stochastic automatas and is dedicated to distributed algorithms modelling. One example of a mutual exclusion algorithm in a distributed environment is extensively analysed. The mathematical analysis is based on tensor algebra for matrices.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {147--154},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/317795.317819},
 doi = {http://doi.acm.org/10.1145/317795.317819},
 acmid = {317819},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Snyder:1985:ANS:317795.317820,
 author = {Snyder, Patricia M. and Stewart, William J.},
 title = {An approximate numerical solution for multiclass preemptive priority queues with general service time distributions},
 abstract = {In this paper an approximate numerical solution for a multiclass preemptive priority single server queue is developed. The arrival process of each class follows a Poisson distribution. The service time distribution must have a rational Laplace transform, but is otherwise arbitrary and may be different for different classes. The work reported here was motivated by a desire to compute the equilibrium probability distribution of networks containing preemptive priority servers. Such networks are frequently encountered when modeling computer systems, medical care delivery systems and communication networks. We wish to use an iterative technique which constructs a series of two station networks consisting of one station from the original network and one ``complementary" station whose behavior with respect to the original station mimics that of the rest of the network. At each iteration, it is necessary to compute the equilibrium probability distribution of one or more preemptive priority queues.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {155--165},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317795.317820},
 doi = {http://doi.acm.org/10.1145/317795.317820},
 acmid = {317820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Snyder:1985:ANS:317786.317820,
 author = {Snyder, Patricia M. and Stewart, William J.},
 title = {An approximate numerical solution for multiclass preemptive priority queues with general service time distributions},
 abstract = {In this paper an approximate numerical solution for a multiclass preemptive priority single server queue is developed. The arrival process of each class follows a Poisson distribution. The service time distribution must have a rational Laplace transform, but is otherwise arbitrary and may be different for different classes. The work reported here was motivated by a desire to compute the equilibrium probability distribution of networks containing preemptive priority servers. Such networks are frequently encountered when modeling computer systems, medical care delivery systems and communication networks. We wish to use an iterative technique which constructs a series of two station networks consisting of one station from the original network and one ``complementary" station whose behavior with respect to the original station mimics that of the rest of the network. At each iteration, it is necessary to compute the equilibrium probability distribution of one or more preemptive priority queues.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {155--165},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/317786.317820},
 doi = {http://doi.acm.org/10.1145/317786.317820},
 acmid = {317820},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hevner:1985:EOD:317786.317821,
 author = {Hevner, Alan R.},
 title = {Evaluation of optical disk systems for very large database applications},
 abstract = {Optical Disk Systems have significant advantages over conventional magnetic mass storage media for very large database applications. Among other features, optical disk systems offer large capacity and high transfer rate. A critical problem is how to integrate the optical disk system into a total application system environment while maintaining the high performance capabilities of the optical disk. In this paper the performance of optical disk system configurations under realistic application environments is analyzed via queueing models. The results provide several important guidelines for the use of optical disk systems on large applications.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {166--172},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/317786.317821},
 doi = {http://doi.acm.org/10.1145/317786.317821},
 acmid = {317821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hevner:1985:EOD:317795.317821,
 author = {Hevner, Alan R.},
 title = {Evaluation of optical disk systems for very large database applications},
 abstract = {Optical Disk Systems have significant advantages over conventional magnetic mass storage media for very large database applications. Among other features, optical disk systems offer large capacity and high transfer rate. A critical problem is how to integrate the optical disk system into a total application system environment while maintaining the high performance capabilities of the optical disk. In this paper the performance of optical disk system configurations under realistic application environments is analyzed via queueing models. The results provide several important guidelines for the use of optical disk systems on large applications.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {166--172},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/317795.317821},
 doi = {http://doi.acm.org/10.1145/317795.317821},
 acmid = {317821},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Houtekamer:1985:LDC:317786.317822,
 author = {Houtekamer, Gilbert E.},
 title = {The local disk controller},
 abstract = {The performance of the I/O subsystem in the 370-XA architecture has been improved considerably with the introduction of the new channel subsystem, as compared to the System/370 architecture. The emphasis in the 370-XA architecture is on reducing the CPU load associated with I/O, and on reducing the congestion in multi-CPU, shared systems, by redesigning the channel system.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {173--182},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317786.317822},
 doi = {http://doi.acm.org/10.1145/317786.317822},
 acmid = {317822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Houtekamer:1985:LDC:317795.317822,
 author = {Houtekamer, Gilbert E.},
 title = {The local disk controller},
 abstract = {The performance of the I/O subsystem in the 370-XA architecture has been improved considerably with the introduction of the new channel subsystem, as compared to the System/370 architecture. The emphasis in the 370-XA architecture is on reducing the CPU load associated with I/O, and on reducing the congestion in multi-CPU, shared systems, by redesigning the channel system.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {173--182},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/317795.317822},
 doi = {http://doi.acm.org/10.1145/317795.317822},
 acmid = {317822},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yu:1985:MCC:317795.317823,
 author = {Yu, Philip S. and Dias, Daniel M. and Robinson, John T. and Iyer, Balakrishna R. and Cornell, Douglas},
 title = {Modelling of centralized concurrency control in a multi-system environment},
 abstract = {The performance of multiple systems sharing a common data base is analyzed for an architecture with concurrency control using a centralized lock engine. The workload is based on traces from large mainframe systems running IBM's IMS database management system. Based on IMS lock traces the lock contention probability and data base buffer invalidation effect in a multi-system environment is predicted. Workload parameters are generated for use in event-driven simulation models that examine the overall performance of multi-system data sharing, and to determine the performance impact of various system parameters and design alternatives. While performance results are presented for realistic system parameters, the emphasis is on the methodology, approximate analysis technique and on examining the factors that affect multi-system performance.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {183--191},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317795.317823},
 doi = {http://doi.acm.org/10.1145/317795.317823},
 acmid = {317823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yu:1985:MCC:317786.317823,
 author = {Yu, Philip S. and Dias, Daniel M. and Robinson, John T. and Iyer, Balakrishna R. and Cornell, Douglas},
 title = {Modelling of centralized concurrency control in a multi-system environment},
 abstract = {The performance of multiple systems sharing a common data base is analyzed for an architecture with concurrency control using a centralized lock engine. The workload is based on traces from large mainframe systems running IBM's IMS database management system. Based on IMS lock traces the lock contention probability and data base buffer invalidation effect in a multi-system environment is predicted. Workload parameters are generated for use in event-driven simulation models that examine the overall performance of multi-system data sharing, and to determine the performance impact of various system parameters and design alternatives. While performance results are presented for realistic system parameters, the emphasis is on the methodology, approximate analysis technique and on examining the factors that affect multi-system performance.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {183--191},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/317786.317823},
 doi = {http://doi.acm.org/10.1145/317786.317823},
 acmid = {317823},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thomasian:1985:AOC:317795.317824,
 author = {Thomasian, Alexander and Ryu, In Kyung},
 title = {Analysis of some optimistic concurrency control schemes based on certification},
 abstract = {Optimistic Concurrency Control-OCC schemes based on certification are analyzed in this paper. We allow two types of data access schemes referred to as static and dynamic. According to the first (second) scheme a transaction reads all the required data items at the beginning of its processing (on demand during its processing), respectively. After completing its processing, each transaction is checked as to whether it has encountered a data conflict. Validated transactions commit; otherwise, they are restarted. A variant of the regular (silent) commit scheme where a committing transaction notifies conflicted transactions to restart immediately (broadcast commit scheme) is also considered. We use an iterative method to analyze the performance of OCC schemes in the framework of a system with a fixed number of transactions in multiple classes with given probabilities for their occurrence. The iterative method is validated against simulation and shown to be highly accurate even for high data contention. We present graphs/tables, which are used to determine how system performance is affected by: (i) various OCC schemes, (ii) transaction size, i.e., number of data items accessed, (iii) number of transactions, (iv) the distribution of transaction processing time requirements, (v) the throughput characteristic of the system, and (vi) granule placement.
},
 booktitle = {Proceedings of the 1985 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '85},
 year = {1985},
 isbn = {0-89791-169-5},
 location = {Austin, Texas, United States},
 pages = {192--203},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317795.317824},
 doi = {http://doi.acm.org/10.1145/317795.317824},
 acmid = {317824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thomasian:1985:AOC:317786.317824,
 author = {Thomasian, Alexander and Ryu, In Kyung},
 title = {Analysis of some optimistic concurrency control schemes based on certification},
 abstract = {Optimistic Concurrency Control-OCC schemes based on certification are analyzed in this paper. We allow two types of data access schemes referred to as static and dynamic. According to the first (second) scheme a transaction reads all the required data items at the beginning of its processing (on demand during its processing), respectively. After completing its processing, each transaction is checked as to whether it has encountered a data conflict. Validated transactions commit; otherwise, they are restarted. A variant of the regular (silent) commit scheme where a committing transaction notifies conflicted transactions to restart immediately (broadcast commit scheme) is also considered. We use an iterative method to analyze the performance of OCC schemes in the framework of a system with a fixed number of transactions in multiple classes with given probabilities for their occurrence. The iterative method is validated against simulation and shown to be highly accurate even for high data contention. We present graphs/tables, which are used to determine how system performance is affected by: (i) various OCC schemes, (ii) transaction size, i.e., number of data items accessed, (iii) number of transactions, (iv) the distribution of transaction processing time requirements, (v) the throughput characteristic of the system, and (vi) granule placement.
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {13},
 issue = {2},
 month = {August},
 year = {1985},
 issn = {0163-5999},
 pages = {192--203},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/317786.317824},
 doi = {http://doi.acm.org/10.1145/317786.317824},
 acmid = {317824},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coffman:1984:RPP:800264.809308,
 author = {Coffman,Jr., E. G.},
 title = {Recent progress in the performance evaluation of fundamental allocation algorithms},
 abstract = {Our understanding of several allocation algorithms basic to operating systems and to data base systems has improved substantially as a result of a number of research efforts within the past one or two years. The results have stirred considerable excitement in both theorists and practitioners. This is not only because of the inroads made into long-standing problems, but also because of the surprising nature of the results; in particular, we refer to proofs that certain classical algorithms described as approximate are in fact optimal in a strong probabilistic sense. The work discussed here will be classified according to the application areas, archival and dynamic storage allocation. In both cases we are concerned with the packing problems that arise in making efficient use of storage. Equivalents of the archival problems also have importance in scheduling applications [4]; however, we shall focus exclusively on the storage allocation setting.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {2--6},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/800264.809308},
 doi = {http://doi.acm.org/10.1145/800264.809308},
 acmid = {809308},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Coffman:1984:RPP:1031382.809308,
 author = {Coffman,Jr., E. G.},
 title = {Recent progress in the performance evaluation of fundamental allocation algorithms},
 abstract = {Our understanding of several allocation algorithms basic to operating systems and to data base systems has improved substantially as a result of a number of research efforts within the past one or two years. The results have stirred considerable excitement in both theorists and practitioners. This is not only because of the inroads made into long-standing problems, but also because of the surprising nature of the results; in particular, we refer to proofs that certain classical algorithms described as approximate are in fact optimal in a strong probabilistic sense. The work discussed here will be classified according to the application areas, archival and dynamic storage allocation. In both cases we are concerned with the packing problems that arise in making efficient use of storage. Equivalents of the archival problems also have importance in scheduling applications [4]; however, we shall focus exclusively on the storage allocation setting.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {2--6},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/1031382.809308},
 doi = {http://doi.acm.org/10.1145/1031382.809308},
 acmid = {809308},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ferrari:1984:FAW:1031382.809309,
 author = {Ferrari, Domenico},
 title = {On the foundations of artificial workload design},
 abstract = {The principles on which artificial workload model design is currently based are reviewed. Design methods are found wanting for three main reasons: their resource orientation, with the selection of resources often unrelated to the performance impact of resource demands; their avoiding to define an accuracy criterion for the resulting workload model; and their ignoring the dynamics of the workload to be modeled. An attempt at establishing conceptual foundations for the design of interactive artificial workloads is described. The problems found in current design methods are taken into account, and sufficient conditions for the applicability of these methods are determined. The study also provides guidance for some of the decisions to be made in workload model design using one of the current methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {8--14},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1031382.809309},
 doi = {http://doi.acm.org/10.1145/1031382.809309},
 acmid = {809309},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ferrari:1984:FAW:800264.809309,
 author = {Ferrari, Domenico},
 title = {On the foundations of artificial workload design},
 abstract = {The principles on which artificial workload model design is currently based are reviewed. Design methods are found wanting for three main reasons: their resource orientation, with the selection of resources often unrelated to the performance impact of resource demands; their avoiding to define an accuracy criterion for the resulting workload model; and their ignoring the dynamics of the workload to be modeled. An attempt at establishing conceptual foundations for the design of interactive artificial workloads is described. The problems found in current design methods are taken into account, and sufficient conditions for the applicability of these methods are determined. The study also provides guidance for some of the decisions to be made in workload model design using one of the current methods.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {8--14},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800264.809309},
 doi = {http://doi.acm.org/10.1145/800264.809309},
 acmid = {809309},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Perez-Davila:1984:PIF:1031382.809310,
 author = {Perez-Davila, Alfredo de J. and Dowdy, Lawrence W.},
 title = {Parameter interdependencies of file placement models in a Unix system},
 abstract = {A file assignment case study of a computer system running Unix is presented. A queueing network model of the system is constructed and validated. A modeling technique for the movement of files between and within disks is proposed. A detailed queueing network model is constructed for several file distributions in secondary storage. The interdependencies between the speed of the CPU, the swapping activity, the visit ratios and the multiprogramming level are examined and included in the modeling technique. The models predict the performance of several possible file assignments. The various file assignments are implemented and comparisons between the predicted and actual performance are made. The models are shown to accurately predict user response time.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1031382.809310},
 doi = {http://doi.acm.org/10.1145/1031382.809310},
 acmid = {809310},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Perez-Davila:1984:PIF:800264.809310,
 author = {Perez-Davila, Alfredo de J. and Dowdy, Lawrence W.},
 title = {Parameter interdependencies of file placement models in a Unix system},
 abstract = {A file assignment case study of a computer system running Unix is presented. A queueing network model of the system is constructed and validated. A modeling technique for the movement of files between and within disks is proposed. A detailed queueing network model is constructed for several file distributions in secondary storage. The interdependencies between the speed of the CPU, the swapping activity, the visit ratios and the multiprogramming level are examined and included in the modeling technique. The models predict the performance of several possible file assignments. The various file assignments are implemented and comparisons between the predicted and actual performance are made. The models are shown to accurately predict user response time.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/800264.809310},
 doi = {http://doi.acm.org/10.1145/800264.809310},
 acmid = {809310},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bunt:1984:MPL:800264.809311,
 author = {Bunt, Richard B. and Murphy, Jennifer M. and Majumdar, Shikharesh},
 title = {A measure of program locality and its application},
 abstract = {Although the phenomenon of locality has long been recognized as the single most important characteristic of program behaviour, relatively little work has been done in attempting to measure it. Recent work has led to the development of an intrinsic measure of program locality based on the Bradford-Zipf distribution. Potential applications for such a measure are many, and include the evaluation of program restructuring methods (manual and automatic), the prediction of system performance, the validation of program behaviour models, and the enhanced understanding of the phenomena that characterize program behaviour. A consideration of each of these areas is given in connection with the proposed measure, both to increase confidence in the validity of the measure and to illustrate a methodology for dealing with such problems.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {28--40},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/800264.809311},
 doi = {http://doi.acm.org/10.1145/800264.809311},
 acmid = {809311},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bunt:1984:MPL:1031382.809311,
 author = {Bunt, Richard B. and Murphy, Jennifer M. and Majumdar, Shikharesh},
 title = {A measure of program locality and its application},
 abstract = {Although the phenomenon of locality has long been recognized as the single most important characteristic of program behaviour, relatively little work has been done in attempting to measure it. Recent work has led to the development of an intrinsic measure of program locality based on the Bradford-Zipf distribution. Potential applications for such a measure are many, and include the evaluation of program restructuring methods (manual and automatic), the prediction of system performance, the validation of program behaviour models, and the enhanced understanding of the phenomena that characterize program behaviour. A consideration of each of these areas is given in connection with the proposed measure, both to increase confidence in the validity of the measure and to illustrate a methodology for dealing with such problems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {28--40},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1031382.809311},
 doi = {http://doi.acm.org/10.1145/1031382.809311},
 acmid = {809311},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Krzesinski:1984:ILM:1031382.809312,
 author = {Krzesinski, A. and Greyling, J.},
 title = {Improved lineariser methods for queueing networks with queue dependent centres},
 abstract = {The Lineariser is an MVA-based technique developed for the approximate solution of large multiclass product form queueing networks. The Lineariser is capable of computing accurate solutions for networks of fixed rate centres. However, problems arise when the Lineariser is applied to networks containing centres with queue dependent service rates. Thus networks exist which seem well suited (a large number of lightly loaded centres, large numbers of customers in each closed chain) for Lineariser solution but whose queue dependent centres cannot be solved accurately by the Lineariser method. Examples have also been found where the Lineariser computes accurate values for the queue lengths, waiting times and throughputs though the values computed for the queue length distributions are totally in error. This paper presents an Improved Lineariser which computes accurate approximate solutions for multiclass networks containing an arbitrary number of queue dependent centres. The Improved Lineariser is based on MVA results and is therefore simple to implement and numerically well behaved. The Improved Lineariser has storage and computation requirements of order (MN) locations and (MNJ<supscrpt>2</supscrpt>) arithmetic operations where M is the number of centres, N the total number of customers and J the number of closed chains. Results from 130 randomly generated test networks are used to compare the accuracy of the standard and Improved Linearisers. The Improved Lineariser is consistently more accurate (tolerance errors on all performance measures less than 2 per cent) than the standard Lineariser and its accuracy is insensitive to the size of the network model. In addition, the Improved Lineariser computes accurate solutions for networks which cause the standard Lineariser to fail.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {41--51},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1031382.809312},
 doi = {http://doi.acm.org/10.1145/1031382.809312},
 acmid = {809312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Approximate solutions, Error analysis, Mean value analysis, Multiclass queueing networks, Product from solutions},
} 

@inproceedings{Krzesinski:1984:ILM:800264.809312,
 author = {Krzesinski, A. and Greyling, J.},
 title = {Improved lineariser methods for queueing networks with queue dependent centres},
 abstract = {The Lineariser is an MVA-based technique developed for the approximate solution of large multiclass product form queueing networks. The Lineariser is capable of computing accurate solutions for networks of fixed rate centres. However, problems arise when the Lineariser is applied to networks containing centres with queue dependent service rates. Thus networks exist which seem well suited (a large number of lightly loaded centres, large numbers of customers in each closed chain) for Lineariser solution but whose queue dependent centres cannot be solved accurately by the Lineariser method. Examples have also been found where the Lineariser computes accurate values for the queue lengths, waiting times and throughputs though the values computed for the queue length distributions are totally in error. This paper presents an Improved Lineariser which computes accurate approximate solutions for multiclass networks containing an arbitrary number of queue dependent centres. The Improved Lineariser is based on MVA results and is therefore simple to implement and numerically well behaved. The Improved Lineariser has storage and computation requirements of order (MN) locations and (MNJ<supscrpt>2</supscrpt>) arithmetic operations where M is the number of centres, N the total number of customers and J the number of closed chains. Results from 130 randomly generated test networks are used to compare the accuracy of the standard and Improved Linearisers. The Improved Lineariser is consistently more accurate (tolerance errors on all performance measures less than 2 per cent) than the standard Lineariser and its accuracy is insensitive to the size of the network model. In addition, the Improved Lineariser computes accurate solutions for networks which cause the standard Lineariser to fail.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {41--51},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800264.809312},
 doi = {http://doi.acm.org/10.1145/800264.809312},
 acmid = {809312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Approximate solutions, Error analysis, Mean value analysis, Multiclass queueing networks, Product from solutions},
} 

@article{Zahorjan:1984:ILD:1031382.809313,
 author = {Zahorjan, John and Lazowska, Edward D.},
 title = {Incorporating load dependent servers in approximate mean value analysis},
 abstract = {Queueing network performance modelling technology has made tremendous strides in recent years. Two of the most important developments in facilitating the modelling of large and complex systems are hierarchical modelling, in which a single load dependent server is used as a surrogate for a subsystem, and approximate mean value analysis, in which reliable approximate solutions of separable models are efficiently obtained. Unfortunately, there has been no successful marriage of these two developments; that is, existing algorithms for approximate mean value analysis do not accommodate load dependent servers reliably. This paper presents a successful technique for incorporating load dependent servers in approximate mean value analysis. We consider multiple class models in which the service rate of each load dependent server is a function of the queue length at that server. In other words, load dependent center k delivers ``service units" at a total rate of f<subscrpt>@@@@</subscrpt> (n<subscrpt>@@@@</subscrpt>) when n<subscrpt>@@@@</subscrpt> customers are present. We present extensive experimental validation which indicates that our algorithm contributes an average error in response times of less than 1\% compared to the (much more expensive) exact solution. In addition to the practical value of our algorithm, several of the techniques that it employs are of independent interest.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {52--62},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1031382.809313},
 doi = {http://doi.acm.org/10.1145/1031382.809313},
 acmid = {809313},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zahorjan:1984:ILD:800264.809313,
 author = {Zahorjan, John and Lazowska, Edward D.},
 title = {Incorporating load dependent servers in approximate mean value analysis},
 abstract = {Queueing network performance modelling technology has made tremendous strides in recent years. Two of the most important developments in facilitating the modelling of large and complex systems are hierarchical modelling, in which a single load dependent server is used as a surrogate for a subsystem, and approximate mean value analysis, in which reliable approximate solutions of separable models are efficiently obtained. Unfortunately, there has been no successful marriage of these two developments; that is, existing algorithms for approximate mean value analysis do not accommodate load dependent servers reliably. This paper presents a successful technique for incorporating load dependent servers in approximate mean value analysis. We consider multiple class models in which the service rate of each load dependent server is a function of the queue length at that server. In other words, load dependent center k delivers ``service units" at a total rate of f<subscrpt>@@@@</subscrpt> (n<subscrpt>@@@@</subscrpt>) when n<subscrpt>@@@@</subscrpt> customers are present. We present extensive experimental validation which indicates that our algorithm contributes an average error in response times of less than 1\% compared to the (much more expensive) exact solution. In addition to the practical value of our algorithm, several of the techniques that it employs are of independent interest.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {52--62},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800264.809313},
 doi = {http://doi.acm.org/10.1145/800264.809313},
 acmid = {809313},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agrawal:1984:RTP:800264.809314,
 author = {Agrawal, Subhash C. and Buzen, Jeffrey P. and Shum, Annie W.},
 title = {Response Time Preservation: A general technique for developing approximate algorithms for queueing networks},
 abstract = {Response Time Preservation (RTP) is introduced as a general technique for developing approximate analysis procedures for queueing networks. The underlying idea is to replace a subsystem by an equivalent server whose response time in isolation equals that of the entire subsystem in isolation. The RTP based approximations, which belong to the class of decomposition approximations, can be viewed as a dual of the Norton's Theorem approach for solving queueing networks since it matches response times rather than throughputs. The generality of the RTP technique is illustrated by developing solution procedures for several important queueing systems which violate product form assumptions. Examples include FCFS servers with general service times, FCFS servers with different service times for multiple classes, priority scheduling, and distributed systems.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {63--77},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800264.809314},
 doi = {http://doi.acm.org/10.1145/800264.809314},
 acmid = {809314},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1984:RTP:1031382.809314,
 author = {Agrawal, Subhash C. and Buzen, Jeffrey P. and Shum, Annie W.},
 title = {Response Time Preservation: A general technique for developing approximate algorithms for queueing networks},
 abstract = {Response Time Preservation (RTP) is introduced as a general technique for developing approximate analysis procedures for queueing networks. The underlying idea is to replace a subsystem by an equivalent server whose response time in isolation equals that of the entire subsystem in isolation. The RTP based approximations, which belong to the class of decomposition approximations, can be viewed as a dual of the Norton's Theorem approach for solving queueing networks since it matches response times rather than throughputs. The generality of the RTP technique is illustrated by developing solution procedures for several important queueing systems which violate product form assumptions. Examples include FCFS servers with general service times, FCFS servers with different service times for multiple classes, priority scheduling, and distributed systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {63--77},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/1031382.809314},
 doi = {http://doi.acm.org/10.1145/1031382.809314},
 acmid = {809314},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mussi:1984:EPE:800264.809315,
 author = {Mussi, Ph. and Nain, Ph.},
 title = {Evaluation of parallel execution of program tree structures},
 abstract = {We define and evaluate two policies (NA-policy, A-policy) for parallel execution of program tree structures. Via a probabilistic model we analytically determine, for each policy, the Laplace-Stieltjes transform for the tree processing time distribution. The acceleration of the program execution time achieved when adding processors to a single processor environment, is computed and plotted for each policy.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {78--87},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800264.809315},
 doi = {http://doi.acm.org/10.1145/800264.809315},
 acmid = {809315},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mussi:1984:EPE:1031382.809315,
 author = {Mussi, Ph. and Nain, Ph.},
 title = {Evaluation of parallel execution of program tree structures},
 abstract = {We define and evaluate two policies (NA-policy, A-policy) for parallel execution of program tree structures. Via a probabilistic model we analytically determine, for each policy, the Laplace-Stieltjes transform for the tree processing time distribution. The acceleration of the program execution time achieved when adding processors to a single processor environment, is computed and plotted for each policy.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {78--87},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1031382.809315},
 doi = {http://doi.acm.org/10.1145/1031382.809315},
 acmid = {809315},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sanguinetti:1984:POP:1031382.809316,
 author = {Sanguinetti, John},
 title = {Program optimization for a pipelined machine a case study},
 abstract = {The Amdahl 580 processor is a pipelined processor whose performance can be affected by characteristics of the instructions it executes. This paper describes certain optimizations made to a set of system software routines during their development. The optimization effort was driven by the execution frequencies of common paths through the programs in question, and by the execution characteristics of those paths, as shown by a processor simulator. Path optimization itself was done with both general program optimization techniques and with techniques specific to the particular characteristics of the 580's pipeline. Overall, the average execution time for these routines was reduced by over 50\%.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {88--95},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1031382.809316},
 doi = {http://doi.acm.org/10.1145/1031382.809316},
 acmid = {809316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sanguinetti:1984:POP:800264.809316,
 author = {Sanguinetti, John},
 title = {Program optimization for a pipelined machine a case study},
 abstract = {The Amdahl 580 processor is a pipelined processor whose performance can be affected by characteristics of the instructions it executes. This paper describes certain optimizations made to a set of system software routines during their development. The optimization effort was driven by the execution frequencies of common paths through the programs in question, and by the execution characteristics of those paths, as shown by a processor simulator. Path optimization itself was done with both general program optimization techniques and with techniques specific to the particular characteristics of the 580's pipeline. Overall, the average execution time for these routines was reduced by over 50\%.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {88--95},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800264.809316},
 doi = {http://doi.acm.org/10.1145/800264.809316},
 acmid = {809316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Turner:1984:PDB:1031382.809317,
 author = {Turner, Rollins and Schriesheim, Jeffrey and Mitra, Indrajit},
 title = {Performance of a DECnet based disk block server},
 abstract = {This report describes an experimental disk block server implemented for the RSX-11M Operating System using DECnet. The block server allows user programs on one system to access files on a disk physically located on a different system. The actual interface is at the level of physical blocks and IO transfers. Results of basic performance measurements are given, and explained in terms of major components. Performance predictions are made for servers of this type supporting more complex workloads.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {96--104},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1031382.809317},
 doi = {http://doi.acm.org/10.1145/1031382.809317},
 acmid = {809317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Turner:1984:PDB:800264.809317,
 author = {Turner, Rollins and Schriesheim, Jeffrey and Mitra, Indrajit},
 title = {Performance of a DECnet based disk block server},
 abstract = {This report describes an experimental disk block server implemented for the RSX-11M Operating System using DECnet. The block server allows user programs on one system to access files on a disk physically located on a different system. The actual interface is at the level of physical blocks and IO transfers. Results of basic performance measurements are given, and explained in terms of major components. Performance predictions are made for servers of this type supporting more complex workloads.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {96--104},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800264.809317},
 doi = {http://doi.acm.org/10.1145/800264.809317},
 acmid = {809317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stavenow:1984:TCS:800264.809318,
 author = {Stavenow, Bengt},
 title = {Throughput-delay characteristics and stability considerations of the access channel in a mobile telephone system},
 abstract = {In this paper a performance study of the access channel in a cellular mobile telephone system /1/ is presented. The method used in the Cellular System for multiplexing the population of mobile terminals over the access channel is a hybrid between the methods known as CSMA/CD and BTMA. In the paper we extend an analysis of CSMA/CD to accomodate the function of the particular random multiaccess protocol. Results are shown which illustrate the equilibrium channel performance and the approximate stability-througput-delay tradeoff. Finally an estimate of the average message delay is given.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {105--112},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800264.809318},
 doi = {http://doi.acm.org/10.1145/800264.809318},
 acmid = {809318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stavenow:1984:TCS:1031382.809318,
 author = {Stavenow, Bengt},
 title = {Throughput-delay characteristics and stability considerations of the access channel in a mobile telephone system},
 abstract = {In this paper a performance study of the access channel in a cellular mobile telephone system /1/ is presented. The method used in the Cellular System for multiplexing the population of mobile terminals over the access channel is a hybrid between the methods known as CSMA/CD and BTMA. In the paper we extend an analysis of CSMA/CD to accomodate the function of the particular random multiaccess protocol. Results are shown which illustrate the equilibrium channel performance and the approximate stability-througput-delay tradeoff. Finally an estimate of the average message delay is given.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {105--112},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1031382.809318},
 doi = {http://doi.acm.org/10.1145/1031382.809318},
 acmid = {809318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Williams:1984:PQD:800264.809319,
 author = {Williams, Elizabeth},
 title = {Processor queueing disciplines in distributed systems},
 abstract = {A distributed program consists of processes, many of which can execute concurrently on different processors in a distributed system of processors. When several processes from the same or different distributed programs have been assigned to a processor in a distributed system, the processor must select the next process to run. The following two questions are investigated: What is an appropriate method for selecting the next process to run? Under what conditions are substantial gains in performance achieved by an appropriate method of selection? Standard processor queueing disciplines, such as first-come-first-serve and round-robin-fixed-quantum, are studied. The results for four classes of queueing disciplines tested on three problems are presented. These problems were run on a testbed, consisting of a compiler and simulator used to run distributed programs on user-specified architectures.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {113--119},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800264.809319},
 doi = {http://doi.acm.org/10.1145/800264.809319},
 acmid = {809319},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Williams:1984:PQD:1031382.809319,
 author = {Williams, Elizabeth},
 title = {Processor queueing disciplines in distributed systems},
 abstract = {A distributed program consists of processes, many of which can execute concurrently on different processors in a distributed system of processors. When several processes from the same or different distributed programs have been assigned to a processor in a distributed system, the processor must select the next process to run. The following two questions are investigated: What is an appropriate method for selecting the next process to run? Under what conditions are substantial gains in performance achieved by an appropriate method of selection? Standard processor queueing disciplines, such as first-come-first-serve and round-robin-fixed-quantum, are studied. The results for four classes of queueing disciplines tested on three problems are presented. These problems were run on a testbed, consisting of a compiler and simulator used to run distributed programs on user-specified architectures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {113--119},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1031382.809319},
 doi = {http://doi.acm.org/10.1145/1031382.809319},
 acmid = {809319},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stephens:1984:CBH:800264.809320,
 author = {Stephens, Lindsey E. and Dowdy, Lawrence W.},
 title = {Convolutional bound hierarchies},
 abstract = {The time required to find the exact solution of a product-form queueing network model of a computer system can be high. Faster and cheaper methods of solution, such as approximations, are natural alternatives. However, the errors incurred when using an approximation technique should be bounded. Several recent techniques have been developed which provide solution bounds. These bounding techniques have the added benefit that the bounds can be made tighter if extra computational effort is expended. Thus, a smooth tradeoff of cost and accuracy is available. These techniques are based upon mean value analysis. In this paper a new bounding technique based upon the convolution algorithm is presented. It provides a continuous range of cost versus accuracy tradeoffs for both upper and lower bounds. The bounds produced by the technique converge to the exact solution as the computational effort approaches that of convolution. Also, the technique may be used to improve any existing set of bounds.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {120--133},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/800264.809320},
 doi = {http://doi.acm.org/10.1145/800264.809320},
 acmid = {809320},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stephens:1984:CBH:1031382.809320,
 author = {Stephens, Lindsey E. and Dowdy, Lawrence W.},
 title = {Convolutional bound hierarchies},
 abstract = {The time required to find the exact solution of a product-form queueing network model of a computer system can be high. Faster and cheaper methods of solution, such as approximations, are natural alternatives. However, the errors incurred when using an approximation technique should be bounded. Several recent techniques have been developed which provide solution bounds. These bounding techniques have the added benefit that the bounds can be made tighter if extra computational effort is expended. Thus, a smooth tradeoff of cost and accuracy is available. These techniques are based upon mean value analysis. In this paper a new bounding technique based upon the convolution algorithm is presented. It provides a continuous range of cost versus accuracy tradeoffs for both upper and lower bounds. The bounds produced by the technique converge to the exact solution as the computational effort approaches that of convolution. Also, the technique may be used to improve any existing set of bounds.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {120--133},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1031382.809320},
 doi = {http://doi.acm.org/10.1145/1031382.809320},
 acmid = {809320},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Suri:1984:NBB:800264.809321,
 author = {Suri, Rajan and Diehl, Gregory W.},
 title = {A new 'building block' for performance evaluation of queueing networks with finite buffers},
 abstract = {We propose a new 'building block', for analyzing queueing networks. This is a model of a server with a variable buffer-size. Such a model enables efficient analysis of certain queueing networks with blocking due to limited buffer spaces, since it uses only product-form submodels. The technique is extensively tested, and found to be reasonably accurate over a wide range of parameters. Several examples are given, illustrating practical situations for which our model would prove to be a useful performance analysis tool, specially since it is simple to understand, and easy to implement using standard software for closed queueing networks.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {134--142},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800264.809321},
 doi = {http://doi.acm.org/10.1145/800264.809321},
 acmid = {809321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Approximate analysis, Blocking, Performance modelling, Performance prediction, Product form networks, Queueing networks},
} 

@article{Suri:1984:NBB:1031382.809321,
 author = {Suri, Rajan and Diehl, Gregory W.},
 title = {A new 'building block' for performance evaluation of queueing networks with finite buffers},
 abstract = {We propose a new 'building block', for analyzing queueing networks. This is a model of a server with a variable buffer-size. Such a model enables efficient analysis of certain queueing networks with blocking due to limited buffer spaces, since it uses only product-form submodels. The technique is extensively tested, and found to be reasonably accurate over a wide range of parameters. Several examples are given, illustrating practical situations for which our model would prove to be a useful performance analysis tool, specially since it is simple to understand, and easy to implement using standard software for closed queueing networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {134--142},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1031382.809321},
 doi = {http://doi.acm.org/10.1145/1031382.809321},
 acmid = {809321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Approximate analysis, Blocking, Performance modelling, Performance prediction, Product form networks, Queueing networks},
} 

@inproceedings{Lavenberg:1984:SAE:800264.809322,
 author = {Lavenberg, Stephen S.},
 title = {A simple analysis of exclusive and shared lock contention in a database system},
 abstract = {We consider a probabilistic model of locking in a database system in which an arriving transaction is blocked and lost when its lock requests conflict with the locks held by currently executing transactions. Both exclusive and shared locks are considered. We derive a simple asymptotic expression for the probability of blocking which is exact to order 1/N where N is the number of lockable items in the database. This expression reduces to one recently derived by Mitra and Weinberger for the special case where all locks are exclusive.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {143--148},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800264.809322},
 doi = {http://doi.acm.org/10.1145/800264.809322},
 acmid = {809322},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lavenberg:1984:SAE:1031382.809322,
 author = {Lavenberg, Stephen S.},
 title = {A simple analysis of exclusive and shared lock contention in a database system},
 abstract = {We consider a probabilistic model of locking in a database system in which an arriving transaction is blocked and lost when its lock requests conflict with the locks held by currently executing transactions. Both exclusive and shared locks are considered. We derive a simple asymptotic expression for the probability of blocking which is exact to order 1/N where N is the number of lockable items in the database. This expression reduces to one recently derived by Mitra and Weinberger for the special case where all locks are exclusive.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {143--148},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1031382.809322},
 doi = {http://doi.acm.org/10.1145/1031382.809322},
 acmid = {809322},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Becker:1984:MMS:1031382.809323,
 author = {Becker, S. T. and Rege, K. M. and Sengupta, B.},
 title = {A modeling methodology for sizing A computer based system in a netted environment},
 abstract = {This paper describes a hybrid model, combining both analytical and simulation techniques, which was developed to study the performance of a netted computer based system. The computer based system that was modeled is the Facility Assignment and Control System (FACS). This system is presently being deployed within several Bell Operating Companies to inventory and assign central office and outside plant facilities. A key feature of the model is its ability to characterize the dynamic nature of FACS. An understanding of this dynamic nature is necessary in establishing important operational guidelines such as allowable CPU utilization, levels of multiprogramming and priority of transaction processing. In addition, the model allows the user to investigate the sensitivity of the system to a wide range of conditions. Typical study items could include the effect of various load scenarios, ability of the system to meet performance objectives, and different hardware configurations. As part of this paper, both the practical aspects of modeling a netted computer based system and the theoretical development of the hybrid model are considered.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {149--157},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1031382.809323},
 doi = {http://doi.acm.org/10.1145/1031382.809323},
 acmid = {809323},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Becker:1984:MMS:800264.809323,
 author = {Becker, S. T. and Rege, K. M. and Sengupta, B.},
 title = {A modeling methodology for sizing A computer based system in a netted environment},
 abstract = {This paper describes a hybrid model, combining both analytical and simulation techniques, which was developed to study the performance of a netted computer based system. The computer based system that was modeled is the Facility Assignment and Control System (FACS). This system is presently being deployed within several Bell Operating Companies to inventory and assign central office and outside plant facilities. A key feature of the model is its ability to characterize the dynamic nature of FACS. An understanding of this dynamic nature is necessary in establishing important operational guidelines such as allowable CPU utilization, levels of multiprogramming and priority of transaction processing. In addition, the model allows the user to investigate the sensitivity of the system to a wide range of conditions. Typical study items could include the effect of various load scenarios, ability of the system to meet performance objectives, and different hardware configurations. As part of this paper, both the practical aspects of modeling a netted computer based system and the theoretical development of the hybrid model are considered.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {149--157},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800264.809323},
 doi = {http://doi.acm.org/10.1145/800264.809323},
 acmid = {809323},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Peachey:1984:EIS:1031382.809324,
 author = {Peachey, Darwyn R. and Bunt, Richard B. and Williamson, Carey L. and Brecht, Tim B.},
 title = {An experimental investigation of scheduling strategies for UNIX},
 abstract = {The scheduler used in an operating system is an important factor in the performance of the system under heavy load. This paper describes the scheduling philosophy employed in the UNIX operating system and outlines the standard scheduling strategies. Modified strategies which address deficiencies in the standard strategies are described. The effectiveness of these modified strategies is assessed by means of performance experiments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {158--166},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1031382.809324},
 doi = {http://doi.acm.org/10.1145/1031382.809324},
 acmid = {809324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Peachey:1984:EIS:800264.809324,
 author = {Peachey, Darwyn R. and Bunt, Richard B. and Williamson, Carey L. and Brecht, Tim B.},
 title = {An experimental investigation of scheduling strategies for UNIX},
 abstract = {The scheduler used in an operating system is an important factor in the performance of the system under heavy load. This paper describes the scheduling philosophy employed in the UNIX operating system and outlines the standard scheduling strategies. Modified strategies which address deficiencies in the standard strategies are described. The effectiveness of these modified strategies is assessed by means of performance experiments.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {158--166},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800264.809324},
 doi = {http://doi.acm.org/10.1145/800264.809324},
 acmid = {809324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Menasce:1984:PEI:1031382.809325,
 author = {Menasc\'{e}, Daniel A. and Leite, Leonardo Lellis P.},
 title = {Performance evaluation of isolated and interconnected token bus local area networks},
 abstract = {The token bus based local area network, REDPUC, designed and implemented at the Pont\&iacute;ficia Universidade Cat\&oacute;lica do Rio de Janeiro is briefly described. Analytic models are presented, which allow one to obtain an approximation for the average packet delay, as well as exact upper and lower bounds for the same performance measure. A performance evaluation of interconnected local networks is also given.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {167--175},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1031382.809325},
 doi = {http://doi.acm.org/10.1145/1031382.809325},
 acmid = {809325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Menasce:1984:PEI:800264.809325,
 author = {Menasc\'{e}, Daniel A. and Leite, Leonardo Lellis P.},
 title = {Performance evaluation of isolated and interconnected token bus local area networks},
 abstract = {The token bus based local area network, REDPUC, designed and implemented at the Pont\&iacute;ficia Universidade Cat\&oacute;lica do Rio de Janeiro is briefly described. Analytic models are presented, which allow one to obtain an approximation for the average packet delay, as well as exact upper and lower bounds for the same performance measure. A performance evaluation of interconnected local networks is also given.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {167--175},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800264.809325},
 doi = {http://doi.acm.org/10.1145/800264.809325},
 acmid = {809325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agrawal:1984:UAS:800264.809326,
 author = {Agrawal, Subhash C. and Buzen, Jeffrey P. and Thareja, Ashok K.},
 title = {A Unified Approach to Scan Time Analysis of Token Rings and Polling Networks.},
 abstract = {Token rings and multipoint polled lines are two widely used network interconnection techniques. The general concept of cyclic allocation processes is defined and used to characterize token passing and polling in these networks. Scan time, the time to poll all nodes at least once, is an important quantity in the response time analysis of such networks. We derive expressions for the mean and variance of scan times using a direct, operational approach. Resulting expressions are general and are applicable to both exhaustive and non-exhaustive service. The effect of higher level protocols is easily incorporated in the analysis via calculations of constituent quantities. The expression for mean scan time is exact and depends only on the means of message transmission times and arrival rates. The approximate analysis of variance takes into account the correlation between message transmissions at different nodes. Expected level of accuracy is indicated by an example.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {176--185},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800264.809326},
 doi = {http://doi.acm.org/10.1145/800264.809326},
 acmid = {809326},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1984:UAS:1031382.809326,
 author = {Agrawal, Subhash C. and Buzen, Jeffrey P. and Thareja, Ashok K.},
 title = {A Unified Approach to Scan Time Analysis of Token Rings and Polling Networks.},
 abstract = {Token rings and multipoint polled lines are two widely used network interconnection techniques. The general concept of cyclic allocation processes is defined and used to characterize token passing and polling in these networks. Scan time, the time to poll all nodes at least once, is an important quantity in the response time analysis of such networks. We derive expressions for the mean and variance of scan times using a direct, operational approach. Resulting expressions are general and are applicable to both exhaustive and non-exhaustive service. The effect of higher level protocols is easily incorporated in the analysis via calculations of constituent quantities. The expression for mean scan time is exact and depends only on the means of message transmission times and arrival rates. The approximate analysis of variance takes into account the correlation between message transmissions at different nodes. Expected level of accuracy is indicated by an example.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {176--185},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1031382.809326},
 doi = {http://doi.acm.org/10.1145/1031382.809326},
 acmid = {809326},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brandwajn:1984:EAM:800264.809327,
 author = {Brandwajn, Alexandre and McCormack, William M.},
 title = {Efficient approximation for models of multiprogramming with shared domains},
 abstract = {Queueing network models of multiprogramming systems with memory constraints and multiple classes of jobs are important in representing large commercial computer systems. Typically, an exact analytical solution of such models is unavailable, and, given the size of their state space, the solution of models of this type is approached through simulation and/or approximation techniques. Recently, a computationally efficient iterative technique has been proposed by Brandwajn, Lazowska and Zahorjan for models of systems in which each job is subject to a separate memory constraint, i.e., has its own memory domain. In some important applications, it is not unusual, however, to have several jobs of different classes share a single memory ``domain" (e.g., IBM's Information Management System). We present a simple approximate solution to the shared domain problem. The approach is inspired by the recently proposed technique which is complemented by a few approximations to preserve the conceptual simplicity and computational efficiency of this technique. The accuracy of the results is generally in fair agreement with simulation.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {186--194},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800264.809327},
 doi = {http://doi.acm.org/10.1145/800264.809327},
 acmid = {809327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brandwajn:1984:EAM:1031382.809327,
 author = {Brandwajn, Alexandre and McCormack, William M.},
 title = {Efficient approximation for models of multiprogramming with shared domains},
 abstract = {Queueing network models of multiprogramming systems with memory constraints and multiple classes of jobs are important in representing large commercial computer systems. Typically, an exact analytical solution of such models is unavailable, and, given the size of their state space, the solution of models of this type is approached through simulation and/or approximation techniques. Recently, a computationally efficient iterative technique has been proposed by Brandwajn, Lazowska and Zahorjan for models of systems in which each job is subject to a separate memory constraint, i.e., has its own memory domain. In some important applications, it is not unusual, however, to have several jobs of different classes share a single memory ``domain" (e.g., IBM's Information Management System). We present a simple approximate solution to the shared domain problem. The approach is inspired by the recently proposed technique which is complemented by a few approximations to preserve the conceptual simplicity and computational efficiency of this technique. The accuracy of the results is generally in fair agreement with simulation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {186--194},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1031382.809327},
 doi = {http://doi.acm.org/10.1145/1031382.809327},
 acmid = {809327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bondi:1984:RTP:800264.809328,
 author = {Bondi, Andr\'{e} B. and Buzen, Jeffrey P.},
 title = {The response times of priority classes under preemptive resume in M/G/m queues},
 abstract = {Approximations are given for the mean response times of each priority level in a multiple-class multiserver M/G/m queue operating under preemptive resume scheduling. The results have been tested against simulations of systems with two and three priority classes and different numbers of servers.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {195--201},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800264.809328},
 doi = {http://doi.acm.org/10.1145/800264.809328},
 acmid = {809328},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bondi:1984:RTP:1031382.809328,
 author = {Bondi, Andr\'{e} B. and Buzen, Jeffrey P.},
 title = {The response times of priority classes under preemptive resume in M/G/m queues},
 abstract = {Approximations are given for the mean response times of each priority level in a multiple-class multiserver M/G/m queue operating under preemptive resume scheduling. The results have been tested against simulations of systems with two and three priority classes and different numbers of servers.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {195--201},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1031382.809328},
 doi = {http://doi.acm.org/10.1145/1031382.809328},
 acmid = {809328},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thomasian:1984:AQN:1031382.809329,
 author = {Thomasian, Alexander and Bay, Paul},
 title = {Analysis of Queueing Network Models with population size constraints and delayed blocked customers},
 abstract = {Queueing Network Models - QNM's with population size constraints and delayed blocked customers occur due to MultiProgramming Level - MPL constraints in computer systems and window flow-control mechanisms in Computer Communication Networks - CCN's. The computational cost of existing algorithms is unacceptable for large numbers of chains and high population sizes. A fast approximate solution technique based on load concealment is presented to solve such QNM's. The solution procedure is non-iterative in the case of fixed rate Poisson arrivals, while iteration is required in the case of quasi-random arrivals. Each iteration requires the solution of a single chain network of queues comprised of stations visited by each chain. We then present an algorithm to detect saturated chains and determine their maximum throughput. A fast solution algorithm due to Reiser for closed chains is also extended to the case of quasi-random arrivals. The accuracy of the proposed solution techniques is compared to previous techniques by applying it to a test case, reported in the literature, and a set of randomly generated examples.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {12},
 issue = {3},
 month = {January},
 year = {1984},
 issn = {0163-5999},
 pages = {202--216},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/1031382.809329},
 doi = {http://doi.acm.org/10.1145/1031382.809329},
 acmid = {809329},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thomasian:1984:AQN:800264.809329,
 author = {Thomasian, Alexander and Bay, Paul},
 title = {Analysis of Queueing Network Models with population size constraints and delayed blocked customers},
 abstract = {Queueing Network Models - QNM's with population size constraints and delayed blocked customers occur due to MultiProgramming Level - MPL constraints in computer systems and window flow-control mechanisms in Computer Communication Networks - CCN's. The computational cost of existing algorithms is unacceptable for large numbers of chains and high population sizes. A fast approximate solution technique based on load concealment is presented to solve such QNM's. The solution procedure is non-iterative in the case of fixed rate Poisson arrivals, while iteration is required in the case of quasi-random arrivals. Each iteration requires the solution of a single chain network of queues comprised of stations visited by each chain. We then present an algorithm to detect saturated chains and determine their maximum throughput. A fast solution algorithm due to Reiser for closed chains is also extended to the case of quasi-random arrivals. The accuracy of the proposed solution techniques is compared to previous techniques by applying it to a test case, reported in the literature, and a set of randomly generated examples.},
 booktitle = {Proceedings of the 1984 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '84},
 year = {1984},
 isbn = {0-89791-141-5},
 pages = {202--216},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800264.809329},
 doi = {http://doi.acm.org/10.1145/800264.809329},
 acmid = {809329},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brandwajn:1983:SDR:800040.801390,
 author = {Brandwajn, Alexandre},
 title = {A study of dynamic reconnection},
 abstract = {The recently introduced Extended Architecture (XA) of large IBM computer systems includes, in the disk I/O area, the ability for an access to be resumed and completed on a path different from the one on which it has been initiated. The expected disk performance improvement due to this feature - known as Dynamic Path Reconnection - is investigated in this note. Two popular double pathing connection schemes are considered: switched substrings (as in IBM 3380 Dynamic Path Selection) and dual ported devices (as in Amdahl 6280 Dynamic Performance Pathing). A simple classical queueing system is used to model missed revolutions when transfer paths are found busy. The numerical results obtained indicate that, depending on load, a substantial reduction in the average I/O time can be expected with the Dynamic Path Reconnection feature. This reduction can top 30\% under moderately heavy loads. The accuracy of the analytical model for missed reconnections has been checked using discrete-event simulation.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800040.801390},
 doi = {http://doi.acm.org/10.1145/800040.801390},
 acmid = {801390},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bryant:1983:MPR:800040.801391,
 author = {Bryant, Raymond M. and Krzesinski, Anthony E. and Teunissen, Peter},
 title = {The MVA Pre-empt resume priority approximation},
 abstract = {A Mean Value Analysis (MVA) approximation is presented for computing the average performance measures of closed multiclass queueing networks containing non pre-emptive Head Of Line (HOL) and Pre-empt Resume (PR) priority centers. The approximation has the same storage and computational requirements as MVA thus allowing computationally efficient solutions of large priority queueing networks. The accuracy of the MVA PR approximation is systematically investigated and presented in terms of error contour diagrams. The contour diagrams reveal that the approximation can compute the average performance measures of priority networks to within an accuracy of 5 percent for a large range of network parameter values. Accuracy of the method is also compared to Sevcik's shadow approximation and another MVA approximation recently proposed by Chandy and Lakshmi.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {12--27},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/800040.801391},
 doi = {http://doi.acm.org/10.1145/800040.801391},
 acmid = {801391},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Approximate solutions, Error analysis, Mean value analysis, Multiclass queueing networks, Priority queueing networks, Product form solutions},
} 

@inproceedings{Haikala:1983:BPB:800040.801392,
 author = {Haikala, Ilkka J. and Pohjanlahti, Harri},
 title = {On the BLI-model of program behaviour},
 abstract = {The BLI-model of program behaviour is sometimes referred to as an exact measure of the locality structure of the programs. However, only limited experimental data is available of the applicability of the BLI-model. In this paper the characteristics of the BLI-model are studied with relatively large empirical data. References to code segments, data segments and references to paged data segments are used in the experiments. The good coverage percentages reported in earlier studies with data segments, are present in our data also. However, the code referencing behaviour turns out to be totally different. Typically only some 50\% of the execution time is covered with BLI's of acceptable length. This is shown to be a consequence of occasional references to segments not belonging to the phase or joining the phase during the phase. The behaviour of paged data segments is typically nearer to the behaviour of the code segments than that of the data segments. The BLI-model is also compared with the VMIN-algorithm. A relatively close correspondence between reasonably long BLI's and the VMIN-sets is observed. In general, the results reported in this paper indicate, that the BLI-model can be used with data segments only, or when information concerning short stable phases of the execution is sufficient.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {28--38},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800040.801392},
 doi = {http://doi.acm.org/10.1145/800040.801392},
 acmid = {801392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Olken:1983:HMV:800040.801393,
 author = {Olken, Frank},
 title = {HOPT: A myopic version of the STOCHOPT automatic file migration policy},
 abstract = {The STOCHOPT automatic file migration policy (proposed by A.J. Smith) minimizes the expected retention and recall costs of an abitrarily sized file. We consider the application of the STOCHOPT policy to a file system in which the file inter-reference time (IRT) distributions are characterized by strictly monotonically decreasing hazard rates (SDHR) (also known as decreasing failure rates, DFR). We show that in this case the STOCHOPT policy can be simply stated in terms of a scaled hazard rate, i.e., the hazard rate divided by the file size. Such decreasing failure rate distributions have been used by Smith to model empirically observed file inter-reference times.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {39--43},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/800040.801393},
 doi = {http://doi.acm.org/10.1145/800040.801393},
 acmid = {801393},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blau:1983:POP:800040.801394,
 author = {Blau, Ricki},
 title = {Paging on an object-oriented personal computer},
 abstract = {A high-performance personal computing environment must avoid perceptible pauses resulting from many page faults within a short period of time. Our performance goals for a paged virtual memory system for the Smalltalk-80<supscrpt>TM@@@@</supscrpt>; programming environment are both to decrease the average page fault rate and to minimize the pauses caused by clusters of page faults. We have applied program restructuring techniques to the Smalltalk-80 object memory in order to improve the locality of reference. The analysis in this paper considers the clustering of page faults over time and distinguishes between steady-state behavior and phase transitions. We compare the effectiveness of different restructuring strategies in reducing the amount of main memory needed to obtain desired levels of performance.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800040.801394},
 doi = {http://doi.acm.org/10.1145/800040.801394},
 acmid = {801394},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Buzen:1983:SST:800040.801395,
 author = {Buzen, Jeffrey P. and Agrawal, Subhash C.},
 title = {State space transformations in queueing network modeling},
 abstract = {An important problem in queueing network modeling is that of characterizing and analyzing relationships among alternative models of a single system. The problem is approached by developing the concept of a state space transformation, which is a mechanism for expressing the way one model can be mapped into another. After discussing state space transformations in general terms, some important transformations are presented. The usefulness of the technique is demonstrated by developing state space transformations for the shadow CPU technique for analyzing preemptive priority scheduling and the aggregate server method for modeling serialization delays.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {55--69},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800040.801395},
 doi = {http://doi.acm.org/10.1145/800040.801395},
 acmid = {801395},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Aggregate server method, Approximation, Metamodeling, Operational analysis, Performance evaluation, Preemptive priority, Product form, Queueing networks, Serialization delays, Shadow CPU algorithm, State space transformations},
} 

@inproceedings{Zahorjan:1983:WRQ:800040.801396,
 author = {Zahorjan, John},
 title = {Workload representations in queueing models of computer systems},
 abstract = {There are two basic representations of workload populations in load independent, separable queueing network models. These correspond to the notions of open and closed classes, an open class being one in which customers may arrive and depart the model, and a closed class being one in which the number of customers is fixed. This paper examines the effect on mean system performance measures of the workload representation chosen. Open and closed representations are compared under the equivalency constraints that they result in identical system throughput or mean system population level for the class being considered. It is shown formally for a limited class of networks that the open representation results in larger system response times than equivalent closed representations, and that one of the closed representations results in the smallest system response time of those considered. Extensive numerical results show for a more general class of models that there is a strict ordering (in terms of system response time) of the natural class representations considered. These results can be used in at least two ways. One is to guide the initial representation of computer system workloads in performance models. The other application is as a component of approximate analysis techniques for queueing models that decompose individual networks into multiple submodels, each of which is then solved in isolation. Here the goal is to represent customer classes in each submodel in a way that is convenient computationally, and that results in performance measures closely matching those observed in the full network. It is this latter application that we assume in this paper. An example of the application of our results to an existing approximation technique is given.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {70--81},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/800040.801396},
 doi = {http://doi.acm.org/10.1145/800040.801396},
 acmid = {801396},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thomasian:1983:DSQ:800040.801397,
 author = {Thomasian, Alexander and Ryu, In Kyung},
 title = {A decomposition solution to the queueing network model of the centralized DBMS with static locking},
 abstract = {The effect of concurrency control methods on the performance of computer systems is analyzed in the context of a centralized database with a static lock request policy, i.e., database transactions should acquire all locks before their activation. In the lock conflict model the L locks required by each transaction are uniformly distributed over the N locks in the database. The computer system is modelled as a queueing network. Two scheduling policies for transaction activation are considered: FCFS with and without skip. In each case the scheduling overhead for scanning the blocked transactions is taken into account. The number of transactions to be scanned is limited by a window size parameter. The system is analyzed using a hierarchical decomposition method, where the highest level model yields the mean user response time. The results of the approximate solution are validated using a detailed simulation, which shows that the analysis based on no resampling of locks is quite accurate and outperforms the simplified analysis with resampling of locks in accuracy. The effect of varying the values of parameters such as transaction size, granularity of locking, scheduling discipline for transaction activation, scheduling overhead, and window size on system performance is investigated.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {82--92},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800040.801397},
 doi = {http://doi.acm.org/10.1145/800040.801397},
 acmid = {801397},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coffman:1983:DAS:800040.801398,
 author = {Coffman,Jr., E. G. and Reiman, M. I.},
 title = {Diffusion approximations for storage processes in computer systems},
 abstract = {In this paper we focus on the storage resource. A basic model of the space time requirements of jobs in a computer system is described, and a number of its variations analyzed by means of diffusion approxmiations. Subject to the usual heavy traffic assumptions, the result of this analysis enable one to quantify the effects of limitations on both storage capacity and processing rates.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {93--117},
 numpages = {25},
 url = {http://doi.acm.org/10.1145/800040.801398},
 doi = {http://doi.acm.org/10.1145/800040.801398},
 acmid = {801398},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Duda:1983:TDA:800040.801399,
 author = {Duda, Andrzej},
 title = {Transient diffusion approximation for some queuening systems.},
 abstract = {There are many situations where information about the transient behaviour of computer systems is wanted. In this paper a diffusion approximation to the transient behaviour of some queueing systems is proposed. The solution of the forward diffusion equation on the real positive line with the elementary return barrier having almost general holding time distribution provides the approximation for the GI/G/1 queue. The solution of the diffusion equation in a finite region approximates the transient behaviour for the two-server cyclic system and gives approximations for the first overflow time in the GI/G/1/N system and for the maximum number of customers in the GI/G/1 system. Also the approximation for the busy period of the GI/G/1 system and of the two-server cyclic system are presented.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {118--128},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800040.801399},
 doi = {http://doi.acm.org/10.1145/800040.801399},
 acmid = {801399},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tantawi:1983:PAC:800040.801400,
 author = {Tantawi, Asser N. and Ruschitzka, Manfred},
 title = {Performance analysis of checkpointing strategies},
 abstract = {A widely used error recovery technique in database systems is the rollback and recovery technique. This technique saves periodically the state of the system and records all activities on a reliable log tape. The operation of saving the system state is called checkpointing. The elapsed time between two consecutive checkpointing operations is called checkpointing interval. When the system fails, the recovery process uses the log tape and the state saved at the most recent checkpoint to bring the system to the correct state that preceded the failure. This process is called error recovery and consists of loading the most recent state and then reprocessing all the activities, stored on the log tape, that took place since the most recent checkpoint and prior to failure. Former models of rollback and recovery assumed Poisson failures and fixed (or exponential) checkpointing intervals. Extending these models, we consider general failure distributions. We also allow checkpointing intervals to depend on the reprocessing time (the time elapsed between the most recent checkpoint prior to failure and the time of failure) and the failure distribution. Furthermore, failures may occur during the checkpointing and error recovery. Our general model unifies a variety of models that have previously been investigated. We denote by F<subscrpt>i</subscrpt>; and t(F<subscrpt>i</subscrpt>), i \&equil; 1, 2, ..., the i<supscrpt>th</supscrpt> failure that occurs during normal processing (not during error recovery) and the time of its occurrence, respectively. We refer to the time period L<subscrpt>i</subscrpt> \&equil; t(F<subscrpt>i+1</subscrpt>) \&minus; t(F<subscrpt>i</subscrpt>), i \&equil; 1, 2, ..., as the i<supscrpt>th</supscrpt> cycle whose length is L<subscrpt>i</subscrpt>. It consists of two portions: the total error recovery time and the normal processing time. The reprocessing time associated with failure F<subscrpt>i</subscrpt> is denoted by Y<subscrpt>i\&minus;1</subscrpt>. Since the variables of the i<supscrpt>th</supscrpt> cycle depend at most on one variable of the (i \&minus; 1)<supscrpt>st</supscrpt> cycle, namely Y<subscrpt>i\&minus;1</subscrpt>, the stochastic process of the reprocessing time Y<subscrpt>i</subscrpt>; i\&ge;0 is a Markov process. We obtain the transition probability density function and the stationary distribution of this process. The performance of the system is measured by the availability, the fraction of time the system is not checkpointing or recovering from errors. In equilibrium, the system availability is expressed as the ratio of the mean production time (normal processing time excluding checkpointing time) during a cycle and the mean length of the cycle. We obtain a general expression for the system availability in our general model. The checkpointing strategy is characterized by the sequence of checkpointing intervals. For the well-known equidistant checkpointing strategy, in which the checkpointing intervals are constant, we find that the resulting system availability depends only on the mean of the failure distribution. We define a checkpointing strategy as failure-dependent if the sequence of checkpointing intervals depends on the failure distribution. Checkpointing strategies that result in a checkpointing operation immediately after error recovery are called reprocessing-independent strategies. We then introduce a novel checkpointing strategy, the equicost strategy, which is failure-dependent and reprocessing-independent. This strategy suggests that a checkpointing operation is to be performed whenever the mean reprocessing cost equals the mean checkpointing cost. Interestingly, the equicost strategy leads to fixed checkpointing intervals for Poisson failures. We compare the maximum system availability resulting from the equidistant and the equicost checkpointing strategies under Weibull distributions which are good approximations of actual failure distributions. Computational results based on Weibull failure distributions (both increasing and decreasing failure rates) show that the equicost strategy achieves higher system availability than the equidistant strategy which is known to be optimal under Poisson failures.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {129--},
 url = {http://doi.acm.org/10.1145/800040.801400},
 doi = {http://doi.acm.org/10.1145/800040.801400},
 acmid = {801400},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kowalk:1983:OVR:800040.801401,
 author = {Kowalk, Wolfgang},
 title = {An operational view on renewal theory},
 abstract = {In this paper we derive a formula for the moments of the residual life in operational context, and show that the Paradox of Residual Life holds also in a finite queueing model. In addition, we prove the renewal theorem, show that forward and backward times are independent, and state the memoryless property. As applications we point out how to derive Tak\&agrave;cs recurrence formula for the moments of the waiting time and how to base the Markovian state theory on this.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {130--137},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800040.801401},
 doi = {http://doi.acm.org/10.1145/800040.801401},
 acmid = {801401},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wolfinger:1983:CVS:800040.801402,
 author = {Wolfinger, Bernd and M\"{u}hlh\"{a}user, Max},
 title = {Construction of a validated simulator for performance prediction of DECnet-based computer networks},
 abstract = {Predicting important performance parameters of computer networks, recognizing potential bottlenecks, comparing design alternatives are factors of decisive importance in building complex computer networks. In this respect, computer aided simulation has proved to be a very effective design tool in a number of practical applications. It is shown by the example of the MOSAIC modeling system how a simulator applicable on a broad basis could be adapted to the specific characteristics of a class of existing computer networks. The class of computer networks chosen for modeling is based on DECnet communication software. Modeling concentrates mainly on the DECnet protocols and their hierarchies. The paper indicates the adaptations necessary to adjust the MOSAIC kernel system adequately to DECnet computer networks and summarizes the results of a rather extensive validation for the modeling system, comprising calibration and accuracy establishment, which has been carried out successfully.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {138--150},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/800040.801402},
 doi = {http://doi.acm.org/10.1145/800040.801402},
 acmid = {801402},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Communication protocols, Computer network architectures, Modeling systems, Performance evaluation, Protocol hierarchies, Simulation, Validation},
} 

@inproceedings{Bucher:1983:CSS:800040.801403,
 author = {Bucher, Ingrid Y.},
 title = {The computational speed of supercomputers},
 abstract = { Problems related to the evaluation of computational speeds of supercomputers are discussed. Measurements of sequential speeds, vector speeds, and asynchronous parallel processing speeds are presented. A simple model is developed that allows us to evaluate the workload-dependent effective speed of current systems such as vector computers and asynchronous parallel processing systems. Results indicate that the effective speed of a supercomputer is severely limited by its slowest processing mode unless the fraction of the workload that has to be processed in this mode is negligibly small. },
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {151--165},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800040.801403},
 doi = {http://doi.acm.org/10.1145/800040.801403},
 acmid = {801403},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Faro:1983:MBX:800040.801404,
 author = {Faro, Alberto and Messina, Gaetano and Martinucci, Franco},
 title = {On the measured behaviour of a X.25 packet switching subnetwork},
 abstract = {The aim of this paper is to present some measurements on the performance of a X.25 packet switching subnetwork and of a typical X.25 DTE-DCE interface. The communication parameters considered are the window size of the link layer flow control, the length and contents of the packets and the internode distance. The collected measurements on the X.25 subnetwork are compared with others obtained through different versions of DTE connected to the same subnetwork. Some considerations are presented about the time spent by the packets inside the DTE and consequently about the efficiency of the DTE version used in these experiments.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {166--174},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800040.801404},
 doi = {http://doi.acm.org/10.1145/800040.801404},
 acmid = {801404},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Krishna:1983:QAC:800040.801405,
 author = {Krishna, C. M and Shin, K. G},
 title = {Queueing analysis of a canonical model of real-time multiprocessors},
 abstract = {Multiprocessors are beginning to be regarded increasingly favorably as candidates for controllers in critical real-time control applications such as aircraft. Their considerable tolerance of component failures together with their great potential for high throughput are contributory factors. In this paper, we present first a logical classification of multiprocessor structures with control applications in mind. We point out that one important subclass has hitherto been neglected by the analysts. This is a class of systems with a common memory, minimal interprocessor communication and perfect processor symmetry. The performance characteristic of the greatest importance in real-time applications is the response time distribution. Indeed, we have shown in a separate paper [2] how it is possible to characterize rigorously and objectively the performance of a real-time multiprocessor given the application and the multiprocessor response time distribution and component failure characteristics. We therefore present here a computation of the response time distribution for a canonical model of real-time multiprocessor. To do so, we approximate the multiprocessor by a blocking model and present a means for efficient analysis. Two separate models are derived: one created from the system's point of view, and the other from the point of view of an incoming task. The former model is analyzed along largely conventional lines. For the latter model, an artificial server is used, and the system is transformed into a queueing network.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {175--189},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800040.801405},
 doi = {http://doi.acm.org/10.1145/800040.801405},
 acmid = {801405},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reed:1983:QNM:800040.801406,
 author = {Reed, Daniel A.},
 title = {Queueing network models of multimicrocomputer networks},
 abstract = {Recent developments in very large scale integration have made it feasible to construct a highly parallel computer composed of large numbers of interconnected microcomputers. The modeling problems posed by this approach to parallel processing differ in several significant respects from those associated with traditional queueing network models of computer systems. Among them are the size of the models, the varied interconnection network topologies, and algorithm dependent internode communication patterns. We extend queueing theoretic models to include multimicrocomputer networks with primary emphasis on two areas: characterizing interconnection network workloads and finding efficient solution techniques for the resulting models.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {190--197},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800040.801406},
 doi = {http://doi.acm.org/10.1145/800040.801406},
 acmid = {801406},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marsan:1983:CGS:800040.801407,
 author = {Marsan, M. Ajmone and Balbo, G. and Conte, G.},
 title = {A class of generalised stochastic petri nets for the performance evaluation of multiprocessor systems},
 abstract = {Graph models have been proposed by many authors as a useful tool for the analysis of peculiar features of computer systems such as concurrency, synchronization, communication, and cooperation among subsystems. Much of the work in this field is related to the original ideas developed by C. A. Petri. These graph models are today generally known as Petri Nets (PNs).},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {198--199},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/800040.801407},
 doi = {http://doi.acm.org/10.1145/800040.801407},
 acmid = {801407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Valero:1983:PEM:800040.801408,
 author = {Valero, Mateo and Llaberia, Jos\'{e} M. and Labarta, Jes\'{u}s and Sanvicente, Emilio and Lang, Tom\'{a}s},
 title = {A performance evaluation of the multiple bus network for multiprocessor systems},
 abstract = {In this paper we present a mathematical model to compute the bandwidth of the multiple bus interconnection network. Due to the computational complexity associated with the exact solution, the processors are removed from the queues at the end of each memory cycle to facilitate the analysis. This leads to approximate solutions which are both easier to obtain and very accurate.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {200--206},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800040.801408},
 doi = {http://doi.acm.org/10.1145/800040.801408},
 acmid = {801408},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Towsley:1983:AAM:800040.801409,
 author = {Towsley, Don},
 title = {An approximate analysis of multiprocessor systems},
 abstract = {This paper presents an approximate analysis of a multiprocessor system consisting of P processors, M memory modules, and B buses. The model assumes constant memory access times, arbitrary memory access patterns, and bus contention. The solution technique aggregates all memories into a composite queue and degrades the service rates of this queue so as to include the effect of bus contention. The throughput predictions from this model are very accurate, typically within 1\% of predictions made with either simulation or exact analysis.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {207--213},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800040.801409},
 doi = {http://doi.acm.org/10.1145/800040.801409},
 acmid = {801409},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Freund:1983:NAA:800040.801410,
 author = {Freund, Darral J. and Bexfield, James N.},
 title = {A new aggregation approximation procedure for solving closed queueing networks with simultaneous resource possession},
 abstract = {A new aggregation approximation procedure is presented for analyzing single class closed queueing networks with simultaneous resource possession. It is applicable to multiple entry systems such as I/O models and relies on a new multi-entrance queue to aggregate the subnetwork with simultaneous resource possession into a variable rate queue. The resulting network is solved using conventional product form techniques. The multi-entrance queue has a service rate function that depends on the utilization of the primary resources of the subnetwork which enables calculation of the delay due to queueing for a specific resource, even when others are available. It has a product form solution, different from the solution currently known. A major advantage of the procedure is that the aggregation can be reversed to obtain estimates of low level performance parameters once the solution is obtained. A solution algorithm is presented along with two examples. Results are compared with simulation models.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {214--223},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800040.801410},
 doi = {http://doi.acm.org/10.1145/800040.801410},
 acmid = {801410},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harrison:1983:EAD:800040.801411,
 author = {Harrison, P G},
 title = {An exact analysis of the distribution of cycle times in a class of queueing networks},
 abstract = {Prediction of detailed characteristics of the time delays experienced by customers in queueing networks is of great importance in various modelling and performance evaluation activities: operations research, computer systems and communication networks. Their statistical properties have been investigated predominantly by simulation techniques with the exception of mean value analyses for which Little's Law is applied. Theoretical studies of the probability distributions of time delays tend to be based on their Laplace transforms, which are of limited use, can be inverted analytically only in very simple cases and present substantial computation problems for numerical inversion. An exact derivation is presented for the distribution of cycle times in so called tree-like queueing networks. The analysis is performed for a network structure which is such that it is not necessary to mark a special customer, so avoiding expansion of the state space. Cycle time distribution is derived initially in the form of its Laplace Transform, from which its moments follow. A recurrence relation for a uniformly convergent discrete representation of the distribution then follows by a similar argument. Finally, the numerical results obtained for some simple test networks are presented and compared with those corresponding to an approximate method, hence indicating the accuracy of the latter.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {224--242},
 numpages = {19},
 url = {http://doi.acm.org/10.1145/800040.801411},
 doi = {http://doi.acm.org/10.1145/800040.801411},
 acmid = {801411},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Markov processes, Modelling, Performance evaluation, Probability theory, Queueing networks, Queueing theory, Response time, Time delays},
} 

@inproceedings{Suri:1983:PCM:800040.801412,
 author = {Suri, Rajan and Cao, Xiren},
 title = {The phantom customer and marked customer methods for optimization of closed queueing networks with blocking and general service times},
 abstract = {This paper is based on a recent technique called perturbation analysis of discrete event systems. The aims of the paper are two-fold. The first, is to bring this technique, which is still relatively new, to the attention of a wider audience, since the method is particularly relevant to practical systems optimization. The second, is to develop two new methods, the phantom customer method and the marked customer method, as extensions of the perturbation analysis approach. These methods enable efficient and accurate optimization of closed queueing networks with blocking and general service-time distributions. Useful practical applications of these methods would be for optimization of performance of computer systems, communication networks, and automated manufacturing complexes. The accuracy of the methods is illustrated by several numerical examples.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {243--256},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/800040.801412},
 doi = {http://doi.acm.org/10.1145/800040.801412},
 acmid = {801412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Discrete event systems, Performance evaluation, Performance prediction, Perturbation analysis, Queueing networks, Sample path analysis, Simulation},
} 

@inproceedings{Thareja:1983:COD:800040.801413,
 author = {Thareja, Ashok K. and Agrawala, Ashok K.},
 title = {Characterization of an optimal delayed resolution policy},
 abstract = {A class of policies called stationary delayed resolution policies have been proposed recently for sharing finite number of buffers at a store-and-forward node in a message switching network [9]. It has been shown that with respect to the total weighted throughput these policies comprise the optimal class of policies. In this paper, we present methods to obtain an optimal policy from the class of stationary delayed resolution policies for given values of the parameters. A method based upon policy iteration technique for Markov decision processes is used to obtain the optimal delayed resolution policy. It is shown that the policy iteration technique while useful in obtaining the exact optimal policy becomes intractable for practical values of buffer sizes and number of message classes. A class of policies called SRS delayed resolution policies is proposed. It is shown that the best SRS delayed resolution policies closely approximate the performance of the optimal delayed resolution policies.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {257--265},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800040.801413},
 doi = {http://doi.acm.org/10.1145/800040.801413},
 acmid = {801413},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Berry:1983:PMT:800040.801414,
 author = {Berry, Robert and Chandy, K. Mani},
 title = {Performance models of token ring local area networks},
 abstract = {This paper presents a simple heuristic analytic algorithm for predicting the ``response times" of messages in asymmetric token ring local area networks. A description of the token ring and the model is presented in section 2 the algorithm is described in section 3 and the empirical results in section 4. The analytic results were compared against a detailed simulation model and the results are extremely close over a wide range of models. Local area networks (or LANS) offer a very attractive solution to the problem of connecting a large number of devices distributed over a small geographic area. They are an inexpensive readily expandable and highly flexible communications media. They are the backbone of the automated office - a significant component of the office of the future. This importance of LANS in the future of applied computer science has resulted in a tremendous burst of interest in the study of their behaviour. There are already many different LAN architectures proposed and studied in the literature [Tropper 81] [Tannenbaum 81] [Babic 78] [Metcalfe 76] [Clark 78] One LAN architecture is significant for several reasons. This architecture is the token ring [Carsten 77]. It has attracted interest because of its simplicity fairness and efficiency. The interest it has generated has resulted in the proposal of several different versions. This paper concentrates on one of these versions - the single token token ring protocol as described in [Bux 81]. This particular version is attractive because of its overall simplicity and reliability. This paper presents an algorithm for predicting response times in a token ring with the single token protocol.},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {266--274},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800040.801414},
 doi = {http://doi.acm.org/10.1145/800040.801414},
 acmid = {801414},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Varghese:1983:QDV:800040.801415,
 author = {Varghese, G. and Chou, W. and Nilsson, A. A.},
 title = {Queueing delays on virtual circuits using a sliding window flow control scheme},
 abstract = {A tandem queue model is developed that models the end-to-end delay behavior in networks that employ sliding window flow control. Messages arriving to find the window filled are assumed to be queued outside the network to await their turn to enter. The model is analyzed using a hierarchical decomposition method; the key step entails incorporating the fact that the interdeparture process is not exponentially distributed. End-to-end delay in virtual circuits can then be obtained by modelling a virtual route as a tandem queue [1] using the method of ``adjusted rates."},
 booktitle = {Proceedings of the 1983 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '83},
 year = {1983},
 isbn = {0-89791-112-1},
 location = {Minneapolis, Minnesota, United States},
 pages = {275--281},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800040.801415},
 doi = {http://doi.acm.org/10.1145/800040.801415},
 acmid = {801415},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Graph algorithms, Virtual circuits, Virtual memory},
} 

@article{Blake:1982:OCT:1035332.1035295,
 author = {Blake, Russ},
 title = {Optimal control of thrashing},
 abstract = {The method of discrete optimal control is applied to control thrashing in a virtual memory. Certain difficulties with several previous approaches are discussed. The mechanism of optimal control is presented as an effective, inexpensive alternative. A simple, ideal policy is devised to illustrate the method. A new feedback parameter, the thrashing level, is found to be a positive and robust indicator of thrashing. When applied to a real system, the idealized policy effectively controlled the virtual memory.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035332.1035295},
 doi = {http://doi.acm.org/10.1145/1035332.1035295},
 acmid = {1035295},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blake:1982:OCT:1035293.1035295,
 author = {Blake, Russ},
 title = {Optimal control of thrashing},
 abstract = {The method of discrete optimal control is applied to control thrashing in a virtual memory. Certain difficulties with several previous approaches are discussed. The mechanism of optimal control is presented as an effective, inexpensive alternative. A simple, ideal policy is devised to illustrate the method. A new feedback parameter, the thrashing level, is found to be a positive and robust indicator of thrashing. When applied to a real system, the idealized policy effectively controlled the virtual memory.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035293.1035295},
 doi = {http://doi.acm.org/10.1145/1035293.1035295},
 acmid = {1035295},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Babaoglu:1982:HRD:1035293.1035296,
 author = {Babao\v{g}lu, \"{O}zalp},
 title = {Hierarchical replacement decisions in hierarchical stores},
 abstract = {One of the primary motivations for implementing virtual memory is its ability to automatically manage a hierarchy of storage systems with different characteristics. The composite system behaves as if it were a single-level system having the more desirable characteristics of each of its constituent levels. In this paper we extend the virtual memory concept to within each of the levels of the hierarchy. Each level is thought of as containing two additional levels within it. This hierarchy is not a physical one, but rather an artificial one arising from the employment of two different replacement algorithms. Given two replacement algorithms, one of which has good performance but high implementation cost and the other poor performance but low implementation cost, we propose and analyze schemes that result in an overall algorithm having the performance characteristics of the former and the cost characteristics of the latter. We discuss the suitability of such schemes in the management of storage hierarchies that lack page reference bits.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {11--19},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035296},
 doi = {http://doi.acm.org/10.1145/1035293.1035296},
 acmid = {1035296},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Babaoglu:1982:HRD:1035332.1035296,
 author = {Babao\v{g}lu, \"{O}zalp},
 title = {Hierarchical replacement decisions in hierarchical stores},
 abstract = {One of the primary motivations for implementing virtual memory is its ability to automatically manage a hierarchy of storage systems with different characteristics. The composite system behaves as if it were a single-level system having the more desirable characteristics of each of its constituent levels. In this paper we extend the virtual memory concept to within each of the levels of the hierarchy. Each level is thought of as containing two additional levels within it. This hierarchy is not a physical one, but rather an artificial one arising from the employment of two different replacement algorithms. Given two replacement algorithms, one of which has good performance but high implementation cost and the other poor performance but low implementation cost, we propose and analyze schemes that result in an overall algorithm having the performance characteristics of the former and the cost characteristics of the latter. We discuss the suitability of such schemes in the management of storage hierarchies that lack page reference bits.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {11--19},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035296},
 doi = {http://doi.acm.org/10.1145/1035332.1035296},
 acmid = {1035296},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hagmann:1982:PPR:1035293.1035298,
 author = {Hagmann, Robert B. and Fabry, Robert S.},
 title = {Program page reference patterns},
 abstract = {This paper describes a set of measurements of the memory reference patterns of some programs. The technique used to obtain these measurements is unusually efficient. The data is presented in graphical form to allow the reader to "see" how the program uses memory. Constant use of a page and sequential access of memory are easily observed. An attempt is made to classify the programs based on their referencing behavior. From this analysis it is hoped that the reader will gain some insights as to the effectiveness of various memory management policies.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {20--29},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035293.1035298},
 doi = {http://doi.acm.org/10.1145/1035293.1035298},
 acmid = {1035298},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hagmann:1982:PPR:1035332.1035298,
 author = {Hagmann, Robert B. and Fabry, Robert S.},
 title = {Program page reference patterns},
 abstract = {This paper describes a set of measurements of the memory reference patterns of some programs. The technique used to obtain these measurements is unusually efficient. The data is presented in graphical form to allow the reader to "see" how the program uses memory. Constant use of a page and sequential access of memory are easily observed. An attempt is made to classify the programs based on their referencing behavior. From this analysis it is hoped that the reader will gain some insights as to the effectiveness of various memory management policies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {20--29},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035332.1035298},
 doi = {http://doi.acm.org/10.1145/1035332.1035298},
 acmid = {1035298},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bunt:1982:EMP:1035332.1035299,
 author = {Bunt, R. B. and Harbus, R. S. and Plumb, S. J.},
 title = {The effective management of paging storage hierarchies},
 abstract = {The use of storage hierarchies in the implementation of a paging system is investigated. Alternative approaches for managing a paging storage hierarchy are described and two are selected for further study - staging and migration. Characteristic behaviour is determined for each of these approaches and a series of simulation experiments is conducted (using program reference strings as data) for the purpose of comparing them. The results clearly show migration to be a superior approach from the point of view of both cost and performance. Conclusions are drawn on the effectiveness of each approach in practice.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {30--38},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035299},
 doi = {http://doi.acm.org/10.1145/1035332.1035299},
 acmid = {1035299},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bunt:1982:EMP:1035293.1035299,
 author = {Bunt, R. B. and Harbus, R. S. and Plumb, S. J.},
 title = {The effective management of paging storage hierarchies},
 abstract = {The use of storage hierarchies in the implementation of a paging system is investigated. Alternative approaches for managing a paging storage hierarchy are described and two are selected for further study - staging and migration. Characteristic behaviour is determined for each of these approaches and a series of simulation experiments is conducted (using program reference strings as data) for the purpose of comparing them. The results clearly show migration to be a superior approach from the point of view of both cost and performance. Conclusions are drawn on the effectiveness of each approach in practice.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {30--38},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035299},
 doi = {http://doi.acm.org/10.1145/1035293.1035299},
 acmid = {1035299},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hodges:1982:WCP:1035293.1035301,
 author = {Hodges, Larry F. and Stewart, William J.},
 title = {Workload characterization and performance evaluation in a research environment},
 abstract = {This paper describes the process of bench-marking the diverse research environment that constitutes the workload of VAX/VMS at the University Analysis and Control Center at North Carolina State University. The benchmarking process began with a study of the system load and performance characteristics over the six-month period from January to June of 1981. Statistics were compiled on the number of active users, CPU usage by individual accounts, and peak load periods. Individual users were interviewed to determine the nature and major computing characteristics of the research they were conducting on VAX. Information from all sources was compiled to produce a benchmark that closely paralleled actual system activity. An analytic model was introduced and used in conjunction with the benchmark data and hardware characteristics to derive performance measures for the system. Comparisons with measured system performance were conducted to demonstrate the accuracy of the model. The model was then employed to predict performance as the system workload was increased, to suggest improvements for the system, and to examine the effects of those improvements.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035293.1035301},
 doi = {http://doi.acm.org/10.1145/1035293.1035301},
 acmid = {1035301},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hodges:1982:WCP:1035332.1035301,
 author = {Hodges, Larry F. and Stewart, William J.},
 title = {Workload characterization and performance evaluation in a research environment},
 abstract = {This paper describes the process of bench-marking the diverse research environment that constitutes the workload of VAX/VMS at the University Analysis and Control Center at North Carolina State University. The benchmarking process began with a study of the system load and performance characteristics over the six-month period from January to June of 1981. Statistics were compiled on the number of active users, CPU usage by individual accounts, and peak load periods. Individual users were interviewed to determine the nature and major computing characteristics of the research they were conducting on VAX. Information from all sources was compiled to produce a benchmark that closely paralleled actual system activity. An analytic model was introduced and used in conjunction with the benchmark data and hardware characteristics to derive performance measures for the system. Comparisons with measured system performance were conducted to demonstrate the accuracy of the model. The model was then employed to predict performance as the system workload was increased, to suggest improvements for the system, and to examine the effects of those improvements.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035332.1035301},
 doi = {http://doi.acm.org/10.1145/1035332.1035301},
 acmid = {1035301},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Haring:1982:SWC:1035293.1035302,
 author = {Haring, G\"{u}nter},
 title = {On state-dependent workload characterization by software resources},
 abstract = {A method for the characterization of computer workload at the task level is presented. After having divided the workload into different classes using a cluster technique, each cluster is further analysed by state dependent transition matrices. Thus it is possible to derive the most probable task sequences in each cluster. This information can be used to construct synthetic scripts at the task level rather than the usual description at the hardware resource level.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {51--57},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1035293.1035302},
 doi = {http://doi.acm.org/10.1145/1035293.1035302},
 acmid = {1035302},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Haring:1982:SWC:1035332.1035302,
 author = {Haring, G\"{u}nter},
 title = {On state-dependent workload characterization by software resources},
 abstract = {A method for the characterization of computer workload at the task level is presented. After having divided the workload into different classes using a cluster technique, each cluster is further analysed by state dependent transition matrices. Thus it is possible to derive the most probable task sequences in each cluster. This information can be used to construct synthetic scripts at the task level rather than the usual description at the hardware resource level.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {51--57},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1035332.1035302},
 doi = {http://doi.acm.org/10.1145/1035332.1035302},
 acmid = {1035302},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bolzoni:1982:PIS:1035332.1035303,
 author = {Bolzoni, M. L. and Calzarossa, M. C. and Mapelli, P. and Serazzi, G.},
 title = {A package for the implementation of static workload models},
 abstract = {The general principles for constructing workload models are reviewed. The differences between static and dynamic workload models are introduced and the importance of the classification phase for the implementation of both types of workload models is pointed out. All the operations required for constructing static workload models have been connected in a package. Its main properties and fields of application are presented. The results of an experimental study performed with the package on a batch and interactive workload show its ease of use and the accuracy of the model obtained.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {58--67},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035332.1035303},
 doi = {http://doi.acm.org/10.1145/1035332.1035303},
 acmid = {1035303},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bolzoni:1982:PIS:1035293.1035303,
 author = {Bolzoni, M. L. and Calzarossa, M. C. and Mapelli, P. and Serazzi, G.},
 title = {A package for the implementation of static workload models},
 abstract = {The general principles for constructing workload models are reviewed. The differences between static and dynamic workload models are introduced and the importance of the classification phase for the implementation of both types of workload models is pointed out. All the operations required for constructing static workload models have been connected in a package. Its main properties and fields of application are presented. The results of an experimental study performed with the package on a batch and interactive workload show its ease of use and the accuracy of the model obtained.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {58--67},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035293.1035303},
 doi = {http://doi.acm.org/10.1145/1035293.1035303},
 acmid = {1035303},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{McDaniel:1982:MSI:1035332.1035305,
 author = {McDaniel, Gene},
 title = {The Mesa Spy: an interactive tool for performance debugging},
 abstract = {The Spy is a performance evaluation tool for the Mesa environment that uses a new extention to the PC sampling technique. The data collection process can use information in the run time call stack to determine what code is responsible for the resources being consumed. The Spy avoids perturbing the user environment when it executes, provides symbolic output at the source-language level, and can be used without recompiling the program to be examined. Depending upon how much complication the user asks for during data collection, the Spy steals between .3\% and 1.8\% of the cycles of a fast machine, and between 1.08\% and 35.9\% of the cycles on a slow machine.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {68--76},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035305},
 doi = {http://doi.acm.org/10.1145/1035332.1035305},
 acmid = {1035305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high level language performance debugging, pc sampling, performance analysis},
} 

@inproceedings{McDaniel:1982:MSI:1035293.1035305,
 author = {McDaniel, Gene},
 title = {The Mesa Spy: an interactive tool for performance debugging},
 abstract = {The Spy is a performance evaluation tool for the Mesa environment that uses a new extention to the PC sampling technique. The data collection process can use information in the run time call stack to determine what code is responsible for the resources being consumed. The Spy avoids perturbing the user environment when it executes, provides symbolic output at the source-language level, and can be used without recompiling the program to be examined. Depending upon how much complication the user asks for during data collection, the Spy steals between .3\% and 1.8\% of the cycles of a fast machine, and between 1.08\% and 35.9\% of the cycles on a slow machine.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {68--76},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035305},
 doi = {http://doi.acm.org/10.1145/1035293.1035305},
 acmid = {1035305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high level language performance debugging, pc sampling, performance analysis},
} 

@inproceedings{Hercksen:1982:MSE:1035293.1035306,
 author = {Hercksen, Uwe and Klar, Rainer and Klein\"{o}der, Wolfgang and Knei\ssl, Franz},
 title = {Measuring simultaneous events in a multiprocessor system},
 abstract = {In the hierarchically organized multiprocessor system EGPA, which has the structure of a pyramid, the performance of concurrent programs is studied. These studies are assisted by a hardware monitor (Z\&Auml;HLMONITOR III), which measures not only the activity and idle states of CPU and channels, but records the complete history of processes in the CPU and interleaved I/O activities. The applied method is distinguished from usual hardware measurements for two reasons: it puts together the a priori independent event-streams coming from the different processors to a well ordered single event stream and it records not only hardware but also software events. Most useful have been traces of software events, which give the programmer insight into the dynamic cooperation of distributed subtasks of his program. This paper describes the measurement method and its application to the analysis of the behaviour of a highly asynchronous parallel algorithm: the projection of contour lines from a given point of view and the elimination of hidden lines. This work is sponsored by the Bundesminister f\&uuml;r Forschung und Technologie (German Federal Minister of Research and Technology).},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {77--88},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035293.1035306},
 doi = {http://doi.acm.org/10.1145/1035293.1035306},
 acmid = {1035306},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Hercksen:1982:MSE:1035332.1035306,
 author = {Hercksen, Uwe and Klar, Rainer and Klein\"{o}der, Wolfgang and Knei\ssl, Franz},
 title = {Measuring simultaneous events in a multiprocessor system},
 abstract = {In the hierarchically organized multiprocessor system EGPA, which has the structure of a pyramid, the performance of concurrent programs is studied. These studies are assisted by a hardware monitor (Z\&Auml;HLMONITOR III), which measures not only the activity and idle states of CPU and channels, but records the complete history of processes in the CPU and interleaved I/O activities. The applied method is distinguished from usual hardware measurements for two reasons: it puts together the a priori independent event-streams coming from the different processors to a well ordered single event stream and it records not only hardware but also software events. Most useful have been traces of software events, which give the programmer insight into the dynamic cooperation of distributed subtasks of his program. This paper describes the measurement method and its application to the analysis of the behaviour of a highly asynchronous parallel algorithm: the projection of contour lines from a given point of view and the elimination of hidden lines. This work is sponsored by the Bundesminister f\&uuml;r Forschung und Technologie (German Federal Minister of Research and Technology).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {77--88},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035332.1035306},
 doi = {http://doi.acm.org/10.1145/1035332.1035306},
 acmid = {1035306},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelenbe:1982:SDF:1035293.1035308,
 author = {Gelenbe, Erol},
 title = {Stationary deterministic flows in discrete systems: i},
 abstract = {We consider a deterministic system whose state space is the n-dimensional first orthant. It may be considered as a network of (deterministic) queues, a Karp-Miller vector addition system, a Petrinet, a complex computer system, etc. Weak assumptions are then made concerning the asymptotic or limiting behaviour of the instants at which events are observed accross a cut in the system : these instants may be considered as "arrival" or "departure" instants. Thus, like in operational analysis, we deal with deterministic and observable properties and we need no stochastic assumptions or restrictions (such as independence, identical distributions, etc.). We consider however asymptotic or stationary properties, as in conventional queueing analysis. Under our assumptions a set of standard theorems are proved: concerning arrival and departure instant measures, concerning, "birth and death" type equations, and concerning Little's formula. Our intention is to set the framework for a new approach to performance modelling of computer systems in a context close to that used in actual measurements, but taking into account infinite time behaviour in order to take advantage of the useful mathematical properties of asymptotic results.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {89--101},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1035293.1035308},
 doi = {http://doi.acm.org/10.1145/1035293.1035308},
 acmid = {1035308},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gelenbe:1982:SDF:1035332.1035308,
 author = {Gelenbe, Erol},
 title = {Stationary deterministic flows in discrete systems: i},
 abstract = {We consider a deterministic system whose state space is the n-dimensional first orthant. It may be considered as a network of (deterministic) queues, a Karp-Miller vector addition system, a Petrinet, a complex computer system, etc. Weak assumptions are then made concerning the asymptotic or limiting behaviour of the instants at which events are observed accross a cut in the system : these instants may be considered as "arrival" or "departure" instants. Thus, like in operational analysis, we deal with deterministic and observable properties and we need no stochastic assumptions or restrictions (such as independence, identical distributions, etc.). We consider however asymptotic or stationary properties, as in conventional queueing analysis. Under our assumptions a set of standard theorems are proved: concerning arrival and departure instant measures, concerning, "birth and death" type equations, and concerning Little's formula. Our intention is to set the framework for a new approach to performance modelling of computer systems in a context close to that used in actual measurements, but taking into account infinite time behaviour in order to take advantage of the useful mathematical properties of asymptotic results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {89--101},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1035332.1035308},
 doi = {http://doi.acm.org/10.1145/1035332.1035308},
 acmid = {1035308},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Baccelli:1982:DBR:1035332.1035309,
 author = {Baccelli, F. and Coffman, E. G.},
 title = {A data base replication analysis using an M/M/m queue with service interruptions},
 abstract = {A study of file replication policies for distributed data bases will be approached through the analysis of an M/M/m queue subjected to state-independent, preemptive interruptions of service. The durations of periods of interruption constitute a sequence of independent, identically distributed random variables. Independently, the times measured from the termination of one period of interruption to the beginning of the next form a sequence of independent, exponentially distributed random variables. Preempted customers resume service at the terminations of interrupt periods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {102--107},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1035332.1035309},
 doi = {http://doi.acm.org/10.1145/1035332.1035309},
 acmid = {1035309},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Baccelli:1982:DBR:1035293.1035309,
 author = {Baccelli, F. and Coffman, E. G.},
 title = {A data base replication analysis using an M/M/m queue with service interruptions},
 abstract = {A study of file replication policies for distributed data bases will be approached through the analysis of an M/M/m queue subjected to state-independent, preemptive interruptions of service. The durations of periods of interruption constitute a sequence of independent, identically distributed random variables. Independently, the times measured from the termination of one period of interruption to the beginning of the next form a sequence of independent, exponentially distributed random variables. Preempted customers resume service at the terminations of interrupt periods.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {102--107},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1035293.1035309},
 doi = {http://doi.acm.org/10.1145/1035293.1035309},
 acmid = {1035309},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Plateau:1982:MPR:1035293.1035310,
 author = {Plateau, Brigitte and Staphylopatis, Andreas},
 title = {Modelling of the parallel resolution of a numerical problem on a locally distributed computing system},
 abstract = {Modern VLSI technology has enabled the development of high-speed computing systems, based upon various multiprocessor architecture [1]. We can distinguish several types of such systems, depending on the control policies adopted, the interprocessor communication modes and the degree of ressource-sharing. The efficiency of parallel processing may be significant in various areas of computer applications; especially, large numerical applications, such as the solution of linear systems and differential equations, are marked by the need of high computation speeds. So, the advance of parallel processing systems goes together with research effort in developping efficient parallel algorithms [2]. The implementation of parallel algorithms concerne the execution of concurrent processes, assigned to the processors of the system, which communicate with each other. The syncronization needed at process interaction points implies the existence of waiting delays, which constitute the main limiting factor of parallel computation. Several modelling techniques have been developped, that allow the prediction and verification of parallel systems performance. The two general approaches followed concern deterministic models [3] and probabilistic models. The latter, based on the theory of stochastic processes [5]... are well adapted to the analysis of complex variable phenomena and provide important measures concerning several aspects of parallel processing.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035293.1035310},
 doi = {http://doi.acm.org/10.1145/1035293.1035310},
 acmid = {1035310},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Plateau:1982:MPR:1035332.1035310,
 author = {Plateau, Brigitte and Staphylopatis, Andreas},
 title = {Modelling of the parallel resolution of a numerical problem on a locally distributed computing system},
 abstract = {Modern VLSI technology has enabled the development of high-speed computing systems, based upon various multiprocessor architecture [1]. We can distinguish several types of such systems, depending on the control policies adopted, the interprocessor communication modes and the degree of ressource-sharing. The efficiency of parallel processing may be significant in various areas of computer applications; especially, large numerical applications, such as the solution of linear systems and differential equations, are marked by the need of high computation speeds. So, the advance of parallel processing systems goes together with research effort in developping efficient parallel algorithms [2]. The implementation of parallel algorithms concerne the execution of concurrent processes, assigned to the processors of the system, which communicate with each other. The syncronization needed at process interaction points implies the existence of waiting delays, which constitute the main limiting factor of parallel computation. Several modelling techniques have been developped, that allow the prediction and verification of parallel systems performance. The two general approaches followed concern deterministic models [3] and probabilistic models. The latter, based on the theory of stochastic processes [5]... are well adapted to the analysis of complex variable phenomena and provide important measures concerning several aspects of parallel processing.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {108--117},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035332.1035310},
 doi = {http://doi.acm.org/10.1145/1035332.1035310},
 acmid = {1035310},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bard:1982:MIS:1035332.1035312,
 author = {Bard, Yonathan},
 title = {Modeling I/O systems with dynamic path selection, and general transmission networks},
 abstract = {This paper examines general transmission networks, of which I/O subsystems are a special case. By using the maximum entropy principle, we answer questions such as what is the probability that a path to a given node is free when that node is ready to transmit. Systems with both dynamic and fixed path selection mechanisms are treated. Approximate methods for large networks are proposed, and numerical examples are given.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {118--129},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035332.1035312},
 doi = {http://doi.acm.org/10.1145/1035332.1035312},
 acmid = {1035312},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bard:1982:MIS:1035293.1035312,
 author = {Bard, Yonathan},
 title = {Modeling I/O systems with dynamic path selection, and general transmission networks},
 abstract = {This paper examines general transmission networks, of which I/O subsystems are a special case. By using the maximum entropy principle, we answer questions such as what is the probability that a path to a given node is free when that node is ready to transmit. Systems with both dynamic and fixed path selection mechanisms are treated. Approximate methods for large networks are proposed, and numerical examples are given.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {118--129},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035293.1035312},
 doi = {http://doi.acm.org/10.1145/1035293.1035312},
 acmid = {1035312},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lazowska:1982:MCM:1035293.1035313,
 author = {Lazowska, Edward D. and Zahorjan, John},
 title = {Multiple class memory constrained queueing networks},
 abstract = {Most computer systems have a memory constraint: a limit on the number of requests that can actively compete for processing resources, imposed by finite memory resources. This characteristic violates the conditions required for queueing network performance models to be \&#60;i>separable,\&#60;/i> i.e., amenable to efficient analysis by standard algorithms. Useful algorithms for analyzing models of memory constrained systems have been devised only for models with a single customer class. In this paper we consider the multiple class case. We introduce and evaluate an algorithm for analyzing multiple class queueing networks in which the classes have independent memory constraints. We extend this algorithm to situations in which several classes share a memory constraint. We sketch a generalization to situations in which a subsystem within an overall system model has a population constraint. Our algorithm is compatible with the extremely time- and space-efficient iterative approximate solution techniques for separable queueing networks. This level of efficiency is mandatory for modelling large systems.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {130--140},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1035293.1035313},
 doi = {http://doi.acm.org/10.1145/1035293.1035313},
 acmid = {1035313},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solution technique, computer system performance evaluation, memory constraint, population constraint, queueing network model},
} 

@article{Lazowska:1982:MCM:1035332.1035313,
 author = {Lazowska, Edward D. and Zahorjan, John},
 title = {Multiple class memory constrained queueing networks},
 abstract = {Most computer systems have a memory constraint: a limit on the number of requests that can actively compete for processing resources, imposed by finite memory resources. This characteristic violates the conditions required for queueing network performance models to be \&#60;i>separable,\&#60;/i> i.e., amenable to efficient analysis by standard algorithms. Useful algorithms for analyzing models of memory constrained systems have been devised only for models with a single customer class. In this paper we consider the multiple class case. We introduce and evaluate an algorithm for analyzing multiple class queueing networks in which the classes have independent memory constraints. We extend this algorithm to situations in which several classes share a memory constraint. We sketch a generalization to situations in which a subsystem within an overall system model has a population constraint. Our algorithm is compatible with the extremely time- and space-efficient iterative approximate solution techniques for separable queueing networks. This level of efficiency is mandatory for modelling large systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {130--140},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1035332.1035313},
 doi = {http://doi.acm.org/10.1145/1035332.1035313},
 acmid = {1035313},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solution technique, computer system performance evaluation, memory constraint, population constraint, queueing network model},
} 

@inproceedings{Brandwajn:1982:FAS:1035293.1035314,
 author = {Brandwajn, Alexandre},
 title = {Fast approximate solution of multiprogramming models},
 abstract = {Queueing network models of computer systems with multiprogramming constraints generally do not possess a product-form solution in the sense of Jackson. Therefore, one is usually led to consider approximation techniques when dealing with such models. Equivalence and decomposition is one way of approaching their solution. With multiple job classes, the equivalent network may be viewed as a set of interdependent queues. In general, the state-dependence in this equivalent network precludes a product-form solution, and the size of its state space grows rapidly with the number of classes and of jobs per class. This paper presents two methods for approximate solution of the equivalent state-dependent queueing network. The first approach is a manifold application of equivalence and decomposition. The second approach, less accurate than the first one, is a fast-converging iteration whose computational complexity grows near-linearly with the number of job classes and jobs in a class. Numerical examples illustrate the accuracy of the two methods.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {141--149},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035314},
 doi = {http://doi.acm.org/10.1145/1035293.1035314},
 acmid = {1035314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solutions, equivalence and decomposition, multiprogramming, queueing network models, simultaneous resource possession},
} 

@article{Brandwajn:1982:FAS:1035332.1035314,
 author = {Brandwajn, Alexandre},
 title = {Fast approximate solution of multiprogramming models},
 abstract = {Queueing network models of computer systems with multiprogramming constraints generally do not possess a product-form solution in the sense of Jackson. Therefore, one is usually led to consider approximation techniques when dealing with such models. Equivalence and decomposition is one way of approaching their solution. With multiple job classes, the equivalent network may be viewed as a set of interdependent queues. In general, the state-dependence in this equivalent network precludes a product-form solution, and the size of its state space grows rapidly with the number of classes and of jobs per class. This paper presents two methods for approximate solution of the equivalent state-dependent queueing network. The first approach is a manifold application of equivalence and decomposition. The second approach, less accurate than the first one, is a fast-converging iteration whose computational complexity grows near-linearly with the number of job classes and jobs in a class. Numerical examples illustrate the accuracy of the two methods.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {141--149},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035314},
 doi = {http://doi.acm.org/10.1145/1035332.1035314},
 acmid = {1035314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {approximate solutions, equivalence and decomposition, multiprogramming, queueing network models, simultaneous resource possession},
} 

@inproceedings{Agrawal:1982:ASM:1035293.1035316,
 author = {Agrawal, Subhash C. and Buzen, Jeffrey P.},
 title = {The aggregate server method for analyzing serialization delays in computer systems},
 abstract = {The aggregate server method is an approximate, iterative technique for analyzing the delays programs encounter while waiting for entry into critical sections, non-reentrant subroutines, and similar software structures that cause processing to become serialized. The method employs a conventional product form queueing network comprised of servers that represent actual I/O devices and processors, plus additional aggregate servers that represent serialized processing activity. The parameters of the product form network are adjusted iteratively to account for contention among serialized and non-serialized customers at each physical device.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {150--150},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1035293.1035316},
 doi = {http://doi.acm.org/10.1145/1035293.1035316},
 acmid = {1035316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agrawal:1982:ASM:1035332.1035316,
 author = {Agrawal, Subhash C. and Buzen, Jeffrey P.},
 title = {The aggregate server method for analyzing serialization delays in computer systems},
 abstract = {The aggregate server method is an approximate, iterative technique for analyzing the delays programs encounter while waiting for entry into critical sections, non-reentrant subroutines, and similar software structures that cause processing to become serialized. The method employs a conventional product form queueing network comprised of servers that represent actual I/O devices and processors, plus additional aggregate servers that represent serialized processing activity. The parameters of the product form network are adjusted iteratively to account for contention among serialized and non-serialized customers at each physical device.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {150--150},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1035332.1035316},
 doi = {http://doi.acm.org/10.1145/1035332.1035316},
 acmid = {1035316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Smith:1982:PAS:1035293.1035317,
 author = {Smith, Connie U. and Loendorf, David D.},
 title = {Performance analysis of software for an MIMD computer},
 abstract = {This paper presents a technique for modeling and analyzing the performance of software for an MIMD (Multiple Instruction Multiple Data) computer. The models can be used as an alternative to experimentation for the evaluation of various algorithms and different degrees of parallelism. They can also be used to study the tradeoffs involved in increasing the amount of parallel computation at the expense of increased overhead for synchronization and communication. The detection and alleviation of performance bottlenecks is facilitated.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035293.1035317},
 doi = {http://doi.acm.org/10.1145/1035293.1035317},
 acmid = {1035317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Smith:1982:PAS:1035332.1035317,
 author = {Smith, Connie U. and Loendorf, David D.},
 title = {Performance analysis of software for an MIMD computer},
 abstract = {This paper presents a technique for modeling and analyzing the performance of software for an MIMD (Multiple Instruction Multiple Data) computer. The models can be used as an alternative to experimentation for the evaluation of various algorithms and different degrees of parallelism. They can also be used to study the tradeoffs involved in increasing the amount of parallel computation at the expense of increased overhead for synchronization and communication. The detection and alleviation of performance bottlenecks is facilitated.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {151--162},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1035332.1035317},
 doi = {http://doi.acm.org/10.1145/1035332.1035317},
 acmid = {1035317},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Agre:1982:MRN:1035332.1035318,
 author = {Agre, Jon R. and Tripathi, Satish K.},
 title = {Modeling reentrant and nonreentrant software},
 abstract = {A description of software module models for computer systems is presented. The software module models are based on a two level description, the software level and the hardware level, of the computer system. In the software module level it is possible to model performance effects of software traits such as reentrant and nonreentrant type software modules. The resulting queueing network models are, in general, not of the product form class and approximation schemes are employed as solution techniques. An example of a software module model of a hypothetical computer system is presented. The model is solved with a simulation program and three approximation schemes. The approximation results were compared with the simulation results and some schemes are found to produce good estimates of the effects of changing from reentrant to non-reentrant software modules.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {163--178},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/1035332.1035318},
 doi = {http://doi.acm.org/10.1145/1035332.1035318},
 acmid = {1035318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Agre:1982:MRN:1035293.1035318,
 author = {Agre, Jon R. and Tripathi, Satish K.},
 title = {Modeling reentrant and nonreentrant software},
 abstract = {A description of software module models for computer systems is presented. The software module models are based on a two level description, the software level and the hardware level, of the computer system. In the software module level it is possible to model performance effects of software traits such as reentrant and nonreentrant type software modules. The resulting queueing network models are, in general, not of the product form class and approximation schemes are employed as solution techniques. An example of a software module model of a hypothetical computer system is presented. The model is solved with a simulation program and three approximation schemes. The approximation results were compared with the simulation results and some schemes are found to produce good estimates of the effects of changing from reentrant to non-reentrant software modules.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {163--178},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/1035293.1035318},
 doi = {http://doi.acm.org/10.1145/1035293.1035318},
 acmid = {1035318},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wu:1982:OME:1035332.1035319,
 author = {Wu, L. T.},
 title = {Operational models for the evaluation of degradable computing systems},
 abstract = {Recent advances in multiprocessor technology have established the need for unified methods to evaluate computing systems performance and reliability. In response to this modeling need, this paper considers a general modeling framework which permits the modeling, analysis and evaluation of degradable computing systems. Within this framework, a simple and useful user-oriented performance variable is identified and shown to be a proper generalization of the traditional notions of system performance and reliability. The modeling and evaluation methods considered in this paper provide a relatively straightforward approach for integrating reliability and availability measures with performance measures. The hierarchical decomposition approach permits the modeling and evaluation of a computing system's subsystems (e.g., hardware, software, peripherals, interfaces, user demand systems) as a whole rather than the traditional methods of evaluating these subsystems independently. Accordingly, it becomes possible to evaluate the performance of the system software and the reliability of the system hardware simultaneously in order to measure the effectiveness of the system design. Since the performance variable introduced permits the characterization of the system performance according to the user's view of the systems, the results obtained represent more accurate assessments of the system's ability to perform than the existing performance or reliability measures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {179--185},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1035332.1035319},
 doi = {http://doi.acm.org/10.1145/1035332.1035319},
 acmid = {1035319},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wu:1982:OME:1035293.1035319,
 author = {Wu, L. T.},
 title = {Operational models for the evaluation of degradable computing systems},
 abstract = {Recent advances in multiprocessor technology have established the need for unified methods to evaluate computing systems performance and reliability. In response to this modeling need, this paper considers a general modeling framework which permits the modeling, analysis and evaluation of degradable computing systems. Within this framework, a simple and useful user-oriented performance variable is identified and shown to be a proper generalization of the traditional notions of system performance and reliability. The modeling and evaluation methods considered in this paper provide a relatively straightforward approach for integrating reliability and availability measures with performance measures. The hierarchical decomposition approach permits the modeling and evaluation of a computing system's subsystems (e.g., hardware, software, peripherals, interfaces, user demand systems) as a whole rather than the traditional methods of evaluating these subsystems independently. Accordingly, it becomes possible to evaluate the performance of the system software and the reliability of the system hardware simultaneously in order to measure the effectiveness of the system design. Since the performance variable introduced permits the characterization of the system performance according to the user's view of the systems, the results obtained represent more accurate assessments of the system's ability to perform than the existing performance or reliability measures.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {179--185},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1035293.1035319},
 doi = {http://doi.acm.org/10.1145/1035293.1035319},
 acmid = {1035319},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marie:1982:ECA:1035293.1035321,
 author = {Marie, Raymond A. and Snyder, Patricia M. and Stewart, William J.},
 title = {Extensions and computational aspects of an iterative method},
 abstract = {The so-called iterative methods are among a class of methods that have recently been applied to obtain approximate solutions to general queueing networks. In this paper it is shown that if the network contains feedback loops, then it is more advantageous to incorporate these loops into the analysis of the station itself rather than into the analysis of the complement of the station. We show how this analysis may be performed for a simple two-phase Coxian server. Additionally, it is shown that the number of iterations required to achieve a specified degree of accuracy may be considerably reduced by using a continuous updating procedure in which the computed throughputs are incorporated as soon as they are available, rather than at the end of an iteration. An efficient computational scheme is presented to accompany this continuous updating. Finally a number of examples are provided to illustrate these features.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {186--194},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035321},
 doi = {http://doi.acm.org/10.1145/1035293.1035321},
 acmid = {1035321},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marie:1982:ECA:1035332.1035321,
 author = {Marie, Raymond A. and Snyder, Patricia M. and Stewart, William J.},
 title = {Extensions and computational aspects of an iterative method},
 abstract = {The so-called iterative methods are among a class of methods that have recently been applied to obtain approximate solutions to general queueing networks. In this paper it is shown that if the network contains feedback loops, then it is more advantageous to incorporate these loops into the analysis of the station itself rather than into the analysis of the complement of the station. We show how this analysis may be performed for a simple two-phase Coxian server. Additionally, it is shown that the number of iterations required to achieve a specified degree of accuracy may be considerably reduced by using a continuous updating procedure in which the computed throughputs are incorporated as soon as they are available, rather than at the end of an iteration. An efficient computational scheme is presented to accompany this continuous updating. Finally a number of examples are provided to illustrate these features.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {186--194},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035321},
 doi = {http://doi.acm.org/10.1145/1035332.1035321},
 acmid = {1035321},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Neuse:1982:HHA:1035332.1035322,
 author = {Neuse, Doug and Chandy, K. Mani},
 title = {HAM: the heuristic aggregation method for solving general closed queueing network models of computer systems},
 abstract = {An approximate analytical method for estimating performance statistics of general closed queueing network models of computing systems is presented. These networks may include queues with priority scheduling disciplines and non-exponential servers and several classes of jobs. The method is based on the aggregation theorem (Norton's theorem) of Chandy, Herzog and Woo.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {195--212},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/1035332.1035322},
 doi = {http://doi.acm.org/10.1145/1035332.1035322},
 acmid = {1035322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aggregation theorem, analytical models, approximations, computer system models, general closed queueing networks, non-local-balance, non-product-form, performance analysis, priority scheduling},
} 

@inproceedings{Neuse:1982:HHA:1035293.1035322,
 author = {Neuse, Doug and Chandy, K. Mani},
 title = {HAM: the heuristic aggregation method for solving general closed queueing network models of computer systems},
 abstract = {An approximate analytical method for estimating performance statistics of general closed queueing network models of computing systems is presented. These networks may include queues with priority scheduling disciplines and non-exponential servers and several classes of jobs. The method is based on the aggregation theorem (Norton's theorem) of Chandy, Herzog and Woo.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {195--212},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/1035293.1035322},
 doi = {http://doi.acm.org/10.1145/1035293.1035322},
 acmid = {1035322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aggregation theorem, analytical models, approximations, computer system models, general closed queueing networks, non-local-balance, non-product-form, performance analysis, priority scheduling},
} 

@article{Eager:1982:PBH:1035332.1035324,
 author = {Eager, D. L. and Sevcik, K. C.},
 title = {Performance bound hierarchies for queueing networks},
 abstract = {In applications of queueing network models to computer system performance prediction, the computational effort required to obtain an exact equilibrium solution of a model may not be justified by the accuracy actually required. In these cases, there is a need for approximation or bounding techniques that can provide the necessary information at reduced cost. This paper presents Performance Bound Hierarchies (PBHs) for single class separable queueing networks consisting of fixed rate and delay service centers. A PBH consists of a hierarchy of upper (pessimistic) or lower (optimistic) bounds on mean system residence time. (The bounds can also be expressed as bounds on system throughput or center utilizations.) Each successive member requires more computational effort, and in the limit, the bounds converge to the exact solution.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {213--214},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1035332.1035324},
 doi = {http://doi.acm.org/10.1145/1035332.1035324},
 acmid = {1035324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Eager:1982:PBH:1035293.1035324,
 author = {Eager, D. L. and Sevcik, K. C.},
 title = {Performance bound hierarchies for queueing networks},
 abstract = {In applications of queueing network models to computer system performance prediction, the computational effort required to obtain an exact equilibrium solution of a model may not be justified by the accuracy actually required. In these cases, there is a need for approximation or bounding techniques that can provide the necessary information at reduced cost. This paper presents Performance Bound Hierarchies (PBHs) for single class separable queueing networks consisting of fixed rate and delay service centers. A PBH consists of a hierarchy of upper (pessimistic) or lower (optimistic) bounds on mean system residence time. (The bounds can also be expressed as bounds on system throughput or center utilizations.) Each successive member requires more computational effort, and in the limit, the bounds converge to the exact solution.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {213--214},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1035293.1035324},
 doi = {http://doi.acm.org/10.1145/1035293.1035324},
 acmid = {1035324},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brumfield:1982:EAH:1035332.1035325,
 author = {Brumfield, Jeffrey A. and Denning, Peter J.},
 title = {Error analysis of homogeneous mean queue and response time estimators},
 abstract = {Flow balance and homogeneity assumptions are needed to derive operational counterparts of M/M/1 queue length and response time formulas. This paper presents relationships between the assumption errors and the errors in the queue length and response time estimates. A simpler set of assumption error measures is used to derive bounds on the error in the response time estimate. An empirical study compares actual errors with their bounds.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {215--221},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1035332.1035325},
 doi = {http://doi.acm.org/10.1145/1035332.1035325},
 acmid = {1035325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brumfield:1982:EAH:1035293.1035325,
 author = {Brumfield, Jeffrey A. and Denning, Peter J.},
 title = {Error analysis of homogeneous mean queue and response time estimators},
 abstract = {Flow balance and homogeneity assumptions are needed to derive operational counterparts of M/M/1 queue length and response time formulas. This paper presents relationships between the assumption errors and the errors in the queue length and response time estimates. A simpler set of assumption error measures is used to derive bounds on the error in the response time estimate. An empirical study compares actual errors with their bounds.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {215--221},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1035293.1035325},
 doi = {http://doi.acm.org/10.1145/1035293.1035325},
 acmid = {1035325},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Harbitter:1982:MTL:1035332.1035327,
 author = {Harbitter, Alan and Tripathi, Satish K.},
 title = {A model of transport level flow control},
 abstract = {A Markov Decision Process model is developed to analyze buffer assignment at the transport level of the ARPAnet protocol. The result of the analysis is a method for obtaining an assignment policy which is optimal with respect to a delay/throughput/overhead reward function. The nature of the optimal policy is investigated by varying parameters of the reward.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1035332.1035327},
 doi = {http://doi.acm.org/10.1145/1035332.1035327},
 acmid = {1035327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Harbitter:1982:MTL:1035293.1035327,
 author = {Harbitter, Alan and Tripathi, Satish K.},
 title = {A model of transport level flow control},
 abstract = {A Markov Decision Process model is developed to analyze buffer assignment at the transport level of the ARPAnet protocol. The result of the analysis is a method for obtaining an assignment policy which is optimal with respect to a delay/throughput/overhead reward function. The nature of the optimal policy is investigated by varying parameters of the reward.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {222--232},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1035293.1035327},
 doi = {http://doi.acm.org/10.1145/1035293.1035327},
 acmid = {1035327},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelenbe:1982:CPC:1035293.1035328,
 author = {Gelenbe, Erol and Mitrani, Isi},
 title = {Control policies in CSMA local area networks: ethernet controls},
 abstract = {An analysis of the random carrier sense multiple access channel is presented in terms of the behaviour of each participating station. A detailed model of the station protocol, including the control policy used in case collisions, is used to derive the traffic and throughput of each station. The channel traffic characteristics are derived from this model and used, in turn, to derive the traffic parameters entering into the station model. This provides a solution method for complete system characteristics for a finite prespecified set of stations. The approach is then used to analyse control policies of the type used in ETHERNET. We show, in particular, that as the propagation delay becomes small, the specific form of the control policy tends to have a marginal effect on network performance. The approach also applies to the DANUBE and XANTHOS networks.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {233--240},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1035293.1035328},
 doi = {http://doi.acm.org/10.1145/1035293.1035328},
 acmid = {1035328},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gelenbe:1982:CPC:1035332.1035328,
 author = {Gelenbe, Erol and Mitrani, Isi},
 title = {Control policies in CSMA local area networks: ethernet controls},
 abstract = {An analysis of the random carrier sense multiple access channel is presented in terms of the behaviour of each participating station. A detailed model of the station protocol, including the control policy used in case collisions, is used to derive the traffic and throughput of each station. The channel traffic characteristics are derived from this model and used, in turn, to derive the traffic parameters entering into the station model. This provides a solution method for complete system characteristics for a finite prespecified set of stations. The approach is then used to analyse control policies of the type used in ETHERNET. We show, in particular, that as the propagation delay becomes small, the specific form of the control policy tends to have a marginal effect on network performance. The approach also applies to the DANUBE and XANTHOS networks.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {233--240},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1035332.1035328},
 doi = {http://doi.acm.org/10.1145/1035332.1035328},
 acmid = {1035328},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tripathi:1982:ATF:1035332.1035329,
 author = {Tripathi, Satish K. and Harbitter, Alan},
 title = {An analysis of two flow control techniques},
 abstract = {Queuing models can be useful tools in comparing the performance characteristics of different flow control techniques. In this paper the window control mechanism, incorporated in protocols such as X.25 is compared to the ARPAnet buffer reservation scheme. Multiclass queuing models are used to examine message throughput and delay characteristics. The analysis highlights the interaction of long and short message (in terms of length in packets) transmitters under the two flow control techniques.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {241--249},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035329},
 doi = {http://doi.acm.org/10.1145/1035332.1035329},
 acmid = {1035329},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tripathi:1982:ATF:1035293.1035329,
 author = {Tripathi, Satish K. and Harbitter, Alan},
 title = {An analysis of two flow control techniques},
 abstract = {Queuing models can be useful tools in comparing the performance characteristics of different flow control techniques. In this paper the window control mechanism, incorporated in protocols such as X.25 is compared to the ARPAnet buffer reservation scheme. Multiclass queuing models are used to examine message throughput and delay characteristics. The analysis highlights the interaction of long and short message (in terms of length in packets) transmitters under the two flow control techniques.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {241--249},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035329},
 doi = {http://doi.acm.org/10.1145/1035293.1035329},
 acmid = {1035329},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{King:1982:MCR:1035332.1035330,
 author = {King, P. J. B. and Mitrani, I.},
 title = {Modelling the Cambridge Ring},
 abstract = {Models for the local area computer network known as the Cambridge Ring are developed and evaluated. Two different levels of protocol are considered: the hardware and the Basic Block. These require different approaches and, in the second case, an approximate solution method. A limited comparison between the Cambridge Ring and another ring architecture - the token ring - is carried out.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {11},
 issue = {4},
 month = {August},
 year = {1982},
 issn = {0163-5999},
 pages = {250--258},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035332.1035330},
 doi = {http://doi.acm.org/10.1145/1035332.1035330},
 acmid = {1035330},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{King:1982:MCR:1035293.1035330,
 author = {King, P. J. B. and Mitrani, I.},
 title = {Modelling the Cambridge Ring},
 abstract = {Models for the local area computer network known as the Cambridge Ring are developed and evaluated. Two different levels of protocol are considered: the hardware and the Basic Block. These require different approaches and, in the second case, an approximate solution method. A limited comparison between the Cambridge Ring and another ring architecture - the token ring - is carried out.},
 booktitle = {Proceedings of the 1982 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '82},
 year = {1982},
 isbn = {0-89791-079-6},
 location = {Seattle, Washington},
 pages = {250--258},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1035293.1035330},
 doi = {http://doi.acm.org/10.1145/1035293.1035330},
 acmid = {1035330},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Browne:1981:DSP:800189.805467,
 author = {Browne, J. C.},
 title = {Designing systems for performance},
 abstract = {Real-time systems and systems to interface human work environments will dominate the growth of computer applications over the next decade. These systems must execute their functions with the time-liness and responsiveness required in these environments. The design, development and testing of such systems must guarantee performance as well as functionality and reliability. There is not yet in place a technology to support this requirement for engineering of performance. The research and development community in performance has focused primarily on analysis and deduction rather than the performance arena. This talk will define and discuss the tasks of engineering performance into software systems and describe the recent progress towards this goal.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/800189.805467},
 doi = {http://doi.acm.org/10.1145/800189.805467},
 acmid = {805467},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Browne:1981:DSP:1010629.805467,
 author = {Browne, J. C.},
 title = {Designing systems for performance},
 abstract = {Real-time systems and systems to interface human work environments will dominate the growth of computer applications over the next decade. These systems must execute their functions with the time-liness and responsiveness required in these environments. The design, development and testing of such systems must guarantee performance as well as functionality and reliability. There is not yet in place a technology to support this requirement for engineering of performance. The research and development community in performance has focused primarily on analysis and deduction rather than the performance arena. This talk will define and discuss the tasks of engineering performance into software systems and describe the recent progress towards this goal.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {1--},
 url = {http://doi.acm.org/10.1145/1010629.805467},
 doi = {http://doi.acm.org/10.1145/1010629.805467},
 acmid = {805467},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reiner:1981:MAP:1010629.805468,
 author = {Reiner, David and Pinkerton, Tad},
 title = {A method for adaptive performance improvement of operating systems},
 abstract = {This paper presents a method for dynamic modification of operating system control parameters to improve system performance. Improved parameter settings are learned by experimenting on the system. The experiments compare the performance of alternative parameter settings in each region of a partitioned load-performance space associated with the system. The results are used to modify important control parameters periodically, responding to fluctuations in system load and performance. The method can be used to implement adaptive tuning, to choose between alternative algorithms and policies, or to select the best fixed settings for parameters which are not modified. The method was validated and proved practical by an investigation of two parameters governing core quantum allocation on a Sperry Univac 1100 system. This experiment yielded significant results, which are presented and discussed. Directions for future research include automating the method, determining the effect of simultaneous modifications to unrelated control parameters, and detecting dominant control parameters.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {2--10},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1010629.805468},
 doi = {http://doi.acm.org/10.1145/1010629.805468},
 acmid = {805468},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reiner:1981:MAP:800189.805468,
 author = {Reiner, David and Pinkerton, Tad},
 title = {A method for adaptive performance improvement of operating systems},
 abstract = {This paper presents a method for dynamic modification of operating system control parameters to improve system performance. Improved parameter settings are learned by experimenting on the system. The experiments compare the performance of alternative parameter settings in each region of a partitioned load-performance space associated with the system. The results are used to modify important control parameters periodically, responding to fluctuations in system load and performance. The method can be used to implement adaptive tuning, to choose between alternative algorithms and policies, or to select the best fixed settings for parameters which are not modified. The method was validated and proved practical by an investigation of two parameters governing core quantum allocation on a Sperry Univac 1100 system. This experiment yielded significant results, which are presented and discussed. Directions for future research include automating the method, determining the effect of simultaneous modifications to unrelated control parameters, and detecting dominant control parameters.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {2--10},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800189.805468},
 doi = {http://doi.acm.org/10.1145/800189.805468},
 acmid = {805468},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:1981:VTP:1010629.805469,
 author = {Wang, Y. T.},
 title = {On the VAX/VMS time-critical process scheduling},
 abstract = {The VAX/VMS process schedule is briefly described. A simple priority-driven round-robin queuing model is then constructed to analyze the behavior of the time-critical processes of VAX/VMS under such a schedule. Mean and variance of the conditional response time of a process at a given priority are derived, conditioned on the amount of service time required by that process. Numerical results are given with comparisons to the ordinary priority queuing systems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {11--18},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1010629.805469},
 doi = {http://doi.acm.org/10.1145/1010629.805469},
 acmid = {805469},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:1981:VTP:800189.805469,
 author = {Wang, Y. T.},
 title = {On the VAX/VMS time-critical process scheduling},
 abstract = {The VAX/VMS process schedule is briefly described. A simple priority-driven round-robin queuing model is then constructed to analyze the behavior of the time-critical processes of VAX/VMS under such a schedule. Mean and variance of the conditional response time of a process at a given priority are derived, conditioned on the amount of service time required by that process. Numerical results are given with comparisons to the ordinary priority queuing systems.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {11--18},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800189.805469},
 doi = {http://doi.acm.org/10.1145/800189.805469},
 acmid = {805469},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Soderlund:1981:ECP:800189.805470,
 author = {S\"{o}derlund, Lars},
 title = {Evaluation of concurrent physical database reorganization through simulation modeling},
 abstract = {The performance of a database system commonly deteriorates due to degradation of the database's physical data structure. The structure degradation is a consequence of the normal operations of a general database management system. When system performance has degraded below acceptable limits the database must be reorganized. In conventional, periodic reorganization the database, or part of it, is taken off line while the data structure is being reorganized. This paper presents results from a study where it is shown that concurrent reorganization, i.e. a continous reorganization of the physical data structure while application processes have full access to the database, is an attractive alternative to conventional reorganization. The paper also presents a solution to a methodological problem concerning the simulation of a system which has activities with extremely varying durations.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {19--32},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/800189.805470},
 doi = {http://doi.acm.org/10.1145/800189.805470},
 acmid = {805470},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Soderlund:1981:ECP:1010629.805470,
 author = {S\"{o}derlund, Lars},
 title = {Evaluation of concurrent physical database reorganization through simulation modeling},
 abstract = {The performance of a database system commonly deteriorates due to degradation of the database's physical data structure. The structure degradation is a consequence of the normal operations of a general database management system. When system performance has degraded below acceptable limits the database must be reorganized. In conventional, periodic reorganization the database, or part of it, is taken off line while the data structure is being reorganized. This paper presents results from a study where it is shown that concurrent reorganization, i.e. a continous reorganization of the physical data structure while application processes have full access to the database, is an attractive alternative to conventional reorganization. The paper also presents a solution to a methodological problem concerning the simulation of a system which has activities with extremely varying durations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {19--32},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1010629.805470},
 doi = {http://doi.acm.org/10.1145/1010629.805470},
 acmid = {805470},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lazowska:1981:AMD:800189.805471,
 author = {Lazowska, Edward D. and Zahorjan, John},
 title = {Analytic modelling of disk I/O subsystems: A tutorial},
 abstract = {This is a summary of a tutorial presented during the conference discussing a number of approaches to representing disk I/O subsystems in analytic models of computer systems. As in any analytic modelling study, the fundamental objective in considering an I/O subsystem is to determine which devices should be represented in the model, and what their loadings should be. The device loadings represent the service required by jobs, and are the basic parameters needed by the computational algorithm which calculates performance measures for the model. To set these parameters, knowledge of service times at the various devices in the I/O subsystem is required. The tutorial begins by distinguishing analytic modelling from alternative approaches, by identifying the parameter values that are required for an analytic modelling study, and by explaining the role of the computational algorithm that is employed (Denning \&amp; Buzen [1978] provide a good, although lengthy, summary). We then consider a sequence of models of increasingly complex I/O subsystems. Next we discuss I/O subsystems with rotational position sensing. We then discuss approaches to modelling shared DASD, emphasizing hierarchical techniques in which highlevel models of each system can be analyzed in isolation. We also mention recent techniques for modelling complex I/O subsystems involving multipathing. Finally, we discuss the analysis of I/O subsystems based on broadcast channels such as Ethernet.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {33--35},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/800189.805471},
 doi = {http://doi.acm.org/10.1145/800189.805471},
 acmid = {805471},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lazowska:1981:AMD:1010629.805471,
 author = {Lazowska, Edward D. and Zahorjan, John},
 title = {Analytic modelling of disk I/O subsystems: A tutorial},
 abstract = {This is a summary of a tutorial presented during the conference discussing a number of approaches to representing disk I/O subsystems in analytic models of computer systems. As in any analytic modelling study, the fundamental objective in considering an I/O subsystem is to determine which devices should be represented in the model, and what their loadings should be. The device loadings represent the service required by jobs, and are the basic parameters needed by the computational algorithm which calculates performance measures for the model. To set these parameters, knowledge of service times at the various devices in the I/O subsystem is required. The tutorial begins by distinguishing analytic modelling from alternative approaches, by identifying the parameter values that are required for an analytic modelling study, and by explaining the role of the computational algorithm that is employed (Denning \&amp; Buzen [1978] provide a good, although lengthy, summary). We then consider a sequence of models of increasingly complex I/O subsystems. Next we discuss I/O subsystems with rotational position sensing. We then discuss approaches to modelling shared DASD, emphasizing hierarchical techniques in which highlevel models of each system can be analyzed in isolation. We also mention recent techniques for modelling complex I/O subsystems involving multipathing. Finally, we discuss the analysis of I/O subsystems based on broadcast channels such as Ethernet.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {33--35},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/1010629.805471},
 doi = {http://doi.acm.org/10.1145/1010629.805471},
 acmid = {805471},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dowdy:1981:MUS:1010629.805472,
 author = {Dowdy, Lawrence W. and Breitenlohner, Hans J.},
 title = {A model of Univac 1100/42 swapping},
 abstract = {The performance of a computer system depends upon the efficiency of its swapping mechanisms. The swapping efficiency is a complex function of many variables. The degree of multiprogramming, the relative loading on the swapping devices, and the speed of the swapping devices are all interdependent variables that affect swapping performance. In this paper, a model of swapping behavior is given. The interdependencies between the degree of multiprogramming, the swapping devices' loadings, and the swapping devices' speeds are modeled using an iterative scheme. The validation of a model is its predictive capability. The given swapping model was applied to a Univac 1100/42 system to predict the effect of moving the swapping activity from drums to discs. When the swapping activity was actually moved, throughput <underline>increased</underline> by 20\%. The model accurately predicted this improvement. Subtopics discussed include: 1) the modeling of blocked and overlapped disc seek activity, 2) the usefulness of empirical formulae, and 3) the calibration of unmeasurable parameters. Extensions and further applications of the model are given.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1010629.805472},
 doi = {http://doi.acm.org/10.1145/1010629.805472},
 acmid = {805472},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Closed queuing networks, Model validation, Parameter interdependencies, Performance prediction, Swapping},
} 

@inproceedings{Dowdy:1981:MUS:800189.805472,
 author = {Dowdy, Lawrence W. and Breitenlohner, Hans J.},
 title = {A model of Univac 1100/42 swapping},
 abstract = {The performance of a computer system depends upon the efficiency of its swapping mechanisms. The swapping efficiency is a complex function of many variables. The degree of multiprogramming, the relative loading on the swapping devices, and the speed of the swapping devices are all interdependent variables that affect swapping performance. In this paper, a model of swapping behavior is given. The interdependencies between the degree of multiprogramming, the swapping devices' loadings, and the swapping devices' speeds are modeled using an iterative scheme. The validation of a model is its predictive capability. The given swapping model was applied to a Univac 1100/42 system to predict the effect of moving the swapping activity from drums to discs. When the swapping activity was actually moved, throughput <underline>increased</underline> by 20\%. The model accurately predicted this improvement. Subtopics discussed include: 1) the modeling of blocked and overlapped disc seek activity, 2) the usefulness of empirical formulae, and 3) the calibration of unmeasurable parameters. Extensions and further applications of the model are given.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {36--47},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/800189.805472},
 doi = {http://doi.acm.org/10.1145/800189.805472},
 acmid = {805472},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Closed queuing networks, Model validation, Parameter interdependencies, Performance prediction, Swapping},
} 

@inproceedings{Turner:1981:SFP:800189.805473,
 author = {Turner, Rollins and Levy, Henry},
 title = {Segmented FIFO page replacement},
 abstract = {A fixed-space page replacement algorithm is presented. A variant of FIFO management using a secondary FIFO buffer, this algorithm provides a family of performance curves lying between FIFO and LRU. The implementation is simple, requires no periodic scanning, and uses no special hardware support. Simulations are used to determine the performance of the algorithm for several memory reference traces. Both the fault rates and overhead cost are examined.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {48--51},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800189.805473},
 doi = {http://doi.acm.org/10.1145/800189.805473},
 acmid = {805473},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FIFO page replacement, LRU page replacement, Page replacement algorithms, Performance evaluation},
} 

@article{Turner:1981:SFP:1010629.805473,
 author = {Turner, Rollins and Levy, Henry},
 title = {Segmented FIFO page replacement},
 abstract = {A fixed-space page replacement algorithm is presented. A variant of FIFO management using a secondary FIFO buffer, this algorithm provides a family of performance curves lying between FIFO and LRU. The implementation is simple, requires no periodic scanning, and uses no special hardware support. Simulations are used to determine the performance of the algorithm for several memory reference traces. Both the fault rates and overhead cost are examined.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {48--51},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1010629.805473},
 doi = {http://doi.acm.org/10.1145/1010629.805473},
 acmid = {805473},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FIFO page replacement, LRU page replacement, Page replacement algorithms, Performance evaluation},
} 

@article{Ferrari:1981:GMW:1010629.805474,
 author = {Ferrari, Domenico},
 title = {A generative model of working set dynamics},
 abstract = {An algorithm for generating a page reference string which exhibits a given working set size behavior in the time domain is presented, and the possible applications of such a string are discussed. The correctness of the algorithm is proved, and its computational complexity found to be linear in the length of the string. A program implementing the algorithm, which is performed in one pass and requires very little space, is briefly described, and some experimental results are given.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {52--57},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1010629.805474},
 doi = {http://doi.acm.org/10.1145/1010629.805474},
 acmid = {805474},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ferrari:1981:GMW:800189.805474,
 author = {Ferrari, Domenico},
 title = {A generative model of working set dynamics},
 abstract = {An algorithm for generating a page reference string which exhibits a given working set size behavior in the time domain is presented, and the possible applications of such a string are discussed. The correctness of the algorithm is proved, and its computational complexity found to be linear in the length of the string. A program implementing the algorithm, which is performed in one pass and requires very little space, is briefly described, and some experimental results are given.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {52--57},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800189.805474},
 doi = {http://doi.acm.org/10.1145/800189.805474},
 acmid = {805474},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahorjan:1981:BJB:1010629.805475,
 author = {Zahorjan, J. and Sevcik, K. C. and Eager, D. L. and Galler, B. I.},
 title = {Balanced job bound analysis of queueing networks},
 abstract = {Applications of queueing network models to computer system performance prediction typically involve the computation of their equilibrium solution. When numerous alternative systems are to be examined and the numbers of devices and customers are large, however, the expense of computing the exact solutions may not be warranted by the accuracy required. In such situations, it is desirable to be able to obtain bounds on the system solution with very little computation. Asymptotic bound analysis (ABA) is one technique for obtaining such bounds. In this paper, we introduce another bounding technique, called balanced job bounds (BJB), which is based on the analysis of systems in which all devices are equally utilized. These bounds are tighter than ABA bounds in many cases, but they are based on more restrictive assumptions (namely, those that lead to separable queueing network models).},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {58--},
 url = {http://doi.acm.org/10.1145/1010629.805475},
 doi = {http://doi.acm.org/10.1145/1010629.805475},
 acmid = {805475},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zahorjan:1981:BJB:800189.805475,
 author = {Zahorjan, J. and Sevcik, K. C. and Eager, D. L. and Galler, B. I.},
 title = {Balanced job bound analysis of queueing networks},
 abstract = {Applications of queueing network models to computer system performance prediction typically involve the computation of their equilibrium solution. When numerous alternative systems are to be examined and the numbers of devices and customers are large, however, the expense of computing the exact solutions may not be warranted by the accuracy required. In such situations, it is desirable to be able to obtain bounds on the system solution with very little computation. Asymptotic bound analysis (ABA) is one technique for obtaining such bounds. In this paper, we introduce another bounding technique, called balanced job bounds (BJB), which is based on the analysis of systems in which all devices are equally utilized. These bounds are tighter than ABA bounds in many cases, but they are based on more restrictive assumptions (namely, those that lead to separable queueing network models).},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {58--},
 url = {http://doi.acm.org/10.1145/800189.805475},
 doi = {http://doi.acm.org/10.1145/800189.805475},
 acmid = {805475},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Neuse:1981:SHA:1010629.805476,
 author = {Neuse, D. and Chandy, K.},
 title = {SCAT: A heuristic algorithm for queueing network models of computing systems},
 abstract = {This paper presents a new algorithm for the approximate analysis of closed product-form queueing networks with fixed-rate, delay (infinite-server), and load-dependent queues. This algorithm has the accuracy, speed, small memory requirements, and simplicity necessary for inclusion in a general network analysis package. The algorithm allows networks with large numbers of queues, job classes, and populations to be analyzed interactively even on microcomputers with very limited memory.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {59--79},
 numpages = {21},
 url = {http://doi.acm.org/10.1145/1010629.805476},
 doi = {http://doi.acm.org/10.1145/1010629.805476},
 acmid = {805476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Analytic models, Approximations, Iterative algorithms, Load-dependent queues, Performance analysis, Product-form, Queueing networks},
} 

@inproceedings{Neuse:1981:SHA:800189.805476,
 author = {Neuse, D. and Chandy, K.},
 title = {SCAT: A heuristic algorithm for queueing network models of computing systems},
 abstract = {This paper presents a new algorithm for the approximate analysis of closed product-form queueing networks with fixed-rate, delay (infinite-server), and load-dependent queues. This algorithm has the accuracy, speed, small memory requirements, and simplicity necessary for inclusion in a general network analysis package. The algorithm allows networks with large numbers of queues, job classes, and populations to be analyzed interactively even on microcomputers with very limited memory.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {59--79},
 numpages = {21},
 url = {http://doi.acm.org/10.1145/800189.805476},
 doi = {http://doi.acm.org/10.1145/800189.805476},
 acmid = {805476},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Analytic models, Approximations, Iterative algorithms, Load-dependent queues, Performance analysis, Product-form, Queueing networks},
} 

@inproceedings{Zahorjan:1981:SSQ:800189.805477,
 author = {Zahorjan, John and Wong, Eugene},
 title = {The solution of separable queueing network models using mean value analysis},
 abstract = {Because it is more intuitively understandable than the previously existing convolution algorithms, Mean Value Analysis (MVA) has gained great popularity as an exact solution technique for separable queueing networks. However, the derivations of MVA presented to date apply only to closed queueing network models. Additionally, the problem of the storage requirement of MVA has not been dealt with satisfactorily. In this paper we address both these problems, presenting MVA solutions for open and mixed load independent networks, and a storage maintenance technique that we postulate is the minimum possible of any ``reasonable" MVA technique.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {80--85},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800189.805477},
 doi = {http://doi.acm.org/10.1145/800189.805477},
 acmid = {805477},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahorjan:1981:SSQ:1010629.805477,
 author = {Zahorjan, John and Wong, Eugene},
 title = {The solution of separable queueing network models using mean value analysis},
 abstract = {Because it is more intuitively understandable than the previously existing convolution algorithms, Mean Value Analysis (MVA) has gained great popularity as an exact solution technique for separable queueing networks. However, the derivations of MVA presented to date apply only to closed queueing network models. Additionally, the problem of the storage requirement of MVA has not been dealt with satisfactorily. In this paper we address both these problems, presenting MVA solutions for open and mixed load independent networks, and a storage maintenance technique that we postulate is the minimum possible of any ``reasonable" MVA technique.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {80--85},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1010629.805477},
 doi = {http://doi.acm.org/10.1145/1010629.805477},
 acmid = {805477},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Thomasian:1981:ASQ:800189.805478,
 author = {Thomasian, Alexander and Nadji, Behzad},
 title = {Aggregation of stations in queueing network models of multiprogrammed computers},
 abstract = {In queueing network models the complexity of the model can be reduced by aggregating stations. This amounts to obtaining the throughput of the flow-equivalent station for the subnetwork of stations to be aggregated. When the subnetwork has a separable solution, aggregation can be carried out using the Chandy-Herzog-Woo theorem. The throughput of the subnetwork can be expressed explicitly in terms of its parameters when the stations are balanced (have equal utilizations). This expression for throughput can be used as an approximation when the stations are relatively unbalanced. The basic expression can be modified to increase the accuracy of the approximation. A generating function approach was used to obtain upper bounds on the relative error due to the basic approximation and its modifications. Provided that the relative error bound is tolerable, a set of unbalanced stations can be replaced by a single aggregate station or a set of balanced stations. Finally, we propose a methodology to simplify the queueing network model of a large-scale multiprogrammed computer, which makes use of the previous aggregation results.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {86--104},
 numpages = {19},
 url = {http://doi.acm.org/10.1145/800189.805478},
 doi = {http://doi.acm.org/10.1145/800189.805478},
 acmid = {805478},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Thomasian:1981:ASQ:1010629.805478,
 author = {Thomasian, Alexander and Nadji, Behzad},
 title = {Aggregation of stations in queueing network models of multiprogrammed computers},
 abstract = {In queueing network models the complexity of the model can be reduced by aggregating stations. This amounts to obtaining the throughput of the flow-equivalent station for the subnetwork of stations to be aggregated. When the subnetwork has a separable solution, aggregation can be carried out using the Chandy-Herzog-Woo theorem. The throughput of the subnetwork can be expressed explicitly in terms of its parameters when the stations are balanced (have equal utilizations). This expression for throughput can be used as an approximation when the stations are relatively unbalanced. The basic expression can be modified to increase the accuracy of the approximation. A generating function approach was used to obtain upper bounds on the relative error due to the basic approximation and its modifications. Provided that the relative error bound is tolerable, a set of unbalanced stations can be replaced by a single aggregate station or a set of balanced stations. Finally, we propose a methodology to simplify the queueing network model of a large-scale multiprogrammed computer, which makes use of the previous aggregation results.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {86--104},
 numpages = {19},
 url = {http://doi.acm.org/10.1145/1010629.805478},
 doi = {http://doi.acm.org/10.1145/1010629.805478},
 acmid = {805478},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schwetman:1981:CSM:800189.805479,
 author = {Schwetman, Herb},
 title = {Computer system models: An introduction},
 abstract = {A system model is a tool used to predict system performance under changing conditions. There are two widely used modeling techniques: one based on discrete event simulation and one based on queuing theory models. Because queueing theory models are so much cheaper to implement and use, as compared to simulation models, there is growing interest in them. Users are developing and using queuing theory models to project system performance, project capacity, analyze bottlenecks and configure systems. This talk uses an operational analysis approach to develop system models. This approach, as presented in Denning and Buzen [1], provides an intuitive basis for analyzing system performance and constructing system models. Very simple calculations lead to estimates of bounds on performance - maximum job throughput rates and minimum message response times. The emphasis is on gaining an understanding of system models which reenforces intuition, not on mathematical formulae. Several examples are included. References to other works and publications are provided. Application areas and limitations of modeling techniques are discussed.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {105--},
 url = {http://doi.acm.org/10.1145/800189.805479},
 doi = {http://doi.acm.org/10.1145/800189.805479},
 acmid = {805479},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schwetman:1981:CSM:1010629.805479,
 author = {Schwetman, Herb},
 title = {Computer system models: An introduction},
 abstract = {A system model is a tool used to predict system performance under changing conditions. There are two widely used modeling techniques: one based on discrete event simulation and one based on queuing theory models. Because queueing theory models are so much cheaper to implement and use, as compared to simulation models, there is growing interest in them. Users are developing and using queuing theory models to project system performance, project capacity, analyze bottlenecks and configure systems. This talk uses an operational analysis approach to develop system models. This approach, as presented in Denning and Buzen [1], provides an intuitive basis for analyzing system performance and constructing system models. Very simple calculations lead to estimates of bounds on performance - maximum job throughput rates and minimum message response times. The emphasis is on gaining an understanding of system models which reenforces intuition, not on mathematical formulae. Several examples are included. References to other works and publications are provided. Application areas and limitations of modeling techniques are discussed.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {105--},
 url = {http://doi.acm.org/10.1145/1010629.805479},
 doi = {http://doi.acm.org/10.1145/1010629.805479},
 acmid = {805479},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Denning:1981:PEE:1010629.805480,
 author = {Denning, Peter J.},
 title = {Performance evaluation: Experimental computer science at its best},
 abstract = {What is experimental computer science? This question has been widely discussed ever since the Feldman Report was published (1979 [18]). Many computer scientists believe that survival of their discipline is intimately linked to their ability to rejuvenate experimentation. The National Science Foundation instituted the Coordinated Experimental Research Program (CERP) in 1979 to help universities set up facilities capable of supporting experimental research. Other agencies of government are considering similar programs. Some industrial firms are offering similar help through modest cash grants and equipment discounts. What is experimental computer science? Surprisingly, computer scientists disagree on the answer. A few believe that computer science is in flux\&mdash;making a transition from theoretical to experimental science\&mdash;and, hence, no operational definition is yet available. Some believe that it is all the non-theoretical activities of computer science, especially those conferring ``hands-on" experience. Quite a few believe that it is large system development projects\&mdash;i.e., computer and software engineering\&mdash;and they cite MIT's Multics, Berkeley's version of Bell Labs' UNIX, the ARPAnet, IBM's database System R, and Xerox's Ethernet-based personal computer network as examples. These beliefs are wrong. There are well-established standards for experimental science. The field of performance evaluation meets these standards and provides examples of experimental science for the rest of the computing field.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {106--109},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1010629.805480},
 doi = {http://doi.acm.org/10.1145/1010629.805480},
 acmid = {805480},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Denning:1981:PEE:800189.805480,
 author = {Denning, Peter J.},
 title = {Performance evaluation: Experimental computer science at its best},
 abstract = {What is experimental computer science? This question has been widely discussed ever since the Feldman Report was published (1979 [18]). Many computer scientists believe that survival of their discipline is intimately linked to their ability to rejuvenate experimentation. The National Science Foundation instituted the Coordinated Experimental Research Program (CERP) in 1979 to help universities set up facilities capable of supporting experimental research. Other agencies of government are considering similar programs. Some industrial firms are offering similar help through modest cash grants and equipment discounts. What is experimental computer science? Surprisingly, computer scientists disagree on the answer. A few believe that computer science is in flux\&mdash;making a transition from theoretical to experimental science\&mdash;and, hence, no operational definition is yet available. Some believe that it is all the non-theoretical activities of computer science, especially those conferring ``hands-on" experience. Quite a few believe that it is large system development projects\&mdash;i.e., computer and software engineering\&mdash;and they cite MIT's Multics, Berkeley's version of Bell Labs' UNIX, the ARPAnet, IBM's database System R, and Xerox's Ethernet-based personal computer network as examples. These beliefs are wrong. There are well-established standards for experimental science. The field of performance evaluation meets these standards and provides examples of experimental science for the rest of the computing field.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {106--109},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800189.805480},
 doi = {http://doi.acm.org/10.1145/800189.805480},
 acmid = {805480},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Rafii:1981:SAM:1010629.805481,
 author = {Rafii, Abbas},
 title = {Structure and application of a measurement tool-SAMPLER/3000},
 abstract = {Design, internal structure, implementation experience and a number of unique features of the SAMPLER/3000 performance evaluation tool are presented. This package can be used to produce program CPU and wait time profiles in several levels of detail in terms of code segments, procedure names and procedure relative addresses. It also provides an accurate profile of the operating systems code which is exercised to service requests from the selective parts of the user code. Programs can be observed under natural load conditions in a single user or shared environment. A program's CPU usage is determined in terms of direct and indirect cost components. The approaches to determine direct and indirect CPU times are described. A program counter sampling technique in virtual memory domain is discussed. Certain interesting aspects of data analysis and on-line data presentation techniques are described. The features of the computer architecture, the services of the loader and compilers which relate to the operation of the tool are discussed. A case study is finally presented.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {110--120},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1010629.805481},
 doi = {http://doi.acm.org/10.1145/1010629.805481},
 acmid = {805481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rafii:1981:SAM:800189.805481,
 author = {Rafii, Abbas},
 title = {Structure and application of a measurement tool-SAMPLER/3000},
 abstract = {Design, internal structure, implementation experience and a number of unique features of the SAMPLER/3000 performance evaluation tool are presented. This package can be used to produce program CPU and wait time profiles in several levels of detail in terms of code segments, procedure names and procedure relative addresses. It also provides an accurate profile of the operating systems code which is exercised to service requests from the selective parts of the user code. Programs can be observed under natural load conditions in a single user or shared environment. A program's CPU usage is determined in terms of direct and indirect cost components. The approaches to determine direct and indirect CPU times are described. A program counter sampling technique in virtual memory domain is discussed. Certain interesting aspects of data analysis and on-line data presentation techniques are described. The features of the computer architecture, the services of the loader and compilers which relate to the operation of the tool are discussed. A case study is finally presented.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {110--120},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800189.805481},
 doi = {http://doi.acm.org/10.1145/800189.805481},
 acmid = {805481},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Tolopka:1981:ETM:1010629.805482,
 author = {Tolopka, Stephen},
 title = {An event trace monitor for the VAX 11/780},
 abstract = {This paper describes an event trace monitor implemented on Version 1.6 of the VMS operating system at Purdue University. Some necessary VMS terminology is covered first. The operation of the data gathering mechanism is then explained, and the events currently being gathered are listed. A second program, which reduces the data gathered by the monitor to usable form, is next examined, and some examples depicting its operation are given. The paper concludes with a brief discussion of some of the monitor's uses.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {121--128},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1010629.805482},
 doi = {http://doi.acm.org/10.1145/1010629.805482},
 acmid = {805482},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tolopka:1981:ETM:800189.805482,
 author = {Tolopka, Stephen},
 title = {An event trace monitor for the VAX 11/780},
 abstract = {This paper describes an event trace monitor implemented on Version 1.6 of the VMS operating system at Purdue University. Some necessary VMS terminology is covered first. The operation of the data gathering mechanism is then explained, and the events currently being gathered are listed. A second program, which reduces the data gathered by the monitor to usable form, is next examined, and some examples depicting its operation are given. The paper concludes with a brief discussion of some of the monitor's uses.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {121--128},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800189.805482},
 doi = {http://doi.acm.org/10.1145/800189.805482},
 acmid = {805482},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Artis:1981:LFD:1010629.805483,
 author = {Artis, H. Pat},
 title = {A log file design for analyzing secondary storage occupancy},
 abstract = {A description of the design and implementation of a log file for analyzing the occupancy of secondary storage on IBM computer systems is discussed. Typical applications of the data contained in the log are also discussed.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {129--135},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1010629.805483},
 doi = {http://doi.acm.org/10.1145/1010629.805483},
 acmid = {805483},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Artis:1981:LFD:800189.805483,
 author = {Artis, H. Pat},
 title = {A log file design for analyzing secondary storage occupancy},
 abstract = {A description of the design and implementation of a log file for analyzing the occupancy of secondary storage on IBM computer systems is discussed. Typical applications of the data contained in the log are also discussed.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {129--135},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800189.805483},
 doi = {http://doi.acm.org/10.1145/800189.805483},
 acmid = {805483},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sanguinetti:1981:ESS:1010629.805484,
 author = {Sanguinetti, John},
 title = {The effects of solid state paging devices in a large time-sharing system},
 abstract = {This paper reports the results of some measurements taken on the effects two new solid state paging devices, the STC 4305 and the Intel 3805, have on paging performance in the Michigan Terminal System at the University of Michigan. The measurements were taken with a software monitor using various configurations of the two solid state devices and the fixed head disk, which they replace. Measurements were taken both during regular production and using an artificial load created to exercise the paging subsystem. The results confirmed the expectation that the solid state paging devices provide shorter page-in waiting times than the fixed-head disk, and also pointed up some of the effects which their differing architectures have on the system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {136--153},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/1010629.805484},
 doi = {http://doi.acm.org/10.1145/1010629.805484},
 acmid = {805484},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sanguinetti:1981:ESS:800189.805484,
 author = {Sanguinetti, John},
 title = {The effects of solid state paging devices in a large time-sharing system},
 abstract = {This paper reports the results of some measurements taken on the effects two new solid state paging devices, the STC 4305 and the Intel 3805, have on paging performance in the Michigan Terminal System at the University of Michigan. The measurements were taken with a software monitor using various configurations of the two solid state devices and the fixed head disk, which they replace. Measurements were taken both during regular production and using an artificial load created to exercise the paging subsystem. The results confirmed the expectation that the solid state paging devices provide shorter page-in waiting times than the fixed-head disk, and also pointed up some of the effects which their differing architectures have on the system.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {136--153},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/800189.805484},
 doi = {http://doi.acm.org/10.1145/800189.805484},
 acmid = {805484},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wang:1981:VMS:800189.805485,
 author = {Wang, Richard T. and Browne, J. C.},
 title = {Virtual machine-based simulation of distributed computing and network computing},
 abstract = {This paper proposes the use of virtual machine architectures as a means of modeling and analyzing networks and distributed computing systems. The requirements for such modeling and analysis are explored and defined along with an illustrative study of an X.25 link-level protocol performance under normal execution conditions. The virtualizable architecture used in this work is the Data General Nova 3/D.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {154--156},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/800189.805485},
 doi = {http://doi.acm.org/10.1145/800189.805485},
 acmid = {805485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wang:1981:VMS:1010629.805485,
 author = {Wang, Richard T. and Browne, J. C.},
 title = {Virtual machine-based simulation of distributed computing and network computing},
 abstract = {This paper proposes the use of virtual machine architectures as a means of modeling and analyzing networks and distributed computing systems. The requirements for such modeling and analysis are explored and defined along with an illustrative study of an X.25 link-level protocol performance under normal execution conditions. The virtualizable architecture used in this work is the Data General Nova 3/D.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {154--156},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/1010629.805485},
 doi = {http://doi.acm.org/10.1145/1010629.805485},
 acmid = {805485},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Huslende:1981:CEP:800189.805486,
 author = {Huslende, Ragnar},
 title = {A combined evaluation of performance and reliability for degradable systems},
 abstract = {As the field of fault-tolerant computing is maturing and results from this field are taken into practical use the effects of a failure in a computer system need not be catastrophic. With good fault-detection mechanisms it is now possible to cover a very high percentage of all the possible failures that can occur. Once a fault is detected, systems are designed to reconfigure and proceed either with full or degraded performance depending on how much redundancy is built into the system. It should be noted that one particular failure may have different effects depending on the circumstances and the time at which it occurs. Today we see that large numbers of resources are being tied together in complex computer systems, either locally or in geographically distributed systems and networks. In such systems it is obviously very undesirable that the failure of one element can bring the entire system down. On the other hand one can usually not afford to design the system with sufficient redundancy to mask the effect of all failures immediately.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {157--164},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800189.805486},
 doi = {http://doi.acm.org/10.1145/800189.805486},
 acmid = {805486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Huslende:1981:CEP:1010629.805486,
 author = {Huslende, Ragnar},
 title = {A combined evaluation of performance and reliability for degradable systems},
 abstract = {As the field of fault-tolerant computing is maturing and results from this field are taken into practical use the effects of a failure in a computer system need not be catastrophic. With good fault-detection mechanisms it is now possible to cover a very high percentage of all the possible failures that can occur. Once a fault is detected, systems are designed to reconfigure and proceed either with full or degraded performance depending on how much redundancy is built into the system. It should be noted that one particular failure may have different effects depending on the circumstances and the time at which it occurs. Today we see that large numbers of resources are being tied together in complex computer systems, either locally or in geographically distributed systems and networks. In such systems it is obviously very undesirable that the failure of one element can bring the entire system down. On the other hand one can usually not afford to design the system with sufficient redundancy to mask the effect of all failures immediately.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {157--164},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1010629.805486},
 doi = {http://doi.acm.org/10.1145/1010629.805486},
 acmid = {805486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jacobson:1981:MSD:1010629.805487,
 author = {Jacobson, Patricia A. and Lazowska, Edward D.},
 title = {The method of surrogate delays: Simultaneous resource possession in analytic models of computer systems},
 abstract = {This paper presents a new approach to modelling the simultaneous or overlapped possession of resources in queueing networks. The key concept is that of iteration between two models, each of which includes an explicit representation of one of the simultaneously held resources and a delay server (an infinite server, with service time but no queueing) acting as a surrogate for queueing delay due to congestion at the other simultaneously held resource. Because of this, we refer to our approximation technique as the ``method of surrogate delays".},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {165--174},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1010629.805487},
 doi = {http://doi.acm.org/10.1145/1010629.805487},
 acmid = {805487},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jacobson:1981:MSD:800189.805487,
 author = {Jacobson, Patricia A. and Lazowska, Edward D.},
 title = {The method of surrogate delays: Simultaneous resource possession in analytic models of computer systems},
 abstract = {This paper presents a new approach to modelling the simultaneous or overlapped possession of resources in queueing networks. The key concept is that of iteration between two models, each of which includes an explicit representation of one of the simultaneously held resources and a delay server (an infinite server, with service time but no queueing) acting as a surrogate for queueing delay due to congestion at the other simultaneously held resource. Because of this, we refer to our approximation technique as the ``method of surrogate delays".},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {165--174},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800189.805487},
 doi = {http://doi.acm.org/10.1145/800189.805487},
 acmid = {805487},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Jacobson:1981:AAM:1010629.805488,
 author = {Jacobson, Patricia},
 title = {Approximate analytic models of arbiters},
 abstract = {Results at very light and very heavy loads are easy to obtain, but at intermediate loads performance modelling is necessary. Because of the considerable cost of simulation, we develope queueing network models which can be solved quickly by approximate analytic techniques. These models are validated by comparing with simulations at certain points, and then used to get a wide range of results quickly.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {175--180},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1010629.805488},
 doi = {http://doi.acm.org/10.1145/1010629.805488},
 acmid = {805488},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Jacobson:1981:AAM:800189.805488,
 author = {Jacobson, Patricia},
 title = {Approximate analytic models of arbiters},
 abstract = {Results at very light and very heavy loads are easy to obtain, but at intermediate loads performance modelling is necessary. Because of the considerable cost of simulation, we develope queueing network models which can be solved quickly by approximate analytic techniques. These models are validated by comparing with simulations at certain points, and then used to get a wide range of results quickly.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {175--180},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800189.805488},
 doi = {http://doi.acm.org/10.1145/800189.805488},
 acmid = {805488},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Briggs:1981:PCM:800189.805489,
 author = {Briggs, Fay\'{e} A. and Dubois, Michel},
 title = {Performance of cache-based multiprocessors},
 abstract = {A possible design alternative to improve the performance of a multiprocessor system is to insert a private cache between each processor and the shared memory. The caches act as high-speed buffers, reducing the memory access time, and affect the delays caused by memory conflicts. In this paper, we study the performance of a multiprocessor system with caches. The shared memory is pipelined and interleaved to improve the block transfer rate, and assumes an L-M organization, previously studied under random word access. An approximate model is developed to estimate the processor utilization and the speedup improvement provided by the caches. These two parameters are essential to a cost-effective design. An example of a design is treated to illustrate the usefulness of this investigation.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800189.805489},
 doi = {http://doi.acm.org/10.1145/800189.805489},
 acmid = {805489},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Briggs:1981:PCM:1010629.805489,
 author = {Briggs, Fay\'{e} A. and Dubois, Michel},
 title = {Performance of cache-based multiprocessors},
 abstract = {A possible design alternative to improve the performance of a multiprocessor system is to insert a private cache between each processor and the shared memory. The caches act as high-speed buffers, reducing the memory access time, and affect the delays caused by memory conflicts. In this paper, we study the performance of a multiprocessor system with caches. The shared memory is pipelined and interleaved to improve the block transfer rate, and assumes an L-M organization, previously studied under random word access. An approximate model is developed to estimate the processor utilization and the speedup improvement provided by the caches. These two parameters are essential to a cost-effective design. An example of a design is treated to illustrate the usefulness of this investigation.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {181--190},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1010629.805489},
 doi = {http://doi.acm.org/10.1145/1010629.805489},
 acmid = {805489},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bryant:1981:QNA:1010629.805490,
 author = {Bryant, R. M. and Agre, J. R.},
 title = {A queueing network approach to the module allocation problem in distributed systems},
 abstract = {Given a collection of distributed programs and the modules they use, the module allocation problem is to determine an assignment of modules to processors that minimizes the total execution cost of the programs. Standard approaches to this problem are based on solving either a network flow problem or a constrained 0-1 integer programming problem. In this paper we discuss an alternative approach to the module allocation problem where a closed, multiclass queueing network is solved to determine the cost of a particular module allocation. The advantage of this approach is that the execution cost can be expressed in terms of performance measures of the system such as response time. An interchange heuristic is proposed as a method of searching for a good module allocation using this model and empirical evidence for the success of the heuristic is given. The heuristic normally finds module allocations with costs within 10 percent of the optimal module allocation. Fast, approximate queueing network solution techniques based on mean-value-analysis allow each heuristic search to be completed in a few seconds of CPU time. The computational complexity of each search is O (M K (K + N) C) where M is the number of modules, K is the number of sites in the network, N is the number of communications processors, and C is the number of distributed program types. It appears that substantial problems of this type could be solved using the methods we describe.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {191--204},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1010629.805490},
 doi = {http://doi.acm.org/10.1145/1010629.805490},
 acmid = {805490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Distributed computer systems, File assignment problem, Mean-value analysis, Multiclass queueing network model, Task allocation problem},
} 

@inproceedings{Bryant:1981:QNA:800189.805490,
 author = {Bryant, R. M. and Agre, J. R.},
 title = {A queueing network approach to the module allocation problem in distributed systems},
 abstract = {Given a collection of distributed programs and the modules they use, the module allocation problem is to determine an assignment of modules to processors that minimizes the total execution cost of the programs. Standard approaches to this problem are based on solving either a network flow problem or a constrained 0-1 integer programming problem. In this paper we discuss an alternative approach to the module allocation problem where a closed, multiclass queueing network is solved to determine the cost of a particular module allocation. The advantage of this approach is that the execution cost can be expressed in terms of performance measures of the system such as response time. An interchange heuristic is proposed as a method of searching for a good module allocation using this model and empirical evidence for the success of the heuristic is given. The heuristic normally finds module allocations with costs within 10 percent of the optimal module allocation. Fast, approximate queueing network solution techniques based on mean-value-analysis allow each heuristic search to be completed in a few seconds of CPU time. The computational complexity of each search is O (M K (K + N) C) where M is the number of modules, K is the number of sites in the network, N is the number of communications processors, and C is the number of distributed program types. It appears that substantial problems of this type could be solved using the methods we describe.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {191--204},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/800189.805490},
 doi = {http://doi.acm.org/10.1145/800189.805490},
 acmid = {805490},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Distributed computer systems, File assignment problem, Mean-value analysis, Multiclass queueing network model, Task allocation problem},
} 

@inproceedings{Marathe:1981:AME:800189.805491,
 author = {Marathe, Madhav and Kumar, Sujit},
 title = {Analytical models for an Ethernet-like local area network link},
 abstract = {Ethernet -like local area network links have been studied by a number of researchers. Most of these studies have involved extensive simulation models operating at the level of individual packets. However, as we begin building models of systems built around such links, detailed simulation models are neither necessary, nor cost-effective. Instead, a simple analytical model of the medium should be adequate as a component of the higher level system models. This paper discusses a number of analytical models and identifies a last-in-first-out M/G/1 model with slightly increased service time as one which adequately captures both the mean and the coefficient of variation of the response time. Given any offered load, this model can be used to predict the mean waiting time and its coefficient of variation. These two can be used to construct a suitable 2 stage hyperexponential distribution. Random numbers can then be drawn from this distribution for use as waiting times of individual packets.},
 booktitle = {Proceedings of the 1981 ACM SIGMETRICS conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '81},
 year = {1981},
 isbn = {0-89791-051-6},
 location = {Las Vegas, Nevada, United States},
 pages = {205--215},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800189.805491},
 doi = {http://doi.acm.org/10.1145/800189.805491},
 acmid = {805491},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marathe:1981:AME:1010629.805491,
 author = {Marathe, Madhav and Kumar, Sujit},
 title = {Analytical models for an Ethernet-like local area network link},
 abstract = {Ethernet -like local area network links have been studied by a number of researchers. Most of these studies have involved extensive simulation models operating at the level of individual packets. However, as we begin building models of systems built around such links, detailed simulation models are neither necessary, nor cost-effective. Instead, a simple analytical model of the medium should be adequate as a component of the higher level system models. This paper discusses a number of analytical models and identifies a last-in-first-out M/G/1 model with slightly increased service time as one which adequately captures both the mean and the coefficient of variation of the response time. Given any offered load, this model can be used to predict the mean waiting time and its coefficient of variation. These two can be used to construct a suitable 2 stage hyperexponential distribution. Random numbers can then be drawn from this distribution for use as waiting times of individual packets.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {10},
 issue = {3},
 month = {September},
 year = {1981},
 issn = {0163-5999},
 pages = {205--215},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1010629.805491},
 doi = {http://doi.acm.org/10.1145/1010629.805491},
 acmid = {805491},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Blake:1979:TSM:800188.805444,
 author = {Blake, Russ},
 title = {Tailor: A simple model that works},
 abstract = {Tailor is an atomic model of the Tandem/16 multiple-computer system. Atomic modeling is based on operational analysis and general considerations from queueing theory. Measurements of system atoms define the underlying components of processor usage. The workload is described to the model through a separate set of measurable parameters that comprise the workload atoms. Simple formulae from operational analysis are then applied to predict the amount of equipment necessary to support the projected application. Tailor's accuracy was tested under two very different workloads. For both a large backend database application and a program development system, Tailor was able to predict the equipment needed to handle the workloads to within 5 percent.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800188.805444},
 doi = {http://doi.acm.org/10.1145/800188.805444},
 acmid = {805444},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Blake:1979:TSM:1009373.805444,
 author = {Blake, Russ},
 title = {Tailor: A simple model that works},
 abstract = {Tailor is an atomic model of the Tandem/16 multiple-computer system. Atomic modeling is based on operational analysis and general considerations from queueing theory. Measurements of system atoms define the underlying components of processor usage. The workload is described to the model through a separate set of measurable parameters that comprise the workload atoms. Simple formulae from operational analysis are then applied to predict the amount of equipment necessary to support the projected application. Tailor's accuracy was tested under two very different workloads. For both a large backend database application and a program development system, Tailor was able to predict the equipment needed to handle the workloads to within 5 percent.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1009373.805444},
 doi = {http://doi.acm.org/10.1145/1009373.805444},
 acmid = {805444},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Blake:1979:TSM:1013608.805444,
 author = {Blake, Russ},
 title = {Tailor: A simple model that works},
 abstract = {Tailor is an atomic model of the Tandem/16 multiple-computer system. Atomic modeling is based on operational analysis and general considerations from queueing theory. Measurements of system atoms define the underlying components of processor usage. The workload is described to the model through a separate set of measurable parameters that comprise the workload atoms. Simple formulae from operational analysis are then applied to predict the amount of equipment necessary to support the projected application. Tailor's accuracy was tested under two very different workloads. For both a large backend database application and a program development system, Tailor was able to predict the equipment needed to handle the workloads to within 5 percent.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013608.805444},
 doi = {http://doi.acm.org/10.1145/1013608.805444},
 acmid = {805444},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Strecker:1979:ACP:1013608.805445,
 author = {Strecker, William D.},
 title = {An analysis of central processor-input-output processor contention},
 abstract = {Most computer systems have separate central (CPU) and input-output (IOP) processors to permit simultaneous computation and input-output (I/O). It is conventional in such systems to avoid any loss of I/O data by granting the IOP priority over the CPU for memory service. Although this priority discipline is simple to implement it may result in a maximum degradation of CPU performance. In this discussion an analysis of the IOP priority discipline is given together with an analysis of other priority disciplines which require the buffering of IOP requests and results are given showing that only a small amount of buffering is required to produce a noticeable improvement in CPU performance.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {27--40},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1013608.805445},
 doi = {http://doi.acm.org/10.1145/1013608.805445},
 acmid = {805445},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU, Contention, I/O interference, Input-output, Memory system, Priority discipline, Processor},
} 

@article{Strecker:1979:ACP:1009373.805445,
 author = {Strecker, William D.},
 title = {An analysis of central processor-input-output processor contention},
 abstract = {Most computer systems have separate central (CPU) and input-output (IOP) processors to permit simultaneous computation and input-output (I/O). It is conventional in such systems to avoid any loss of I/O data by granting the IOP priority over the CPU for memory service. Although this priority discipline is simple to implement it may result in a maximum degradation of CPU performance. In this discussion an analysis of the IOP priority discipline is given together with an analysis of other priority disciplines which require the buffering of IOP requests and results are given showing that only a small amount of buffering is required to produce a noticeable improvement in CPU performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {27--40},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1009373.805445},
 doi = {http://doi.acm.org/10.1145/1009373.805445},
 acmid = {805445},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU, Contention, I/O interference, Input-output, Memory system, Priority discipline, Processor},
} 

@inproceedings{Strecker:1979:ACP:800188.805445,
 author = {Strecker, William D.},
 title = {An analysis of central processor-input-output processor contention},
 abstract = {Most computer systems have separate central (CPU) and input-output (IOP) processors to permit simultaneous computation and input-output (I/O). It is conventional in such systems to avoid any loss of I/O data by granting the IOP priority over the CPU for memory service. Although this priority discipline is simple to implement it may result in a maximum degradation of CPU performance. In this discussion an analysis of the IOP priority discipline is given together with an analysis of other priority disciplines which require the buffering of IOP requests and results are given showing that only a small amount of buffering is required to produce a noticeable improvement in CPU performance.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {27--40},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/800188.805445},
 doi = {http://doi.acm.org/10.1145/800188.805445},
 acmid = {805445},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU, Contention, I/O interference, Input-output, Memory system, Priority discipline, Processor},
} 

@article{Wiecek:1979:PST:1009373.805446,
 author = {Wiecek, Cheryl A. and Steely,Jr., Simon C.},
 title = {Performance simulation as a tool in central processing unit design},
 abstract = {Performance analysis has always been considered important in computer design work. The area of central processing unit (CPU) design is no exception, where the successful development of performance evaluation tools provides valuable information in the analysis of design tradeoffs. Increasing integration of hardware is producing more complicated processor modules which add to the number of alternatives and decisions to be made in the design process. It is important that these modules work together as a balanced unit with no hidden bottlenecks. This paper describes a project to develop performance simulation as an analysis tool in CPU design. The methodology is first detailed as a three part process in which a performance simulation program is realized that executes an instruction trace using command file directions. Discussion follows on the software implemented, applications of this tool in CPU design, and future goals.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {41--47},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1009373.805446},
 doi = {http://doi.acm.org/10.1145/1009373.805446},
 acmid = {805446},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Wiecek:1979:PST:800188.805446,
 author = {Wiecek, Cheryl A. and Steely,Jr., Simon C.},
 title = {Performance simulation as a tool in central processing unit design},
 abstract = {Performance analysis has always been considered important in computer design work. The area of central processing unit (CPU) design is no exception, where the successful development of performance evaluation tools provides valuable information in the analysis of design tradeoffs. Increasing integration of hardware is producing more complicated processor modules which add to the number of alternatives and decisions to be made in the design process. It is important that these modules work together as a balanced unit with no hidden bottlenecks. This paper describes a project to develop performance simulation as an analysis tool in CPU design. The methodology is first detailed as a three part process in which a performance simulation program is realized that executes an instruction trace using command file directions. Discussion follows on the software implemented, applications of this tool in CPU design, and future goals.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {41--47},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800188.805446},
 doi = {http://doi.acm.org/10.1145/800188.805446},
 acmid = {805446},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wiecek:1979:PST:1013608.805446,
 author = {Wiecek, Cheryl A. and Steely,Jr., Simon C.},
 title = {Performance simulation as a tool in central processing unit design},
 abstract = {Performance analysis has always been considered important in computer design work. The area of central processing unit (CPU) design is no exception, where the successful development of performance evaluation tools provides valuable information in the analysis of design tradeoffs. Increasing integration of hardware is producing more complicated processor modules which add to the number of alternatives and decisions to be made in the design process. It is important that these modules work together as a balanced unit with no hidden bottlenecks. This paper describes a project to develop performance simulation as an analysis tool in CPU design. The methodology is first detailed as a three part process in which a performance simulation program is realized that executes an instruction trace using command file directions. Discussion follows on the software implemented, applications of this tool in CPU design, and future goals.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {41--47},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1013608.805446},
 doi = {http://doi.acm.org/10.1145/1013608.805446},
 acmid = {805446},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bennett:1979:SDS:1009373.805447,
 author = {Bennett, David A. and Landauer, Christopher A.},
 title = {Simulation of a distributed system for performance modelling},
 abstract = {A distributed system of cooperating minicomputers is simulated by AIMER (Automatic Integration of Multiple Element Radars) to model and analyze the behavior of a radar tracking system. Simulation is applied in the AIMER project in an attempt to model a network of minicomputers to discover a maximally flexible network architecture. Because building the tracking system out of real hardware would not result in a flexible enough testbed system, the proposed configuration is represented by a software emulation. The instruction sets of the individual processors are emulated in order to allow separation of the measurement facilities from the execution of the system. The emulation is supported by a Nano-data QM-1 micro and nano-programmable host. Extensive performance monitoring hooks have been built into the emulation system which allow small performance perturbations to become visible. The tracking network is controlled by a combination firmware operating system and a special emulated virtual control machine. The tracking algorithms run on virtual machines whose instruction sets and computational throughput can be parameterized when the model is generated, or dynamically by an operator during a run. The radar and ground truth environments for the tracking system are simulated with logic resident in one of the emulated machines, allowing these functions to be monitored as accurately as the tracking algorithms. The use of this simulation technique has resulted in an extremely flexible testbed for the development of distributed radar tracking system models. The testbed itself can be quickly tailored to other application problems.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {49--56},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1009373.805447},
 doi = {http://doi.acm.org/10.1145/1009373.805447},
 acmid = {805447},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bennett:1979:SDS:800188.805447,
 author = {Bennett, David A. and Landauer, Christopher A.},
 title = {Simulation of a distributed system for performance modelling},
 abstract = {A distributed system of cooperating minicomputers is simulated by AIMER (Automatic Integration of Multiple Element Radars) to model and analyze the behavior of a radar tracking system. Simulation is applied in the AIMER project in an attempt to model a network of minicomputers to discover a maximally flexible network architecture. Because building the tracking system out of real hardware would not result in a flexible enough testbed system, the proposed configuration is represented by a software emulation. The instruction sets of the individual processors are emulated in order to allow separation of the measurement facilities from the execution of the system. The emulation is supported by a Nano-data QM-1 micro and nano-programmable host. Extensive performance monitoring hooks have been built into the emulation system which allow small performance perturbations to become visible. The tracking network is controlled by a combination firmware operating system and a special emulated virtual control machine. The tracking algorithms run on virtual machines whose instruction sets and computational throughput can be parameterized when the model is generated, or dynamically by an operator during a run. The radar and ground truth environments for the tracking system are simulated with logic resident in one of the emulated machines, allowing these functions to be monitored as accurately as the tracking algorithms. The use of this simulation technique has resulted in an extremely flexible testbed for the development of distributed radar tracking system models. The testbed itself can be quickly tailored to other application problems.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {49--56},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800188.805447},
 doi = {http://doi.acm.org/10.1145/800188.805447},
 acmid = {805447},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bennett:1979:SDS:1013608.805447,
 author = {Bennett, David A. and Landauer, Christopher A.},
 title = {Simulation of a distributed system for performance modelling},
 abstract = {A distributed system of cooperating minicomputers is simulated by AIMER (Automatic Integration of Multiple Element Radars) to model and analyze the behavior of a radar tracking system. Simulation is applied in the AIMER project in an attempt to model a network of minicomputers to discover a maximally flexible network architecture. Because building the tracking system out of real hardware would not result in a flexible enough testbed system, the proposed configuration is represented by a software emulation. The instruction sets of the individual processors are emulated in order to allow separation of the measurement facilities from the execution of the system. The emulation is supported by a Nano-data QM-1 micro and nano-programmable host. Extensive performance monitoring hooks have been built into the emulation system which allow small performance perturbations to become visible. The tracking network is controlled by a combination firmware operating system and a special emulated virtual control machine. The tracking algorithms run on virtual machines whose instruction sets and computational throughput can be parameterized when the model is generated, or dynamically by an operator during a run. The radar and ground truth environments for the tracking system are simulated with logic resident in one of the emulated machines, allowing these functions to be monitored as accurately as the tracking algorithms. The use of this simulation technique has resulted in an extremely flexible testbed for the development of distributed radar tracking system models. The testbed itself can be quickly tailored to other application problems.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {49--56},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1013608.805447},
 doi = {http://doi.acm.org/10.1145/1013608.805447},
 acmid = {805447},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lazowska:1979:BTA:1009373.805448,
 author = {Lazowska, Edward D.},
 title = {The benchmarking, tuning and analytic modeling of VAX/VMS},
 abstract = {This paper describes a recent experience in benchmarking, tuning and modelling Digital Equipment Corporation's VMS executive running on their VAX-11/780 computer. Although we emphasize modelling here, the three aspects are closely interrelated.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {57--64},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1009373.805448},
 doi = {http://doi.acm.org/10.1145/1009373.805448},
 acmid = {805448},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lazowska:1979:BTA:800188.805448,
 author = {Lazowska, Edward D.},
 title = {The benchmarking, tuning and analytic modeling of VAX/VMS},
 abstract = {This paper describes a recent experience in benchmarking, tuning and modelling Digital Equipment Corporation's VMS executive running on their VAX-11/780 computer. Although we emphasize modelling here, the three aspects are closely interrelated.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {57--64},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800188.805448},
 doi = {http://doi.acm.org/10.1145/800188.805448},
 acmid = {805448},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Lazowska:1979:BTA:1013608.805448,
 author = {Lazowska, Edward D.},
 title = {The benchmarking, tuning and analytic modeling of VAX/VMS},
 abstract = {This paper describes a recent experience in benchmarking, tuning and modelling Digital Equipment Corporation's VMS executive running on their VAX-11/780 computer. Although we emphasize modelling here, the three aspects are closely interrelated.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {57--64},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1013608.805448},
 doi = {http://doi.acm.org/10.1145/1013608.805448},
 acmid = {805448},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marshall:1979:AML:1009373.805449,
 author = {Marshall, William T. and Nute, C. Thomas},
 title = {Analytic modelling of \&ldquo;working set like\&rdquo; replacement algorithms},
 abstract = {Although a large amount of theoretical work has been performed in the analysis of the pure working set replacement algorithm, little has been done applying these results to the approximations that have been implemented. This paper presents a general technique for the analysis of these implementations by analytic methods. Extensive simulations are reported which validate the analytic model and show significant simplifications that can be made with little loss of accuracy. The problem of choosing memory policy parameter values is examined and related in a simple way to the choice of a working set window size.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {65--72},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1009373.805449},
 doi = {http://doi.acm.org/10.1145/1009373.805449},
 acmid = {805449},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Marshall:1979:AML:800188.805449,
 author = {Marshall, William T. and Nute, C. Thomas},
 title = {Analytic modelling of \&ldquo;working set like\&rdquo; replacement algorithms},
 abstract = {Although a large amount of theoretical work has been performed in the analysis of the pure working set replacement algorithm, little has been done applying these results to the approximations that have been implemented. This paper presents a general technique for the analysis of these implementations by analytic methods. Extensive simulations are reported which validate the analytic model and show significant simplifications that can be made with little loss of accuracy. The problem of choosing memory policy parameter values is examined and related in a simple way to the choice of a working set window size.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {65--72},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800188.805449},
 doi = {http://doi.acm.org/10.1145/800188.805449},
 acmid = {805449},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Marshall:1979:AML:1013608.805449,
 author = {Marshall, William T. and Nute, C. Thomas},
 title = {Analytic modelling of \&ldquo;working set like\&rdquo; replacement algorithms},
 abstract = {Although a large amount of theoretical work has been performed in the analysis of the pure working set replacement algorithm, little has been done applying these results to the approximations that have been implemented. This paper presents a general technique for the analysis of these implementations by analytic methods. Extensive simulations are reported which validate the analytic model and show significant simplifications that can be made with little loss of accuracy. The problem of choosing memory policy parameter values is examined and related in a simple way to the choice of a working set window size.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {65--72},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1013608.805449},
 doi = {http://doi.acm.org/10.1145/1013608.805449},
 acmid = {805449},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Briggs:1979:EBM:1013608.805450,
 author = {Briggs, Fay\'{e} A.},
 title = {Effects of buffered memory requests in multiprocessor systems},
 abstract = {A simulation model is developed and used to study the effect of buffering of memory requests on the performance of multiprocessor systems. A multiprocessor system is generalized as a parallel-pipelined processor of order (s,p), which consists of p parallel processors each of which is a pipelined processor with s degrees of multiprogramming, there can be up to s*p memory requests in each instruction cycle. The memory, which consists of N(\&equil;2<supscrpt>n</supscrpt>) identical memory modules, is organized such that there are l(\&equil;2<supscrpt>i</supscrpt>) lines and m(\&equil;2<supscrpt>n\&minus;i</supscrpt>) identical memory modules, where each module is characterized by the address cycle (address hold time) and memory cycle of a and c time units respectively. Too large an l is undesirable in a multiprocessor system because of the cost of the processor-memory interconnection network. Hence, we will show how effective buffering can be used to reduce the system cost while effectively maintaining a high level of performance.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {73--81},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1013608.805450},
 doi = {http://doi.acm.org/10.1145/1013608.805450},
 acmid = {805450},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Briggs:1979:EBM:800188.805450,
 author = {Briggs, Fay\'{e} A.},
 title = {Effects of buffered memory requests in multiprocessor systems},
 abstract = {A simulation model is developed and used to study the effect of buffering of memory requests on the performance of multiprocessor systems. A multiprocessor system is generalized as a parallel-pipelined processor of order (s,p), which consists of p parallel processors each of which is a pipelined processor with s degrees of multiprogramming, there can be up to s*p memory requests in each instruction cycle. The memory, which consists of N(\&equil;2<supscrpt>n</supscrpt>) identical memory modules, is organized such that there are l(\&equil;2<supscrpt>i</supscrpt>) lines and m(\&equil;2<supscrpt>n\&minus;i</supscrpt>) identical memory modules, where each module is characterized by the address cycle (address hold time) and memory cycle of a and c time units respectively. Too large an l is undesirable in a multiprocessor system because of the cost of the processor-memory interconnection network. Hence, we will show how effective buffering can be used to reduce the system cost while effectively maintaining a high level of performance.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {73--81},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800188.805450},
 doi = {http://doi.acm.org/10.1145/800188.805450},
 acmid = {805450},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Briggs:1979:EBM:1009373.805450,
 author = {Briggs, Fay\'{e} A.},
 title = {Effects of buffered memory requests in multiprocessor systems},
 abstract = {A simulation model is developed and used to study the effect of buffering of memory requests on the performance of multiprocessor systems. A multiprocessor system is generalized as a parallel-pipelined processor of order (s,p), which consists of p parallel processors each of which is a pipelined processor with s degrees of multiprogramming, there can be up to s*p memory requests in each instruction cycle. The memory, which consists of N(\&equil;2<supscrpt>n</supscrpt>) identical memory modules, is organized such that there are l(\&equil;2<supscrpt>i</supscrpt>) lines and m(\&equil;2<supscrpt>n\&minus;i</supscrpt>) identical memory modules, where each module is characterized by the address cycle (address hold time) and memory cycle of a and c time units respectively. Too large an l is undesirable in a multiprocessor system because of the cost of the processor-memory interconnection network. Hence, we will show how effective buffering can be used to reduce the system cost while effectively maintaining a high level of performance.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {73--81},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1009373.805450},
 doi = {http://doi.acm.org/10.1145/1009373.805450},
 acmid = {805450},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Raffi:1979:ECB:800188.805451,
 author = {Raffi, Abbas},
 title = {Effects of channel blocking on the performance of shared disk pack in a multi-computer system},
 abstract = {In a multi-computer environment where several computers share packs of disk drives, the architecture of the disk controller can have significant effect on the throughput of the disk pack. In a simple configuration a controller can allow access to only one disk in the pack at a time, and effectively block other channels from accessing other disks in the pack. A desirable alternative is to be able to access different disks of the same pack simultaneously from different channels. Motivated by the presence of a mixed hardware in an installation to support both configurations, an attempt is made to model each system and produce analytical and simulation results to compare their relative performances. It is predicted that under the prevalent conditions in the installation, a complete switchover to either system should not give rise to significant performance change.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {83--87},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/800188.805451},
 doi = {http://doi.acm.org/10.1145/800188.805451},
 acmid = {805451},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raffi:1979:ECB:1013608.805451,
 author = {Raffi, Abbas},
 title = {Effects of channel blocking on the performance of shared disk pack in a multi-computer system},
 abstract = {In a multi-computer environment where several computers share packs of disk drives, the architecture of the disk controller can have significant effect on the throughput of the disk pack. In a simple configuration a controller can allow access to only one disk in the pack at a time, and effectively block other channels from accessing other disks in the pack. A desirable alternative is to be able to access different disks of the same pack simultaneously from different channels. Motivated by the presence of a mixed hardware in an installation to support both configurations, an attempt is made to model each system and produce analytical and simulation results to compare their relative performances. It is predicted that under the prevalent conditions in the installation, a complete switchover to either system should not give rise to significant performance change.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {83--87},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/1013608.805451},
 doi = {http://doi.acm.org/10.1145/1013608.805451},
 acmid = {805451},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Raffi:1979:ECB:1009373.805451,
 author = {Raffi, Abbas},
 title = {Effects of channel blocking on the performance of shared disk pack in a multi-computer system},
 abstract = {In a multi-computer environment where several computers share packs of disk drives, the architecture of the disk controller can have significant effect on the throughput of the disk pack. In a simple configuration a controller can allow access to only one disk in the pack at a time, and effectively block other channels from accessing other disks in the pack. A desirable alternative is to be able to access different disks of the same pack simultaneously from different channels. Motivated by the presence of a mixed hardware in an installation to support both configurations, an attempt is made to model each system and produce analytical and simulation results to compare their relative performances. It is predicted that under the prevalent conditions in the installation, a complete switchover to either system should not give rise to significant performance change.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {83--87},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/1009373.805451},
 doi = {http://doi.acm.org/10.1145/1009373.805451},
 acmid = {805451},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zahorjan:1979:ESM:800188.805452,
 author = {Zahorjan, John},
 title = {An exact solution method for the general class of closed separable queueing networks},
 abstract = {In this paper we present a convolution algorithm for the full class of closed, separable queueing networks. In particular, the algorithm represents an alternative method to those already known for the solution of networks with class changes, and is the first efficient algorithm to deal with Lam-type networks [11]. As an application of the algorithm, we study a simple queueing network with disk I/O devices connected to a single CPU through a single channel. The algorithm is then used to develop a simple, accurate approximation for the blocking of disk devices that takes place when a customer using a disk is waiting for or in service at the channel.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {107--112},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800188.805452},
 doi = {http://doi.acm.org/10.1145/800188.805452},
 acmid = {805452},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahorjan:1979:ESM:1009373.805452,
 author = {Zahorjan, John},
 title = {An exact solution method for the general class of closed separable queueing networks},
 abstract = {In this paper we present a convolution algorithm for the full class of closed, separable queueing networks. In particular, the algorithm represents an alternative method to those already known for the solution of networks with class changes, and is the first efficient algorithm to deal with Lam-type networks [11]. As an application of the algorithm, we study a simple queueing network with disk I/O devices connected to a single CPU through a single channel. The algorithm is then used to develop a simple, accurate approximation for the blocking of disk devices that takes place when a customer using a disk is waiting for or in service at the channel.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {107--112},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1009373.805452},
 doi = {http://doi.acm.org/10.1145/1009373.805452},
 acmid = {805452},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Zahorjan:1979:ESM:1013608.805452,
 author = {Zahorjan, John},
 title = {An exact solution method for the general class of closed separable queueing networks},
 abstract = {In this paper we present a convolution algorithm for the full class of closed, separable queueing networks. In particular, the algorithm represents an alternative method to those already known for the solution of networks with class changes, and is the first efficient algorithm to deal with Lam-type networks [11]. As an application of the algorithm, we study a simple queueing network with disk I/O devices connected to a single CPU through a single channel. The algorithm is then used to develop a simple, accurate approximation for the blocking of disk devices that takes place when a customer using a disk is waiting for or in service at the channel.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {107--112},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1013608.805452},
 doi = {http://doi.acm.org/10.1145/1013608.805452},
 acmid = {805452},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kienzle:1979:SAQ:800188.805453,
 author = {Kienzle, Martin G. and Sevcik, K. C.},
 title = {Survey of analytic queueing network models of computer systems},
 abstract = {A number of case studies involving the use of queueing network models to investigate actual computer systems are surveyed. After suggesting a framework by which case studies can be classified, we contrast various parameter estimation methods for specifying model parameters based on measurement data. A tabular summary indicates the relationships among nineteen case studies.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {113--129},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/800188.805453},
 doi = {http://doi.acm.org/10.1145/800188.805453},
 acmid = {805453},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kienzle:1979:SAQ:1009373.805453,
 author = {Kienzle, Martin G. and Sevcik, K. C.},
 title = {Survey of analytic queueing network models of computer systems},
 abstract = {A number of case studies involving the use of queueing network models to investigate actual computer systems are surveyed. After suggesting a framework by which case studies can be classified, we contrast various parameter estimation methods for specifying model parameters based on measurement data. A tabular summary indicates the relationships among nineteen case studies.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {113--129},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/1009373.805453},
 doi = {http://doi.acm.org/10.1145/1009373.805453},
 acmid = {805453},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kienzle:1979:SAQ:1013608.805453,
 author = {Kienzle, Martin G. and Sevcik, K. C.},
 title = {Survey of analytic queueing network models of computer systems},
 abstract = {A number of case studies involving the use of queueing network models to investigate actual computer systems are surveyed. After suggesting a framework by which case studies can be classified, we contrast various parameter estimation methods for specifying model parameters based on measurement data. A tabular summary indicates the relationships among nineteen case studies.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {113--129},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/1013608.805453},
 doi = {http://doi.acm.org/10.1145/1013608.805453},
 acmid = {805453},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Landry:1979:SEP:1013608.805454,
 author = {Landry, Steve P. and Shriver, Bruce D.},
 title = {A simulation environment for performing dataflow research},
 abstract = {Dataflow languages and processors are currently being extensively studied because of their respective ability to specify and execute programs which exhibit a high degree of parallel and/or asynchronous activity [12, 7]. This paper describes a comprehensive simulation environment that allows for the execution and monitoring of dataflow programs. One overall objective of this facility was to meet the needs of researchers in such diverse areas as computer architecture, algorithm analysis, and language design and implementation. Another objective was to accommodate the semantics of several of the contending abstract dataflow models [2, 4]. Additionally, it was desired to enhance the abstract dataflow models which the simulator would support. These objectives, combined with the desired debugging and metering requirements, directed the design of the overall system. A brief introduction to dataflow and its related terminology is given to assist the reader. A companion paper [6] describes an augmentation to the basic simulation facility presented here that allows for the execution of dataflow programs on processors having finite resources.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {131--139},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1013608.805454},
 doi = {http://doi.acm.org/10.1145/1013608.805454},
 acmid = {805454},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Landry:1979:SEP:1009373.805454,
 author = {Landry, Steve P. and Shriver, Bruce D.},
 title = {A simulation environment for performing dataflow research},
 abstract = {Dataflow languages and processors are currently being extensively studied because of their respective ability to specify and execute programs which exhibit a high degree of parallel and/or asynchronous activity [12, 7]. This paper describes a comprehensive simulation environment that allows for the execution and monitoring of dataflow programs. One overall objective of this facility was to meet the needs of researchers in such diverse areas as computer architecture, algorithm analysis, and language design and implementation. Another objective was to accommodate the semantics of several of the contending abstract dataflow models [2, 4]. Additionally, it was desired to enhance the abstract dataflow models which the simulator would support. These objectives, combined with the desired debugging and metering requirements, directed the design of the overall system. A brief introduction to dataflow and its related terminology is given to assist the reader. A companion paper [6] describes an augmentation to the basic simulation facility presented here that allows for the execution of dataflow programs on processors having finite resources.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {131--139},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1009373.805454},
 doi = {http://doi.acm.org/10.1145/1009373.805454},
 acmid = {805454},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Landry:1979:SEP:800188.805454,
 author = {Landry, Steve P. and Shriver, Bruce D.},
 title = {A simulation environment for performing dataflow research},
 abstract = {Dataflow languages and processors are currently being extensively studied because of their respective ability to specify and execute programs which exhibit a high degree of parallel and/or asynchronous activity [12, 7]. This paper describes a comprehensive simulation environment that allows for the execution and monitoring of dataflow programs. One overall objective of this facility was to meet the needs of researchers in such diverse areas as computer architecture, algorithm analysis, and language design and implementation. Another objective was to accommodate the semantics of several of the contending abstract dataflow models [2, 4]. Additionally, it was desired to enhance the abstract dataflow models which the simulator would support. These objectives, combined with the desired debugging and metering requirements, directed the design of the overall system. A brief introduction to dataflow and its related terminology is given to assist the reader. A companion paper [6] describes an augmentation to the basic simulation facility presented here that allows for the execution of dataflow programs on processors having finite resources.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {131--139},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800188.805454},
 doi = {http://doi.acm.org/10.1145/800188.805454},
 acmid = {805454},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Langan:1979:SED:1009373.805455,
 author = {Langan, David D. and Shriver, Bruce D.},
 title = {Simulated execution of dataflow programs on processors having finite resources},
 abstract = {Dataflow languages and processors are currently being extensively studied because they provide for the specification and realization of processes exhibiting a high degree of parallel and/or asynchronous activity [12, 8]. Several researchers have developed simulators for specific candidate dataflow architectures in which there are essentially an infinite number of resources available to the nost machine [9, 1]. This is done to study the degree of parallelism which is achievable with a given version of an algorithm. However, it is an equally important (and neglected) area to study the behavior of programs executing in candidate computer systems having a finite amount of resources. This paper presents results which have been obtained from such modeling. It is shown that in such a system certain ``critical nodes" must be given priority of execution when competing with other nodes for the same resources in order to achieve the maximum system throughput. It is suggested that the abstract dataflow model be modified to accommodate such situations. Various design trade-offs associated with the implementation of the simulator are discussed along with a description of available features. A companion paper [6] describes the general dataflow simulation facility which provided the basis of this work.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {141--149},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1009373.805455},
 doi = {http://doi.acm.org/10.1145/1009373.805455},
 acmid = {805455},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Langan:1979:SED:1013608.805455,
 author = {Langan, David D. and Shriver, Bruce D.},
 title = {Simulated execution of dataflow programs on processors having finite resources},
 abstract = {Dataflow languages and processors are currently being extensively studied because they provide for the specification and realization of processes exhibiting a high degree of parallel and/or asynchronous activity [12, 8]. Several researchers have developed simulators for specific candidate dataflow architectures in which there are essentially an infinite number of resources available to the nost machine [9, 1]. This is done to study the degree of parallelism which is achievable with a given version of an algorithm. However, it is an equally important (and neglected) area to study the behavior of programs executing in candidate computer systems having a finite amount of resources. This paper presents results which have been obtained from such modeling. It is shown that in such a system certain ``critical nodes" must be given priority of execution when competing with other nodes for the same resources in order to achieve the maximum system throughput. It is suggested that the abstract dataflow model be modified to accommodate such situations. Various design trade-offs associated with the implementation of the simulator are discussed along with a description of available features. A companion paper [6] describes the general dataflow simulation facility which provided the basis of this work.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {141--149},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1013608.805455},
 doi = {http://doi.acm.org/10.1145/1013608.805455},
 acmid = {805455},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Langan:1979:SED:800188.805455,
 author = {Langan, David D. and Shriver, Bruce D.},
 title = {Simulated execution of dataflow programs on processors having finite resources},
 abstract = {Dataflow languages and processors are currently being extensively studied because they provide for the specification and realization of processes exhibiting a high degree of parallel and/or asynchronous activity [12, 8]. Several researchers have developed simulators for specific candidate dataflow architectures in which there are essentially an infinite number of resources available to the nost machine [9, 1]. This is done to study the degree of parallelism which is achievable with a given version of an algorithm. However, it is an equally important (and neglected) area to study the behavior of programs executing in candidate computer systems having a finite amount of resources. This paper presents results which have been obtained from such modeling. It is shown that in such a system certain ``critical nodes" must be given priority of execution when competing with other nodes for the same resources in order to achieve the maximum system throughput. It is suggested that the abstract dataflow model be modified to accommodate such situations. Various design trade-offs associated with the implementation of the simulator are discussed along with a description of available features. A companion paper [6] describes the general dataflow simulation facility which provided the basis of this work.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {141--149},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800188.805455},
 doi = {http://doi.acm.org/10.1145/800188.805455},
 acmid = {805455},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Unger:1979:OSI:800188.805456,
 author = {Unger, Brian W. and Parker, James R.},
 title = {An operating system implementation and simulation language (OASIS)},
 abstract = {An approach to the implementation and simulation of system software for multicomputer architectures is described. OASIS, a variant of the SIMULA 67 language, provides tools for both hardware modelling and system software development. The latter includes an extensible module type with flexible intermodule access control. Hardware is characterized at the processor/memory level so that system software resource control and allocation policies can be implemented at a functional level. Concurrent module execution by multiple processors, with or without shared memory, can be simulated directly. The OASIS modules in such a simulation can closely parallel the structure of actual system software. Thus, once a design is shown viable by simulation, the implementation of actual software can be a simple translation of OASIS modules. A brief overview of OASIS features is presented followed by a simple example.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {151--161},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800188.805456},
 doi = {http://doi.acm.org/10.1145/800188.805456},
 acmid = {805456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Unger:1979:OSI:1013608.805456,
 author = {Unger, Brian W. and Parker, James R.},
 title = {An operating system implementation and simulation language (OASIS)},
 abstract = {An approach to the implementation and simulation of system software for multicomputer architectures is described. OASIS, a variant of the SIMULA 67 language, provides tools for both hardware modelling and system software development. The latter includes an extensible module type with flexible intermodule access control. Hardware is characterized at the processor/memory level so that system software resource control and allocation policies can be implemented at a functional level. Concurrent module execution by multiple processors, with or without shared memory, can be simulated directly. The OASIS modules in such a simulation can closely parallel the structure of actual system software. Thus, once a design is shown viable by simulation, the implementation of actual software can be a simple translation of OASIS modules. A brief overview of OASIS features is presented followed by a simple example.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {151--161},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1013608.805456},
 doi = {http://doi.acm.org/10.1145/1013608.805456},
 acmid = {805456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Unger:1979:OSI:1009373.805456,
 author = {Unger, Brian W. and Parker, James R.},
 title = {An operating system implementation and simulation language (OASIS)},
 abstract = {An approach to the implementation and simulation of system software for multicomputer architectures is described. OASIS, a variant of the SIMULA 67 language, provides tools for both hardware modelling and system software development. The latter includes an extensible module type with flexible intermodule access control. Hardware is characterized at the processor/memory level so that system software resource control and allocation policies can be implemented at a functional level. Concurrent module execution by multiple processors, with or without shared memory, can be simulated directly. The OASIS modules in such a simulation can closely parallel the structure of actual system software. Thus, once a design is shown viable by simulation, the implementation of actual software can be a simple translation of OASIS modules. A brief overview of OASIS features is presented followed by a simple example.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {151--161},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1009373.805456},
 doi = {http://doi.acm.org/10.1145/1009373.805456},
 acmid = {805456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sanguinetti:1979:TIS:1009373.805457,
 author = {Sanguinetti, John},
 title = {A technique for integrating simulation and system design},
 abstract = {A technique for simulating incomplete systems is given which allows performance prediction during system design. This technique, called integrated simulation, allows the system design to itself be a simulation model, thus avoiding the overhead of maintaining a separate, valid simulation model for the system. The paper presents integrated simulation in the framework of a system modeling language called the Program Process Modeling Language, PPML. This language provides a means for describing systems of concurrent processes in both abstract and explicit terms, thus lending itself well to a top-down design method. In the design process, any PPML representation of the system can be simulated directly, from the most abstract design to the completely elaborated system. Simulation of the completely elaborated system is, in fact, simply the system in execution. The paper defines PPML and describes the techniques required to simulate PPML systems given various underlying machines. It concludes with a discussion of the limitations of the integrated simulation method.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1009373.805457},
 doi = {http://doi.acm.org/10.1145/1009373.805457},
 acmid = {805457},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sanguinetti:1979:TIS:800188.805457,
 author = {Sanguinetti, John},
 title = {A technique for integrating simulation and system design},
 abstract = {A technique for simulating incomplete systems is given which allows performance prediction during system design. This technique, called integrated simulation, allows the system design to itself be a simulation model, thus avoiding the overhead of maintaining a separate, valid simulation model for the system. The paper presents integrated simulation in the framework of a system modeling language called the Program Process Modeling Language, PPML. This language provides a means for describing systems of concurrent processes in both abstract and explicit terms, thus lending itself well to a top-down design method. In the design process, any PPML representation of the system can be simulated directly, from the most abstract design to the completely elaborated system. Simulation of the completely elaborated system is, in fact, simply the system in execution. The paper defines PPML and describes the techniques required to simulate PPML systems given various underlying machines. It concludes with a discussion of the limitations of the integrated simulation method.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800188.805457},
 doi = {http://doi.acm.org/10.1145/800188.805457},
 acmid = {805457},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sanguinetti:1979:TIS:1013608.805457,
 author = {Sanguinetti, John},
 title = {A technique for integrating simulation and system design},
 abstract = {A technique for simulating incomplete systems is given which allows performance prediction during system design. This technique, called integrated simulation, allows the system design to itself be a simulation model, thus avoiding the overhead of maintaining a separate, valid simulation model for the system. The paper presents integrated simulation in the framework of a system modeling language called the Program Process Modeling Language, PPML. This language provides a means for describing systems of concurrent processes in both abstract and explicit terms, thus lending itself well to a top-down design method. In the design process, any PPML representation of the system can be simulated directly, from the most abstract design to the completely elaborated system. Simulation of the completely elaborated system is, in fact, simply the system in execution. The paper defines PPML and describes the techniques required to simulate PPML systems given various underlying machines. It concludes with a discussion of the limitations of the integrated simulation method.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013608.805457},
 doi = {http://doi.acm.org/10.1145/1013608.805457},
 acmid = {805457},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Razouk:1979:EMS:1013608.805458,
 author = {Razouk, Rami R. and Vernon, Mary and Estrin, Gerald},
 title = {Evaluation methods in SARA\&mdash;the graph model simulator},
 abstract = {The supported methodology evolving in the SARA (System ARchitects' Apprentice) system creates a design frame-work on which increasingly powerful analytical tools are to be grafted. Control flow analyses and program verification tools have shown promise. However, in the realm of the complex systems which interest us there is a great deal of research and development to be done before we can count on the use of such powerful tools. We must always be prepared to resort to experiments for evaluation of proposed designs. This paper describes a fundamental SARA tool, the graph model simulator. During top-down refinement of a design, the simulator is used to test consistency between the levels of abstraction. During composition, known building blocks are linked together and the composite graph model is tested relative to the lowest top-down model. Design of test environments is integrated with the multilevel design process. The SARA methodology is exemplified through design of a higher level building block to do a simple FFT.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {189--206},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/1013608.805458},
 doi = {http://doi.acm.org/10.1145/1013608.805458},
 acmid = {805458},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Razouk:1979:EMS:800188.805458,
 author = {Razouk, Rami R. and Vernon, Mary and Estrin, Gerald},
 title = {Evaluation methods in SARA\&mdash;the graph model simulator},
 abstract = {The supported methodology evolving in the SARA (System ARchitects' Apprentice) system creates a design frame-work on which increasingly powerful analytical tools are to be grafted. Control flow analyses and program verification tools have shown promise. However, in the realm of the complex systems which interest us there is a great deal of research and development to be done before we can count on the use of such powerful tools. We must always be prepared to resort to experiments for evaluation of proposed designs. This paper describes a fundamental SARA tool, the graph model simulator. During top-down refinement of a design, the simulator is used to test consistency between the levels of abstraction. During composition, known building blocks are linked together and the composite graph model is tested relative to the lowest top-down model. Design of test environments is integrated with the multilevel design process. The SARA methodology is exemplified through design of a higher level building block to do a simple FFT.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {189--206},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/800188.805458},
 doi = {http://doi.acm.org/10.1145/800188.805458},
 acmid = {805458},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Razouk:1979:EMS:1009373.805458,
 author = {Razouk, Rami R. and Vernon, Mary and Estrin, Gerald},
 title = {Evaluation methods in SARA\&mdash;the graph model simulator},
 abstract = {The supported methodology evolving in the SARA (System ARchitects' Apprentice) system creates a design frame-work on which increasingly powerful analytical tools are to be grafted. Control flow analyses and program verification tools have shown promise. However, in the realm of the complex systems which interest us there is a great deal of research and development to be done before we can count on the use of such powerful tools. We must always be prepared to resort to experiments for evaluation of proposed designs. This paper describes a fundamental SARA tool, the graph model simulator. During top-down refinement of a design, the simulator is used to test consistency between the levels of abstraction. During composition, known building blocks are linked together and the composite graph model is tested relative to the lowest top-down model. Design of test environments is integrated with the multilevel design process. The SARA methodology is exemplified through design of a higher level building block to do a simple FFT.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {189--206},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/1009373.805458},
 doi = {http://doi.acm.org/10.1145/1009373.805458},
 acmid = {805458},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yu:1979:MSD:1013608.805459,
 author = {Yu, Stone H. and Murata, Tadao},
 title = {Modeling and simulating data flow computations at machine language level},
 abstract = {This paper is concerned with the data flow organization of computers and programs, which exhibits a good deal of inherent concurrencies in a computation by imposing no superfluous precedence constraints. In view of the popularity of parallel and distributed processing, this organization can be expected to play an increasingly prominent role in the design and development of computer systems. A schematic diagram called DF-graphs, suitable for modeling data flow computations at the machine language level, is introduced. To facilitate the storage of DF-graphs in computers, matrix equations which fully describe their structure and their dynamic behaviors are developed as an alternate representation. Also demonstrated is the feasibility of simulating the execution of computations specified by DF-graphs on a network of conventional mini- and microprocessors.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {207--213},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1013608.805459},
 doi = {http://doi.acm.org/10.1145/1013608.805459},
 acmid = {805459},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Yu:1979:MSD:800188.805459,
 author = {Yu, Stone H. and Murata, Tadao},
 title = {Modeling and simulating data flow computations at machine language level},
 abstract = {This paper is concerned with the data flow organization of computers and programs, which exhibits a good deal of inherent concurrencies in a computation by imposing no superfluous precedence constraints. In view of the popularity of parallel and distributed processing, this organization can be expected to play an increasingly prominent role in the design and development of computer systems. A schematic diagram called DF-graphs, suitable for modeling data flow computations at the machine language level, is introduced. To facilitate the storage of DF-graphs in computers, matrix equations which fully describe their structure and their dynamic behaviors are developed as an alternate representation. Also demonstrated is the feasibility of simulating the execution of computations specified by DF-graphs on a network of conventional mini- and microprocessors.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {207--213},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800188.805459},
 doi = {http://doi.acm.org/10.1145/800188.805459},
 acmid = {805459},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Yu:1979:MSD:1009373.805459,
 author = {Yu, Stone H. and Murata, Tadao},
 title = {Modeling and simulating data flow computations at machine language level},
 abstract = {This paper is concerned with the data flow organization of computers and programs, which exhibits a good deal of inherent concurrencies in a computation by imposing no superfluous precedence constraints. In view of the popularity of parallel and distributed processing, this organization can be expected to play an increasingly prominent role in the design and development of computer systems. A schematic diagram called DF-graphs, suitable for modeling data flow computations at the machine language level, is introduced. To facilitate the storage of DF-graphs in computers, matrix equations which fully describe their structure and their dynamic behaviors are developed as an alternate representation. Also demonstrated is the feasibility of simulating the execution of computations specified by DF-graphs on a network of conventional mini- and microprocessors.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {207--213},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1009373.805459},
 doi = {http://doi.acm.org/10.1145/1009373.805459},
 acmid = {805459},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mattheyses:1979:MSA:1009373.805460,
 author = {Mattheyses, R. M. and Conry, S. E.},
 title = {Models for specification and anaysis of parallel computing systems},
 abstract = {The problem of designing a properly functioning parallel hardware or software system is considerably more difficult than that of designing a similar sequential system. In this paper we formulate criteria which a design methodology for parallel systems should satisfy and explore the use of various models as the basis for such a design tool.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {215--224},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1009373.805460},
 doi = {http://doi.acm.org/10.1145/1009373.805460},
 acmid = {805460},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Mattheyses:1979:MSA:1013608.805460,
 author = {Mattheyses, R. M. and Conry, S. E.},
 title = {Models for specification and anaysis of parallel computing systems},
 abstract = {The problem of designing a properly functioning parallel hardware or software system is considerably more difficult than that of designing a similar sequential system. In this paper we formulate criteria which a design methodology for parallel systems should satisfy and explore the use of various models as the basis for such a design tool.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {215--224},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1013608.805460},
 doi = {http://doi.acm.org/10.1145/1013608.805460},
 acmid = {805460},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mattheyses:1979:MSA:800188.805460,
 author = {Mattheyses, R. M. and Conry, S. E.},
 title = {Models for specification and anaysis of parallel computing systems},
 abstract = {The problem of designing a properly functioning parallel hardware or software system is considerably more difficult than that of designing a similar sequential system. In this paper we formulate criteria which a design methodology for parallel systems should satisfy and explore the use of various models as the basis for such a design tool.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {215--224},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800188.805460},
 doi = {http://doi.acm.org/10.1145/800188.805460},
 acmid = {805460},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gertner:1979:PEC:1013608.805461,
 author = {Gertner, Ilya},
 title = {Performance evaluation of communicating processes},
 abstract = {This paper concerns the performance evaluation of an operating system based on communicating processes. Processes communicate via messages and there is no shared data. Execution of a program is abstracted as a sequence of events to denote significant computational steps. A finite state machine model of computation is used for the specifications of abstract computational properties and, thereafter, for the selective analysis of measurement data. A set of conventions is developed to characterize the performance of communicating processes. A hierarchical layering technique is used to concisely describe the characteristics of large systems. A performance monitoring system was implemented and applied to the analysis of RIG, a message-based operating system.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {241--248},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1013608.805461},
 doi = {http://doi.acm.org/10.1145/1013608.805461},
 acmid = {805461},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gertner:1979:PEC:800188.805461,
 author = {Gertner, Ilya},
 title = {Performance evaluation of communicating processes},
 abstract = {This paper concerns the performance evaluation of an operating system based on communicating processes. Processes communicate via messages and there is no shared data. Execution of a program is abstracted as a sequence of events to denote significant computational steps. A finite state machine model of computation is used for the specifications of abstract computational properties and, thereafter, for the selective analysis of measurement data. A set of conventions is developed to characterize the performance of communicating processes. A hierarchical layering technique is used to concisely describe the characteristics of large systems. A performance monitoring system was implemented and applied to the analysis of RIG, a message-based operating system.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {241--248},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800188.805461},
 doi = {http://doi.acm.org/10.1145/800188.805461},
 acmid = {805461},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Gertner:1979:PEC:1009373.805461,
 author = {Gertner, Ilya},
 title = {Performance evaluation of communicating processes},
 abstract = {This paper concerns the performance evaluation of an operating system based on communicating processes. Processes communicate via messages and there is no shared data. Execution of a program is abstracted as a sequence of events to denote significant computational steps. A finite state machine model of computation is used for the specifications of abstract computational properties and, thereafter, for the selective analysis of measurement data. A set of conventions is developed to characterize the performance of communicating processes. A hierarchical layering technique is used to concisely describe the characteristics of large systems. A performance monitoring system was implemented and applied to the analysis of RIG, a message-based operating system.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {241--248},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1009373.805461},
 doi = {http://doi.acm.org/10.1145/1009373.805461},
 acmid = {805461},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Spooner:1979:BIS:1009373.805462,
 author = {Spooner, Christopher R.},
 title = {Benchmarking interactive systems: Producing the software},
 abstract = {The author has recently developed a new methodology of benchmarking, which is being applied to a procurement in which (a) a single integrated interactive application is to span a distributed configuration of computing hardware, (b) the configuration is unknown when the benchmark is being developed, and (c) the application software will be written after the benchmark has been run. The buyer prepares a simulation model of the intended application in the form of programs that will run on the hardware being benchmarked. Each competing vendor is expected to tune the performance of this model to the hardware configuration that he has proposed, so he will require several versions of the model. This presents the buyer with a formidable software-production problem, which is further complicated by a requirement for extreme flexibility and reliability. The paper addresses the software-production problem and describes its solution. The solution was to develop an automated code-production system based on two principal design features. First, the model and its translator are both written in the same language; secondly, the common language is selected on the basis of readability and extensibility. The paper examines why this approach to the code-production problem was successful. Though the code-production system was developed to support a particular benchmarking approach, it should also be useful in other modeling situations. Indeed it might be of interest in any field where readability, reliability, ease of maintenance, and economy of programming effort are considered important.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1009373.805462},
 doi = {http://doi.acm.org/10.1145/1009373.805462},
 acmid = {805462},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Spooner:1979:BIS:800188.805462,
 author = {Spooner, Christopher R.},
 title = {Benchmarking interactive systems: Producing the software},
 abstract = {The author has recently developed a new methodology of benchmarking, which is being applied to a procurement in which (a) a single integrated interactive application is to span a distributed configuration of computing hardware, (b) the configuration is unknown when the benchmark is being developed, and (c) the application software will be written after the benchmark has been run. The buyer prepares a simulation model of the intended application in the form of programs that will run on the hardware being benchmarked. Each competing vendor is expected to tune the performance of this model to the hardware configuration that he has proposed, so he will require several versions of the model. This presents the buyer with a formidable software-production problem, which is further complicated by a requirement for extreme flexibility and reliability. The paper addresses the software-production problem and describes its solution. The solution was to develop an automated code-production system based on two principal design features. First, the model and its translator are both written in the same language; secondly, the common language is selected on the basis of readability and extensibility. The paper examines why this approach to the code-production problem was successful. Though the code-production system was developed to support a particular benchmarking approach, it should also be useful in other modeling situations. Indeed it might be of interest in any field where readability, reliability, ease of maintenance, and economy of programming effort are considered important.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800188.805462},
 doi = {http://doi.acm.org/10.1145/800188.805462},
 acmid = {805462},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Spooner:1979:BIS:1013608.805462,
 author = {Spooner, Christopher R.},
 title = {Benchmarking interactive systems: Producing the software},
 abstract = {The author has recently developed a new methodology of benchmarking, which is being applied to a procurement in which (a) a single integrated interactive application is to span a distributed configuration of computing hardware, (b) the configuration is unknown when the benchmark is being developed, and (c) the application software will be written after the benchmark has been run. The buyer prepares a simulation model of the intended application in the form of programs that will run on the hardware being benchmarked. Each competing vendor is expected to tune the performance of this model to the hardware configuration that he has proposed, so he will require several versions of the model. This presents the buyer with a formidable software-production problem, which is further complicated by a requirement for extreme flexibility and reliability. The paper addresses the software-production problem and describes its solution. The solution was to develop an automated code-production system based on two principal design features. First, the model and its translator are both written in the same language; secondly, the common language is selected on the basis of readability and extensibility. The paper examines why this approach to the code-production problem was successful. Though the code-production system was developed to support a particular benchmarking approach, it should also be useful in other modeling situations. Indeed it might be of interest in any field where readability, reliability, ease of maintenance, and economy of programming effort are considered important.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {249--257},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1013608.805462},
 doi = {http://doi.acm.org/10.1145/1013608.805462},
 acmid = {805462},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dujomovic:1979:CCP:1009373.805463,
 author = {Dujomovi\'{c}, Jozo J.},
 title = {Criteria for computer performance analysis},
 abstract = {Computer evaluation, comparison, and selection is essentially a decision process. The decision making is based on a number of worth indicators, including various computer performance indicators. The performance indicators are obtained through the computer performance measurement procedure. Consequently, this procedure should be completely conditioned by the decision process. This paper investigates various aspects of computer performance measurement and evaluation procedure within the context of computer evaluation, comparison and selection process based on the Logic Scoring of Preference method. The set of elementary criteria for performance evaluation is proposed and the corresponding set of performance indicators is defined. The necessary performance measurements are based on the standardized set of synthetic benchmark programs and include three separate measurements: monoprogramming performance measurement, multiprogramming performance measurement, and multiprogramming efficiency measurement. Using the proposed elementary criteria, the measured performance indicators can be transformed into elementary preferences and aggregated with other non-performance elementary preferences obtained through the evaluation process. The applicability of presented elementary criteria is illustrated by numerical examples.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {259--267},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1009373.805463},
 doi = {http://doi.acm.org/10.1145/1009373.805463},
 acmid = {805463},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dujomovic:1979:CCP:800188.805463,
 author = {Dujomovi\'{c}, Jozo J.},
 title = {Criteria for computer performance analysis},
 abstract = {Computer evaluation, comparison, and selection is essentially a decision process. The decision making is based on a number of worth indicators, including various computer performance indicators. The performance indicators are obtained through the computer performance measurement procedure. Consequently, this procedure should be completely conditioned by the decision process. This paper investigates various aspects of computer performance measurement and evaluation procedure within the context of computer evaluation, comparison and selection process based on the Logic Scoring of Preference method. The set of elementary criteria for performance evaluation is proposed and the corresponding set of performance indicators is defined. The necessary performance measurements are based on the standardized set of synthetic benchmark programs and include three separate measurements: monoprogramming performance measurement, multiprogramming performance measurement, and multiprogramming efficiency measurement. Using the proposed elementary criteria, the measured performance indicators can be transformed into elementary preferences and aggregated with other non-performance elementary preferences obtained through the evaluation process. The applicability of presented elementary criteria is illustrated by numerical examples.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {259--267},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800188.805463},
 doi = {http://doi.acm.org/10.1145/800188.805463},
 acmid = {805463},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dujomovic:1979:CCP:1013608.805463,
 author = {Dujomovi\'{c}, Jozo J.},
 title = {Criteria for computer performance analysis},
 abstract = {Computer evaluation, comparison, and selection is essentially a decision process. The decision making is based on a number of worth indicators, including various computer performance indicators. The performance indicators are obtained through the computer performance measurement procedure. Consequently, this procedure should be completely conditioned by the decision process. This paper investigates various aspects of computer performance measurement and evaluation procedure within the context of computer evaluation, comparison and selection process based on the Logic Scoring of Preference method. The set of elementary criteria for performance evaluation is proposed and the corresponding set of performance indicators is defined. The necessary performance measurements are based on the standardized set of synthetic benchmark programs and include three separate measurements: monoprogramming performance measurement, multiprogramming performance measurement, and multiprogramming efficiency measurement. Using the proposed elementary criteria, the measured performance indicators can be transformed into elementary preferences and aggregated with other non-performance elementary preferences obtained through the evaluation process. The applicability of presented elementary criteria is illustrated by numerical examples.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {259--267},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1013608.805463},
 doi = {http://doi.acm.org/10.1145/1013608.805463},
 acmid = {805463},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Dyal:1979:SBS:800188.805464,
 author = {Dyal, James O. and DeWald,Jr., William},
 title = {Small business system performance analysis},
 abstract = {This paper presents results from the performance simulation study of a small business-oriented computer system. The system, SPERRY UNIVAC BC/7-700, is commercially available in the configuration modeled and in other higher performance models. All BC/7 systems modeled are supported with highly interactive applications software systems. The model is parameterized to select one or more workstations and one or more cartridge disks. File allocations are by cylinder. Seek times are computed by remembering the position of each movable arm. References are randomized within each file, but the sequence in which files are accessed is controlled by the application logic, in conjunction with the number of line items/order. Most event times are not constant, but the result of drawing randomly against empirical distributions with specified mean and standard deviation. For this study, the system simulated is composed of a single work-station running the highly interactive on-line version of a sophisticated order entry application package. Principal performance measures are system throughput and response time, including operator action times. It is found that, in the single workstation environment, performance is very cost effective in this highly competitive part of the information system market.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {269--275},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800188.805464},
 doi = {http://doi.acm.org/10.1145/800188.805464},
 acmid = {805464},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dyal:1979:SBS:1009373.805464,
 author = {Dyal, James O. and DeWald,Jr., William},
 title = {Small business system performance analysis},
 abstract = {This paper presents results from the performance simulation study of a small business-oriented computer system. The system, SPERRY UNIVAC BC/7-700, is commercially available in the configuration modeled and in other higher performance models. All BC/7 systems modeled are supported with highly interactive applications software systems. The model is parameterized to select one or more workstations and one or more cartridge disks. File allocations are by cylinder. Seek times are computed by remembering the position of each movable arm. References are randomized within each file, but the sequence in which files are accessed is controlled by the application logic, in conjunction with the number of line items/order. Most event times are not constant, but the result of drawing randomly against empirical distributions with specified mean and standard deviation. For this study, the system simulated is composed of a single work-station running the highly interactive on-line version of a sophisticated order entry application package. Principal performance measures are system throughput and response time, including operator action times. It is found that, in the single workstation environment, performance is very cost effective in this highly competitive part of the information system market.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {269--275},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1009373.805464},
 doi = {http://doi.acm.org/10.1145/1009373.805464},
 acmid = {805464},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Dyal:1979:SBS:1013608.805464,
 author = {Dyal, James O. and DeWald,Jr., William},
 title = {Small business system performance analysis},
 abstract = {This paper presents results from the performance simulation study of a small business-oriented computer system. The system, SPERRY UNIVAC BC/7-700, is commercially available in the configuration modeled and in other higher performance models. All BC/7 systems modeled are supported with highly interactive applications software systems. The model is parameterized to select one or more workstations and one or more cartridge disks. File allocations are by cylinder. Seek times are computed by remembering the position of each movable arm. References are randomized within each file, but the sequence in which files are accessed is controlled by the application logic, in conjunction with the number of line items/order. Most event times are not constant, but the result of drawing randomly against empirical distributions with specified mean and standard deviation. For this study, the system simulated is composed of a single work-station running the highly interactive on-line version of a sophisticated order entry application package. Principal performance measures are system throughput and response time, including operator action times. It is found that, in the single workstation environment, performance is very cost effective in this highly competitive part of the information system market.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {269--275},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1013608.805464},
 doi = {http://doi.acm.org/10.1145/1013608.805464},
 acmid = {805464},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Huff:1979:SCR:800188.805465,
 author = {Huff, Robert W.},
 title = {System characterization of a Retail Business System},
 abstract = {The complexities of Retail Business Systems today require a thorough understanding of how functional requirements impact desired system performance. It is no longer feasible to discretely test and evaluate individual system components without considering their inter-relationship. The techniques described in this presentation will define the method of system characterization of products prior to customer delivery. Three techniques are utilized to characterize system performance - simulation, stimulation, and performance measurement. Simulation involves writing a mathematical model which is enhanced from a product feasibility model to a system configuration tool as a result of stimulation and measurement activities. Stimulation consists of using emulators to load the system component under test as if the actual system is inter-connected. The emulators are programmed to produce a processing volume which can exceed the peak benchmark of the potential user. Performance measurement is accomplished during the stimulation activity using hardware/ software probes to monitor specific system parameters. These monitors provide vital information to determine total system capacity and the expected system performance for a given configuration. The information derived from system characterization is invaluable in providing the customer with a realistic expectation of system capability to perform its present functions and in projecting future growth potential.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {277--284},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800188.805465},
 doi = {http://doi.acm.org/10.1145/800188.805465},
 acmid = {805465},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Huff:1979:SCR:1013608.805465,
 author = {Huff, Robert W.},
 title = {System characterization of a Retail Business System},
 abstract = {The complexities of Retail Business Systems today require a thorough understanding of how functional requirements impact desired system performance. It is no longer feasible to discretely test and evaluate individual system components without considering their inter-relationship. The techniques described in this presentation will define the method of system characterization of products prior to customer delivery. Three techniques are utilized to characterize system performance - simulation, stimulation, and performance measurement. Simulation involves writing a mathematical model which is enhanced from a product feasibility model to a system configuration tool as a result of stimulation and measurement activities. Stimulation consists of using emulators to load the system component under test as if the actual system is inter-connected. The emulators are programmed to produce a processing volume which can exceed the peak benchmark of the potential user. Performance measurement is accomplished during the stimulation activity using hardware/ software probes to monitor specific system parameters. These monitors provide vital information to determine total system capacity and the expected system performance for a given configuration. The information derived from system characterization is invaluable in providing the customer with a realistic expectation of system capability to perform its present functions and in projecting future growth potential.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {277--284},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1013608.805465},
 doi = {http://doi.acm.org/10.1145/1013608.805465},
 acmid = {805465},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Huff:1979:SCR:1009373.805465,
 author = {Huff, Robert W.},
 title = {System characterization of a Retail Business System},
 abstract = {The complexities of Retail Business Systems today require a thorough understanding of how functional requirements impact desired system performance. It is no longer feasible to discretely test and evaluate individual system components without considering their inter-relationship. The techniques described in this presentation will define the method of system characterization of products prior to customer delivery. Three techniques are utilized to characterize system performance - simulation, stimulation, and performance measurement. Simulation involves writing a mathematical model which is enhanced from a product feasibility model to a system configuration tool as a result of stimulation and measurement activities. Stimulation consists of using emulators to load the system component under test as if the actual system is inter-connected. The emulators are programmed to produce a processing volume which can exceed the peak benchmark of the potential user. Performance measurement is accomplished during the stimulation activity using hardware/ software probes to monitor specific system parameters. These monitors provide vital information to determine total system capacity and the expected system performance for a given configuration. The information derived from system characterization is invaluable in providing the customer with a realistic expectation of system capability to perform its present functions and in projecting future growth potential.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {277--284},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1009373.805465},
 doi = {http://doi.acm.org/10.1145/1009373.805465},
 acmid = {805465},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Stroebel:1979:FPA:800188.805466,
 author = {Stroebel, Gary},
 title = {Field performance aids for IBM GSD systems},
 abstract = {A series of field performance aids have been developed to assist IBM Systems Engineers evaluate the performance of System/3, System/34, and System/38 configurations. Use of those aids is appropriate at proposal time, for preinstallation design, for tuning, and for upgrade studies. This paper overviews some of the key features of these aids as they pertain to the user interface, workload characterization, and performance models.},
 booktitle = {Proceedings of the 1979 ACM SIGMETRICS conference on Simulation, measurement and modeling of computer systems},
 series = {SIGMETRICS '79},
 year = {1979},
 location = {Boulder, Colorado, United States},
 pages = {285--291},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800188.805466},
 doi = {http://doi.acm.org/10.1145/800188.805466},
 acmid = {805466},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stroebel:1979:FPA:1009373.805466,
 author = {Stroebel, Gary},
 title = {Field performance aids for IBM GSD systems},
 abstract = {A series of field performance aids have been developed to assist IBM Systems Engineers evaluate the performance of System/3, System/34, and System/38 configurations. Use of those aids is appropriate at proposal time, for preinstallation design, for tuning, and for upgrade studies. This paper overviews some of the key features of these aids as they pertain to the user interface, workload characterization, and performance models.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {8},
 issue = {3},
 month = {August},
 year = {1979},
 issn = {0163-5999},
 pages = {285--291},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1009373.805466},
 doi = {http://doi.acm.org/10.1145/1009373.805466},
 acmid = {805466},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Stroebel:1979:FPA:1013608.805466,
 author = {Stroebel, Gary},
 title = {Field performance aids for IBM GSD systems},
 abstract = {A series of field performance aids have been developed to assist IBM Systems Engineers evaluate the performance of System/3, System/34, and System/38 configurations. Use of those aids is appropriate at proposal time, for preinstallation design, for tuning, and for upgrade studies. This paper overviews some of the key features of these aids as they pertain to the user interface, workload characterization, and performance models.},
 journal = {SIGSIM Simul. Dig.},
 volume = {11},
 issue = {1},
 month = {August},
 year = {1979},
 issn = {0163-6103},
 pages = {285--291},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1013608.805466},
 doi = {http://doi.acm.org/10.1145/1013608.805466},
 acmid = {805466},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reiter:1976:EDO:800200.806176,
 author = {Reiter, Allen},
 title = {Some experiments in directory organization - a simulation study},
 abstract = {Using a simulation model, experiments were conducted on various directory organization schemes and their performance implications. In particular we tested the effects of a multiprogrammed environment on system throughput for retrieval operations. Analysis of the results shows that different factors are relevant to performance for the various systems, and that under some circumstances ISAM and hash-coding may lose the advantages they possess over B-trees in a stand-alone environment when multiprogramming is used. Some results of sharing data among simultaneous users are also presented.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {1--9},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800200.806176},
 doi = {http://doi.acm.org/10.1145/800200.806176},
 acmid = {806176},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {B-tree, Directories, Hash-coding, Index sequential, Performance analysis, Throughput prediction},
} 

@inproceedings{Chandy:1976:FAD:800200.806177,
 author = {Chandy, K. M. and Hewes, J. E.},
 title = {File allocation in distributed systems},
 abstract = {The problem of allocating files in a computer network is a complex combinatorial problem due to the number of integer design parameters involved. These parameters include system cost, number of copies of each file to be stored, and sites at which the copies should be stored. The tradeoffs between these parameters are discussed. The design problem is formulated as an integer programming problem. A branch and bound algorithm is proposed to solve the problem. A linear programming formulation which ignores integer restrictions (and allows a fraction of a file to reside at a site) is shown to yield integer solutions in most cases. In other words integer restrictions are satisfied automatically. A near-optimal heuristic is presented, along with computational results. An efficient method to solve the file allocation problem for medium-scale networks is proposed.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {10--13},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800200.806177},
 doi = {http://doi.acm.org/10.1145/800200.806177},
 acmid = {806177},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Analytic solution techniques, Branch and bound algorithms, Computer networks and distributed subsystems, Data base management system and subsystem, Decomposition of linear programs, Integer programming, Linear programming, Memory hierarchies},
} 

@inproceedings{Welch:1976:SCM:800200.806178,
 author = {Welch, Peter D.},
 title = {On the self contained modelling of DB/DC systems},
 abstract = {This paper discusses the modelling of DB/DC systems operating in a demand paging environment. As an extension of an earlier definition of Denning a class of systems is defined, those operating with ``working set integrity", for which one can obtain system response parameters solely from the resource requirements of the input processes when running in isolation. No external information such as a miss ratio curve from another systems environment is required. The interdependence between the resource requirements of processes when running in a systems environment and the system performance parameters is faced directly and expressed as a set of simultaneous equations. The emphasis is on the effect of the sharing of a common set of pages among the active set of processes. Both unrestricted shared usage and usage with locking are treated.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {14--24},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800200.806178},
 doi = {http://doi.acm.org/10.1145/800200.806178},
 acmid = {806178},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Liu:1976:PMI:800200.806179,
 author = {Liu, Jane W. S. and Milner, J. M.},
 title = {Probabilistic models of inverted file information retrieval systems},
 abstract = {In this paper, we study several probabilistic models of inverted file information retrieval systems. In particular, the portion of the system which performs the tasks of list accessing and merging is modeled. The average response times of the system for three different methods of managing the disk access and merge processor queues are discussed.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {25--37},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/800200.806179},
 doi = {http://doi.acm.org/10.1145/800200.806179},
 acmid = {806179},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gaver:1976:MMP:800200.806180,
 author = {Gaver, Donald P. and Humfeld, George},
 title = {Multitype multiprogramming: Probability models and numerical procedures},
 abstract = {The purpose of the present paper is to study systems in which different job types are present at the various servers and are processed according to a variety of scheduling disciplines, in particular in ``first-come, first-served" order. Our approach is numerical, and is applied to an example which is also analyzed by means of the Gordon and Newell (GN) model. Using a plausible method of fitting the latter provides a reasonably, satisfactory approximation to the ``true" multitype system parameters.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {38--43},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800200.806180},
 doi = {http://doi.acm.org/10.1145/800200.806180},
 acmid = {806180},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Giammo:1976:VCP:800200.806181,
 author = {Giammo, Thomas},
 title = {Validation of a computer performance model of the exponential queuing network family},
 abstract = {Beginning in mid-1974, the Data Management Center of the Office of the Secretary of the Department of Health, Education, and Welfare undertook to validate an analytic model of computer performance in the context of a large multiprogrammed configuration operating under a variety of actual production workload conditions. A primary goal of this validation effort was to test the adequacy of the framework offered by the ``exponential queuing network" model schema (as developed by Buzen, et al.) in representing ``real world" situations with sufficient accuracy to be useful as a general tool in support of long range planning. The purpose of this paper is to report on the results of that validation effort.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {44--58},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800200.806181},
 doi = {http://doi.acm.org/10.1145/800200.806181},
 acmid = {806181},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Neilson:1976:APM:800200.806182,
 author = {Neilson, John E.},
 title = {An analytic performance model of a multiprogrammed batch-timeshared computer},
 abstract = {The paper presents an analytic performance model of a multiprogrammed batch and timeshared context swapped system. The model predicts resource utilizations, batch throughput and timesharing response times as functions of the system hardware characteristics, including core size; load characteristics, differing between batch and interactive users; and the numbers of batch and interactive users. Thus, it allows one to study the effects of different hardware configurations and/or load patterns on system performance.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/800200.806182},
 doi = {http://doi.acm.org/10.1145/800200.806182},
 acmid = {806182},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Turner:1976:PEI:800200.806183,
 author = {Turner, Rollins and Levy, Hank},
 title = {Performance evaluation of IAS on the PDP-11/70},
 abstract = {Digital Equipment Corporation has recently developed a new PDP-11 operating system called IAS\&mdash;Interactive Application System. Since the system was to be significantly different from existing PDP-11 systems, customers and DEC field personnel had little information about the kinds of load it would handle or what level of performance could be expected. The work reported in this paper had the objective of producing such information prior to the release of the product. The major goal was to produce a set of guidelines indicating the kinds of loads that could be handled within limits of acceptable performance by various IAS configurations. The guidelines had to be in terms understandable to salesmen and customers. At the same time they had to be based on technically sound performance measurements that could be precisely explained and repeated. Our approach to this problem was to characterize each hardware configuration in terms of the users that it could support while meeting specific response time criteria.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {71--74},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800200.806183},
 doi = {http://doi.acm.org/10.1145/800200.806183},
 acmid = {806183},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Batson:1976:MML:800200.806184,
 author = {Batson, A. P. and Madison, A. W.},
 title = {Measurements of major locality phases in symbolic reference strings},
 abstract = {Previous studies have shown that a program's behavior can be characterized as a sequence of transitions between phases of execution during which some subset of the program's segments is referenced. The nature of the phases (sometimes called localities or regimes), and the characteristics of the transitions between neighboring phases, will determine the performance of the program on a virtual memory system. We here investigate methods for identifying those ``major" phases in programs which correspond to intervals of distinctive referencing behavior. Experimental measurements on symbolic reference strings generated by production Algol-60 programs are used to determine the properties of these phases and the transitions between them. The experimental results are discussed in terms of current models for program behavior and also with regard to their implications for memory management systems.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800200.806184},
 doi = {http://doi.acm.org/10.1145/800200.806184},
 acmid = {806184},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Locality, Memory management, Program behavior, Reference strings},
} 

@inproceedings{Partridge:1976:HMR:800200.806185,
 author = {Partridge, D. R. and Card, R. E.},
 title = {Hardware monitoring of real-time aerospace computer systems},
 abstract = {Hardware monitoring has proven to be a useful means for measuring the performance of computer systems generally, and is particularly attractive for use on real-time systems due to its attribute of non-interference with system operation. This technique is uniquely able to quantify precisely the interactions between hardware and software, which must be completely understood in these systems. In this paper, we report the application of a commercially-developed hardware monitor to two real-time computer systems. The first was an airborne computer whose basic task was radar data processing. The second system was a more powerful shipboard computer which included a unique, extensive internal performance data gathering facility. The objectives of these projects were to answer several specific performance questions, to demonstrate techniques for software verification and performance enhancement, and to obtain data for system modifications and future designs. A variety of measurements were employed to meet these objectives. They are described in the paper together with our rational in selecting them, typical measurement results, and a discussion of the measurement significance. We found that, while our monitor provided much valuable data, it was not completely sufficient for our purposes. We therefore developed special-purpose hardware to augment the monitor and this combination has been quite successful. Of our various measurements, software event traces were found to be most valuable for system verification and problem diagnosis. The internal data gathering hardware in one of the monitored systems proved convenient to use and powerful as well.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {85--101},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/800200.806185},
 doi = {http://doi.acm.org/10.1145/800200.806185},
 acmid = {806185},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Spirn:1976:MST:800200.806186,
 author = {Spirn, Jeffrey R.},
 title = {Multi-queue scheduling of two tasks},
 abstract = {A class of schedules in the two customer central server queueing model, consisting of a ``CPU" server and m ``I/O" servers, is considered. Optimal (maximal CPU utilization) CPU and I/O schedules are obtained. The best CPU schedule depends on the I/O schedule in effect; and is either Longest or Shortest-Expected-Remaining-Processing-Time-First. However, for certain I/O schedules the CPU schedule is immaterial. The best I/O schedule is always to process the (expected) longer CPU customer first.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {102--108},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800200.806186},
 doi = {http://doi.acm.org/10.1145/800200.806186},
 acmid = {806186},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reiser:1976:CAS:800200.806187,
 author = {Reiser, M. and Kobayashi, H.},
 title = {On the convolution algorithm for separable queuing networks},
 abstract = {Research into queuing networks and their applications to computer systems is in a state of prosperity. The object of this paper is to discuss the computational aspect of separable queuing networks. Separable networks constitute that class of models for which a solution can be computed efficiently for fairly large problems. Open networks do not pose any computational problem. It is the case of closed networks where the subject of numerical algorithms becomes an issue. In this paper, we shall take a fresh look at closed queuing networks, which we introduce as conditioned solution of suitably chosen open networks. This view will provide a probabilistic interpretation of what is normally called the normalization constant. Computational algorithms, then, result in a systematic way.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {109--117},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800200.806187},
 doi = {http://doi.acm.org/10.1145/800200.806187},
 acmid = {806187},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gelenbe:1976:PMC:800200.806188,
 author = {Gelenbe, E. and Pujolle, G.},
 title = {Probabilistic models of computer systems},
 abstract = {We develop a method based on diffusion approximations in order to compute, under some general conditions, the queue length distribution for a queue in a network. Applications to computer networks and to time-sharing systems are presented.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {118--125},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800200.806188},
 doi = {http://doi.acm.org/10.1145/800200.806188},
 acmid = {806188},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Svobodova:1976:REP:800200.806189,
 author = {Svobodova, Liba and Mattson, Roy},
 title = {The role of emulation in performance measurement and evaluation},
 abstract = {Emulation of systems makes it possible to combine the predictive power of simulation with the advantages of measurement carried under a real system workload. An emulator is a microprogrammed implementation of the basic hardware machine. It can be easily instrumented to collect performance statistics on the instruction set processor (ISP) level and support performance measurement of different configurations and software of the emulated system. This paper describes the monitoring capabilities of the Microprogrammable Multi-Processor (MMP), a powerful emulator system that serves as an experimental tool for evaluating computer systems. The measurement capabilities of the MMP on various system levels are described, as well as existing performance monitoring tools and their applications. Preliminary results contrasting the Gibson mix and measured instruction frequencies on the AN/GYK-12 computer in a TACFIRE system are given.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {126--135},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800200.806189},
 doi = {http://doi.acm.org/10.1145/800200.806189},
 acmid = {806189},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Lindsay:1976:HMS:800200.806190,
 author = {Lindsay, David S.},
 title = {A hardware monitor study of a CDC KRONOS system},
 abstract = {This study was performed on a CDC-6600 computer having 500K of Extended Core Storage (ECS), twenty Peripheral Processor Units (PPU's) and twenty-four I/O channels, as shown in Figure I-1. The CPU can load, store, and execute from the 131K of 1 microsecond storage which comprises CM. It can also access ECS, but this access is limited to transferring blocks of data between CM and ECS at the rate of ten 60-bit words per microsecond. The CPU cannot access peripheral devices in any manner whatever. The KRONOS operating system supports both batch and time-sharing users. As many user programs as will fit in CM may be loaded at any one time, with the CPU time-sliced among them. Swapping (called rollin and rollout under KRONOS) is performed to ECS with overflow to disk. A serious drawback of the CDC supplied system is that ECS rollouts are performed through a PPU and the Distributed Data Path (DDP), which is one hundred times slower than the CM/ECS direct transfer available to the CPU.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {136--144},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800200.806190},
 doi = {http://doi.acm.org/10.1145/800200.806190},
 acmid = {806190},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hughes:1976:FIM:800200.806191,
 author = {Hughes, James H.},
 title = {A functional instruction mix and some related topics},
 abstract = {In recent years the use of instruction mixes to define measures of relative computational power has diminished. This paper will describe how a systemic functional mix of much greater power and complexity than the traditional Gibson Mix was derived, then employed effectively in solving problems which arise in central processor definition and design. Some extensions of the basic technique are also covered, which include decomposition into fractional mixes as well as the study of instruction doublets and triplets.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {145--153},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800200.806191},
 doi = {http://doi.acm.org/10.1145/800200.806191},
 acmid = {806191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ries:1976:TSP:800200.806192,
 author = {Ries, Edward G. and Harmon, Donald J.},
 title = {The Test Support Program (TSP) a real-time interactive simulation system},
 abstract = {The Test Support Program (TSP) is a real-time interactive computer-based simulation model that provides the environment for testing and verifying the operation of netted air control systems. It provides computer-to-computer simulation of data link timing; message interpretation, response, and transmission; and on-line data analysis and recording for up to nine interfaced systems of various types. Event execution and timing is performed in accordance with a time-ordered pre-stored scenario and operator-initiated console inputs. Live and simulated system interfaces and data may be used. The simulation model can accept and use data from live systems. Event and data inputs are structured in a special language and are validated by a pre-processor to aid in test design and error-free execution. Event control and data entry during program execution are from a CRT/keyboard or magnetic tape. TSP permits earlier and more effective verification of program performance, easier design of complicated tests, and reduces reliance on establishment and operation of large complexes of operational and test bed systems. Program operation can be verified in advance of the availability of interfacing hardware and software in a controlled environment.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {154--165},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/800200.806192},
 doi = {http://doi.acm.org/10.1145/800200.806192},
 acmid = {806192},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Anderson:1976:GMC:800200.806193,
 author = {Anderson, James Wayne and Browne, J. C.},
 title = {Graph models of computer systems: Application to performance evaluation of an operating system},
 abstract = {This paper defines and determines a graph model of a computer system in a form applicable to system performance analysis. The power of this modeling technique with respect to comprehensibility, accuracy of representation and ease of validation and modification is demonstrated by application to modeling of the UT-2 operating system for the CDC 6000 series computer system. This multiprocessor-multi-programmed operating system with its high degree of parallelism provides an excellent test for the utility and range of application of graph models in performance evaluation. A programmed representation of the kernel monitor is used. All other system processes are represented in graph form and are input data to the simulator. A generally applicable technique for extracting graph representations of processes are represented in graph form and are input data to the simulator. A generally applicable technique for extracting graph representations of processes from event trace data is described and applied to the event trace generated by the UT-2 operating system. The technique is both complete and general and may be profitably applied for either partial or complete models of any type of complex computer system process where data or techniques for automated graph construction are available or can he applied.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {166--178},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/800200.806193},
 doi = {http://doi.acm.org/10.1145/800200.806193},
 acmid = {806193},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Gonzalez:1976:UCA:800200.806194,
 author = {Gonz\'{a}lez, Carlos},
 title = {Using Covariance Analysis as an aid to interpret the results of a performance measurement},
 abstract = {This paper reports on a measurement study of the Scheduler in the PDP-10 TENEX Operating System under normal loads in a Computer Science research environment at Case Western Reserve University. A complete description of the methodology and the results of a Covariance Analysis is presented here. The analysis was made in parallel with the analysis of the collected data, therefore a brief description of the experiments as well as some comments of the most interesting results are also discussed in this report. The complete report of the study is in Gonz\&aacute;lez [5] which shall serve as a reference document for this paper.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {179--186},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800200.806194},
 doi = {http://doi.acm.org/10.1145/800200.806194},
 acmid = {806194},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kuck:1976:SMC:800200.806195,
 author = {Kuck, D. J. and Kumar, B.},
 title = {A system model for computer performance evaluation},
 abstract = {A framework for the study of computer capacity is given by means of a definition of capacity in terms of speeds of various parts of a computer as well as memory size. In addition to these machine parameters, we also include certain parameters of the programs to be run on a given machine. The calculation of theoretical capacity is given for several combinations of processor, memory, and I/O bandwidth for overlapped machines. The tradeoff between primary memory size and I/O bandwidth is discussed in terms of the new definition.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {187--199},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/800200.806195},
 doi = {http://doi.acm.org/10.1145/800200.806195},
 acmid = {806195},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Buzen:1976:FLC:800200.806196,
 author = {Buzen, J. P.},
 title = {Fundamental laws of computer system performance},
 abstract = {A number of laws are derived which establish relationships between throughput, response time, device utilization, space-time products and various other factors related to computer system performance. These laws are obtained by using the operational method of computer system analysis. The operational method, which differs significantly from the conventional stochastic modeling approach, is based on a set of concepts that correspond naturally and directly to observed properties of real computer systems. Except for measurement errors, the operational laws presented in this paper apply with complete precision to all collections of observational data, and they are similar to fundamental laws found in other areas of engineering and applied science.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {200--210},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800200.806196},
 doi = {http://doi.acm.org/10.1145/800200.806196},
 acmid = {806196},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Computer performance evaluation, Operational method, Queueing systems, Response time, Throughput},
} 

@inproceedings{Leroudier:1976:POM:800200.806197,
 author = {Leroudier, Jacques and Potier, Dominique},
 title = {Principles of optimality for multiprogramming},
 abstract = {In this paper, we shall use the same analytical model to investigate the behaviour of the paging drum when the degree of multiprogramming is set to its optimal value. The main conclusion, which corroborates the general feeling on performance of virtual memory systems [3, 4, 5], is that drum utilization remains in the 50\% range whenever CPU utilization is maximized, if no resource is saturated. The results are validated through simulation experiments in order to relax some theoretical assumptions used in the analytical model and to take into account some detailed mechanisms such as CPU overheads. As an application, an adaptive control algorithm based on the activity of the paging device is proposed. The algorithm has been simulated and comparisons with results reported in [2] are presented. Moreover, after investigations we show also that optimal performance can only be achieved through a balanced use of the different resources of the system. This conclusion cross-checks and extends Buzen's theoretical results [6] and Wulf's practical ones [7].},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {211--218},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/800200.806197},
 doi = {http://doi.acm.org/10.1145/800200.806197},
 acmid = {806197},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Denning:1976:LCO:800200.806198,
 author = {Denning, Peter J. and Kahn, Kevin C.},
 title = {An L=S criterion for optimal multiprogramming},
 abstract = {Balancing interpagefault lifetime (L) against page swap time (S) has always been a performance criterion of great intuitive appeal. This paper shows that, under normal conditions, controlling the memory policy parameter to enforce the constraint L \&ge; S, and allowing the multiprogramming load to rise as high as demand warrants without violating this constraint, will produce a load slightly higher than optimum. Equivalently, using the criterion L \&equil; uS for some u slightly larger than 1 will approximate an optimal load. Using simulations, this criterion is compared with two others reported in the literature, namely the ``knee criterion" (operate with L at the knee of the lifetime curve) and the 50\% criterion (operate with the paging device at 50\% utilization).The knee criterion produced optimal loads more often than the L\&equil;S criterion, which in turn produced optimal loads more often than the 50\% criterion. Since no practical implementation of the knee criterion is known, the L\&equil;S criterion is the most attractive of the three.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {219--229},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800200.806198},
 doi = {http://doi.acm.org/10.1145/800200.806198},
 acmid = {806198},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Labetoulle:1976:SQN:800200.806199,
 author = {Labetoulle, J. and Pujolle, G.},
 title = {A study of queueing networks with deterministic service and applications to computer networks},
 abstract = {We consider packet-switching network as a queueing system with constant service times: that means that the lengths of the packets are equal. We study such a network by isolating a particular path, that we consider as a tandem system of queueing. Two aspects of the question will be examined: 1 - The study of the response time of such a system 2 - The study of the throughput of such a system when introducing the notion of time-out for each packet. The results we obtained show that when neglecting the interferences between paths the response time is independent of the order of the stations. The control policy we examined prove the necessity to have a limited memorisation capacity at each node of a packet-switching network. These results confirm the intuitive options taken in a network like CYCLADES.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {230--240},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/800200.806199},
 doi = {http://doi.acm.org/10.1145/800200.806199},
 acmid = {806199},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Mahmoud:1976:PCS:800200.806200,
 author = {Mahmoud, Samy and Riordon, J. S.},
 title = {Protocol considerations for software controlled access methods in distributed data bases},
 abstract = {Access control to shared files in a distributed computing environment requires an efficient method of allocating file resources with local and remote user processes. While software controlled access methods are convenient from the user's point of view, they give rise to serious operational problems such as job interferences (deadlock situations) and critical race conditions. Two software controlled access schemes, one centralized and one distributed are described in this paper. A basic set of protocols is derived to illustrate the implementation aspects of each scheme in a given network environment. The two approaches are then evaluated in the light of several performance criteria.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {241--264},
 numpages = {24},
 url = {http://doi.acm.org/10.1145/800200.806200},
 doi = {http://doi.acm.org/10.1145/800200.806200},
 acmid = {806200},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cady:1976:PEB:800200.806201,
 author = {Cady, John and Sorli, Ronald M. and Thornton, Barry S.},
 title = {Performance evaluation of a batch processing computer network under different processing strategies},
 abstract = {In an Australian national batch-oriented computer network, processing strategies have been investigated, using simulation, to give guidelines for management of the work load and to provide direction for measurement of the performance of the system under actual conditions. The nature of the conflicting interactions between computer resource utilization and transmission line requirements has been studied to provide the basis for an economic analysis to determine the most cost-effective processing strategy.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {265--271},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800200.806201},
 doi = {http://doi.acm.org/10.1145/800200.806201},
 acmid = {806201},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Te:1976:HMA:800200.806202,
 author = {T\^{e}, Tr\^{a}n Qu\^{o}c},
 title = {An heuristic model for analysis of memory use under static partition allocation strategies},
 abstract = {This paper examines minimum memory wastage strategies for partitioning and allocating main memory to a stream of jobs, in a context where each job resides throughout its execution in a frame to which it is assigned. In contrast to usual approaches, the present one considers core memory as the main limiting resource, and assumes that job competition for other resources can be neglected. From job mix characteristics, a condition for a given memory partition to avoid system saturation is postulated. Then, among such partitions avoiding saturation, we are interested in 1) those requiring the least amount of space, 2) those leading to the lowest core fragmentation. Concerning the first problem, we give a simple method for generating the smallest partition, and show that ``exact fit" allocation should be used. As for the second problem, if a partition is given so that exact fit meets the overload constraint then this algorithm is optimal; otherwise, best fit would be the best algorithm.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {272--281},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800200.806202},
 doi = {http://doi.acm.org/10.1145/800200.806202},
 acmid = {806202},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Landwehr:1976:EPM:800200.806203,
 author = {Landwehr, Carl E.},
 title = {An endogenous priority model for load control in combined batch - interactive computer systems},
 abstract = {A relatively high level analytical model for computer systems serving both batch and interactive users is presented. The model is unusual in its employment of an endogenous priority scheme to represent a class of strategies for controlling service to the two types of customers. Numerical methods developed by V. L. Wallace are used to generate steady state probability distributions for the infinite state Markov chain formed by the model. Data from the Michigan Terminal System, which includes a load controlling mechanism of the type modelled, is used to validate the model. Finally, additional parameter studies indicate that the model reflects the dynamic behavior of such system in a reasonable way.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {282--295},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/800200.806203},
 doi = {http://doi.acm.org/10.1145/800200.806203},
 acmid = {806203},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bard:1976:EAS:800200.806204,
 author = {Bard, Yonathan},
 title = {An experimental approach to system tuning},
 abstract = {It is desired to find the values of certain system parameters which maximize some performance criterion. Using standard experimental design techniques, one runs an initial set of experiments which explore the system's response surface. Subsequently, the data are smoothed, and a hill climbing technique is used to locate the maximum. This technique was employed successfully to tune the parameters in an experimental version of the VM/370 scheduler.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {296--305},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800200.806204},
 doi = {http://doi.acm.org/10.1145/800200.806204},
 acmid = {806204},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Coffman:1976:GBL:800200.806205,
 author = {Coffman,Jr., E. G. and Sethi, Ravi},
 title = {A generalized bound on LPT sequencing},
 abstract = {In this paper we shall generalize Graham's result so as to include a parameter characterizing the number of tasks assigned to processors by the LPT rule. The new result will show that the worst-case performance bound for LPT sequencing approaches unity approximately as 1+1/k, where k is the least number of tasks on any processor, or where k is the number of tasks on a processor whose last task terminates the schedule. Thus, we shall have a result very similar to the parameterized bounds for bin-packing heuristics [JDUGG]. We shall also obtain out of the analysis an alternate proof of Graham's result.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {306--310},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/800200.806205},
 doi = {http://doi.acm.org/10.1145/800200.806205},
 acmid = {806205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ting:1976:ACM:800200.806206,
 author = {Ting, Dennis W.},
 title = {Allocation and compaction - a mathematical model for memory management},
 abstract = {A mathematical model for the processes of allocation and compaction is developed. The behavior of the most commonly used compaction algorithm is studied under this model. The relationship between time and space and trade-offs between ``conservative" and ``non-conservative" compaction algorithms are also investigated.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {311--317},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/800200.806206},
 doi = {http://doi.acm.org/10.1145/800200.806206},
 acmid = {806206},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rose:1976:VQM:800200.806207,
 author = {Rose, C. A.},
 title = {Validation of a queueing model with classes of customers},
 abstract = {There have been many queueing models of computer systems published in the literature, but relatively few studies have appeared in which the models have been validated on actual systems. A procedure is described which permits an analyst to obtain values for the parameters of queueing models using measured statistics of operational computer systems. The particular model which was validated is the first queueing network model which will permit more than one class of customer. Partitioning a computer system environment into classes of timesharing and batch jobs appears to be useful, and the model was validated with these classes on an IBM 370/155-2 and an IBM 370/168.},
 booktitle = {Proceedings of the 1976 ACM SIGMETRICS conference on Computer performance modeling measurement and evaluation},
 series = {SIGMETRICS '76},
 year = {1976},
 location = {Cambridge, Massachusetts, United States},
 pages = {318--326},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/800200.806207},
 doi = {http://doi.acm.org/10.1145/800200.806207},
 acmid = {806207},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Graham:1974:MPB:1007773.809367,
 author = {Graham, G. Scott and Denning, Peter J.},
 title = {Multiprogramming and program behavior},
 abstract = {Dynamic multiprogramming memory management strategies are classified and compared using extant test data. Conclusions about program behavior are then drawn.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {1--8},
 numpages = {8},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809367},
 doi = {10.1145/1007773.809367},
 acmid = {809367},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Graham:1974:MPB:800277.809367,
 author = {Graham, G. Scott and Denning, Peter J.},
 title = {Multiprogramming and program behavior},
 abstract = {Dynamic multiprogramming memory management strategies are classified and compared using extant test data. Conclusions about program behavior are then drawn.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {1--8},
 numpages = {8},
 url = {http://portal.acm.org/citation.cfm?id=800277.809367},
 doi = {10.1145/800277.809367},
 acmid = {809367},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brandwain:1974:MPV:800277.809368,
 author = {Brandwain, A. and Buzen, J. and Gelenbe, E. and Potier, D.},
 title = {A model of performance for virtual memory systems},
 abstract = {Queueing network models are well suited for analyzing certain resource allocation problems associated with operating system design. An example of such a problem is the selection of the level of multiprogramming in virtual memory systems. If the number of programs actively competing for main memory is allowed to reach too high a value, trashing will occur and performance will be seriously degraded. On the other hand, performance may also suffer if the level of multiprogramming drops too low since system resources can become seriously under utilized in this case. Thus it is important for virtual memory systems to maintain optimal or near optimal levels of multiprogramming at all times. This paper presents an analytic model of computer system behavior which can be used to study multiprogramming optimization in virtual memory systems. The model, which explicitly represents the numerous interactions which occur as the level of multiprogramming varies, is used to numerically generate performance curves for representative sets of parameters. A simplified model consisting of a CPU and a single backing store device is then used to derive an approximate expression for the optimal level of multiprogramming. The simplified model is also used to examine the transient behavior of such systems. The mathematical model we present is based on some simplifying assumptions; in particular all programs executing in the system are supposed to be statistically identical. In this respect the model we present must be considered to be a theoretical explanation of a phenomenon (thrashing) observed in certain operating systems rather than an exact representation of reality. Certain assumptions of the mathematical model are relaxed in a simulation model where distribution functions of service times at the secondary memory and input-output devices are arbitrary; by comparison with the theoretical results we see that CPU utilization and throughput are not very sensitive to the specific forms of these distributions and that the usual exponential assumptions yield quite satisfactory results. The simulation model is also programmed to contain overhead. Again we observe that the mathematical model's predictions are in fair agreement with the useful CPU utilization predicted by the simulation experiments.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {9--},
 url = {http://portal.acm.org/citation.cfm?id=800277.809368},
 doi = {10.1145/800277.809368},
 acmid = {809368},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brandwain:1974:MPV:1007773.809368,
 author = {Brandwain, A. and Buzen, J. and Gelenbe, E. and Potier, D.},
 title = {A model of performance for virtual memory systems},
 abstract = {Queueing network models are well suited for analyzing certain resource allocation problems associated with operating system design. An example of such a problem is the selection of the level of multiprogramming in virtual memory systems. If the number of programs actively competing for main memory is allowed to reach too high a value, trashing will occur and performance will be seriously degraded. On the other hand, performance may also suffer if the level of multiprogramming drops too low since system resources can become seriously under utilized in this case. Thus it is important for virtual memory systems to maintain optimal or near optimal levels of multiprogramming at all times. This paper presents an analytic model of computer system behavior which can be used to study multiprogramming optimization in virtual memory systems. The model, which explicitly represents the numerous interactions which occur as the level of multiprogramming varies, is used to numerically generate performance curves for representative sets of parameters. A simplified model consisting of a CPU and a single backing store device is then used to derive an approximate expression for the optimal level of multiprogramming. The simplified model is also used to examine the transient behavior of such systems. The mathematical model we present is based on some simplifying assumptions; in particular all programs executing in the system are supposed to be statistically identical. In this respect the model we present must be considered to be a theoretical explanation of a phenomenon (thrashing) observed in certain operating systems rather than an exact representation of reality. Certain assumptions of the mathematical model are relaxed in a simulation model where distribution functions of service times at the secondary memory and input-output devices are arbitrary; by comparison with the theoretical results we see that CPU utilization and throughput are not very sensitive to the specific forms of these distributions and that the usual exponential assumptions yield quite satisfactory results. The simulation model is also programmed to contain overhead. Again we observe that the mathematical model's predictions are in fair agreement with the useful CPU utilization predicted by the simulation experiments.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {9--},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809368},
 doi = {10.1145/1007773.809368},
 acmid = {809368},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Henderson:1974:OCW:800277.809369,
 author = {Henderson, Greg and Rodriguez-Rosell, Juan},
 title = {The optimal choice of window sizes for working set dispatching},
 abstract = {The concept of varying window size in a working set dispatcher to control working set size and number of page faults is examined. A space-time cost equation is developed and used to compare fixed window size to variable window size for different types of secondary storage based on the simulated execution of real programs. A general approach is indicated for studying the relative merit of the two dispatching algorithms and their interaction with different hardware configurations.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {10--33},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=800277.809369},
 doi = {10.1145/800277.809369},
 acmid = {809369},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Dispatching, Optimal control, Resource allocation, Supervisory systems, Time-sharing systems, Working set},
} 

@article{Henderson:1974:OCW:1007773.809369,
 author = {Henderson, Greg and Rodriguez-Rosell, Juan},
 title = {The optimal choice of window sizes for working set dispatching},
 abstract = {The concept of varying window size in a working set dispatcher to control working set size and number of page faults is examined. A space-time cost equation is developed and used to compare fixed window size to variable window size for different types of secondary storage based on the simulated execution of real programs. A general approach is indicated for studying the relative merit of the two dispatching algorithms and their interaction with different hardware configurations.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {10--33},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809369},
 doi = {10.1145/1007773.809369},
 acmid = {809369},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Dispatching, Optimal control, Resource allocation, Supervisory systems, Time-sharing systems, Working set},
} 

@article{Denning:1974:CLP:1007773.809370,
 author = {Denning, Peter J.},
 title = {Comments on a linear paging model},
 abstract = {The linear approximation relating mean time between page transfers between levels of memory, as reported by Saltzer for Multics, is examined. It is tentatively concluded that this approximation is untenable for main memory, especially under working set policies; and that the linearity of the data for the drum reflects the behavior of the Multics scheduler for background jobs, not the behavior of programs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {34--48},
 numpages = {15},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809370},
 doi = {10.1145/1007773.809370},
 acmid = {809370},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Denning:1974:CLP:800277.809370,
 author = {Denning, Peter J.},
 title = {Comments on a linear paging model},
 abstract = {The linear approximation relating mean time between page transfers between levels of memory, as reported by Saltzer for Multics, is examined. It is tentatively concluded that this approximation is untenable for main memory, especially under working set policies; and that the linearity of the data for the drum reflects the behavior of the Multics scheduler for background jobs, not the behavior of programs.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {34--48},
 numpages = {15},
 url = {http://portal.acm.org/citation.cfm?id=800277.809370},
 doi = {10.1145/800277.809370},
 acmid = {809370},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brice:1974:FCR:800277.809371,
 author = {Brice, Richard S. and Browne, J. C.},
 title = {Feedback coupled resource allocation policies in the multiprogramming-multiprocessor computer system},
 abstract = {This paper presents model studies of some integrated feedback-driven scheduling systems for a multiprogrammed computer system. This abstract can present only the conclusions of the studies and little of the supporting data and detail. The basic format of the analysis is to fix a size for the local buffers and a total size for the collection buffers, to define a set of algorithms for the determination of the data removal quanta to the local buffers, the allocation of space in the collection buffers, and the look-ahead mechanism for input and then to evaluate the relative merits of the various strategies by the resulting CPU efficiency. Three feedback algorithms are studied as examples in this work.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {49--53},
 numpages = {5},
 url = {http://portal.acm.org/citation.cfm?id=800277.809371},
 doi = {10.1145/800277.809371},
 acmid = {809371},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brice:1974:FCR:1007773.809371,
 author = {Brice, Richard S. and Browne, J. C.},
 title = {Feedback coupled resource allocation policies in the multiprogramming-multiprocessor computer system},
 abstract = {This paper presents model studies of some integrated feedback-driven scheduling systems for a multiprogrammed computer system. This abstract can present only the conclusions of the studies and little of the supporting data and detail. The basic format of the analysis is to fix a size for the local buffers and a total size for the collection buffers, to define a set of algorithms for the determination of the data removal quanta to the local buffers, the allocation of space in the collection buffers, and the look-ahead mechanism for input and then to evaluate the relative merits of the various strategies by the resulting CPU efficiency. Three feedback algorithms are studied as examples in this work.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {49--53},
 numpages = {5},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809371},
 doi = {10.1145/1007773.809371},
 acmid = {809371},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Halachmi:1974:CCT:1007773.809372,
 author = {Halachmi, Baruch and Franta, W. R.},
 title = {A closed, cyclic, two-stage multiprogrammed system model and its diffusion approximation solution},
 abstract = {In this paper attention is focused on closed multiprogrammed computer type systems. In particular, two-stage closed queueing systems are considered. The first stage can be associated with the CPU (Central Processing Unit) and the other with the I/O (Input-Output) operations. For all the models discussed. For the first model we consider the GI<subscrpt>1</subscrpt>/M<subscrpt>S</subscrpt>/N system, which allows the service times of a single CPU to obey any general probability distribution, with finite variance, while the I/O servers are taken to be exponential. The second model is an extension of the first where the concept of feedback is implemented in the CPU stage. This concept plays an important role in computer environments where the operating system includes the multiplexing or page on demand property. The third model, the M<subscrpt>S</subscrpt><subscrpt>1</subscrpt>/M<subscrpt>S</subscrpt><subscrpt>2</subscrpt>/N, deals with multiprocessing computer systems where possibly more than one CPU is available, but all servers are assumed to be exponential. In the spirit of the approximation to the GI/G/S open system, as a final model, we construct the approximate solution to the GI<subscrpt>S</subscrpt><subscrpt>1</subscrpt>/GI<subscrpt>S</subscrpt><subscrpt>2</subscrpt>/N closed system and discuss the circumstances under which its use is advisable. Several numerical examples for each of the models are given, each accompanied by appropriate simulation results for comparison. It is on the basis of these comparisons that the quality of the suggested diffusion approximations can be judged. The diffusion approximating formulas should be regarded not only as a numerical technique, but also as a simplifying approach, by which deeper insight can be gained into complicated queueing systems. Considerable work remains to be done, using as a methodology the approach, given here, and several possible extensions are presented.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {54--64},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809372},
 doi = {10.1145/1007773.809372},
 acmid = {809372},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Halachmi:1974:CCT:800277.809372,
 author = {Halachmi, Baruch and Franta, W. R.},
 title = {A closed, cyclic, two-stage multiprogrammed system model and its diffusion approximation solution},
 abstract = {In this paper attention is focused on closed multiprogrammed computer type systems. In particular, two-stage closed queueing systems are considered. The first stage can be associated with the CPU (Central Processing Unit) and the other with the I/O (Input-Output) operations. For all the models discussed. For the first model we consider the GI<subscrpt>1</subscrpt>/M<subscrpt>S</subscrpt>/N system, which allows the service times of a single CPU to obey any general probability distribution, with finite variance, while the I/O servers are taken to be exponential. The second model is an extension of the first where the concept of feedback is implemented in the CPU stage. This concept plays an important role in computer environments where the operating system includes the multiplexing or page on demand property. The third model, the M<subscrpt>S</subscrpt><subscrpt>1</subscrpt>/M<subscrpt>S</subscrpt><subscrpt>2</subscrpt>/N, deals with multiprocessing computer systems where possibly more than one CPU is available, but all servers are assumed to be exponential. In the spirit of the approximation to the GI/G/S open system, as a final model, we construct the approximate solution to the GI<subscrpt>S</subscrpt><subscrpt>1</subscrpt>/GI<subscrpt>S</subscrpt><subscrpt>2</subscrpt>/N closed system and discuss the circumstances under which its use is advisable. Several numerical examples for each of the models are given, each accompanied by appropriate simulation results for comparison. It is on the basis of these comparisons that the quality of the suggested diffusion approximations can be judged. The diffusion approximating formulas should be regarded not only as a numerical technique, but also as a simplifying approach, by which deeper insight can be gained into complicated queueing systems. Considerable work remains to be done, using as a methodology the approach, given here, and several possible extensions are presented.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {54--64},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=800277.809372},
 doi = {10.1145/800277.809372},
 acmid = {809372},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schwetman:1974:ATS:800277.809373,
 author = {Schwetman, H. D.},
 title = {Analysis of a time-sharing subsystem (A Preliminary Report)},
 abstract = {The MESA subsystem provides a wide variety of services to remotely located users of the computing facilities of the Purdue University Computing Center. This paper presents the preliminary steps of an in-depth study into the behavior of MESA. The study uses a software data-gathering facility to analyze the usage and queueing aspects of this behavior and to provide values for parameters used by two models of the subsystem. These models, a network-of-queues model and a simulation model, are designed to project subsystem behavior in different operating environments. The paper includes a number of tables and figures which highlight the results, so far, of the study.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {65--75},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=800277.809373},
 doi = {10.1145/800277.809373},
 acmid = {809373},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schwetman:1974:ATS:1007773.809373,
 author = {Schwetman, H. D.},
 title = {Analysis of a time-sharing subsystem (A Preliminary Report)},
 abstract = {The MESA subsystem provides a wide variety of services to remotely located users of the computing facilities of the Purdue University Computing Center. This paper presents the preliminary steps of an in-depth study into the behavior of MESA. The study uses a software data-gathering facility to analyze the usage and queueing aspects of this behavior and to provide values for parameters used by two models of the subsystem. These models, a network-of-queues model and a simulation model, are designed to project subsystem behavior in different operating environments. The paper includes a number of tables and figures which highlight the results, so far, of the study.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {65--75},
 numpages = {11},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809373},
 doi = {10.1145/1007773.809373},
 acmid = {809373},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Reiser:1974:ASC:800277.809374,
 author = {Reiser, M. and Konheim, A. G.},
 title = {The analysis of storage constraints by a queueing network model with blocking},
 abstract = {The finite capacity of storage has a significant effect on the performance of a contemporary computer system. Yet it is difficult to formulate this problem and analyze it by existing queueing network models. We present an analysis of an open queueing model with two servers in series in which the second server has finite storage capacity. This network is an exponential service system; the arrival of requests into the system is modeled by a Poisson process (of rate \&lgr;) and service times in each stage are exponentially distributed (with rates \&agr; and \&bgr; respectively). Requests are served in each stage according to the order of their arrival. The principal characteristic of the service in this network is <underline>blocking</underline>; when M requests are queued or in service in the second stage, the server in the first stage is blocked and ceases to offer service. Service resumes in the first stage when the queue length in the second stage falls to M-1. Neuts [1] has studied two-stage blocking networks (without feedback) under more general statistical hypothesis than ours. Our goal is to provide an algorithmic solution which may be more accessible to engineers.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {76--81},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=800277.809374},
 doi = {10.1145/800277.809374},
 acmid = {809374},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Reiser:1974:ASC:1007773.809374,
 author = {Reiser, M. and Konheim, A. G.},
 title = {The analysis of storage constraints by a queueing network model with blocking},
 abstract = {The finite capacity of storage has a significant effect on the performance of a contemporary computer system. Yet it is difficult to formulate this problem and analyze it by existing queueing network models. We present an analysis of an open queueing model with two servers in series in which the second server has finite storage capacity. This network is an exponential service system; the arrival of requests into the system is modeled by a Poisson process (of rate \&lgr;) and service times in each stage are exponentially distributed (with rates \&agr; and \&bgr; respectively). Requests are served in each stage according to the order of their arrival. The principal characteristic of the service in this network is <underline>blocking</underline>; when M requests are queued or in service in the second stage, the server in the first stage is blocked and ceases to offer service. Service resumes in the first stage when the queue length in the second stage falls to M-1. Neuts [1] has studied two-stage blocking networks (without feedback) under more general statistical hypothesis than ours. Our goal is to provide an algorithmic solution which may be more accessible to engineers.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {76--81},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809374},
 doi = {10.1145/1007773.809374},
 acmid = {809374},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Schatzoff:1974:SVT:1007773.809375,
 author = {Schatzoff, M. and Tillman, C. C.},
 title = {Statistical validation of a trace-driven simulator},
 abstract = {A common problem encountered in computer system simulation is that of validating that the simulator can produce, with a reasonable degree of accuracy, the same information that can be obtained from the modelled system. This is basically a statistical problem because there are usually limitations with respect to the number of controlled tests which can be carried out, and assessment of the fidelity of the model is a function of the signal to noise ratio. That is, the magnitude of error which can be tolerated depends upon the size of the effect to be predicted. In this paper, we describe by example how techniques of statistical design and analysis of experiments have been used to validate the modeling of the dispatching algorithm of a time sharing system. The examples are based on a detailed, trace-driven simulator of CP-67. They show that identical factorial experiments involving parameters of this algorithm, when carried out on both the simulator and on the actual system, produced statistically comparable effects.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {82--93},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809375},
 doi = {10.1145/1007773.809375},
 acmid = {809375},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Schatzoff:1974:SVT:800277.809375,
 author = {Schatzoff, M. and Tillman, C. C.},
 title = {Statistical validation of a trace-driven simulator},
 abstract = {A common problem encountered in computer system simulation is that of validating that the simulator can produce, with a reasonable degree of accuracy, the same information that can be obtained from the modelled system. This is basically a statistical problem because there are usually limitations with respect to the number of controlled tests which can be carried out, and assessment of the fidelity of the model is a function of the signal to noise ratio. That is, the magnitude of error which can be tolerated depends upon the size of the effect to be predicted. In this paper, we describe by example how techniques of statistical design and analysis of experiments have been used to validate the modeling of the dispatching algorithm of a time sharing system. The examples are based on a detailed, trace-driven simulator of CP-67. They show that identical factorial experiments involving parameters of this algorithm, when carried out on both the simulator and on the actual system, produced statistically comparable effects.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {82--93},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=800277.809375},
 doi = {10.1145/800277.809375},
 acmid = {809375},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Ferrari:1974:GSM:800277.809376,
 author = {Ferrari, Domenico and Liu, Mark},
 title = {A general-purpose software measurement tool},
 abstract = {A software measurement tool designed for the users of PRIME, an interactive system being developed, is presented. The tool, called SMT, allows its user to instrument a program, modify a pre-existing instrumentation and specify how the collected data are to be reduced by typing in a few simple commands. The user can also write his own measurement routines, which specify the actions to be taken at event detection time, and submit them to the SMT; after checking their correctness, the SMT deals with them as with its built-in, standard measurement routines. The design goals of a general-purpose tool like the SMT are discussed, and the prototype version of the tool, which has been implemented, is described from the two distinct viewpoints of a user and of a measurement-tool designer. An example of the application of the prototype to a measurement problem is illustrated, the reasons why not all of the design goals have been achieved in the implementation of the prototype are reviewed, and some of the foreseeable extensions of the SMT are described.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {94--105},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=800277.809376},
 doi = {10.1145/800277.809376},
 acmid = {809376},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Ferrari:1974:GSM:1007773.809376,
 author = {Ferrari, Domenico and Liu, Mark},
 title = {A general-purpose software measurement tool},
 abstract = {A software measurement tool designed for the users of PRIME, an interactive system being developed, is presented. The tool, called SMT, allows its user to instrument a program, modify a pre-existing instrumentation and specify how the collected data are to be reduced by typing in a few simple commands. The user can also write his own measurement routines, which specify the actions to be taken at event detection time, and submit them to the SMT; after checking their correctness, the SMT deals with them as with its built-in, standard measurement routines. The design goals of a general-purpose tool like the SMT are discussed, and the prototype version of the tool, which has been implemented, is described from the two distinct viewpoints of a user and of a measurement-tool designer. An example of the application of the prototype to a measurement problem is illustrated, the reasons why not all of the design goals have been achieved in the implementation of the prototype are reviewed, and some of the foreseeable extensions of the SMT are described.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {94--105},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809376},
 doi = {10.1145/1007773.809376},
 acmid = {809376},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Foley:1974:EDC:800277.809377,
 author = {Foley, James D. and McInroy, John W.},
 title = {An event-driven data collection and analysis facility for a two-computer network},
 abstract = {In this paper we describe an event-driven data collection facility, and a general-purpose program to perform a set of analyses on the collected data. There are several features which distinguish this facility from others. First, the system being monitored is a network of loosely-coupled computers. Although there are just two computers in the network, the facility could be readily extended to larger networks. Second, the main purpose of the facility is to monitor the execution of interactive graphics application programs whose processing and data are distributed between the network's computers. Third, the data collector and analyzer are readily extendible to treat new kinds of data. This is accomplished by a data and event independent collector, and a table-driven data analyzer.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {106--120},
 numpages = {15},
 url = {http://portal.acm.org/citation.cfm?id=800277.809377},
 doi = {10.1145/800277.809377},
 acmid = {809377},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Foley:1974:EDC:1007773.809377,
 author = {Foley, James D. and McInroy, John W.},
 title = {An event-driven data collection and analysis facility for a two-computer network},
 abstract = {In this paper we describe an event-driven data collection facility, and a general-purpose program to perform a set of analyses on the collected data. There are several features which distinguish this facility from others. First, the system being monitored is a network of loosely-coupled computers. Although there are just two computers in the network, the facility could be readily extended to larger networks. Second, the main purpose of the facility is to monitor the execution of interactive graphics application programs whose processing and data are distributed between the network's computers. Third, the data collector and analyzer are readily extendible to treat new kinds of data. This is accomplished by a data and event independent collector, and a table-driven data analyzer.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {106--120},
 numpages = {15},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809377},
 doi = {10.1145/1007773.809377},
 acmid = {809377},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Batson:1974:MVM:1007773.809378,
 author = {Batson, A. P. and Brundage, R. E.},
 title = {Measurements of the virtual memory demands of Algol-60 programs (Extended Abstract)},
 abstract = {Programming languages such as Algol-60 use block structure to express the way in which the name space of the current environment, in the contour model (1) sense of that word, changes during program execution. This dynamically-varying name space corresponds to the virtual memory required by the process during its execution on a computer system. The research to be presented is an empirical study of the nature of the memory demands made by a collection of Algol-60 programs during execution. The essential characteristics of any such resource requiest are the amount of memory requested, and the holding time for which the resource is retained and these distributions will be presented for several components of the virtual memory required by the Algol programs.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {121--126},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809378},
 doi = {10.1145/1007773.809378},
 acmid = {809378},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Batson:1974:MVM:800277.809378,
 author = {Batson, A. P. and Brundage, R. E.},
 title = {Measurements of the virtual memory demands of Algol-60 programs (Extended Abstract)},
 abstract = {Programming languages such as Algol-60 use block structure to express the way in which the name space of the current environment, in the contour model (1) sense of that word, changes during program execution. This dynamically-varying name space corresponds to the virtual memory required by the process during its execution on a computer system. The research to be presented is an empirical study of the nature of the memory demands made by a collection of Algol-60 programs during execution. The essential characteristics of any such resource requiest are the amount of memory requested, and the holding time for which the resource is retained and these distributions will be presented for several components of the virtual memory required by the Algol programs.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {121--126},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=800277.809378},
 doi = {10.1145/800277.809378},
 acmid = {809378},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sebastian:1974:H(E:800277.809379,
 author = {Sebastian, Peter R.},
 title = {HEMI (Hybrid Events Monitoring Instrument)},
 abstract = {HEMI is an experimental instrumentation system being developed for use with the CYBER 70 and 170 Series computers in order to ascertain the extent to which an integrated approach to instrumentation is economically and technologically viable for performance measurement and evaluation purposes. HEMI takes advantage of the distributed CYBER computer architecture. This consists of a pool of Peripheral Processors (PPs) - (mainly dedicated to I/O and system tasks) while the CPU capabilities are reserved mostly for computation; Central Memory constitutes the communications link. HEMI uses one of the PPs as its major processor. A hardware data acquisition front end is interfaced to one of the I/O channels and driven by the PP. Hardware probes sample events at suitable testpoints, while the PP has software access to Central Memory (Operating System tables and parameters), Status Registers, I/O Channel Flags, etc. A data reduction package is used to produce a variety of reports from the data collected. A limited on-line data reduction and display capability is also provided. This paper will describe the current status of the project as well as anticipated applications of HEMI.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {127--139},
 numpages = {13},
 url = {http://portal.acm.org/citation.cfm?id=800277.809379},
 doi = {10.1145/800277.809379},
 acmid = {809379},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sebastian:1974:H(E:1007773.809379,
 author = {Sebastian, Peter R.},
 title = {HEMI (Hybrid Events Monitoring Instrument)},
 abstract = {HEMI is an experimental instrumentation system being developed for use with the CYBER 70 and 170 Series computers in order to ascertain the extent to which an integrated approach to instrumentation is economically and technologically viable for performance measurement and evaluation purposes. HEMI takes advantage of the distributed CYBER computer architecture. This consists of a pool of Peripheral Processors (PPs) - (mainly dedicated to I/O and system tasks) while the CPU capabilities are reserved mostly for computation; Central Memory constitutes the communications link. HEMI uses one of the PPs as its major processor. A hardware data acquisition front end is interfaced to one of the I/O channels and driven by the PP. Hardware probes sample events at suitable testpoints, while the PP has software access to Central Memory (Operating System tables and parameters), Status Registers, I/O Channel Flags, etc. A data reduction package is used to produce a variety of reports from the data collected. A limited on-line data reduction and display capability is also provided. This paper will describe the current status of the project as well as anticipated applications of HEMI.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {127--139},
 numpages = {13},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809379},
 doi = {10.1145/1007773.809379},
 acmid = {809379},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cox:1974:IAC:1007773.809380,
 author = {Cox, Springer W.},
 title = {Interpretive analysis of computer system performance},
 abstract = {A typical performance evaluation consists of the identification of resources, the definition of system boundaries, the measurement of external and internal performance variables, and finally the interpretation of data and projection of system performance to hypothetical environments. These projections may be used to estimate the cost savings to be expected when changes are made to the system. The fundamental external performance measures such as response time and thruput are intimately related, but may be defined differently depending on how the system is defined. They can be analyzed with respect to the internal performance measures (such as activities, queue lengths and busy times) by applying one or more interpretations such as: absolute utilizations, normalized busy times, system profiles, analysis of response, workload relaxation, and resource consumption hyperplanes. These models, which are generally free of assumptions regarding interarrival and service time distributions, can be adjusted to represent potential changes to the system. Then the interpretations may be used to evaluate the predicted external performance measures.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {140--155},
 numpages = {16},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809380},
 doi = {10.1145/1007773.809380},
 acmid = {809380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Cox:1974:IAC:800277.809380,
 author = {Cox, Springer W.},
 title = {Interpretive analysis of computer system performance},
 abstract = {A typical performance evaluation consists of the identification of resources, the definition of system boundaries, the measurement of external and internal performance variables, and finally the interpretation of data and projection of system performance to hypothetical environments. These projections may be used to estimate the cost savings to be expected when changes are made to the system. The fundamental external performance measures such as response time and thruput are intimately related, but may be defined differently depending on how the system is defined. They can be analyzed with respect to the internal performance measures (such as activities, queue lengths and busy times) by applying one or more interpretations such as: absolute utilizations, normalized busy times, system profiles, analysis of response, workload relaxation, and resource consumption hyperplanes. These models, which are generally free of assumptions regarding interarrival and service time distributions, can be adjusted to represent potential changes to the system. Then the interpretations may be used to evaluate the predicted external performance measures.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {140--155},
 numpages = {16},
 url = {http://portal.acm.org/citation.cfm?id=800277.809380},
 doi = {10.1145/800277.809380},
 acmid = {809380},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Noe:1974:DYC:800277.809381,
 author = {Noe, J. D. and Runstein, N. W.},
 title = {Develop your computer performance pattern},
 abstract = {Is the load on your computer shifting? Did that change to faster access disks really help? Would more core memory increase throughput appreciably, or would it be necessary to also increase central processor power? These are three quite different kinds of questions; one concerns detecting a long-term trend, another assessing the effects of a system change, and a third estimating effects of the decision to alter the configuration. Yet all of these require knowledge of current and past system performance, the type of knowledge that must be the result of long-term performance monitoring. This is not simple enough to be picked up overnight or in one series of experiments, nor can it be assessed by watching one or two parameters over a long period. One must have a thorough understanding of the pattern of performance by knowing the mean values of a number of measures and knowing something about the variations from these means. This paper hardly needs to recommend that computer managers establish an understanding of performance pattern; they already are very conscious of the need. What it does is recount development of a method of doing so for the CDC 6400 at the University of Washington and of the selection of ``Kiviat Graphs" as a means to present data in a synoptic form. The remainder of this paper will give a brief account of the authors' experience in designing a measurement system for the CDC 6400 at the University of Washington Computer Center. This will include comments on the approach to deciding what to measure an d display for the synoptic view of the system, as well as how to provide more detailed data for backup. Examples of the use of Kiviat Graphs [4] to show the effects of load shift and of a system configuration change are included, and the effect of a change of operating system will be noted.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {156--165},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=800277.809381},
 doi = {10.1145/800277.809381},
 acmid = {809381},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Noe:1974:DYC:1007773.809381,
 author = {Noe, J. D. and Runstein, N. W.},
 title = {Develop your computer performance pattern},
 abstract = {Is the load on your computer shifting? Did that change to faster access disks really help? Would more core memory increase throughput appreciably, or would it be necessary to also increase central processor power? These are three quite different kinds of questions; one concerns detecting a long-term trend, another assessing the effects of a system change, and a third estimating effects of the decision to alter the configuration. Yet all of these require knowledge of current and past system performance, the type of knowledge that must be the result of long-term performance monitoring. This is not simple enough to be picked up overnight or in one series of experiments, nor can it be assessed by watching one or two parameters over a long period. One must have a thorough understanding of the pattern of performance by knowing the mean values of a number of measures and knowing something about the variations from these means. This paper hardly needs to recommend that computer managers establish an understanding of performance pattern; they already are very conscious of the need. What it does is recount development of a method of doing so for the CDC 6400 at the University of Washington and of the selection of ``Kiviat Graphs" as a means to present data in a synoptic form. The remainder of this paper will give a brief account of the authors' experience in designing a measurement system for the CDC 6400 at the University of Washington Computer Center. This will include comments on the approach to deciding what to measure an d display for the synoptic view of the system, as well as how to provide more detailed data for backup. Examples of the use of Kiviat Graphs [4] to show the effects of load shift and of a system configuration change are included, and the effect of a change of operating system will be noted.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {156--165},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809381},
 doi = {10.1145/1007773.809381},
 acmid = {809381},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Brotherton:1974:CCC:800277.809382,
 author = {Brotherton, D. E.},
 title = {The computer capacity curve - a prerequisite for computer performance evaluation and improvement},
 abstract = {Measurements of themselves have tended to concentrate on specific computer configuration components (e.g., CPU load, channel load, disk data set contention, problem program optimization, operating system optimization, etc.) rather than at the total computer configuration level. As a consequence, since these components can have a high degree of interaction, the requirement currently exists for a workable configuration performance concept which will reflect the configuration performance change that is the resultant of single or multiple component change. It is the author's opinion that such a concept will provide management and measurement specialists a planning and analysis tool that can be well Used in evaluating the costs. It is to this configuration performance concept that this paper is addressed, and the concept by my choosing is named ``The Computer Capacity Curve."},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {166--179},
 numpages = {14},
 url = {http://portal.acm.org/citation.cfm?id=800277.809382},
 doi = {10.1145/800277.809382},
 acmid = {809382},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Brotherton:1974:CCC:1007773.809382,
 author = {Brotherton, D. E.},
 title = {The computer capacity curve - a prerequisite for computer performance evaluation and improvement},
 abstract = {Measurements of themselves have tended to concentrate on specific computer configuration components (e.g., CPU load, channel load, disk data set contention, problem program optimization, operating system optimization, etc.) rather than at the total computer configuration level. As a consequence, since these components can have a high degree of interaction, the requirement currently exists for a workable configuration performance concept which will reflect the configuration performance change that is the resultant of single or multiple component change. It is the author's opinion that such a concept will provide management and measurement specialists a planning and analysis tool that can be well Used in evaluating the costs. It is to this configuration performance concept that this paper is addressed, and the concept by my choosing is named ``The Computer Capacity Curve."},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {166--179},
 numpages = {14},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809382},
 doi = {10.1145/1007773.809382},
 acmid = {809382},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Erikson:1974:VCU:1007773.809383,
 author = {Erikson, Warren J.},
 title = {The value of CPU utilization as a criterion for computer system usage},
 abstract = {It is generally agreed that a computer system's CPU utilization means little by itself, but there has been only a limited amount of research to determine the value of CPU utilization when used with other performance measures. This paper focuses on time-sharing systems (or similar systems such as some remote batch systems) as viewed by someone who wants to minimize the mean cost per job run on the system. The paper considers cost per job to include both the computer cost (as allocated among all the jobs run on the system) and the user cost (where user cost is the time spent waiting for a response from the system multiplied by the user's wage rate). Given this approach, cost per job is a function of some constants (user wage rate, computer system cost, and mean processing time per job) and only one variable (CPU utilization). The model thus developed can be used to determine the optimum CPU utilization for any system. It can also be used to determine the value of different tuning efforts.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {180--187},
 numpages = {8},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809383},
 doi = {10.1145/1007773.809383},
 acmid = {809383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Erikson:1974:VCU:800277.809383,
 author = {Erikson, Warren J.},
 title = {The value of CPU utilization as a criterion for computer system usage},
 abstract = {It is generally agreed that a computer system's CPU utilization means little by itself, but there has been only a limited amount of research to determine the value of CPU utilization when used with other performance measures. This paper focuses on time-sharing systems (or similar systems such as some remote batch systems) as viewed by someone who wants to minimize the mean cost per job run on the system. The paper considers cost per job to include both the computer cost (as allocated among all the jobs run on the system) and the user cost (where user cost is the time spent waiting for a response from the system multiplied by the user's wage rate). Given this approach, cost per job is a function of some constants (user wage rate, computer system cost, and mean processing time per job) and only one variable (CPU utilization). The model thus developed can be used to determine the optimum CPU utilization for any system. It can also be used to determine the value of different tuning efforts.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {180--187},
 numpages = {8},
 url = {http://portal.acm.org/citation.cfm?id=800277.809383},
 doi = {10.1145/800277.809383},
 acmid = {809383},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Badel:1974:AOP:1007773.809384,
 author = {Badel, M. and Gelenbe, E. and Leroudier, J. and Potier, D. and Lenfant, J.},
 title = {Adaptive optimization of the performance of a virtual memory computer},
 abstract = {It is known that the regulation of the degree of multiprogramming is perhaps one of the most important factors determining the overall performance of a virtual memory computer. In this paper we present an approach which differs some what from the approaches usually taken to regulate the degree of multiprogramming, which are mainly derived from the working-set principles. We design a controller which will regulate the system in order to optimize a given performance measure. The controller is applied to a system where the critical resource is primary memory, and we are only concerned with systems where ineffective regulation leads to the phenomenon known as thrashing due to extensive paging activity. In the first section, the dynamics of the system we wish to regulate are investigated using an analytical model. The system consists of a set of terminals and of a resource loop (CPU, secondary memory device, file disk) shared by the users. Using classical assumptions about program behavior (e.g., life-time function), the throughput of the RL is obtained as a function of the degree of multiprogramming n (number of users sharing the resources at a given instant of time) and of the system parameters. This result provides a greater insight of the ``plant" we wish to control. The mathematical results are validated and extended with data from simulation experiments using a more detailed model (overheads and non-exponential assumption). In the next section, a criterion called ``dilatation" based on the utilization of the different resources is defined. From the analytical and simulation results of the first section, it can be shown that there exists a value n<subscrpt>o</subscrpt> of the degree of multiprogramming which maximizes this criterion. The regulation of n to n<subscrpt>o</subscrpt> is achieved by controlling the access of the users to the RL. The value of n<subscrpt>o</subscrpt> is estimated in real-time through a continuous estimation of the two first moments of the criterion. Using these estimations, the decision of introducing or not a new user in the RL is taken whenever a user leaves a terminal or departs from the RL. Extensive simulation experiments were conducted, where the implementation of the different functions of the controller have been thoroughly simulated. They have shown that the control scheme leaves to an improvement of the system performance in mean response time and resource utilization, and, overall, adapts in real-time the degree of multiprogramming to the characteristics of the users (the adaptation is performed in 4 sec. or so for a unit variation of the optimal degree of multiprogramming). A discussion of practical application of results ends the paper.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {188--},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809384},
 doi = {10.1145/1007773.809384},
 acmid = {809384},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Badel:1974:AOP:800277.809384,
 author = {Badel, M. and Gelenbe, E. and Leroudier, J. and Potier, D. and Lenfant, J.},
 title = {Adaptive optimization of the performance of a virtual memory computer},
 abstract = {It is known that the regulation of the degree of multiprogramming is perhaps one of the most important factors determining the overall performance of a virtual memory computer. In this paper we present an approach which differs some what from the approaches usually taken to regulate the degree of multiprogramming, which are mainly derived from the working-set principles. We design a controller which will regulate the system in order to optimize a given performance measure. The controller is applied to a system where the critical resource is primary memory, and we are only concerned with systems where ineffective regulation leads to the phenomenon known as thrashing due to extensive paging activity. In the first section, the dynamics of the system we wish to regulate are investigated using an analytical model. The system consists of a set of terminals and of a resource loop (CPU, secondary memory device, file disk) shared by the users. Using classical assumptions about program behavior (e.g., life-time function), the throughput of the RL is obtained as a function of the degree of multiprogramming n (number of users sharing the resources at a given instant of time) and of the system parameters. This result provides a greater insight of the ``plant" we wish to control. The mathematical results are validated and extended with data from simulation experiments using a more detailed model (overheads and non-exponential assumption). In the next section, a criterion called ``dilatation" based on the utilization of the different resources is defined. From the analytical and simulation results of the first section, it can be shown that there exists a value n<subscrpt>o</subscrpt> of the degree of multiprogramming which maximizes this criterion. The regulation of n to n<subscrpt>o</subscrpt> is achieved by controlling the access of the users to the RL. The value of n<subscrpt>o</subscrpt> is estimated in real-time through a continuous estimation of the two first moments of the criterion. Using these estimations, the decision of introducing or not a new user in the RL is taken whenever a user leaves a terminal or departs from the RL. Extensive simulation experiments were conducted, where the implementation of the different functions of the controller have been thoroughly simulated. They have shown that the control scheme leaves to an improvement of the system performance in mean response time and resource utilization, and, overall, adapts in real-time the degree of multiprogramming to the characteristics of the users (the adaptation is performed in 4 sec. or so for a unit variation of the optimal degree of multiprogramming). A discussion of practical application of results ends the paper.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {188--},
 url = {http://portal.acm.org/citation.cfm?id=800277.809384},
 doi = {10.1145/800277.809384},
 acmid = {809384},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kimbleton:1974:BCS:800277.809385,
 author = {Kimbleton, Stephen R.},
 title = {Batch computer scheduling: A heuristically motivated approach},
 abstract = {Efficient scheduling of jobs for computer systems is a problem of continuing concern. The applicability of scheduling methodology described in the operations research literature is severely restricted by the dimensionality of job characteristics, the number of distinct resource types comprising a computer system, the non-deterministic nature of the system due to both interprocess interaction and contention, and the existence of a multitude of constraints effecting job initiation times, job completion times, and job interactions. In view of the large number of issues which must be considered in job scheduling, a heuristic approach seems appropriate. This paper describes an initial implementation of such an approach based upon a fast, analytically driven, performance prediction tool.},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {189--198},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=800277.809385},
 doi = {10.1145/800277.809385},
 acmid = {809385},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Kimbleton:1974:BCS:1007773.809385,
 author = {Kimbleton, Stephen R.},
 title = {Batch computer scheduling: A heuristically motivated approach},
 abstract = {Efficient scheduling of jobs for computer systems is a problem of continuing concern. The applicability of scheduling methodology described in the operations research literature is severely restricted by the dimensionality of job characteristics, the number of distinct resource types comprising a computer system, the non-deterministic nature of the system due to both interprocess interaction and contention, and the existence of a multitude of constraints effecting job initiation times, job completion times, and job interactions. In view of the large number of issues which must be considered in job scheduling, a heuristic approach seems appropriate. This paper describes an initial implementation of such an approach based upon a fast, analytically driven, performance prediction tool.},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {189--198},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809385},
 doi = {10.1145/1007773.809385},
 acmid = {809385},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Sharp:1974:APD:800277.809386,
 author = {Sharp, Joseph C. and Roberts, James N.},
 title = {An adaptive policy driven scheduler},
 abstract = {The theory of policy driven schedulers (Ref. [1]) is extended to cover cases in which the scheduling parameters are allowed to adapt dynamically as the system's job load varies. The system under consideration offers batch, time sharing and limited real time services. Data from simulated and live loads are presented to evaluate both the static and the adaptive schedulers. A policy driven scheduler makes its decisions with respect to a set of policy functions, f<subscrpt>i</subscrpt>(t). Each of the policy functions corresponds to a different type of user and specifies the amount of computing resources that the system will try to give a user in that group within a given total amount of elapsed time. It is found that the policy functions must be set conservatively in order to avoid response problems during periods of heavy load, but that during more lightly loaded periods the conservative settings result in widely disparate rates of service to similar jobs. One solution is to vary the policy functions as the job load changes. A dynamic algorithm is presented that maintains responsiveness during heavy loads and provides fairly uniform service rates at other times. <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
},
 booktitle = {Proceedings of the 1974 ACM SIGMETRICS conference on Measurement and evaluation},
 year = {1974},
 pages = {199--208},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=800277.809386},
 doi = {10.1145/800277.809386},
 acmid = {809386},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Sharp:1974:APD:1007773.809386,
 author = {Sharp, Joseph C. and Roberts, James N.},
 title = {An adaptive policy driven scheduler},
 abstract = {The theory of policy driven schedulers (Ref. [1]) is extended to cover cases in which the scheduling parameters are allowed to adapt dynamically as the system's job load varies. The system under consideration offers batch, time sharing and limited real time services. Data from simulated and live loads are presented to evaluate both the static and the adaptive schedulers. A policy driven scheduler makes its decisions with respect to a set of policy functions, f<subscrpt>i</subscrpt>(t). Each of the policy functions corresponds to a different type of user and specifies the amount of computing resources that the system will try to give a user in that group within a given total amount of elapsed time. It is found that the policy functions must be set conservatively in order to avoid response problems during periods of heavy load, but that during more lightly loaded periods the conservative settings result in widely disparate rates of service to similar jobs. One solution is to vary the policy functions as the job load changes. A dynamic algorithm is presented that maintains responsiveness during heavy loads and provides fairly uniform service rates at other times. <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 volume = {3},
 issue = {4},
 month = {January},
 year = {1974},
 issn = {0163-5999},
 pages = {199--208},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=1007773.809386},
 doi = {10.1145/1007773.809386},
 acmid = {809386},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

